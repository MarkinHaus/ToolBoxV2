model:
  max_seq_len: 128
  embedding_dim: 256
  num_layers: 4
  num_attn_heads: 8
  lsm_reservoir_size: 512
  lsm_sparsity: 0.9
  lif_beta: 0.95
  lif_sfa_beta: 0.99
  lif_delta_threshold: 0.02
  lif_initial_threshold: 1.0
  reasoning_steps: 2
  use_spikes: false
  use_ternary: false
  lsm_e_i_ratio: 0.8
  recurrent_weight_clamp: 1.5
  lif_beta_mean: 0.95
  lif_beta_std: 0.05
  use_sfa: true
  sfa_increment: 0.05
  lif_tau_sfa_mean: 150.0
  lif_tau_sfa_std: 25.0
  dropout_rate: 0.1
  ff_loss_weight: 0.5
training:
  device: cpu
  batch_size: 8
  epochs: 10
  learning_rate: 0.0001
  clip_grad_norm: 1.0
  num_dataloader_workers: 2
  lambda_pixel_loss: 1.0
  lambda_perceptual_loss: 0.05
  steps_per_epoch: 5000
  tts_batch_size: 4
  num_classes: 10
pretraining:
  epochs: 20
  batch_size: 4
  gradient_accumulation_steps: 2
  chunks_per_sequence_step: 2
  learning_rate: 0.0001
  weight_decay: 1.0e-05
  use_amp: true
  use_torch_compile: false
  torch_compile_mode: default
  clip_grad_norm: 1.0
  log_interval: 50
  num_dataloader_workers: 2
  min_chunk_duration_sec: 1.0
  mask_probability: 0.15
  no_text_chunk_ratio: 0.1
  loss_weights:
    mlm: 1.0
    v_mpp: 0.7
    a_mpp: 0.7
    img_recon: 0.8
  lambda_pixel_loss: 1.0
  lambda_perceptual_loss: 0.1
data:
  image_in_channels: 1
  image_patch_size: 28
  audio_n_mels: 80
  max_text_len: 256
  vocab_size: 30522
  tokenizer_model: bert-base-uncased
  chunk_duration_sec: 5
  video_fps_sampling: 1
  audio_patch_kernel_size:
  - 10
  - 4
  audio_patch_stride:
  - 10
  - 4
  audio_f_min: 0.0
  audio_f_max: 8000.0
inference:
  default_task: text
  text_head: {}
  text_max_new_tokens: 128
  text_temperature: 0.2
  text_top_k: 40
  text_repetition_penalty: 1.25
  text_no_repeat_ngram_size: 3
live_feedback:
  enabled: true
  log_interval: 10
  speak_feedback: false
paths:
  checkpoints: checkpoints
  temp_data: temp_data
  log_dir: logs
  generated_video_frames_dir: generated_video_frames
vocoder:
  n_fft: 1024
  hop_length: 256
  win_length: 1024
