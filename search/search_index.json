{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ToolBoxV2 \ud83e\uddf0","text":"<p>ToolBoxV2 is a flexible, modular framework designed for creating and managing a wide range of tools, functions, and complete applications. It supports deployment locally, on the web, or as cross-platform desktop/mobile applications.</p> <p>At its core, ToolBoxV2 integrates a Python backend with a Rust server and a Tauri-based UI, offering a powerful and versatile development experience.</p> <ul> <li>Free software: Custom License</li> <li>Officel Web page: https://simplecore.app/</li> <li>GitHub Repository: https://github.com/MarkinHaus/ToolBoxV2</li> </ul>"},{"location":"#key-goals-features","title":"Key Goals &amp; Features","text":"<p>ToolBoxV2 aims to simplify the development and usage of digital tools by:</p> <ul> <li>\ud83d\udd0c Modularity: Build applications from reusable Python modules (<code>mods</code>) and utilities (<code>utils</code>).</li> <li>\u2699\ufe0f Automation: Facilitate automation of tasks through CLI interactions and programmable APIs.</li> <li>\ud83c\udf10 Cross-Platform Interfaces:<ul> <li>Develop Desktop Applications using Tauri (Rust + Web UI).</li> <li>Create Web Applications with the <code>tbjs</code> frontend framework.</li> <li>Interact via a robust Command Line Interface (CLI).</li> </ul> </li> <li>\ud83d\ude80 Performance &amp; Safety: Leverage Rust for backend server components (Actix) and Python for scripting and application logic.</li> <li>\ud83e\udde9 Extensibility: Easily create and integrate new functions, tools, or full mini-applications.</li> <li>System Independence: Strives to make applications and tools runnable across different operating systems.</li> <li>Unified Development: Provides a cohesive environment for Python, Rust, and web technologies.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li> <p>Installation:     For detailed instructions on how to install the core Python library or set up the full-stack development environment, please see the Installation Guide.     <pre><code># Quick install for the Python package\npip install ToolBoxV2\n</code></pre></p> </li> <li> <p>Developer Guide:     To learn how to create modules, use the <code>App</code> class, and interact with the CLI, explore the full Developer Documentation. </p> </li> <li> <p>Explore the Code:     Dive into the GitHub Repository to see the project structure and contribute.</p> </li> </ul>"},{"location":"#example-use-cases","title":"Example Use Cases","text":"<p>ToolBoxV2 can be used for: *   Personal productivity tools (calendars, note-takers). *   Development utilities and automation scripts. *   Custom internal business applications. *   Interactive data processing and visualization tools. *   And much more!</p>"},{"location":"#credits","title":"Credits","text":"<p>This package was created with inspiration from project structures like those generated by Cookiecutter and templates such as giswqs/pypackage.</p> <p>\u00a9 2022\u20132025 Markin Hausmanns \u2013 All rights reserved.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/MarkinHaus/ToolBoxV2/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>ToolBox could always use more documentation, whether as part of the official ToolBox docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/MarkinHaus/ToolBoxV2/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up ToolBoxV2 for local development.</p> <ol> <li> <p>Fork the ToolBoxV2 repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> </li> </ol> <pre><code>$ git clone git@github.com:MarkinHaus/ToolBoxV2.git\n</code></pre> <ol> <li>Install your local copy into a virtualenv. Assuming you have    virtualenvwrapper installed, this is how you set up your fork for    local development:</li> </ol> <pre><code>$ mkvirtualenv ToolBoxV2\n$ cd ToolBoxV2/\n$ python setup.py develop\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li>When you're done making changes, check that your changes pass flake8    and the tests, including testing other Python versions with tox:</li> </ol> <pre><code>$ flake8 ToolBoxV2 tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.    Put your new functionality into a function with a docstring, and add    the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and    for PyPy. Check https://github.com/MarkinHaus/ToolBoxV2/pull_requests and make sure that the tests pass for all    supported Python versions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#gei-isaa-redy-in-conda-with-cuda-conda-install-pytorch-torchvision-torchaudio-pytorch-cuda124-c-pytorch-c-nvidia","title":"Gei isaa redy in conda with cuda  # conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia","text":""},{"location":"faq/#errors","title":"Errors :","text":""},{"location":"faq/#modulenotfounderror-no-module-named-_cffi_backend-fix-pip-vvv-install-upgrade-force-reinstall-cffi","title":"ModuleNotFoundError: No module named '_cffi_backend' fix -&gt; pip -vvv install --upgrade --force-reinstall cffi","text":""},{"location":"faq/#extraes-langchain-experimental-astor-pyaudio-pebble-transformers-litellm-nltk-gpt4all-speechrecognition-chromadb-pydub-duckduckgo-search-langchain-groq-beautifulsoup4-langchain-huggingface-langchain-langchain-chroma-langchain-ollama-tiktoken","title":"extraes : langchain-experimental astor PyAudio Pebble transformers litellm nltk gpt4all SpeechRecognition chromadb pydub duckduckgo-search langchain-groq beautifulsoup4 langchain-huggingface langchain  langchain-chroma langchain-ollama tiktoken","text":""},{"location":"installation/","title":"ToolBoxV2: Installation Guide","text":"<p>This guide provides instructions for installing ToolBoxV2, whether you need just the core Python library or the full-stack application including the Rust server and Tauri/Web frontend.</p>"},{"location":"installation/#1-installing-the-core-python-library","title":"1. Installing the Core Python Library","text":"<p>This method is suitable if you primarily need to use ToolBoxV2 as a Python library within your own projects or want to develop Python-based modules for it.</p>"},{"location":"installation/#option-a-stable-release-from-pypi-recommended","title":"Option A: Stable Release from PyPI (Recommended)","text":"<p>This is the preferred method for installing the latest stable release of the ToolBoxV2 Python package.</p> <ol> <li> <p>Ensure you have Python and pip:     If you don't have Python and pip installed, this Python installation guide can help. We recommend Python 3.10 or newer.</p> </li> <li> <p>Install ToolBoxV2:     Open your terminal or command prompt and run:     <pre><code>pip install ToolBoxV2\n</code></pre> Consider using a virtual environment to manage project dependencies: <pre><code># Create a virtual environment (optional but recommended)\npython -m venv .venv\n# Activate it (Windows)\n# .venv\\Scripts\\activate\n# Activate it (macOS/Linux)\n# source .venv/bin/activate\n\npip install ToolBoxV2\n</code></pre></p> </li> </ol>"},{"location":"installation/#option-b-from-source-latest-development-version","title":"Option B: From Source (Latest Development Version)","text":"<p>This method allows you to get the very latest code from the GitHub repository, which might include new features or changes not yet in a stable release.</p> <ol> <li> <p>Clone the Repository: <pre><code>git clone https://github.com/MarkinHaus/ToolBoxV2.git\ncd ToolBoxV2\n</code></pre></p> </li> <li> <p>Install in Editable Mode:     This installs the package from your local clone, and any changes you make to the source code will be immediately reflected in your environment.</p> <ul> <li>Using pip: <pre><code># Recommended: Activate a virtual environment first\npip install -e .\n</code></pre></li> <li>Using <code>uv</code> (a fast Python package installer and resolver): <pre><code># Recommended: Activate a virtual environment first\nuv pip install -e .\n</code></pre></li> <li>Using the provided script (sets up environment):     This script creates a virtual environment and installs dependencies.     <pre><code>chmod +x install_python_env.sh\n./install_python_env.sh\n</code></pre></li> </ul> </li> </ol>"},{"location":"installation/#option-c-directly-from-github-with-pip","title":"Option C: Directly from GitHub with pip","text":"<p>You can also install directly from the GitHub repository without cloning it first: <pre><code>pip install git+https://github.com/MarkinHaus/ToolBoxV2.git\n</code></pre></p>"},{"location":"installation/#2-installing-the-full-stack-desktopweb-application","title":"2. Installing the Full Stack Desktop/Web Application","text":"<p>This setup is for developers who want to run or develop the complete ToolBoxV2 application, including the Python backend, Rust server (Actix), and the Tauri-based desktop application or <code>tbjs</code> web frontend.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed on your system:</p> <ul> <li>Python: Version 3.10 or higher.</li> <li>Rust and Cargo: Install from rust-lang.org.</li> <li>Node.js and npm/pnpm: Install from nodejs.org. We recommend <code>pnpm</code> for managing Node.js dependencies in this project.<ul> <li>Install <code>pnpm</code> globally: <code>npm install -g pnpm</code></li> </ul> </li> <li>Tauri CLI: Install using Cargo: <code>cargo install tauri-cli</code></li> </ul> <p>Ensure the virtual environment created by the script (or one you created manually) is activated for the subsequent steps.</p> <ol> <li>Install Node.js Dependencies and Build Rust Components:     From the root of the <code>ToolBoxV2</code> directory:     <pre><code>pnpm install  # Installs Node.js dependencies for tbjs and Tauri frontend\n</code></pre>     The Rust backend (<code>src-core/</code>) and Tauri components are typically built as part of the <code>pnpm</code> scripts defined in <code>package.json</code>. If you need to build the Rust core manually:     <pre><code># (Usually not needed if using pnpm scripts)\n# cargo build --release --manifest-path src-core/Cargo.toml\n</code></pre> the build step is Usually handled by the api flow</li> </ol>"},{"location":"installation/#running-the-application-in-cli","title":"Running the Application in CLI","text":"<ul> <li>Row python runner tb <pre><code>tb -c {MOD_NAME} {FUCTION_NAME} {AGRGS} --kwargs name:value\n</code></pre></li> <li>or run in ipython <pre><code>tb --ipy\n</code></pre></li> </ul>"},{"location":"installation/#running-the-application-in-server-mode-for-web-and-desktop","title":"Running the Application in Server mode for web and Desktop","text":"<p>Refer to the scripts in the <code>package.json</code> file for various ways to run and build the application. Common commands include:</p> <ul> <li> <p>Web Development Mode (tbjs frontend with hot-reloading): <pre><code>pnpm dev\n# or live\n</code></pre>     This typically starts the Rust server and the web frontend development server.</p> </li> <li> <p>Tauri Desktop Application (Development Mode): <pre><code>pnpm tauri dev\n</code></pre>     This will build and run the Tauri desktop application with hot-reloading for the frontend.</p> </li> <li> <p>Build Tauri Desktop Application (Production): <pre><code>pnpm tauri build # Or a custom script like `pnpm tauriB` if defined\n</code></pre>     This creates a distributable binary of the desktop application.</p> </li> </ul> <p>For more specific build and run commands, please consult the <code>scripts</code> section in the <code>package.json</code> file located in the <code>ToolBoxV2</code> repository root or use the CLI help: <pre><code>    tb --help\n    # or\n    python -m toolboxv2 --help\n</code></pre></p>"},{"location":"installation/#developing-tip-use-to-activate-all-hooks","title":"developing tip use to activate all hooks","text":"<pre><code>    bash .github/hooks/setup_hooks.sh\n</code></pre>"},{"location":"installation/#auto-version-commit-hook-add-to-the-commit-msg-and-for-auto-summary","title":"auto version commit hook add &lt;#&gt; to the commit msg and  for auto summary","text":""},{"location":"installation/#auto-tagging-of-version-dev-alpha-or-release-tagging-syntax-in-commit-msg","title":"auto tagging of version dev, alpha or release tagging syntax in commit msg <ul> <li>[t:d] for dev</li> <li>[t:a] for alpha and</li> <li>[t:r] for release</li> </ul> <p>all with auto versioning</p>","text":""},{"location":"installation/#pre-commit-hook","title":"pre-commit hook","text":"<p>runs Ruff Bandit Safety versions and on  in the commit msg auto summary of the changes crates an report in local-reports"},{"location":"installation/#_1","title":"????????? <p><pre><code>INSTALLER_URL=\"https://raw.githubusercontent.com/MarkinHaus/ToolBoxV2/refs/heads/master/installer.sh\"; (echo \"Fetching installer script...\" &amp;&amp; curl -sSL -o installer.sh \"$INSTALLER_URL\" &amp;&amp; echo \"Creating default 'init.config'...\" &amp;&amp; cat &lt;&lt;EOL &gt; init.config &amp;&amp; echo \"# ToolBoxV2 Installer Configuration\" &amp;&amp; echo \"# File will be located at: $(pwd)/init.config\" &amp;&amp; echo \"# Modify values below as needed before proceeding.\" &amp;&amp; echo \"# The installer (installer.sh) will use these if this file exists and no arguments are provided to it.\" &amp;&amp; echo \"# --- Example values (uncomment and change if needed): ---\" &amp;&amp; echo \"# TB_VERSION=latest\" &amp;&amp; echo \"# INSTALL_SOURCE=pip\" &amp;&amp; echo \"# PKG_MANAGER=pip\" &amp;&amp; echo \"# PYTHON_VERSION_TARGET=3.11\" &amp;&amp; echo \"# ISAA_EXTRA=false\" &amp;&amp; echo \"# DEV_EXTRA=false\" &amp;&amp; echo \"# INSTALL_LOCATION_TYPE=apps_folder\" &amp;&amp; EOL &amp;&amp; INIT_CONFIG_PATH=\"$(pwd)/init.config\" &amp;&amp; echo -e \"\\n\\033[0;32m\ud83d\udcc4 Default 'init.config' created at:\\033[0m \\033[1;33m$INIT_CONFIG_PATH\\033[0m\" &amp;&amp; echo -e \"   You can review or modify it now in another terminal if you wish.\" &amp;&amp; echo -e \"   The main script (installer.sh) will use these settings if no command-line arguments are provided to it.\" &amp;&amp; read -p \"\u23f3 Press [Enter] to make the installer executable and run it...\" REPLY &amp;&amp; chmod +x installer.sh &amp;&amp; echo \"\ud83d\ude80 Running installer...\" &amp;&amp; ./installer.sh) || echo -e \"\\033[0;31m\u274c An error occurred during the setup process. Please check messages above.\\033[0m\"\n</code></pre> onliner installer</p>","text":""},{"location":"tbjs/","title":"tbjs Mini Framework Documentation","text":"<p>Version: 0.1.0-alpha</p> <p><code>tbjs</code> is a lightweight, modular JavaScript framework designed for building modern web applications, with a focus on integrating with tools like HTMX and Three.js. It provides core functionalities such as routing, state management, API communication, and a UI component system.</p>"},{"location":"tbjs/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Getting Started<ul> <li>Prerequisites</li> <li>Installation</li> <li>HTML Setup</li> <li>Initialization</li> </ul> </li> <li>Core Modules<ul> <li>Configuration (<code>TB.config</code>)</li> <li>State Management (<code>TB.state</code>)</li> <li>Routing (<code>TB.router</code>)</li> <li>API Communication (<code>TB.api</code>)</li> <li>Event System (<code>TB.events</code>)</li> <li>Logging (<code>TB.logger</code>)</li> <li>Environment Detection (<code>TB.env</code>)</li> <li>Cryptography (<code>TB.crypto</code>)</li> <li>User Management (<code>TB.user</code>)</li> <li>Server-Sent Events (<code>TB.sse</code>)</li> <li>Service Worker (<code>TB.sw</code>)</li> <li>Utilities (<code>TB.utils</code>)</li> </ul> </li> <li>UI System (<code>TB.ui</code>)<ul> <li>Theme Management (<code>TB.ui.theme</code>)</li> <li>Graphics (<code>TB.graphics</code>)</li> <li>HTMX Integration (<code>TB.ui.htmxIntegration</code>)</li> <li>Dynamic Content Processing</li> <li>Components<ul> <li>Modal (<code>TB.ui.Modal</code>)</li> <li>Toast (<code>TB.ui.Toast</code>)</li> <li>Loader (<code>TB.ui.Loader</code>)</li> <li>Button (<code>TB.ui.Button</code>)</li> <li>DarkModeToggle (<code>TB.ui.DarkModeToggle</code>)</li> <li>CookieBanner (<code>TB.ui.CookieBanner</code>)</li> <li>MarkdownRenderer (<code>TB.ui.MarkdownRenderer</code>)</li> <li>AutocompleteWidget (<code>TB.ui.AutocompleteWidget</code>)</li> <li>NavMenu (<code>TB.ui.NavMenu</code>)</li> </ul> </li> </ul> </li> <li>Usage Examples<ul> <li>Basic Application Setup</li> <li>Fetching Data and Updating State</li> <li>Client-Side Routing</li> <li>Displaying a Modal</li> <li>User Authentication Flow</li> </ul> </li> <li>Styling with Tailwind CSS</li> <li>Building tbjs (For Developers)</li> </ol>"},{"location":"tbjs/#1-introduction","title":"1. Introduction","text":"<p><code>tbjs</code> aims to provide a solid foundation for single-page applications (SPAs) and dynamic web experiences. It's built with modularity in mind, allowing you to use only the parts you need. Key features include:</p> <ul> <li>Configuration-driven: Easily customize framework behavior.</li> <li>State Management: Centralized application state.</li> <li>SPA Router: Handles client-side navigation and view loading.</li> <li>API Abstraction: Simplifies backend communication (HTTP &amp; Tauri).</li> <li>Event Bus: Decoupled communication between modules.</li> <li>UI System: Includes theme management and reusable components.</li> <li>3D Graphics: Integration with Three.js for dynamic backgrounds or scenes.</li> <li>User Authentication: Built-in support for various authentication flows, including WebAuthn.</li> <li>HTMX Friendly: Designed to work alongside HTMX for enhancing HTML.</li> </ul>"},{"location":"tbjs/#2-getting-started","title":"2. Getting Started","text":""},{"location":"tbjs/#prerequisites","title":"Prerequisites","text":"<p>Before using <code>tbjs</code>, ensure you have the following:</p> <ol> <li>HTMX: <code>tbjs</code> is designed to work well with HTMX. Include HTMX in your project.     <pre><code>&lt;script src=\"https://unpkg.com/htmx.org@2.0.0/dist/htmx.min.js\"&gt;&lt;/script&gt;\n</code></pre></li> <li>Three.js (Optional, if using <code>TB.graphics</code>):     <pre><code>&lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/three.js/0.153.0/three.min.js\"&gt;&lt;/script&gt;\n</code></pre></li> <li>Marked &amp; Highlight.js (Optional, if using <code>TB.ui.MarkdownRenderer</code>): These should be available globally.     <pre><code>&lt;script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js\"&gt;&lt;/script&gt;\n&lt;!-- Optional: Styles for highlight.js --&gt;\n&lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css\"&gt;\n&lt;!-- If using marked-highlight extension --&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/marked-highlight/lib/index.umd.js\"&gt;&lt;/script&gt;\n</code></pre> Note: Ensure <code>window.marked</code> and <code>window.hljs</code> (and <code>window.markedHighlight</code> if using the extension) are available before <code>TB.ui.MarkdownRenderer.init()</code> is called or any markdown rendering is attempted.</li> </ol>"},{"location":"tbjs/#installation","title":"Installation","text":"<p><code>tbjs</code> is distributed as <code>tbjs.js</code> and <code>tbjs.css</code>.</p> <ol> <li>Build <code>tbjs</code>: If you have the source, run <code>npm run build</code> to generate the <code>dist/</code> folder.</li> <li>Include files in your HTML:     <pre><code>&lt;link rel=\"stylesheet\" href=\"path/to/your/tbjs/dist/tbjs.css\"&gt;\n&lt;script type=\"module\" src=\"path/to/your/tbjs/dist/tbjs.js\"&gt;&lt;/script&gt;\n</code></pre>     If you are using <code>tbjs</code> as a module in your own build system, you can import it:     <pre><code>import TB from 'path/to/your/tbjs/src/index.js'; // Or from dist/tbjs.js if UMD\n</code></pre></li> </ol>"},{"location":"tbjs/#html-setup","title":"HTML Setup","text":"<p>Your main HTML file (<code>index.html</code>) should have a root element for your application content and optionally, containers for background effects or 3D graphics.</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;My tbjs App&lt;/title&gt;\n\n    &lt;!-- tbjs CSS --&gt;\n    &lt;link rel=\"stylesheet\" href=\"path/to/tbjs/dist/tbjs.css\"&gt;\n\n    &lt;!-- Tailwind (if you're using it directly in your app) --&gt;\n    &lt;!-- &lt;script src=\"https://cdn.tailwindcss.com\"&gt;&lt;/script&gt; --&gt;\n    &lt;!-- Or your compiled Tailwind CSS --&gt;\n    &lt;link rel=\"stylesheet\" href=\"path/to/your/app.css\"&gt;\n\n    &lt;!-- HTMX --&gt;\n    &lt;script src=\"https://unpkg.com/htmx.org@2.0.0/dist/htmx.min.js\"&gt;&lt;/script&gt;\n\n    &lt;!-- Three.js (if using graphics module) --&gt;\n    &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/three.js/0.153.0/three.min.js\"&gt;&lt;/script&gt;\n\n    &lt;!-- Marked &amp; Highlight.js (if using MarkdownRenderer) --&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/marked-highlight/lib/index.umd.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js\"&gt;&lt;/script&gt;\n    &lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/atom-one-dark.min.css\"&gt;\n\n\n    &lt;!-- Material Symbols (used by some components) --&gt;\n    &lt;link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200\" /&gt;\n\n    &lt;!-- tbjs Script (type=\"module\" if loading the ES module directly) --&gt;\n    &lt;!-- Option 1: Loading tbjs as a module --&gt;\n    &lt;!-- &lt;script type=\"module\" src=\"path/to/tbjs/src/index.js\"&gt;&lt;/script&gt; --&gt;\n    &lt;!-- Option 2: Loading the UMD build --&gt;\n    &lt;script src=\"path/to/tbjs/dist/tbjs.js\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;!-- Dedicated background container (used by TB.ui.theme) --&gt;\n    &lt;div id=\"appBackgroundContainer\"&gt;&lt;/div&gt;\n\n    &lt;!-- Optional: Container for 3D graphics (used by TB.graphics) --&gt;\n    &lt;div id=\"threeDScene\" style=\"position: fixed; top: 0; left: 0; width: 100%; height: 100%; z-index: -10;\"&gt;&lt;/div&gt;\n\n    &lt;!-- Main application root --&gt;\n    &lt;div id=\"app-root\"&gt;\n        &lt;!-- Initial content or loader can go here --&gt;\n        &lt;div class=\"loaderCenter\" style=\"display: flex; justify-content: center; align-items: center; height: 100vh;\"&gt;\n            Loading application...\n        &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;!-- Your main application script --&gt;\n    &lt;script type=\"module\"&gt;\n        // Import TB if using ES Modules and not UMD global\n        // import TB from 'path/to/tbjs/src/index.js'; // or from 'path/to/tbjs/dist/tbjs.js' if it's an ES module build\n\n        // Wait for the DOM to be fully loaded\n        document.addEventListener('DOMContentLoaded', () =&gt; {\n            // Access TB (it's global if you included the UMD build)\n            if (window.TB) {\n                window.TB.init({\n                    appRootId: 'app-root', // Matches the div above\n                    baseApiUrl: '/api',    // Your backend API base URL\n                    // baseFileUrl: window.location.origin + '/app/', // If HTML files are in a subfolder\n                    initialState: {\n                        appName: 'My Awesome App'\n                    },\n                    themeSettings: {\n                        defaultPreference: 'system', // 'light', 'dark', or 'system'\n                        background: {\n                            type: 'color', // 'color', 'image', or '3d'\n                            light: { color: '#f0f0f0' },\n                            dark: { color: '#202020' },\n                            // placeholder: { image: '/path/to/placeholder.jpg', displayUntil3DReady: true }\n                        }\n                    },\n                    routes: [\n                        // You can predefine routes, though router primarily fetches HTML\n                        // { path: '/home', component: 'views/home.html' }\n                    ],\n                    logLevel: 'debug', // 'debug', 'info', 'warn', 'error'\n                    serviceWorker: {\n                        enabled: false, // Set to true to enable\n                        url: '/sw.js'\n                    }\n                });\n            } else {\n                console.error(\"TB object not found. Ensure tbjs.js is loaded correctly.\");\n            }\n        });\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"tbjs/#initialization","title":"Initialization","text":"<p>Initialize <code>tbjs</code> by calling <code>TB.init()</code> with your application's configuration. This is typically done in a <code>&lt;script type=\"module\"&gt;</code> tag at the end of your <code>&lt;body&gt;</code> or after the <code>DOMContentLoaded</code> event.</p> <p><pre><code>// In your main application script (e.g., app.js or inline script)\ndocument.addEventListener('DOMContentLoaded', () =&gt; {\n    TB.init({\n        appRootId: 'app-root',\n        baseApiUrl: '/api/v1', // Example: your backend API\n        // baseFileUrl: window.location.origin, // Default\n        initialState: {\n            userPreferences: {\n                notifications: true\n            }\n        },\n        themeSettings: {\n            defaultPreference: 'dark', // 'light', 'dark', or 'system'\n            background: {\n                type: '3d', // 'color', 'image', '3d'\n                light: { color: '#FFFFFF' }, // Fallback if 3D fails or not used\n                dark: { color: '#121212' },  // Fallback\n                placeholder: { image: '/images/background-placeholder.jpg', displayUntil3DReady: true }\n            }\n        },\n        routes: [\n             // Example: Define a route that maps to an HTML file.\n             // { path: '/dashboard', view: '/web/pages/dashboard.html' }, // Not directly used by current router.navigateTo structure\n        ],\n        logLevel: 'info', // 'debug', 'info', 'warn', 'error', 'none'\n        isProduction: false, // Manually set or let tbjs infer\n        serviceWorker: {\n            enabled: true,\n            url: '/custom-sw.js',\n            scope: '/'\n        }\n    });\n\n    // Example: Listen for tbjs initialization completion\n    TB.events.on('tbjs:initialized', (tbInstance) =&gt; {\n        console.log('tbjs is ready!', tbInstance.VERSION);\n        // You can now safely use all TB modules\n        // Example: TB.router.navigateTo('/home');\n    });\n});\n</code></pre> If the <code>appRootId</code> element is not found on the page when <code>TB.init()</code> is called, <code>tbjs</code> will attempt to redirect to <code>/index.html</code> after storing the intended path in <code>sessionStorage</code>. This helps handle deep linking when the main HTML shell hasn't loaded yet.</p>"},{"location":"tbjs/#3-core-modules","title":"3. Core Modules","text":""},{"location":"tbjs/#configuration-tbconfig","title":"Configuration (<code>TB.config</code>)","text":"<p>The configuration module (<code>TB.config</code>) manages all framework and application settings. It's initialized by <code>TB.init()</code> with default values merged with your provided configuration.</p> <p>Default Configuration Structure: <pre><code>{\n    appRootId: 'app-root',\n    baseApiUrl: '/api',\n    baseFileUrl: window.location.origin, // Base URL for HTML views\n    initialState: {},\n    themeSettings: {\n        defaultPreference: 'system', // 'light', 'dark', 'system'\n        background: {\n            type: 'color', // '3d', 'image', 'color', 'none'\n            light: { color: '#FFFFFF', image: '' },\n            dark: { color: '#121212', image: '' },\n            placeholder: { image: '', displayUntil3DReady: true }\n        }\n    },\n    routes: [], // Primarily for reference, router fetches HTML directly\n    logLevel: 'info',\n    isProduction: /* inferred or user-set */,\n    serviceWorker: {\n        enabled: false,\n        url: '/sw.js',\n        scope: '/'\n    }\n}\n</code></pre></p> <p>Methods: *   <code>TB.config.init(initialUserConfig)</code>: Called internally by <code>TB.init()</code>. Merges user config with defaults.     *   <code>baseApiUrl</code> is made absolute. If relative (e.g., <code>/api</code>), it's prepended with <code>window.location.origin</code>. If just <code>api</code>, it becomes <code>/api</code>.     *   <code>baseFileUrl</code> ensures it ends with a <code>/</code> if it contains a path.     *   <code>isProduction</code> is inferred based on hostname (<code>localhost</code>, <code>127.0.0.1</code>) if not explicitly set. *   <code>TB.config.get(key)</code>: Retrieves a configuration value. Supports dot notation for nested properties (e.g., <code>TB.config.get('themeSettings.background.type')</code>). *   <code>TB.config.getAll()</code>: Returns a copy of the entire configuration object. *   <code>TB.config.set(key, value)</code>: Sets a configuration value. Supports dot notation. Note: Use with caution after initialization, as not all modules may react to live changes.</p> <p>Example: <pre><code>const apiUrl = TB.config.get('baseApiUrl');\nconsole.log('API URL:', apiUrl);\n\nconst isProd = TB.config.get('isProduction');\nif (isProd) {\n    TB.logger.setLevel('warn');\n}\n</code></pre></p>"},{"location":"tbjs/#state-management-tbstate","title":"State Management (<code>TB.state</code>)","text":"<p><code>TB.state</code> provides a simple, centralized store for your application's state.</p> <p>Methods: *   <code>TB.state.init(initialState)</code>: Called internally by <code>TB.init()</code>. Loads any persisted state from <code>localStorage</code>. *   <code>TB.state.get(key)</code>: Retrieves a state value. Supports dot notation. If <code>key</code> is undefined, returns a copy of the entire state. *   <code>TB.state.set(key, value, options = { persist: false })</code>: Sets a state value.     *   <code>options.persist</code>: If <code>true</code>, the top-level key of this state will be saved to <code>localStorage</code> and reloaded on next init.     *   Emits <code>state:changed</code> event with <code>{ key, value, fullState }</code>.     *   Emits <code>state:changed:your:key</code> (dots replaced with colons) for more specific listeners. *   <code>TB.state.delete(key, options = { persist: false })</code>: Deletes a key from the state. Handles persisted state removal. *   Legacy methods for simple key-value persistence (discouraged for new code, prefer structured state with <code>persist</code> option):     *   <code>TB.state.initVar(v_name, v_value)</code>: Initializes if not set, persists.     *   <code>TB.state.delVar(v_name)</code>: Deletes, persists change.     *   <code>TB.state.getVar(v_name)</code>: Gets value.     *   <code>TB.state.setVar(v_name, v_value)</code>: Sets value, persists.</p> <p>Example: <pre><code>// Set initial user data (e.g., after login)\nTB.state.set('user.profile', { name: 'Alice', theme: 'dark' }, { persist: true });\n\n// Get user name\nconst userName = TB.state.get('user.profile.name'); // Alice\n\n// Listen for changes to a specific part of the state\nTB.events.on('state:changed:user:profile:theme', (newTheme) =&gt; {\n    console.log('User theme changed to:', newTheme);\n    // Update UI or apply theme\n});\n\n// Update the theme\nTB.state.set('user.profile.theme', 'light'); // Listener above will be triggered\n</code></pre></p>"},{"location":"tbjs/#routing-tbrouter","title":"Routing (<code>TB.router</code>)","text":"<p><code>TB.router</code> handles client-side navigation for your SPA. It fetches HTML content for views and updates the DOM.</p> <p>Key Features: *   Loads HTML content from <code>TB.config.get('baseFileUrl') + path</code>. *   Updates browser history (<code>pushState</code>, <code>replaceState</code>). *   Handles <code>popstate</code> events (browser back/forward). *   Intercepts clicks on local links. *   Manages script execution for dynamically loaded content:     *   Scripts with <code>src</code> are loaded and executed once (cached by URL).     *   Inline scripts are executed.     *   Inline scripts with <code>unsave=\"true\"</code> are executed via a Blob URL (fresh execution each time).     *   Important: It attempts to prevent re-execution of main application bundle scripts (e.g., <code>main.js</code>, <code>bundle.js</code>) if they are found within fetched HTML. *   Optional session-based caching for HTML content. *   Emits router events (<code>router:beforeNavigation</code>, <code>router:navigationSuccess</code>, <code>router:navigationError</code>, <code>router:contentProcessed</code>).</p> <p>Methods: *   <code>TB.router.init(rootElement, predefinedRoutes)</code>: Initializes the router.     *   <code>rootElement</code>: The DOM element where views will be rendered.     *   <code>predefinedRoutes</code>: (Currently for reference) Array of route objects.     *   Automatically navigates to the initial URL. *   <code>TB.router.navigateTo(path, replace = false, isInitialLoad = false)</code>: Navigates to the given path.     *   <code>path</code>: The URL path (e.g., <code>/about</code>, <code>/users/123?param=value#section</code>). Can be absolute or relative.     *   <code>replace</code>: If <code>true</code>, uses <code>history.replaceState</code> instead of <code>history.pushState</code>.     *   <code>isInitialLoad</code>: Internal flag for the first navigation.     *   Handles 404 errors by trying to navigate to <code>/web/assets/404.html</code>.     *   Handles 401 errors by trying to navigate to <code>/web/assets/401.html</code>. *   <code>TB.router.getCurrentPath()</code>: Returns the current normalized path. *   <code>TB.router.clearCache(path)</code>: Clears the sessionStorage cache for a specific path or all cached pages if <code>path</code> is omitted. (Only if <code>USE_SESSION_CACHE</code> is true in <code>router.js</code>).</p> <p>Example: Navigating and Handling Events <pre><code>&lt;!-- In your main HTML --&gt;\n&lt;nav&gt;\n    &lt;a href=\"/home\"&gt;Home&lt;/a&gt;\n    &lt;a href=\"/products\"&gt;Products&lt;/a&gt;\n    &lt;a href=\"/web/assets/contact.html\"&gt;Contact (relative to baseFileUrl)&lt;/a&gt;\n&lt;/nav&gt;\n&lt;div id=\"app-root\"&gt;&lt;/div&gt;\n</code></pre></p> <pre><code>// Initialize router (typically done in TB.init)\n// TB.router.init(document.getElementById('app-root'));\n\n// Programmatic navigation\nTB.router.navigateTo('/home');\n\nTB.events.on('router:navigationSuccess', ({ path, contentSource }) =&gt; {\n    console.log(`Navigated to ${path} (from ${contentSource})`);\n    // TB.ui.processDynamicContent is called internally by the router for the appRootElement\n    // but if you load content into other areas, you might call it manually:\n    // TB.ui.processDynamicContent(document.getElementById('some-other-area'));\n});\n\nTB.events.on('router:navigationError', ({ path, error }) =&gt; {\n    console.error(`Failed to navigate to ${path}:`, error);\n    TB.ui.Toast.showError(`Could not load page: ${path}`);\n});\n</code></pre>"},{"location":"tbjs/#api-communication-tbapi","title":"API Communication (<code>TB.api</code>)","text":"<p><code>TB.api</code> is responsible for all backend communication, supporting standard HTTP requests and Tauri <code>invoke</code> calls.</p> <p>Key Structures: *   <code>Result</code> Object: Standardized wrapper for API responses.     *   <code>origin</code>: Source of the data (e.g., <code>['http']</code>, <code>['tauri']</code>).     *   <code>error</code>: Error type (e.g., <code>TB.api.ToolBoxError.none</code>, <code>TB.api.ToolBoxError.input_error</code>).     *   <code>result</code>: A <code>ToolBoxResult</code> object.         *   <code>data_to</code>: Interface target (e.g., <code>TB.api.ToolBoxInterfaces.api</code>, <code>.cli</code>).         *   <code>data_info</code>: Additional info about the data.         *   <code>data</code>: The actual payload.     *   <code>info</code>: A <code>ToolBoxInfo</code> object.         *   <code>exec_code</code>: Execution code (e.g., HTTP status or custom code).         *   <code>help_text</code>: Descriptive message.     *   Methods: <code>.log()</code>, <code>.html()</code>, <code>.get()</code> (returns <code>result.data</code>). *   <code>ToolBoxError</code>: Enum for error types. *   <code>ToolBoxInterfaces</code>: Enum for data destinations.</p> <p>Main Method: <code>TB.api.request()</code> <pre><code>async TB.api.request(\n    moduleName,      // string: Backend module/class OR full path (e.g., '/special/endpoint')\n    functionName,    // string: Backend function/method (ignored if moduleName is full path)\n                     // OR object: Query parameters for GET/DELETE if moduleName is full path\n    payload = null,  // object|string: Data to send. If string, used as query params for GET/POST-URL.\n    method = 'POST', // string: HTTP method ('GET', 'POST', 'PUT', 'DELETE', etc.)\n    useTauri = 'auto', // string: 'auto', 'force' (Tauri only), or 'never' (HTTP only)\n    isSpecialAuthRoute = false // boolean: For routes with custom auth header handling (rarely needed)\n)\n</code></pre> *   Tauri Integration: If <code>env.isTauri()</code> is true and <code>useTauri</code> is 'auto' or 'force', it attempts <code>window.__TAURI__.invoke(\"moduleName.functionName\", payload)</code>. Falls back to HTTP if 'auto' and invoke fails. *   HTTP Requests:     *   URL construction:         *   Standard: <code>baseApiUrl/moduleName/functionName</code>         *   Full path: <code>baseApiUrl/moduleName</code> (where <code>moduleName</code> is e.g., <code>/login</code>)     *   Headers:         *   <code>Content-Type: application/json</code> and <code>Accept: application/json</code> by default for JSON.         *   <code>Authorization: Bearer &lt;token&gt;</code> is added if <code>TB.state.get('user.token')</code> exists (unless <code>isSpecialAuthRoute</code> has special handling, though current <code>_getRequestHeaders</code> adds it generally).     *   Payload:         *   GET/DELETE: Payload (object or string) becomes URL query parameters.         *   POST/PUT/PATCH: Object payload is JSON.stringified. String payload for POST can be query params if URL doesn't already have them.     *   Response Handling:         *   Automatically parses JSON responses.         *   Handles non-JSON responses (e.g., 204 No Content) gracefully.         *   Wraps responses (or errors) in a <code>Result</code> object.</p> <p>Helper Methods: *   <code>TB.api.fetchHtml(path)</code>: Fetches HTML content, typically used by the router. Path is relative to <code>baseFileUrl</code>. *   <code>TB.api.httpPostData(module_name, function_name, data)</code>: Alias for <code>request(..., 'POST')</code>. *   <code>TB.api.AuthHttpPostData(username)</code>: Specific method for session validation. Calls <code>/validateSession</code> with <code>jwt_claim_device</code> and <code>Username</code>. *   <code>TB.api.logoutServer()</code>: Calls <code>/web/logoutS</code> to notify the server of logout.</p> <p>Example: Making a GET Request <pre><code>async function fetchProducts() {\n    TB.ui.Loader.show('Fetching products...');\n    try {\n        // GET /api/ProductManager/getProductsList?category=electronics&amp;limit=10\n        const result = await TB.api.request(\n            'ProductManager',\n            'getProductsList',\n            { category: 'electronics', limit: 10 },\n            'GET'\n        );\n\n        if (result.error === TB.api.ToolBoxError.none) {\n            const products = result.get(); // products === result.result.data\n            console.log('Products:', products);\n            TB.state.set('products.list', products);\n        } else {\n            TB.logger.error('Failed to fetch products:', result.info.help_text);\n            TB.ui.Toast.showError(result.info.help_text || 'Could not load products.');\n        }\n    } catch (error) { // Network errors or other exceptions\n        TB.logger.error('Network error fetching products:', error);\n        TB.ui.Toast.showError('Network error. Please try again.');\n    } finally {\n        TB.ui.Loader.hide();\n    }\n}\n\nfetchProducts();\n</code></pre></p> <p>Example: Tauri Invoke or HTTP POST <pre><code>async function saveData(dataToSave) {\n    const result = await TB.api.request(\n        'DataManager',  // Tauri: DataManager.save_data\n        'save_data',    // HTTP: /api/DataManager/save_data\n        dataToSave,     // Payload\n        'POST',         // HTTP Method\n        'auto'          // Try Tauri first, then HTTP\n    );\n\n    result.log(); // Logs the structured result to console\n\n    if (result.error === TB.api.ToolBoxError.none) {\n        TB.ui.Toast.showSuccess('Data saved successfully!');\n        return result.get(); // Return the data from server response\n    } else {\n        TB.ui.Toast.showError(`Save failed: ${result.info.help_text}`);\n        return null;\n    }\n}\n</code></pre></p>"},{"location":"tbjs/#event-system-tbevents","title":"Event System (<code>TB.events</code>)","text":"<p>A simple publish/subscribe system for decoupled communication.</p> <p>Methods: *   <code>TB.events.on(eventName, callback)</code>: Subscribes to an event. *   <code>TB.events.off(eventName, callback)</code>: Unsubscribes from an event. *   <code>TB.events.emit(eventName, data)</code>: Publishes an event with optional data. *   <code>TB.events.once(eventName, callback)</code>: Subscribes to an event for a single occurrence.</p> <p>Example: <pre><code>// Module A\nfunction doSomething() {\n    // ...\n    TB.events.emit('user:actionCompleted', { action: 'saveSettings', status: 'success' });\n}\n\n// Module B\nTB.events.on('user:actionCompleted', (eventData) =&gt; {\n    if (eventData.action === 'saveSettings' &amp;&amp; eventData.status === 'success') {\n        console.log('User settings saved!');\n    }\n});\n</code></pre> Framework Core Events (Examples): *   <code>state:changed</code> *   <code>state:changed:path:to:key</code> *   <code>router:beforeNavigation</code>, <code>router:navigationSuccess</code>, <code>router:navigationError</code>, <code>router:contentProcessed</code> *   <code>theme:changed</code> *   <code>tbjs:initialized</code> *   <code>api:networkError</code> *   <code>graphics:initialized</code>, <code>graphics:disposed</code> *   <code>cookieConsent:updated</code> *   <code>user:stateChanged</code>, <code>user:loggedOut</code></p>"},{"location":"tbjs/#logging-tblogger","title":"Logging (<code>TB.logger</code>)","text":"<p>Provides prefixed and timestamped console logging with different levels.</p> <p>Methods: *   <code>TB.logger.init({ logLevel })</code>: Called internally. *   <code>TB.logger.setLevel(levelName)</code>: Sets the minimum log level ('debug', 'info', 'warn', 'error', 'none'). *   <code>TB.logger.debug(...args)</code> *   <code>TB.logger.log(...args)</code> (alias for <code>info</code>) *   <code>TB.logger.info(...args)</code> *   <code>TB.logger.warn(...args)</code> *   <code>TB.logger.error(...args)</code></p> <p>Example: <pre><code>TB.logger.debug('This is a debug message with an object:', { id: 1, name: 'Test' });\nTB.logger.info('Application started.');\nTB.logger.warn('Something might be wrong here.');\nTB.logger.error('A critical error occurred!', new Error('Oops'));\n</code></pre> Log level is configured via <code>TB.config.logLevel</code> during <code>TB.init</code>.</p>"},{"location":"tbjs/#environment-detection-tbenv","title":"Environment Detection (<code>TB.env</code>)","text":"<p>Detects the current runtime environment.</p> <p>Methods: *   <code>TB.env.detect()</code>: Called internally by <code>TB.init()</code>. *   <code>TB.env.isTauri()</code>: Returns <code>true</code> if running in a Tauri environment. *   <code>TB.env.isWeb()</code>: Returns <code>true</code> if running in a standard web browser environment. *   <code>TB.env.isMobile()</code>: (Placeholder) Intended for Tauri mobile detection, currently may not be fully implemented.</p> <p>Example: <pre><code>if (TB.env.isTauri()) {\n    TB.logger.log('Running in Tauri, specific Tauri features can be used.');\n    // Example: window.__TAURI__.fs.readTextFile(...)\n} else if (TB.env.isWeb()) {\n    TB.logger.log('Running in a web browser.');\n}\n</code></pre></p>"},{"location":"tbjs/#cryptography-tbcrypto","title":"Cryptography (<code>TB.crypto</code>)","text":"<p>Provides various cryptographic utilities, including WebAuthn support.</p> <p>Key Management: *   <code>TB.crypto.generateAsymmetricKeys()</code>: Generates RSA-OAEP key pair (PEM and base64). *   <code>TB.crypto.decryptAsymmetric(encryptedTextBase64, privateKeyBase64, convertHex = false)</code>: Decrypts RSA-OAEP encrypted text. *   <code>TB.crypto.signMessage(privateKeyBase64, message)</code>: Signs a message using RSA-PSS with the private key. *   <code>TB.crypto.storePrivateKey(privateKeyBase64, username)</code>: Stores private key in <code>localStorage</code>. *   <code>TB.crypto.retrievePrivateKey(username)</code>: Retrieves private key from <code>localStorage</code>.</p> <p>Symmetric Encryption (Example - requires careful IV handling): *   <code>TB.crypto.generateSymmetricKey()</code>: Generates an AES-GCM key (base64 raw). *   <code>TB.crypto.decryptSymmetric(encryptedDataB64, password)</code>: Decrypts AES-GCM data. Note: This implementation assumes the IV is prepended to the ciphertext (first 12 bytes). The <code>password</code> is used to derive the key via PBKDF2.</p> <p>WebAuthn: *   <code>getRpId()</code> (internal, used by WebAuthn functions): Determines the Relying Party ID based on <code>TB.config.get('baseAppUrl')</code> or <code>window.location.hostname</code>. For localhost, it's \"localhost\". *   <code>TB.crypto.registerWebAuthnCredential(registrationData, sing)</code>:     *   <code>registrationData</code>: <code>{ challenge, userId, username }</code>.         *   <code>challenge</code>: Server-provided challenge (string, base64url encoded then decoded to ArrayBuffer).         *   <code>userId</code>: Server-provided user ID (string, base64url encoded then decoded to ArrayBuffer).         *   <code>username</code>: User's display name.     *   <code>sing</code>: Additional data (e.g., session token) that might be included in the payload sent to the server.     *   Calls <code>navigator.credentials.create()</code>.     *   Returns a promise resolving to the payload object to be sent to the server for <code>/register_user_personal_key</code> (or similar endpoint). *   <code>TB.crypto.authorizeWebAuthnCredential(rawIdAsBase64, challenge, username)</code>:     *   <code>rawIdAsBase64</code>: Base64 representation of the credential's rawId (from server).     *   <code>challenge</code>: Server-provided challenge string.     *   <code>username</code>: User's display name.     *   Calls <code>navigator.credentials.get()</code>.     *   Returns a promise resolving to the payload object to be sent to the server for <code>/validate_persona</code> (or similar endpoint).</p> <p>Helper Functions: *   <code>arrayBufferToBase64(buffer)</code> *   <code>base64ToArrayBuffer(base64)</code> *   <code>strToArrayBuffer(str)</code> *   <code>arrayBufferToStr(arrayBuffer)</code> *   And others...</p> <p>Example: Registering a WebAuthn Credential (Passkey) <pre><code>// Assuming 'username' is known and user is authenticated to add a new key\nasync function registerNewPasskey(username) {\n    try {\n        // 1. Client requests challenge from server for this user\n        const challengeRes = await TB.api.request('AuthManager', 'getWebAuthnRegistrationChallenge', { username }, 'POST');\n        if (challengeRes.error !== TB.api.ToolBoxError.none || !challengeRes.get()?.challengeInfo) {\n            throw new Error(challengeRes.info.help_text || \"Failed to get registration challenge.\");\n        }\n        const { challenge, userId } = challengeRes.get().challengeInfo; // Server provides its internal userId for WebAuthn\n\n        // 2. Client uses TB.crypto to create credential\n        const currentSessionToken = TB.user.getToken(); // Example for 'sing' parameter\n        const webAuthnPayload = await TB.crypto.registerWebAuthnCredential(\n            { challenge, userId, username },\n            currentSessionToken\n        );\n\n        // 3. Client sends new credential to server for verification and storage\n        const registrationResult = await TB.api.request('AuthManager', 'completeWebAuthnRegistration', webAuthnPayload, 'POST');\n\n        if (registrationResult.error === TB.api.ToolBoxError.none) {\n            TB.ui.Toast.showSuccess('Passkey registered successfully!');\n        } else {\n            TB.ui.Toast.showError(`Passkey registration failed: ${registrationResult.info.help_text}`);\n        }\n    } catch (error) {\n        TB.logger.error('[WebAuthnDemo] Registration error:', error);\n        TB.ui.Toast.showError(error.message || 'An error occurred during passkey registration.');\n    }\n}\n</code></pre></p>"},{"location":"tbjs/#user-management-tbuser","title":"User Management (<code>TB.user</code>)","text":"<p>Manages user authentication state, sessions, and user-specific data.</p> <p>State: The user's state (isAuthenticated, username, token, etc.) is stored under <code>TB.state.get('user')</code>.</p> <p>Methods: *   <code>TB.user.init(forceServerFetch = false)</code>: Initializes the user module.     *   Loads session from <code>localStorage</code> (<code>tbjs_user_session</code>).     *   If authenticated, validates the session with the server (<code>TB.api.AuthHttpPostData</code>).     *   Synchronizes <code>userData</code> based on timestamps (<code>tbjs_user_data_timestamp</code>) if <code>forceServerFetch</code> is true or server data is newer. *   <code>TB.user.signup(username, email, initiationKey, registerAsPersona = false)</code>: Placeholder for signup flow. *   <code>TB.user.loginWithDeviceKey(username)</code>: Performs login using a locally stored asymmetric key.     1.  Retrieves private key using <code>TB.crypto.retrievePrivateKey()</code>.     2.  Requests challenge from <code>CloudM.AuthManager.get_to_sing_data</code>.     3.  Signs challenge using <code>TB.crypto.signMessage()</code>.     4.  Validates signature with <code>CloudM.AuthManager.validate_device</code>.     5.  If successful, updates user state with token and user data. *   <code>TB.user.loginWithWebAuthn(username)</code>: Performs WebAuthn (passkey) login.     1.  Requests challenge &amp; rawId from <code>CloudM.AuthManager.get_to_sing_data</code> (<code>personal_key: true</code>).     2.  Calls <code>TB.crypto.authorizeWebAuthnCredential()</code>.     3.  Sends assertion to <code>CloudM.AuthManager.validate_persona</code>. *   <code>TB.user.requestMagicLink(username)</code>: Requests a magic link email via <code>CloudM.AuthManager.get_magic_link_email</code>. *   <code>TB.user.registerDeviceWithInvitation(username, invitationKey)</code>: Registers a new device using an invitation key.     1.  Generates new asymmetric keys (<code>TB.crypto.generateAsymmetricKeys()</code>).     2.  Stores private key (<code>TB.crypto.storePrivateKey()</code>).     3.  Sends public key and invitation to <code>CloudM.AuthManager.add_user_device</code>.     4.  Attempts login via <code>loginWithDeviceKey()</code> upon success. *   <code>TB.user.registerWebAuthnForCurrentUser(username)</code>: Registers a WebAuthn credential for an already authenticated user. *   <code>TB.user.logout(notifyServer = true)</code>: Logs out the user, clears local session, and optionally notifies the server via <code>TB.api.logoutServer()</code>. *   <code>TB.user.checkSessionValidity()</code>: Checks if current session token is valid via <code>/IsValidSession</code>. *   Getters:     *   <code>TB.user.isAuthenticated()</code>     *   <code>TB.user.getUsername()</code>     *   <code>TB.user.getUserLevel()</code>     *   <code>TB.user.getToken()</code>     *   <code>TB.user.isDeviceRegisteredWithKey()</code> *   User Data Management:     *   <code>TB.user.getUserData(key)</code>: Gets a specific piece of user data from <code>TB.state.get('user.userData')</code>.     *   <code>TB.user.setUserData(keyOrObject, value, syncToServer = false)</code>: Sets user data locally. If <code>syncToServer</code> is true, calls <code>syncUserData</code>.     *   <code>TB.user.syncUserData(updatedFields = null)</code>: Syncs <code>userData</code> (or specified fields) to the server via <code>UserManager.updateUserData</code>.     *   <code>TB.user.fetchUserData()</code>: Fetches all user data from <code>UserManager.getUserData</code>.</p> <p>Example: Login and Accessing User Info <pre><code>async function handleLogin(username) {\n    const loginResult = await TB.user.loginWithDeviceKey(username);\n    if (loginResult.success) {\n        TB.ui.Toast.showSuccess(`Welcome, ${TB.user.getUsername()}!`);\n        TB.router.navigateTo('/dashboard');\n    } else {\n        TB.ui.Toast.showError(loginResult.message);\n    }\n}\n\n// Check if user is logged in before accessing a protected route\nif (!TB.user.isAuthenticated()) {\n    TB.router.navigateTo('/login');\n} else {\n    console.log('User Level:', TB.user.getUserLevel());\n}\n</code></pre></p>"},{"location":"tbjs/#server-sent-events-tbsse","title":"Server-Sent Events (<code>TB.sse</code>)","text":"<p>Manages Server-Sent Event (SSE) connections.</p> <p>Methods: *   <code>TB.sse.connect(url, options = {})</code>: Establishes an SSE connection.     *   <code>url</code>: The SSE endpoint URL.     *   <code>options</code>:         *   <code>eventSourceOptions</code>: Options passed directly to the <code>EventSource</code> constructor (e.g., <code>{ withCredentials: true }</code>).         *   <code>onOpen</code>: Callback for <code>open</code> event.         *   <code>onError</code>: Callback for <code>error</code> event.         *   <code>onMessage</code>: Callback for generic <code>message</code> events.         *   <code>listeners</code>: An object of <code>{ eventName: handlerFunction }</code> for custom named SSE events.     *   Emits events like <code>sse:open:&lt;url&gt;</code>, <code>sse:error:&lt;url&gt;</code>, <code>sse:message:&lt;url&gt;</code>, <code>sse:event:&lt;url&gt;:&lt;eventName&gt;</code>. *   <code>TB.sse.disconnect(url)</code>: Closes a specific SSE connection. *   <code>TB.sse.disconnectAll()</code>: Closes all active SSE connections. *   <code>TB.sse.getConnection(url)</code>: Returns the <code>EventSource</code> object for a given URL.</p> <p>Example: <pre><code>const sseConnection = TB.sse.connect('/api/notifications', {\n    onOpen: () =&gt; TB.logger.info('SSE connection for notifications opened.'),\n    onError: (err) =&gt; TB.logger.error('SSE notifications error:', err),\n    listeners: {\n        'new_message': (data) =&gt; {\n            TB.ui.Toast.showInfo(`New message: ${data.text}`);\n        },\n        'user_update': (data) =&gt; {\n            TB.state.set('userDetails', data);\n        }\n    }\n});\n\n// To close\n// TB.sse.disconnect('/api/notifications');\n</code></pre></p>"},{"location":"tbjs/#service-worker-tbsw","title":"Service Worker (<code>TB.sw</code>)","text":"<p>Manages the registration and communication with your application's Service Worker.</p> <p>Configuration (<code>TB.config.serviceWorker</code>): *   <code>enabled</code>: (boolean) Master switch for SW registration. *   <code>url</code>: (string) Path to your <code>sw.js</code> file (default: <code>/sw.js</code>). *   <code>scope</code>: (string) Scope for the Service Worker (default: <code>/</code>).</p> <p>Methods: *   <code>TB.sw.register()</code>: Registers the Service Worker based on configuration.     *   Handles <code>updatefound</code> and state changes of the installing worker.     *   Emits <code>sw:updateAvailable</code> or <code>sw:contentCached</code>. *   <code>TB.sw.unregister()</code>: Unregisters all active Service Workers for the current origin. *   <code>TB.sw.sendMessage(message)</code>: Sends a message to the active Service Worker controller and returns a Promise for the response.</p> <p>Example: <pre><code>// In TB.init config:\n// serviceWorker: { enabled: true, url: '/my-app-sw.js' }\n\n// tbjs will attempt to register it automatically.\n\n// Listening for updates\nTB.events.on('sw:updateAvailable', ({ registration }) =&gt; {\n    if (confirm('A new version is available. Reload to update?')) {\n        // Logic to skip waiting and activate new SW\n        registration.waiting.postMessage({ type: 'SKIP_WAITING' });\n        // Usually, you'd listen for 'controllerchange' event then reload\n        navigator.serviceWorker.addEventListener('controllerchange', () =&gt; {\n            window.location.reload();\n        });\n    }\n});\n\n// Sending a message to SW\nasync function clearAppCacheViaSW() {\n    try {\n        const response = await TB.sw.sendMessage({ type: 'CLEAR_CACHE', cacheName: 'app-data-v1' });\n        console.log('SW Cache Clear Response:', response);\n        TB.ui.Toast.showSuccess('App cache cleared by Service Worker.');\n    } catch (error) {\n        TB.ui.Toast.showError(`Error communicating with SW: ${error}`);\n    }\n}\n</code></pre></p>"},{"location":"tbjs/#utilities-tbutils","title":"Utilities (<code>TB.utils</code>)","text":"<p>A collection of general-purpose helper functions.</p> <p>Methods: *   <code>TB.utils.autocomplete(inputElement, array)</code>: Adds basic autocomplete to an input field. *   <code>TB.utils.debounce(func, delay)</code>: Debounces a function. *   <code>TB.utils.throttle(func, limit)</code>: Throttles a function. *   <code>TB.utils.uniqueId(prefix = 'id-')</code>: Generates a simple unique ID. *   <code>TB.utils.deepClone(obj)</code>: Deep clones an object or array. *   <code>TB.utils.cleanUrl(url)</code>: Removes protocol from a URL.</p> <p>Example: <pre><code>const myInput = document.getElementById('search');\nconst suggestions = ['Apple', 'Banana', 'Orange', 'Apricot'];\nTB.utils.autocomplete(myInput, suggestions);\n\nconst debouncedSave = TB.utils.debounce((data) =&gt; {\n    console.log('Saving data:', data);\n    // TB.api.request(...) to save\n}, 500);\n\nmyInput.addEventListener('input', (e) =&gt; debouncedSave(e.target.value));\n</code></pre></p>"},{"location":"tbjs/#4-ui-system-tbui","title":"4. UI System (<code>TB.ui</code>)","text":"<p>The <code>TB.ui</code> namespace contains modules and components for managing the user interface.</p>"},{"location":"tbjs/#theme-management-tbuitheme","title":"Theme Management (<code>TB.ui.theme</code>)","text":"<p>Manages application themes (light/dark mode) and dynamic backgrounds.</p> <p>Configuration (<code>TB.config.themeSettings</code>): *   <code>defaultPreference</code>: 'light', 'dark', or 'system'. *   <code>background</code>: Object defining background types and sources.     *   <code>type</code>: 'color', 'image', '3d', 'none'.     *   <code>light</code>: <code>{ color: '#hex', image: 'path/to/image.jpg' }</code>     *   <code>dark</code>: <code>{ color: '#hex', image: 'path/to/image.jpg' }</code>     *   <code>placeholder</code>: <code>{ image: 'path/to/placeholder.jpg', displayUntil3DReady: true }</code> (used when <code>type</code> is '3d').</p> <p>Initialization: <code>TB.ui.theme.init()</code> is called by <code>TB.init()</code> using <code>TB.config.get('themeSettings')</code>. It: *   Loads user preference from <code>localStorage</code> or defaults. *   Sets up a background container div (<code>#appBackgroundContainer</code>) if not present. *   Applies the initial theme and background. *   Listens for system theme changes and <code>graphics:initialized</code>/<code>graphics:disposed</code> events to update the background.</p> <p>Methods: *   <code>TB.ui.theme.setPreference(preference)</code>: Sets the theme preference ('light', 'dark', 'system') and saves to <code>localStorage</code>. Updates the theme immediately. *   <code>TB.ui.theme.togglePreference()</code>: Toggles between 'light' and 'dark' modes. If current preference is 'system', it effectively picks the opposite of the current effective mode. *   <code>TB.ui.theme.getCurrentMode()</code>: Returns the current active mode ('light' or 'dark'). *   <code>TB.ui.theme.getPreference()</code>: Returns the user's set preference ('light', 'dark', or 'system'). *   <code>TB.ui.theme.getBackgroundConfig()</code>: Returns the current background configuration.</p> <p>Events: *   <code>theme:changed</code>: Emitted with <code>{ mode: 'light'|'dark' }</code> when the effective theme changes. *   The <code>TB.graphics.updateTheme(mode)</code> method is called internally when the theme changes and graphics are active.</p> <p>Example: <pre><code>// In your app initialization (TB.init config)\n// themeSettings: {\n//   defaultPreference: 'system',\n//   background: {\n//     type: 'image',\n//     light: { image: '/images/bg-light.jpg', color: '#E0E0E0' },\n//     dark: { image: '/images/bg-dark.jpg', color: '#303030' }\n//   }\n// }\n\n// Toggle theme with a button\nconst themeToggleButton = document.getElementById('theme-toggle-btn');\nthemeToggleButton.addEventListener('click', () =&gt; {\n    TB.ui.theme.togglePreference();\n});\n\n// Listen for theme changes to update other UI elements\nTB.events.on('theme:changed', (eventData) =&gt; {\n    console.log('Theme is now:', eventData.mode);\n    // Update icons, specific component styles, etc.\n});\n</code></pre> The background container <code>#appBackgroundContainer</code> is styled to be fixed and behind all other content (<code>z-index: -1</code>). The 3D graphics, if used as a background, are expected to render into <code>#threeDScene</code>.</p>"},{"location":"tbjs/#graphics-tbgraphics","title":"Graphics (<code>TB.graphics</code>)","text":"<p>Manages 3D rendering using Three.js, often used for dynamic backgrounds.</p> <p>Initialization: <code>TB.graphics.init(canvasContainerSelector, options = {})</code> *   <code>canvasContainerSelector</code>: A CSS selector for the DOM element where the Three.js canvas will be appended (e.g., <code>#threeDScene</code>). *   <code>options</code>:     *   <code>cameraY</code>, <code>cameraZ</code>: Initial camera position.     *   <code>sierpinskiDepth</code>: Depth for the default Sierpinski tetrahedron animation.     *   <code>loaderHideDelay</code>: Delay in ms before hiding a <code>.loaderCenter</code> element after graphics init.</p> <p>Key Features &amp; Methods: *   Creates a WebGLRenderer, Scene, and PerspectiveCamera. *   Builds a default Sierpinski tetrahedron fractal. *   Adds ambient and point lights. *   <code>TB.graphics.updateTheme(themeMode)</code>: Adjusts light colors based on 'light' or 'dark' mode. Called automatically by <code>TB.ui.theme</code>. *   <code>TB.graphics.setSierpinskiDepth(newDepth)</code>: Rebuilds the fractal with new depth. *   <code>TB.graphics.setAnimationSpeed(x, y, z, factor)</code>: Controls the rotation speed of the main 3D object. *   <code>TB.graphics.adjustCameraZoom(delta)</code> / <code>TB.graphics.setCameraZoom(value)</code>: Controls camera Z position. *   Interactive rotation via mouse/touch drag. *   Animation Sequences:     *   <code>TB.graphics.playAnimationSequence(sequenceString, onComplete, baseSpeed, speedFactor)</code>: Plays a predefined animation sequence.         *   <code>sequenceString</code>: Colon-separated steps, e.g., <code>\"R1+32:P2-51:Y1+15\"</code>.             *   Format: <code>Type(1)Repeat(N)Direction(1)Speed(1)Complexity(1)</code>             *   Type: <code>R</code> (Roll/X), <code>P</code> (Pan/Z), <code>Y</code> (Yaw/Y), <code>Z</code> (Zoom - placeholder).             *   Repeat: Number of repetitions.             *   Direction: <code>+</code> or <code>-</code>.             *   Speed: 1-9.             *   Complexity: 1-9 (influences duration).     *   <code>TB.graphics.stopAnimationSequence()</code>: Stops the current sequence. *   <code>TB.graphics.pause()</code> / <code>TB.graphics.resume()</code>: Pause/resume rendering loop. *   <code>TB.graphics.dispose()</code>: Cleans up Three.js resources and removes event listeners.</p> <p>Events: *   <code>graphics:initialized</code>: Emitted when graphics setup is complete. <code>TB.ui.theme</code> listens to this. *   <code>graphics:disposed</code>: Emitted on cleanup.</p> <p>Example: Initializing 3D Background <pre><code>&lt;!-- In your HTML --&gt;\n&lt;div id=\"myThreeCanvasContainer\" style=\"position: fixed; top: 0; left: 0; width: 100%; height: 100%; z-index: -10;\"&gt;&lt;/div&gt;\n</code></pre> <pre><code>// In your TB.init config:\n// themeSettings: {\n//   background: {\n//     type: '3d'\n//   }\n// }\n\n// If TB.ui.theme is configured with type '3d', it will attempt to initialize graphics automatically.\n// However, you might need to ensure the container exists and is ready.\n// Or, initialize manually if you don't use themeSettings.background for it:\ndocument.addEventListener('DOMContentLoaded', () =&gt; {\n    if (TB.config.get('themeSettings.background.type') === '3d') {\n        const graphicsContext = TB.graphics.init('#myThreeCanvasContainer', {\n            sierpinskiDepth: 4,\n            cameraZ: 8\n        });\n        if (graphicsContext) {\n            console.log('3D Graphics initialized for background.');\n        }\n    }\n});\n\n// Example: Play an animation sequence\n// TB.graphics.playAnimationSequence(\"R5+55:P2-22\", () =&gt; console.log(\"Sequence done!\"));\n</code></pre></p>"},{"location":"tbjs/#htmx-integration-tbuihtmxintegration","title":"HTMX Integration (<code>TB.ui.htmxIntegration</code>)","text":"<p>Manages interactions with HTMX.</p> <p>Initialization: <code>TB.ui.htmxIntegration.init()</code> is called by <code>TB.init()</code>. It sets up global event listeners for HTMX events.</p> <p>Event Handling: *   <code>htmx:afterSwap</code>:     *   Triggered after HTMX swaps content into the DOM.     *   Calls <code>TB.ui.processDynamicContent(event.detail.target)</code> to initialize <code>tbjs</code> components, run scripts, and apply Markdown rendering on the newly added content. *   <code>htmx:afterRequest</code>:     *   Triggered after an HTMX AJAX request completes.     *   If the response is JSON:         *   It attempts to wrap the JSON data in a <code>TB.api.Result</code> object.         *   Logs errors and shows a <code>TB.ui.Toast</code> if the <code>Result</code> indicates an error.         *   If the <code>Result.result.data_to</code> is <code>TB.api.ToolBoxInterfaces.remote</code> and contains a <code>render</code> command, it emits a <code>ws:renderCommand</code> event (for potential WebSocket/SSE driven rendering updates).         *   Emits an <code>htmx:jsonResponse</code> event with the processed data.     *   If the response is HTML, HTMX handles the swap, and <code>htmx:afterSwap</code> will subsequently fire.</p> <p>Usage: This module works mostly in the background. Ensure your HTMX-powered components return appropriate HTML fragments or structured JSON that <code>tbjs</code> can understand.</p> <p>Example: HTMX with JSON Response Backend (Python/Flask example) returning JSON that might trigger a toast or update state: <pre><code>@app.route('/htmx/submit-form', methods=['POST'])\ndef submit_form_htmx():\n    # ... process form ...\n    if success:\n        return jsonify({\n            \"error\": \"none\",\n            \"result\": {\n                \"data_to\": \"API\", # Or \"CLIENT\"\n                \"data_info\": \"Form submitted successfully.\",\n                \"data\": {\"new_id\": 123, \"message\": \"Item created.\"}\n            },\n            \"info\": {\"exec_code\": 0, \"help_text\": \"Success\"}\n        })\n    else:\n        return jsonify({\n            \"error\": \"InputError\",\n            \"result\": {},\n            \"info\": {\"exec_code\": 1, \"help_text\": \"Invalid data provided.\"}\n        }), 400\n</code></pre> JavaScript to listen for custom JSON processing: <pre><code>TB.events.on('htmx:jsonResponse', ({ detail, data: tbResult }) =&gt; {\n    console.log('HTMX JSON Response:', tbResult.get());\n    if (tbResult.get() &amp;&amp; tbResult.get().message) {\n        TB.ui.Toast.showInfo(tbResult.get().message);\n    }\n});\n</code></pre></p>"},{"location":"tbjs/#dynamic-content-processing","title":"Dynamic Content Processing","text":"<p><code>TB.ui.processDynamicContent(parentElement, options = {})</code></p> <p>This function is crucial when new HTML is added to the DOM, for example, by <code>TB.router</code> or HTMX after an <code>hx-swap</code>.</p> <ul> <li><code>parentElement</code>: The DOM element (or a wrapper around) the newly added content.</li> <li><code>options</code>:<ul> <li><code>addScripts</code> (boolean, default <code>true</code>): Whether to process <code>&lt;script&gt;</code> tags.</li> <li><code>scriptCache</code> (Set, default <code>new Set()</code>): A Set to keep track of loaded script URLs to prevent re-execution. <code>TB.router</code> passes its cache.</li> </ul> </li> </ul> <p>Actions Performed: 1.  Calls <code>window.htmx.process(parentElement)</code> to initialize HTMX attributes on the new content. 2.  Handles <code>&lt;script&gt;</code> tags (see <code>TB.router</code> script handling for details). 3.  Initializes <code>tbjs</code> UI components found within <code>parentElement</code> (e.g., by looking for specific data attributes or classes). 4.  Calls <code>TB.ui.MarkdownRenderer.renderAllIn(parentElement)</code> if applicable.</p> <p>Usage: Generally, you don't need to call this manually if content is loaded via <code>TB.router</code> or standard HTMX swaps, as <code>TB.router</code> and <code>TB.ui.htmxIntegration</code> handle it. However, if you manually inject HTML that needs <code>tbjs</code> processing: <pre><code>const newContentContainer = document.getElementById('dynamic-area');\nnewContentContainer.innerHTML = '... new HTML with tbjs components, markdown, or scripts ...';\nTB.ui.processDynamicContent(newContentContainer);\n</code></pre></p>"},{"location":"tbjs/#components","title":"Components","text":"<p><code>tbjs</code> provides a set of pre-built UI components. They generally use Tailwind CSS classes for styling (often prefixed with <code>tb-</code> as per <code>tailwind.config.js</code>) and can be initialized programmatically or sometimes via data attributes.</p>"},{"location":"tbjs/#modal-tbuimodal","title":"Modal (<code>TB.ui.Modal</code>)","text":"<p>Displays content in a modal dialog.</p> <p>Options: *   <code>content</code>: HTML string or HTMLElement for the modal body. *   <code>title</code>: (string) Optional modal title. *   <code>closeOnOutsideClick</code>: (boolean, default <code>true</code>) *   <code>closeOnEsc</code>: (boolean, default <code>true</code>) *   <code>buttons</code>: Array of button config objects: <code>{ text, action, variant, className, size }</code>. *   <code>onOpen</code>, <code>onClose</code>, <code>beforeClose</code>: Callbacks. <code>beforeClose</code> can return <code>false</code> to prevent closing. *   <code>maxWidth</code>: (string, Tailwind class, default <code>'max-w-lg'</code>) *   <code>modalId</code>: Custom ID for the modal element. *   <code>customClasses</code>: Object to override default Tailwind classes for <code>overlay</code>, <code>modalContainer</code>, etc.</p> <p>Methods: *   <code>modal.show()</code>: Displays the modal. *   <code>modal.close()</code>: Closes the modal.</p> <p>Static Method: *   <code>TB.ui.Modal.show(options)</code>: Creates and shows a modal.</p> <p>Example: <pre><code>TB.ui.Modal.show({\n    title: 'Confirm Action',\n    content: '&lt;p&gt;Are you sure you want to delete this item?&lt;/p&gt;',\n    maxWidth: 'max-w-md', // Tailwind class\n    buttons: [\n        {\n            text: 'Cancel',\n            variant: 'secondary', // Uses TB.ui.Button styling\n            action: (modal) =&gt; modal.close()\n        },\n        {\n            text: 'Delete',\n            variant: 'danger',\n            action: (modal) =&gt; {\n                console.log('Item deleted!');\n                modal.close();\n                TB.ui.Toast.showSuccess('Item deleted successfully.');\n            }\n        }\n    ],\n    onOpen: () =&gt; console.log('Confirmation modal opened.'),\n    onClose: () =&gt; console.log('Confirmation modal closed.')\n});\n</code></pre></p>"},{"location":"tbjs/#toast-tbuitoast","title":"Toast (<code>TB.ui.Toast</code>)","text":"<p>Displays small, non-blocking notification messages.</p> <p>Options: *   <code>message</code>: (string) The message content. *   <code>type</code>: <code>'info'</code>, <code>'success'</code>, <code>'warning'</code>, <code>'error'</code> (default <code>'info'</code>). *   <code>duration</code>: (number) Milliseconds to display (0 for sticky, default <code>5000</code>). *   <code>position</code>: <code>'top-right'</code>, <code>'top-center'</code>, etc. (default <code>'top-right'</code>). *   <code>title</code>: (string) Optional title. *   <code>actions</code>: Array of action objects: <code>[{ text: 'Undo', action: () =&gt; { /* ... */ } }]</code>. *   <code>icon</code>: (boolean, default <code>true</code>) Show type-specific icon. *   <code>closable</code>: (boolean, default <code>true</code>) Show a close button. *   <code>customClasses</code>: For overriding default styling of toast elements.</p> <p>Static Methods: *   <code>TB.ui.Toast.showInfo(message, options)</code> *   <code>TB.ui.Toast.showSuccess(message, options)</code> *   <code>TB.ui.Toast.showWarning(message, options)</code> *   <code>TB.ui.Toast.showError(message, options)</code> *   <code>TB.ui.Toast.hideAll()</code></p> <p>Example: <pre><code>TB.ui.Toast.showSuccess('Profile updated successfully!', {\n    duration: 3000,\n    position: 'bottom-center'\n});\n\nTB.ui.Toast.showError('Failed to connect to server.', {\n    title: 'Network Error',\n    duration: 0, // Sticky\n    actions: [\n        { text: 'Retry', action: () =&gt; console.log('Retry clicked!') }\n    ]\n});\n</code></pre></p>"},{"location":"tbjs/#loader-tbuiloader","title":"Loader (<code>TB.ui.Loader</code>)","text":"<p>Displays a loading indicator.</p> <p>Options: *   <code>text</code>: (string, default <code>'Loading...'</code>) Text displayed below the spinner. *   <code>fullscreen</code>: (boolean, default <code>true</code>) If true, covers the whole page. *   <code>customSpinnerHtml</code>: (string) Custom HTML for the spinner. *   <code>customClasses</code>: For overriding default styling of overlay, spinner container, text.</p> <p>Static Methods: *   <code>TB.ui.Loader.show(textOrOptions)</code>: Shows a fullscreen loader. Returns the loader DOM element. *   <code>TB.ui.Loader.hide(loaderElement)</code>: Hides a specific loader element or the default fullscreen loader.</p> <p>Example: <pre><code>// Show default fullscreen loader\nconst myLoader = TB.ui.Loader.show('Processing your request...');\n\n// Simulate an async operation\nsetTimeout(() =&gt; {\n    TB.ui.Loader.hide(myLoader); // Hide the specific loader instance\n    // Or TB.ui.Loader.hide(); to hide the default ID loader\n}, 2000);\n</code></pre> Note: The Loader component injects its own minimal CSS for the spinner animation (<code>tbjs_spin</code>) and basic layout if not overridden by Tailwind classes.</p>"},{"location":"tbjs/#button-tbuibutton","title":"Button (<code>TB.ui.Button</code>)","text":"<p>A class to create styled button elements programmatically.</p> <p>Options: *   <code>text</code>: (string, default <code>'Button'</code>) *   <code>action</code>: (function) Click handler <code>(event, buttonInstance) =&gt; {}</code>. *   <code>variant</code>: <code>'primary'</code>, <code>'secondary'</code>, <code>'danger'</code>, <code>'outline'</code>, <code>'ghost'</code>, <code>'link'</code> (default <code>'primary'</code>). *   <code>size</code>: <code>'sm'</code>, <code>'md'</code>, <code>'lg'</code> (default <code>'md'</code>). *   <code>type</code>: <code>'button'</code>, <code>'submit'</code>, <code>'reset'</code> (default <code>'button'</code>). *   <code>disabled</code>: (boolean, default <code>false</code>) *   <code>isLoading</code>: (boolean, default <code>false</code>) *   <code>iconLeft</code>, <code>iconRight</code>: HTML string for icons (e.g., Material Symbol span). *   <code>customClasses</code>: Additional CSS classes. *   <code>attributes</code>: Object of custom attributes.</p> <p>Instance Methods: *   <code>setText(text)</code> *   <code>setLoading(isLoading, updateDom = true)</code> *   <code>setDisabled(isDisabled, updateDom = true)</code> *   <code>element</code>: The DOM element of the button.</p> <p>Static Method: *   <code>TB.ui.Button.create(text, action, options)</code>: Creates a button and returns its DOM element.</p> <p>Example: <pre><code>const submitBtnElement = TB.ui.Button.create('Submit Form', async (event, btnInstance) =&gt; {\n    btnInstance.setLoading(true);\n    try {\n        // await someApiCall();\n        TB.ui.Toast.showSuccess('Form submitted!');\n    } catch (e) {\n        TB.ui.Toast.showError('Submission failed.');\n    } finally {\n        btnInstance.setLoading(false);\n    }\n}, {\n    variant: 'primary',\n    size: 'lg',\n    iconLeft: '&lt;span class=\"material-symbols-outlined\"&gt;save&lt;/span&gt;',\n    attributes: { 'data-form-id': 'user-reg-form' }\n});\n\ndocument.getElementById('form-actions').appendChild(submitBtnElement);\n</code></pre></p>"},{"location":"tbjs/#darkmodetoggle-tbuidarkmodetoggle","title":"DarkModeToggle (<code>TB.ui.DarkModeToggle</code>)","text":"<p>Manages a dark mode toggle button/UI element, syncing with <code>TB.ui.theme</code>.</p> <p>Options (passed to constructor or <code>TB.ui.DarkModeToggle.init()</code>): *   <code>containerSelector</code>: (string, default <code>'#darkModeToggleContainer'</code>) The main clickable element or wrapper. *   <code>iconSelector</code>: (string, default <code>.tb-toggle-icon'</code>) Selector for the icon element (e.g., a <code>&lt;span&gt;</code> for Material Symbols). *   <code>checkboxSelector</code>: (string, default <code>'#darkModeSwitch'</code>) Selector for an optional underlying <code>&lt;input type=\"checkbox\"&gt;</code>. *   <code>lightModeIconClass</code>, <code>darkModeIconClass</code>: Text content for the icon (e.g., Material Symbol names like <code>'light_mode'</code>, <code>'dark_mode'</code>). *   <code>rotationActiveDeg</code>, <code>rotationInactiveDeg</code>, <code>rotationTransition</code>: For icon rotation animation.</p> <p>Initialization: *   <code>new TB.ui.DarkModeToggle(options)</code> *   <code>TB.ui.DarkModeToggle.init(optionsOrSelector)</code>: Static convenience method.</p> <p>HTML Structure Examples:</p> <ol> <li> <p>Icon-only Toggle (recommended): <pre><code>&lt;button id=\"darkModeToggleContainer\" aria-label=\"Toggle dark mode\" class=\"tb-p-2 tb-rounded-full hover:tb-bg-gray-200 dark:hover:tb-bg-gray-700\"&gt;\n    &lt;span class=\"material-symbols-outlined tb-toggle-icon\"&gt;&lt;/span&gt;\n&lt;/button&gt;\n</code></pre>     Initialize with: <code>TB.ui.DarkModeToggle.init();</code> (uses default selectors) or <code>TB.ui.DarkModeToggle.init('#darkModeToggleContainer');</code></p> </li> <li> <p>Checkbox-driven Toggle: <pre><code>&lt;label for=\"darkModeSwitch\" id=\"darkModeToggleContainer\" class=\"tb-inline-flex tb-items-center tb-cursor-pointer\"&gt;\n    &lt;input type=\"checkbox\" id=\"darkModeSwitch\" class=\"tb-sr-only tb-peer\"&gt;\n    &lt;div class=\"tb-relative tb-w-11 tb-h-6 tb-bg-gray-200 peer-focus:tb-outline-none peer-focus:tb-ring-4 peer-focus:tb-ring-blue-300 dark:peer-focus:tb-ring-blue-800 tb-rounded-full peer dark:tb-bg-gray-700 peer-checked:tb-after:tb-translate-x-full rtl:peer-checked:tb-after:-tb-translate-x-full peer-checked:tb-after:tb-border-white tb-after:tb-content-[''] tb-after:tb-absolute tb-after:tb-top-[2px] tb-after:tb-start-[2px] tb-after:tb-bg-white tb-after:tb-border-gray-300 tb-after:tb-border tb-after:tb-rounded-full tb-after:tb-h-5 tb-after:tb-w-5 tb-after:tb-transition-all dark:tb-border-gray-600 peer-checked:tb-bg-blue-600\"&gt;&lt;/div&gt;\n    &lt;span class=\"material-symbols-outlined tb-toggle-icon tb-ml-3 tb-text-gray-900 dark:tb-text-gray-300\"&gt;&lt;/span&gt;\n&lt;/label&gt;\n</code></pre>     Initialize with: <code>TB.ui.DarkModeToggle.init({ containerSelector: '#darkModeToggleContainer', iconSelector: '.tb-toggle-icon', checkboxSelector: '#darkModeSwitch' });</code></p> </li> </ol> <p>Functionality: *   Updates its visual state (icon, checkbox checked status) based on <code>TB.ui.theme.getCurrentMode()</code>. *   Listens for clicks (on container or changes on checkbox) to call <code>TB.ui.theme.setPreference()</code> or <code>TB.ui.theme.togglePreference()</code>. *   Reacts to <code>theme:changed</code> events to keep its visual state synchronized.</p>"},{"location":"tbjs/#cookiebanner-tbuicookiebanner","title":"CookieBanner (<code>TB.ui.CookieBanner</code>)","text":"<p>Displays a cookie consent banner and optional settings modal.</p> <p>Options: *   <code>title</code>, <code>message</code>, <code>termsLink</code>, <code>termsLinkText</code>, <code>acceptMinimalText</code>, <code>showAdvancedOptions</code>, <code>advancedOptionsText</code>: Text and behavior customization. *   <code>onConsent</code>: Callback <code>(consentSettings) =&gt; {}</code> triggered when consent is given/updated. *   <code>customClasses</code>: For styling banner, modal, etc.</p> <p>Static Method: *   <code>TB.ui.CookieBanner.show(options)</code>: Creates and displays the banner if no consent is found in <code>localStorage</code> (<code>tbjs_cookie_consent</code>). *   <code>TB.ui.CookieBanner.getConsent()</code>: Retrieves current consent status from <code>localStorage</code>.</p> <p>Functionality: *   Shows a banner at the bottom of the page. *   Allows accepting recommended settings or opening a modal for granular control (Essential, Preferences, Analytics). *   Saves consent to <code>localStorage</code>. *   Emits <code>cookieConsent:updated</code> event with consent settings: <code>{ essential, preferences, analytics, source }</code>.</p> <p>Example: <pre><code>// This can be called early in your application setup\nTB.ui.CookieBanner.show({\n    title: 'Our Cookie Policy',\n    message: 'We use cookies to improve your experience. By clicking \"Accept\", you agree to our use of cookies.',\n    termsLink: '/privacy-policy',\n    onConsent: (settings) =&gt; {\n        console.log('Cookie consent given:', settings);\n        if (settings.analytics) {\n            // Initialize analytics tools\n        }\n    }\n});\n\n// Check consent later\nconst consent = TB.ui.CookieBanner.getConsent();\nif (consent &amp;&amp; consent.analytics) {\n    // ...\n}\n</code></pre></p>"},{"location":"tbjs/#markdownrenderer-tbuimarkdownrenderer","title":"MarkdownRenderer (<code>TB.ui.MarkdownRenderer</code>)","text":"<p>Renders Markdown content to HTML, with optional syntax highlighting using <code>highlight.js</code>.</p> <p>Prerequisites: *   <code>marked.js</code> (e.g., <code>window.marked</code>) must be globally available. *   <code>highlight.js</code> (e.g., <code>window.hljs</code>) must be globally available for syntax highlighting. *   <code>marked-highlight</code> (e.g., <code>window.markedHighlight</code>) if using the extension for <code>hljs</code>.</p> <p>Methods: *   <code>TB.ui.MarkdownRenderer.init()</code>: Initializes <code>marked</code> with <code>highlight.js</code>. Called automatically on first render if needed, or can be called explicitly. *   <code>TB.ui.MarkdownRenderer.render(markdownString)</code>: Converts a Markdown string to HTML. *   <code>TB.ui.MarkdownRenderer.renderElement(element)</code>: Renders the content of a DOM element (if it has class <code>.markdown</code> and not already rendered). Adds Tailwind Prose classes. *   <code>TB.ui.MarkdownRenderer.renderAllIn(parentElement)</code>: Finds all elements with class <code>.markdown</code> within <code>parentElement</code> (that haven't been rendered yet) and renders them.</p> <p>Usage: Typically used by <code>TB.ui.processDynamicContent</code> when new HTML containing elements with the class <code>markdown</code> is added to the DOM.</p> <p>Example: HTML: <pre><code>&lt;div class=\"markdown\"&gt;\n# My Markdown Title\n\nThis is some **bold** text and a [link](https://example.com).\n\n```javascript\nconsole.log('This is JavaScript code');\n</code></pre> <pre><code>JavaScript (rendering is often automatic after content swap):\n```javascript\n// If you add markdown content dynamically outside of router/HTMX swaps:\nconst myDiv = document.createElement('div');\nmyDiv.className = 'markdown';\nmyDiv.textContent = '## Subtitle\\n* Item 1\\n* Item 2';\ndocument.body.appendChild(myDiv);\nTB.ui.MarkdownRenderer.renderElement(myDiv); // Or TB.ui.MarkdownRenderer.renderAllIn(document.body);\n</code></pre> The rendered output will be styled with Tailwind Prose classes (<code>prose dark:prose-invert max-w-none</code>).</p>"},{"location":"tbjs/#autocompletewidget-tbuiautocompletewidget","title":"AutocompleteWidget (<code>TB.ui.AutocompleteWidget</code>)","text":"<p>Provides autocomplete functionality for input fields.</p> <p>Options: *   <code>source</code>: Array of strings, or a function <code>(inputValue) =&gt; Promise&lt;string[]&gt;</code> or <code>(inputValue) =&gt; string[]</code>. *   <code>minLength</code>: (number, default <code>1</code>) Minimum characters to type before suggestions appear. *   <code>onSelect</code>: Callback <code>(value, inputElement) =&gt; {}</code> when an item is selected. *   <code>customClasses</code>: Object to customize Tailwind classes for <code>list</code>, <code>item</code>, <code>activeItem</code>, <code>highlight</code>.</p> <p>Initialization: *   <code>new TB.ui.AutocompleteWidget(inputElement, options)</code> *   <code>TB.ui.AutocompleteWidget.initAll(selector = 'input[data-tb-autocomplete]')</code>: Initializes for all matching elements.     *   Uses <code>data-tb-autocomplete-source</code> attribute (JSON array or global function name) if present.</p> <p>Example: HTML: <pre><code>&lt;div class=\"tb-relative\"&gt; &lt;!-- Autocomplete list will be positioned relative to this --&gt;\n    &lt;input type=\"text\" id=\"myAutocompleteInput\" class=\"tb-border tb-p-2 tb-w-full\" placeholder=\"Search...\"&gt;\n&lt;/div&gt;\n&lt;div class=\"tb-relative\"&gt;\n    &lt;input type=\"text\" data-tb-autocomplete data-tb-autocomplete-source='[\"Apple\", \"Banana\", \"Cherry\"]' class=\"tb-border tb-p-2 tb-w-full\" placeholder=\"Fruit Search...\"&gt;\n&lt;/div&gt;\n</code></pre> JavaScript: <pre><code>const acInput = document.getElementById('myAutocompleteInput');\nconst mySourceFunction = async (term) =&gt; {\n    // In a real app, fetch from an API\n    const items = ['JavaScript', 'Java', 'Python', 'PHP', 'Perl'];\n    return items.filter(item =&gt; item.toLowerCase().includes(term.toLowerCase()));\n};\n\nnew TB.ui.AutocompleteWidget(acInput, {\n    source: mySourceFunction,\n    minLength: 2,\n    onSelect: (value, el) =&gt; {\n        console.log(`Selected: ${value} from input:`, el);\n    }\n});\n\n// Initialize declarative autocomplete inputs\nTB.ui.AutocompleteWidget.initAll();\n</code></pre></p>"},{"location":"tbjs/#navmenu-tbuinavmenu","title":"NavMenu (<code>TB.ui.NavMenu</code>)","text":"<p>Manages a responsive navigation menu, typically a slide-in or modal menu for mobile.</p> <p>Options: *   <code>triggerSelector</code>: (string, default <code>'#links'</code>) Selector for the menu toggle button. *   <code>menuContentHtml</code>: (string) HTML content for the menu. *   <code>menuId</code>: (string, default <code>'tb-nav-menu-modal'</code>) ID for the menu container. *   <code>openIconClass</code>, <code>closeIconClass</code>: Material Symbols class names for the toggle icon. *   <code>customClasses</code>: For styling <code>overlay</code>, <code>menuContainer</code>, <code>iconContainer</code>.</p> <p>HTML Structure Expectation (for default trigger): The trigger element (e.g., <code>#links</code>) should ideally contain a <code>&lt;span&gt;</code> (often with <code>class=\"material-symbols-outlined\"</code>) for the icon. If empty, the component will append one. The menu itself is appended to an element with <code>id=\"Nav-Main\"</code>.</p> <p>Example HTML: <pre><code>&lt;nav id=\"Nav-Main\" class=\"tb-bg-gray-800 tb-text-white tb-p-4 tb-flex tb-justify-between tb-items-center\"&gt;\n    &lt;a href=\"/\" class=\"tb-text-xl tb-font-bold\"&gt;MyApp&lt;/a&gt;\n    &lt;button id=\"menuTrigger\" class=\"tb-p-2 md:tb-hidden\"&gt; &lt;!-- md:hidden to hide on larger screens --&gt;\n        &lt;span class=\"material-symbols-outlined\"&gt;menu&lt;/span&gt;\n    &lt;/button&gt;\n    &lt;ul class=\"hidden md:tb-flex tb-space-x-4\"&gt; &lt;!-- Desktop links --&gt;\n        &lt;li&gt;&lt;a href=\"/page1\"&gt;Page 1&lt;/a&gt;&lt;/li&gt;\n        &lt;li&gt;&lt;a href=\"/page2\"&gt;Page 2&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;/nav&gt;\n</code></pre> Example JavaScript: <pre><code>// Initialize the NavMenu\nconst navMenu = TB.ui.NavMenu.init({\n    triggerSelector: '#menuTrigger', // Custom trigger\n    menuContentHtml: `\n        &lt;ul class=\"tb-space-y-2 tb-p-4\"&gt;\n            &lt;li&gt;&lt;a href=\"/home\" class=\"tb-block tb-p-2 hover:tb-bg-gray-700 tb-rounded\"&gt;Home&lt;/a&gt;&lt;/li&gt;\n            &lt;li&gt;&lt;a href=\"/about\" class=\"tb-block tb-p-2 hover:tb-bg-gray-700 tb-rounded\"&gt;About&lt;/a&gt;&lt;/li&gt;\n            &lt;li&gt;&lt;a href=\"/contact\" class=\"tb-block tb-p-2 hover:tb-bg-gray-700 tb-rounded\"&gt;Contact&lt;/a&gt;&lt;/li&gt;\n        &lt;/ul&gt;\n    `,\n    // customize classes if needed, e.g., for a different background:\n    // customClasses: {\n    //   menuContainer: 'fixed top-0 left-0 h-full w-64 sm:w-72 bg-neutral-800 shadow-xl z-[1041] transform -translate-x-full transition-transform duration-300 ease-in-out text-white',\n    // }\n});\n\n// The menu links will automatically close the menu upon navigation if handled by TB.router.\n</code></pre></p>"},{"location":"tbjs/#5-usage-examples","title":"5. Usage Examples","text":""},{"location":"tbjs/#basic-application-setup","title":"Basic Application Setup","text":"<p>This example shows a minimal setup to get <code>tbjs</code> running with a simple home page.</p> <p><code>index.html</code>: <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;title&gt;tbjs App&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"path/to/tbjs/dist/tbjs.css\"&gt;\n    &lt;link rel=\"stylesheet\" href=\"your-app-styles.css\"&gt; &lt;!-- Your app specific styles --&gt;\n    &lt;script src=\"https://unpkg.com/htmx.org@2.0.0/dist/htmx.min.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"path/to/tbjs/dist/tbjs.js\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;header&gt;\n        &lt;nav&gt;\n            &lt;a href=\"/home\"&gt;Home&lt;/a&gt;\n            &lt;a href=\"/about\"&gt;About&lt;/a&gt;\n        &lt;/nav&gt;\n    &lt;/header&gt;\n    &lt;main id=\"app-root\"&gt;&lt;/main&gt;\n\n    &lt;script type=\"module\"&gt;\n        document.addEventListener('DOMContentLoaded', () =&gt; {\n            TB.init({\n                appRootId: 'app-root',\n                baseApiUrl: '/api',\n                logLevel: 'debug',\n                // Default initial navigation will be to current path or /index.html handled by router\n            });\n        });\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p><code>/web/pages/home.html</code> (assuming <code>baseFileUrl</code> allows fetching this via <code>/home</code> or similar) <pre><code>&lt;h1&gt;Welcome to the Home Page!&lt;/h1&gt;\n&lt;p&gt;This content was loaded by tbjs router.&lt;/p&gt;\n&lt;button hx-get=\"/api/time\" hx-target=\"#time-div\"&gt;Get Server Time&lt;/button&gt;\n&lt;div id=\"time-div\"&gt;&lt;/div&gt;\n</code></pre> *   <code>TB.init()</code> sets up the router, which loads the initial page content (e.g., based on URL, or defaults to <code>/index.html</code> which might then redirect or show base content). *   Router intercepts clicks on <code>&lt;a&gt;</code> tags. *   HTMX integration is active.</p>"},{"location":"tbjs/#fetching-data-and-updating-state","title":"Fetching Data and Updating State","text":"<pre><code>// products.js\nasync function loadAndDisplayProducts() {\n    TB.ui.Loader.show('Loading products...');\n    const result = await TB.api.request('products', 'list', { limit: 5 }, 'GET');\n    TB.ui.Loader.hide();\n\n    if (result.error === TB.api.ToolBoxError.none) {\n        const products = result.get();\n        TB.state.set('shop.products', products);\n        renderProductList(products);\n    } else {\n        TB.ui.Toast.showError('Failed to load products: ' + result.info.help_text);\n    }\n}\n\nfunction renderProductList(products) {\n    const container = document.getElementById('product-list-container');\n    if (!container) return;\n    container.innerHTML = `\n        &lt;ul class=\"tb-list-disc tb-pl-5\"&gt;\n            ${products.map(p =&gt; `&lt;li class=\"tb-mb-2\"&gt;${p.name} - $${p.price}&lt;/li&gt;`).join('')}\n        &lt;/ul&gt;\n    `;\n}\n\n// Listen for state changes to re-render if needed elsewhere\nTB.events.on('state:changed:shop.products', (newProducts) =&gt; {\n    // Potentially update other parts of the UI, or just log\n    console.log('Product list updated in state:', newProducts);\n});\n\n// Initial load\n// loadAndDisplayProducts(); // Call this when the relevant view is loaded\n</code></pre>"},{"location":"tbjs/#client-side-routing","title":"Client-Side Routing","text":"<p>The router automatically handles link clicks and browser navigation.</p> <p><code>/web/pages/about.html</code>: <pre><code>&lt;h2&gt;About Us&lt;/h2&gt;\n&lt;p&gt;This is the about page, loaded dynamically by the router.&lt;/p&gt;\n&lt;script&gt;\n    // This script will be executed when about.html is loaded\n    console.log('About page script executed!');\n    TB.logger.info('[AboutPage] Loaded and script ran.');\n&lt;/script&gt;\n</code></pre> *   When a user clicks <code>&lt;a href=\"/about\"&gt;About&lt;/a&gt;</code>, <code>TB.router</code> fetches <code>/web/pages/about.html</code> (assuming <code>baseFileUrl</code> + <code>/about</code> maps to it) and injects its content into <code>#app-root</code>. *   The inline script in <code>about.html</code> is executed.</p>"},{"location":"tbjs/#displaying-a-modal","title":"Displaying a Modal","text":"<pre><code>document.getElementById('show-info-modal').addEventListener('click', () =&gt; {\n    TB.ui.Modal.show({\n        title: 'Important Information',\n        content: `\n            &lt;p&gt;This is some important information presented in a modal.&lt;/p&gt;\n            &lt;p&gt;Current time from state: ${TB.state.get('app.currentTime') || 'Not set'}&lt;/p&gt;\n        `,\n        buttons: [\n            { text: 'OK', action: (modal) =&gt; modal.close(), variant: 'primary' }\n        ]\n    });\n});\n</code></pre>"},{"location":"tbjs/#user-authentication-flow","title":"User Authentication Flow","text":"<p>A simplified example of a device key login.</p> <p>HTML for Login: <pre><code>&lt;!-- login.html --&gt;\n&lt;h2&gt;Login with Device Key&lt;/h2&gt;\n&lt;input type=\"text\" id=\"username\" placeholder=\"Username\" class=\"tb-border tb-p-2\"&gt;\n&lt;button id=\"loginButton\" class=\"tb-bg-blue-500 tb-text-white tb-p-2 tb-rounded\"&gt;Login&lt;/button&gt;\n&lt;div id=\"login-status\"&gt;&lt;/div&gt;\n</code></pre></p> <p>JavaScript for login.html (or global script managing this view): <pre><code>// Assuming this script runs when login.html is loaded\ndocument.addEventListener('DOMContentLoaded', () =&gt; { // Or use TB.events router:contentProcessed\n    const loginButton = document.getElementById('loginButton');\n    const usernameInput = document.getElementById('username');\n    const statusDiv = document.getElementById('login-status');\n\n    if (loginButton) {\n        loginButton.addEventListener('click', async () =&gt; {\n            const username = usernameInput.value.trim();\n            if (!username) {\n                statusDiv.textContent = 'Please enter a username.';\n                return;\n            }\n\n            statusDiv.textContent = 'Attempting login...';\n            TB.ui.Loader.show('Logging in...');\n\n            try {\n                const result = await TB.user.loginWithDeviceKey(username);\n                TB.ui.Loader.hide();\n\n                if (result.success) {\n                    statusDiv.textContent = `Login successful! Welcome ${TB.user.getUsername()}.`;\n                    TB.ui.Toast.showSuccess('Login successful!');\n                    TB.router.navigateTo('/dashboard'); // Navigate to a protected area\n                } else {\n                    statusDiv.textContent = `Login failed: ${result.message}`;\n                    TB.ui.Toast.showError(result.message);\n                }\n            } catch (error) {\n                TB.ui.Loader.hide();\n                statusDiv.textContent = `Login error: ${error.message}`;\n                TB.logger.error('Login process error:', error);\n            }\n        });\n    }\n});\n</code></pre></p>"},{"location":"tbjs/#6-styling-with-tailwind-css","title":"6. Styling with Tailwind CSS","text":"<p><code>tbjs</code> components are designed to be styled with Tailwind CSS. The framework includes a <code>tailwind.config.js</code> and <code>postcss.config.js</code>.</p> <p>Key Points: *   Prefix: The provided <code>tailwind.config.js</code> uses <code>prefix: 'tb-'</code>. This means all Tailwind utility classes used internally by <code>tbjs</code> components will be prefixed (e.g., <code>tb-bg-blue-500</code>, <code>tb-text-lg</code>). This helps avoid conflicts with your application's own Tailwind classes if it doesn't use a prefix or uses a different one. *   CSS Variables: The configuration defines CSS variables for theming (e.g., <code>--tb-color-primary-500</code>, <code>--tb-color-background</code>). These are used in the <code>theme.extend.colors</code> section of <code>tailwind.config.js</code> and in <code>tbjs-main.css</code>. This allows themes (light/dark) to be easily applied and customized. *   <code>tbjs-main.css</code>: This file imports Tailwind utilities and defines the base CSS variables and some default styles for components like the Loader. *   Customization: You can customize the <code>tbjs</code> Tailwind configuration (<code>tbjs/tailwind.config.js</code>) or integrate its plugin settings into your main application's Tailwind config.</p> <p>Using <code>tbjs</code> Tailwind Config in Your Project: If your project also uses Tailwind CSS, you can either: 1.  Run two PostCSS processes: One for <code>tbjs</code> (using its config) and one for your app (using your app's config). 2.  Merge configurations: If your app's Tailwind setup can consume the <code>tbjs</code> Tailwind config (e.g., as a preset or by merging <code>content</code> paths and <code>theme</code> extensions).</p> <pre><code>Example of merging in your app's `tailwind.config.js`:\n```javascript\n// your-app/tailwind.config.js\nimport tbjsTailwindConfig from 'path/to/tbjs/tailwind.config.js';\n\nexport default {\n  content: [\n    './src/**/*.{html,js}', // Your app's content\n    './node_modules/tbjs/src/**/*.{html,js}', // Include tbjs components for scanning\n  ],\n  darkMode: 'class', // Ensure consistency\n  prefix: '', // Or your app's prefix\n  theme: {\n    extend: {\n      // Merge tbjs theme extensions if needed, being mindful of prefix differences\n      colors: {\n        ...tbjsTailwindConfig.theme.extend.colors, // May need adjustment if your app doesn't use 'tb-' prefix\n        // Your app-specific colors\n      },\n      // ... other extensions\n    },\n  },\n  plugins: [\n    // Your app's plugins\n  ],\n};\n```\n*The key is to ensure Tailwind processes the classes used in `tbjs` components.* Using the `tb-` prefix in `tbjs` helps isolate its styles.\n</code></pre>"},{"location":"tbjs/#7-building-tbjs-for-developers","title":"7. Building <code>tbjs</code> (For Developers)","text":"<p>If you are modifying the <code>tbjs</code> framework itself:</p> <ul> <li>Dependencies: Install development dependencies: <code>npm install</code></li> <li>Build: <code>npm run build</code> (creates production build in <code>dist/</code>)</li> <li>Watch: <code>npm run watch</code> (watches for changes and rebuilds in production mode)</li> <li>Lint: <code>npm run lint</code> (checks JavaScript code style)</li> </ul> <p>The build process uses Webpack, configured in <code>webpack.config.js</code>. It bundles the JavaScript into <code>dist/tbjs.js</code> (UMD format) and extracts CSS into <code>dist/tbjs.css</code>.</p>"},{"location":"toolboxv2/","title":"toolboxv2 API Reference","text":"<p>This section provides an API reference for key components directly available from the <code>toolboxv2</code> package.</p>"},{"location":"toolboxv2/#core-application-tooling","title":"Core Application &amp; Tooling","text":""},{"location":"toolboxv2/#toolboxv2.AppType","title":"<code>toolboxv2.AppType</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class AppType:\n    prefix: str\n    id: str\n    globals: dict[str, Any] = {\"root\": dict, }\n    locals: dict[str, Any] = {\"user\": {'app': \"self\"}, }\n\n    local_test: bool = False\n    start_dir: str\n    data_dir: str\n    config_dir: str\n    info_dir: str\n\n    logger: logging.Logger\n    logging_filename: str\n\n    api_allowed_mods_list: list[str] = []\n\n    version: str\n    loop: asyncio.AbstractEventLoop\n\n    keys: dict[str, str] = {\n        \"MACRO\": \"macro~~~~:\",\n        \"MACRO_C\": \"m_color~~:\",\n        \"HELPER\": \"helper~~~:\",\n        \"debug\": \"debug~~~~:\",\n        \"id\": \"name-spa~:\",\n        \"st-load\": \"mute~load:\",\n        \"comm-his\": \"comm-his~:\",\n        \"develop-mode\": \"dev~mode~:\",\n        \"provider::\": \"provider::\",\n    }\n\n    defaults: dict[str, (bool or dict or dict[str, dict[str, str]] or str or list[str] or list[list]) | None] = {\n        \"MACRO\": list[str],\n        \"MACRO_C\": dict,\n        \"HELPER\": dict,\n        \"debug\": str,\n        \"id\": str,\n        \"st-load\": False,\n        \"comm-his\": list[list],\n        \"develop-mode\": bool,\n    }\n\n    config_fh: FileHandler\n    _debug: bool\n    flows: dict[str, Callable]\n    dev_modi: bool\n    functions: dict[str, Any]\n    modules: dict[str, Any]\n\n    interface_type: ToolBoxInterfaces\n    REFIX: str\n\n    alive: bool\n    called_exit: tuple[bool, float]\n    args_sto: AppArgs\n    system_flag = None\n    session = None\n    appdata = None\n    exit_tasks = []\n\n    enable_profiling: bool = False\n    sto = None\n\n    def __init__(self, prefix: None | str= None, args: AppArgs | None = None):\n        self.args_sto = args\n        self.prefix = prefix\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def exit_main(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def hide_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def show_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def disconnect(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def set_logger(self, debug=False):\n        \"\"\"proxi attr\"\"\"\n\n    @property\n    def debug(self):\n        \"\"\"proxi attr\"\"\"\n        return self._debug\n\n    def debug_rains(self, e):\n        \"\"\"proxi attr\"\"\"\n\n    def set_flows(self, r):\n        \"\"\"proxi attr\"\"\"\n\n    def run_flows(self, name, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def rrun_flows(self, name, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def idle(self):\n        import time\n        self.print(\"idle\")\n        try:\n            while self.alive:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            pass\n        self.print(\"idle done\")\n\n    async def a_idle(self):\n        self.print(\"a idle\")\n        try:\n            if hasattr(self, 'daemon_app'):\n                self.print(\"serving daemon\")\n                await self.daemon_app.connect(self)\n            else:\n                self.print(\"serving default\")\n                while self.alive:\n                    await asyncio.sleep(1)\n        except KeyboardInterrupt:\n            pass\n        self.print(\"a idle done\")\n\n    @debug.setter\n    def debug(self, value):\n        \"\"\"proxi attr\"\"\"\n\n    def _coppy_mod(self, content, new_mod_dir, mod_name, file_type='py'):\n        \"\"\"proxi attr\"\"\"\n\n    def _pre_lib_mod(self, mod_name, path_to=\"./runtime\", file_type='py'):\n        \"\"\"proxi attr\"\"\"\n\n    def _copy_load(self, mod_name, file_type='py', **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True):\n        \"\"\"proxi attr\"\"\"\n\n    def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n        \"\"\"proxi attr\"\"\"\n\n    def save_initialized_module(self, tools_class, spec):\n        \"\"\"proxi attr\"\"\"\n\n    def mod_online(self, mod_name, installed=False):\n        \"\"\"proxi attr\"\"\"\n\n    def _get_function(self,\n                      name: Enum or None,\n                      state: bool = True,\n                      specification: str = \"app\",\n                      metadata=False, as_str: tuple or None = None, r=0):\n        \"\"\"proxi attr\"\"\"\n\n    def save_exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def load_mod(self, mod_name: str, mlm='I', **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    async def init_module(self, modular):\n        return await self.load_mod(modular)\n\n    async def load_all_mods_in_file(self, working_dir=\"mods\"):\n        \"\"\"proxi attr\"\"\"\n\n    def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\"):\n        \"\"\"proxi attr\"\"\"\n\n    def remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            self.remove_mod(mod, delete=delete)\n\n    async def a_remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            await self.a_remove_mod(mod, delete=delete)\n\n    def print_ok(self):\n        \"\"\"proxi attr\"\"\"\n        self.logger.info(\"OK\")\n\n    def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n        \"\"\"proxi attr\"\"\"\n\n    def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None):\n        \"\"\"proxi attr\"\"\"\n\n    def remove_mod(self, mod_name, spec='app', delete=True):\n        \"\"\"proxi attr\"\"\"\n\n    async def a_remove_mod(self, mod_name, spec='app', delete=True):\n        \"\"\"proxi attr\"\"\"\n\n    def exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def web_context(self) -&gt; str:\n        \"\"\"returns the build index ( toolbox web component )\"\"\"\n\n    async def a_exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def save_load(self, modname, spec='app'):\n        \"\"\"proxi attr\"\"\"\n\n    def get_function(self, name: Enum or tuple, **kwargs):\n        \"\"\"\n        Kwargs for _get_function\n            metadata:: return the registered function dictionary\n                stateless: (function_data, None), 0\n                stateful: (function_data, higher_order_function), 0\n            state::boolean\n                specification::str default app\n        \"\"\"\n\n    def run_a_from_sync(self, function, *args):\n        \"\"\"\n        run a async fuction\n        \"\"\"\n\n    def run_function(self, mod_function_name: Enum or tuple,\n                     tb_run_function_with_state=True,\n                     tb_run_with_specification='app',\n                     args_=None,\n                     kwargs_=None,\n                     *args,\n                     **kwargs) -&gt; Result:\n\n        \"\"\"proxi attr\"\"\"\n\n    async def a_run_function(self, mod_function_name: Enum or tuple,\n                             tb_run_function_with_state=True,\n                             tb_run_with_specification='app',\n                             args_=None,\n                             kwargs_=None,\n                             *args,\n                             **kwargs) -&gt; Result:\n\n        \"\"\"proxi attr\"\"\"\n\n    def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n        \"\"\"\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        proxi attr\n        \"\"\"\n\n    async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict):\n        \"\"\"\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        proxi attr\n        \"\"\"\n\n    async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None, method=\"GET\",\n                       args_=None,\n                       kwargs_=None,\n                       *args, **kwargs):\n        \"\"\"run a function remote via http / https\"\"\"\n\n    def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n                get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                kwargs_=None,\n                *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                        backwords_compability_variabel_string_holder=None,\n                        get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                        kwargs_=None,\n                        *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def print(text, *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def sprint(text, *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    # ----------------------------------------------------------------\n    # Decorators for the toolbox\n\n    def _register_function(self, module_name, func_name, data):\n        \"\"\"proxi attr\"\"\"\n\n    def _create_decorator(self, type_: str,\n                          name: str = \"\",\n                          mod_name: str = \"\",\n                          level: int = -1,\n                          restrict_in_virtual_mode: bool = False,\n                          api: bool = False,\n                          helper: str = \"\",\n                          version: str or None = None,\n                          initial=False,\n                          exit_f=False,\n                          test=True,\n                          samples=None,\n                          state=None,\n                          pre_compute=None,\n                          post_compute=None,\n                          memory_cache=False,\n                          file_cache=False,\n                          row=False,\n                          request_as_kwarg=False,\n                          memory_cache_max_size=100,\n                          memory_cache_ttl=300):\n        \"\"\"proxi attr\"\"\"\n\n        # data = {\n        #     \"type\": type_,\n        #     \"module_name\": module_name,\n        #     \"func_name\": func_name,\n        #     \"level\": level,\n        #     \"restrict_in_virtual_mode\": restrict_in_virtual_mode,\n        #     \"func\": func,\n        #     \"api\": api,\n        #     \"helper\": helper,\n        #     \"version\": version,\n        #     \"initial\": initial,\n        #     \"exit_f\": exit_f,\n        #     \"__module__\": func.__module__,\n        #     \"signature\": sig,\n        #     \"params\": params,\n        #     \"state\": (\n        #         False if len(params) == 0 else params[0] in ['self', 'state', 'app']) if state is None else state,\n        #     \"do_test\": test,\n        #     \"samples\": samples,\n        #     \"request_as_kwarg\": request_as_kwarg,\n\n    def tb(self, name=None,\n           mod_name: str = \"\",\n           helper: str = \"\",\n           version: str or None = None,\n           test: bool = True,\n           restrict_in_virtual_mode: bool = False,\n           api: bool = False,\n           initial: bool = False,\n           exit_f: bool = False,\n           test_only: bool = False,\n           memory_cache: bool = False,\n           file_cache: bool = False,\n           row=False,\n           request_as_kwarg: bool = False,\n           state: bool or None = None,\n           level: int = 0,\n           memory_cache_max_size: int = 100,\n           memory_cache_ttl: int = 300,\n           samples: list or dict or None = None,\n           interface: ToolBoxInterfaces or None or str = None,\n           pre_compute=None,\n           post_compute=None,\n           api_methods=None,\n           ):\n        \"\"\"\n    A decorator for registering and configuring functions within a module.\n\n    This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\n    Args:\n        name (str, optional): The name to register the function under. Defaults to the function's own name.\n        mod_name (str, optional): The name of the module the function belongs to.\n        helper (str, optional): A helper string providing additional information about the function.\n        version (str or None, optional): The version of the function or module.\n        test (bool, optional): Flag to indicate if the function is for testing purposes.\n        restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n        api (bool, optional): Flag to indicate if the function is part of an API.\n        initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n        exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n        test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n        memory_cache (bool, optional): Flag to enable memory caching for the function.\n        request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n        file_cache (bool, optional): Flag to enable file caching for the function.\n        row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n        state (bool or None, optional): Flag to indicate if the function maintains state.\n        level (int, optional): The level of the function, used for prioritization or categorization.\n        memory_cache_max_size (int, optional): Maximum size of the memory cache.\n        memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n        samples (list or dict or None, optional): Samples or examples of function usage.\n        interface (str, optional): The interface type for the function.\n        pre_compute (callable, optional): A function to be called before the main function.\n        post_compute (callable, optional): A function to be called after the main function.\n        api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\n    Returns:\n        function: The decorated function with additional processing and registration capabilities.\n    \"\"\"\n        if interface is None:\n            interface = \"tb\"\n        if test_only and 'test' not in self.id:\n            return lambda *args, **kwargs: args\n        return self._create_decorator(interface,\n                                      name,\n                                      mod_name,\n                                      level=level,\n                                      restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                      helper=helper,\n                                      api=api,\n                                      version=version,\n                                      initial=initial,\n                                      exit_f=exit_f,\n                                      test=test,\n                                      samples=samples,\n                                      state=state,\n                                      pre_compute=pre_compute,\n                                      post_compute=post_compute,\n                                      memory_cache=memory_cache,\n                                      file_cache=file_cache,\n                                      row=row,\n                                      request_as_kwarg=request_as_kwarg,\n                                      memory_cache_max_size=memory_cache_max_size,\n                                      memory_cache_ttl=memory_cache_ttl)\n\n    def print_functions(self, name=None):\n\n\n        if not self.functions:\n            print(\"Nothing to see\")\n            return\n\n        def helper(_functions):\n            for func_name, data in _functions.items():\n                if not isinstance(data, dict):\n                    continue\n\n                func_type = data.get('type', 'Unknown')\n                func_level = 'r' if data['level'] == -1 else data['level']\n                api_status = 'Api' if data.get('api', False) else 'Non-Api'\n\n                print(f\"  Function: {func_name}{data.get('signature', '()')}; \"\n                      f\"Type: {func_type}, Level: {func_level}, {api_status}\")\n\n        if name is not None:\n            functions = self.functions.get(name)\n            if functions is not None:\n                print(f\"\\nModule: {name}; Type: {functions.get('app_instance_type', 'Unknown')}\")\n                helper(functions)\n                return\n        for module, functions in self.functions.items():\n            print(f\"\\nModule: {module}; Type: {functions.get('app_instance_type', 'Unknown')}\")\n            helper(functions)\n\n    def save_autocompletion_dict(self):\n        \"\"\"proxi attr\"\"\"\n\n    def get_autocompletion_dict(self):\n        \"\"\"proxi attr\"\"\"\n\n    def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n        \"\"\"proxi attr\"\"\"\n\n    def save_registry_as_enums(self, directory: str, filename: str):\n        \"\"\"proxi attr\"\"\"\n\n    async def execute_all_functions_(self, m_query='', f_query=''):\n        print(\"Executing all functions\")\n        from ..extras import generate_test_cases\n        all_data = {\n            \"modular_run\": 0,\n            \"modular_fatal_error\": 0,\n            \"errors\": 0,\n            \"modular_sug\": 0,\n            \"coverage\": [],\n            \"total_coverage\": {},\n        }\n        items = list(self.functions.items()).copy()\n        for module_name, functions in items:\n            infos = {\n                \"functions_run\": 0,\n                \"functions_fatal_error\": 0,\n                \"error\": 0,\n                \"functions_sug\": 0,\n                'calls': {},\n                'callse': {},\n                \"coverage\": [0, 0],\n            }\n            all_data['modular_run'] += 1\n            if not module_name.startswith(m_query):\n                all_data['modular_sug'] += 1\n                continue\n\n            with Spinner(message=f\"In {module_name}| \"):\n                f_items = list(functions.items()).copy()\n                for function_name, function_data in f_items:\n                    if not isinstance(function_data, dict):\n                        continue\n                    if not function_name.startswith(f_query):\n                        continue\n                    test: list = function_data.get('do_test')\n                    # print(test, module_name, function_name, function_data)\n                    infos[\"coverage\"][0] += 1\n                    if test is False:\n                        continue\n\n                    with Spinner(message=f\"\\t\\t\\t\\t\\t\\tfuction {function_name}...\"):\n                        params: list = function_data.get('params')\n                        sig: signature = function_data.get('signature')\n                        state: bool = function_data.get('state')\n                        samples: bool = function_data.get('samples')\n\n                        test_kwargs_list = [{}]\n\n                        if params is not None:\n                            test_kwargs_list = samples if samples is not None else generate_test_cases(sig=sig)\n                            # print(test_kwargs)\n                            # print(test_kwargs[0])\n                            # test_kwargs = test_kwargs_list[0]\n                        # print(module_name, function_name, test_kwargs_list)\n                        infos[\"coverage\"][1] += 1\n                        for test_kwargs in test_kwargs_list:\n                            try:\n                                # print(f\"test Running {state=} |{module_name}.{function_name}\")\n                                result = await self.a_run_function((module_name, function_name),\n                                                                   tb_run_function_with_state=state,\n                                                                   **test_kwargs)\n                                if not isinstance(result, Result):\n                                    result = Result.ok(result)\n                                if result.info.exec_code == 0:\n                                    infos['calls'][function_name] = [test_kwargs, str(result)]\n                                    infos['functions_sug'] += 1\n                                else:\n                                    infos['functions_sug'] += 1\n                                    infos['error'] += 1\n                                    infos['callse'][function_name] = [test_kwargs, str(result)]\n                            except Exception as e:\n                                infos['functions_fatal_error'] += 1\n                                infos['callse'][function_name] = [test_kwargs, str(e)]\n                            finally:\n                                infos['functions_run'] += 1\n\n                if infos['functions_run'] == infos['functions_sug']:\n                    all_data['modular_sug'] += 1\n                else:\n                    all_data['modular_fatal_error'] += 1\n                if infos['error'] &gt; 0:\n                    all_data['errors'] += infos['error']\n\n                all_data[module_name] = infos\n                if infos['coverage'][0] == 0:\n                    c = 0\n                else:\n                    c = infos['coverage'][1] / infos['coverage'][0]\n                all_data[\"coverage\"].append(f\"{module_name}:{c:.2f}\\n\")\n        total_coverage = sum([float(t.split(\":\")[-1]) for t in all_data[\"coverage\"]]) / len(all_data[\"coverage\"])\n        print(\n            f\"\\n{all_data['modular_run']=}\\n{all_data['modular_sug']=}\\n{all_data['modular_fatal_error']=}\\n{total_coverage=}\")\n        d = analyze_data(all_data)\n        return Result.ok(data=all_data, data_info=d)\n\n    @staticmethod\n    def calculate_complexity(filename_or_code):\n        from radon.complexity import cc_rank, cc_visit\n        if os.path.exists(filename_or_code):\n            with open(filename_or_code) as file:\n                code = file.read()\n        else:\n            code = filename_or_code\n\n        # Calculate and print Cyclomatic Complexity\n        complexity_results = cc_visit(code)\n        i = -1\n        avg_complexity = 0\n        for block in complexity_results:\n            complexity = block.complexity\n            i += 1\n            print(f\"block: {block.name} {i} Class/Fuction/Methode : {block.letter}\")\n            print(f\"    fullname: {block.fullname}\")\n            print(f\"    Cyclomatic Complexity: {complexity}\")\n            # Optional: Get complexity rank\n            avg_complexity += complexity\n            rank = cc_rank(complexity)\n            print(f\"    Complexity Rank: {rank}\")\n            # print(f\"    lineno: {block.lineno}\")\n            print(f\"    endline: {block.endline}\")\n            print(f\"    col_offset: {block.col_offset}\\n\")\n        if i &lt;= 0:\n            i += 2\n        avg_complexity = avg_complexity / i\n        print(f\"\\nAVG Complexity: {avg_complexity:.2f}\")\n        print(f\"Total Rank: {cc_rank(int(avg_complexity + i // 10))}\")\n\n    async def execute_function_test(self, module_name: str, function_name: str,\n                                    function_data: dict, test_kwargs: dict,\n                                    profiler: cProfile.Profile) -&gt; tuple[bool, str, dict, float]:\n        start_time = time.time()\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            try:\n                result = await self.a_run_function(\n                    (module_name, function_name),\n                    tb_run_function_with_state=function_data.get('state'),\n                    **test_kwargs\n                )\n\n                if not isinstance(result, Result):\n                    result = Result.ok(result)\n\n                success = result.info.exec_code == 0\n                execution_time = time.time() - start_time\n                return success, str(result), test_kwargs, execution_time\n            except Exception as e:\n                execution_time = time.time() - start_time\n                return False, str(e), test_kwargs, execution_time\n\n    async def process_function(self, module_name: str, function_name: str,\n                               function_data: dict, profiler: cProfile.Profile) -&gt; tuple[str, ModuleInfo]:\n        start_time = time.time()\n        info = ModuleInfo()\n\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            if not isinstance(function_data, dict):\n                return function_name, info\n\n            test = function_data.get('do_test')\n            info.coverage[0] += 1\n\n            if test is False:\n                return function_name, info\n\n            params = function_data.get('params')\n            sig = function_data.get('signature')\n            samples = function_data.get('samples')\n\n            test_kwargs_list = [{}] if params is None else (\n                samples if samples is not None else generate_test_cases(sig=sig)\n            )\n\n            info.coverage[1] += 1\n\n            # Create tasks for all test cases\n            tasks = [\n                self.execute_function_test(module_name, function_name, function_data, test_kwargs, profiler)\n                for test_kwargs in test_kwargs_list\n            ]\n\n            # Execute all tests concurrently\n            results = await asyncio.gather(*tasks)\n\n            total_execution_time = 0\n            for success, result_str, test_kwargs, execution_time in results:\n                info.functions_run += 1\n                total_execution_time += execution_time\n\n                if success:\n                    info.functions_sug += 1\n                    info.calls[function_name] = [test_kwargs, result_str]\n                else:\n                    info.functions_sug += 1\n                    info.error += 1\n                    info.callse[function_name] = [test_kwargs, result_str]\n\n            info.execution_time = time.time() - start_time\n            return function_name, info\n\n    async def process_module(self, module_name: str, functions: dict,\n                             f_query: str, profiler: cProfile.Profile) -&gt; tuple[str, ModuleInfo]:\n        start_time = time.time()\n\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            async with asyncio.Semaphore(mp.cpu_count()):\n                tasks = [\n                    self.process_function(module_name, fname, fdata, profiler)\n                    for fname, fdata in functions.items()\n                    if fname.startswith(f_query)\n                ]\n\n                if not tasks:\n                    return module_name, ModuleInfo()\n\n                results = await asyncio.gather(*tasks)\n\n                # Combine results from all functions in the module\n                combined_info = ModuleInfo()\n                total_execution_time = 0\n\n                for _, info in results:\n                    combined_info.functions_run += info.functions_run\n                    combined_info.functions_fatal_error += info.functions_fatal_error\n                    combined_info.error += info.error\n                    combined_info.functions_sug += info.functions_sug\n                    combined_info.calls.update(info.calls)\n                    combined_info.callse.update(info.callse)\n                    combined_info.coverage[0] += info.coverage[0]\n                    combined_info.coverage[1] += info.coverage[1]\n                    total_execution_time += info.execution_time\n\n                combined_info.execution_time = time.time() - start_time\n                return module_name, combined_info\n\n    async def execute_all_functions(self, m_query='', f_query='', enable_profiling=True):\n        \"\"\"\n        Execute all functions with parallel processing and optional profiling.\n\n        Args:\n            m_query (str): Module name query filter\n            f_query (str): Function name query filter\n            enable_profiling (bool): Enable detailed profiling information\n        \"\"\"\n        print(\"Executing all functions in parallel\" + (\" with profiling\" if enable_profiling else \"\"))\n\n        start_time = time.time()\n        stats = ExecutionStats()\n        items = list(self.functions.items()).copy()\n\n        # Set up profiling\n        self.enable_profiling = enable_profiling\n        profiler = cProfile.Profile()\n\n        with profile_section(profiler, enable_profiling):\n            # Filter modules based on query\n            filtered_modules = [\n                (mname, mfuncs) for mname, mfuncs in items\n                if mname.startswith(m_query)\n            ]\n\n            stats.modular_run = len(filtered_modules)\n\n            # Process all modules concurrently\n            async with asyncio.Semaphore(mp.cpu_count()):\n                tasks = [\n                    self.process_module(mname, mfuncs, f_query, profiler)\n                    for mname, mfuncs in filtered_modules\n                ]\n\n                results = await asyncio.gather(*tasks)\n\n            # Combine results and calculate statistics\n            for module_name, info in results:\n                if info.functions_run == info.functions_sug:\n                    stats.modular_sug += 1\n                else:\n                    stats.modular_fatal_error += 1\n\n                stats.errors += info.error\n\n                # Calculate coverage\n                coverage = (info.coverage[1] / info.coverage[0]) if info.coverage[0] &gt; 0 else 0\n                stats.coverage.append(f\"{module_name}:{coverage:.2f}\\n\")\n\n                # Store module info\n                stats.__dict__[module_name] = info\n\n            # Calculate total coverage\n            total_coverage = (\n                sum(float(t.split(\":\")[-1]) for t in stats.coverage) / len(stats.coverage)\n                if stats.coverage else 0\n            )\n\n            stats.total_execution_time = time.time() - start_time\n\n            # Generate profiling stats if enabled\n            if enable_profiling:\n                s = io.StringIO()\n                ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n                ps.print_stats()\n                stats.profiling_data = {\n                    'detailed_stats': s.getvalue(),\n                    'total_time': stats.total_execution_time,\n                    'function_count': stats.modular_run,\n                    'successful_functions': stats.modular_sug\n                }\n\n            print(\n                f\"\\n{stats.modular_run=}\"\n                f\"\\n{stats.modular_sug=}\"\n                f\"\\n{stats.modular_fatal_error=}\"\n                f\"\\n{total_coverage=}\"\n                f\"\\nTotal execution time: {stats.total_execution_time:.2f}s\"\n            )\n\n            if enable_profiling:\n                print(\"\\nProfiling Summary:\")\n                print(f\"{'=' * 50}\")\n                print(\"Top 10 time-consuming functions:\")\n                ps.print_stats(10)\n\n            analyzed_data = analyze_data(stats.__dict__)\n            return Result.ok(data=stats.__dict__, data_info=analyzed_data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.debug","title":"<code>debug</code>  <code>property</code> <code>writable</code>","text":"<p>proxi attr</p>"},{"location":"toolboxv2/#toolboxv2.AppType.prefix","title":"<code>prefix = prefix</code>  <code>instance-attribute</code>","text":"<p>proxi attr</p>"},{"location":"toolboxv2/#toolboxv2.AppType.a_exit","title":"<code>a_exit()</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.a_fuction_runner","title":"<code>a_fuction_runner(function, function_data, args, kwargs)</code>  <code>async</code>","text":"<p>parameters = function_data.get('params') modular_name = function_data.get('module_name') function_name = function_data.get('func_name') mod_function_name = f\"{modular_name}.{function_name}\"</p> <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict):\n    \"\"\"\n    parameters = function_data.get('params')\n    modular_name = function_data.get('module_name')\n    function_name = function_data.get('func_name')\n    mod_function_name = f\"{modular_name}.{function_name}\"\n\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.a_remove_mod","title":"<code>a_remove_mod(mod_name, spec='app', delete=True)</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_remove_mod(self, mod_name, spec='app', delete=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.a_run_any","title":"<code>a_run_any(mod_function_name, backwords_compability_variabel_string_holder=None, get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                    backwords_compability_variabel_string_holder=None,\n                    get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                    kwargs_=None,\n                    *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.a_run_function","title":"<code>a_run_function(mod_function_name, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_run_function(self, mod_function_name: Enum or tuple,\n                         tb_run_function_with_state=True,\n                         tb_run_with_specification='app',\n                         args_=None,\n                         kwargs_=None,\n                         *args,\n                         **kwargs) -&gt; Result:\n\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.debug_rains","title":"<code>debug_rains(e)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def debug_rains(self, e):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.disconnect","title":"<code>disconnect(*args, **kwargs)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def disconnect(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.execute_all_functions","title":"<code>execute_all_functions(m_query='', f_query='', enable_profiling=True)</code>  <code>async</code>","text":"<p>Execute all functions with parallel processing and optional profiling.</p> <p>Parameters:</p> Name Type Description Default <code>m_query</code> <code>str</code> <p>Module name query filter</p> <code>''</code> <code>f_query</code> <code>str</code> <p>Function name query filter</p> <code>''</code> <code>enable_profiling</code> <code>bool</code> <p>Enable detailed profiling information</p> <code>True</code> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def execute_all_functions(self, m_query='', f_query='', enable_profiling=True):\n    \"\"\"\n    Execute all functions with parallel processing and optional profiling.\n\n    Args:\n        m_query (str): Module name query filter\n        f_query (str): Function name query filter\n        enable_profiling (bool): Enable detailed profiling information\n    \"\"\"\n    print(\"Executing all functions in parallel\" + (\" with profiling\" if enable_profiling else \"\"))\n\n    start_time = time.time()\n    stats = ExecutionStats()\n    items = list(self.functions.items()).copy()\n\n    # Set up profiling\n    self.enable_profiling = enable_profiling\n    profiler = cProfile.Profile()\n\n    with profile_section(profiler, enable_profiling):\n        # Filter modules based on query\n        filtered_modules = [\n            (mname, mfuncs) for mname, mfuncs in items\n            if mname.startswith(m_query)\n        ]\n\n        stats.modular_run = len(filtered_modules)\n\n        # Process all modules concurrently\n        async with asyncio.Semaphore(mp.cpu_count()):\n            tasks = [\n                self.process_module(mname, mfuncs, f_query, profiler)\n                for mname, mfuncs in filtered_modules\n            ]\n\n            results = await asyncio.gather(*tasks)\n\n        # Combine results and calculate statistics\n        for module_name, info in results:\n            if info.functions_run == info.functions_sug:\n                stats.modular_sug += 1\n            else:\n                stats.modular_fatal_error += 1\n\n            stats.errors += info.error\n\n            # Calculate coverage\n            coverage = (info.coverage[1] / info.coverage[0]) if info.coverage[0] &gt; 0 else 0\n            stats.coverage.append(f\"{module_name}:{coverage:.2f}\\n\")\n\n            # Store module info\n            stats.__dict__[module_name] = info\n\n        # Calculate total coverage\n        total_coverage = (\n            sum(float(t.split(\":\")[-1]) for t in stats.coverage) / len(stats.coverage)\n            if stats.coverage else 0\n        )\n\n        stats.total_execution_time = time.time() - start_time\n\n        # Generate profiling stats if enabled\n        if enable_profiling:\n            s = io.StringIO()\n            ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n            ps.print_stats()\n            stats.profiling_data = {\n                'detailed_stats': s.getvalue(),\n                'total_time': stats.total_execution_time,\n                'function_count': stats.modular_run,\n                'successful_functions': stats.modular_sug\n            }\n\n        print(\n            f\"\\n{stats.modular_run=}\"\n            f\"\\n{stats.modular_sug=}\"\n            f\"\\n{stats.modular_fatal_error=}\"\n            f\"\\n{total_coverage=}\"\n            f\"\\nTotal execution time: {stats.total_execution_time:.2f}s\"\n        )\n\n        if enable_profiling:\n            print(\"\\nProfiling Summary:\")\n            print(f\"{'=' * 50}\")\n            print(\"Top 10 time-consuming functions:\")\n            ps.print_stats(10)\n\n        analyzed_data = analyze_data(stats.__dict__)\n        return Result.ok(data=stats.__dict__, data_info=analyzed_data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.exit","title":"<code>exit()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.exit_main","title":"<code>exit_main(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef exit_main(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.fuction_runner","title":"<code>fuction_runner(function, function_data, args, kwargs, t0=0.0)</code>","text":"<p>parameters = function_data.get('params') modular_name = function_data.get('module_name') function_name = function_data.get('func_name') mod_function_name = f\"{modular_name}.{function_name}\"</p> <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n    \"\"\"\n    parameters = function_data.get('params')\n    modular_name = function_data.get('module_name')\n    function_name = function_data.get('func_name')\n    mod_function_name = f\"{modular_name}.{function_name}\"\n\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.get_all_mods","title":"<code>get_all_mods(working_dir='mods', path_to='./runtime')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.get_autocompletion_dict","title":"<code>get_autocompletion_dict()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_autocompletion_dict(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.get_function","title":"<code>get_function(name, **kwargs)</code>","text":"<p>Kwargs for _get_function     metadata:: return the registered function dictionary         stateless: (function_data, None), 0         stateful: (function_data, higher_order_function), 0     state::boolean         specification::str default app</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_function(self, name: Enum or tuple, **kwargs):\n    \"\"\"\n    Kwargs for _get_function\n        metadata:: return the registered function dictionary\n            stateless: (function_data, None), 0\n            stateful: (function_data, higher_order_function), 0\n        state::boolean\n            specification::str default app\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.get_mod","title":"<code>get_mod(name, spec='app')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.get_username","title":"<code>get_username(get_input=False, default='loot')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.hide_console","title":"<code>hide_console(*args, **kwargs)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def hide_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.inplace_load_instance","title":"<code>inplace_load_instance(mod_name, loc='toolboxv2.mods.', spec='app', save=True)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.load_all_mods_in_file","title":"<code>load_all_mods_in_file(working_dir='mods')</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def load_all_mods_in_file(self, working_dir=\"mods\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.load_mod","title":"<code>load_mod(mod_name, mlm='I', **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def load_mod(self, mod_name: str, mlm='I', **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.mod_online","title":"<code>mod_online(mod_name, installed=False)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def mod_online(self, mod_name, installed=False):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.print","title":"<code>print(text, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef print(text, *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.print_ok","title":"<code>print_ok()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def print_ok(self):\n    \"\"\"proxi attr\"\"\"\n    self.logger.info(\"OK\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.reload_mod","title":"<code>reload_mod(mod_name, spec='app', is_file=True, loc='toolboxv2.mods.')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.remove_mod","title":"<code>remove_mod(mod_name, spec='app', delete=True)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def remove_mod(self, mod_name, spec='app', delete=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.rrun_flows","title":"<code>rrun_flows(name, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def rrun_flows(self, name, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_a_from_sync","title":"<code>run_a_from_sync(function, *args)</code>","text":"<p>run a async fuction</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_a_from_sync(self, function, *args):\n    \"\"\"\n    run a async fuction\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_any","title":"<code>run_any(mod_function_name, backwords_compability_variabel_string_holder=None, get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n            get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n            kwargs_=None,\n            *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_flows","title":"<code>run_flows(name, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_flows(self, name, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_function","title":"<code>run_function(mod_function_name, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_function(self, mod_function_name: Enum or tuple,\n                 tb_run_function_with_state=True,\n                 tb_run_with_specification='app',\n                 args_=None,\n                 kwargs_=None,\n                 *args,\n                 **kwargs) -&gt; Result:\n\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_http","title":"<code>run_http(mod_function_name, function_name=None, method='GET', args_=None, kwargs_=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>run a function remote via http / https</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None, method=\"GET\",\n                   args_=None,\n                   kwargs_=None,\n                   *args, **kwargs):\n    \"\"\"run a function remote via http / https\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.save_autocompletion_dict","title":"<code>save_autocompletion_dict()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_autocompletion_dict(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.save_exit","title":"<code>save_exit()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.save_initialized_module","title":"<code>save_initialized_module(tools_class, spec)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_initialized_module(self, tools_class, spec):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.save_instance","title":"<code>save_instance(instance, modular_id, spec='app', instance_type='file/application', tools_class=None)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.save_load","title":"<code>save_load(modname, spec='app')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_load(self, modname, spec='app'):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.save_registry_as_enums","title":"<code>save_registry_as_enums(directory, filename)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_registry_as_enums(self, directory: str, filename: str):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.set_flows","title":"<code>set_flows(r)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def set_flows(self, r):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.set_logger","title":"<code>set_logger(debug=False)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def set_logger(self, debug=False):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.show_console","title":"<code>show_console(*args, **kwargs)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def show_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.sprint","title":"<code>sprint(text, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef sprint(text, *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.tb","title":"<code>tb(name=None, mod_name='', helper='', version=None, test=True, restrict_in_virtual_mode=False, api=False, initial=False, exit_f=False, test_only=False, memory_cache=False, file_cache=False, row=False, request_as_kwarg=False, state=None, level=0, memory_cache_max_size=100, memory_cache_ttl=300, samples=None, interface=None, pre_compute=None, post_compute=None, api_methods=None)</code>","text":"<p>A decorator for registering and configuring functions within a module.</p> <p>This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to register the function under. Defaults to the function's own name.</p> <code>None</code> <code>mod_name</code> <code>str</code> <p>The name of the module the function belongs to.</p> <code>''</code> <code>helper</code> <code>str</code> <p>A helper string providing additional information about the function.</p> <code>''</code> <code>version</code> <code>str or None</code> <p>The version of the function or module.</p> <code>None</code> <code>test</code> <code>bool</code> <p>Flag to indicate if the function is for testing purposes.</p> <code>True</code> <code>restrict_in_virtual_mode</code> <code>bool</code> <p>Flag to restrict the function in virtual mode.</p> <code>False</code> <code>api</code> <code>bool</code> <p>Flag to indicate if the function is part of an API.</p> <code>False</code> <code>initial</code> <code>bool</code> <p>Flag to indicate if the function should be executed at initialization.</p> <code>False</code> <code>exit_f</code> <code>bool</code> <p>Flag to indicate if the function should be executed at exit.</p> <code>False</code> <code>test_only</code> <code>bool</code> <p>Flag to indicate if the function should only be used for testing.</p> <code>False</code> <code>memory_cache</code> <code>bool</code> <p>Flag to enable memory caching for the function.</p> <code>False</code> <code>request_as_kwarg</code> <code>bool</code> <p>Flag to get request if the fuction is calld from api.</p> <code>False</code> <code>file_cache</code> <code>bool</code> <p>Flag to enable file caching for the function.</p> <code>False</code> <code>row</code> <code>bool</code> <p>rather to auto wrap the result in Result type default False means no row data aka result type</p> <code>False</code> <code>state</code> <code>bool or None</code> <p>Flag to indicate if the function maintains state.</p> <code>None</code> <code>level</code> <code>int</code> <p>The level of the function, used for prioritization or categorization.</p> <code>0</code> <code>memory_cache_max_size</code> <code>int</code> <p>Maximum size of the memory cache.</p> <code>100</code> <code>memory_cache_ttl</code> <code>int</code> <p>Time-to-live for the memory cache entries.</p> <code>300</code> <code>samples</code> <code>list or dict or None</code> <p>Samples or examples of function usage.</p> <code>None</code> <code>interface</code> <code>str</code> <p>The interface type for the function.</p> <code>None</code> <code>pre_compute</code> <code>callable</code> <p>A function to be called before the main function.</p> <code>None</code> <code>post_compute</code> <code>callable</code> <p>A function to be called after the main function.</p> <code>None</code> <code>api_methods</code> <code>list[str]</code> <p>default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>function</code> <p>The decorated function with additional processing and registration capabilities.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def tb(self, name=None,\n       mod_name: str = \"\",\n       helper: str = \"\",\n       version: str or None = None,\n       test: bool = True,\n       restrict_in_virtual_mode: bool = False,\n       api: bool = False,\n       initial: bool = False,\n       exit_f: bool = False,\n       test_only: bool = False,\n       memory_cache: bool = False,\n       file_cache: bool = False,\n       row=False,\n       request_as_kwarg: bool = False,\n       state: bool or None = None,\n       level: int = 0,\n       memory_cache_max_size: int = 100,\n       memory_cache_ttl: int = 300,\n       samples: list or dict or None = None,\n       interface: ToolBoxInterfaces or None or str = None,\n       pre_compute=None,\n       post_compute=None,\n       api_methods=None,\n       ):\n    \"\"\"\nA decorator for registering and configuring functions within a module.\n\nThis decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\nArgs:\n    name (str, optional): The name to register the function under. Defaults to the function's own name.\n    mod_name (str, optional): The name of the module the function belongs to.\n    helper (str, optional): A helper string providing additional information about the function.\n    version (str or None, optional): The version of the function or module.\n    test (bool, optional): Flag to indicate if the function is for testing purposes.\n    restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n    api (bool, optional): Flag to indicate if the function is part of an API.\n    initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n    exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n    test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n    memory_cache (bool, optional): Flag to enable memory caching for the function.\n    request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n    file_cache (bool, optional): Flag to enable file caching for the function.\n    row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n    state (bool or None, optional): Flag to indicate if the function maintains state.\n    level (int, optional): The level of the function, used for prioritization or categorization.\n    memory_cache_max_size (int, optional): Maximum size of the memory cache.\n    memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n    samples (list or dict or None, optional): Samples or examples of function usage.\n    interface (str, optional): The interface type for the function.\n    pre_compute (callable, optional): A function to be called before the main function.\n    post_compute (callable, optional): A function to be called after the main function.\n    api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\nReturns:\n    function: The decorated function with additional processing and registration capabilities.\n\"\"\"\n    if interface is None:\n        interface = \"tb\"\n    if test_only and 'test' not in self.id:\n        return lambda *args, **kwargs: args\n    return self._create_decorator(interface,\n                                  name,\n                                  mod_name,\n                                  level=level,\n                                  restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                  helper=helper,\n                                  api=api,\n                                  version=version,\n                                  initial=initial,\n                                  exit_f=exit_f,\n                                  test=test,\n                                  samples=samples,\n                                  state=state,\n                                  pre_compute=pre_compute,\n                                  post_compute=post_compute,\n                                  memory_cache=memory_cache,\n                                  file_cache=file_cache,\n                                  row=row,\n                                  request_as_kwarg=request_as_kwarg,\n                                  memory_cache_max_size=memory_cache_max_size,\n                                  memory_cache_ttl=memory_cache_ttl)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.watch_mod","title":"<code>watch_mod(mod_name, spec='app', loc='toolboxv2.mods.', use_thread=True, path_name=None)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.web_context","title":"<code>web_context()</code>","text":"<p>returns the build index ( toolbox web component )</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def web_context(self) -&gt; str:\n    \"\"\"returns the build index ( toolbox web component )\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.MainTool","title":"<code>toolboxv2.MainTool</code>","text":"Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>class MainTool:\n    toolID: str = \"\"\n    # app = None\n    interface = None\n    spec = \"app\"\n    name = \"\"\n    color = \"Bold\"\n    stuf = False\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.__storedargs = args, kwargs\n        self.async_initialized = False\n\n    async def __ainit__(self, *args, **kwargs):\n        self.version = kwargs[\"v\"]\n        self.tools = kwargs.get(\"tool\", {})\n        self.name = kwargs[\"name\"]\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n        if not hasattr(self, 'config'):\n            self.config = {}\n        self.user = None\n        self.description = \"A toolbox mod\" if kwargs.get(\"description\") is None else kwargs.get(\"description\")\n        if MainTool.interface is None:\n            MainTool.interface = self.app.interface_type\n        # Result.default(self.app.interface)\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    await self.todo()\n                else:\n                    self.todo()\n                await asyncio.sleep(0.1)\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\" Error loading mod {self.name} {e}\")\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n\n        self.app.print(f\"TOOL : {self.spec}.{self.name} online\")\n\n    @property\n    def app(self):\n        return get_app(\n            from_=f\"{self.spec}.{self.name}|{self.toolID if self.toolID else '*' + MainTool.toolID} {self.interface if self.interface else MainTool.interface}\")\n\n    @app.setter\n    def app(self, v):\n        raise PermissionError(f\"You cannot set the App Instance! {v=}\")\n\n    @staticmethod\n    def return_result(error: ToolBoxError = ToolBoxError.none,\n                      exec_code: int = 0,\n                      help_text: str = \"\",\n                      data_info=None,\n                      data=None,\n                      data_to=None):\n\n        if data_to is None:\n            data_to = MainTool.interface if MainTool.interface is not None else ToolBoxInterfaces.cli\n\n        if data is None:\n            data = {}\n\n        if data_info is None:\n            data_info = {}\n\n        return Result(\n            error,\n            ToolBoxResult(data_info=data_info, data=data, data_to=data_to),\n            ToolBoxInfo(exec_code=exec_code, help_text=help_text)\n        )\n\n    def print(self, message, end=\"\\n\", **kwargs):\n        if self.stuf:\n            return\n\n        self.app.print(Style.style_dic[self.color] + self.name + Style.style_dic[\"END\"] + \":\", message, end=end,\n                       **kwargs)\n\n    def add_str_to_config(self, command):\n        if len(command) != 2:\n            self.logger.error('Invalid command must be key value')\n            return False\n        self.config[command[0]] = command[1]\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n\n    def get_version(self) -&gt; str:\n        \"\"\"\"Returns the version\"\"\"\n        return self.version\n\n    async def get_user(self, username: str) -&gt; Result:\n        return await self.app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME, username=username, get_results=True)\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.MainTool.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.__storedargs = args, kwargs\n    self.async_initialized = False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.MainTool.__initobj","title":"<code>__initobj()</code>  <code>async</code>","text":"<p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.MainTool.get_version","title":"<code>get_version()</code>","text":"<p>\"Returns the version</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\"Returns the version\"\"\"\n    return self.version\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.MainTool.webInstall","title":"<code>webInstall(user_instance, construct_render)</code>","text":"<p>\"Returns a web installer for the given user instance and construct render template</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def webInstall(self, user_instance, construct_render) -&gt; str:\n    \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.get_app","title":"<code>toolboxv2.get_app(from_=None, name=None, args=AppArgs().default(), app_con=None, sync=False)</code>","text":"Source code in <code>toolboxv2/utils/system/getting_and_closing_app.py</code> <pre><code>def get_app(from_=None, name=None, args=AppArgs().default(), app_con=None, sync=False) -&gt; AppType:\n    global registered_apps\n    # name = None\n    # print(f\"get app requested from: {from_} withe name: {name}\")\n    logger = get_logger()\n    logger.info(Style.GREYBG(f\"get app requested from: {from_}\"))\n    if registered_apps[0] is not None:\n        return registered_apps[0]\n\n    if app_con is None:\n        from ... import App\n        app_con = App\n    app = app_con(name, args=args) if name else app_con()\n    logger.info(Style.Bold(f\"App instance, returned ID: {app.id}\"))\n\n    registered_apps[0] = app\n    return app\n</code></pre>"},{"location":"toolboxv2/#system-utilities-configuration","title":"System Utilities &amp; Configuration","text":""},{"location":"toolboxv2/#toolboxv2.FileHandler","title":"<code>toolboxv2.FileHandler</code>","text":"<p>               Bases: <code>Code</code></p> Source code in <code>toolboxv2/utils/system/file_handler.py</code> <pre><code>class FileHandler(Code):\n\n    def __init__(self, filename, name='mainTool', keys=None, defaults=None):\n        if defaults is None:\n            defaults = {}\n        if keys is None:\n            keys = {}\n        assert filename.endswith(\".config\") or filename.endswith(\".data\"), \\\n            f\"filename must end with .config or .data {filename=}\"\n        self.file_handler_save = {}\n        self.file_handler_load = {}\n        self.file_handler_key_mapper = {}\n        self.file_handler_filename = filename\n        self.file_handler_storage = None\n        self.file_handler_max_loaded_index_ = 0\n        self.file_handler_file_prefix = (f\".{filename.split('.')[1]}/\"\n                                         f\"{name.replace('.', '-')}/\")\n        # self.load_file_handler()\n        self.set_defaults_keys_file_handler(keys, defaults)\n\n    def _open_file_handler(self, mode: str, rdu):\n        logger = get_logger()\n        logger.info(Style.Bold(Style.YELLOW(f\"Opening file in mode : {mode}\")))\n        if self.file_handler_storage:\n            self.file_handler_storage.close()\n            self.file_handler_storage = None\n        try:\n            self.file_handler_storage = open(self.file_handler_file_prefix + self.file_handler_filename, mode)\n            self.file_handler_max_loaded_index_ += 1\n        except FileNotFoundError:\n            if self.file_handler_max_loaded_index_ == 2:\n                os.makedirs(self.file_handler_file_prefix, exist_ok=True)\n            if self.file_handler_max_loaded_index_ == 3:\n                os.makedirs(\".config/mainTool\", exist_ok=True)\n            if self.file_handler_max_loaded_index_ &gt;= 5:\n                print(Style.RED(f\"pleas create this file to prosed : {self.file_handler_file_prefix}\"\n                                f\"{self.file_handler_filename}\"))\n                logger.critical(f\"{self.file_handler_file_prefix} {self.file_handler_filename} FileNotFoundError cannot\"\n                                f\" be Created\")\n                exit(0)\n            self.file_handler_max_loaded_index_ += 1\n            logger.info(Style.YELLOW(f\"Try Creating File: {self.file_handler_file_prefix}{self.file_handler_filename}\"))\n\n            if not os.path.exists(f\"{self.file_handler_file_prefix}\"):\n                os.makedirs(f\"{self.file_handler_file_prefix}\")\n\n            with open(self.file_handler_file_prefix + self.file_handler_filename, 'a'):\n                logger.info(Style.GREEN(\"File created successfully\"))\n                self.file_handler_max_loaded_index_ = -1\n            rdu()\n        except OSError and PermissionError as e:\n            raise e\n\n    def open_s_file_handler(self):\n        self._open_file_handler('w+', self.open_s_file_handler)\n        return self\n\n    def open_l_file_handler(self):\n        self._open_file_handler('r+', self.open_l_file_handler)\n        return self\n\n    def save_file_handler(self):\n        get_logger().info(\n            Style.BLUE(\n                f\"init Saving (S) {self.file_handler_filename} \"\n            )\n        )\n        if self.file_handler_storage:\n            get_logger().warning(\n                f\"WARNING file is already open (S): {self.file_handler_filename} {self.file_handler_storage}\")\n\n        self.open_s_file_handler()\n\n        get_logger().info(\n            Style.BLUE(\n                f\"Elements to save : ({len(self.file_handler_save.keys())})\"\n            )\n        )\n\n        self.file_handler_storage.write(json.dumps(self.file_handler_save))\n\n        self.file_handler_storage.close()\n        self.file_handler_storage = None\n\n        get_logger().info(\n            Style.BLUE(\n                f\"closing file : {self.file_handler_filename} \"\n            )\n        )\n\n        return self\n\n    def add_to_save_file_handler(self, key: str, value: str):\n        if len(key) != 10:\n            get_logger(). \\\n                warning(\n                Style.YELLOW(\n                    'WARNING: key length is not 10 characters'\n                )\n            )\n            return False\n        if key not in self.file_handler_load:\n            if key in self.file_handler_key_mapper:\n                key = self.file_handler_key_mapper[key]\n\n        self.file_handler_load[key] = value\n        self.file_handler_save[key] = self.encode_code(value)\n        return True\n\n    def remove_key_file_handler(self, key: str):\n        if key == 'Pka7237327':\n            print(\"Cant remove Root Key\")\n            return\n        if key in self.file_handler_load:\n            del self.file_handler_load[key]\n        if key in self.file_handler_save:\n            del self.file_handler_save[key]\n\n    def load_file_handler(self):\n        get_logger().info(\n            Style.BLUE(\n                f\"loading {self.file_handler_filename} \"\n            )\n        )\n        if self.file_handler_storage:\n            get_logger().warning(\n                Style.YELLOW(\n                    f\"WARNING file is already open (L) {self.file_handler_filename}\"\n                )\n            )\n        self.open_l_file_handler()\n\n        try:\n\n            self.file_handler_save = json.load(self.file_handler_storage)\n            for key, line in self.file_handler_save.items():\n                self.file_handler_load[key] = self.decode_code(line)\n\n        except json.decoder.JSONDecodeError and Exception:\n\n            for line in self.file_handler_storage:\n                line = line[:-1]\n                heda = line[:10]\n                self.file_handler_save[heda] = line[10:]\n                enc = self.decode_code(line[10:])\n                self.file_handler_load[heda] = enc\n\n            self.file_handler_save = {}\n\n        self.file_handler_storage.close()\n        self.file_handler_storage = None\n\n        return self\n\n    def get_file_handler(self, obj: str, default=None) -&gt; str or None:\n        logger = get_logger()\n        if obj not in self.file_handler_load:\n            if obj in self.file_handler_key_mapper:\n                obj = self.file_handler_key_mapper[obj]\n        logger.info(Style.ITALIC(Style.GREY(f\"Collecting data from storage key : {obj}\")))\n        self.file_handler_max_loaded_index_ = -1\n        for objects in self.file_handler_load.items():\n            self.file_handler_max_loaded_index_ += 1\n            if obj == objects[0]:\n\n                try:\n                    if len(objects[1]) &gt; 0:\n                        return ast.literal_eval(objects[1]) if isinstance(objects[1], str) else objects[1]\n                    logger.warning(\n                        Style.YELLOW(\n                            f\"No data  {obj}  ; {self.file_handler_filename}\"\n                        )\n                    )\n                except ValueError:\n                    logger.error(f\"ValueError Loading {obj} ; {self.file_handler_filename}\")\n                except SyntaxError:\n                    if isinstance(objects[1], str):\n                        return objects[1]\n                    logger.warning(\n                        Style.YELLOW(\n                            f\"Possible SyntaxError Loading {obj} ; {self.file_handler_filename}\"\n                            f\" {len(objects[1])} {type(objects[1])}\"\n                        )\n                    )\n                    return objects[1]\n                except NameError:\n                    return str(objects[1])\n\n        if obj in list(self.file_handler_save.keys()):\n            r = self.decode_code(self.file_handler_save[obj])\n            logger.info(f\"returning Default for {obj}\")\n            return r\n\n        if default is None:\n            default = self.file_handler_load.get(obj)\n\n        logger.info(\"no data found\")\n        return default\n\n    def set_defaults_keys_file_handler(self, keys: dict, defaults: dict):\n        list_keys = iter(list(keys.keys()))\n        df_keys = defaults.keys()\n        for key in list_keys:\n            self.file_handler_key_mapper[key] = keys[key]\n            self.file_handler_key_mapper[keys[key]] = key\n            if key in df_keys:\n                self.file_handler_load[keys[key]] = str(defaults[key])\n                self.file_handler_save[keys[key]] = self.encode_code(defaults[key])\n            else:\n                self.file_handler_load[keys[key]] = \"None\"\n\n    def delete_file(self):\n        os.remove(self.file_handler_file_prefix + self.file_handler_filename)\n        get_logger().warning(Style.GREEN(f\"File deleted {self.file_handler_file_prefix + self.file_handler_filename}\"))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils","title":"<code>toolboxv2.utils</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.App","title":"<code>App</code>","text":"Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>class App(AppType, metaclass=Singleton):\n\n    def __init__(self, prefix: str = \"\", args=AppArgs().default()):\n        super().__init__(prefix, args)\n        self._web_context = None\n        t0 = time.perf_counter()\n        abspath = os.path.abspath(__file__)\n        self.system_flag = system()  # Linux: Linux Mac: Darwin Windows: Windows\n\n        self.appdata = os.getenv('APPDATA') if os.name == 'nt' else os.getenv('XDG_CONFIG_HOME') or os.path.expanduser(\n                '~/.config') if os.name == 'posix' else None\n\n        if self.system_flag == \"Darwin\" or self.system_flag == \"Linux\":\n            dir_name = os.path.dirname(abspath).replace(\"/utils\", \"\")\n        else:\n            dir_name = os.path.dirname(abspath).replace(\"\\\\utils\", \"\")\n\n        self.start_dir = str(dir_name)\n\n        self.bg_tasks = []\n\n        lapp = dir_name + '\\\\.data\\\\'\n\n        if not prefix:\n            if not os.path.exists(f\"{lapp}last-app-prefix.txt\"):\n                os.makedirs(lapp, exist_ok=True)\n                open(f\"{lapp}last-app-prefix.txt\", \"a\").close()\n            with open(f\"{lapp}last-app-prefix.txt\") as prefix_file:\n                cont = prefix_file.read()\n                if cont:\n                    prefix = cont.rstrip()\n        else:\n            if not os.path.exists(f\"{lapp}last-app-prefix.txt\"):\n                os.makedirs(lapp, exist_ok=True)\n                open(f\"{lapp}last-app-prefix.txt\", \"a\").close()\n            with open(f\"{lapp}last-app-prefix.txt\", \"w\") as prefix_file:\n                prefix_file.write(prefix)\n\n        self.prefix = prefix\n\n        node_ = node()\n\n        if 'localhost' in node_ and (host := os.getenv('HOSTNAME', 'localhost')) != 'localhost':\n            node_ = node_.replace('localhost', host)\n        self.id = prefix + '-' + node_\n        self.globals = {\n            \"root\": {**globals()},\n        }\n        self.locals = {\n            \"user\": {'app': self, **locals()},\n        }\n\n        identification = self.id\n\n        if \"test\" in prefix:\n            if self.system_flag == \"Darwin\" or self.system_flag == \"Linux\":\n                start_dir = self.start_dir.replace(\"ToolBoxV2/toolboxv2\", \"toolboxv2\")\n            else:\n                start_dir = self.start_dir.replace(\"ToolBoxV2\\\\toolboxv2\", \"toolboxv2\")\n            self.data_dir = start_dir + '\\\\.data\\\\' + \"test\"\n            self.config_dir = start_dir + '\\\\.config\\\\' + \"test\"\n            self.info_dir = start_dir + '\\\\.info\\\\' + \"test\"\n        else:\n            self.data_dir = self.start_dir + '\\\\.data\\\\' + identification\n            self.config_dir = self.start_dir + '\\\\.config\\\\' + identification\n            self.info_dir = self.start_dir + '\\\\.info\\\\' + identification\n\n        if self.appdata is None:\n            self.appdata = self.data_dir\n        else:\n            self.appdata += \"/ToolBoxV2\"\n\n        if not os.path.exists(self.appdata):\n            os.makedirs(self.appdata, exist_ok=True)\n        if not os.path.exists(self.data_dir):\n            os.makedirs(self.data_dir, exist_ok=True)\n        if not os.path.exists(self.config_dir):\n            os.makedirs(self.config_dir, exist_ok=True)\n        if not os.path.exists(self.info_dir):\n            os.makedirs(self.info_dir, exist_ok=True)\n\n        print(f\"Starting ToolBox as {prefix} from :\", Style.Bold(Style.CYAN(f\"{os.getcwd()}\")))\n\n        logger_info_str, self.logger, self.logging_filename = self.set_logger(args.debug)\n\n        print(\"Logger \" + logger_info_str)\n        print(\"================================\")\n        self.logger.info(\"Logger initialized\")\n        get_logger().info(Style.GREEN(\"Starting Application instance\"))\n        if args.init and args.init is not None and self.start_dir not in sys.path:\n            sys.path.append(self.start_dir)\n\n\n        __version__ = get_version_from_pyproject()\n\n        self.version = __version__\n\n        self.keys = {\n            \"MACRO\": \"macro~~~~:\",\n            \"MACRO_C\": \"m_color~~:\",\n            \"HELPER\": \"helper~~~:\",\n            \"debug\": \"debug~~~~:\",\n            \"id\": \"name-spa~:\",\n            \"st-load\": \"mute~load:\",\n            \"comm-his\": \"comm-his~:\",\n            \"develop-mode\": \"dev~mode~:\",\n            \"provider::\": \"provider::\",\n        }\n\n        defaults = {\n            \"MACRO\": ['Exit'],\n            \"MACRO_C\": {},\n            \"HELPER\": {},\n            \"debug\": args.debug,\n            \"id\": self.id,\n            \"st-load\": False,\n            \"comm-his\": [[]],\n            \"develop-mode\": False,\n        }\n        self.config_fh = FileHandler(self.id + \".config\", keys=self.keys, defaults=defaults)\n        self.config_fh.load_file_handler()\n        self._debug = args.debug\n        self.flows = {}\n        self.dev_modi = self.config_fh.get_file_handler(self.keys[\"develop-mode\"])\n        if self.config_fh.get_file_handler(\"provider::\") is None:\n            self.config_fh.add_to_save_file_handler(\"provider::\", \"http://localhost:\" + str(\n                self.args_sto.port) if os.environ.get(\"HOSTNAME\",\n                                                                     \"localhost\") == \"localhost\" else \"https://simplecore.app\")\n        self.functions = {}\n        self.modules = {}\n\n        self.interface_type = ToolBoxInterfaces.native\n        self.PREFIX = Style.CYAN(f\"~{node()}@&gt;\")\n        self.alive = True\n        self.called_exit = False, time.time()\n\n        self.print(f\"Infos:\\n  {'Name':&lt;8} -&gt; {node()}\\n  {'ID':&lt;8} -&gt; {self.id}\\n  {'Version':&lt;8} -&gt; {self.version}\\n\")\n\n        self.logger.info(\n            Style.GREEN(\n                f\"Finish init up in {time.perf_counter() - t0:.2f}s\"\n            )\n        )\n\n        self.args_sto = args\n        self.loop = None\n\n        from .system.session import Session\n        self.session: Session = Session(self.get_username())\n\n    def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n        user_name = self.config_fh.get_file_handler(\"ac_user:::\")\n        if get_input and user_name is None:\n            user_name = input(\"Input your username: \")\n            self.config_fh.add_to_save_file_handler(\"ac_user:::\", user_name)\n        if user_name is None:\n            user_name = default\n            self.config_fh.add_to_save_file_handler(\"ac_user:::\", user_name)\n        return user_name\n\n    def set_username(self, username):\n        return self.config_fh.add_to_save_file_handler(\"ac_user:::\", username)\n\n    @staticmethod\n    def exit_main(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def hide_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def show_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def disconnect(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def set_logger(self, debug=False):\n        if \"test\" in self.prefix and not debug:\n            logger, logging_filename = setup_logging(logging.NOTSET, name=\"toolbox-test\", interminal=True,\n                                                     file_level=logging.NOTSET, app_name=self.id)\n            logger_info_str = \"in Test Mode\"\n        elif \"live\" in self.prefix and not debug:\n            logger, logging_filename = setup_logging(logging.DEBUG, name=\"toolbox-live\", interminal=False,\n                                                     file_level=logging.WARNING, app_name=self.id)\n            logger_info_str = \"in Live Mode\"\n            # setup_logging(logging.WARNING, name=\"toolbox-live\", is_online=True\n            #              , online_level=logging.WARNING).info(\"Logger initialized\")\n        elif \"debug\" in self.prefix or self.prefix.endswith(\"D\"):\n            self.prefix = self.prefix.replace(\"-debug\", '').replace(\"debug\", '')\n            logger, logging_filename = setup_logging(logging.DEBUG, name=\"toolbox-debug\", interminal=True,\n                                                     file_level=logging.WARNING, app_name=self.id)\n            logger_info_str = \"in debug Mode\"\n            self.debug = True\n        elif debug:\n            logger, logging_filename = setup_logging(logging.DEBUG, name=f\"toolbox-{self.prefix}-debug\",\n                                                     interminal=True,\n                                                     file_level=logging.DEBUG, app_name=self.id)\n            logger_info_str = \"in args debug Mode\"\n        else:\n            logger, logging_filename = setup_logging(logging.ERROR, name=f\"toolbox-{self.prefix}\", app_name=self.id)\n            logger_info_str = \"in Default\"\n\n        return logger_info_str, logger, logging_filename\n\n    @property\n    def debug(self):\n        return self._debug\n\n    @debug.setter\n    def debug(self, value):\n        if not isinstance(value, bool):\n            self.logger.debug(f\"Value must be an boolean. is : {value} type of {type(value)}\")\n            raise ValueError(\"Value must be an boolean.\")\n\n        # self.logger.info(f\"Setting debug {value}\")\n        self._debug = value\n\n    def debug_rains(self, e):\n        if self.debug:\n            import traceback\n            print(traceback.format_exc())\n            raise e\n\n    def set_flows(self, r):\n        self.flows = r\n\n    async def run_flows(self, name, **kwargs):\n        from ..flows import flows_dict as flows_dict_func\n        if name not in self.flows:\n            self.flows = {**self.flows, **flows_dict_func(s=name, remote=True)}\n        if name in self.flows:\n            if asyncio.iscoroutinefunction(self.flows[name]):\n                return await self.flows[name](get_app(from_=\"runner\"), self.args_sto, **kwargs)\n            else:\n                return self.flows[name](get_app(from_=\"runner\"), self.args_sto, **kwargs)\n        else:\n            print(\"Flow not found, active flows:\", len(self.flows.keys()))\n\n    def _coppy_mod(self, content, new_mod_dir, mod_name, file_type='py'):\n\n        mode = 'xb'\n        self.logger.info(f\" coppy mod {mod_name} to {new_mod_dir} size : {sys.getsizeof(content) / 8388608:.3f} mb\")\n\n        if not os.path.exists(new_mod_dir):\n            os.makedirs(new_mod_dir)\n            with open(f\"{new_mod_dir}/__init__.py\", \"w\") as nmd:\n                nmd.write(f\"__version__ = '{self.version}'\")\n\n        if os.path.exists(f\"{new_mod_dir}/{mod_name}.{file_type}\"):\n            mode = False\n\n            with open(f\"{new_mod_dir}/{mod_name}.{file_type}\", 'rb') as d:\n                runtime_mod = d.read()  # Testing version but not efficient\n\n            if len(content) != len(runtime_mod):\n                mode = 'wb'\n\n        if mode:\n            with open(f\"{new_mod_dir}/{mod_name}.{file_type}\", mode) as f:\n                f.write(content)\n\n    def _pre_lib_mod(self, mod_name, path_to=\"./runtime\", file_type='py'):\n        working_dir = self.id.replace(\".\", \"_\")\n        lib_mod_dir = f\"toolboxv2.runtime.{working_dir}.mod_lib.\"\n\n        self.logger.info(f\"pre_lib_mod {mod_name} from {lib_mod_dir}\")\n\n        postfix = \"_dev\" if self.dev_modi else \"\"\n        mod_file_dir = f\"./mods{postfix}/{mod_name}.{file_type}\"\n        new_mod_dir = f\"{path_to}/{working_dir}/mod_lib\"\n        with open(mod_file_dir, \"rb\") as c:\n            content = c.read()\n        self._coppy_mod(content, new_mod_dir, mod_name, file_type=file_type)\n        return lib_mod_dir\n\n    def _copy_load(self, mod_name, file_type='py', **kwargs):\n        loc = self._pre_lib_mod(mod_name, file_type)\n        return self.inplace_load_instance(mod_name, loc=loc, **kwargs)\n\n    def helper_install_pip_module(self, module_name):\n        if 'main' in self.id:\n            return\n        self.print(f\"Installing {module_name} GREEDY\")\n        os.system(f\"{sys.executable} -m pip install {module_name}\")\n\n    def python_module_import_classifier(self, mod_name, error_message):\n\n        if error_message.startswith(\"No module named 'toolboxv2.utils\"):\n            return Result.default_internal_error(f\"404 {error_message.split('utils')[1]} not found\")\n        if error_message.startswith(\"No module named 'toolboxv2.mods\"):\n            if mod_name.startswith('.'):\n                return\n            return self.run_a_from_sync(self.a_run_any, (\"CloudM\", \"install\"), module_name=mod_name)\n        if error_message.startswith(\"No module named '\"):\n            pip_requ = error_message.split(\"'\")[1].replace(\"'\", \"\").strip()\n            # if 'y' in input(f\"\\t\\t\\tAuto install {pip_requ} Y/n\").lower:\n            return self.helper_install_pip_module(pip_requ)\n            # return Result.default_internal_error(f\"404 {pip_requ} not found\")\n\n    def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True, mfo=None):\n        if self.dev_modi and loc == \"toolboxv2.mods.\":\n            loc = \"toolboxv2.mods_dev.\"\n        if self.mod_online(mod_name):\n            self.logger.info(f\"Reloading mod from : {loc + mod_name}\")\n            self.remove_mod(mod_name, spec=spec, delete=False)\n\n        if (os.path.exists(self.start_dir + '/mods/' + mod_name) or os.path.exists(\n            self.start_dir + '/mods/' + mod_name + '.py')) and (\n            os.path.isdir(self.start_dir + '/mods/' + mod_name) or os.path.isfile(\n            self.start_dir + '/mods/' + mod_name + '.py')):\n            try:\n                if mfo is None:\n                    modular_file_object = import_module(loc + mod_name)\n                else:\n                    modular_file_object = mfo\n                self.modules[mod_name] = modular_file_object\n            except ModuleNotFoundError as e:\n                self.logger.error(Style.RED(f\"module {loc + mod_name} not found is type sensitive {e}\"))\n                self.print(Style.RED(f\"module {loc + mod_name} not found is type sensitive {e}\"))\n                if self.debug or self.args_sto.sysPrint:\n                    self.python_module_import_classifier(mod_name, str(e))\n                self.debug_rains(e)\n                return None\n        else:\n            self.print(f\"module {loc + mod_name} is not valid\")\n            return None\n        if hasattr(modular_file_object, \"Tools\"):\n            tools_class = modular_file_object.Tools\n        else:\n            if hasattr(modular_file_object, \"name\"):\n                tools_class = modular_file_object\n                modular_file_object = import_module(loc + mod_name)\n            else:\n                tools_class = None\n\n        modular_id = None\n        instance = modular_file_object\n        app_instance_type = \"file/application\"\n\n        if tools_class is None:\n            modular_id = modular_file_object.Name if hasattr(modular_file_object, \"Name\") else mod_name\n\n        if tools_class is None and modular_id is None:\n            modular_id = str(modular_file_object.__name__)\n            self.logger.warning(f\"Unknown instance loaded {mod_name}\")\n            return modular_file_object\n\n        if tools_class is not None:\n            tools_class = self.save_initialized_module(tools_class, spec)\n            modular_id = tools_class.name\n            app_instance_type = \"functions/class\"\n        else:\n            instance.spec = spec\n        # if private:\n        #     self.functions[modular_id][f\"{spec}_private\"] = private\n\n        if not save:\n            return instance if tools_class is None else tools_class\n\n        return self.save_instance(instance, modular_id, spec, app_instance_type, tools_class=tools_class)\n\n    def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n\n        if modular_id in self.functions and tools_class is None:\n            if self.functions[modular_id].get(f\"{spec}_instance\", None) is None:\n                self.functions[modular_id][f\"{spec}_instance\"] = instance\n                self.functions[modular_id][f\"{spec}_instance_type\"] = instance_type\n            else:\n                self.print(\"ERROR OVERRIDE\")\n                raise ImportError(f\"Module already known {modular_id}\")\n\n        elif tools_class is not None:\n            if modular_id not in self.functions:\n                self.functions[modular_id] = {}\n            self.functions[modular_id][f\"{spec}_instance\"] = tools_class\n            self.functions[modular_id][f\"{spec}_instance_type\"] = instance_type\n\n            try:\n                if not hasattr(tools_class, 'tools'):\n                    tools_class.tools = {\"Version\": tools_class.get_version, 'name': tools_class.name}\n                for function_name in list(tools_class.tools.keys()):\n                    t_function_name = function_name.lower()\n                    if t_function_name != \"all\" and t_function_name != \"name\":\n                        self.tb(function_name, mod_name=modular_id)(tools_class.tools.get(function_name))\n                self.functions[modular_id][f\"{spec}_instance_type\"] += \"/BC\"\n            except Exception as e:\n                self.logger.error(f\"Starting Module {modular_id} compatibility failed with : {e}\")\n                pass\n        elif modular_id not in self.functions and tools_class is None:\n            self.functions[modular_id] = {}\n            self.functions[modular_id][f\"{spec}_instance\"] = instance\n            self.functions[modular_id][f\"{spec}_instance_type\"] = instance_type\n\n        else:\n            raise ImportError(f\"Modular {modular_id} is not a valid mod\")\n        on_start = self.functions[modular_id].get(\"on_start\")\n        if on_start is not None:\n            i = 1\n            for f in on_start:\n                try:\n                    f_, e = self.get_function((modular_id, f), state=True, specification=spec)\n                    if e == 0:\n                        self.logger.info(Style.GREY(f\"Running On start {f} {i}/{len(on_start)}\"))\n                        if asyncio.iscoroutinefunction(f_):\n                            self.print(f\"Async on start is only in Tool claas supported for {modular_id}.{f}\" if tools_class is None else f\"initialization starting soon for {modular_id}.{f}\")\n                        else:\n                            o = f_()\n                            if o is not None:\n                                self.print(f\"Function {modular_id} On start result: {o}\")\n                    else:\n                        self.logger.warning(f\"starting function not found {e}\")\n                except Exception as e:\n                    self.logger.debug(Style.YELLOW(\n                        Style.Bold(f\"modular:{modular_id}.{f} on_start error {i}/{len(on_start)} -&gt; {e}\")))\n                    self.debug_rains(e)\n                finally:\n                    i += 1\n        return instance if tools_class is None else tools_class\n\n    def save_initialized_module(self, tools_class, spec):\n        tools_class.spec = spec\n        live_tools_class = tools_class(app=self)\n        return live_tools_class\n\n    def mod_online(self, mod_name, installed=False):\n        if installed and mod_name not in self.functions:\n            self.save_load(mod_name)\n        return mod_name in self.functions\n\n    def _get_function(self,\n                      name: Enum or None,\n                      state: bool = True,\n                      specification: str = \"app\",\n                      metadata=False, as_str: tuple or None = None, r=0):\n\n        if as_str is None and isinstance(name, Enum):\n            modular_id = str(name.NAME.value)\n            function_id = str(name.value)\n        elif as_str is None and isinstance(name, list):\n            modular_id, function_id = name[0], name[1]\n        else:\n            modular_id, function_id = as_str\n\n        self.logger.info(f\"getting function : {specification}.{modular_id}.{function_id}\")\n\n        if modular_id not in self.functions:\n            if r == 0:\n                self.save_load(modular_id, spec=specification)\n                return self.get_function(name=(modular_id, function_id),\n                                         state=state,\n                                         specification=specification,\n                                         metadata=metadata,\n                                         r=1)\n            self.logger.warning(f\"function modular not found {modular_id} 404\")\n            return \"404\", 100\n\n        if function_id not in self.functions[modular_id]:\n            self.logger.warning(f\"function data not found {modular_id}.{function_id} 404\")\n            return \"404\", 404\n\n        function_data = self.functions[modular_id][function_id]\n\n        if isinstance(function_data, list):\n            print(f\"functions {function_id} : {function_data}\")\n            function_data = self.functions[modular_id][function_data[-1]]\n\n        function = function_data.get(\"func\")\n        params = function_data.get(\"params\")\n\n        state_ = function_data.get(\"state\")\n        if state_ is not None and state != state_:\n            state = state_\n\n        if function is None:\n            self.logger.warning(\"No function found\")\n            return \"404\", 300\n\n        if params is None:\n            self.logger.warning(\"No function (params) found\")\n            return \"404\", 301\n\n        if metadata and not state:\n            self.logger.info(\"returning metadata stateless\")\n            return (function_data, function), 0\n\n        if not state:  # mens a stateless function\n            self.logger.info(\"returning stateless function\")\n            return function, 0\n\n        instance = self.functions[modular_id].get(f\"{specification}_instance\")\n\n        # instance_type = self.functions[modular_id].get(f\"{specification}_instance_type\", \"functions/class\")\n\n        if params[0] == 'app':\n            instance = get_app(from_=f\"fuction {specification}.{modular_id}.{function_id}\")\n\n        if instance is None and self.alive:\n            self.inplace_load_instance(modular_id, spec=specification)\n            instance = self.functions[modular_id].get(f\"{specification}_instance\")\n\n        if instance is None:\n            self.logger.warning(\"No live Instance found\")\n            return \"404\", 400\n\n        # if instance_type.endswith(\"/BC\"):  # for backwards compatibility  functions/class/BC old modules\n        #     # returning as stateless\n        #     # return \"422\", -1\n        #     self.logger.info(\n        #         f\"returning stateless function, cant find tools class for state handling found {instance_type}\")\n        #     if metadata:\n        #         self.logger.info(f\"returning metadata stateless\")\n        #         return (function_data, function), 0\n        #     return function, 0\n\n        self.logger.info(\"wrapping in higher_order_function\")\n\n        self.logger.info(f\"returned fuction {specification}.{modular_id}.{function_id}\")\n        higher_order_function = partial(function, instance)\n\n        if metadata:\n            self.logger.info(\"returning metadata stateful\")\n            return (function_data, higher_order_function), 0\n\n        self.logger.info(\"returning stateful function\")\n        return higher_order_function, 0\n\n    def save_exit(self):\n        self.logger.info(f\"save exiting saving data to {self.config_fh.file_handler_filename} states of {self.debug=}\")\n        self.config_fh.add_to_save_file_handler(self.keys[\"debug\"], str(self.debug))\n\n    def init_mod(self, mod_name, spec='app'):\n        if '.' in mod_name:\n            mod_name = mod_name.split('.')[0]\n        return self.loop_gard().run_until_complete(self.a_init_mod(mod_name, spec))\n\n    def run_bg_task(self, task):\n        \"\"\"\n        Run a task in the background that will properly handle nested asyncio operations.\n        This implementation ensures that asyncio.create_task() and asyncio.gather() work\n        correctly within the background task.\n\n        Args:\n            task: A callable function that can be synchronous or asynchronous\n        \"\"\"\n        if not callable(task):\n            self.logger.warning(\"Task is not callable!\")\n            return None\n\n        # Function that will run in a separate thread with its own event loop\n        def thread_target(task_):\n            # Create a new event loop for this thread\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n\n            try:\n                # Determine how to run the task based on its type\n                if asyncio.iscoroutinefunction(task_):\n                    # If it's an async function, run it directly\n                    loop.run_until_complete(task_())\n                elif asyncio.iscoroutine(task_):\n                    # If it's already a coroutine object\n                    loop.run_until_complete(task_)\n                else:\n                    # If it's a synchronous function that might create async tasks internally\n                    async def wrapper():\n                        # Run potentially blocking synchronous code in an executor\n                        return await loop.run_in_executor(None, task_)\n\n                    loop.run_until_complete(wrapper())\n\n                self.logger.debug(\"Background task completed successfully\")\n            except Exception as e:\n                self.logger.error(f\"Background task failed with error: {str(e)}\")\n            finally:\n                # Clean up any pending tasks\n                pending = asyncio.all_tasks(loop)\n                if pending:\n                    # Cancel any remaining tasks\n                    for task_ in pending:\n                        task_.cancel()\n\n                    # Allow tasks to finish cancellation\n                    loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))\n\n                loop.close()\n\n        # Create and start a non-daemon thread that will run to completion\n        # Using non-daemon thread ensures the task completes even if main thread exits\n        t = threading.Thread(target=thread_target, args=(task,))\n        t.daemon = False  # Non-daemon thread will keep program alive until it completes\n        self.bg_tasks.append(t)\n        t.start()\n        return t\n\n    # Alternative implementation that may be needed if your function creates many nested tasks\n    def run_bg_task_advanced(self, task, *args, **kwargs):\n        \"\"\"\n        Alternative implementation for complex async scenarios where the task creates\n        nested asyncio tasks using create_task() and gather().\n\n        This version ensures proper execution of nested tasks by maintaining the thread\n        and its event loop throughout the lifetime of all child tasks.\n\n        Args:\n            task: A callable function that can be synchronous or asynchronous\n            *args, **kwargs: Arguments to pass to the task\n        \"\"\"\n        if not callable(task):\n            self.logger.warning(\"Task is not callable!\")\n            return None\n\n        # Create a dedicated thread with its own event loop\n        async def async_wrapper():\n            try:\n                if asyncio.iscoroutinefunction(task):\n                    return await task(*args, **kwargs)\n                elif asyncio.iscoroutine(task):\n                    return await task\n                else:\n                    # Run in executor to avoid blocking\n                    loop = asyncio.get_event_loop()\n                    return await loop.run_in_executor(None, lambda: task(*args, **kwargs))\n            except Exception as e:\n                self.logger.error(f\"Background task error: {str(e)}\")\n                raise\n\n        def thread_target():\n            # Create new event loop for this thread\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n\n            try:\n                # Run the task to completion with all its nested tasks\n                loop.run_until_complete(async_wrapper())\n            except Exception as e:\n                self.logger.error(f\"Background task thread failed: {str(e)}\")\n            finally:\n                # Clean up any pending tasks that might still be running\n                try:\n                    pending = asyncio.all_tasks(loop)\n                    if pending:\n                        # Allow tasks time to clean up\n                        loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))\n                except Exception:\n                    pass\n\n                loop.close()\n\n        # Use a non-daemon thread so it will run to completion\n        t = threading.Thread(target=thread_target, daemon=True)\n        t.daemon = False\n        self.bg_tasks.append(t)\n        t.start()\n        return t\n\n    # Helper method to wait for background tasks to complete (optional)\n    def wait_for_bg_tasks(self, timeout=None):\n        \"\"\"\n        Wait for all background tasks to complete.\n\n        Args:\n            timeout: Maximum time to wait (in seconds) for all tasks to complete.\n                     None means wait indefinitely.\n\n        Returns:\n            bool: True if all tasks completed, False if timeout occurred\n        \"\"\"\n        active_tasks = [t for t in self.bg_tasks if t.is_alive()]\n\n        for task in active_tasks:\n            task.join(timeout=timeout)\n            if task.is_alive():\n                return False\n\n        return True\n\n    def run(self, *args, request=None, running_function_coro=None, **kwargs):\n        \"\"\"\n        Run a function with support for SSE streaming in both\n        threaded and non-threaded contexts.\n        \"\"\"\n        if running_function_coro is None:\n            mn, fn = args[0]\n            if self.functions.get(mn, {}).get(fn, {}).get('request_as_kwarg', False):\n                kwargs[\"request\"] = request\n                if 'data' in kwargs and 'data' not in self.functions.get(mn, {}).get(fn, {}).get('params', []):\n                    kwargs[\"request\"]['data'] = kwargs['data']\n                    del kwargs['data']\n                if 'form_data' in kwargs and 'form_data' not in self.functions.get(mn, {}).get(fn, {}).get('params',\n                                                                                                           []):\n                    kwargs[\"request\"]['form_data'] = kwargs['form_data']\n                    del kwargs['form_data']\n                kwargs[\"request\"] = RequestData.from_dict(request)\n\n        # Create the coroutine\n        coro = running_function_coro or self.a_run_any(*args, **kwargs)\n\n        # Get or create an event loop\n        try:\n            loop = asyncio.get_event_loop()\n            is_running = loop.is_running()\n        except RuntimeError:\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n            is_running = False\n\n        # If the loop is already running, run in a separate thread\n        if is_running:\n            # Create thread pool executor as needed\n            if not hasattr(self.__class__, '_executor'):\n                self.__class__._executor = ThreadPoolExecutor(max_workers=4)\n\n            def run_in_new_thread():\n                # Set up a new loop in this thread\n                new_loop = asyncio.new_event_loop()\n                asyncio.set_event_loop(new_loop)\n\n                try:\n                    # Run the coroutine\n                    return new_loop.run_until_complete(coro)\n                finally:\n                    new_loop.close()\n\n            # Run in thread and get result\n            thread_result = self.__class__._executor.submit(run_in_new_thread).result()\n\n            # Handle streaming results from thread\n            if isinstance(thread_result, dict) and thread_result.get(\"is_stream\"):\n                # Create a new SSE stream in the main thread\n                async def stream_from_function():\n                    # Re-run the function with direct async access\n                    stream_result = await self.a_run_any(*args, **kwargs)\n\n                    if (isinstance(stream_result, Result) and\n                        getattr(stream_result.result, 'data_type', None) == \"stream\"):\n                        # Get and forward data from the original generator\n                        original_gen = stream_result.result.data.get(\"generator\")\n                        if inspect.isasyncgen(original_gen):\n                            async for item in original_gen:\n                                yield item\n\n                # Return a new streaming Result\n                return Result.stream(\n                    stream_generator=stream_from_function(),\n                    headers=thread_result.get(\"headers\", {})\n                )\n\n            result = thread_result\n        else:\n            # Direct execution when loop is not running\n            result = loop.run_until_complete(coro)\n\n        # Process the final result\n        if isinstance(result, Result):\n            result.print()\n            if getattr(result.result, 'data_type', None) == \"stream\":\n                return result\n            return result.to_api_result().model_dump(mode='json')\n\n        return result\n\n    def loop_gard(self):\n        if self.loop is None:\n            self.loop = asyncio.get_event_loop()\n        if self.loop.is_closed():\n            self.loop = asyncio.get_event_loop()\n        return self.loop\n\n    async def a_init_mod(self, mod_name, spec='app'):\n        mod = self.save_load(mod_name, spec=spec)\n        if hasattr(mod, \"__initobj\") and not mod.async_initialized:\n            await mod\n        return mod\n\n\n    def load_mod(self, mod_name: str, mlm='I', **kwargs):\n\n        action_list_helper = ['I (inplace load dill on error python)',\n                              # 'C (coppy py file to runtime dir)',\n                              # 'S (save py file to dill)',\n                              # 'CS (coppy and save py file)',\n                              # 'D (development mode, inplace load py file)'\n                              ]\n        action_list = {\"I\": lambda: self.inplace_load_instance(mod_name, **kwargs),\n                       \"C\": lambda: self._copy_load(mod_name, **kwargs)\n                       }\n\n        try:\n            if mlm in action_list:\n\n                return action_list.get(mlm)()\n            else:\n                self.logger.critical(\n                    f\"config mlm must be {' or '.join(action_list_helper)} is {mlm=}\")\n                raise ValueError(f\"config mlm must be {' or '.join(action_list_helper)} is {mlm=}\")\n        except ValueError as e:\n            self.logger.warning(Style.YELLOW(f\"Error Loading Module '{mod_name}', with error :{e}\"))\n            self.debug_rains(e)\n        except ImportError as e:\n            self.logger.error(Style.YELLOW(f\"Error Loading Module '{mod_name}', with error :{e}\"))\n            self.debug_rains(e)\n        except Exception as e:\n            self.logger.critical(Style.RED(f\"Error Loading Module '{mod_name}', with critical error :{e}\"))\n            print(Style.RED(f\"Error Loading Module '{mod_name}'\"))\n            self.debug_rains(e)\n\n        return Result.default_internal_error(info=\"info's in logs.\")\n\n    async def load_all_mods_in_file(self, working_dir=\"mods\"):\n        print(f\"LOADING ALL MODS FROM FOLDER : {working_dir}\")\n        t0 = time.perf_counter()\n        # Get the list of all modules\n        module_list = self.get_all_mods(working_dir)\n        open_modules = self.functions.keys()\n        start_len = len(open_modules)\n\n        for om in open_modules:\n            if om in module_list:\n                module_list.remove(om)\n\n        tasks: set[Task] = set()\n\n        _ = {tasks.add(asyncio.create_task(asyncio.to_thread(self.save_load, mod, 'app'))) for mod in module_list}\n        for t in asyncio.as_completed(tasks):\n            try:\n                result = await t\n                if hasattr(result, 'Name'):\n                    print('Opened :', result.Name)\n                elif hasattr(result, 'name'):\n                    if hasattr(result, 'async_initialized'):\n                        if not result.async_initialized:\n                            async def _():\n                                try:\n                                    if asyncio.iscoroutine(result):\n                                        await result\n                                    if hasattr(result, 'Name'):\n                                        print('Opened :', result.Name)\n                                    elif hasattr(result, 'name'):\n                                        print('Opened :', result.name)\n                                except Exception as e:\n                                    self.debug_rains(e)\n                                    if hasattr(result, 'Name'):\n                                        print('Error opening :', result.Name)\n                                    elif hasattr(result, 'name'):\n                                        print('Error opening :', result.name)\n                            asyncio.create_task(_())\n                        else:\n                            print('Opened :', result.name)\n                else:\n                    print('Opened :', result)\n            except Exception as e:\n                self.logger.error(Style.RED(f\"An Error occurred while opening all modules error: {str(e)}\"))\n                self.debug_rains(e)\n        opened = len(self.functions.keys()) - start_len\n\n        self.logger.info(f\"Opened {opened} modules in {time.perf_counter() - t0:.2f}s\")\n        return f\"Opened {opened} modules in {time.perf_counter() - t0:.2f}s\"\n\n    def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\", use_wd=True):\n        self.logger.info(f\"collating all mods in working directory {working_dir}\")\n\n        pr = \"_dev\" if self.dev_modi else \"\"\n        if working_dir == \"mods\" and use_wd:\n            working_dir = f\"{self.start_dir}/mods{pr}\"\n        elif use_wd:\n            pass\n        else:\n            w_dir = self.id.replace(\".\", \"_\")\n            working_dir = f\"{path_to}/{w_dir}/mod_lib{pr}/\"\n        res = os.listdir(working_dir)\n\n        self.logger.info(f\"found : {len(res)} files\")\n\n        def do_helper(_mod):\n            if \"mainTool\" in _mod:\n                return False\n            # if not _mod.endswith(\".py\"):\n            #     return False\n            if _mod.startswith(\"__\"):\n                return False\n            if _mod.startswith(\".\"):\n                return False\n            return not _mod.startswith(\"test_\")\n\n        def r_endings(word: str):\n            if word.endswith(\".py\"):\n                return word[:-3]\n            return word\n\n        mods_list = list(map(r_endings, filter(do_helper, res)))\n\n        self.logger.info(f\"found : {len(mods_list)} Modules\")\n        return mods_list\n\n    def remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            self.remove_mod(mod, delete=delete)\n\n    def remove_mod(self, mod_name, spec='app', delete=True):\n        if mod_name not in self.functions:\n            self.logger.info(f\"mod not active {mod_name}\")\n            return\n        on_exit = self.functions[mod_name].get(\"on_exit\")\n\n        def helper():\n            if f\"{spec}_instance\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance\"]\n            if f\"{spec}_instance_type\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance_type\"]\n\n        if on_exit is None and self.functions[mod_name].get(f\"{spec}_instance_type\", \"\").endswith(\"/BC\"):\n            instance = self.functions[mod_name].get(f\"{spec}_instance\", None)\n            if instance is not None and hasattr(instance, 'on_exit'):\n                if asyncio.iscoroutinefunction(instance.on_exit):\n                    self.exit_tasks.append(instance.on_exit)\n                else:\n                    instance.on_exit()\n\n        if on_exit is None and delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n            return\n        if on_exit is None:\n            helper()\n            return\n\n        i = 1\n        for f in on_exit:\n            try:\n                f_, e = self.get_function((mod_name, f), state=True, specification=spec)\n                if e == 0:\n                    self.logger.info(Style.GREY(f\"Running On exit {f} {i}/{len(on_exit)}\"))\n                    if asyncio.iscoroutinefunction(f_):\n                        self.exit_tasks.append(f_)\n                        o = None\n                    else:\n                        o = f_()\n                    if o is not None:\n                        self.print(f\"Function On Exit result: {o}\")\n                else:\n                    self.logger.warning(\"closing function not found\")\n            except Exception as e:\n                self.logger.debug(\n                    Style.YELLOW(Style.Bold(f\"modular:{mod_name}.{f} on_exit error {i}/{len(on_exit)} -&gt; {e}\")))\n            finally:\n                i += 1\n\n        helper()\n\n        if delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n\n    async def a_remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            await self.a_remove_mod(mod, delete=delete)\n\n    async def a_remove_mod(self, mod_name, spec='app', delete=True):\n        if mod_name not in self.functions:\n            self.logger.info(f\"mod not active {mod_name}\")\n            return\n        on_exit = self.functions[mod_name].get(\"on_exit\")\n\n        def helper():\n            if f\"{spec}_instance\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance\"]\n            if f\"{spec}_instance_type\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance_type\"]\n\n        if on_exit is None and self.functions[mod_name].get(f\"{spec}_instance_type\", \"\").endswith(\"/BC\"):\n            instance = self.functions[mod_name].get(f\"{spec}_instance\", None)\n            if instance is not None and hasattr(instance, 'on_exit'):\n                if asyncio.iscoroutinefunction(instance.on_exit):\n                    await instance.on_exit()\n                else:\n                    instance.on_exit()\n\n        if on_exit is None and delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n            return\n        if on_exit is None:\n            helper()\n            return\n\n        i = 1\n        for f in on_exit:\n            try:\n                f_, e = self.get_function((mod_name, f), state=True, specification=spec)\n                if e == 0:\n                    self.logger.info(Style.GREY(f\"Running On exit {f} {i}/{len(on_exit)}\"))\n                    if asyncio.iscoroutinefunction(f_):\n                        o = await f_()\n                    else:\n                        o = f_()\n                    if o is not None:\n                        self.print(f\"Function On Exit result: {o}\")\n                else:\n                    self.logger.warning(\"closing function not found\")\n            except Exception as e:\n                self.logger.debug(\n                    Style.YELLOW(Style.Bold(f\"modular:{mod_name}.{f} on_exit error {i}/{len(on_exit)} -&gt; {e}\")))\n            finally:\n                i += 1\n\n        helper()\n\n        if delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n\n    def exit(self, remove_all=True):\n        if not self.alive:\n            return\n        if self.args_sto.debug:\n            self.hide_console()\n        self.disconnect()\n        if remove_all:\n            self.remove_all_modules()\n        self.logger.info(\"Exiting ToolBox interface\")\n        self.alive = False\n        self.called_exit = True, time.time()\n        self.save_exit()\n        try:\n            self.config_fh.save_file_handler()\n        except SystemExit:\n            print(\"If u ar testing this is fine else ...\")\n\n        if hasattr(self, 'daemon_app'):\n            import threading\n\n            for thread in threading.enumerate()[::-1]:\n                if thread.name == \"MainThread\":\n                    continue\n                try:\n                    with Spinner(f\"closing Thread {thread.name:^50}|\", symbols=\"s\", count_down=True,\n                                 time_in_s=0.751 if not self.debug else 0.6):\n                        thread.join(timeout=0.751 if not self.debug else 0.6)\n                except TimeoutError as e:\n                    self.logger.error(f\"Timeout error on exit {thread.name} {str(e)}\")\n                    print(str(e), f\"Timeout {thread.name}\")\n                except KeyboardInterrupt:\n                    print(\"Unsave Exit\")\n                    break\n        if hasattr(self, 'loop') and self.loop is not None:\n            with Spinner(\"closing Event loop:\", symbols=\"+\"):\n                self.loop.stop()\n\n    async def a_exit(self):\n        await self.a_remove_all_modules()\n        results = await asyncio.gather(\n            *[asyncio.create_task(f()) for f in self.exit_tasks if asyncio.iscoroutinefunction(f)])\n        for result in results:\n            self.print(f\"Function On Exit result: {result}\")\n        self.exit(remove_all=False)\n\n    def save_load(self, modname, spec='app'):\n        self.logger.debug(f\"Save load module {modname}\")\n        if not modname:\n            self.logger.warning(\"no filename specified\")\n            return False\n        try:\n            return self.load_mod(modname, spec=spec)\n        except ModuleNotFoundError as e:\n            self.logger.error(Style.RED(f\"Module {modname} not found\"))\n            self.debug_rains(e)\n\n        return False\n\n    def get_function(self, name: Enum or tuple, **kwargs):\n        \"\"\"\n        Kwargs for _get_function\n            metadata:: return the registered function dictionary\n                stateless: (function_data, None), 0\n                stateful: (function_data, higher_order_function), 0\n            state::boolean\n                specification::str default app\n        \"\"\"\n        if isinstance(name, tuple):\n            return self._get_function(None, as_str=name, **kwargs)\n        else:\n            return self._get_function(name, **kwargs)\n\n    async def a_run_function(self, mod_function_name: Enum or tuple,\n                             tb_run_function_with_state=True,\n                             tb_run_with_specification='app',\n                             args_=None,\n                             kwargs_=None,\n                             *args,\n                             **kwargs) -&gt; Result:\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n        if isinstance(mod_function_name, tuple):\n            modular_name, function_name = mod_function_name\n        elif isinstance(mod_function_name, list):\n            modular_name, function_name = mod_function_name[0], mod_function_name[1]\n        elif isinstance(mod_function_name, Enum):\n            modular_name, function_name = mod_function_name.__class__.NAME.value, mod_function_name.value\n        else:\n            raise TypeError(\"Unknown function type\")\n\n        if not self.mod_online(modular_name, installed=True):\n            self.get_mod(modular_name)\n\n        function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                      metadata=True, specification=tb_run_with_specification)\n        self.logger.info(f\"Received fuction : {mod_function_name}, with execode: {error_code}\")\n        if error_code == 404:\n            mod = self.get_mod(modular_name)\n            if hasattr(mod, \"async_initialized\") and not mod.async_initialized:\n                await mod\n            function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                          metadata=True, specification=tb_run_with_specification)\n\n        if error_code == 404:\n            self.logger.warning(Style.RED(\"Function Not Found\"))\n            return (Result.default_user_error(interface=self.interface_type,\n                                              exec_code=404,\n                                              info=\"function not found function is not decorated\").\n                    set_origin(mod_function_name))\n\n        if error_code == 300:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 info=f\"module {modular_name}\"\n                                                      f\" has no state (instance)\").set_origin(mod_function_name)\n\n        if error_code != 0:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=error_code,\n                                                 info=f\"Internal error\"\n                                                      f\" {modular_name}.\"\n                                                      f\"{function_name}\").set_origin(mod_function_name)\n\n        if not tb_run_function_with_state:\n            function_data, _ = function_data\n            function = function_data.get('func')\n        else:\n            function_data, function = function_data\n\n        if not function:\n            self.logger.warning(Style.RED(f\"Function {function_name} not found\"))\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=404,\n                                                 info=\"function not found function\").set_origin(mod_function_name)\n\n        self.logger.info(\"Profiling function\")\n        t0 = time.perf_counter()\n        if asyncio.iscoroutinefunction(function):\n            return await self.a_fuction_runner(function, function_data, args, kwargs, t0)\n        else:\n            return self.fuction_runner(function, function_data, args, kwargs, t0)\n\n    def run_function(self, mod_function_name: Enum or tuple,\n                     tb_run_function_with_state=True,\n                     tb_run_with_specification='app',\n                     args_=None,\n                     kwargs_=None,\n                     *args,\n                     **kwargs) -&gt; Result:\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n        if isinstance(mod_function_name, tuple):\n            modular_name, function_name = mod_function_name\n        elif isinstance(mod_function_name, list):\n            modular_name, function_name = mod_function_name[0], mod_function_name[1]\n        elif isinstance(mod_function_name, Enum):\n            modular_name, function_name = mod_function_name.__class__.NAME.value, mod_function_name.value\n        else:\n            raise TypeError(\"Unknown function type\")\n\n        if not self.mod_online(modular_name, installed=True):\n            self.get_mod(modular_name)\n\n        function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                      metadata=True, specification=tb_run_with_specification)\n        self.logger.info(f\"Received fuction : {mod_function_name}, with execode: {error_code}\")\n        if error_code == 1 or error_code == 3 or error_code == 400:\n            self.get_mod(modular_name)\n            function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                          metadata=True, specification=tb_run_with_specification)\n\n        if error_code == 2:\n            self.logger.warning(Style.RED(\"Function Not Found\"))\n            return (Result.default_user_error(interface=self.interface_type,\n                                              exec_code=404,\n                                              info=\"function not found function is not decorated\").\n                    set_origin(mod_function_name))\n\n        if error_code == -1:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 info=f\"module {modular_name}\"\n                                                      f\" has no state (instance)\").set_origin(mod_function_name)\n\n        if error_code != 0:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=error_code,\n                                                 info=f\"Internal error\"\n                                                      f\" {modular_name}.\"\n                                                      f\"{function_name}\").set_origin(mod_function_name)\n\n        if not tb_run_function_with_state:\n            function_data, _ = function_data\n            function = function_data.get('func')\n        else:\n            function_data, function = function_data\n\n        if not function:\n            self.logger.warning(Style.RED(f\"Function {function_name} not found\"))\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=404,\n                                                 info=\"function not found function\").set_origin(mod_function_name)\n\n        self.logger.info(\"Profiling function\")\n        t0 = time.perf_counter()\n        if asyncio.iscoroutinefunction(function):\n            raise ValueError(f\"Fuction {function_name} is Async use a_run_any\")\n        else:\n            return self.fuction_runner(function, function_data, args, kwargs, t0)\n\n    def run_a_from_sync(self, function, *args, **kwargs):\n        # Initialize self.loop if not already set.\n        if self.loop is None:\n            try:\n                self.loop = asyncio.get_running_loop()\n            except RuntimeError:\n                self.loop = asyncio.new_event_loop()\n\n        # If the loop is running, offload the coroutine to a new thread.\n        if self.loop.is_running():\n            result_future = Future()\n\n            def run_in_new_loop():\n                new_loop = asyncio.new_event_loop()\n                asyncio.set_event_loop(new_loop)\n                try:\n                    result = new_loop.run_until_complete(function(*args, **kwargs))\n                    result_future.set_result(result)\n                except Exception as e:\n                    result_future.set_exception(e)\n                finally:\n                    new_loop.close()\n\n            thread = threading.Thread(target=run_in_new_loop)\n            thread.start()\n            thread.join()  # Block until the thread completes.\n            return result_future.result()\n        else:\n            # If the loop is not running, schedule and run the coroutine directly.\n            future = self.loop.create_task(function(*args, **kwargs))\n            return self.loop.run_until_complete(future)\n\n    def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        row = function_data.get('row')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        if_self_state = 1 if 'self' in parameters else 0\n\n        try:\n            if len(parameters) == 0:\n                res = function()\n            elif len(parameters) == len(args) + if_self_state:\n                res = function(*args)\n            elif len(parameters) == len(kwargs.keys()) + if_self_state:\n                res = function(**kwargs)\n            else:\n                res = function(*args, **kwargs)\n            self.logger.info(f\"Execution done in {time.perf_counter()-t0:.4f}\")\n            if isinstance(res, Result):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.set_origin(mod_function_name)\n            elif isinstance(res, ApiResult):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.as_result().set_origin(mod_function_name).to_api_result()\n            elif row:\n                formatted_result = res\n            else:\n                # Wrap the result in a Result object\n                formatted_result = Result.ok(\n                    interface=self.interface_type,\n                    data_info=\"Auto generated result\",\n                    data=res,\n                    info=\"Function executed successfully\"\n                ).set_origin(mod_function_name)\n            if not row:\n                self.logger.info(\n                    f\"Function Exec code: {formatted_result.info.exec_code} Info's: {formatted_result.info.help_text}\")\n            else:\n                self.logger.info(\n                    f\"Function Exec data: {formatted_result}\")\n        except Exception as e:\n            self.logger.error(\n                Style.YELLOW(Style.Bold(\n                    f\"! Function ERROR: in {modular_name}.{function_name}\")))\n            # Wrap the exception in a Result object\n            formatted_result = Result.default_internal_error(info=str(e)).set_origin(mod_function_name)\n            # res = formatted_result\n            self.logger.error(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed wit an error {str(e)}, {type(e)}\")\n            self.debug_rains(e)\n            self.print(f\"! Function ERROR: in {modular_name}.{function_name} \")\n        else:\n            self.print_ok()\n\n            self.logger.info(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed successfully\")\n\n        return formatted_result\n\n    async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        row = function_data.get('row')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        if_self_state = 1 if 'self' in parameters else 0\n\n        try:\n            if len(parameters) == 0:\n                res = await function()\n            elif len(parameters) == len(args) + if_self_state:\n                res = await function(*args)\n            elif len(parameters) == len(kwargs.keys()) + if_self_state:\n                res = await function(**kwargs)\n            else:\n                res = await function(*args, **kwargs)\n            self.logger.info(f\"Execution done in {time.perf_counter()-t0:.4f}\")\n            if isinstance(res, Result):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.set_origin(mod_function_name)\n            elif isinstance(res, ApiResult):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.as_result().set_origin(mod_function_name).to_api_result()\n            elif row:\n                formatted_result = res\n            else:\n                # Wrap the result in a Result object\n                formatted_result = Result.ok(\n                    interface=self.interface_type,\n                    data_info=\"Auto generated result\",\n                    data=res,\n                    info=\"Function executed successfully\"\n                ).set_origin(mod_function_name)\n            if not row:\n                self.logger.info(\n                    f\"Function Exec code: {formatted_result.info.exec_code} Info's: {formatted_result.info.help_text}\")\n            else:\n                self.logger.info(\n                    f\"Function Exec data: {formatted_result}\")\n        except Exception as e:\n            self.logger.error(\n                Style.YELLOW(Style.Bold(\n                    f\"! Function ERROR: in {modular_name}.{function_name}\")))\n            # Wrap the exception in a Result object\n            formatted_result = Result.default_internal_error(info=str(e)).set_origin(mod_function_name)\n            # res = formatted_result\n            self.logger.error(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed wit an error {str(e)}, {type(e)}\")\n            self.debug_rains(e)\n\n        else:\n            self.print_ok()\n\n            self.logger.info(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed successfully\")\n\n        return formatted_result\n\n    async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None,\n                       args_=None,\n                       kwargs_=None, method=\"GET\",\n                       *args, **kwargs):\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n\n        modular_name = mod_function_name\n        function_name = function_name\n\n        if isinstance(mod_function_name, str) and isinstance(function_name, str):\n            mod_function_name = (mod_function_name, function_name)\n\n        if isinstance(mod_function_name, tuple):\n            modular_name, function_name = mod_function_name\n        elif isinstance(mod_function_name, list):\n            modular_name, function_name = mod_function_name[0], mod_function_name[1]\n        elif isinstance(mod_function_name, Enum):\n            modular_name, function_name = mod_function_name.__class__.NAME.value, mod_function_name.value\n\n        r = await self.session.fetch(f\"/api/{modular_name}/{function_name}{'?' + args_ if args_ is not None else ''}\",\n                                     data=kwargs, method=method)\n        try:\n            if not r:\n                print(\"\u00a7 Session server Offline!\", self.session.base)\n                return Result.default_internal_error(msg=\"Session fetch failed\").as_dict()\n\n            content_type = r.headers.get('Content-Type', '').lower()\n            raw = await r.read()\n            encoding = r.get_encoding() or 'utf-8'\n            text = raw.decode(encoding, errors='ignore')\n\n            # Attempt JSON\n            if 'application/json' in content_type:\n                try:\n                    return await r.json()\n                except Exception as e:\n                    print(\"\u26a0 JSON decode error:\", e)\n\n            # Attempt YAML\n            if 'yaml' in content_type or text.strip().startswith('---'):\n                try:\n                    import yaml\n                    return yaml.safe_load(text)\n                except Exception as e:\n                    print(\"\u26a0 YAML decode error:\", e)\n\n            # Attempt XML\n            if 'xml' in content_type or text.strip().startswith('&lt;?xml'):\n                try:\n                    import xmltodict\n                    return xmltodict.parse(text)\n                except Exception as e:\n                    print(\"\u26a0 XML decode error:\", e)\n\n            # Fallback: return plain text\n            return Result.default_internal_error(data={'raw_text': text, 'content_type': content_type}).as_dict()\n\n        except Exception as e:\n            print(\"\u274c Fatal error during API call:\", e)\n            return Result.default_internal_error(str(e)).as_dict()\n\n    def run_local(self, *args, **kwargs):\n        return self.run_any(*args, **kwargs)\n\n    async def a_run_local(self, *args, **kwargs):\n        return await self.a_run_any(*args, **kwargs)\n\n    def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n                get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                kwargs_=None,\n                *args, **kwargs):\n\n        # if self.debug:\n        #     self.logger.info(f'Called from: {getouterframes(currentframe(), 2)}')\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n\n        if isinstance(mod_function_name, str) and isinstance(backwords_compability_variabel_string_holder, str):\n            mod_function_name = (mod_function_name, backwords_compability_variabel_string_holder)\n\n        res: Result = self.run_function(mod_function_name,\n                                        tb_run_function_with_state=tb_run_function_with_state,\n                                        tb_run_with_specification=tb_run_with_specification,\n                                        args_=args, kwargs_=kwargs).as_result()\n        if isinstance(res, ApiResult):\n            res = res.as_result()\n\n        if isinstance(res, Result) and res.bg_task is not None:\n            self.run_bg_task(res.bg_task)\n\n        if self.debug:\n            res.log(show_data=False)\n        if not get_results and isinstance(res, Result):\n            return res.get()\n\n        return res\n\n    async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                        backwords_compability_variabel_string_holder=None,\n                        get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                        kwargs_=None,\n                        *args, **kwargs):\n\n        # if self.debug:\n        #     self.logger.info(f'Called from: {getouterframes(currentframe(), 2)}')\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n\n        if isinstance(mod_function_name, str) and isinstance(backwords_compability_variabel_string_holder, str):\n            mod_function_name = (mod_function_name, backwords_compability_variabel_string_holder)\n\n        res: Result = await self.a_run_function(mod_function_name,\n                                                tb_run_function_with_state=tb_run_function_with_state,\n                                                tb_run_with_specification=tb_run_with_specification,\n                                                args_=args, kwargs_=kwargs)\n        if isinstance(res, ApiResult):\n            res = res.as_result()\n\n        if isinstance(res, Result) and res.bg_task is not None:\n            self.run_bg_task(res.bg_task)\n\n        if self.debug:\n            res.log(show_data=False)\n        if not get_results and isinstance(res, Result):\n            return res.get()\n\n        return res\n\n\n    def web_context(self):\n        if self._web_context is None:\n            self._web_context = open(\"./dist/helper.html\", encoding=\"utf-8\").read()\n        return self._web_context\n\n    def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n        if spec != \"app\":\n            self.print(f\"Getting Module {name} spec: {spec}\")\n        if name not in self.functions:\n            mod = self.save_load(name, spec=spec)\n            if mod is False or (isinstance(mod, Result) and mod.is_error()):\n                self.logger.warning(f\"Could not find {name} in {list(self.functions.keys())}\")\n                raise ValueError(f\"Could not find {name} in {list(self.functions.keys())} pleas install the module, or its posibly broken use --debug for infos\")\n        # private = self.functions[name].get(f\"{spec}_private\")\n        # if private is not None:\n        #     if private and spec != 'app':\n        #         raise ValueError(\"Module is private\")\n        if name not in self.functions:\n            self.logger.warning(f\"Module '{name}' is not found\")\n            return None\n        instance = self.functions[name].get(f\"{spec}_instance\")\n        if instance is None:\n            return self.load_mod(name, spec=spec)\n        return self.functions[name].get(f\"{spec}_instance\")\n\n    def print(self, text, *args, **kwargs):\n        # self.logger.info(f\"Output : {text}\")\n        if self.sprint(None):\n            print(Style.CYAN(f\"System${self.id}:\"), end=\" \")\n        print(text, *args, **kwargs)\n\n    def sprint(self, text, *args, **kwargs):\n        if text is None:\n            return True\n        # self.logger.info(f\"Output : {text}\")\n        print(Style.CYAN(f\"System${self.id}:\"), end=\" \")\n        if isinstance(text, str) and kwargs == {} and text:\n            stram_print(text + ' '.join(args))\n            print()\n        else:\n            print(text, *args, **kwargs)\n\n    # ----------------------------------------------------------------\n    # Decorators for the toolbox\n\n    def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n        self.remove_mod(mod_name, delete=True)\n        if hasattr(self.modules[mod_name], 'reload_save') and self.modules[mod_name].reload_save:\n            def reexecute_module_code(x):\n                return x\n        else:\n            def reexecute_module_code(module_name):\n                if isinstance(module_name, str):\n                    module = import_module(module_name)\n                else:\n                    module = module_name\n                # Get the source code of the module\n                try:\n                    source = inspect.getsource(module)\n                except Exception:\n                    # print(f\"No source for {str(module_name).split('from')[0]}: {e}\")\n                    return module\n                # Compile the source code\n                try:\n                    code = compile(source, module.__file__, 'exec')\n                    # Execute the code in the module's namespace\n                    exec(code, module.__dict__)\n                except Exception:\n                    # print(f\"No source for {str(module_name).split('from')[0]}: {e}\")\n                    pass\n                return module\n\n        if not is_file:\n            mods = self.get_all_mods(\"./mods/\" + mod_name)\n            def recursive_reload(package_name):\n                package = import_module(package_name)\n\n                # First, reload all submodules\n                if hasattr(package, '__path__'):\n                    for _finder, name, _ispkg in pkgutil.walk_packages(package.__path__, package.__name__ + \".\"):\n                        try:\n                            mod = import_module(name)\n                            reexecute_module_code(mod)\n                            reload(mod)\n                        except Exception as e:\n                            print(f\"Error reloading module {name}: {e}\")\n                            break\n\n                # Finally, reload the package itself\n                reexecute_module_code(package)\n                reload(package)\n\n            for mod in mods:\n                if mod.endswith(\".txt\") or mod.endswith(\".yaml\"):\n                    continue\n                try:\n                    recursive_reload(loc + mod_name + '.' + mod)\n                    self.print(f\"Reloaded {mod_name}.{mod}\")\n                except ImportError:\n                    self.print(f\"Could not load {mod_name}.{mod}\")\n        reexecute_module_code(self.modules[mod_name])\n        if mod_name in self.functions:\n            if \"on_exit\" in self.functions[mod_name]:\n                self.functions[mod_name][\"on_exit\"] = []\n            if \"on_start\" in self.functions[mod_name]:\n                self.functions[mod_name][\"on_start\"] = []\n        self.inplace_load_instance(mod_name, spec=spec, mfo=reload(self.modules[mod_name]) if mod_name in self.modules else None)\n\n    def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None, on_reload=None):\n        if path_name is None:\n            path_name = mod_name\n        is_file = os.path.isfile(self.start_dir + '/mods/' + path_name + '.py')\n        import watchfiles\n        def helper():\n            paths = f'mods/{path_name}' + ('.py' if is_file else '')\n            self.print(f'Watching Path: {paths}')\n            for changes in watchfiles.watch(paths):\n                if not changes:\n                    continue\n                self.reload_mod(mod_name, spec, is_file, loc)\n                if on_reload:\n                    on_reload()\n\n        if not use_thread:\n            helper()\n        else:\n            threading.Thread(target=helper, daemon=True).start()\n\n    def _register_function(self, module_name, func_name, data):\n        if module_name not in self.functions:\n            self.functions[module_name] = {}\n        if func_name in self.functions[module_name]:\n            self.print(f\"Overriding function {func_name} from {module_name}\", end=\"\\r\")\n            self.functions[module_name][func_name] = data\n        else:\n            self.functions[module_name][func_name] = data\n\n    def _create_decorator(self, type_: str,\n                          name: str = \"\",\n                          mod_name: str = \"\",\n                          level: int = -1,\n                          restrict_in_virtual_mode: bool = False,\n                          api: bool = False,\n                          helper: str = \"\",\n                          version: str or None = None,\n                          initial: bool=False,\n                          exit_f: bool=False,\n                          test: bool=True,\n                          samples:list[dict[str, Any]] | None=None,\n                          state:bool | None=None,\n                          pre_compute:Callable | None=None,\n                          post_compute:Callable[[], Result] | None=None,\n                          api_methods:list[str] | None=None,\n                          memory_cache: bool=False,\n                          file_cache: bool=False,\n                          request_as_kwarg: bool=False,\n                          row: bool=False,\n                          memory_cache_max_size:int=100,\n                          memory_cache_ttl:int=300):\n\n        if isinstance(type_, Enum):\n            type_ = type_.value\n\n        if memory_cache and file_cache:\n            raise ValueError(\"Don't use both cash at the same time for the same fuction\")\n\n        use_cache = memory_cache or file_cache\n        cache = {}\n        if file_cache:\n            cache = FileCache(folder=self.data_dir + f'\\\\cache\\\\{mod_name}\\\\',\n                              filename=self.data_dir + f'\\\\cache\\\\{mod_name}\\\\{name}cache.db')\n        if memory_cache:\n            cache = MemoryCache(maxsize=memory_cache_max_size, ttl=memory_cache_ttl)\n\n        version = self.version if version is None else self.version + ':' + version\n\n        def a_additional_process(func):\n\n            async def executor(*args, **kwargs):\n\n                if pre_compute is not None:\n                    args, kwargs = await pre_compute(*args, **kwargs)\n                if asyncio.iscoroutinefunction(func):\n                    result = await func(*args, **kwargs)\n                else:\n                    result = func(*args, **kwargs)\n                if post_compute is not None:\n                    result = await post_compute(result)\n                if row:\n                    return result\n                if not isinstance(result, Result):\n                    result = Result.ok(data=result)\n                if result.origin is None:\n                    result.set_origin((mod_name if mod_name else func.__module__.split('.')[-1]\n                                       , name if name else func.__name__\n                                       , type_))\n                if result.result.data_to == ToolBoxInterfaces.native.name:\n                    result.result.data_to = ToolBoxInterfaces.remote if api else ToolBoxInterfaces.native\n                # Wenden Sie die to_api_result Methode auf das Ergebnis an, falls verf\u00fcgbar\n                if api and hasattr(result, 'to_api_result'):\n                    return result.to_api_result()\n                return result\n\n            @wraps(func)\n            async def wrapper(*args, **kwargs):\n\n                if not use_cache:\n                    return await executor(*args, **kwargs)\n\n                try:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{str(args)},{str(kwargs.items())}\")\n                except ValueError:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{bytes(args)},{str(kwargs.items())}\")\n\n                result = cache.get(cache_key)\n                if result is not None:\n                    return result\n\n                result = await executor(*args, **kwargs)\n\n                cache.set(cache_key, result)\n\n                return result\n\n            return wrapper\n\n        def additional_process(func):\n\n            def executor(*args, **kwargs):\n\n                if pre_compute is not None:\n                    args, kwargs = pre_compute(*args, **kwargs)\n                if asyncio.iscoroutinefunction(func):\n                    result = func(*args, **kwargs)\n                else:\n                    result = func(*args, **kwargs)\n                if post_compute is not None:\n                    result = post_compute(result)\n                if row:\n                    return result\n                if not isinstance(result, Result):\n                    result = Result.ok(data=result)\n                if result.origin is None:\n                    result.set_origin((mod_name if mod_name else func.__module__.split('.')[-1]\n                                       , name if name else func.__name__\n                                       , type_))\n                if result.result.data_to == ToolBoxInterfaces.native.name:\n                    result.result.data_to = ToolBoxInterfaces.remote if api else ToolBoxInterfaces.native\n                # Wenden Sie die to_api_result Methode auf das Ergebnis an, falls verf\u00fcgbar\n                if api and hasattr(result, 'to_api_result'):\n                    return result.to_api_result()\n                return result\n\n            @wraps(func)\n            def wrapper(*args, **kwargs):\n\n                if not use_cache:\n                    return executor(*args, **kwargs)\n\n                try:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{str(args)},{str(kwargs.items())}\")\n                except ValueError:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{bytes(args)},{str(kwargs.items())}\")\n\n                result = cache.get(cache_key)\n                if result is not None:\n                    return result\n\n                result = executor(*args, **kwargs)\n\n                cache.set(cache_key, result)\n\n                return result\n\n            return wrapper\n\n        def decorator(func):\n            sig = signature(func)\n            params = list(sig.parameters)\n            module_name = mod_name if mod_name else func.__module__.split('.')[-1]\n            func_name = name if name else func.__name__\n            if func_name == 'on_start':\n                func_name = 'on_startup'\n            if func_name == 'on_exit':\n                func_name = 'on_close'\n            if api or pre_compute is not None or post_compute is not None or memory_cache or file_cache:\n                if asyncio.iscoroutinefunction(func):\n                    func = a_additional_process(func)\n                else:\n                    func = additional_process(func)\n            if api and str(sig.return_annotation) == 'Result':\n                raise ValueError(f\"Fuction {module_name}.{func_name} registered as \"\n                                 f\"Api fuction but uses {str(sig.return_annotation)}\\n\"\n                                 f\"Please change the sig from ..)-&gt; Result to ..)-&gt; ApiResult\")\n            data = {\n                \"type\": type_,\n                \"module_name\": module_name,\n                \"func_name\": func_name,\n                \"level\": level,\n                \"restrict_in_virtual_mode\": restrict_in_virtual_mode,\n                \"func\": func,\n                \"api\": api,\n                \"helper\": helper,\n                \"version\": version,\n                \"initial\": initial,\n                \"exit_f\": exit_f,\n                \"api_methods\": api_methods if api_methods is not None else [\"AUTO\"],\n                \"__module__\": func.__module__,\n                \"signature\": sig,\n                \"params\": params,\n                \"row\": row,\n                \"state\": (\n                    False if len(params) == 0 else params[0] in ['self', 'state', 'app']) if state is None else state,\n                \"do_test\": test,\n                \"samples\": samples,\n                \"request_as_kwarg\": request_as_kwarg,\n\n            }\n            self._register_function(module_name, func_name, data)\n            if exit_f:\n                if \"on_exit\" not in self.functions[module_name]:\n                    self.functions[module_name][\"on_exit\"] = []\n                self.functions[module_name][\"on_exit\"].append(func_name)\n            if initial:\n                if \"on_start\" not in self.functions[module_name]:\n                    self.functions[module_name][\"on_start\"] = []\n                self.functions[module_name][\"on_start\"].append(func_name)\n\n            return func\n\n        decorator.tb_init = True\n\n        return decorator\n\n    def tb(self, name=None,\n           mod_name: str = \"\",\n           helper: str = \"\",\n           version: str | None = None,\n           test: bool = True,\n           restrict_in_virtual_mode: bool = False,\n           api: bool = False,\n           initial: bool = False,\n           exit_f: bool = False,\n           test_only: bool = False,\n           memory_cache: bool = False,\n           file_cache: bool = False,\n           request_as_kwarg: bool = False,\n           row: bool = False,\n           state: bool | None = None,\n           level: int = -1,\n           memory_cache_max_size: int = 100,\n           memory_cache_ttl: int = 300,\n           samples: list or dict or None = None,\n           interface: ToolBoxInterfaces or None or str = None,\n           pre_compute=None,\n           post_compute=None,\n           api_methods=None,\n           ):\n        \"\"\"\n    A decorator for registering and configuring functions within a module.\n\n    This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\n    Args:\n        name (str, optional): The name to register the function under. Defaults to the function's own name.\n        mod_name (str, optional): The name of the module the function belongs to.\n        helper (str, optional): A helper string providing additional information about the function.\n        version (str or None, optional): The version of the function or module.\n        test (bool, optional): Flag to indicate if the function is for testing purposes.\n        restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n        api (bool, optional): Flag to indicate if the function is part of an API.\n        initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n        exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n        test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n        memory_cache (bool, optional): Flag to enable memory caching for the function.\n        request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n        file_cache (bool, optional): Flag to enable file caching for the function.\n        row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n        state (bool or None, optional): Flag to indicate if the function maintains state.\n        level (int, optional): The level of the function, used for prioritization or categorization.\n        memory_cache_max_size (int, optional): Maximum size of the memory cache.\n        memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n        samples (list or dict or None, optional): Samples or examples of function usage.\n        interface (str, optional): The interface type for the function.\n        pre_compute (callable, optional): A function to be called before the main function.\n        post_compute (callable, optional): A function to be called after the main function.\n        api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\n    Returns:\n        function: The decorated function with additional processing and registration capabilities.\n    \"\"\"\n        if interface is None:\n            interface = \"tb\"\n        if test_only and 'test' not in self.id:\n            return lambda *args, **kwargs: args\n        return self._create_decorator(interface,\n                                      name,\n                                      mod_name,\n                                      level=level,\n                                      restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                      helper=helper,\n                                      api=api,\n                                      version=version,\n                                      initial=initial,\n                                      exit_f=exit_f,\n                                      test=test,\n                                      samples=samples,\n                                      state=state,\n                                      pre_compute=pre_compute,\n                                      post_compute=post_compute,\n                                      memory_cache=memory_cache,\n                                      file_cache=file_cache,\n                                      request_as_kwarg=request_as_kwarg,\n                                      row=row,\n                                      api_methods=api_methods,\n                                      memory_cache_max_size=memory_cache_max_size,\n                                      memory_cache_ttl=memory_cache_ttl)\n\n    def save_autocompletion_dict(self):\n        autocompletion_dict = {}\n        for module_name, _module in self.functions.items():\n            data = {}\n            for function_name, function_data in self.functions[module_name].items():\n                if not isinstance(function_data, dict):\n                    continue\n                data[function_name] = {arg: None for arg in\n                                       function_data.get(\"params\", [])}\n                if len(data[function_name].keys()) == 0:\n                    data[function_name] = None\n            autocompletion_dict[module_name] = data if len(data.keys()) &gt; 0 else None\n        self.config_fh.add_to_save_file_handler(\"auto~~~~~~\", str(autocompletion_dict))\n\n    def get_autocompletion_dict(self):\n        return self.config_fh.get_file_handler(\"auto~~~~~~\")\n\n    def save_registry_as_enums(self, directory: str, filename: str):\n        # Ordner erstellen, falls nicht vorhanden\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n        # Dateipfad vorbereiten\n        filepath = os.path.join(directory, filename)\n\n        # Enum-Klassen als Strings generieren\n        enum_classes = [f'\"\"\"Automatic generated by ToolBox v = {self.version}\"\"\"'\n                        f'\\nfrom enum import Enum\\nfrom dataclasses import dataclass'\n                        f'\\n\\n\\n']\n        for module, functions in self.functions.items():\n            if module.startswith(\"APP_INSTANCE\"):\n                continue\n            class_name = module\n            enum_members = \"\\n    \".join(\n                [\n                    f\"{func_name.upper().replace('-', '')}\"\n                    f\" = '{func_name}' \"\n                    f\"# Input: ({fuction_data['params'] if isinstance(fuction_data, dict) else ''}),\"\n                    f\" Output: {fuction_data['signature'].return_annotation if isinstance(fuction_data, dict) else 'None'}\"\n                    for func_name, fuction_data in functions.items()])\n            enum_class = (f'@dataclass\\nclass {class_name.upper().replace(\".\", \"_\").replace(\"-\", \"\")}(Enum):'\n                          f\"\\n    NAME = '{class_name}'\\n    {enum_members}\")\n            enum_classes.append(enum_class)\n\n        # Enums in die Datei schreiben\n        data = \"\\n\\n\\n\".join(enum_classes)\n        if len(data) &lt; 12:\n            raise ValueError(\n                \"Invalid Enums Loosing content pleas delete it ur self in the (utils/system/all_functions_enums.py) or add mor new stuff :}\")\n        with open(filepath, 'w') as file:\n            file.write(data)\n\n        print(Style.Bold(Style.BLUE(f\"Enums gespeichert in {filepath}\")))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.disconnect","title":"<code>disconnect(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef disconnect(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.exit_main","title":"<code>exit_main(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef exit_main(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.get_function","title":"<code>get_function(name, **kwargs)</code>","text":"<p>Kwargs for _get_function     metadata:: return the registered function dictionary         stateless: (function_data, None), 0         stateful: (function_data, higher_order_function), 0     state::boolean         specification::str default app</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def get_function(self, name: Enum or tuple, **kwargs):\n    \"\"\"\n    Kwargs for _get_function\n        metadata:: return the registered function dictionary\n            stateless: (function_data, None), 0\n            stateful: (function_data, higher_order_function), 0\n        state::boolean\n            specification::str default app\n    \"\"\"\n    if isinstance(name, tuple):\n        return self._get_function(None, as_str=name, **kwargs)\n    else:\n        return self._get_function(name, **kwargs)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.hide_console","title":"<code>hide_console(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef hide_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.run","title":"<code>run(*args, request=None, running_function_coro=None, **kwargs)</code>","text":"<p>Run a function with support for SSE streaming in both threaded and non-threaded contexts.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def run(self, *args, request=None, running_function_coro=None, **kwargs):\n    \"\"\"\n    Run a function with support for SSE streaming in both\n    threaded and non-threaded contexts.\n    \"\"\"\n    if running_function_coro is None:\n        mn, fn = args[0]\n        if self.functions.get(mn, {}).get(fn, {}).get('request_as_kwarg', False):\n            kwargs[\"request\"] = request\n            if 'data' in kwargs and 'data' not in self.functions.get(mn, {}).get(fn, {}).get('params', []):\n                kwargs[\"request\"]['data'] = kwargs['data']\n                del kwargs['data']\n            if 'form_data' in kwargs and 'form_data' not in self.functions.get(mn, {}).get(fn, {}).get('params',\n                                                                                                       []):\n                kwargs[\"request\"]['form_data'] = kwargs['form_data']\n                del kwargs['form_data']\n            kwargs[\"request\"] = RequestData.from_dict(request)\n\n    # Create the coroutine\n    coro = running_function_coro or self.a_run_any(*args, **kwargs)\n\n    # Get or create an event loop\n    try:\n        loop = asyncio.get_event_loop()\n        is_running = loop.is_running()\n    except RuntimeError:\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        is_running = False\n\n    # If the loop is already running, run in a separate thread\n    if is_running:\n        # Create thread pool executor as needed\n        if not hasattr(self.__class__, '_executor'):\n            self.__class__._executor = ThreadPoolExecutor(max_workers=4)\n\n        def run_in_new_thread():\n            # Set up a new loop in this thread\n            new_loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(new_loop)\n\n            try:\n                # Run the coroutine\n                return new_loop.run_until_complete(coro)\n            finally:\n                new_loop.close()\n\n        # Run in thread and get result\n        thread_result = self.__class__._executor.submit(run_in_new_thread).result()\n\n        # Handle streaming results from thread\n        if isinstance(thread_result, dict) and thread_result.get(\"is_stream\"):\n            # Create a new SSE stream in the main thread\n            async def stream_from_function():\n                # Re-run the function with direct async access\n                stream_result = await self.a_run_any(*args, **kwargs)\n\n                if (isinstance(stream_result, Result) and\n                    getattr(stream_result.result, 'data_type', None) == \"stream\"):\n                    # Get and forward data from the original generator\n                    original_gen = stream_result.result.data.get(\"generator\")\n                    if inspect.isasyncgen(original_gen):\n                        async for item in original_gen:\n                            yield item\n\n            # Return a new streaming Result\n            return Result.stream(\n                stream_generator=stream_from_function(),\n                headers=thread_result.get(\"headers\", {})\n            )\n\n        result = thread_result\n    else:\n        # Direct execution when loop is not running\n        result = loop.run_until_complete(coro)\n\n    # Process the final result\n    if isinstance(result, Result):\n        result.print()\n        if getattr(result.result, 'data_type', None) == \"stream\":\n            return result\n        return result.to_api_result().model_dump(mode='json')\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.run_bg_task","title":"<code>run_bg_task(task)</code>","text":"<p>Run a task in the background that will properly handle nested asyncio operations. This implementation ensures that asyncio.create_task() and asyncio.gather() work correctly within the background task.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <p>A callable function that can be synchronous or asynchronous</p> required Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def run_bg_task(self, task):\n    \"\"\"\n    Run a task in the background that will properly handle nested asyncio operations.\n    This implementation ensures that asyncio.create_task() and asyncio.gather() work\n    correctly within the background task.\n\n    Args:\n        task: A callable function that can be synchronous or asynchronous\n    \"\"\"\n    if not callable(task):\n        self.logger.warning(\"Task is not callable!\")\n        return None\n\n    # Function that will run in a separate thread with its own event loop\n    def thread_target(task_):\n        # Create a new event loop for this thread\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n\n        try:\n            # Determine how to run the task based on its type\n            if asyncio.iscoroutinefunction(task_):\n                # If it's an async function, run it directly\n                loop.run_until_complete(task_())\n            elif asyncio.iscoroutine(task_):\n                # If it's already a coroutine object\n                loop.run_until_complete(task_)\n            else:\n                # If it's a synchronous function that might create async tasks internally\n                async def wrapper():\n                    # Run potentially blocking synchronous code in an executor\n                    return await loop.run_in_executor(None, task_)\n\n                loop.run_until_complete(wrapper())\n\n            self.logger.debug(\"Background task completed successfully\")\n        except Exception as e:\n            self.logger.error(f\"Background task failed with error: {str(e)}\")\n        finally:\n            # Clean up any pending tasks\n            pending = asyncio.all_tasks(loop)\n            if pending:\n                # Cancel any remaining tasks\n                for task_ in pending:\n                    task_.cancel()\n\n                # Allow tasks to finish cancellation\n                loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))\n\n            loop.close()\n\n    # Create and start a non-daemon thread that will run to completion\n    # Using non-daemon thread ensures the task completes even if main thread exits\n    t = threading.Thread(target=thread_target, args=(task,))\n    t.daemon = False  # Non-daemon thread will keep program alive until it completes\n    self.bg_tasks.append(t)\n    t.start()\n    return t\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.run_bg_task_advanced","title":"<code>run_bg_task_advanced(task, *args, **kwargs)</code>","text":"<p>Alternative implementation for complex async scenarios where the task creates nested asyncio tasks using create_task() and gather().</p> <p>This version ensures proper execution of nested tasks by maintaining the thread and its event loop throughout the lifetime of all child tasks.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <p>A callable function that can be synchronous or asynchronous</p> required <code>*args,</code> <code>**kwargs</code> <p>Arguments to pass to the task</p> required Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def run_bg_task_advanced(self, task, *args, **kwargs):\n    \"\"\"\n    Alternative implementation for complex async scenarios where the task creates\n    nested asyncio tasks using create_task() and gather().\n\n    This version ensures proper execution of nested tasks by maintaining the thread\n    and its event loop throughout the lifetime of all child tasks.\n\n    Args:\n        task: A callable function that can be synchronous or asynchronous\n        *args, **kwargs: Arguments to pass to the task\n    \"\"\"\n    if not callable(task):\n        self.logger.warning(\"Task is not callable!\")\n        return None\n\n    # Create a dedicated thread with its own event loop\n    async def async_wrapper():\n        try:\n            if asyncio.iscoroutinefunction(task):\n                return await task(*args, **kwargs)\n            elif asyncio.iscoroutine(task):\n                return await task\n            else:\n                # Run in executor to avoid blocking\n                loop = asyncio.get_event_loop()\n                return await loop.run_in_executor(None, lambda: task(*args, **kwargs))\n        except Exception as e:\n            self.logger.error(f\"Background task error: {str(e)}\")\n            raise\n\n    def thread_target():\n        # Create new event loop for this thread\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n\n        try:\n            # Run the task to completion with all its nested tasks\n            loop.run_until_complete(async_wrapper())\n        except Exception as e:\n            self.logger.error(f\"Background task thread failed: {str(e)}\")\n        finally:\n            # Clean up any pending tasks that might still be running\n            try:\n                pending = asyncio.all_tasks(loop)\n                if pending:\n                    # Allow tasks time to clean up\n                    loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))\n            except Exception:\n                pass\n\n            loop.close()\n\n    # Use a non-daemon thread so it will run to completion\n    t = threading.Thread(target=thread_target, daemon=True)\n    t.daemon = False\n    self.bg_tasks.append(t)\n    t.start()\n    return t\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.show_console","title":"<code>show_console(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef show_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.tb","title":"<code>tb(name=None, mod_name='', helper='', version=None, test=True, restrict_in_virtual_mode=False, api=False, initial=False, exit_f=False, test_only=False, memory_cache=False, file_cache=False, request_as_kwarg=False, row=False, state=None, level=-1, memory_cache_max_size=100, memory_cache_ttl=300, samples=None, interface=None, pre_compute=None, post_compute=None, api_methods=None)</code>","text":"<p>A decorator for registering and configuring functions within a module.</p> <p>This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to register the function under. Defaults to the function's own name.</p> <code>None</code> <code>mod_name</code> <code>str</code> <p>The name of the module the function belongs to.</p> <code>''</code> <code>helper</code> <code>str</code> <p>A helper string providing additional information about the function.</p> <code>''</code> <code>version</code> <code>str or None</code> <p>The version of the function or module.</p> <code>None</code> <code>test</code> <code>bool</code> <p>Flag to indicate if the function is for testing purposes.</p> <code>True</code> <code>restrict_in_virtual_mode</code> <code>bool</code> <p>Flag to restrict the function in virtual mode.</p> <code>False</code> <code>api</code> <code>bool</code> <p>Flag to indicate if the function is part of an API.</p> <code>False</code> <code>initial</code> <code>bool</code> <p>Flag to indicate if the function should be executed at initialization.</p> <code>False</code> <code>exit_f</code> <code>bool</code> <p>Flag to indicate if the function should be executed at exit.</p> <code>False</code> <code>test_only</code> <code>bool</code> <p>Flag to indicate if the function should only be used for testing.</p> <code>False</code> <code>memory_cache</code> <code>bool</code> <p>Flag to enable memory caching for the function.</p> <code>False</code> <code>request_as_kwarg</code> <code>bool</code> <p>Flag to get request if the fuction is calld from api.</p> <code>False</code> <code>file_cache</code> <code>bool</code> <p>Flag to enable file caching for the function.</p> <code>False</code> <code>row</code> <code>bool</code> <p>rather to auto wrap the result in Result type default False means no row data aka result type</p> <code>False</code> <code>state</code> <code>bool or None</code> <p>Flag to indicate if the function maintains state.</p> <code>None</code> <code>level</code> <code>int</code> <p>The level of the function, used for prioritization or categorization.</p> <code>-1</code> <code>memory_cache_max_size</code> <code>int</code> <p>Maximum size of the memory cache.</p> <code>100</code> <code>memory_cache_ttl</code> <code>int</code> <p>Time-to-live for the memory cache entries.</p> <code>300</code> <code>samples</code> <code>list or dict or None</code> <p>Samples or examples of function usage.</p> <code>None</code> <code>interface</code> <code>str</code> <p>The interface type for the function.</p> <code>None</code> <code>pre_compute</code> <code>callable</code> <p>A function to be called before the main function.</p> <code>None</code> <code>post_compute</code> <code>callable</code> <p>A function to be called after the main function.</p> <code>None</code> <code>api_methods</code> <code>list[str]</code> <p>default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>function</code> <p>The decorated function with additional processing and registration capabilities.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def tb(self, name=None,\n       mod_name: str = \"\",\n       helper: str = \"\",\n       version: str | None = None,\n       test: bool = True,\n       restrict_in_virtual_mode: bool = False,\n       api: bool = False,\n       initial: bool = False,\n       exit_f: bool = False,\n       test_only: bool = False,\n       memory_cache: bool = False,\n       file_cache: bool = False,\n       request_as_kwarg: bool = False,\n       row: bool = False,\n       state: bool | None = None,\n       level: int = -1,\n       memory_cache_max_size: int = 100,\n       memory_cache_ttl: int = 300,\n       samples: list or dict or None = None,\n       interface: ToolBoxInterfaces or None or str = None,\n       pre_compute=None,\n       post_compute=None,\n       api_methods=None,\n       ):\n    \"\"\"\nA decorator for registering and configuring functions within a module.\n\nThis decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\nArgs:\n    name (str, optional): The name to register the function under. Defaults to the function's own name.\n    mod_name (str, optional): The name of the module the function belongs to.\n    helper (str, optional): A helper string providing additional information about the function.\n    version (str or None, optional): The version of the function or module.\n    test (bool, optional): Flag to indicate if the function is for testing purposes.\n    restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n    api (bool, optional): Flag to indicate if the function is part of an API.\n    initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n    exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n    test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n    memory_cache (bool, optional): Flag to enable memory caching for the function.\n    request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n    file_cache (bool, optional): Flag to enable file caching for the function.\n    row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n    state (bool or None, optional): Flag to indicate if the function maintains state.\n    level (int, optional): The level of the function, used for prioritization or categorization.\n    memory_cache_max_size (int, optional): Maximum size of the memory cache.\n    memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n    samples (list or dict or None, optional): Samples or examples of function usage.\n    interface (str, optional): The interface type for the function.\n    pre_compute (callable, optional): A function to be called before the main function.\n    post_compute (callable, optional): A function to be called after the main function.\n    api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\nReturns:\n    function: The decorated function with additional processing and registration capabilities.\n\"\"\"\n    if interface is None:\n        interface = \"tb\"\n    if test_only and 'test' not in self.id:\n        return lambda *args, **kwargs: args\n    return self._create_decorator(interface,\n                                  name,\n                                  mod_name,\n                                  level=level,\n                                  restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                  helper=helper,\n                                  api=api,\n                                  version=version,\n                                  initial=initial,\n                                  exit_f=exit_f,\n                                  test=test,\n                                  samples=samples,\n                                  state=state,\n                                  pre_compute=pre_compute,\n                                  post_compute=post_compute,\n                                  memory_cache=memory_cache,\n                                  file_cache=file_cache,\n                                  request_as_kwarg=request_as_kwarg,\n                                  row=row,\n                                  api_methods=api_methods,\n                                  memory_cache_max_size=memory_cache_max_size,\n                                  memory_cache_ttl=memory_cache_ttl)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.wait_for_bg_tasks","title":"<code>wait_for_bg_tasks(timeout=None)</code>","text":"<p>Wait for all background tasks to complete.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <p>Maximum time to wait (in seconds) for all tasks to complete.      None means wait indefinitely.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if all tasks completed, False if timeout occurred</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def wait_for_bg_tasks(self, timeout=None):\n    \"\"\"\n    Wait for all background tasks to complete.\n\n    Args:\n        timeout: Maximum time to wait (in seconds) for all tasks to complete.\n                 None means wait indefinitely.\n\n    Returns:\n        bool: True if all tasks completed, False if timeout occurred\n    \"\"\"\n    active_tasks = [t for t in self.bg_tasks if t.is_alive()]\n\n    for task in active_tasks:\n        task.join(timeout=timeout)\n        if task.is_alive():\n            return False\n\n    return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code","title":"<code>Code</code>","text":"Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>class Code:\n\n    @staticmethod\n    def DK():\n        return DEVICE_KEY\n\n    def decode_code(self, encrypted_data, key=None):\n\n        if not isinstance(encrypted_data, str):\n            encrypted_data = str(encrypted_data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.decrypt_symmetric(encrypted_data, key)\n\n    def encode_code(self, data, key=None):\n\n        if not isinstance(data, str):\n            data = str(data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.encrypt_symmetric(data, key)\n\n    @staticmethod\n    def generate_seed() -&gt; int:\n        \"\"\"\n        Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n        Returns:\n            int: Eine zuf\u00e4llige Zahl.\n        \"\"\"\n        return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n\n    @staticmethod\n    def one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n        \"\"\"\n        Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n        Args:\n            text (str): Der zu hashende Text.\n            salt (str): Der Salt-Wert.\n            pepper (str): Der Pepper-Wert.\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            str: Der resultierende Hash-Wert.\n        \"\"\"\n        return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n\n    @staticmethod\n    def generate_symmetric_key() -&gt; str:\n        \"\"\"\n        Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n        Returns:\n            str: Der generierte Schl\u00fcssel.\n        \"\"\"\n        return Fernet.generate_key().decode()\n\n    @staticmethod\n    def encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        if isinstance(text, str):\n            text = text.encode()\n\n        try:\n            fernet = Fernet(key.encode())\n            return fernet.encrypt(text).decode()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n            return \"Error encrypt\"\n\n    @staticmethod\n    def decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n            to_str (bool): default true returns str if false returns bytes\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n\n        if isinstance(key, str):\n            key = key.encode()\n\n        #try:\n        fernet = Fernet(key)\n        text_b = fernet.decrypt(encrypted_text)\n        if not to_str:\n            return text_b\n        return text_b.decode()\n        # except Exception as e:\n        #     get_logger().error(f\"Error decrypt_symmetric {e}\")\n        #     if not mute:\n        #         raise e\n        #     if not to_str:\n        #         return f\"Error decoding\".encode()\n        #     return f\"Error decoding\"\n\n    @staticmethod\n    def generate_asymmetric_keys() -&gt; (str, str):\n        \"\"\"\n        Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n        Args:\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n        \"\"\"\n        private_key = rsa.generate_private_key(\n            public_exponent=65537,\n            key_size=2048 * 3,\n        )\n        public_key = private_key.public_key()\n\n        # Serialisieren der Schl\u00fcssel\n        pem_private_key = private_key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.NoEncryption()\n        ).decode()\n\n        pem_public_key = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        ).decode()\n\n        return pem_public_key, pem_private_key\n\n    @staticmethod\n    def save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n        \"\"\"\n        Speichert die generierten Schl\u00fcssel in separate Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n        Args:\n            public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n            private_key (str): Der private Schl\u00fcssel im PEM-Format\n            directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n        \"\"\"\n        # Erstelle das Verzeichnis, falls es nicht existiert\n        os.makedirs(directory, exist_ok=True)\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n        encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n        # Speichere den \u00f6ffentlichen Schl\u00fcssel\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        with open(public_key_path, \"w\") as f:\n            f.write(public_key)\n\n        # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n        with open(private_key_path, \"w\") as f:\n            f.write(encrypted_private_key)\n\n        print(\"Saved keys in \", public_key_path)\n\n    @staticmethod\n    def load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n        \"\"\"\n        L\u00e4dt die Schl\u00fcssel aus den Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n        Args:\n            directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n        Raises:\n            FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n        \"\"\"\n        # Pfade zu den Schl\u00fcsseldateien\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n\n        # Pr\u00fcfe ob die Dateien existieren\n        if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n            return \"\", \"\"\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Lade den \u00f6ffentlichen Schl\u00fcssel\n        with open(public_key_path) as f:\n            public_key = f.read()\n\n        # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n        with open(private_key_path) as f:\n            encrypted_private_key = f.read()\n            private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n        return public_key, private_key\n\n    @staticmethod\n    def encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        # try:\n        #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        #  except Exception as e:\n        #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            encrypted = public_key.encrypt(\n                text.encode(),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return encrypted.hex()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_asymmetric {e}\")\n            return \"Invalid\"\n\n    @staticmethod\n    def decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n        Args:\n            encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n            private_key_str (str): Der private Schl\u00fcssel als String.\n\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            decrypted = private_key.decrypt(\n                bytes.fromhex(encrypted_text_hex),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return decrypted.decode()\n\n        except Exception as e:\n            get_logger().error(f\"Error decrypt_asymmetric {e}\")\n        return \"Invalid\"\n\n    @staticmethod\n    def verify_signature(signature: str or bytes, message: str or bytes, public_key_str: str,\n                         salt_length=padding.PSS.MAX_LENGTH) -&gt; bool:\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                padding=padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                algorithm=hashes.SHA512()\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def verify_signature_web_algo(signature: str or bytes, message: str or bytes, public_key_str: str,\n                                  algo: int = -512) -&gt; bool:\n        signature_algorithm = ECDSA(hashes.SHA512())\n        if algo != -512:\n            signature_algorithm = ECDSA(hashes.SHA256())\n\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                # padding=padding.PSS(\n                #    mgf=padding.MGF1(hashes.SHA512()),\n                #    salt_length=padding.PSS.MAX_LENGTH\n                # ),\n                signature_algorithm=signature_algorithm\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def create_signature(message: str, private_key_str: str, salt_length=padding.PSS.MAX_LENGTH,\n                         row=False) -&gt; str or bytes:\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            signature = private_key.sign(\n                message.encode(),\n                padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                hashes.SHA512()\n            )\n            if row:\n                return signature\n            return base64.b64encode(signature).decode()\n        except Exception as e:\n            get_logger().error(f\"Error create_signature {e}\")\n            print(e)\n        return \"Invalid Key\"\n\n    @staticmethod\n    def pem_to_public_key(pem_key: str):\n        \"\"\"\n        Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n        Args:\n            pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n        Returns:\n            PublicKey: Das PublicKey-Objekt.\n        \"\"\"\n        public_key = serialization.load_pem_public_key(pem_key.encode())\n        return public_key\n\n    @staticmethod\n    def public_key_to_pem(public_key: RSAPublicKey):\n        \"\"\"\n        Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n        Args:\n            public_key (PublicKey): Das PublicKey-Objekt.\n\n        Returns:\n            str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n        \"\"\"\n        pem = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n        return pem.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.decrypt_asymmetric","title":"<code>decrypt_asymmetric(encrypted_text_hex, private_key_str)</code>  <code>staticmethod</code>","text":"<p>Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text_hex</code> <code>str</code> <p>Der verschl\u00fcsselte Text als Hex-String.</p> required <code>private_key_str</code> <code>str</code> <p>Der private Schl\u00fcssel als String.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n    Args:\n        encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n        private_key_str (str): Der private Schl\u00fcssel als String.\n\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n    try:\n        private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n        decrypted = private_key.decrypt(\n            bytes.fromhex(encrypted_text_hex),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return decrypted.decode()\n\n    except Exception as e:\n        get_logger().error(f\"Error decrypt_asymmetric {e}\")\n    return \"Invalid\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.decrypt_symmetric","title":"<code>decrypt_symmetric(encrypted_text, key, to_str=True, mute=False)</code>  <code>staticmethod</code>","text":"<p>Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text</code> <code>str</code> <p>Der zu entschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <code>to_str</code> <code>bool</code> <p>default true returns str if false returns bytes</p> <code>True</code> <p>Returns:     str: Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n        to_str (bool): default true returns str if false returns bytes\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n\n    if isinstance(key, str):\n        key = key.encode()\n\n    #try:\n    fernet = Fernet(key)\n    text_b = fernet.decrypt(encrypted_text)\n    if not to_str:\n        return text_b\n    return text_b.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.encrypt_asymmetric","title":"<code>encrypt_asymmetric(text, public_key_str)</code>  <code>staticmethod</code>","text":"<p>Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>public_key_str</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    # try:\n    #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n    #  except Exception as e:\n    #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n    try:\n        public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        encrypted = public_key.encrypt(\n            text.encode(),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return encrypted.hex()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        return \"Invalid\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.encrypt_symmetric","title":"<code>encrypt_symmetric(text, key)</code>  <code>staticmethod</code>","text":"<p>Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    if isinstance(text, str):\n        text = text.encode()\n\n    try:\n        fernet = Fernet(key.encode())\n        return fernet.encrypt(text).decode()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n        return \"Error encrypt\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.generate_asymmetric_keys","title":"<code>generate_asymmetric_keys()</code>  <code>staticmethod</code>","text":"<p>Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_asymmetric_keys() -&gt; (str, str):\n    \"\"\"\n    Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n    Args:\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n    \"\"\"\n    private_key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048 * 3,\n    )\n    public_key = private_key.public_key()\n\n    # Serialisieren der Schl\u00fcssel\n    pem_private_key = private_key.private_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PrivateFormat.PKCS8,\n        encryption_algorithm=serialization.NoEncryption()\n    ).decode()\n\n    pem_public_key = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    ).decode()\n\n    return pem_public_key, pem_private_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.generate_seed","title":"<code>generate_seed()</code>  <code>staticmethod</code>","text":"<p>Erzeugt eine zuf\u00e4llige Zahl als Seed.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Eine zuf\u00e4llige Zahl.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_seed() -&gt; int:\n    \"\"\"\n    Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n    Returns:\n        int: Eine zuf\u00e4llige Zahl.\n    \"\"\"\n    return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.generate_symmetric_key","title":"<code>generate_symmetric_key()</code>  <code>staticmethod</code>","text":"<p>Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der generierte Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_symmetric_key() -&gt; str:\n    \"\"\"\n    Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n    Returns:\n        str: Der generierte Schl\u00fcssel.\n    \"\"\"\n    return Fernet.generate_key().decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.load_keys_from_files","title":"<code>load_keys_from_files(directory='keys')</code>  <code>staticmethod</code>","text":"<p>L\u00e4dt die Schl\u00fcssel aus den Dateien. Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen</p> <code>'keys'</code> <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n    \"\"\"\n    L\u00e4dt die Schl\u00fcssel aus den Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n    Args:\n        directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n    Raises:\n        FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n    \"\"\"\n    # Pfade zu den Schl\u00fcsseldateien\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n\n    # Pr\u00fcfe ob die Dateien existieren\n    if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n        return \"\", \"\"\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Lade den \u00f6ffentlichen Schl\u00fcssel\n    with open(public_key_path) as f:\n        public_key = f.read()\n\n    # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n    with open(private_key_path) as f:\n        encrypted_private_key = f.read()\n        private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n    return public_key, private_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.one_way_hash","title":"<code>one_way_hash(text, salt='', pepper='')</code>  <code>staticmethod</code>","text":"<p>Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu hashende Text.</p> required <code>salt</code> <code>str</code> <p>Der Salt-Wert.</p> <code>''</code> <code>pepper</code> <code>str</code> <p>Der Pepper-Wert.</p> <code>''</code> <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der resultierende Hash-Wert.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n    \"\"\"\n    Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n    Args:\n        text (str): Der zu hashende Text.\n        salt (str): Der Salt-Wert.\n        pepper (str): Der Pepper-Wert.\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        str: Der resultierende Hash-Wert.\n    \"\"\"\n    return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.pem_to_public_key","title":"<code>pem_to_public_key(pem_key)</code>  <code>staticmethod</code>","text":"<p>Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.</p> <p>Parameters:</p> Name Type Description Default <code>pem_key</code> <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef pem_to_public_key(pem_key: str):\n    \"\"\"\n    Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n    Args:\n        pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n    Returns:\n        PublicKey: Das PublicKey-Objekt.\n    \"\"\"\n    public_key = serialization.load_pem_public_key(pem_key.encode())\n    return public_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.public_key_to_pem","title":"<code>public_key_to_pem(public_key)</code>  <code>staticmethod</code>","text":"<p>Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef public_key_to_pem(public_key: RSAPublicKey):\n    \"\"\"\n    Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n    Args:\n        public_key (PublicKey): Das PublicKey-Objekt.\n\n    Returns:\n        str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n    \"\"\"\n    pem = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    )\n    return pem.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.save_keys_to_files","title":"<code>save_keys_to_files(public_key, private_key, directory='keys')</code>  <code>staticmethod</code>","text":"<p>Speichert die generierten Schl\u00fcssel in separate Dateien. Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel im PEM-Format</p> required <code>private_key</code> <code>str</code> <p>Der private Schl\u00fcssel im PEM-Format</p> required <code>directory</code> <code>str</code> <p>Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen</p> <code>'keys'</code> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n    \"\"\"\n    Speichert die generierten Schl\u00fcssel in separate Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n    Args:\n        public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n        private_key (str): Der private Schl\u00fcssel im PEM-Format\n        directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n    \"\"\"\n    # Erstelle das Verzeichnis, falls es nicht existiert\n    os.makedirs(directory, exist_ok=True)\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n    encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n    # Speichere den \u00f6ffentlichen Schl\u00fcssel\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    with open(public_key_path, \"w\") as f:\n        f.write(public_key)\n\n    # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n    with open(private_key_path, \"w\") as f:\n        f.write(encrypted_private_key)\n\n    print(\"Saved keys in \", public_key_path)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.MainTool","title":"<code>MainTool</code>","text":"Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>class MainTool:\n    toolID: str = \"\"\n    # app = None\n    interface = None\n    spec = \"app\"\n    name = \"\"\n    color = \"Bold\"\n    stuf = False\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.__storedargs = args, kwargs\n        self.async_initialized = False\n\n    async def __ainit__(self, *args, **kwargs):\n        self.version = kwargs[\"v\"]\n        self.tools = kwargs.get(\"tool\", {})\n        self.name = kwargs[\"name\"]\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n        if not hasattr(self, 'config'):\n            self.config = {}\n        self.user = None\n        self.description = \"A toolbox mod\" if kwargs.get(\"description\") is None else kwargs.get(\"description\")\n        if MainTool.interface is None:\n            MainTool.interface = self.app.interface_type\n        # Result.default(self.app.interface)\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    await self.todo()\n                else:\n                    self.todo()\n                await asyncio.sleep(0.1)\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\" Error loading mod {self.name} {e}\")\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n\n        self.app.print(f\"TOOL : {self.spec}.{self.name} online\")\n\n    @property\n    def app(self):\n        return get_app(\n            from_=f\"{self.spec}.{self.name}|{self.toolID if self.toolID else '*' + MainTool.toolID} {self.interface if self.interface else MainTool.interface}\")\n\n    @app.setter\n    def app(self, v):\n        raise PermissionError(f\"You cannot set the App Instance! {v=}\")\n\n    @staticmethod\n    def return_result(error: ToolBoxError = ToolBoxError.none,\n                      exec_code: int = 0,\n                      help_text: str = \"\",\n                      data_info=None,\n                      data=None,\n                      data_to=None):\n\n        if data_to is None:\n            data_to = MainTool.interface if MainTool.interface is not None else ToolBoxInterfaces.cli\n\n        if data is None:\n            data = {}\n\n        if data_info is None:\n            data_info = {}\n\n        return Result(\n            error,\n            ToolBoxResult(data_info=data_info, data=data, data_to=data_to),\n            ToolBoxInfo(exec_code=exec_code, help_text=help_text)\n        )\n\n    def print(self, message, end=\"\\n\", **kwargs):\n        if self.stuf:\n            return\n\n        self.app.print(Style.style_dic[self.color] + self.name + Style.style_dic[\"END\"] + \":\", message, end=end,\n                       **kwargs)\n\n    def add_str_to_config(self, command):\n        if len(command) != 2:\n            self.logger.error('Invalid command must be key value')\n            return False\n        self.config[command[0]] = command[1]\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n\n    def get_version(self) -&gt; str:\n        \"\"\"\"Returns the version\"\"\"\n        return self.version\n\n    async def get_user(self, username: str) -&gt; Result:\n        return await self.app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME, username=username, get_results=True)\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.MainTool.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.__storedargs = args, kwargs\n    self.async_initialized = False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.MainTool.__initobj","title":"<code>__initobj()</code>  <code>async</code>","text":"<p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.MainTool.get_version","title":"<code>get_version()</code>","text":"<p>\"Returns the version</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\"Returns the version\"\"\"\n    return self.version\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.MainTool.webInstall","title":"<code>webInstall(user_instance, construct_render)</code>","text":"<p>\"Returns a web installer for the given user instance and construct render template</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def webInstall(self, user_instance, construct_render) -&gt; str:\n    \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result","title":"<code>Result</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class Result:\n    _task = None\n    def __init__(self,\n                 error: ToolBoxError,\n                 result: ToolBoxResult,\n                 info: ToolBoxInfo,\n                 origin: Any | None = None,\n                 ):\n        self.error: ToolBoxError = error\n        self.result: ToolBoxResult = result\n        self.info: ToolBoxInfo = info\n        self.origin = origin\n\n    def as_result(self):\n        return self\n\n    def as_dict(self):\n        return {\n            \"error\":self.error.value if isinstance(self.error, Enum) else self.error,\n        \"result\" : {\n            \"data_to\":self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n            \"data_info\":self.result.data_info,\n            \"data\":self.result.data,\n            \"data_type\":self.result.data_type\n        } if self.result else None,\n        \"info\" : {\n            \"exec_code\" : self.info.exec_code,  # exec_code umwandel in http resposn codes\n        \"help_text\" : self.info.help_text\n        } if self.info else None,\n        \"origin\" : self.origin\n        }\n\n    def set_origin(self, origin):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = origin\n        return self\n\n    def set_dir_origin(self, name, extras=\"assets/\"):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = f\"mods/{name}/{extras}\"\n        return self\n\n    def is_error(self):\n        if _test_is_result(self.result.data):\n            return self.result.data.is_error()\n        return self.info.exec_code != 0\n\n    def is_data(self):\n        return self.result.data is not None\n\n    def to_api_result(self):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=self.error.value if isinstance(self.error, Enum) else self.error,\n            result=ToolBoxResultBM(\n                data_to=self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n                data_info=self.result.data_info,\n                data=self.result.data,\n                data_type=self.result.data_type\n            ) if self.result else None,\n            info=ToolBoxInfoBM(\n                exec_code=self.info.exec_code,  # exec_code umwandel in http resposn codes\n                help_text=self.info.help_text\n            ) if self.info else None,\n            origin=self.origin\n        )\n\n    def task(self, task):\n        self._task = task\n        return self\n\n    @staticmethod\n    def result_from_dict(error: str, result: dict, info: dict, origin: list or None or str):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=error if isinstance(error, Enum) else error,\n            result=ToolBoxResultBM(\n                data_to=result.get('data_to') if isinstance(result.get('data_to'), Enum) else result.get('data_to'),\n                data_info=result.get('data_info', '404'),\n                data=result.get('data'),\n                data_type=result.get('data_type', '404'),\n            ) if result else None,\n            info=ToolBoxInfoBM(\n                exec_code=info.get('exec_code', 404),\n                help_text=info.get('help_text', '404')\n            ) if info else None,\n            origin=origin\n        ).as_result()\n\n    @classmethod\n    def stream(cls,\n               stream_generator,\n               content_type=\"text/event-stream\",\n               headers=None,\n               info=\"OK\",\n               interface=ToolBoxInterfaces.remote,\n               cleanup_func=None):\n        \"\"\"\n        Create a streaming response Result that properly handles all types of stream sources.\n\n        Args:\n            stream_generator: Any stream source (async generator, sync generator, iterable, or even string)\n            content_type: Content-Type header (default: text/event-stream for SSE)\n            headers: Additional HTTP headers\n            info: Help text for the result\n            interface: Interface to send data to\n\n        Returns:\n            A Result object configured for streaming\n        \"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        # Standard SSE headers\n        standard_headers = {\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"X-Accel-Buffering\": \"no\"\n        }\n\n        # Apply custom headers\n        all_headers = standard_headers.copy()\n        if headers:\n            all_headers.update(headers)\n\n        # Handle different types of stream sources\n        if content_type == \"text/event-stream\":\n            wrapped_generator = stream_generator\n            if inspect.isgenerator(stream_generator) or hasattr(stream_generator, '__iter__'):\n                # Sync generator or iterable\n                wrapped_generator = SSEGenerator.create_sse_stream(stream_generator, cleanup_func)\n\n            elif isinstance(stream_generator, str):\n                # String (could be a memory address or other reference)\n                # Convert to a generator that yields a single string\n                async def string_to_stream():\n                    yield stream_generator\n\n                wrapped_generator = SSEGenerator.create_sse_stream(string_to_stream(), cleanup_func)\n\n            # The final generator to use\n            final_generator = wrapped_generator\n\n        else:\n            # For non-SSE streams, use the original generator\n            final_generator = stream_generator\n\n        # Prepare streaming data\n        streaming_data = {\n            \"type\": \"stream\",\n            \"generator\": final_generator,\n            \"content_type\": content_type,\n            \"headers\": all_headers\n        }\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=streaming_data,\n            data_info=\"Streaming response\",\n            data_type=\"stream\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def default(cls, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=-1, help_text=\"\")\n        result = ToolBoxResult(data_to=interface)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a JSON response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=data,\n            data_info=\"JSON response\",\n            data_type=\"json\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n        \"\"\"Create a text response Result with specific content type.\"\"\"\n        if headers is not None:\n            return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=text_data,\n            data_info=\"Text response\",\n            data_type=content_type\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n               interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a binary data response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        # Create a dictionary with binary data and metadata\n        binary_data = {\n            \"data\": data,\n            \"content_type\": content_type,\n            \"filename\": download_name\n        }\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=binary_data,\n            data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n            data_type=\"binary\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a redirect response.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=url,\n            data_info=\"Redirect response\",\n            data_type=\"redirect\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def ok(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def html(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.remote, data_type=\"html\",status=200, headers=None):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=status, help_text=info)\n\n        if isinstance(headers, dict):\n            result = ToolBoxResult(data_to=interface, data={'html':data,'headers':headers}, data_info=data_info,\n                                   data_type=\"special_html\")\n        else:\n            result = ToolBoxResult(data_to=interface, data=data, data_info=data_info,\n                                   data_type=data_type if data_type is not None else type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def future(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.future):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=\"future\")\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def custom_error(cls, data=None, data_info=\"\", info=\"\", exec_code=-1, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def error(cls, data=None, data_info=\"\", info=\"\", exec_code=450, interface=ToolBoxInterfaces.remote):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_user_error(cls, info=\"\", exec_code=-3, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.input_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_internal_error(cls, info=\"\", exec_code=-2, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.internal_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    def print(self, show=True, show_data=True, prifix=\"\"):\n        data = '\\n' + f\"{((prifix + 'Data: ' + str(self.result.data) if self.result.data is not None else 'NO Data') if not isinstance(self.result.data, Result) else self.result.data.print(show=False, show_data=show_data, prifix=prifix + '-')) if show_data else 'Data: private'}\"\n        origin = '\\n' + f\"{prifix + 'Origin: ' + str(self.origin) if self.origin is not None else 'NO Origin'}\"\n        text = (f\"Function Exec code: {self.info.exec_code}\"\n                f\"\\n{prifix}Info's:\"\n                f\" {self.info.help_text} {'&lt;|&gt; ' + str(self.result.data_info) if self.result.data_info is not None else ''}\"\n                f\"{origin}{data if not data.endswith('NO Data') else ''}\")\n        if not show:\n            return text\n        print(\"\\n======== Result ========\\n\" + text + \"\\n------- EndOfD -------\")\n        return self\n\n    def log(self, show_data=True, prifix=\"\"):\n        from toolboxv2 import get_logger\n        get_logger().debug(self.print(show=False, show_data=show_data, prifix=prifix).replace(\"\\n\", \" - \"))\n        return self\n\n    def __str__(self):\n        return self.print(show=False, show_data=True)\n\n    def get(self, key=None, default=None):\n        data = self.result.data\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    async def aget(self, key=None, default=None):\n        if asyncio.isfuture(self.result.data) or asyncio.iscoroutine(self.result.data) or (\n            isinstance(self.result.data_to, Enum) and self.result.data_to.name == ToolBoxInterfaces.future.name):\n            data = await self.result.data\n        else:\n            data = self.get(key=None, default=None)\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    def lazy_return(self, _=0, data=None, **kwargs):\n        flags = ['raise', 'logg', 'user', 'intern']\n        flag = flags[_] if isinstance(_, int) else _\n        if self.info.exec_code == 0:\n            return self if data is None else data if _test_is_result(data) else self.ok(data=data, **kwargs)\n        if flag == 'raise':\n            raise ValueError(self.print(show=False))\n        if flag == 'logg':\n            from .. import get_logger\n            get_logger().error(self.print(show=False))\n\n        if flag == 'user':\n            return self if data is None else data if _test_is_result(data) else self.default_user_error(data=data,\n                                                                                                        **kwargs)\n        if flag == 'intern':\n            return self if data is None else data if _test_is_result(data) else self.default_internal_error(data=data,\n                                                                                                            **kwargs)\n\n        return self if data is None else data if _test_is_result(data) else self.custom_error(data=data, **kwargs)\n\n    @property\n    def bg_task(self):\n        return self._task\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.binary","title":"<code>binary(data, content_type='application/octet-stream', download_name=None, info='OK', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a binary data response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n           interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a binary data response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    # Create a dictionary with binary data and metadata\n    binary_data = {\n        \"data\": data,\n        \"content_type\": content_type,\n        \"filename\": download_name\n    }\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=binary_data,\n        data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n        data_type=\"binary\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.json","title":"<code>json(data, info='OK', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a JSON response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a JSON response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=data,\n        data_info=\"JSON response\",\n        data_type=\"json\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.redirect","title":"<code>redirect(url, status_code=302, info='Redirect', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a redirect response.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a redirect response.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=url,\n        data_info=\"Redirect response\",\n        data_type=\"redirect\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.stream","title":"<code>stream(stream_generator, content_type='text/event-stream', headers=None, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code>  <code>classmethod</code>","text":"<p>Create a streaming response Result that properly handles all types of stream sources.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <p>Any stream source (async generator, sync generator, iterable, or even string)</p> required <code>content_type</code> <p>Content-Type header (default: text/event-stream for SSE)</p> <code>'text/event-stream'</code> <code>headers</code> <p>Additional HTTP headers</p> <code>None</code> <code>info</code> <p>Help text for the result</p> <code>'OK'</code> <code>interface</code> <p>Interface to send data to</p> <code>remote</code> <p>Returns:</p> Type Description <p>A Result object configured for streaming</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef stream(cls,\n           stream_generator,\n           content_type=\"text/event-stream\",\n           headers=None,\n           info=\"OK\",\n           interface=ToolBoxInterfaces.remote,\n           cleanup_func=None):\n    \"\"\"\n    Create a streaming response Result that properly handles all types of stream sources.\n\n    Args:\n        stream_generator: Any stream source (async generator, sync generator, iterable, or even string)\n        content_type: Content-Type header (default: text/event-stream for SSE)\n        headers: Additional HTTP headers\n        info: Help text for the result\n        interface: Interface to send data to\n\n    Returns:\n        A Result object configured for streaming\n    \"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    # Standard SSE headers\n    standard_headers = {\n        \"Cache-Control\": \"no-cache\",\n        \"Connection\": \"keep-alive\",\n        \"X-Accel-Buffering\": \"no\"\n    }\n\n    # Apply custom headers\n    all_headers = standard_headers.copy()\n    if headers:\n        all_headers.update(headers)\n\n    # Handle different types of stream sources\n    if content_type == \"text/event-stream\":\n        wrapped_generator = stream_generator\n        if inspect.isgenerator(stream_generator) or hasattr(stream_generator, '__iter__'):\n            # Sync generator or iterable\n            wrapped_generator = SSEGenerator.create_sse_stream(stream_generator, cleanup_func)\n\n        elif isinstance(stream_generator, str):\n            # String (could be a memory address or other reference)\n            # Convert to a generator that yields a single string\n            async def string_to_stream():\n                yield stream_generator\n\n            wrapped_generator = SSEGenerator.create_sse_stream(string_to_stream(), cleanup_func)\n\n        # The final generator to use\n        final_generator = wrapped_generator\n\n    else:\n        # For non-SSE streams, use the original generator\n        final_generator = stream_generator\n\n    # Prepare streaming data\n    streaming_data = {\n        \"type\": \"stream\",\n        \"generator\": final_generator,\n        \"content_type\": content_type,\n        \"headers\": all_headers\n    }\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=streaming_data,\n        data_info=\"Streaming response\",\n        data_type=\"stream\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.text","title":"<code>text(text_data, content_type='text/plain', exec_code=None, status=200, info='OK', interface=ToolBoxInterfaces.remote, headers=None)</code>  <code>classmethod</code>","text":"<p>Create a text response Result with specific content type.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n    \"\"\"Create a text response Result with specific content type.\"\"\"\n    if headers is not None:\n        return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=text_data,\n        data_info=\"Text response\",\n        data_type=content_type\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Singleton","title":"<code>Singleton</code>","text":"<p>Singleton metaclass for ensuring only one instance of a class.</p> Source code in <code>toolboxv2/utils/singelton_class.py</code> <pre><code>class Singleton(type):\n    \"\"\"\n    Singleton metaclass for ensuring only one instance of a class.\n    \"\"\"\n\n    _instances = {}\n    _kwargs = {}\n    _args = {}\n\n    def __call__(cls, *args, **kwargs):\n        if cls not in cls._instances:\n            cls._instances[cls] = super().__call__(*args, **kwargs)\n            cls._args[cls] = args\n            cls._kwargs[cls] = kwargs\n        return cls._instances[cls]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Spinner","title":"<code>Spinner</code>","text":"<p>Enhanced Spinner with tqdm-like line rendering.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>class Spinner:\n    \"\"\"\n    Enhanced Spinner with tqdm-like line rendering.\n    \"\"\"\n    SYMBOL_SETS = {\n        \"c\": [\"\u25d0\", \"\u25d3\", \"\u25d1\", \"\u25d2\"],\n        \"b\": [\"\u2581\", \"\u2583\", \"\u2584\", \"\u2585\", \"\u2586\", \"\u2587\", \"\u2588\", \"\u2587\", \"\u2586\", \"\u2585\", \"\u2584\", \"\u2583\"],\n        \"d\": [\"\u28fe\", \"\u28fd\", \"\u28fb\", \"\u28bf\", \"\u287f\", \"\u28df\", \"\u28ef\", \"\u28f7\"],\n        \"w\": [\"\ud83c\udf0d\", \"\ud83c\udf0e\", \"\ud83c\udf0f\"],\n        \"s\": [\"\ud83c\udf00   \", \" \ud83c\udf00  \", \"  \ud83c\udf00 \", \"   \ud83c\udf00\", \"  \ud83c\udf00 \", \" \ud83c\udf00  \"],\n        \"+\": [\"+\", \"x\"],\n        \"t\": [\"\u2736\", \"\u2738\", \"\u2739\", \"\u273a\", \"\u2739\", \"\u2737\"]\n    }\n\n    def __init__(\n        self,\n        message: str = \"Loading...\",\n        delay: float = 0.1,\n        symbols=None,\n        count_down: bool = False,\n        time_in_s: float = 0\n    ):\n        \"\"\"Initialize spinner with flexible configuration.\"\"\"\n        # Resolve symbol set.\n        if isinstance(symbols, str):\n            symbols = self.SYMBOL_SETS.get(symbols, None)\n\n        # Default symbols if not provided.\n        if symbols is None:\n            symbols = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n\n        # Test mode symbol set.\n        if 'unittest' in sys.argv[0]:\n            symbols = ['#', '=', '-']\n\n        self.spinner = itertools.cycle(symbols)\n        self.delay = delay\n        self.message = message\n        self.running = False\n        self.spinner_thread = None\n        self.max_t = time_in_s\n        self.contd = count_down\n\n        # Rendering management.\n        self._is_primary = False\n        self._start_time = 0\n\n        # Central manager.\n        self.manager = SpinnerManager()\n\n    def _generate_render_line(self):\n        \"\"\"Generate the primary render line.\"\"\"\n        current_time = time.time()\n        if self.contd:\n            remaining = max(0, self.max_t - (current_time - self._start_time))\n            time_display = f\"{remaining:.2f}\"\n        else:\n            time_display = f\"{current_time - self._start_time:.2f}\"\n\n        symbol = next(self.spinner)\n        return f\"{symbol} {self.message} | {time_display}\"\n\n    def _generate_secondary_info(self):\n        \"\"\"Generate secondary spinner info for additional spinners.\"\"\"\n        return f\"{self.message}\"\n\n    def __enter__(self):\n        \"\"\"Start the spinner.\"\"\"\n        self.running = True\n        self._start_time = time.time()\n        self.manager.register_spinner(self)\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        \"\"\"Stop the spinner.\"\"\"\n        self.running = False\n        self.manager.unregister_spinner(self)\n        # Clear the spinner's line if it was the primary spinner.\n        if self._is_primary:\n            sys.stdout.write(\"\\r\\033[K\")\n            sys.stdout.flush()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Spinner.__enter__","title":"<code>__enter__()</code>","text":"<p>Start the spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __enter__(self):\n    \"\"\"Start the spinner.\"\"\"\n    self.running = True\n    self._start_time = time.time()\n    self.manager.register_spinner(self)\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Spinner.__exit__","title":"<code>__exit__(exc_type, exc_value, exc_traceback)</code>","text":"<p>Stop the spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __exit__(self, exc_type, exc_value, exc_traceback):\n    \"\"\"Stop the spinner.\"\"\"\n    self.running = False\n    self.manager.unregister_spinner(self)\n    # Clear the spinner's line if it was the primary spinner.\n    if self._is_primary:\n        sys.stdout.write(\"\\r\\033[K\")\n        sys.stdout.flush()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Spinner.__init__","title":"<code>__init__(message='Loading...', delay=0.1, symbols=None, count_down=False, time_in_s=0)</code>","text":"<p>Initialize spinner with flexible configuration.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __init__(\n    self,\n    message: str = \"Loading...\",\n    delay: float = 0.1,\n    symbols=None,\n    count_down: bool = False,\n    time_in_s: float = 0\n):\n    \"\"\"Initialize spinner with flexible configuration.\"\"\"\n    # Resolve symbol set.\n    if isinstance(symbols, str):\n        symbols = self.SYMBOL_SETS.get(symbols, None)\n\n    # Default symbols if not provided.\n    if symbols is None:\n        symbols = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n\n    # Test mode symbol set.\n    if 'unittest' in sys.argv[0]:\n        symbols = ['#', '=', '-']\n\n    self.spinner = itertools.cycle(symbols)\n    self.delay = delay\n    self.message = message\n    self.running = False\n    self.spinner_thread = None\n    self.max_t = time_in_s\n    self.contd = count_down\n\n    # Rendering management.\n    self._is_primary = False\n    self._start_time = 0\n\n    # Central manager.\n    self.manager = SpinnerManager()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.TBEF","title":"<code>TBEF</code>","text":"<p>Automatic generated by ToolBox v = 0.1.21</p>"},{"location":"toolboxv2/#toolboxv2.utils.daemon","title":"<code>daemon</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.daemon.DaemonUtil","title":"<code>DaemonUtil</code>","text":"Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>class DaemonUtil:\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.server = None\n        self.alive = False\n        self.__storedargs = args, kwargs\n        self.async_initialized = False\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n\n    async def __ainit__(self, class_instance: Any, host='0.0.0.0', port=6587, t=False,\n                        app: (App or AppType) | None = None,\n                        peer=False, name='daemonApp-server', on_register=None, on_client_exit=None, on_server_exit=None,\n                        unix_socket=False, test_override=False):\n        from toolboxv2.mods.SocketManager import SocketType\n        self.class_instance = class_instance\n        self.server = None\n        self.port = port\n        self.host = host\n        self.alive = False\n        self.test_override = test_override\n        self._name = name\n        if on_register is None:\n            def on_register(*args):\n                return None\n        self._on_register = on_register\n        if on_client_exit is None:\n            def on_client_exit(*args):\n                return None\n        self.on_client_exit = on_client_exit\n        if on_server_exit is None:\n            def on_server_exit():\n                return None\n        self.on_server_exit = on_server_exit\n        self.unix_socket = unix_socket\n        self.online = None\n        connection_type = SocketType.server\n        if peer:\n            connection_type = SocketType.peer\n\n        await self.start_server(connection_type)\n        app = app if app is not None else get_app(from_=f\"DaemonUtil.{self._name}\")\n        self.online = await asyncio.to_thread(self.connect, app)\n        if t:\n            await self.online\n\n    async def start_server(self, connection_type=None):\n        \"\"\"Start the server using app and the socket manager\"\"\"\n        from toolboxv2.mods.SocketManager import SocketType\n        if connection_type is None:\n            connection_type = SocketType.server\n        app = get_app(from_=\"Starting.Daemon\")\n        print(app.mod_online(\"SocketManager\"), \"SocketManager\")\n        if not app.mod_online(\"SocketManager\"):\n            await app.load_mod(\"SocketManager\")\n        server_result = await app.a_run_any(SOCKETMANAGER.CREATE_SOCKET,\n                                            get_results=True,\n                                            name=self._name,\n                                            host=self.host,\n                                            port=self.port,\n                                            type_id=connection_type,\n                                            max_connections=-1,\n                                            return_full_object=True,\n                                            test_override=self.test_override,\n                                            unix_file=self.unix_socket)\n        if server_result.is_error():\n            raise Exception(f\"Server error: {server_result.print(False)}\")\n        if not server_result.is_data():\n            raise Exception(f\"Server error: {server_result.print(False)}\")\n        self.alive = True\n        self.server = server_result\n        # 'socket': socket,\n        # 'receiver_socket': r_socket,\n        # 'host': host,\n        # 'port': port,\n        # 'p2p-port': endpoint_port,\n        # 'sender': send,\n        # 'receiver_queue': receiver_queue,\n        # 'connection_error': connection_error,\n        # 'receiver_thread': s_thread,\n        # 'keepalive_thread': keep_alive_thread,\n        # 'running_dict': running_dict,\n        # 'client_to_receiver_thread': to_receive,\n        # 'client_receiver_threads': threeds,\n\n    async def send(self, data: dict or bytes or str, identifier: tuple[str, int] or str = \"main\"):\n        result = await self.server.aget()\n        sender = result.get('sender')\n        await sender(data, identifier)\n        return \"Data Transmitted\"\n\n    @staticmethod\n    async def runner_co(fuction, *args, **kwargs):\n        if asyncio.iscoroutinefunction(fuction):\n            return await fuction(*args, **kwargs)\n        return fuction(*args, **kwargs)\n\n    async def connect(self, app):\n        result = await self.server.aget()\n        if not isinstance(result, dict) or result.get('connection_error') != 0:\n            raise Exception(f\"Server error: {result}\")\n        self.server = Result.ok(result)\n        receiver_queue: queue.Queue = self.server.get('receiver_queue')\n        client_to_receiver_thread = self.server.get('client_to_receiver_thread')\n        running_dict = self.server.get('running_dict')\n        sender = self.server.get('sender')\n        known_clients = {}\n        valid_clients = {}\n        app.print(f\"Starting Demon {self._name}\")\n\n        while self.alive:\n\n            if not receiver_queue.empty():\n                data = receiver_queue.get()\n                if not data:\n                    continue\n                if 'identifier' not in data:\n                    continue\n\n                identifier = data.get('identifier', 'unknown')\n                try:\n                    if identifier == \"new_con\":\n                        client, address = data.get('data')\n                        get_logger().info(f\"New connection: {address}\")\n                        known_clients[str(address)] = client\n                        await client_to_receiver_thread(client, str(address))\n\n                        await self.runner_co(self._on_register, identifier, address)\n                        identifier = str(address)\n                        # await sender({'ok': 0}, identifier)\n\n                    print(\"Receiver queue\", identifier, identifier in known_clients, identifier in valid_clients)\n                    # validation\n                    if identifier in known_clients:\n                        get_logger().info(identifier)\n                        if identifier.startswith(\"('127.0.0.1'\"):\n                            valid_clients[identifier] = known_clients[identifier]\n                            await self.runner_co(self._on_register, identifier, data)\n                        elif data.get(\"claim\", False):\n                            do = app.run_any((\"CloudM.UserInstances\", \"validate_ws_id\"),\n                                             ws_id=data.get(\"claim\"))[0]\n                            get_logger().info(do)\n                            if do:\n                                valid_clients[identifier] = known_clients[identifier]\n                                await self.runner_co(self._on_register, identifier, data)\n                        elif data.get(\"key\", False) == os.getenv(\"TB_R_KEY\"):\n                            valid_clients[identifier] = known_clients[identifier]\n                            await self.runner_co(self._on_register, identifier, data)\n                        else:\n                            get_logger().warning(f\"Validating Failed: {identifier}\")\n                            # sender({'Validating Failed': -1}, eval(identifier))\n                        get_logger().info(f\"Validating New: {identifier}\")\n                        del known_clients[identifier]\n\n                    elif identifier in valid_clients:\n                        get_logger().info(f\"New valid Request: {identifier}\")\n                        name = data.get('name')\n                        args = data.get('args')\n                        kwargs = data.get('kwargs')\n\n                        get_logger().info(f\"Request data: {name=}{args=}{kwargs=}{identifier=}\")\n\n                        if name == 'exit_main':\n                            self.alive = False\n                            break\n\n                        if name == 'show_console':\n                            show_console(True)\n                            await sender({'ok': 0}, identifier)\n                            continue\n\n                        if name == 'hide_console':\n                            show_console(False)\n                            await sender({'ok': 0}, identifier)\n                            continue\n\n                        if name == 'rrun_flow':\n                            show_console(True)\n                            runnner = self.class_instance.run_flow\n                            threading.Thread(target=runnner, args=args, kwargs=kwargs, daemon=True).start()\n                            await sender({'ok': 0}, identifier)\n                            show_console(False)\n                            continue\n\n                        async def _helper_runner():\n                            try:\n                                attr_f = getattr(self.class_instance, name)\n\n                                if asyncio.iscoroutinefunction(attr_f):\n                                    res = await attr_f(*args, **kwargs)\n                                else:\n                                    res = attr_f(*args, **kwargs)\n\n                                if res is None:\n                                    res = {'data': res}\n                                elif isinstance(res, Result):\n                                    if asyncio.iscoroutine(res.get()) or isinstance(res.get(), asyncio.Task):\n                                        res_ = await res.aget()\n                                        res.result.data = res_\n                                    res = json.loads(res.to_api_result().json())\n                                elif isinstance(res, bytes | dict):\n                                    pass\n                                else:\n                                    res = {'data': 'unsupported type', 'type': str(type(res))}\n\n                                get_logger().info(f\"sending response {res} {type(res)}\")\n\n                                await sender(res, identifier)\n                            except Exception as e:\n                                await sender({\"data\": str(e)}, identifier)\n\n                        await _helper_runner()\n                    else:\n                        print(\"Unknown connection data:\", data)\n\n                except Exception as e:\n                    get_logger().warning(Style.RED(f\"An error occurred on {identifier} {str(e)}\"))\n                    if identifier != \"unknown\":\n                        running_dict[\"receive\"][str(identifier)] = False\n                        await self.runner_co(self.on_client_exit,  identifier)\n            await asyncio.sleep(0.1)\n        running_dict[\"server_receiver\"] = False\n        for x in running_dict[\"receive\"]:\n            running_dict[\"receive\"][x] = False\n        running_dict[\"keep_alive_var\"] = False\n        await self.runner_co(self.on_server_exit)\n        app.print(f\"Closing Demon {self._name}\")\n        return Result.ok()\n\n    async def a_exit(self):\n        result = await self.server.aget()\n        await result.get(\"close\")()\n        self.alive = False\n        if asyncio.iscoroutine(self.online):\n            await self.online\n        print(\"Connection result :\", result.get(\"host\"), result.get(\"port\"),\n              \"total connections:\", result.get(\"connections\"))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.daemon.DaemonUtil.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.server = None\n    self.alive = False\n    self.__storedargs = args, kwargs\n    self.async_initialized = False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.daemon.DaemonUtil.__initobj","title":"<code>__initobj()</code>  <code>async</code>","text":"<p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.daemon.DaemonUtil.start_server","title":"<code>start_server(connection_type=None)</code>  <code>async</code>","text":"<p>Start the server using app and the socket manager</p> Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>async def start_server(self, connection_type=None):\n    \"\"\"Start the server using app and the socket manager\"\"\"\n    from toolboxv2.mods.SocketManager import SocketType\n    if connection_type is None:\n        connection_type = SocketType.server\n    app = get_app(from_=\"Starting.Daemon\")\n    print(app.mod_online(\"SocketManager\"), \"SocketManager\")\n    if not app.mod_online(\"SocketManager\"):\n        await app.load_mod(\"SocketManager\")\n    server_result = await app.a_run_any(SOCKETMANAGER.CREATE_SOCKET,\n                                        get_results=True,\n                                        name=self._name,\n                                        host=self.host,\n                                        port=self.port,\n                                        type_id=connection_type,\n                                        max_connections=-1,\n                                        return_full_object=True,\n                                        test_override=self.test_override,\n                                        unix_file=self.unix_socket)\n    if server_result.is_error():\n        raise Exception(f\"Server error: {server_result.print(False)}\")\n    if not server_result.is_data():\n        raise Exception(f\"Server error: {server_result.print(False)}\")\n    self.alive = True\n    self.server = server_result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.daemon.daemon_util","title":"<code>daemon_util</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.daemon.daemon_util.DaemonUtil","title":"<code>DaemonUtil</code>","text":"Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>class DaemonUtil:\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.server = None\n        self.alive = False\n        self.__storedargs = args, kwargs\n        self.async_initialized = False\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n\n    async def __ainit__(self, class_instance: Any, host='0.0.0.0', port=6587, t=False,\n                        app: (App or AppType) | None = None,\n                        peer=False, name='daemonApp-server', on_register=None, on_client_exit=None, on_server_exit=None,\n                        unix_socket=False, test_override=False):\n        from toolboxv2.mods.SocketManager import SocketType\n        self.class_instance = class_instance\n        self.server = None\n        self.port = port\n        self.host = host\n        self.alive = False\n        self.test_override = test_override\n        self._name = name\n        if on_register is None:\n            def on_register(*args):\n                return None\n        self._on_register = on_register\n        if on_client_exit is None:\n            def on_client_exit(*args):\n                return None\n        self.on_client_exit = on_client_exit\n        if on_server_exit is None:\n            def on_server_exit():\n                return None\n        self.on_server_exit = on_server_exit\n        self.unix_socket = unix_socket\n        self.online = None\n        connection_type = SocketType.server\n        if peer:\n            connection_type = SocketType.peer\n\n        await self.start_server(connection_type)\n        app = app if app is not None else get_app(from_=f\"DaemonUtil.{self._name}\")\n        self.online = await asyncio.to_thread(self.connect, app)\n        if t:\n            await self.online\n\n    async def start_server(self, connection_type=None):\n        \"\"\"Start the server using app and the socket manager\"\"\"\n        from toolboxv2.mods.SocketManager import SocketType\n        if connection_type is None:\n            connection_type = SocketType.server\n        app = get_app(from_=\"Starting.Daemon\")\n        print(app.mod_online(\"SocketManager\"), \"SocketManager\")\n        if not app.mod_online(\"SocketManager\"):\n            await app.load_mod(\"SocketManager\")\n        server_result = await app.a_run_any(SOCKETMANAGER.CREATE_SOCKET,\n                                            get_results=True,\n                                            name=self._name,\n                                            host=self.host,\n                                            port=self.port,\n                                            type_id=connection_type,\n                                            max_connections=-1,\n                                            return_full_object=True,\n                                            test_override=self.test_override,\n                                            unix_file=self.unix_socket)\n        if server_result.is_error():\n            raise Exception(f\"Server error: {server_result.print(False)}\")\n        if not server_result.is_data():\n            raise Exception(f\"Server error: {server_result.print(False)}\")\n        self.alive = True\n        self.server = server_result\n        # 'socket': socket,\n        # 'receiver_socket': r_socket,\n        # 'host': host,\n        # 'port': port,\n        # 'p2p-port': endpoint_port,\n        # 'sender': send,\n        # 'receiver_queue': receiver_queue,\n        # 'connection_error': connection_error,\n        # 'receiver_thread': s_thread,\n        # 'keepalive_thread': keep_alive_thread,\n        # 'running_dict': running_dict,\n        # 'client_to_receiver_thread': to_receive,\n        # 'client_receiver_threads': threeds,\n\n    async def send(self, data: dict or bytes or str, identifier: tuple[str, int] or str = \"main\"):\n        result = await self.server.aget()\n        sender = result.get('sender')\n        await sender(data, identifier)\n        return \"Data Transmitted\"\n\n    @staticmethod\n    async def runner_co(fuction, *args, **kwargs):\n        if asyncio.iscoroutinefunction(fuction):\n            return await fuction(*args, **kwargs)\n        return fuction(*args, **kwargs)\n\n    async def connect(self, app):\n        result = await self.server.aget()\n        if not isinstance(result, dict) or result.get('connection_error') != 0:\n            raise Exception(f\"Server error: {result}\")\n        self.server = Result.ok(result)\n        receiver_queue: queue.Queue = self.server.get('receiver_queue')\n        client_to_receiver_thread = self.server.get('client_to_receiver_thread')\n        running_dict = self.server.get('running_dict')\n        sender = self.server.get('sender')\n        known_clients = {}\n        valid_clients = {}\n        app.print(f\"Starting Demon {self._name}\")\n\n        while self.alive:\n\n            if not receiver_queue.empty():\n                data = receiver_queue.get()\n                if not data:\n                    continue\n                if 'identifier' not in data:\n                    continue\n\n                identifier = data.get('identifier', 'unknown')\n                try:\n                    if identifier == \"new_con\":\n                        client, address = data.get('data')\n                        get_logger().info(f\"New connection: {address}\")\n                        known_clients[str(address)] = client\n                        await client_to_receiver_thread(client, str(address))\n\n                        await self.runner_co(self._on_register, identifier, address)\n                        identifier = str(address)\n                        # await sender({'ok': 0}, identifier)\n\n                    print(\"Receiver queue\", identifier, identifier in known_clients, identifier in valid_clients)\n                    # validation\n                    if identifier in known_clients:\n                        get_logger().info(identifier)\n                        if identifier.startswith(\"('127.0.0.1'\"):\n                            valid_clients[identifier] = known_clients[identifier]\n                            await self.runner_co(self._on_register, identifier, data)\n                        elif data.get(\"claim\", False):\n                            do = app.run_any((\"CloudM.UserInstances\", \"validate_ws_id\"),\n                                             ws_id=data.get(\"claim\"))[0]\n                            get_logger().info(do)\n                            if do:\n                                valid_clients[identifier] = known_clients[identifier]\n                                await self.runner_co(self._on_register, identifier, data)\n                        elif data.get(\"key\", False) == os.getenv(\"TB_R_KEY\"):\n                            valid_clients[identifier] = known_clients[identifier]\n                            await self.runner_co(self._on_register, identifier, data)\n                        else:\n                            get_logger().warning(f\"Validating Failed: {identifier}\")\n                            # sender({'Validating Failed': -1}, eval(identifier))\n                        get_logger().info(f\"Validating New: {identifier}\")\n                        del known_clients[identifier]\n\n                    elif identifier in valid_clients:\n                        get_logger().info(f\"New valid Request: {identifier}\")\n                        name = data.get('name')\n                        args = data.get('args')\n                        kwargs = data.get('kwargs')\n\n                        get_logger().info(f\"Request data: {name=}{args=}{kwargs=}{identifier=}\")\n\n                        if name == 'exit_main':\n                            self.alive = False\n                            break\n\n                        if name == 'show_console':\n                            show_console(True)\n                            await sender({'ok': 0}, identifier)\n                            continue\n\n                        if name == 'hide_console':\n                            show_console(False)\n                            await sender({'ok': 0}, identifier)\n                            continue\n\n                        if name == 'rrun_flow':\n                            show_console(True)\n                            runnner = self.class_instance.run_flow\n                            threading.Thread(target=runnner, args=args, kwargs=kwargs, daemon=True).start()\n                            await sender({'ok': 0}, identifier)\n                            show_console(False)\n                            continue\n\n                        async def _helper_runner():\n                            try:\n                                attr_f = getattr(self.class_instance, name)\n\n                                if asyncio.iscoroutinefunction(attr_f):\n                                    res = await attr_f(*args, **kwargs)\n                                else:\n                                    res = attr_f(*args, **kwargs)\n\n                                if res is None:\n                                    res = {'data': res}\n                                elif isinstance(res, Result):\n                                    if asyncio.iscoroutine(res.get()) or isinstance(res.get(), asyncio.Task):\n                                        res_ = await res.aget()\n                                        res.result.data = res_\n                                    res = json.loads(res.to_api_result().json())\n                                elif isinstance(res, bytes | dict):\n                                    pass\n                                else:\n                                    res = {'data': 'unsupported type', 'type': str(type(res))}\n\n                                get_logger().info(f\"sending response {res} {type(res)}\")\n\n                                await sender(res, identifier)\n                            except Exception as e:\n                                await sender({\"data\": str(e)}, identifier)\n\n                        await _helper_runner()\n                    else:\n                        print(\"Unknown connection data:\", data)\n\n                except Exception as e:\n                    get_logger().warning(Style.RED(f\"An error occurred on {identifier} {str(e)}\"))\n                    if identifier != \"unknown\":\n                        running_dict[\"receive\"][str(identifier)] = False\n                        await self.runner_co(self.on_client_exit,  identifier)\n            await asyncio.sleep(0.1)\n        running_dict[\"server_receiver\"] = False\n        for x in running_dict[\"receive\"]:\n            running_dict[\"receive\"][x] = False\n        running_dict[\"keep_alive_var\"] = False\n        await self.runner_co(self.on_server_exit)\n        app.print(f\"Closing Demon {self._name}\")\n        return Result.ok()\n\n    async def a_exit(self):\n        result = await self.server.aget()\n        await result.get(\"close\")()\n        self.alive = False\n        if asyncio.iscoroutine(self.online):\n            await self.online\n        print(\"Connection result :\", result.get(\"host\"), result.get(\"port\"),\n              \"total connections:\", result.get(\"connections\"))\n</code></pre> <code>__init__(*args, **kwargs)</code> \u00b6 <p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.server = None\n    self.alive = False\n    self.__storedargs = args, kwargs\n    self.async_initialized = False\n</code></pre> <code>__initobj()</code> <code>async</code> \u00b6 <p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre> <code>start_server(connection_type=None)</code> <code>async</code> \u00b6 <p>Start the server using app and the socket manager</p> Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>async def start_server(self, connection_type=None):\n    \"\"\"Start the server using app and the socket manager\"\"\"\n    from toolboxv2.mods.SocketManager import SocketType\n    if connection_type is None:\n        connection_type = SocketType.server\n    app = get_app(from_=\"Starting.Daemon\")\n    print(app.mod_online(\"SocketManager\"), \"SocketManager\")\n    if not app.mod_online(\"SocketManager\"):\n        await app.load_mod(\"SocketManager\")\n    server_result = await app.a_run_any(SOCKETMANAGER.CREATE_SOCKET,\n                                        get_results=True,\n                                        name=self._name,\n                                        host=self.host,\n                                        port=self.port,\n                                        type_id=connection_type,\n                                        max_connections=-1,\n                                        return_full_object=True,\n                                        test_override=self.test_override,\n                                        unix_file=self.unix_socket)\n    if server_result.is_error():\n        raise Exception(f\"Server error: {server_result.print(False)}\")\n    if not server_result.is_data():\n        raise Exception(f\"Server error: {server_result.print(False)}\")\n    self.alive = True\n    self.server = server_result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras","title":"<code>extras</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.BaseWidget","title":"<code>BaseWidget</code>","text":"Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>class BaseWidget:\n    def __init__(self, name: str):\n        self.name = name\n        self.openWidgetsIDs = {}\n        self.onReload = []\n        self.iframes = {}\n\n    def register(self, app, fuction, version=None, name=\"get_widget\", level=1, **kwargs):\n        if version is None:\n            version = app.version\n        app.tb(mod_name=self.name, version=version, request_as_kwarg=True, level=level, api=True, name=name, **kwargs)(\n            fuction)\n\n    def modify_iterator(self, iterator, replace):\n        \"\"\"\n        ['a', 'b'] -&gt; [{replace[0]: 'a',..., replace[len(replace)-1]: 'a'},\n        {replace[0]: 'b',..., replace[len(replace)-1]: 'b'}, ]\n        \"\"\"\n\n        for item in iterator:\n            modified_item = {replace[i]: (self.name if replace[i] == \"name\" else '') + item for i in\n                             range(len(replace))}\n            yield modified_item\n\n    def register2reload(self, *functions):\n        for fuction in functions:\n            def x(r):\n                return fuction(request=r)\n            self.onReload.append(x)\n\n    def reload_guard(self, function):\n        c = None\n        if len(self.onReload) == 0:\n            c = function()\n        return c\n\n    async def oa_reload_guard(self, function):\n        c = None\n        if len(self.onReload) == 0:\n            c = await function() if asyncio.iscoroutinefunction(function) else function()\n        return c\n\n    @staticmethod\n    def get_a_group(asset_name, template=None, file_path=None, a_kwargs=None):\n        if a_kwargs is None:\n            raise ValueError(\"a_kwargs must be specified\")\n        return [{'name': asset_name,\n                 'file_path': file_path,\n                 'kwargs': a_kwargs\n                 } if file_path is not None else {'name': asset_name,\n                                                  'template': template,\n                                                  'kwargs': a_kwargs\n                                                  }]\n\n    def group_generator(self, asset_name: str, iterator: iter, template=None, file_path=None, a_kwargs=None):\n        groups = []\n        work_kwargs = a_kwargs\n        for _i, data in enumerate(iterator):\n            if isinstance(data, dict):\n                work_kwargs = {**a_kwargs, **data}\n            groups.append(self.get_a_group(asset_name, template=template, file_path=file_path, a_kwargs=work_kwargs))\n        return groups\n\n    def asset_loder(self, app, name, asset_id, file_path=None, template=None, iterator=None, **kwargs):\n        a_kwargs = {**{\n            'root': f\"/api/{self.name}\",\n            'WidgetID': asset_id},\n                    **kwargs}\n        asset_name = f\"{name}-{asset_id}\"\n        if iterator is None:\n            group = self.get_a_group(asset_name,\n                                     template=template,\n                                     file_path=file_path,\n                                     a_kwargs=a_kwargs)\n        else:\n            group = self.group_generator(asset_name,\n                                         iterator=iterator,\n                                         template=template,\n                                         file_path=file_path,\n                                         a_kwargs=a_kwargs)\n\n        asset = app.run_any(MINIMALHTML.ADD_COLLECTION_TO_GROUP,\n                            group_name=self.name,\n                            collection={'name': f\"{asset_name}\",\n                                        'group': group},\n                            get_results=True)\n        if asset.is_error():\n            app.run_any(MINIMALHTML.ADD_GROUP, command=self.name)\n            asset = app.run_any(MINIMALHTML.ADD_COLLECTION_TO_GROUP,\n                                group_name=self.name,\n                                collection={'name': f\"{self.name}-{asset_name}\",\n                                            'group': group},\n                                get_results=True)\n        return asset\n\n    def generate_html(self, app, name=\"MainWidget\", asset_id=str(uuid.uuid4())[:4]):\n        return app.run_any(MINIMALHTML.GENERATE_HTML,\n                           group_name=self.name,\n                           collection_name=f\"{name}-{asset_id}\")\n\n    def load_widget(self, app, request, name=\"MainWidget\", asset_id=str(uuid.uuid4())[:4]):\n        app.run_any(MINIMALHTML.ADD_GROUP, command=self.name)\n        self.reload(request)\n        html_widget = self.generate_html(app, name, asset_id)\n        return html_widget[0]['html_element']\n\n    @staticmethod\n    async def get_user_from_request(app, request):\n        from toolboxv2.mods.CloudM import User\n        if request is None:\n            return User()\n        name = request.session.get('live_data', {}).get('user_name', \"Cud be ur name\")\n        if name != \"Cud be ur name\":\n            user = await app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME,\n                                       username=app.config_fh.decode_code(name))\n        else:\n            user = User()\n        return user\n\n    @staticmethod\n    def get_s_id(request):\n        from ..system.types import Result\n        if request is None:\n            return Result.default_internal_error(\"No request specified\")\n        return Result.ok(request.session.get('ID', ''))\n\n    def reload(self, request):\n        [_(request) for _ in self.onReload]\n\n    async def oa_reload(self, request):\n        [_(request) if not asyncio.iscoroutinefunction(_) else await _(request) for _ in self.onReload]\n\n    async def get_widget(self, request, **kwargs):\n        raise NotImplementedError\n\n    def hash_wrapper(self, _id, _salt=''):\n        from ..security.cryp import Code\n        return Code.one_way_hash(text=_id, salt=_salt, pepper=self.name)\n\n    def register_iframe(self, iframe_id: str, src: str, width: str = \"100%\", height: str = \"500px\", **kwargs):\n        \"\"\"\n        Registriert einen iframe mit gegebener ID und Quelle\n\n        Args:\n            iframe_id: Eindeutige ID f\u00fcr den iframe\n            src: URL oder Pfad zur Quelle des iframes\n            width: Breite des iframes (default: \"100%\")\n            height: H\u00f6he des iframes (default: \"500px\")\n            **kwargs: Weitere iframe-Attribute\n        \"\"\"\n        iframe_config = {\n            'src': src,\n            'width': width,\n            'height': height,\n            **kwargs\n        }\n        self.iframes[iframe_id] = iframe_config\n\n    def create_iframe_asset(self, app, iframe_id: str, asset_id: str = None):\n        \"\"\"\n        Erstellt ein Asset f\u00fcr einen registrierten iframe\n\n        Args:\n            app: App-Instanz\n            iframe_id: ID des registrierten iframes\n            asset_id: Optional, spezifische Asset-ID\n        \"\"\"\n        if iframe_id not in self.iframes:\n            raise ValueError(f\"iframe mit ID {iframe_id} nicht registriert\")\n\n        if asset_id is None:\n            asset_id = str(uuid.uuid4())[:4]\n\n        iframe_config = self.iframes[iframe_id]\n        iframe_template = \"\"\"\n        &lt;iframe id=\"{iframe_id}\"\n                src=\"{src}\"\n                width=\"{width}\"\n                height=\"{height}\"\n                frameborder=\"0\"\n                {additional_attrs}&gt;&lt;/iframe&gt;\n        \"\"\".strip()\n\n        # Filtere bekannte Attribute heraus und erstelle String f\u00fcr zus\u00e4tzliche Attribute\n        known_attrs = {'src', 'width', 'height'}\n        additional_attrs = ' '.join(\n            f'{k}=\"{v}\"' for k, v in iframe_config.items()\n            if k not in known_attrs\n        )\n\n        iframe_html = iframe_template.format(\n            iframe_id=iframe_id,\n            src=iframe_config['src'],\n            width=iframe_config['width'],\n            height=iframe_config['height'],\n            additional_attrs=additional_attrs\n        )\n\n        return self.asset_loder(\n            app=app,\n            name=f\"iframe-{iframe_id}\",\n            asset_id=asset_id,\n            template=iframe_html\n        )\n\n    def load_iframe(self, app, iframe_id: str, asset_id: str = None):\n        \"\"\"\n        L\u00e4dt einen registrierten iframe und gibt das HTML-Element zur\u00fcck\n\n        Args:\n            app: App-Instanz\n            iframe_id: ID des registrierten iframes\n            asset_id: Optional, spezifische Asset-ID\n        \"\"\"\n        self.create_iframe_asset(app, iframe_id, asset_id)\n        return self.generate_html(app, f\"iframe-{iframe_id}\", asset_id)[0]['html_element']\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.BaseWidget.create_iframe_asset","title":"<code>create_iframe_asset(app, iframe_id, asset_id=None)</code>","text":"<p>Erstellt ein Asset f\u00fcr einen registrierten iframe</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <p>App-Instanz</p> required <code>iframe_id</code> <code>str</code> <p>ID des registrierten iframes</p> required <code>asset_id</code> <code>str</code> <p>Optional, spezifische Asset-ID</p> <code>None</code> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def create_iframe_asset(self, app, iframe_id: str, asset_id: str = None):\n    \"\"\"\n    Erstellt ein Asset f\u00fcr einen registrierten iframe\n\n    Args:\n        app: App-Instanz\n        iframe_id: ID des registrierten iframes\n        asset_id: Optional, spezifische Asset-ID\n    \"\"\"\n    if iframe_id not in self.iframes:\n        raise ValueError(f\"iframe mit ID {iframe_id} nicht registriert\")\n\n    if asset_id is None:\n        asset_id = str(uuid.uuid4())[:4]\n\n    iframe_config = self.iframes[iframe_id]\n    iframe_template = \"\"\"\n    &lt;iframe id=\"{iframe_id}\"\n            src=\"{src}\"\n            width=\"{width}\"\n            height=\"{height}\"\n            frameborder=\"0\"\n            {additional_attrs}&gt;&lt;/iframe&gt;\n    \"\"\".strip()\n\n    # Filtere bekannte Attribute heraus und erstelle String f\u00fcr zus\u00e4tzliche Attribute\n    known_attrs = {'src', 'width', 'height'}\n    additional_attrs = ' '.join(\n        f'{k}=\"{v}\"' for k, v in iframe_config.items()\n        if k not in known_attrs\n    )\n\n    iframe_html = iframe_template.format(\n        iframe_id=iframe_id,\n        src=iframe_config['src'],\n        width=iframe_config['width'],\n        height=iframe_config['height'],\n        additional_attrs=additional_attrs\n    )\n\n    return self.asset_loder(\n        app=app,\n        name=f\"iframe-{iframe_id}\",\n        asset_id=asset_id,\n        template=iframe_html\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.BaseWidget.load_iframe","title":"<code>load_iframe(app, iframe_id, asset_id=None)</code>","text":"<p>L\u00e4dt einen registrierten iframe und gibt das HTML-Element zur\u00fcck</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <p>App-Instanz</p> required <code>iframe_id</code> <code>str</code> <p>ID des registrierten iframes</p> required <code>asset_id</code> <code>str</code> <p>Optional, spezifische Asset-ID</p> <code>None</code> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def load_iframe(self, app, iframe_id: str, asset_id: str = None):\n    \"\"\"\n    L\u00e4dt einen registrierten iframe und gibt das HTML-Element zur\u00fcck\n\n    Args:\n        app: App-Instanz\n        iframe_id: ID des registrierten iframes\n        asset_id: Optional, spezifische Asset-ID\n    \"\"\"\n    self.create_iframe_asset(app, iframe_id, asset_id)\n    return self.generate_html(app, f\"iframe-{iframe_id}\", asset_id)[0]['html_element']\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.BaseWidget.modify_iterator","title":"<code>modify_iterator(iterator, replace)</code>","text":"<p>['a', 'b'] -&gt; [{replace[0]: 'a',..., replace[len(replace)-1]: 'a'}, {replace[0]: 'b',..., replace[len(replace)-1]: 'b'}, ]</p> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def modify_iterator(self, iterator, replace):\n    \"\"\"\n    ['a', 'b'] -&gt; [{replace[0]: 'a',..., replace[len(replace)-1]: 'a'},\n    {replace[0]: 'b',..., replace[len(replace)-1]: 'b'}, ]\n    \"\"\"\n\n    for item in iterator:\n        modified_item = {replace[i]: (self.name if replace[i] == \"name\" else '') + item for i in\n                         range(len(replace))}\n        yield modified_item\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.BaseWidget.register_iframe","title":"<code>register_iframe(iframe_id, src, width='100%', height='500px', **kwargs)</code>","text":"<p>Registriert einen iframe mit gegebener ID und Quelle</p> <p>Parameters:</p> Name Type Description Default <code>iframe_id</code> <code>str</code> <p>Eindeutige ID f\u00fcr den iframe</p> required <code>src</code> <code>str</code> <p>URL oder Pfad zur Quelle des iframes</p> required <code>width</code> <code>str</code> <p>Breite des iframes (default: \"100%\")</p> <code>'100%'</code> <code>height</code> <code>str</code> <p>H\u00f6he des iframes (default: \"500px\")</p> <code>'500px'</code> <code>**kwargs</code> <p>Weitere iframe-Attribute</p> <code>{}</code> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def register_iframe(self, iframe_id: str, src: str, width: str = \"100%\", height: str = \"500px\", **kwargs):\n    \"\"\"\n    Registriert einen iframe mit gegebener ID und Quelle\n\n    Args:\n        iframe_id: Eindeutige ID f\u00fcr den iframe\n        src: URL oder Pfad zur Quelle des iframes\n        width: Breite des iframes (default: \"100%\")\n        height: H\u00f6he des iframes (default: \"500px\")\n        **kwargs: Weitere iframe-Attribute\n    \"\"\"\n    iframe_config = {\n        'src': src,\n        'width': width,\n        'height': height,\n        **kwargs\n    }\n    self.iframes[iframe_id] = iframe_config\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.Style","title":"<code>Style</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.Style.Spinner","title":"<code>Spinner</code>","text":"<p>Enhanced Spinner with tqdm-like line rendering.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>class Spinner:\n    \"\"\"\n    Enhanced Spinner with tqdm-like line rendering.\n    \"\"\"\n    SYMBOL_SETS = {\n        \"c\": [\"\u25d0\", \"\u25d3\", \"\u25d1\", \"\u25d2\"],\n        \"b\": [\"\u2581\", \"\u2583\", \"\u2584\", \"\u2585\", \"\u2586\", \"\u2587\", \"\u2588\", \"\u2587\", \"\u2586\", \"\u2585\", \"\u2584\", \"\u2583\"],\n        \"d\": [\"\u28fe\", \"\u28fd\", \"\u28fb\", \"\u28bf\", \"\u287f\", \"\u28df\", \"\u28ef\", \"\u28f7\"],\n        \"w\": [\"\ud83c\udf0d\", \"\ud83c\udf0e\", \"\ud83c\udf0f\"],\n        \"s\": [\"\ud83c\udf00   \", \" \ud83c\udf00  \", \"  \ud83c\udf00 \", \"   \ud83c\udf00\", \"  \ud83c\udf00 \", \" \ud83c\udf00  \"],\n        \"+\": [\"+\", \"x\"],\n        \"t\": [\"\u2736\", \"\u2738\", \"\u2739\", \"\u273a\", \"\u2739\", \"\u2737\"]\n    }\n\n    def __init__(\n        self,\n        message: str = \"Loading...\",\n        delay: float = 0.1,\n        symbols=None,\n        count_down: bool = False,\n        time_in_s: float = 0\n    ):\n        \"\"\"Initialize spinner with flexible configuration.\"\"\"\n        # Resolve symbol set.\n        if isinstance(symbols, str):\n            symbols = self.SYMBOL_SETS.get(symbols, None)\n\n        # Default symbols if not provided.\n        if symbols is None:\n            symbols = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n\n        # Test mode symbol set.\n        if 'unittest' in sys.argv[0]:\n            symbols = ['#', '=', '-']\n\n        self.spinner = itertools.cycle(symbols)\n        self.delay = delay\n        self.message = message\n        self.running = False\n        self.spinner_thread = None\n        self.max_t = time_in_s\n        self.contd = count_down\n\n        # Rendering management.\n        self._is_primary = False\n        self._start_time = 0\n\n        # Central manager.\n        self.manager = SpinnerManager()\n\n    def _generate_render_line(self):\n        \"\"\"Generate the primary render line.\"\"\"\n        current_time = time.time()\n        if self.contd:\n            remaining = max(0, self.max_t - (current_time - self._start_time))\n            time_display = f\"{remaining:.2f}\"\n        else:\n            time_display = f\"{current_time - self._start_time:.2f}\"\n\n        symbol = next(self.spinner)\n        return f\"{symbol} {self.message} | {time_display}\"\n\n    def _generate_secondary_info(self):\n        \"\"\"Generate secondary spinner info for additional spinners.\"\"\"\n        return f\"{self.message}\"\n\n    def __enter__(self):\n        \"\"\"Start the spinner.\"\"\"\n        self.running = True\n        self._start_time = time.time()\n        self.manager.register_spinner(self)\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        \"\"\"Stop the spinner.\"\"\"\n        self.running = False\n        self.manager.unregister_spinner(self)\n        # Clear the spinner's line if it was the primary spinner.\n        if self._is_primary:\n            sys.stdout.write(\"\\r\\033[K\")\n            sys.stdout.flush()\n</code></pre> <code>__enter__()</code> \u00b6 <p>Start the spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __enter__(self):\n    \"\"\"Start the spinner.\"\"\"\n    self.running = True\n    self._start_time = time.time()\n    self.manager.register_spinner(self)\n    return self\n</code></pre> <code>__exit__(exc_type, exc_value, exc_traceback)</code> \u00b6 <p>Stop the spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __exit__(self, exc_type, exc_value, exc_traceback):\n    \"\"\"Stop the spinner.\"\"\"\n    self.running = False\n    self.manager.unregister_spinner(self)\n    # Clear the spinner's line if it was the primary spinner.\n    if self._is_primary:\n        sys.stdout.write(\"\\r\\033[K\")\n        sys.stdout.flush()\n</code></pre> <code>__init__(message='Loading...', delay=0.1, symbols=None, count_down=False, time_in_s=0)</code> \u00b6 <p>Initialize spinner with flexible configuration.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __init__(\n    self,\n    message: str = \"Loading...\",\n    delay: float = 0.1,\n    symbols=None,\n    count_down: bool = False,\n    time_in_s: float = 0\n):\n    \"\"\"Initialize spinner with flexible configuration.\"\"\"\n    # Resolve symbol set.\n    if isinstance(symbols, str):\n        symbols = self.SYMBOL_SETS.get(symbols, None)\n\n    # Default symbols if not provided.\n    if symbols is None:\n        symbols = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n\n    # Test mode symbol set.\n    if 'unittest' in sys.argv[0]:\n        symbols = ['#', '=', '-']\n\n    self.spinner = itertools.cycle(symbols)\n    self.delay = delay\n    self.message = message\n    self.running = False\n    self.spinner_thread = None\n    self.max_t = time_in_s\n    self.contd = count_down\n\n    # Rendering management.\n    self._is_primary = False\n    self._start_time = 0\n\n    # Central manager.\n    self.manager = SpinnerManager()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.Style.SpinnerManager","title":"<code>SpinnerManager</code>","text":"<p>Manages multiple spinners to ensure tqdm-like line rendering. Automatically captures SIGINT (Ctrl+C) to stop all spinners.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>class SpinnerManager(metaclass=Singleton):\n    \"\"\"\n    Manages multiple spinners to ensure tqdm-like line rendering.\n    Automatically captures SIGINT (Ctrl+C) to stop all spinners.\n    \"\"\"\n    _instance = None\n\n    def __new__(cls):\n        if not cls._instance:\n            cls._instance = super().__new__(cls)\n            cls._instance._init_manager()\n        return cls._instance\n\n    def _init_manager(self):\n        \"\"\"Initialize spinner management resources and register SIGINT handler.\"\"\"\n        self._spinners = []\n        self._lock = threading.Lock()\n        self._render_thread = None\n        self._should_run = False\n        try:\n            signal.signal(signal.SIGINT, self._signal_handler)\n        except ValueError:\n            print(\"Spinner Manager not in the min Thread no signal possible\")\n            pass\n\n    def _signal_handler(self, signum, frame):\n        \"\"\"Handle SIGINT by stopping all spinners gracefully.\"\"\"\n        with self._lock:\n            for spinner in self._spinners:\n                spinner.running = False\n            self._spinners.clear()\n        self._should_run = False\n        sys.stdout.write(\"\\r\\033[K\")  # Clear the spinner's line.\n        sys.stdout.flush()\n        sys.exit(0)\n\n    def register_spinner(self, spinner):\n        \"\"\"Register a new spinner.\"\"\"\n        with self._lock:\n            # First spinner defines the rendering line.\n            if not self._spinners:\n                spinner._is_primary = True\n            self._spinners.append(spinner)\n            # Start rendering if not already running.\n            if not self._should_run:\n                self._should_run = True\n                self._render_thread = threading.Thread(\n                    target=self._render_loop,\n                    daemon=True\n                )\n                self._render_thread.start()\n\n    def unregister_spinner(self, spinner):\n        \"\"\"Unregister a completed spinner.\"\"\"\n        with self._lock:\n            if spinner in self._spinners:\n                self._spinners.remove(spinner)\n\n    def _render_loop(self):\n        \"\"\"Continuous rendering loop for all active spinners.\"\"\"\n        while self._should_run:\n            if not self._spinners:\n                self._should_run = False\n                break\n\n            with self._lock:\n                # Find primary spinner (first registered).\n                primary_spinner = next((s for s in self._spinners if s._is_primary), None)\n\n                if primary_spinner and primary_spinner.running:\n                    # Render in the same line.\n                    render_line = primary_spinner._generate_render_line()\n\n                    # Append additional spinner info if multiple exist.\n                    if len(self._spinners) &gt; 1:\n                        secondary_info = \" | \".join(\n                            s._generate_secondary_info()\n                            for s in self._spinners\n                            if s is not primary_spinner and s.running\n                        )\n                        render_line += f\" [{secondary_info}]\"\n\n                    # Clear line and write.\n                    try:\n                        sys.stdout.write(\"\\r\" + render_line + \"\\033[K\")\n                        sys.stdout.flush()\n                    except Exception:\n                        self._should_run = False\n\n            time.sleep(0.1)  # Render interval.\n</code></pre> <code>register_spinner(spinner)</code> \u00b6 <p>Register a new spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def register_spinner(self, spinner):\n    \"\"\"Register a new spinner.\"\"\"\n    with self._lock:\n        # First spinner defines the rendering line.\n        if not self._spinners:\n            spinner._is_primary = True\n        self._spinners.append(spinner)\n        # Start rendering if not already running.\n        if not self._should_run:\n            self._should_run = True\n            self._render_thread = threading.Thread(\n                target=self._render_loop,\n                daemon=True\n            )\n            self._render_thread.start()\n</code></pre> <code>unregister_spinner(spinner)</code> \u00b6 <p>Unregister a completed spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def unregister_spinner(self, spinner):\n    \"\"\"Unregister a completed spinner.\"\"\"\n    with self._lock:\n        if spinner in self._spinners:\n            self._spinners.remove(spinner)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.base_widget","title":"<code>base_widget</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.base_widget.BaseWidget","title":"<code>BaseWidget</code>","text":"Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>class BaseWidget:\n    def __init__(self, name: str):\n        self.name = name\n        self.openWidgetsIDs = {}\n        self.onReload = []\n        self.iframes = {}\n\n    def register(self, app, fuction, version=None, name=\"get_widget\", level=1, **kwargs):\n        if version is None:\n            version = app.version\n        app.tb(mod_name=self.name, version=version, request_as_kwarg=True, level=level, api=True, name=name, **kwargs)(\n            fuction)\n\n    def modify_iterator(self, iterator, replace):\n        \"\"\"\n        ['a', 'b'] -&gt; [{replace[0]: 'a',..., replace[len(replace)-1]: 'a'},\n        {replace[0]: 'b',..., replace[len(replace)-1]: 'b'}, ]\n        \"\"\"\n\n        for item in iterator:\n            modified_item = {replace[i]: (self.name if replace[i] == \"name\" else '') + item for i in\n                             range(len(replace))}\n            yield modified_item\n\n    def register2reload(self, *functions):\n        for fuction in functions:\n            def x(r):\n                return fuction(request=r)\n            self.onReload.append(x)\n\n    def reload_guard(self, function):\n        c = None\n        if len(self.onReload) == 0:\n            c = function()\n        return c\n\n    async def oa_reload_guard(self, function):\n        c = None\n        if len(self.onReload) == 0:\n            c = await function() if asyncio.iscoroutinefunction(function) else function()\n        return c\n\n    @staticmethod\n    def get_a_group(asset_name, template=None, file_path=None, a_kwargs=None):\n        if a_kwargs is None:\n            raise ValueError(\"a_kwargs must be specified\")\n        return [{'name': asset_name,\n                 'file_path': file_path,\n                 'kwargs': a_kwargs\n                 } if file_path is not None else {'name': asset_name,\n                                                  'template': template,\n                                                  'kwargs': a_kwargs\n                                                  }]\n\n    def group_generator(self, asset_name: str, iterator: iter, template=None, file_path=None, a_kwargs=None):\n        groups = []\n        work_kwargs = a_kwargs\n        for _i, data in enumerate(iterator):\n            if isinstance(data, dict):\n                work_kwargs = {**a_kwargs, **data}\n            groups.append(self.get_a_group(asset_name, template=template, file_path=file_path, a_kwargs=work_kwargs))\n        return groups\n\n    def asset_loder(self, app, name, asset_id, file_path=None, template=None, iterator=None, **kwargs):\n        a_kwargs = {**{\n            'root': f\"/api/{self.name}\",\n            'WidgetID': asset_id},\n                    **kwargs}\n        asset_name = f\"{name}-{asset_id}\"\n        if iterator is None:\n            group = self.get_a_group(asset_name,\n                                     template=template,\n                                     file_path=file_path,\n                                     a_kwargs=a_kwargs)\n        else:\n            group = self.group_generator(asset_name,\n                                         iterator=iterator,\n                                         template=template,\n                                         file_path=file_path,\n                                         a_kwargs=a_kwargs)\n\n        asset = app.run_any(MINIMALHTML.ADD_COLLECTION_TO_GROUP,\n                            group_name=self.name,\n                            collection={'name': f\"{asset_name}\",\n                                        'group': group},\n                            get_results=True)\n        if asset.is_error():\n            app.run_any(MINIMALHTML.ADD_GROUP, command=self.name)\n            asset = app.run_any(MINIMALHTML.ADD_COLLECTION_TO_GROUP,\n                                group_name=self.name,\n                                collection={'name': f\"{self.name}-{asset_name}\",\n                                            'group': group},\n                                get_results=True)\n        return asset\n\n    def generate_html(self, app, name=\"MainWidget\", asset_id=str(uuid.uuid4())[:4]):\n        return app.run_any(MINIMALHTML.GENERATE_HTML,\n                           group_name=self.name,\n                           collection_name=f\"{name}-{asset_id}\")\n\n    def load_widget(self, app, request, name=\"MainWidget\", asset_id=str(uuid.uuid4())[:4]):\n        app.run_any(MINIMALHTML.ADD_GROUP, command=self.name)\n        self.reload(request)\n        html_widget = self.generate_html(app, name, asset_id)\n        return html_widget[0]['html_element']\n\n    @staticmethod\n    async def get_user_from_request(app, request):\n        from toolboxv2.mods.CloudM import User\n        if request is None:\n            return User()\n        name = request.session.get('live_data', {}).get('user_name', \"Cud be ur name\")\n        if name != \"Cud be ur name\":\n            user = await app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME,\n                                       username=app.config_fh.decode_code(name))\n        else:\n            user = User()\n        return user\n\n    @staticmethod\n    def get_s_id(request):\n        from ..system.types import Result\n        if request is None:\n            return Result.default_internal_error(\"No request specified\")\n        return Result.ok(request.session.get('ID', ''))\n\n    def reload(self, request):\n        [_(request) for _ in self.onReload]\n\n    async def oa_reload(self, request):\n        [_(request) if not asyncio.iscoroutinefunction(_) else await _(request) for _ in self.onReload]\n\n    async def get_widget(self, request, **kwargs):\n        raise NotImplementedError\n\n    def hash_wrapper(self, _id, _salt=''):\n        from ..security.cryp import Code\n        return Code.one_way_hash(text=_id, salt=_salt, pepper=self.name)\n\n    def register_iframe(self, iframe_id: str, src: str, width: str = \"100%\", height: str = \"500px\", **kwargs):\n        \"\"\"\n        Registriert einen iframe mit gegebener ID und Quelle\n\n        Args:\n            iframe_id: Eindeutige ID f\u00fcr den iframe\n            src: URL oder Pfad zur Quelle des iframes\n            width: Breite des iframes (default: \"100%\")\n            height: H\u00f6he des iframes (default: \"500px\")\n            **kwargs: Weitere iframe-Attribute\n        \"\"\"\n        iframe_config = {\n            'src': src,\n            'width': width,\n            'height': height,\n            **kwargs\n        }\n        self.iframes[iframe_id] = iframe_config\n\n    def create_iframe_asset(self, app, iframe_id: str, asset_id: str = None):\n        \"\"\"\n        Erstellt ein Asset f\u00fcr einen registrierten iframe\n\n        Args:\n            app: App-Instanz\n            iframe_id: ID des registrierten iframes\n            asset_id: Optional, spezifische Asset-ID\n        \"\"\"\n        if iframe_id not in self.iframes:\n            raise ValueError(f\"iframe mit ID {iframe_id} nicht registriert\")\n\n        if asset_id is None:\n            asset_id = str(uuid.uuid4())[:4]\n\n        iframe_config = self.iframes[iframe_id]\n        iframe_template = \"\"\"\n        &lt;iframe id=\"{iframe_id}\"\n                src=\"{src}\"\n                width=\"{width}\"\n                height=\"{height}\"\n                frameborder=\"0\"\n                {additional_attrs}&gt;&lt;/iframe&gt;\n        \"\"\".strip()\n\n        # Filtere bekannte Attribute heraus und erstelle String f\u00fcr zus\u00e4tzliche Attribute\n        known_attrs = {'src', 'width', 'height'}\n        additional_attrs = ' '.join(\n            f'{k}=\"{v}\"' for k, v in iframe_config.items()\n            if k not in known_attrs\n        )\n\n        iframe_html = iframe_template.format(\n            iframe_id=iframe_id,\n            src=iframe_config['src'],\n            width=iframe_config['width'],\n            height=iframe_config['height'],\n            additional_attrs=additional_attrs\n        )\n\n        return self.asset_loder(\n            app=app,\n            name=f\"iframe-{iframe_id}\",\n            asset_id=asset_id,\n            template=iframe_html\n        )\n\n    def load_iframe(self, app, iframe_id: str, asset_id: str = None):\n        \"\"\"\n        L\u00e4dt einen registrierten iframe und gibt das HTML-Element zur\u00fcck\n\n        Args:\n            app: App-Instanz\n            iframe_id: ID des registrierten iframes\n            asset_id: Optional, spezifische Asset-ID\n        \"\"\"\n        self.create_iframe_asset(app, iframe_id, asset_id)\n        return self.generate_html(app, f\"iframe-{iframe_id}\", asset_id)[0]['html_element']\n</code></pre> <code>create_iframe_asset(app, iframe_id, asset_id=None)</code> \u00b6 <p>Erstellt ein Asset f\u00fcr einen registrierten iframe</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <p>App-Instanz</p> required <code>iframe_id</code> <code>str</code> <p>ID des registrierten iframes</p> required <code>asset_id</code> <code>str</code> <p>Optional, spezifische Asset-ID</p> <code>None</code> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def create_iframe_asset(self, app, iframe_id: str, asset_id: str = None):\n    \"\"\"\n    Erstellt ein Asset f\u00fcr einen registrierten iframe\n\n    Args:\n        app: App-Instanz\n        iframe_id: ID des registrierten iframes\n        asset_id: Optional, spezifische Asset-ID\n    \"\"\"\n    if iframe_id not in self.iframes:\n        raise ValueError(f\"iframe mit ID {iframe_id} nicht registriert\")\n\n    if asset_id is None:\n        asset_id = str(uuid.uuid4())[:4]\n\n    iframe_config = self.iframes[iframe_id]\n    iframe_template = \"\"\"\n    &lt;iframe id=\"{iframe_id}\"\n            src=\"{src}\"\n            width=\"{width}\"\n            height=\"{height}\"\n            frameborder=\"0\"\n            {additional_attrs}&gt;&lt;/iframe&gt;\n    \"\"\".strip()\n\n    # Filtere bekannte Attribute heraus und erstelle String f\u00fcr zus\u00e4tzliche Attribute\n    known_attrs = {'src', 'width', 'height'}\n    additional_attrs = ' '.join(\n        f'{k}=\"{v}\"' for k, v in iframe_config.items()\n        if k not in known_attrs\n    )\n\n    iframe_html = iframe_template.format(\n        iframe_id=iframe_id,\n        src=iframe_config['src'],\n        width=iframe_config['width'],\n        height=iframe_config['height'],\n        additional_attrs=additional_attrs\n    )\n\n    return self.asset_loder(\n        app=app,\n        name=f\"iframe-{iframe_id}\",\n        asset_id=asset_id,\n        template=iframe_html\n    )\n</code></pre> <code>load_iframe(app, iframe_id, asset_id=None)</code> \u00b6 <p>L\u00e4dt einen registrierten iframe und gibt das HTML-Element zur\u00fcck</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <p>App-Instanz</p> required <code>iframe_id</code> <code>str</code> <p>ID des registrierten iframes</p> required <code>asset_id</code> <code>str</code> <p>Optional, spezifische Asset-ID</p> <code>None</code> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def load_iframe(self, app, iframe_id: str, asset_id: str = None):\n    \"\"\"\n    L\u00e4dt einen registrierten iframe und gibt das HTML-Element zur\u00fcck\n\n    Args:\n        app: App-Instanz\n        iframe_id: ID des registrierten iframes\n        asset_id: Optional, spezifische Asset-ID\n    \"\"\"\n    self.create_iframe_asset(app, iframe_id, asset_id)\n    return self.generate_html(app, f\"iframe-{iframe_id}\", asset_id)[0]['html_element']\n</code></pre> <code>modify_iterator(iterator, replace)</code> \u00b6 <p>['a', 'b'] -&gt; [{replace[0]: 'a',..., replace[len(replace)-1]: 'a'}, {replace[0]: 'b',..., replace[len(replace)-1]: 'b'}, ]</p> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def modify_iterator(self, iterator, replace):\n    \"\"\"\n    ['a', 'b'] -&gt; [{replace[0]: 'a',..., replace[len(replace)-1]: 'a'},\n    {replace[0]: 'b',..., replace[len(replace)-1]: 'b'}, ]\n    \"\"\"\n\n    for item in iterator:\n        modified_item = {replace[i]: (self.name if replace[i] == \"name\" else '') + item for i in\n                         range(len(replace))}\n        yield modified_item\n</code></pre> <code>register_iframe(iframe_id, src, width='100%', height='500px', **kwargs)</code> \u00b6 <p>Registriert einen iframe mit gegebener ID und Quelle</p> <p>Parameters:</p> Name Type Description Default <code>iframe_id</code> <code>str</code> <p>Eindeutige ID f\u00fcr den iframe</p> required <code>src</code> <code>str</code> <p>URL oder Pfad zur Quelle des iframes</p> required <code>width</code> <code>str</code> <p>Breite des iframes (default: \"100%\")</p> <code>'100%'</code> <code>height</code> <code>str</code> <p>H\u00f6he des iframes (default: \"500px\")</p> <code>'500px'</code> <code>**kwargs</code> <p>Weitere iframe-Attribute</p> <code>{}</code> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def register_iframe(self, iframe_id: str, src: str, width: str = \"100%\", height: str = \"500px\", **kwargs):\n    \"\"\"\n    Registriert einen iframe mit gegebener ID und Quelle\n\n    Args:\n        iframe_id: Eindeutige ID f\u00fcr den iframe\n        src: URL oder Pfad zur Quelle des iframes\n        width: Breite des iframes (default: \"100%\")\n        height: H\u00f6he des iframes (default: \"500px\")\n        **kwargs: Weitere iframe-Attribute\n    \"\"\"\n    iframe_config = {\n        'src': src,\n        'width': width,\n        'height': height,\n        **kwargs\n    }\n    self.iframes[iframe_id] = iframe_config\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.gist_control","title":"<code>gist_control</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.gist_control.GistLoader","title":"<code>GistLoader</code>","text":"Source code in <code>toolboxv2/utils/extras/gist_control.py</code> <pre><code>class GistLoader:\n    def __init__(self, gist_url):\n        self.gist_url = gist_url\n        self.module_code = None\n\n    def load_module(self, module_name):\n        \"\"\"L\u00e4dt das Modul mit dem gegebenen Namen.\"\"\"\n        if self.module_code is None:\n            self.module_code = self._fetch_gist_content()\n\n        # Erstelle ein neues Modul\n        module = importlib.util.module_from_spec(self.get_spec(module_name))\n        exec(self.module_code, module.__dict__)\n        return module\n\n    def get_spec(self, module_name):\n        \"\"\"Gibt die Modul-Specifikation zur\u00fcck.\"\"\"\n        return ModuleSpec(module_name, self)\n\n    def get_filename(self, module_name):\n        return f\"&lt;gist:{self.gist_url}&gt;\"\n\n    def _fetch_gist_content(self):\n        \"\"\"L\u00e4dt den Inhalt des Gists von der GitHub API herunter.\"\"\"\n        gist_id = self.gist_url.split('/')[-1]\n        api_url = f\"https://api.github.com/gists/{gist_id}\"\n\n        response = requests.get(api_url)\n\n        if response.status_code == 200:\n            gist_data = response.json()\n            first_file = next(iter(gist_data['files'].values()))\n            return first_file['content']\n        else:\n            raise Exception(f\"Failed to fetch gist: {response.status_code}\")\n</code></pre> <code>get_spec(module_name)</code> \u00b6 <p>Gibt die Modul-Specifikation zur\u00fcck.</p> Source code in <code>toolboxv2/utils/extras/gist_control.py</code> <pre><code>def get_spec(self, module_name):\n    \"\"\"Gibt die Modul-Specifikation zur\u00fcck.\"\"\"\n    return ModuleSpec(module_name, self)\n</code></pre> <code>load_module(module_name)</code> \u00b6 <p>L\u00e4dt das Modul mit dem gegebenen Namen.</p> Source code in <code>toolboxv2/utils/extras/gist_control.py</code> <pre><code>def load_module(self, module_name):\n    \"\"\"L\u00e4dt das Modul mit dem gegebenen Namen.\"\"\"\n    if self.module_code is None:\n        self.module_code = self._fetch_gist_content()\n\n    # Erstelle ein neues Modul\n    module = importlib.util.module_from_spec(self.get_spec(module_name))\n    exec(self.module_code, module.__dict__)\n    return module\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.helper_test_functions","title":"<code>helper_test_functions</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.helper_test_functions.generate_edge_value","title":"<code>generate_edge_value(param_type)</code>","text":"<p>Generiert Edge-Case-Werte basierend auf dem Parametertyp.</p> Source code in <code>toolboxv2/utils/extras/helper_test_functions.py</code> <pre><code>def generate_edge_value(param_type: Any) -&gt; Any:\n    \"\"\"\n    Generiert Edge-Case-Werte basierend auf dem Parametertyp.\n    \"\"\"\n    if param_type in [int, float]:\n        return -999  # Beispiel f\u00fcr negative Zahlen\n    elif param_type == str:\n        return \"test \" * 100  # Lange zuf\u00e4llige Strings\n    # F\u00fcgen Sie hier weitere Bedingungen f\u00fcr andere Datentypen hinzu\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.helper_test_functions.generate_normal_value","title":"<code>generate_normal_value(param_type)</code>","text":"<p>Generiert normale Werte basierend auf dem Parametertyp.</p> Source code in <code>toolboxv2/utils/extras/helper_test_functions.py</code> <pre><code>def generate_normal_value(param_type: Any) -&gt; Any:\n    \"\"\"\n    Generiert normale Werte basierend auf dem Parametertyp.\n    \"\"\"\n    if param_type in [int, float]:\n        return random.randint(0, 100)  # Zuf\u00e4llige normale Zahlen\n    elif param_type == str:\n        return \"test\" # Zuf\u00e4lliges Wort\n    # F\u00fcgen Sie hier weitere Bedingungen f\u00fcr andere Datentypen hinzu\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.keword_matcher","title":"<code>keword_matcher</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.keword_matcher.calculate_keyword_score","title":"<code>calculate_keyword_score(text, keywords)</code>","text":"<p>Berechnet den Keyword-Score basierend auf der H\u00e4ufigkeit der Keywords im Text. Case-insensitive und optimiert f\u00fcr Geschwindigkeit.</p> <p>:param text: Eingabetext als String :param keywords: Set von Keywords :return: Gesamt-Score als Integer</p> Source code in <code>toolboxv2/utils/extras/keword_matcher.py</code> <pre><code>def calculate_keyword_score(text: str, keywords: set[str]) -&gt; int:\n    \"\"\"\n    Berechnet den Keyword-Score basierend auf der H\u00e4ufigkeit der Keywords im Text.\n    Case-insensitive und optimiert f\u00fcr Geschwindigkeit.\n\n    :param text: Eingabetext als String\n    :param keywords: Set von Keywords\n    :return: Gesamt-Score als Integer\n    \"\"\"\n    # Vorverarbeitung der Keywords\n    keyword_pattern = re.compile(\n        r'\\b(' + '|'.join(re.escape(k.lower()) for k in keywords) + r')\\b',\n        flags=re.IGNORECASE\n    )\n\n    # Erstelle Frequenz-W\u00f6rterbuch\n    freq_dict = defaultdict(int)\n\n    # Finde alle \u00dcbereinstimmungen\n    matches = keyword_pattern.findall(text.lower())\n\n    # Z\u00e4hle die Treffer\n    for match in matches:\n        freq_dict[match.lower()] += 1\n\n    # Berechne Gesamt-Score\n    total_score = sum(freq_dict.values())\n\n    return total_score\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.keword_matcher.calculate_weighted_score","title":"<code>calculate_weighted_score(text, keyword_weights)</code>","text":"<p>Berechnet gewichteten Score mit unterschiedlichen Gewichten pro Keyword</p> <p>:param text: Eingabetext :param keyword_weights: Dictionary mit {Keyword: Gewicht} :return: Gewichteter Gesamt-Score</p> Source code in <code>toolboxv2/utils/extras/keword_matcher.py</code> <pre><code>def calculate_weighted_score(text: str, keyword_weights: dict or list) -&gt; float:\n    \"\"\"\n    Berechnet gewichteten Score mit unterschiedlichen Gewichten pro Keyword\n\n    :param text: Eingabetext\n    :param keyword_weights: Dictionary mit {Keyword: Gewicht}\n    :return: Gewichteter Gesamt-Score\n    \"\"\"\n    total = 0.0\n    text_lower = text.lower()\n\n    if isinstance(keyword_weights, list):\n        keyword_weights = {k:v for k, v in keyword_weights}\n\n    for keyword, weight in keyword_weights.items():\n        count = len(re.findall(r'\\b' + re.escape(keyword.lower()) + r'\\b', text_lower))\n        total += count * weight\n\n    return round(total, 2)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.keword_matcher.extract_keywords","title":"<code>extract_keywords(text, max_len=-1, min_word_length=3, with_weights=False, remove_stopwords=True, stopwords=True)</code>","text":"<p>Extrahiert Keywords mit optionaler Frequenzgewichtung</p> <p>:param text: Eingabetext :param max_len: Maximale Anzahl Keywords (-1 = alle) :param min_word_length: Minimale Wortl\u00e4nge :param with_weights: Gibt Wort+Frequenz zur\u00fcck wenn True :param remove_stopwords: Filtert deutsche Stopw\u00f6rter :param german_stopwords: Verwendet deutsche Standard-Stopw\u00f6rter :return: Keywords oder (Keyword, H\u00e4ufigkeit) Paare</p> Source code in <code>toolboxv2/utils/extras/keword_matcher.py</code> <pre><code>def extract_keywords(\n    text: str,\n    max_len: int = -1,\n    min_word_length: int = 3,\n    with_weights: bool = False,\n    remove_stopwords: bool = True,\n    stopwords: bool = True\n) -&gt; list[str] | list[tuple[str, int]]:\n    \"\"\"\n    Extrahiert Keywords mit optionaler Frequenzgewichtung\n\n    :param text: Eingabetext\n    :param max_len: Maximale Anzahl Keywords (-1 = alle)\n    :param min_word_length: Minimale Wortl\u00e4nge\n    :param with_weights: Gibt Wort+Frequenz zur\u00fcck wenn True\n    :param remove_stopwords: Filtert deutsche Stopw\u00f6rter\n    :param german_stopwords: Verwendet deutsche Standard-Stopw\u00f6rter\n    :return: Keywords oder (Keyword, H\u00e4ufigkeit) Paare\n    \"\"\"\n\n    # Deutsche Basis-Stopw\u00f6rter\n    DEFAULT_STOPWORDS = STOPWORDS if stopwords else set()\n\n    # Text vorverarbeiten\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Worte filtern\n    filtered_words = [\n        word for word in words\n        if len(word) &gt; min_word_length\n           and (not remove_stopwords or word not in DEFAULT_STOPWORDS)\n    ]\n\n    # Frequenzanalyse\n    word_counts = defaultdict(int)\n    for word in filtered_words:\n        word_counts[word] += 1\n\n    # Sortierung: Zuerst H\u00e4ufigkeit, dann alphabetisch\n    sorted_words = sorted(\n        word_counts.items(),\n        key=lambda x: (-x[1], x[0])\n    )\n\n    # L\u00e4ngenbegrenzung\n    if max_len == -1:\n        max_len = None\n    result = sorted_words[:max_len]\n\n    return result if with_weights else [word for word, _ in result]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.reqbuilder","title":"<code>reqbuilder</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.reqbuilder.generate_requirements","title":"<code>generate_requirements(folder, output_file)</code>","text":"<p>Generates requirements.txt for the specified folder using pipreqs.</p> Source code in <code>toolboxv2/utils/extras/reqbuilder.py</code> <pre><code>def generate_requirements(folder: str, output_file: str):\n    \"\"\"Generates requirements.txt for the specified folder using pipreqs.\"\"\"\n    print(folder, output_file, os.path.abspath(os.curdir))\n    try:\n        from pipreqs.pipreqs import get_all_imports\n    except ImportError:\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pipreqs\"], check=True)\n    from pipreqs.pipreqs import get_all_imports\n    imports = set(get_all_imports(os.path.abspath(folder)))\n    imports.remove('toolboxv2') if 'toolboxv2' in imports else None\n    with open(os.path.abspath(output_file), \"w\") as f:\n        f.write(\"\\n\".join(imports))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.reqbuilder.run_pipeline","title":"<code>run_pipeline(base_dir)</code>","text":"<p>Runs the entire pipeline to generate requirements files.</p> Source code in <code>toolboxv2/utils/extras/reqbuilder.py</code> <pre><code>def run_pipeline(base_dir: str):\n    \"\"\"Runs the entire pipeline to generate requirements files.\"\"\"\n    toolbox_path = os.path.join(base_dir, \"toolboxv2\")\n    utils_path = os.path.join(toolbox_path, \"utils\")\n    mini_req_file = os.path.join(base_dir, \"requirements_mini.txt\")\n    extras_req_file = os.path.join(base_dir, \"requirements_tests.txt\")\n\n    # Step 1: Generate minimal requirements\n    print(\"Step 1/2: \")\n    generate_requirements(utils_path, mini_req_file)\n\n    # Step 2: Generate extended requirements\n    print(\"Step 2/2: \")\n    extras_path = os.path.join(toolbox_path, \"tests\")\n    generate_requirements(extras_path, extras_req_file)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.proxy","title":"<code>proxy</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.proxy.ProxyUtil","title":"<code>ProxyUtil</code>","text":"Source code in <code>toolboxv2/utils/proxy/prox_util.py</code> <pre><code>class ProxyUtil:\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.__storedargs = args, kwargs\n        self.async_initialized = False\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        # assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n\n    async def __ainit__(self, class_instance: Any, host='0.0.0.0', port=6587, timeout=6,\n                        app: (App or AppType) | None = None,\n                        remote_functions=None, peer=False, name='ProxyApp-client', do_connect=True, unix_socket=False,\n                        test_override=False):\n        self.class_instance = class_instance\n        self.client = None\n        self.test_override = test_override\n        self.port = port\n        self.host = host\n        self.timeout = timeout\n        if app is None:\n            app = get_app(\"ProxyUtil\")\n        self.app = app\n        self._name = name\n        self.unix_socket = unix_socket\n        if remote_functions is None:\n            remote_functions = [\"run_any\", \"a_run_any\", \"remove_mod\", \"save_load\", \"exit_main\", \"show_console\", \"hide_console\",\n                                \"rrun_flow\",\n                                \"get_autocompletion_dict\",\n                                \"exit_main\", \"watch_mod\"]\n        self.remote_functions = remote_functions\n\n        from toolboxv2.mods.SocketManager import SocketType\n        self.connection_type = SocketType.client\n        if peer:\n            self.connection_type = SocketType.peer\n        if do_connect:\n            await self.connect()\n\n    async def connect(self):\n        client_result = await self.app.a_run_local(SOCKETMANAGER.CREATE_SOCKET,\n                                           get_results=True,\n                                           name=self._name,\n                                           host=self.host,\n                                           port=self.port,\n                                           type_id=self.connection_type,\n                                           max_connections=-1,\n                                           return_full_object=True,\n                                           test_override=self.test_override,\n                                           unix_file=self.unix_socket)\n\n        if client_result.is_error():\n            raise Exception(f\"Client {self._name} error: {client_result.print(False)}\")\n        if not client_result.is_data():\n            raise Exception(f\"Client {self._name} error: {client_result.print(False)}\")\n        # 'socket': socket,\n        # 'receiver_socket': r_socket,\n        # 'host': host,\n        # 'port': port,\n        # 'p2p-port': endpoint_port,\n        # 'sender': send,\n        # 'receiver_queue': receiver_queue,\n        # 'connection_error': connection_error,\n        # 'receiver_thread': s_thread,\n        # 'keepalive_thread': keep_alive_thread,\n        # 'running_dict': running_dict,\n        # 'client_to_receiver_thread': to_receive,\n        # 'client_receiver_threads': threeds,\n        result = await client_result.aget()\n        if result is None or result.get('connection_error') != 0:\n            raise Exception(f\"Client {self._name} error: {client_result.print(False)}\")\n        self.client = Result.ok(result)\n\n    async def disconnect(self):\n        time.sleep(1)\n        close = self.client.get(\"close\")\n        await close()\n        self.client = None\n\n    async def reconnect(self):\n        if self.client is not None:\n            await self.disconnect()\n        await self.connect()\n\n    async def verify(self, message=b\"verify\"):\n        await asyncio.sleep(1)\n        # self.client.get('sender')({'keepalive': 0})\n        await self.client.get('sender')(message)\n\n    def __getattr__(self, name):\n\n        # print(f\"ProxyApp: {name}, {self.client is None}\")\n        if name == \"on_exit\":\n            return self.disconnect\n        if name == \"rc\":\n            return self.reconnect\n\n        if name == \"r\":\n            try:\n                return self.client.get('receiver_queue').get(timeout=self.timeout)\n            except:\n                return \"No data\"\n\n        app_attr = getattr(self.class_instance, name)\n\n        async def method(*args, **kwargs):\n            # if name == 'run_any':\n            #     print(\"method\", name, kwargs.get('get_results', False), args[0])\n            if self.client is None:\n                await self.reconnect()\n            if kwargs.get('spec', '-') == 'app':\n                if asyncio.iscoroutinefunction(app_attr):\n                    return await app_attr(*args, **kwargs)\n                return app_attr(*args, **kwargs)\n            try:\n                if name in self.remote_functions:\n                    if (name == 'run_any' or name == 'a_run_any') and not kwargs.get('get_results', False):\n                        if asyncio.iscoroutinefunction(app_attr):\n                            return await app_attr(*args, **kwargs)\n                        return app_attr(*args, **kwargs)\n                    if (name == 'run_any' or name == 'a_run_any') and kwargs.get('get_results', False):\n                        if isinstance(args[0], Enum):\n                            args = (args[0].__class__.NAME.value, args[0].value), args[1:]\n                    self.app.sprint(f\"Calling method {name}, {args=}, {kwargs}=\")\n                    await self.client.get('sender')({'name': name, 'args': args, 'kwargs': kwargs})\n                    while Spinner(\"Waiting for result\"):\n                        try:\n                            data = self.client.get('receiver_queue').get(timeout=self.timeout)\n                            if isinstance(data, dict) and 'identifier' in data:\n                                del data[\"identifier\"]\n                            if 'error' in data and 'origin' in data and 'result' in data and 'info' in data:\n                                data = ApiResult(**data).as_result()\n                            return data\n                        except:\n                            print(\"No data look later with class_instance.r\")\n                            return Result.default_internal_error(\"No data received from Demon.\"\n                                                                 \" uns class_instance.r to get data later\")\n            except:\n                if self.client.get('socket') is None:\n                    self.client = None\n            return app_attr(*args, **kwargs)\n\n        if callable(app_attr) and name in self.remote_functions and self.client is not None:\n            return method\n        return app_attr\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.proxy.ProxyUtil.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/proxy/prox_util.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.__storedargs = args, kwargs\n    self.async_initialized = False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.proxy.ProxyUtil.__initobj","title":"<code>__initobj()</code>  <code>async</code>","text":"<p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/proxy/prox_util.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    # assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.proxy.prox_util","title":"<code>prox_util</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.proxy.prox_util.ProxyUtil","title":"<code>ProxyUtil</code>","text":"Source code in <code>toolboxv2/utils/proxy/prox_util.py</code> <pre><code>class ProxyUtil:\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.__storedargs = args, kwargs\n        self.async_initialized = False\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        # assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n\n    async def __ainit__(self, class_instance: Any, host='0.0.0.0', port=6587, timeout=6,\n                        app: (App or AppType) | None = None,\n                        remote_functions=None, peer=False, name='ProxyApp-client', do_connect=True, unix_socket=False,\n                        test_override=False):\n        self.class_instance = class_instance\n        self.client = None\n        self.test_override = test_override\n        self.port = port\n        self.host = host\n        self.timeout = timeout\n        if app is None:\n            app = get_app(\"ProxyUtil\")\n        self.app = app\n        self._name = name\n        self.unix_socket = unix_socket\n        if remote_functions is None:\n            remote_functions = [\"run_any\", \"a_run_any\", \"remove_mod\", \"save_load\", \"exit_main\", \"show_console\", \"hide_console\",\n                                \"rrun_flow\",\n                                \"get_autocompletion_dict\",\n                                \"exit_main\", \"watch_mod\"]\n        self.remote_functions = remote_functions\n\n        from toolboxv2.mods.SocketManager import SocketType\n        self.connection_type = SocketType.client\n        if peer:\n            self.connection_type = SocketType.peer\n        if do_connect:\n            await self.connect()\n\n    async def connect(self):\n        client_result = await self.app.a_run_local(SOCKETMANAGER.CREATE_SOCKET,\n                                           get_results=True,\n                                           name=self._name,\n                                           host=self.host,\n                                           port=self.port,\n                                           type_id=self.connection_type,\n                                           max_connections=-1,\n                                           return_full_object=True,\n                                           test_override=self.test_override,\n                                           unix_file=self.unix_socket)\n\n        if client_result.is_error():\n            raise Exception(f\"Client {self._name} error: {client_result.print(False)}\")\n        if not client_result.is_data():\n            raise Exception(f\"Client {self._name} error: {client_result.print(False)}\")\n        # 'socket': socket,\n        # 'receiver_socket': r_socket,\n        # 'host': host,\n        # 'port': port,\n        # 'p2p-port': endpoint_port,\n        # 'sender': send,\n        # 'receiver_queue': receiver_queue,\n        # 'connection_error': connection_error,\n        # 'receiver_thread': s_thread,\n        # 'keepalive_thread': keep_alive_thread,\n        # 'running_dict': running_dict,\n        # 'client_to_receiver_thread': to_receive,\n        # 'client_receiver_threads': threeds,\n        result = await client_result.aget()\n        if result is None or result.get('connection_error') != 0:\n            raise Exception(f\"Client {self._name} error: {client_result.print(False)}\")\n        self.client = Result.ok(result)\n\n    async def disconnect(self):\n        time.sleep(1)\n        close = self.client.get(\"close\")\n        await close()\n        self.client = None\n\n    async def reconnect(self):\n        if self.client is not None:\n            await self.disconnect()\n        await self.connect()\n\n    async def verify(self, message=b\"verify\"):\n        await asyncio.sleep(1)\n        # self.client.get('sender')({'keepalive': 0})\n        await self.client.get('sender')(message)\n\n    def __getattr__(self, name):\n\n        # print(f\"ProxyApp: {name}, {self.client is None}\")\n        if name == \"on_exit\":\n            return self.disconnect\n        if name == \"rc\":\n            return self.reconnect\n\n        if name == \"r\":\n            try:\n                return self.client.get('receiver_queue').get(timeout=self.timeout)\n            except:\n                return \"No data\"\n\n        app_attr = getattr(self.class_instance, name)\n\n        async def method(*args, **kwargs):\n            # if name == 'run_any':\n            #     print(\"method\", name, kwargs.get('get_results', False), args[0])\n            if self.client is None:\n                await self.reconnect()\n            if kwargs.get('spec', '-') == 'app':\n                if asyncio.iscoroutinefunction(app_attr):\n                    return await app_attr(*args, **kwargs)\n                return app_attr(*args, **kwargs)\n            try:\n                if name in self.remote_functions:\n                    if (name == 'run_any' or name == 'a_run_any') and not kwargs.get('get_results', False):\n                        if asyncio.iscoroutinefunction(app_attr):\n                            return await app_attr(*args, **kwargs)\n                        return app_attr(*args, **kwargs)\n                    if (name == 'run_any' or name == 'a_run_any') and kwargs.get('get_results', False):\n                        if isinstance(args[0], Enum):\n                            args = (args[0].__class__.NAME.value, args[0].value), args[1:]\n                    self.app.sprint(f\"Calling method {name}, {args=}, {kwargs}=\")\n                    await self.client.get('sender')({'name': name, 'args': args, 'kwargs': kwargs})\n                    while Spinner(\"Waiting for result\"):\n                        try:\n                            data = self.client.get('receiver_queue').get(timeout=self.timeout)\n                            if isinstance(data, dict) and 'identifier' in data:\n                                del data[\"identifier\"]\n                            if 'error' in data and 'origin' in data and 'result' in data and 'info' in data:\n                                data = ApiResult(**data).as_result()\n                            return data\n                        except:\n                            print(\"No data look later with class_instance.r\")\n                            return Result.default_internal_error(\"No data received from Demon.\"\n                                                                 \" uns class_instance.r to get data later\")\n            except:\n                if self.client.get('socket') is None:\n                    self.client = None\n            return app_attr(*args, **kwargs)\n\n        if callable(app_attr) and name in self.remote_functions and self.client is not None:\n            return method\n        return app_attr\n</code></pre> <code>__init__(*args, **kwargs)</code> \u00b6 <p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/proxy/prox_util.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.__storedargs = args, kwargs\n    self.async_initialized = False\n</code></pre> <code>__initobj()</code> <code>async</code> \u00b6 <p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/proxy/prox_util.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    # assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security","title":"<code>security</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.security.Code","title":"<code>Code</code>","text":"Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>class Code:\n\n    @staticmethod\n    def DK():\n        return DEVICE_KEY\n\n    def decode_code(self, encrypted_data, key=None):\n\n        if not isinstance(encrypted_data, str):\n            encrypted_data = str(encrypted_data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.decrypt_symmetric(encrypted_data, key)\n\n    def encode_code(self, data, key=None):\n\n        if not isinstance(data, str):\n            data = str(data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.encrypt_symmetric(data, key)\n\n    @staticmethod\n    def generate_seed() -&gt; int:\n        \"\"\"\n        Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n        Returns:\n            int: Eine zuf\u00e4llige Zahl.\n        \"\"\"\n        return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n\n    @staticmethod\n    def one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n        \"\"\"\n        Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n        Args:\n            text (str): Der zu hashende Text.\n            salt (str): Der Salt-Wert.\n            pepper (str): Der Pepper-Wert.\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            str: Der resultierende Hash-Wert.\n        \"\"\"\n        return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n\n    @staticmethod\n    def generate_symmetric_key() -&gt; str:\n        \"\"\"\n        Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n        Returns:\n            str: Der generierte Schl\u00fcssel.\n        \"\"\"\n        return Fernet.generate_key().decode()\n\n    @staticmethod\n    def encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        if isinstance(text, str):\n            text = text.encode()\n\n        try:\n            fernet = Fernet(key.encode())\n            return fernet.encrypt(text).decode()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n            return \"Error encrypt\"\n\n    @staticmethod\n    def decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n            to_str (bool): default true returns str if false returns bytes\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n\n        if isinstance(key, str):\n            key = key.encode()\n\n        #try:\n        fernet = Fernet(key)\n        text_b = fernet.decrypt(encrypted_text)\n        if not to_str:\n            return text_b\n        return text_b.decode()\n        # except Exception as e:\n        #     get_logger().error(f\"Error decrypt_symmetric {e}\")\n        #     if not mute:\n        #         raise e\n        #     if not to_str:\n        #         return f\"Error decoding\".encode()\n        #     return f\"Error decoding\"\n\n    @staticmethod\n    def generate_asymmetric_keys() -&gt; (str, str):\n        \"\"\"\n        Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n        Args:\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n        \"\"\"\n        private_key = rsa.generate_private_key(\n            public_exponent=65537,\n            key_size=2048 * 3,\n        )\n        public_key = private_key.public_key()\n\n        # Serialisieren der Schl\u00fcssel\n        pem_private_key = private_key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.NoEncryption()\n        ).decode()\n\n        pem_public_key = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        ).decode()\n\n        return pem_public_key, pem_private_key\n\n    @staticmethod\n    def save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n        \"\"\"\n        Speichert die generierten Schl\u00fcssel in separate Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n        Args:\n            public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n            private_key (str): Der private Schl\u00fcssel im PEM-Format\n            directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n        \"\"\"\n        # Erstelle das Verzeichnis, falls es nicht existiert\n        os.makedirs(directory, exist_ok=True)\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n        encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n        # Speichere den \u00f6ffentlichen Schl\u00fcssel\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        with open(public_key_path, \"w\") as f:\n            f.write(public_key)\n\n        # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n        with open(private_key_path, \"w\") as f:\n            f.write(encrypted_private_key)\n\n        print(\"Saved keys in \", public_key_path)\n\n    @staticmethod\n    def load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n        \"\"\"\n        L\u00e4dt die Schl\u00fcssel aus den Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n        Args:\n            directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n        Raises:\n            FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n        \"\"\"\n        # Pfade zu den Schl\u00fcsseldateien\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n\n        # Pr\u00fcfe ob die Dateien existieren\n        if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n            return \"\", \"\"\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Lade den \u00f6ffentlichen Schl\u00fcssel\n        with open(public_key_path) as f:\n            public_key = f.read()\n\n        # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n        with open(private_key_path) as f:\n            encrypted_private_key = f.read()\n            private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n        return public_key, private_key\n\n    @staticmethod\n    def encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        # try:\n        #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        #  except Exception as e:\n        #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            encrypted = public_key.encrypt(\n                text.encode(),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return encrypted.hex()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_asymmetric {e}\")\n            return \"Invalid\"\n\n    @staticmethod\n    def decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n        Args:\n            encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n            private_key_str (str): Der private Schl\u00fcssel als String.\n\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            decrypted = private_key.decrypt(\n                bytes.fromhex(encrypted_text_hex),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return decrypted.decode()\n\n        except Exception as e:\n            get_logger().error(f\"Error decrypt_asymmetric {e}\")\n        return \"Invalid\"\n\n    @staticmethod\n    def verify_signature(signature: str or bytes, message: str or bytes, public_key_str: str,\n                         salt_length=padding.PSS.MAX_LENGTH) -&gt; bool:\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                padding=padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                algorithm=hashes.SHA512()\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def verify_signature_web_algo(signature: str or bytes, message: str or bytes, public_key_str: str,\n                                  algo: int = -512) -&gt; bool:\n        signature_algorithm = ECDSA(hashes.SHA512())\n        if algo != -512:\n            signature_algorithm = ECDSA(hashes.SHA256())\n\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                # padding=padding.PSS(\n                #    mgf=padding.MGF1(hashes.SHA512()),\n                #    salt_length=padding.PSS.MAX_LENGTH\n                # ),\n                signature_algorithm=signature_algorithm\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def create_signature(message: str, private_key_str: str, salt_length=padding.PSS.MAX_LENGTH,\n                         row=False) -&gt; str or bytes:\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            signature = private_key.sign(\n                message.encode(),\n                padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                hashes.SHA512()\n            )\n            if row:\n                return signature\n            return base64.b64encode(signature).decode()\n        except Exception as e:\n            get_logger().error(f\"Error create_signature {e}\")\n            print(e)\n        return \"Invalid Key\"\n\n    @staticmethod\n    def pem_to_public_key(pem_key: str):\n        \"\"\"\n        Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n        Args:\n            pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n        Returns:\n            PublicKey: Das PublicKey-Objekt.\n        \"\"\"\n        public_key = serialization.load_pem_public_key(pem_key.encode())\n        return public_key\n\n    @staticmethod\n    def public_key_to_pem(public_key: RSAPublicKey):\n        \"\"\"\n        Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n        Args:\n            public_key (PublicKey): Das PublicKey-Objekt.\n\n        Returns:\n            str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n        \"\"\"\n        pem = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n        return pem.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.decrypt_asymmetric","title":"<code>decrypt_asymmetric(encrypted_text_hex, private_key_str)</code>  <code>staticmethod</code>","text":"<p>Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text_hex</code> <code>str</code> <p>Der verschl\u00fcsselte Text als Hex-String.</p> required <code>private_key_str</code> <code>str</code> <p>Der private Schl\u00fcssel als String.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n    Args:\n        encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n        private_key_str (str): Der private Schl\u00fcssel als String.\n\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n    try:\n        private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n        decrypted = private_key.decrypt(\n            bytes.fromhex(encrypted_text_hex),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return decrypted.decode()\n\n    except Exception as e:\n        get_logger().error(f\"Error decrypt_asymmetric {e}\")\n    return \"Invalid\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.decrypt_symmetric","title":"<code>decrypt_symmetric(encrypted_text, key, to_str=True, mute=False)</code>  <code>staticmethod</code>","text":"<p>Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text</code> <code>str</code> <p>Der zu entschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <code>to_str</code> <code>bool</code> <p>default true returns str if false returns bytes</p> <code>True</code> <p>Returns:     str: Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n        to_str (bool): default true returns str if false returns bytes\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n\n    if isinstance(key, str):\n        key = key.encode()\n\n    #try:\n    fernet = Fernet(key)\n    text_b = fernet.decrypt(encrypted_text)\n    if not to_str:\n        return text_b\n    return text_b.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.encrypt_asymmetric","title":"<code>encrypt_asymmetric(text, public_key_str)</code>  <code>staticmethod</code>","text":"<p>Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>public_key_str</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    # try:\n    #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n    #  except Exception as e:\n    #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n    try:\n        public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        encrypted = public_key.encrypt(\n            text.encode(),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return encrypted.hex()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        return \"Invalid\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.encrypt_symmetric","title":"<code>encrypt_symmetric(text, key)</code>  <code>staticmethod</code>","text":"<p>Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    if isinstance(text, str):\n        text = text.encode()\n\n    try:\n        fernet = Fernet(key.encode())\n        return fernet.encrypt(text).decode()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n        return \"Error encrypt\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.generate_asymmetric_keys","title":"<code>generate_asymmetric_keys()</code>  <code>staticmethod</code>","text":"<p>Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_asymmetric_keys() -&gt; (str, str):\n    \"\"\"\n    Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n    Args:\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n    \"\"\"\n    private_key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048 * 3,\n    )\n    public_key = private_key.public_key()\n\n    # Serialisieren der Schl\u00fcssel\n    pem_private_key = private_key.private_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PrivateFormat.PKCS8,\n        encryption_algorithm=serialization.NoEncryption()\n    ).decode()\n\n    pem_public_key = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    ).decode()\n\n    return pem_public_key, pem_private_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.generate_seed","title":"<code>generate_seed()</code>  <code>staticmethod</code>","text":"<p>Erzeugt eine zuf\u00e4llige Zahl als Seed.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Eine zuf\u00e4llige Zahl.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_seed() -&gt; int:\n    \"\"\"\n    Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n    Returns:\n        int: Eine zuf\u00e4llige Zahl.\n    \"\"\"\n    return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.generate_symmetric_key","title":"<code>generate_symmetric_key()</code>  <code>staticmethod</code>","text":"<p>Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der generierte Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_symmetric_key() -&gt; str:\n    \"\"\"\n    Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n    Returns:\n        str: Der generierte Schl\u00fcssel.\n    \"\"\"\n    return Fernet.generate_key().decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.load_keys_from_files","title":"<code>load_keys_from_files(directory='keys')</code>  <code>staticmethod</code>","text":"<p>L\u00e4dt die Schl\u00fcssel aus den Dateien. Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen</p> <code>'keys'</code> <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n    \"\"\"\n    L\u00e4dt die Schl\u00fcssel aus den Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n    Args:\n        directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n    Raises:\n        FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n    \"\"\"\n    # Pfade zu den Schl\u00fcsseldateien\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n\n    # Pr\u00fcfe ob die Dateien existieren\n    if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n        return \"\", \"\"\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Lade den \u00f6ffentlichen Schl\u00fcssel\n    with open(public_key_path) as f:\n        public_key = f.read()\n\n    # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n    with open(private_key_path) as f:\n        encrypted_private_key = f.read()\n        private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n    return public_key, private_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.one_way_hash","title":"<code>one_way_hash(text, salt='', pepper='')</code>  <code>staticmethod</code>","text":"<p>Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu hashende Text.</p> required <code>salt</code> <code>str</code> <p>Der Salt-Wert.</p> <code>''</code> <code>pepper</code> <code>str</code> <p>Der Pepper-Wert.</p> <code>''</code> <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der resultierende Hash-Wert.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n    \"\"\"\n    Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n    Args:\n        text (str): Der zu hashende Text.\n        salt (str): Der Salt-Wert.\n        pepper (str): Der Pepper-Wert.\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        str: Der resultierende Hash-Wert.\n    \"\"\"\n    return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.pem_to_public_key","title":"<code>pem_to_public_key(pem_key)</code>  <code>staticmethod</code>","text":"<p>Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.</p> <p>Parameters:</p> Name Type Description Default <code>pem_key</code> <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef pem_to_public_key(pem_key: str):\n    \"\"\"\n    Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n    Args:\n        pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n    Returns:\n        PublicKey: Das PublicKey-Objekt.\n    \"\"\"\n    public_key = serialization.load_pem_public_key(pem_key.encode())\n    return public_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.public_key_to_pem","title":"<code>public_key_to_pem(public_key)</code>  <code>staticmethod</code>","text":"<p>Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef public_key_to_pem(public_key: RSAPublicKey):\n    \"\"\"\n    Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n    Args:\n        public_key (PublicKey): Das PublicKey-Objekt.\n\n    Returns:\n        str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n    \"\"\"\n    pem = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    )\n    return pem.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.save_keys_to_files","title":"<code>save_keys_to_files(public_key, private_key, directory='keys')</code>  <code>staticmethod</code>","text":"<p>Speichert die generierten Schl\u00fcssel in separate Dateien. Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel im PEM-Format</p> required <code>private_key</code> <code>str</code> <p>Der private Schl\u00fcssel im PEM-Format</p> required <code>directory</code> <code>str</code> <p>Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen</p> <code>'keys'</code> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n    \"\"\"\n    Speichert die generierten Schl\u00fcssel in separate Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n    Args:\n        public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n        private_key (str): Der private Schl\u00fcssel im PEM-Format\n        directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n    \"\"\"\n    # Erstelle das Verzeichnis, falls es nicht existiert\n    os.makedirs(directory, exist_ok=True)\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n    encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n    # Speichere den \u00f6ffentlichen Schl\u00fcssel\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    with open(public_key_path, \"w\") as f:\n        f.write(public_key)\n\n    # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n    with open(private_key_path, \"w\") as f:\n        f.write(encrypted_private_key)\n\n    print(\"Saved keys in \", public_key_path)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.cryp","title":"<code>cryp</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.security.cryp.Code","title":"<code>Code</code>","text":"Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>class Code:\n\n    @staticmethod\n    def DK():\n        return DEVICE_KEY\n\n    def decode_code(self, encrypted_data, key=None):\n\n        if not isinstance(encrypted_data, str):\n            encrypted_data = str(encrypted_data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.decrypt_symmetric(encrypted_data, key)\n\n    def encode_code(self, data, key=None):\n\n        if not isinstance(data, str):\n            data = str(data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.encrypt_symmetric(data, key)\n\n    @staticmethod\n    def generate_seed() -&gt; int:\n        \"\"\"\n        Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n        Returns:\n            int: Eine zuf\u00e4llige Zahl.\n        \"\"\"\n        return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n\n    @staticmethod\n    def one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n        \"\"\"\n        Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n        Args:\n            text (str): Der zu hashende Text.\n            salt (str): Der Salt-Wert.\n            pepper (str): Der Pepper-Wert.\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            str: Der resultierende Hash-Wert.\n        \"\"\"\n        return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n\n    @staticmethod\n    def generate_symmetric_key() -&gt; str:\n        \"\"\"\n        Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n        Returns:\n            str: Der generierte Schl\u00fcssel.\n        \"\"\"\n        return Fernet.generate_key().decode()\n\n    @staticmethod\n    def encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        if isinstance(text, str):\n            text = text.encode()\n\n        try:\n            fernet = Fernet(key.encode())\n            return fernet.encrypt(text).decode()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n            return \"Error encrypt\"\n\n    @staticmethod\n    def decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n            to_str (bool): default true returns str if false returns bytes\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n\n        if isinstance(key, str):\n            key = key.encode()\n\n        #try:\n        fernet = Fernet(key)\n        text_b = fernet.decrypt(encrypted_text)\n        if not to_str:\n            return text_b\n        return text_b.decode()\n        # except Exception as e:\n        #     get_logger().error(f\"Error decrypt_symmetric {e}\")\n        #     if not mute:\n        #         raise e\n        #     if not to_str:\n        #         return f\"Error decoding\".encode()\n        #     return f\"Error decoding\"\n\n    @staticmethod\n    def generate_asymmetric_keys() -&gt; (str, str):\n        \"\"\"\n        Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n        Args:\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n        \"\"\"\n        private_key = rsa.generate_private_key(\n            public_exponent=65537,\n            key_size=2048 * 3,\n        )\n        public_key = private_key.public_key()\n\n        # Serialisieren der Schl\u00fcssel\n        pem_private_key = private_key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.NoEncryption()\n        ).decode()\n\n        pem_public_key = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        ).decode()\n\n        return pem_public_key, pem_private_key\n\n    @staticmethod\n    def save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n        \"\"\"\n        Speichert die generierten Schl\u00fcssel in separate Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n        Args:\n            public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n            private_key (str): Der private Schl\u00fcssel im PEM-Format\n            directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n        \"\"\"\n        # Erstelle das Verzeichnis, falls es nicht existiert\n        os.makedirs(directory, exist_ok=True)\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n        encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n        # Speichere den \u00f6ffentlichen Schl\u00fcssel\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        with open(public_key_path, \"w\") as f:\n            f.write(public_key)\n\n        # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n        with open(private_key_path, \"w\") as f:\n            f.write(encrypted_private_key)\n\n        print(\"Saved keys in \", public_key_path)\n\n    @staticmethod\n    def load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n        \"\"\"\n        L\u00e4dt die Schl\u00fcssel aus den Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n        Args:\n            directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n        Raises:\n            FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n        \"\"\"\n        # Pfade zu den Schl\u00fcsseldateien\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n\n        # Pr\u00fcfe ob die Dateien existieren\n        if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n            return \"\", \"\"\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Lade den \u00f6ffentlichen Schl\u00fcssel\n        with open(public_key_path) as f:\n            public_key = f.read()\n\n        # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n        with open(private_key_path) as f:\n            encrypted_private_key = f.read()\n            private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n        return public_key, private_key\n\n    @staticmethod\n    def encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        # try:\n        #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        #  except Exception as e:\n        #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            encrypted = public_key.encrypt(\n                text.encode(),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return encrypted.hex()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_asymmetric {e}\")\n            return \"Invalid\"\n\n    @staticmethod\n    def decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n        Args:\n            encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n            private_key_str (str): Der private Schl\u00fcssel als String.\n\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            decrypted = private_key.decrypt(\n                bytes.fromhex(encrypted_text_hex),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return decrypted.decode()\n\n        except Exception as e:\n            get_logger().error(f\"Error decrypt_asymmetric {e}\")\n        return \"Invalid\"\n\n    @staticmethod\n    def verify_signature(signature: str or bytes, message: str or bytes, public_key_str: str,\n                         salt_length=padding.PSS.MAX_LENGTH) -&gt; bool:\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                padding=padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                algorithm=hashes.SHA512()\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def verify_signature_web_algo(signature: str or bytes, message: str or bytes, public_key_str: str,\n                                  algo: int = -512) -&gt; bool:\n        signature_algorithm = ECDSA(hashes.SHA512())\n        if algo != -512:\n            signature_algorithm = ECDSA(hashes.SHA256())\n\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                # padding=padding.PSS(\n                #    mgf=padding.MGF1(hashes.SHA512()),\n                #    salt_length=padding.PSS.MAX_LENGTH\n                # ),\n                signature_algorithm=signature_algorithm\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def create_signature(message: str, private_key_str: str, salt_length=padding.PSS.MAX_LENGTH,\n                         row=False) -&gt; str or bytes:\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            signature = private_key.sign(\n                message.encode(),\n                padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                hashes.SHA512()\n            )\n            if row:\n                return signature\n            return base64.b64encode(signature).decode()\n        except Exception as e:\n            get_logger().error(f\"Error create_signature {e}\")\n            print(e)\n        return \"Invalid Key\"\n\n    @staticmethod\n    def pem_to_public_key(pem_key: str):\n        \"\"\"\n        Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n        Args:\n            pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n        Returns:\n            PublicKey: Das PublicKey-Objekt.\n        \"\"\"\n        public_key = serialization.load_pem_public_key(pem_key.encode())\n        return public_key\n\n    @staticmethod\n    def public_key_to_pem(public_key: RSAPublicKey):\n        \"\"\"\n        Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n        Args:\n            public_key (PublicKey): Das PublicKey-Objekt.\n\n        Returns:\n            str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n        \"\"\"\n        pem = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n        return pem.decode()\n</code></pre> <code>decrypt_asymmetric(encrypted_text_hex, private_key_str)</code> <code>staticmethod</code> \u00b6 <p>Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text_hex</code> <code>str</code> <p>Der verschl\u00fcsselte Text als Hex-String.</p> required <code>private_key_str</code> <code>str</code> <p>Der private Schl\u00fcssel als String.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n    Args:\n        encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n        private_key_str (str): Der private Schl\u00fcssel als String.\n\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n    try:\n        private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n        decrypted = private_key.decrypt(\n            bytes.fromhex(encrypted_text_hex),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return decrypted.decode()\n\n    except Exception as e:\n        get_logger().error(f\"Error decrypt_asymmetric {e}\")\n    return \"Invalid\"\n</code></pre> <code>decrypt_symmetric(encrypted_text, key, to_str=True, mute=False)</code> <code>staticmethod</code> \u00b6 <p>Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text</code> <code>str</code> <p>Der zu entschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <code>to_str</code> <code>bool</code> <p>default true returns str if false returns bytes</p> <code>True</code> <p>Returns:     str: Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n        to_str (bool): default true returns str if false returns bytes\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n\n    if isinstance(key, str):\n        key = key.encode()\n\n    #try:\n    fernet = Fernet(key)\n    text_b = fernet.decrypt(encrypted_text)\n    if not to_str:\n        return text_b\n    return text_b.decode()\n</code></pre> <code>encrypt_asymmetric(text, public_key_str)</code> <code>staticmethod</code> \u00b6 <p>Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>public_key_str</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    # try:\n    #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n    #  except Exception as e:\n    #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n    try:\n        public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        encrypted = public_key.encrypt(\n            text.encode(),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return encrypted.hex()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        return \"Invalid\"\n</code></pre> <code>encrypt_symmetric(text, key)</code> <code>staticmethod</code> \u00b6 <p>Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    if isinstance(text, str):\n        text = text.encode()\n\n    try:\n        fernet = Fernet(key.encode())\n        return fernet.encrypt(text).decode()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n        return \"Error encrypt\"\n</code></pre> <code>generate_asymmetric_keys()</code> <code>staticmethod</code> \u00b6 <p>Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_asymmetric_keys() -&gt; (str, str):\n    \"\"\"\n    Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n    Args:\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n    \"\"\"\n    private_key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048 * 3,\n    )\n    public_key = private_key.public_key()\n\n    # Serialisieren der Schl\u00fcssel\n    pem_private_key = private_key.private_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PrivateFormat.PKCS8,\n        encryption_algorithm=serialization.NoEncryption()\n    ).decode()\n\n    pem_public_key = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    ).decode()\n\n    return pem_public_key, pem_private_key\n</code></pre> <code>generate_seed()</code> <code>staticmethod</code> \u00b6 <p>Erzeugt eine zuf\u00e4llige Zahl als Seed.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Eine zuf\u00e4llige Zahl.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_seed() -&gt; int:\n    \"\"\"\n    Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n    Returns:\n        int: Eine zuf\u00e4llige Zahl.\n    \"\"\"\n    return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n</code></pre> <code>generate_symmetric_key()</code> <code>staticmethod</code> \u00b6 <p>Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der generierte Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_symmetric_key() -&gt; str:\n    \"\"\"\n    Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n    Returns:\n        str: Der generierte Schl\u00fcssel.\n    \"\"\"\n    return Fernet.generate_key().decode()\n</code></pre> <code>load_keys_from_files(directory='keys')</code> <code>staticmethod</code> \u00b6 <p>L\u00e4dt die Schl\u00fcssel aus den Dateien. Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen</p> <code>'keys'</code> <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n    \"\"\"\n    L\u00e4dt die Schl\u00fcssel aus den Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n    Args:\n        directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n    Raises:\n        FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n    \"\"\"\n    # Pfade zu den Schl\u00fcsseldateien\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n\n    # Pr\u00fcfe ob die Dateien existieren\n    if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n        return \"\", \"\"\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Lade den \u00f6ffentlichen Schl\u00fcssel\n    with open(public_key_path) as f:\n        public_key = f.read()\n\n    # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n    with open(private_key_path) as f:\n        encrypted_private_key = f.read()\n        private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n    return public_key, private_key\n</code></pre> <code>one_way_hash(text, salt='', pepper='')</code> <code>staticmethod</code> \u00b6 <p>Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu hashende Text.</p> required <code>salt</code> <code>str</code> <p>Der Salt-Wert.</p> <code>''</code> <code>pepper</code> <code>str</code> <p>Der Pepper-Wert.</p> <code>''</code> <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der resultierende Hash-Wert.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n    \"\"\"\n    Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n    Args:\n        text (str): Der zu hashende Text.\n        salt (str): Der Salt-Wert.\n        pepper (str): Der Pepper-Wert.\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        str: Der resultierende Hash-Wert.\n    \"\"\"\n    return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n</code></pre> <code>pem_to_public_key(pem_key)</code> <code>staticmethod</code> \u00b6 <p>Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.</p> <p>Parameters:</p> Name Type Description Default <code>pem_key</code> <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef pem_to_public_key(pem_key: str):\n    \"\"\"\n    Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n    Args:\n        pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n    Returns:\n        PublicKey: Das PublicKey-Objekt.\n    \"\"\"\n    public_key = serialization.load_pem_public_key(pem_key.encode())\n    return public_key\n</code></pre> <code>public_key_to_pem(public_key)</code> <code>staticmethod</code> \u00b6 <p>Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef public_key_to_pem(public_key: RSAPublicKey):\n    \"\"\"\n    Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n    Args:\n        public_key (PublicKey): Das PublicKey-Objekt.\n\n    Returns:\n        str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n    \"\"\"\n    pem = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    )\n    return pem.decode()\n</code></pre> <code>save_keys_to_files(public_key, private_key, directory='keys')</code> <code>staticmethod</code> \u00b6 <p>Speichert die generierten Schl\u00fcssel in separate Dateien. Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel im PEM-Format</p> required <code>private_key</code> <code>str</code> <p>Der private Schl\u00fcssel im PEM-Format</p> required <code>directory</code> <code>str</code> <p>Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen</p> <code>'keys'</code> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n    \"\"\"\n    Speichert die generierten Schl\u00fcssel in separate Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n    Args:\n        public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n        private_key (str): Der private Schl\u00fcssel im PEM-Format\n        directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n    \"\"\"\n    # Erstelle das Verzeichnis, falls es nicht existiert\n    os.makedirs(directory, exist_ok=True)\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n    encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n    # Speichere den \u00f6ffentlichen Schl\u00fcssel\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    with open(public_key_path, \"w\") as f:\n        f.write(public_key)\n\n    # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n    with open(private_key_path, \"w\") as f:\n        f.write(encrypted_private_key)\n\n    print(\"Saved keys in \", public_key_path)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.singelton_class","title":"<code>singelton_class</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.singelton_class.Singleton","title":"<code>Singleton</code>","text":"<p>Singleton metaclass for ensuring only one instance of a class.</p> Source code in <code>toolboxv2/utils/singelton_class.py</code> <pre><code>class Singleton(type):\n    \"\"\"\n    Singleton metaclass for ensuring only one instance of a class.\n    \"\"\"\n\n    _instances = {}\n    _kwargs = {}\n    _args = {}\n\n    def __call__(cls, *args, **kwargs):\n        if cls not in cls._instances:\n            cls._instances[cls] = super().__call__(*args, **kwargs)\n            cls._args[cls] = args\n            cls._kwargs[cls] = kwargs\n        return cls._instances[cls]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system","title":"<code>system</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.AppType","title":"<code>AppType</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class AppType:\n    prefix: str\n    id: str\n    globals: dict[str, Any] = {\"root\": dict, }\n    locals: dict[str, Any] = {\"user\": {'app': \"self\"}, }\n\n    local_test: bool = False\n    start_dir: str\n    data_dir: str\n    config_dir: str\n    info_dir: str\n\n    logger: logging.Logger\n    logging_filename: str\n\n    api_allowed_mods_list: list[str] = []\n\n    version: str\n    loop: asyncio.AbstractEventLoop\n\n    keys: dict[str, str] = {\n        \"MACRO\": \"macro~~~~:\",\n        \"MACRO_C\": \"m_color~~:\",\n        \"HELPER\": \"helper~~~:\",\n        \"debug\": \"debug~~~~:\",\n        \"id\": \"name-spa~:\",\n        \"st-load\": \"mute~load:\",\n        \"comm-his\": \"comm-his~:\",\n        \"develop-mode\": \"dev~mode~:\",\n        \"provider::\": \"provider::\",\n    }\n\n    defaults: dict[str, (bool or dict or dict[str, dict[str, str]] or str or list[str] or list[list]) | None] = {\n        \"MACRO\": list[str],\n        \"MACRO_C\": dict,\n        \"HELPER\": dict,\n        \"debug\": str,\n        \"id\": str,\n        \"st-load\": False,\n        \"comm-his\": list[list],\n        \"develop-mode\": bool,\n    }\n\n    config_fh: FileHandler\n    _debug: bool\n    flows: dict[str, Callable]\n    dev_modi: bool\n    functions: dict[str, Any]\n    modules: dict[str, Any]\n\n    interface_type: ToolBoxInterfaces\n    REFIX: str\n\n    alive: bool\n    called_exit: tuple[bool, float]\n    args_sto: AppArgs\n    system_flag = None\n    session = None\n    appdata = None\n    exit_tasks = []\n\n    enable_profiling: bool = False\n    sto = None\n\n    def __init__(self, prefix: None | str= None, args: AppArgs | None = None):\n        self.args_sto = args\n        self.prefix = prefix\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def exit_main(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def hide_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def show_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def disconnect(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def set_logger(self, debug=False):\n        \"\"\"proxi attr\"\"\"\n\n    @property\n    def debug(self):\n        \"\"\"proxi attr\"\"\"\n        return self._debug\n\n    def debug_rains(self, e):\n        \"\"\"proxi attr\"\"\"\n\n    def set_flows(self, r):\n        \"\"\"proxi attr\"\"\"\n\n    def run_flows(self, name, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def rrun_flows(self, name, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def idle(self):\n        import time\n        self.print(\"idle\")\n        try:\n            while self.alive:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            pass\n        self.print(\"idle done\")\n\n    async def a_idle(self):\n        self.print(\"a idle\")\n        try:\n            if hasattr(self, 'daemon_app'):\n                self.print(\"serving daemon\")\n                await self.daemon_app.connect(self)\n            else:\n                self.print(\"serving default\")\n                while self.alive:\n                    await asyncio.sleep(1)\n        except KeyboardInterrupt:\n            pass\n        self.print(\"a idle done\")\n\n    @debug.setter\n    def debug(self, value):\n        \"\"\"proxi attr\"\"\"\n\n    def _coppy_mod(self, content, new_mod_dir, mod_name, file_type='py'):\n        \"\"\"proxi attr\"\"\"\n\n    def _pre_lib_mod(self, mod_name, path_to=\"./runtime\", file_type='py'):\n        \"\"\"proxi attr\"\"\"\n\n    def _copy_load(self, mod_name, file_type='py', **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True):\n        \"\"\"proxi attr\"\"\"\n\n    def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n        \"\"\"proxi attr\"\"\"\n\n    def save_initialized_module(self, tools_class, spec):\n        \"\"\"proxi attr\"\"\"\n\n    def mod_online(self, mod_name, installed=False):\n        \"\"\"proxi attr\"\"\"\n\n    def _get_function(self,\n                      name: Enum or None,\n                      state: bool = True,\n                      specification: str = \"app\",\n                      metadata=False, as_str: tuple or None = None, r=0):\n        \"\"\"proxi attr\"\"\"\n\n    def save_exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def load_mod(self, mod_name: str, mlm='I', **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    async def init_module(self, modular):\n        return await self.load_mod(modular)\n\n    async def load_all_mods_in_file(self, working_dir=\"mods\"):\n        \"\"\"proxi attr\"\"\"\n\n    def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\"):\n        \"\"\"proxi attr\"\"\"\n\n    def remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            self.remove_mod(mod, delete=delete)\n\n    async def a_remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            await self.a_remove_mod(mod, delete=delete)\n\n    def print_ok(self):\n        \"\"\"proxi attr\"\"\"\n        self.logger.info(\"OK\")\n\n    def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n        \"\"\"proxi attr\"\"\"\n\n    def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None):\n        \"\"\"proxi attr\"\"\"\n\n    def remove_mod(self, mod_name, spec='app', delete=True):\n        \"\"\"proxi attr\"\"\"\n\n    async def a_remove_mod(self, mod_name, spec='app', delete=True):\n        \"\"\"proxi attr\"\"\"\n\n    def exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def web_context(self) -&gt; str:\n        \"\"\"returns the build index ( toolbox web component )\"\"\"\n\n    async def a_exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def save_load(self, modname, spec='app'):\n        \"\"\"proxi attr\"\"\"\n\n    def get_function(self, name: Enum or tuple, **kwargs):\n        \"\"\"\n        Kwargs for _get_function\n            metadata:: return the registered function dictionary\n                stateless: (function_data, None), 0\n                stateful: (function_data, higher_order_function), 0\n            state::boolean\n                specification::str default app\n        \"\"\"\n\n    def run_a_from_sync(self, function, *args):\n        \"\"\"\n        run a async fuction\n        \"\"\"\n\n    def run_function(self, mod_function_name: Enum or tuple,\n                     tb_run_function_with_state=True,\n                     tb_run_with_specification='app',\n                     args_=None,\n                     kwargs_=None,\n                     *args,\n                     **kwargs) -&gt; Result:\n\n        \"\"\"proxi attr\"\"\"\n\n    async def a_run_function(self, mod_function_name: Enum or tuple,\n                             tb_run_function_with_state=True,\n                             tb_run_with_specification='app',\n                             args_=None,\n                             kwargs_=None,\n                             *args,\n                             **kwargs) -&gt; Result:\n\n        \"\"\"proxi attr\"\"\"\n\n    def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n        \"\"\"\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        proxi attr\n        \"\"\"\n\n    async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict):\n        \"\"\"\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        proxi attr\n        \"\"\"\n\n    async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None, method=\"GET\",\n                       args_=None,\n                       kwargs_=None,\n                       *args, **kwargs):\n        \"\"\"run a function remote via http / https\"\"\"\n\n    def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n                get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                kwargs_=None,\n                *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                        backwords_compability_variabel_string_holder=None,\n                        get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                        kwargs_=None,\n                        *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def print(text, *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def sprint(text, *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    # ----------------------------------------------------------------\n    # Decorators for the toolbox\n\n    def _register_function(self, module_name, func_name, data):\n        \"\"\"proxi attr\"\"\"\n\n    def _create_decorator(self, type_: str,\n                          name: str = \"\",\n                          mod_name: str = \"\",\n                          level: int = -1,\n                          restrict_in_virtual_mode: bool = False,\n                          api: bool = False,\n                          helper: str = \"\",\n                          version: str or None = None,\n                          initial=False,\n                          exit_f=False,\n                          test=True,\n                          samples=None,\n                          state=None,\n                          pre_compute=None,\n                          post_compute=None,\n                          memory_cache=False,\n                          file_cache=False,\n                          row=False,\n                          request_as_kwarg=False,\n                          memory_cache_max_size=100,\n                          memory_cache_ttl=300):\n        \"\"\"proxi attr\"\"\"\n\n        # data = {\n        #     \"type\": type_,\n        #     \"module_name\": module_name,\n        #     \"func_name\": func_name,\n        #     \"level\": level,\n        #     \"restrict_in_virtual_mode\": restrict_in_virtual_mode,\n        #     \"func\": func,\n        #     \"api\": api,\n        #     \"helper\": helper,\n        #     \"version\": version,\n        #     \"initial\": initial,\n        #     \"exit_f\": exit_f,\n        #     \"__module__\": func.__module__,\n        #     \"signature\": sig,\n        #     \"params\": params,\n        #     \"state\": (\n        #         False if len(params) == 0 else params[0] in ['self', 'state', 'app']) if state is None else state,\n        #     \"do_test\": test,\n        #     \"samples\": samples,\n        #     \"request_as_kwarg\": request_as_kwarg,\n\n    def tb(self, name=None,\n           mod_name: str = \"\",\n           helper: str = \"\",\n           version: str or None = None,\n           test: bool = True,\n           restrict_in_virtual_mode: bool = False,\n           api: bool = False,\n           initial: bool = False,\n           exit_f: bool = False,\n           test_only: bool = False,\n           memory_cache: bool = False,\n           file_cache: bool = False,\n           row=False,\n           request_as_kwarg: bool = False,\n           state: bool or None = None,\n           level: int = 0,\n           memory_cache_max_size: int = 100,\n           memory_cache_ttl: int = 300,\n           samples: list or dict or None = None,\n           interface: ToolBoxInterfaces or None or str = None,\n           pre_compute=None,\n           post_compute=None,\n           api_methods=None,\n           ):\n        \"\"\"\n    A decorator for registering and configuring functions within a module.\n\n    This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\n    Args:\n        name (str, optional): The name to register the function under. Defaults to the function's own name.\n        mod_name (str, optional): The name of the module the function belongs to.\n        helper (str, optional): A helper string providing additional information about the function.\n        version (str or None, optional): The version of the function or module.\n        test (bool, optional): Flag to indicate if the function is for testing purposes.\n        restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n        api (bool, optional): Flag to indicate if the function is part of an API.\n        initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n        exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n        test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n        memory_cache (bool, optional): Flag to enable memory caching for the function.\n        request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n        file_cache (bool, optional): Flag to enable file caching for the function.\n        row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n        state (bool or None, optional): Flag to indicate if the function maintains state.\n        level (int, optional): The level of the function, used for prioritization or categorization.\n        memory_cache_max_size (int, optional): Maximum size of the memory cache.\n        memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n        samples (list or dict or None, optional): Samples or examples of function usage.\n        interface (str, optional): The interface type for the function.\n        pre_compute (callable, optional): A function to be called before the main function.\n        post_compute (callable, optional): A function to be called after the main function.\n        api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\n    Returns:\n        function: The decorated function with additional processing and registration capabilities.\n    \"\"\"\n        if interface is None:\n            interface = \"tb\"\n        if test_only and 'test' not in self.id:\n            return lambda *args, **kwargs: args\n        return self._create_decorator(interface,\n                                      name,\n                                      mod_name,\n                                      level=level,\n                                      restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                      helper=helper,\n                                      api=api,\n                                      version=version,\n                                      initial=initial,\n                                      exit_f=exit_f,\n                                      test=test,\n                                      samples=samples,\n                                      state=state,\n                                      pre_compute=pre_compute,\n                                      post_compute=post_compute,\n                                      memory_cache=memory_cache,\n                                      file_cache=file_cache,\n                                      row=row,\n                                      request_as_kwarg=request_as_kwarg,\n                                      memory_cache_max_size=memory_cache_max_size,\n                                      memory_cache_ttl=memory_cache_ttl)\n\n    def print_functions(self, name=None):\n\n\n        if not self.functions:\n            print(\"Nothing to see\")\n            return\n\n        def helper(_functions):\n            for func_name, data in _functions.items():\n                if not isinstance(data, dict):\n                    continue\n\n                func_type = data.get('type', 'Unknown')\n                func_level = 'r' if data['level'] == -1 else data['level']\n                api_status = 'Api' if data.get('api', False) else 'Non-Api'\n\n                print(f\"  Function: {func_name}{data.get('signature', '()')}; \"\n                      f\"Type: {func_type}, Level: {func_level}, {api_status}\")\n\n        if name is not None:\n            functions = self.functions.get(name)\n            if functions is not None:\n                print(f\"\\nModule: {name}; Type: {functions.get('app_instance_type', 'Unknown')}\")\n                helper(functions)\n                return\n        for module, functions in self.functions.items():\n            print(f\"\\nModule: {module}; Type: {functions.get('app_instance_type', 'Unknown')}\")\n            helper(functions)\n\n    def save_autocompletion_dict(self):\n        \"\"\"proxi attr\"\"\"\n\n    def get_autocompletion_dict(self):\n        \"\"\"proxi attr\"\"\"\n\n    def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n        \"\"\"proxi attr\"\"\"\n\n    def save_registry_as_enums(self, directory: str, filename: str):\n        \"\"\"proxi attr\"\"\"\n\n    async def execute_all_functions_(self, m_query='', f_query=''):\n        print(\"Executing all functions\")\n        from ..extras import generate_test_cases\n        all_data = {\n            \"modular_run\": 0,\n            \"modular_fatal_error\": 0,\n            \"errors\": 0,\n            \"modular_sug\": 0,\n            \"coverage\": [],\n            \"total_coverage\": {},\n        }\n        items = list(self.functions.items()).copy()\n        for module_name, functions in items:\n            infos = {\n                \"functions_run\": 0,\n                \"functions_fatal_error\": 0,\n                \"error\": 0,\n                \"functions_sug\": 0,\n                'calls': {},\n                'callse': {},\n                \"coverage\": [0, 0],\n            }\n            all_data['modular_run'] += 1\n            if not module_name.startswith(m_query):\n                all_data['modular_sug'] += 1\n                continue\n\n            with Spinner(message=f\"In {module_name}| \"):\n                f_items = list(functions.items()).copy()\n                for function_name, function_data in f_items:\n                    if not isinstance(function_data, dict):\n                        continue\n                    if not function_name.startswith(f_query):\n                        continue\n                    test: list = function_data.get('do_test')\n                    # print(test, module_name, function_name, function_data)\n                    infos[\"coverage\"][0] += 1\n                    if test is False:\n                        continue\n\n                    with Spinner(message=f\"\\t\\t\\t\\t\\t\\tfuction {function_name}...\"):\n                        params: list = function_data.get('params')\n                        sig: signature = function_data.get('signature')\n                        state: bool = function_data.get('state')\n                        samples: bool = function_data.get('samples')\n\n                        test_kwargs_list = [{}]\n\n                        if params is not None:\n                            test_kwargs_list = samples if samples is not None else generate_test_cases(sig=sig)\n                            # print(test_kwargs)\n                            # print(test_kwargs[0])\n                            # test_kwargs = test_kwargs_list[0]\n                        # print(module_name, function_name, test_kwargs_list)\n                        infos[\"coverage\"][1] += 1\n                        for test_kwargs in test_kwargs_list:\n                            try:\n                                # print(f\"test Running {state=} |{module_name}.{function_name}\")\n                                result = await self.a_run_function((module_name, function_name),\n                                                                   tb_run_function_with_state=state,\n                                                                   **test_kwargs)\n                                if not isinstance(result, Result):\n                                    result = Result.ok(result)\n                                if result.info.exec_code == 0:\n                                    infos['calls'][function_name] = [test_kwargs, str(result)]\n                                    infos['functions_sug'] += 1\n                                else:\n                                    infos['functions_sug'] += 1\n                                    infos['error'] += 1\n                                    infos['callse'][function_name] = [test_kwargs, str(result)]\n                            except Exception as e:\n                                infos['functions_fatal_error'] += 1\n                                infos['callse'][function_name] = [test_kwargs, str(e)]\n                            finally:\n                                infos['functions_run'] += 1\n\n                if infos['functions_run'] == infos['functions_sug']:\n                    all_data['modular_sug'] += 1\n                else:\n                    all_data['modular_fatal_error'] += 1\n                if infos['error'] &gt; 0:\n                    all_data['errors'] += infos['error']\n\n                all_data[module_name] = infos\n                if infos['coverage'][0] == 0:\n                    c = 0\n                else:\n                    c = infos['coverage'][1] / infos['coverage'][0]\n                all_data[\"coverage\"].append(f\"{module_name}:{c:.2f}\\n\")\n        total_coverage = sum([float(t.split(\":\")[-1]) for t in all_data[\"coverage\"]]) / len(all_data[\"coverage\"])\n        print(\n            f\"\\n{all_data['modular_run']=}\\n{all_data['modular_sug']=}\\n{all_data['modular_fatal_error']=}\\n{total_coverage=}\")\n        d = analyze_data(all_data)\n        return Result.ok(data=all_data, data_info=d)\n\n    @staticmethod\n    def calculate_complexity(filename_or_code):\n        from radon.complexity import cc_rank, cc_visit\n        if os.path.exists(filename_or_code):\n            with open(filename_or_code) as file:\n                code = file.read()\n        else:\n            code = filename_or_code\n\n        # Calculate and print Cyclomatic Complexity\n        complexity_results = cc_visit(code)\n        i = -1\n        avg_complexity = 0\n        for block in complexity_results:\n            complexity = block.complexity\n            i += 1\n            print(f\"block: {block.name} {i} Class/Fuction/Methode : {block.letter}\")\n            print(f\"    fullname: {block.fullname}\")\n            print(f\"    Cyclomatic Complexity: {complexity}\")\n            # Optional: Get complexity rank\n            avg_complexity += complexity\n            rank = cc_rank(complexity)\n            print(f\"    Complexity Rank: {rank}\")\n            # print(f\"    lineno: {block.lineno}\")\n            print(f\"    endline: {block.endline}\")\n            print(f\"    col_offset: {block.col_offset}\\n\")\n        if i &lt;= 0:\n            i += 2\n        avg_complexity = avg_complexity / i\n        print(f\"\\nAVG Complexity: {avg_complexity:.2f}\")\n        print(f\"Total Rank: {cc_rank(int(avg_complexity + i // 10))}\")\n\n    async def execute_function_test(self, module_name: str, function_name: str,\n                                    function_data: dict, test_kwargs: dict,\n                                    profiler: cProfile.Profile) -&gt; tuple[bool, str, dict, float]:\n        start_time = time.time()\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            try:\n                result = await self.a_run_function(\n                    (module_name, function_name),\n                    tb_run_function_with_state=function_data.get('state'),\n                    **test_kwargs\n                )\n\n                if not isinstance(result, Result):\n                    result = Result.ok(result)\n\n                success = result.info.exec_code == 0\n                execution_time = time.time() - start_time\n                return success, str(result), test_kwargs, execution_time\n            except Exception as e:\n                execution_time = time.time() - start_time\n                return False, str(e), test_kwargs, execution_time\n\n    async def process_function(self, module_name: str, function_name: str,\n                               function_data: dict, profiler: cProfile.Profile) -&gt; tuple[str, ModuleInfo]:\n        start_time = time.time()\n        info = ModuleInfo()\n\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            if not isinstance(function_data, dict):\n                return function_name, info\n\n            test = function_data.get('do_test')\n            info.coverage[0] += 1\n\n            if test is False:\n                return function_name, info\n\n            params = function_data.get('params')\n            sig = function_data.get('signature')\n            samples = function_data.get('samples')\n\n            test_kwargs_list = [{}] if params is None else (\n                samples if samples is not None else generate_test_cases(sig=sig)\n            )\n\n            info.coverage[1] += 1\n\n            # Create tasks for all test cases\n            tasks = [\n                self.execute_function_test(module_name, function_name, function_data, test_kwargs, profiler)\n                for test_kwargs in test_kwargs_list\n            ]\n\n            # Execute all tests concurrently\n            results = await asyncio.gather(*tasks)\n\n            total_execution_time = 0\n            for success, result_str, test_kwargs, execution_time in results:\n                info.functions_run += 1\n                total_execution_time += execution_time\n\n                if success:\n                    info.functions_sug += 1\n                    info.calls[function_name] = [test_kwargs, result_str]\n                else:\n                    info.functions_sug += 1\n                    info.error += 1\n                    info.callse[function_name] = [test_kwargs, result_str]\n\n            info.execution_time = time.time() - start_time\n            return function_name, info\n\n    async def process_module(self, module_name: str, functions: dict,\n                             f_query: str, profiler: cProfile.Profile) -&gt; tuple[str, ModuleInfo]:\n        start_time = time.time()\n\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            async with asyncio.Semaphore(mp.cpu_count()):\n                tasks = [\n                    self.process_function(module_name, fname, fdata, profiler)\n                    for fname, fdata in functions.items()\n                    if fname.startswith(f_query)\n                ]\n\n                if not tasks:\n                    return module_name, ModuleInfo()\n\n                results = await asyncio.gather(*tasks)\n\n                # Combine results from all functions in the module\n                combined_info = ModuleInfo()\n                total_execution_time = 0\n\n                for _, info in results:\n                    combined_info.functions_run += info.functions_run\n                    combined_info.functions_fatal_error += info.functions_fatal_error\n                    combined_info.error += info.error\n                    combined_info.functions_sug += info.functions_sug\n                    combined_info.calls.update(info.calls)\n                    combined_info.callse.update(info.callse)\n                    combined_info.coverage[0] += info.coverage[0]\n                    combined_info.coverage[1] += info.coverage[1]\n                    total_execution_time += info.execution_time\n\n                combined_info.execution_time = time.time() - start_time\n                return module_name, combined_info\n\n    async def execute_all_functions(self, m_query='', f_query='', enable_profiling=True):\n        \"\"\"\n        Execute all functions with parallel processing and optional profiling.\n\n        Args:\n            m_query (str): Module name query filter\n            f_query (str): Function name query filter\n            enable_profiling (bool): Enable detailed profiling information\n        \"\"\"\n        print(\"Executing all functions in parallel\" + (\" with profiling\" if enable_profiling else \"\"))\n\n        start_time = time.time()\n        stats = ExecutionStats()\n        items = list(self.functions.items()).copy()\n\n        # Set up profiling\n        self.enable_profiling = enable_profiling\n        profiler = cProfile.Profile()\n\n        with profile_section(profiler, enable_profiling):\n            # Filter modules based on query\n            filtered_modules = [\n                (mname, mfuncs) for mname, mfuncs in items\n                if mname.startswith(m_query)\n            ]\n\n            stats.modular_run = len(filtered_modules)\n\n            # Process all modules concurrently\n            async with asyncio.Semaphore(mp.cpu_count()):\n                tasks = [\n                    self.process_module(mname, mfuncs, f_query, profiler)\n                    for mname, mfuncs in filtered_modules\n                ]\n\n                results = await asyncio.gather(*tasks)\n\n            # Combine results and calculate statistics\n            for module_name, info in results:\n                if info.functions_run == info.functions_sug:\n                    stats.modular_sug += 1\n                else:\n                    stats.modular_fatal_error += 1\n\n                stats.errors += info.error\n\n                # Calculate coverage\n                coverage = (info.coverage[1] / info.coverage[0]) if info.coverage[0] &gt; 0 else 0\n                stats.coverage.append(f\"{module_name}:{coverage:.2f}\\n\")\n\n                # Store module info\n                stats.__dict__[module_name] = info\n\n            # Calculate total coverage\n            total_coverage = (\n                sum(float(t.split(\":\")[-1]) for t in stats.coverage) / len(stats.coverage)\n                if stats.coverage else 0\n            )\n\n            stats.total_execution_time = time.time() - start_time\n\n            # Generate profiling stats if enabled\n            if enable_profiling:\n                s = io.StringIO()\n                ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n                ps.print_stats()\n                stats.profiling_data = {\n                    'detailed_stats': s.getvalue(),\n                    'total_time': stats.total_execution_time,\n                    'function_count': stats.modular_run,\n                    'successful_functions': stats.modular_sug\n                }\n\n            print(\n                f\"\\n{stats.modular_run=}\"\n                f\"\\n{stats.modular_sug=}\"\n                f\"\\n{stats.modular_fatal_error=}\"\n                f\"\\n{total_coverage=}\"\n                f\"\\nTotal execution time: {stats.total_execution_time:.2f}s\"\n            )\n\n            if enable_profiling:\n                print(\"\\nProfiling Summary:\")\n                print(f\"{'=' * 50}\")\n                print(\"Top 10 time-consuming functions:\")\n                ps.print_stats(10)\n\n            analyzed_data = analyze_data(stats.__dict__)\n            return Result.ok(data=stats.__dict__, data_info=analyzed_data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.debug","title":"<code>debug</code>  <code>property</code> <code>writable</code>","text":"<p>proxi attr</p>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.prefix","title":"<code>prefix = prefix</code>  <code>instance-attribute</code>","text":"<p>proxi attr</p>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.a_exit","title":"<code>a_exit()</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.a_fuction_runner","title":"<code>a_fuction_runner(function, function_data, args, kwargs)</code>  <code>async</code>","text":"<p>parameters = function_data.get('params') modular_name = function_data.get('module_name') function_name = function_data.get('func_name') mod_function_name = f\"{modular_name}.{function_name}\"</p> <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict):\n    \"\"\"\n    parameters = function_data.get('params')\n    modular_name = function_data.get('module_name')\n    function_name = function_data.get('func_name')\n    mod_function_name = f\"{modular_name}.{function_name}\"\n\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.a_remove_mod","title":"<code>a_remove_mod(mod_name, spec='app', delete=True)</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_remove_mod(self, mod_name, spec='app', delete=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.a_run_any","title":"<code>a_run_any(mod_function_name, backwords_compability_variabel_string_holder=None, get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                    backwords_compability_variabel_string_holder=None,\n                    get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                    kwargs_=None,\n                    *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.a_run_function","title":"<code>a_run_function(mod_function_name, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_run_function(self, mod_function_name: Enum or tuple,\n                         tb_run_function_with_state=True,\n                         tb_run_with_specification='app',\n                         args_=None,\n                         kwargs_=None,\n                         *args,\n                         **kwargs) -&gt; Result:\n\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.debug_rains","title":"<code>debug_rains(e)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def debug_rains(self, e):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.disconnect","title":"<code>disconnect(*args, **kwargs)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def disconnect(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.execute_all_functions","title":"<code>execute_all_functions(m_query='', f_query='', enable_profiling=True)</code>  <code>async</code>","text":"<p>Execute all functions with parallel processing and optional profiling.</p> <p>Parameters:</p> Name Type Description Default <code>m_query</code> <code>str</code> <p>Module name query filter</p> <code>''</code> <code>f_query</code> <code>str</code> <p>Function name query filter</p> <code>''</code> <code>enable_profiling</code> <code>bool</code> <p>Enable detailed profiling information</p> <code>True</code> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def execute_all_functions(self, m_query='', f_query='', enable_profiling=True):\n    \"\"\"\n    Execute all functions with parallel processing and optional profiling.\n\n    Args:\n        m_query (str): Module name query filter\n        f_query (str): Function name query filter\n        enable_profiling (bool): Enable detailed profiling information\n    \"\"\"\n    print(\"Executing all functions in parallel\" + (\" with profiling\" if enable_profiling else \"\"))\n\n    start_time = time.time()\n    stats = ExecutionStats()\n    items = list(self.functions.items()).copy()\n\n    # Set up profiling\n    self.enable_profiling = enable_profiling\n    profiler = cProfile.Profile()\n\n    with profile_section(profiler, enable_profiling):\n        # Filter modules based on query\n        filtered_modules = [\n            (mname, mfuncs) for mname, mfuncs in items\n            if mname.startswith(m_query)\n        ]\n\n        stats.modular_run = len(filtered_modules)\n\n        # Process all modules concurrently\n        async with asyncio.Semaphore(mp.cpu_count()):\n            tasks = [\n                self.process_module(mname, mfuncs, f_query, profiler)\n                for mname, mfuncs in filtered_modules\n            ]\n\n            results = await asyncio.gather(*tasks)\n\n        # Combine results and calculate statistics\n        for module_name, info in results:\n            if info.functions_run == info.functions_sug:\n                stats.modular_sug += 1\n            else:\n                stats.modular_fatal_error += 1\n\n            stats.errors += info.error\n\n            # Calculate coverage\n            coverage = (info.coverage[1] / info.coverage[0]) if info.coverage[0] &gt; 0 else 0\n            stats.coverage.append(f\"{module_name}:{coverage:.2f}\\n\")\n\n            # Store module info\n            stats.__dict__[module_name] = info\n\n        # Calculate total coverage\n        total_coverage = (\n            sum(float(t.split(\":\")[-1]) for t in stats.coverage) / len(stats.coverage)\n            if stats.coverage else 0\n        )\n\n        stats.total_execution_time = time.time() - start_time\n\n        # Generate profiling stats if enabled\n        if enable_profiling:\n            s = io.StringIO()\n            ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n            ps.print_stats()\n            stats.profiling_data = {\n                'detailed_stats': s.getvalue(),\n                'total_time': stats.total_execution_time,\n                'function_count': stats.modular_run,\n                'successful_functions': stats.modular_sug\n            }\n\n        print(\n            f\"\\n{stats.modular_run=}\"\n            f\"\\n{stats.modular_sug=}\"\n            f\"\\n{stats.modular_fatal_error=}\"\n            f\"\\n{total_coverage=}\"\n            f\"\\nTotal execution time: {stats.total_execution_time:.2f}s\"\n        )\n\n        if enable_profiling:\n            print(\"\\nProfiling Summary:\")\n            print(f\"{'=' * 50}\")\n            print(\"Top 10 time-consuming functions:\")\n            ps.print_stats(10)\n\n        analyzed_data = analyze_data(stats.__dict__)\n        return Result.ok(data=stats.__dict__, data_info=analyzed_data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.exit","title":"<code>exit()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.exit_main","title":"<code>exit_main(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef exit_main(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.fuction_runner","title":"<code>fuction_runner(function, function_data, args, kwargs, t0=0.0)</code>","text":"<p>parameters = function_data.get('params') modular_name = function_data.get('module_name') function_name = function_data.get('func_name') mod_function_name = f\"{modular_name}.{function_name}\"</p> <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n    \"\"\"\n    parameters = function_data.get('params')\n    modular_name = function_data.get('module_name')\n    function_name = function_data.get('func_name')\n    mod_function_name = f\"{modular_name}.{function_name}\"\n\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.get_all_mods","title":"<code>get_all_mods(working_dir='mods', path_to='./runtime')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.get_autocompletion_dict","title":"<code>get_autocompletion_dict()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_autocompletion_dict(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.get_function","title":"<code>get_function(name, **kwargs)</code>","text":"<p>Kwargs for _get_function     metadata:: return the registered function dictionary         stateless: (function_data, None), 0         stateful: (function_data, higher_order_function), 0     state::boolean         specification::str default app</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_function(self, name: Enum or tuple, **kwargs):\n    \"\"\"\n    Kwargs for _get_function\n        metadata:: return the registered function dictionary\n            stateless: (function_data, None), 0\n            stateful: (function_data, higher_order_function), 0\n        state::boolean\n            specification::str default app\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.get_mod","title":"<code>get_mod(name, spec='app')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.get_username","title":"<code>get_username(get_input=False, default='loot')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.hide_console","title":"<code>hide_console(*args, **kwargs)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def hide_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.inplace_load_instance","title":"<code>inplace_load_instance(mod_name, loc='toolboxv2.mods.', spec='app', save=True)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.load_all_mods_in_file","title":"<code>load_all_mods_in_file(working_dir='mods')</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def load_all_mods_in_file(self, working_dir=\"mods\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.load_mod","title":"<code>load_mod(mod_name, mlm='I', **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def load_mod(self, mod_name: str, mlm='I', **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.mod_online","title":"<code>mod_online(mod_name, installed=False)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def mod_online(self, mod_name, installed=False):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.print","title":"<code>print(text, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef print(text, *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.print_ok","title":"<code>print_ok()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def print_ok(self):\n    \"\"\"proxi attr\"\"\"\n    self.logger.info(\"OK\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.reload_mod","title":"<code>reload_mod(mod_name, spec='app', is_file=True, loc='toolboxv2.mods.')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.remove_mod","title":"<code>remove_mod(mod_name, spec='app', delete=True)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def remove_mod(self, mod_name, spec='app', delete=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.rrun_flows","title":"<code>rrun_flows(name, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def rrun_flows(self, name, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_a_from_sync","title":"<code>run_a_from_sync(function, *args)</code>","text":"<p>run a async fuction</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_a_from_sync(self, function, *args):\n    \"\"\"\n    run a async fuction\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_any","title":"<code>run_any(mod_function_name, backwords_compability_variabel_string_holder=None, get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n            get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n            kwargs_=None,\n            *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_flows","title":"<code>run_flows(name, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_flows(self, name, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_function","title":"<code>run_function(mod_function_name, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_function(self, mod_function_name: Enum or tuple,\n                 tb_run_function_with_state=True,\n                 tb_run_with_specification='app',\n                 args_=None,\n                 kwargs_=None,\n                 *args,\n                 **kwargs) -&gt; Result:\n\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_http","title":"<code>run_http(mod_function_name, function_name=None, method='GET', args_=None, kwargs_=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>run a function remote via http / https</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None, method=\"GET\",\n                   args_=None,\n                   kwargs_=None,\n                   *args, **kwargs):\n    \"\"\"run a function remote via http / https\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.save_autocompletion_dict","title":"<code>save_autocompletion_dict()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_autocompletion_dict(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.save_exit","title":"<code>save_exit()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.save_initialized_module","title":"<code>save_initialized_module(tools_class, spec)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_initialized_module(self, tools_class, spec):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.save_instance","title":"<code>save_instance(instance, modular_id, spec='app', instance_type='file/application', tools_class=None)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.save_load","title":"<code>save_load(modname, spec='app')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_load(self, modname, spec='app'):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.save_registry_as_enums","title":"<code>save_registry_as_enums(directory, filename)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_registry_as_enums(self, directory: str, filename: str):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.set_flows","title":"<code>set_flows(r)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def set_flows(self, r):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.set_logger","title":"<code>set_logger(debug=False)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def set_logger(self, debug=False):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.show_console","title":"<code>show_console(*args, **kwargs)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def show_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.sprint","title":"<code>sprint(text, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef sprint(text, *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.tb","title":"<code>tb(name=None, mod_name='', helper='', version=None, test=True, restrict_in_virtual_mode=False, api=False, initial=False, exit_f=False, test_only=False, memory_cache=False, file_cache=False, row=False, request_as_kwarg=False, state=None, level=0, memory_cache_max_size=100, memory_cache_ttl=300, samples=None, interface=None, pre_compute=None, post_compute=None, api_methods=None)</code>","text":"<p>A decorator for registering and configuring functions within a module.</p> <p>This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to register the function under. Defaults to the function's own name.</p> <code>None</code> <code>mod_name</code> <code>str</code> <p>The name of the module the function belongs to.</p> <code>''</code> <code>helper</code> <code>str</code> <p>A helper string providing additional information about the function.</p> <code>''</code> <code>version</code> <code>str or None</code> <p>The version of the function or module.</p> <code>None</code> <code>test</code> <code>bool</code> <p>Flag to indicate if the function is for testing purposes.</p> <code>True</code> <code>restrict_in_virtual_mode</code> <code>bool</code> <p>Flag to restrict the function in virtual mode.</p> <code>False</code> <code>api</code> <code>bool</code> <p>Flag to indicate if the function is part of an API.</p> <code>False</code> <code>initial</code> <code>bool</code> <p>Flag to indicate if the function should be executed at initialization.</p> <code>False</code> <code>exit_f</code> <code>bool</code> <p>Flag to indicate if the function should be executed at exit.</p> <code>False</code> <code>test_only</code> <code>bool</code> <p>Flag to indicate if the function should only be used for testing.</p> <code>False</code> <code>memory_cache</code> <code>bool</code> <p>Flag to enable memory caching for the function.</p> <code>False</code> <code>request_as_kwarg</code> <code>bool</code> <p>Flag to get request if the fuction is calld from api.</p> <code>False</code> <code>file_cache</code> <code>bool</code> <p>Flag to enable file caching for the function.</p> <code>False</code> <code>row</code> <code>bool</code> <p>rather to auto wrap the result in Result type default False means no row data aka result type</p> <code>False</code> <code>state</code> <code>bool or None</code> <p>Flag to indicate if the function maintains state.</p> <code>None</code> <code>level</code> <code>int</code> <p>The level of the function, used for prioritization or categorization.</p> <code>0</code> <code>memory_cache_max_size</code> <code>int</code> <p>Maximum size of the memory cache.</p> <code>100</code> <code>memory_cache_ttl</code> <code>int</code> <p>Time-to-live for the memory cache entries.</p> <code>300</code> <code>samples</code> <code>list or dict or None</code> <p>Samples or examples of function usage.</p> <code>None</code> <code>interface</code> <code>str</code> <p>The interface type for the function.</p> <code>None</code> <code>pre_compute</code> <code>callable</code> <p>A function to be called before the main function.</p> <code>None</code> <code>post_compute</code> <code>callable</code> <p>A function to be called after the main function.</p> <code>None</code> <code>api_methods</code> <code>list[str]</code> <p>default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>function</code> <p>The decorated function with additional processing and registration capabilities.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def tb(self, name=None,\n       mod_name: str = \"\",\n       helper: str = \"\",\n       version: str or None = None,\n       test: bool = True,\n       restrict_in_virtual_mode: bool = False,\n       api: bool = False,\n       initial: bool = False,\n       exit_f: bool = False,\n       test_only: bool = False,\n       memory_cache: bool = False,\n       file_cache: bool = False,\n       row=False,\n       request_as_kwarg: bool = False,\n       state: bool or None = None,\n       level: int = 0,\n       memory_cache_max_size: int = 100,\n       memory_cache_ttl: int = 300,\n       samples: list or dict or None = None,\n       interface: ToolBoxInterfaces or None or str = None,\n       pre_compute=None,\n       post_compute=None,\n       api_methods=None,\n       ):\n    \"\"\"\nA decorator for registering and configuring functions within a module.\n\nThis decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\nArgs:\n    name (str, optional): The name to register the function under. Defaults to the function's own name.\n    mod_name (str, optional): The name of the module the function belongs to.\n    helper (str, optional): A helper string providing additional information about the function.\n    version (str or None, optional): The version of the function or module.\n    test (bool, optional): Flag to indicate if the function is for testing purposes.\n    restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n    api (bool, optional): Flag to indicate if the function is part of an API.\n    initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n    exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n    test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n    memory_cache (bool, optional): Flag to enable memory caching for the function.\n    request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n    file_cache (bool, optional): Flag to enable file caching for the function.\n    row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n    state (bool or None, optional): Flag to indicate if the function maintains state.\n    level (int, optional): The level of the function, used for prioritization or categorization.\n    memory_cache_max_size (int, optional): Maximum size of the memory cache.\n    memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n    samples (list or dict or None, optional): Samples or examples of function usage.\n    interface (str, optional): The interface type for the function.\n    pre_compute (callable, optional): A function to be called before the main function.\n    post_compute (callable, optional): A function to be called after the main function.\n    api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\nReturns:\n    function: The decorated function with additional processing and registration capabilities.\n\"\"\"\n    if interface is None:\n        interface = \"tb\"\n    if test_only and 'test' not in self.id:\n        return lambda *args, **kwargs: args\n    return self._create_decorator(interface,\n                                  name,\n                                  mod_name,\n                                  level=level,\n                                  restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                  helper=helper,\n                                  api=api,\n                                  version=version,\n                                  initial=initial,\n                                  exit_f=exit_f,\n                                  test=test,\n                                  samples=samples,\n                                  state=state,\n                                  pre_compute=pre_compute,\n                                  post_compute=post_compute,\n                                  memory_cache=memory_cache,\n                                  file_cache=file_cache,\n                                  row=row,\n                                  request_as_kwarg=request_as_kwarg,\n                                  memory_cache_max_size=memory_cache_max_size,\n                                  memory_cache_ttl=memory_cache_ttl)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.watch_mod","title":"<code>watch_mod(mod_name, spec='app', loc='toolboxv2.mods.', use_thread=True, path_name=None)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.web_context","title":"<code>web_context()</code>","text":"<p>returns the build index ( toolbox web component )</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def web_context(self) -&gt; str:\n    \"\"\"returns the build index ( toolbox web component )\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainTool","title":"<code>MainTool</code>","text":"Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>class MainTool:\n    toolID: str = \"\"\n    # app = None\n    interface = None\n    spec = \"app\"\n    name = \"\"\n    color = \"Bold\"\n    stuf = False\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.__storedargs = args, kwargs\n        self.async_initialized = False\n\n    async def __ainit__(self, *args, **kwargs):\n        self.version = kwargs[\"v\"]\n        self.tools = kwargs.get(\"tool\", {})\n        self.name = kwargs[\"name\"]\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n        if not hasattr(self, 'config'):\n            self.config = {}\n        self.user = None\n        self.description = \"A toolbox mod\" if kwargs.get(\"description\") is None else kwargs.get(\"description\")\n        if MainTool.interface is None:\n            MainTool.interface = self.app.interface_type\n        # Result.default(self.app.interface)\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    await self.todo()\n                else:\n                    self.todo()\n                await asyncio.sleep(0.1)\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\" Error loading mod {self.name} {e}\")\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n\n        self.app.print(f\"TOOL : {self.spec}.{self.name} online\")\n\n    @property\n    def app(self):\n        return get_app(\n            from_=f\"{self.spec}.{self.name}|{self.toolID if self.toolID else '*' + MainTool.toolID} {self.interface if self.interface else MainTool.interface}\")\n\n    @app.setter\n    def app(self, v):\n        raise PermissionError(f\"You cannot set the App Instance! {v=}\")\n\n    @staticmethod\n    def return_result(error: ToolBoxError = ToolBoxError.none,\n                      exec_code: int = 0,\n                      help_text: str = \"\",\n                      data_info=None,\n                      data=None,\n                      data_to=None):\n\n        if data_to is None:\n            data_to = MainTool.interface if MainTool.interface is not None else ToolBoxInterfaces.cli\n\n        if data is None:\n            data = {}\n\n        if data_info is None:\n            data_info = {}\n\n        return Result(\n            error,\n            ToolBoxResult(data_info=data_info, data=data, data_to=data_to),\n            ToolBoxInfo(exec_code=exec_code, help_text=help_text)\n        )\n\n    def print(self, message, end=\"\\n\", **kwargs):\n        if self.stuf:\n            return\n\n        self.app.print(Style.style_dic[self.color] + self.name + Style.style_dic[\"END\"] + \":\", message, end=end,\n                       **kwargs)\n\n    def add_str_to_config(self, command):\n        if len(command) != 2:\n            self.logger.error('Invalid command must be key value')\n            return False\n        self.config[command[0]] = command[1]\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n\n    def get_version(self) -&gt; str:\n        \"\"\"\"Returns the version\"\"\"\n        return self.version\n\n    async def get_user(self, username: str) -&gt; Result:\n        return await self.app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME, username=username, get_results=True)\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainTool.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.__storedargs = args, kwargs\n    self.async_initialized = False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainTool.__initobj","title":"<code>__initobj()</code>  <code>async</code>","text":"<p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainTool.get_version","title":"<code>get_version()</code>","text":"<p>\"Returns the version</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\"Returns the version\"\"\"\n    return self.version\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainTool.webInstall","title":"<code>webInstall(user_instance, construct_render)</code>","text":"<p>\"Returns a web installer for the given user instance and construct render template</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def webInstall(self, user_instance, construct_render) -&gt; str:\n    \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainToolType","title":"<code>MainToolType</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class MainToolType:\n    toolID: str\n    app: A\n    interface: ToolBoxInterfaces\n    spec: str\n\n    version: str\n    tools: dict  # legacy\n    name: str\n    logger: logging\n    color: str\n    todo: Callable\n    _on_exit: Callable\n    stuf: bool\n    config: dict\n    user: U | None\n    description: str\n\n    @staticmethod\n    def return_result(error: ToolBoxError = ToolBoxError.none,\n                      exec_code: int = 0,\n                      help_text: str = \"\",\n                      data_info=None,\n                      data=None,\n                      data_to=None) -&gt; Result:\n        \"\"\"proxi attr\"\"\"\n\n    def load(self):\n        \"\"\"proxi attr\"\"\"\n\n    def print(self, message, end=\"\\n\", **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def add_str_to_config(self, command):\n        if len(command) != 2:\n            self.logger.error('Invalid command must be key value')\n            return False\n        self.config[command[0]] = command[1]\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n\n    async def get_user(self, username: str) -&gt; Result:\n        return self.app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME, username=username, get_results=True)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainToolType.load","title":"<code>load()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def load(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainToolType.print","title":"<code>print(message, end='\\n', **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def print(self, message, end=\"\\n\", **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainToolType.return_result","title":"<code>return_result(error=ToolBoxError.none, exec_code=0, help_text='', data_info=None, data=None, data_to=None)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef return_result(error: ToolBoxError = ToolBoxError.none,\n                  exec_code: int = 0,\n                  help_text: str = \"\",\n                  data_info=None,\n                  data=None,\n                  data_to=None) -&gt; Result:\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainToolType.webInstall","title":"<code>webInstall(user_instance, construct_render)</code>","text":"<p>\"Returns a web installer for the given user instance and construct render template</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def webInstall(self, user_instance, construct_render) -&gt; str:\n    \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result","title":"<code>Result</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class Result:\n    _task = None\n    def __init__(self,\n                 error: ToolBoxError,\n                 result: ToolBoxResult,\n                 info: ToolBoxInfo,\n                 origin: Any | None = None,\n                 ):\n        self.error: ToolBoxError = error\n        self.result: ToolBoxResult = result\n        self.info: ToolBoxInfo = info\n        self.origin = origin\n\n    def as_result(self):\n        return self\n\n    def as_dict(self):\n        return {\n            \"error\":self.error.value if isinstance(self.error, Enum) else self.error,\n        \"result\" : {\n            \"data_to\":self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n            \"data_info\":self.result.data_info,\n            \"data\":self.result.data,\n            \"data_type\":self.result.data_type\n        } if self.result else None,\n        \"info\" : {\n            \"exec_code\" : self.info.exec_code,  # exec_code umwandel in http resposn codes\n        \"help_text\" : self.info.help_text\n        } if self.info else None,\n        \"origin\" : self.origin\n        }\n\n    def set_origin(self, origin):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = origin\n        return self\n\n    def set_dir_origin(self, name, extras=\"assets/\"):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = f\"mods/{name}/{extras}\"\n        return self\n\n    def is_error(self):\n        if _test_is_result(self.result.data):\n            return self.result.data.is_error()\n        return self.info.exec_code != 0\n\n    def is_data(self):\n        return self.result.data is not None\n\n    def to_api_result(self):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=self.error.value if isinstance(self.error, Enum) else self.error,\n            result=ToolBoxResultBM(\n                data_to=self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n                data_info=self.result.data_info,\n                data=self.result.data,\n                data_type=self.result.data_type\n            ) if self.result else None,\n            info=ToolBoxInfoBM(\n                exec_code=self.info.exec_code,  # exec_code umwandel in http resposn codes\n                help_text=self.info.help_text\n            ) if self.info else None,\n            origin=self.origin\n        )\n\n    def task(self, task):\n        self._task = task\n        return self\n\n    @staticmethod\n    def result_from_dict(error: str, result: dict, info: dict, origin: list or None or str):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=error if isinstance(error, Enum) else error,\n            result=ToolBoxResultBM(\n                data_to=result.get('data_to') if isinstance(result.get('data_to'), Enum) else result.get('data_to'),\n                data_info=result.get('data_info', '404'),\n                data=result.get('data'),\n                data_type=result.get('data_type', '404'),\n            ) if result else None,\n            info=ToolBoxInfoBM(\n                exec_code=info.get('exec_code', 404),\n                help_text=info.get('help_text', '404')\n            ) if info else None,\n            origin=origin\n        ).as_result()\n\n    @classmethod\n    def stream(cls,\n               stream_generator,\n               content_type=\"text/event-stream\",\n               headers=None,\n               info=\"OK\",\n               interface=ToolBoxInterfaces.remote,\n               cleanup_func=None):\n        \"\"\"\n        Create a streaming response Result that properly handles all types of stream sources.\n\n        Args:\n            stream_generator: Any stream source (async generator, sync generator, iterable, or even string)\n            content_type: Content-Type header (default: text/event-stream for SSE)\n            headers: Additional HTTP headers\n            info: Help text for the result\n            interface: Interface to send data to\n\n        Returns:\n            A Result object configured for streaming\n        \"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        # Standard SSE headers\n        standard_headers = {\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"X-Accel-Buffering\": \"no\"\n        }\n\n        # Apply custom headers\n        all_headers = standard_headers.copy()\n        if headers:\n            all_headers.update(headers)\n\n        # Handle different types of stream sources\n        if content_type == \"text/event-stream\":\n            wrapped_generator = stream_generator\n            if inspect.isgenerator(stream_generator) or hasattr(stream_generator, '__iter__'):\n                # Sync generator or iterable\n                wrapped_generator = SSEGenerator.create_sse_stream(stream_generator, cleanup_func)\n\n            elif isinstance(stream_generator, str):\n                # String (could be a memory address or other reference)\n                # Convert to a generator that yields a single string\n                async def string_to_stream():\n                    yield stream_generator\n\n                wrapped_generator = SSEGenerator.create_sse_stream(string_to_stream(), cleanup_func)\n\n            # The final generator to use\n            final_generator = wrapped_generator\n\n        else:\n            # For non-SSE streams, use the original generator\n            final_generator = stream_generator\n\n        # Prepare streaming data\n        streaming_data = {\n            \"type\": \"stream\",\n            \"generator\": final_generator,\n            \"content_type\": content_type,\n            \"headers\": all_headers\n        }\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=streaming_data,\n            data_info=\"Streaming response\",\n            data_type=\"stream\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def default(cls, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=-1, help_text=\"\")\n        result = ToolBoxResult(data_to=interface)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a JSON response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=data,\n            data_info=\"JSON response\",\n            data_type=\"json\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n        \"\"\"Create a text response Result with specific content type.\"\"\"\n        if headers is not None:\n            return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=text_data,\n            data_info=\"Text response\",\n            data_type=content_type\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n               interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a binary data response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        # Create a dictionary with binary data and metadata\n        binary_data = {\n            \"data\": data,\n            \"content_type\": content_type,\n            \"filename\": download_name\n        }\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=binary_data,\n            data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n            data_type=\"binary\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a redirect response.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=url,\n            data_info=\"Redirect response\",\n            data_type=\"redirect\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def ok(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def html(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.remote, data_type=\"html\",status=200, headers=None):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=status, help_text=info)\n\n        if isinstance(headers, dict):\n            result = ToolBoxResult(data_to=interface, data={'html':data,'headers':headers}, data_info=data_info,\n                                   data_type=\"special_html\")\n        else:\n            result = ToolBoxResult(data_to=interface, data=data, data_info=data_info,\n                                   data_type=data_type if data_type is not None else type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def future(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.future):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=\"future\")\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def custom_error(cls, data=None, data_info=\"\", info=\"\", exec_code=-1, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def error(cls, data=None, data_info=\"\", info=\"\", exec_code=450, interface=ToolBoxInterfaces.remote):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_user_error(cls, info=\"\", exec_code=-3, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.input_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_internal_error(cls, info=\"\", exec_code=-2, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.internal_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    def print(self, show=True, show_data=True, prifix=\"\"):\n        data = '\\n' + f\"{((prifix + 'Data: ' + str(self.result.data) if self.result.data is not None else 'NO Data') if not isinstance(self.result.data, Result) else self.result.data.print(show=False, show_data=show_data, prifix=prifix + '-')) if show_data else 'Data: private'}\"\n        origin = '\\n' + f\"{prifix + 'Origin: ' + str(self.origin) if self.origin is not None else 'NO Origin'}\"\n        text = (f\"Function Exec code: {self.info.exec_code}\"\n                f\"\\n{prifix}Info's:\"\n                f\" {self.info.help_text} {'&lt;|&gt; ' + str(self.result.data_info) if self.result.data_info is not None else ''}\"\n                f\"{origin}{data if not data.endswith('NO Data') else ''}\")\n        if not show:\n            return text\n        print(\"\\n======== Result ========\\n\" + text + \"\\n------- EndOfD -------\")\n        return self\n\n    def log(self, show_data=True, prifix=\"\"):\n        from toolboxv2 import get_logger\n        get_logger().debug(self.print(show=False, show_data=show_data, prifix=prifix).replace(\"\\n\", \" - \"))\n        return self\n\n    def __str__(self):\n        return self.print(show=False, show_data=True)\n\n    def get(self, key=None, default=None):\n        data = self.result.data\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    async def aget(self, key=None, default=None):\n        if asyncio.isfuture(self.result.data) or asyncio.iscoroutine(self.result.data) or (\n            isinstance(self.result.data_to, Enum) and self.result.data_to.name == ToolBoxInterfaces.future.name):\n            data = await self.result.data\n        else:\n            data = self.get(key=None, default=None)\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    def lazy_return(self, _=0, data=None, **kwargs):\n        flags = ['raise', 'logg', 'user', 'intern']\n        flag = flags[_] if isinstance(_, int) else _\n        if self.info.exec_code == 0:\n            return self if data is None else data if _test_is_result(data) else self.ok(data=data, **kwargs)\n        if flag == 'raise':\n            raise ValueError(self.print(show=False))\n        if flag == 'logg':\n            from .. import get_logger\n            get_logger().error(self.print(show=False))\n\n        if flag == 'user':\n            return self if data is None else data if _test_is_result(data) else self.default_user_error(data=data,\n                                                                                                        **kwargs)\n        if flag == 'intern':\n            return self if data is None else data if _test_is_result(data) else self.default_internal_error(data=data,\n                                                                                                            **kwargs)\n\n        return self if data is None else data if _test_is_result(data) else self.custom_error(data=data, **kwargs)\n\n    @property\n    def bg_task(self):\n        return self._task\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.binary","title":"<code>binary(data, content_type='application/octet-stream', download_name=None, info='OK', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a binary data response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n           interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a binary data response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    # Create a dictionary with binary data and metadata\n    binary_data = {\n        \"data\": data,\n        \"content_type\": content_type,\n        \"filename\": download_name\n    }\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=binary_data,\n        data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n        data_type=\"binary\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.json","title":"<code>json(data, info='OK', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a JSON response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a JSON response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=data,\n        data_info=\"JSON response\",\n        data_type=\"json\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.redirect","title":"<code>redirect(url, status_code=302, info='Redirect', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a redirect response.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a redirect response.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=url,\n        data_info=\"Redirect response\",\n        data_type=\"redirect\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.stream","title":"<code>stream(stream_generator, content_type='text/event-stream', headers=None, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code>  <code>classmethod</code>","text":"<p>Create a streaming response Result that properly handles all types of stream sources.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <p>Any stream source (async generator, sync generator, iterable, or even string)</p> required <code>content_type</code> <p>Content-Type header (default: text/event-stream for SSE)</p> <code>'text/event-stream'</code> <code>headers</code> <p>Additional HTTP headers</p> <code>None</code> <code>info</code> <p>Help text for the result</p> <code>'OK'</code> <code>interface</code> <p>Interface to send data to</p> <code>remote</code> <p>Returns:</p> Type Description <p>A Result object configured for streaming</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef stream(cls,\n           stream_generator,\n           content_type=\"text/event-stream\",\n           headers=None,\n           info=\"OK\",\n           interface=ToolBoxInterfaces.remote,\n           cleanup_func=None):\n    \"\"\"\n    Create a streaming response Result that properly handles all types of stream sources.\n\n    Args:\n        stream_generator: Any stream source (async generator, sync generator, iterable, or even string)\n        content_type: Content-Type header (default: text/event-stream for SSE)\n        headers: Additional HTTP headers\n        info: Help text for the result\n        interface: Interface to send data to\n\n    Returns:\n        A Result object configured for streaming\n    \"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    # Standard SSE headers\n    standard_headers = {\n        \"Cache-Control\": \"no-cache\",\n        \"Connection\": \"keep-alive\",\n        \"X-Accel-Buffering\": \"no\"\n    }\n\n    # Apply custom headers\n    all_headers = standard_headers.copy()\n    if headers:\n        all_headers.update(headers)\n\n    # Handle different types of stream sources\n    if content_type == \"text/event-stream\":\n        wrapped_generator = stream_generator\n        if inspect.isgenerator(stream_generator) or hasattr(stream_generator, '__iter__'):\n            # Sync generator or iterable\n            wrapped_generator = SSEGenerator.create_sse_stream(stream_generator, cleanup_func)\n\n        elif isinstance(stream_generator, str):\n            # String (could be a memory address or other reference)\n            # Convert to a generator that yields a single string\n            async def string_to_stream():\n                yield stream_generator\n\n            wrapped_generator = SSEGenerator.create_sse_stream(string_to_stream(), cleanup_func)\n\n        # The final generator to use\n        final_generator = wrapped_generator\n\n    else:\n        # For non-SSE streams, use the original generator\n        final_generator = stream_generator\n\n    # Prepare streaming data\n    streaming_data = {\n        \"type\": \"stream\",\n        \"generator\": final_generator,\n        \"content_type\": content_type,\n        \"headers\": all_headers\n    }\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=streaming_data,\n        data_info=\"Streaming response\",\n        data_type=\"stream\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.text","title":"<code>text(text_data, content_type='text/plain', exec_code=None, status=200, info='OK', interface=ToolBoxInterfaces.remote, headers=None)</code>  <code>classmethod</code>","text":"<p>Create a text response Result with specific content type.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n    \"\"\"Create a text response Result with specific content type.\"\"\"\n    if headers is not None:\n        return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=text_data,\n        data_info=\"Text response\",\n        data_type=content_type\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.all_functions_enums","title":"<code>all_functions_enums</code>","text":"<p>Automatic generated by ToolBox v = 0.1.21</p>"},{"location":"toolboxv2/#toolboxv2.utils.system.api","title":"<code>api</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.api.build_cargo_project","title":"<code>build_cargo_project(debug=False)</code>","text":"<p>Build the Cargo project, optionally in debug mode.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def build_cargo_project(debug=False):\n    \"\"\"Build the Cargo project, optionally in debug mode.\"\"\"\n    mode = \"debug\" if debug else \"release\"\n    args = [\"cargo\", \"build\"]\n    if not debug:\n        args.append(\"--release\")\n\n    print(f\"Building in {mode} mode...\")\n    try:\n        subprocess.run(args, cwd=os.path.join(\".\", \"src-core\"), check=True)\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Cargo build failed: {e}\")\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.check_and_run_local_release","title":"<code>check_and_run_local_release(do_run=True)</code>","text":"<p>Search for a pre-built release executable in the src-core folder and run it if found.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def check_and_run_local_release(do_run=True):\n    \"\"\"Search for a pre-built release executable in the src-core folder and run it if found.\"\"\"\n    src_core_path = os.path.join(\".\", \"src-core\")\n    if os.path.isdir(src_core_path):\n        # Define the path to the expected release executable, assuming a Cargo project structure\n        expected_name = \"simple-core-server.exe\" if platform.system().lower() == \"windows\" else \"simple-core-server\"\n        release_path = os.path.join(src_core_path, expected_name)\n        if os.path.isfile(release_path):\n            print(\"Found pre-built release executable.\")\n            return release_path if not do_run else run_executable(release_path)\n        release_path = os.path.join(src_core_path, \"target\", \"release\", expected_name)\n        if os.path.isfile(release_path):\n            print(\"Found pre-built release executable.\")\n            # Move the executable from target/release to src_core_path for easier access next time\n            dest_path = os.path.join(src_core_path, expected_name)\n            try:\n                import shutil\n                shutil.copy2(release_path, dest_path)\n                print(f\"Copied executable to {dest_path} for easier access next time\")\n            except Exception as e:\n                print(f\"Failed to copy executable: {e}\")\n                return False\n            if do_run:\n                run_executable(dest_path)\n            else:\n                return dest_path\n            return True\n    return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.check_cargo_installed","title":"<code>check_cargo_installed()</code>","text":"<p>Check if Cargo (Rust package manager) is installed on the system.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def check_cargo_installed():\n    \"\"\"Check if Cargo (Rust package manager) is installed on the system.\"\"\"\n    try:\n        subprocess.run([\"cargo\", \"--version\"], check=True, capture_output=True)\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.cleanup_build_files","title":"<code>cleanup_build_files()</code>","text":"<p>Cleans up build files.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def cleanup_build_files():\n    \"\"\"Cleans up build files.\"\"\"\n    src_core_path = os.path.join(\".\", \"src-core\")\n    target_path = os.path.join(src_core_path, \"target\")\n\n    if os.path.exists(target_path):\n        try:\n            print(f\"Cleaning up build files in {target_path}...\")\n            # First try using cargo clean\n            try:\n                subprocess.run([\"cargo\", \"clean\"], cwd=src_core_path, check=True)\n                print(\"Successfully cleaned up build files with cargo clean\")\n            except subprocess.CalledProcessError:\n                # If cargo clean fails, manually remove directories\n                print(\"Cargo clean failed, manually removing build directories...\")\n                for item in os.listdir(target_path):\n                    item_path = os.path.join(target_path, item)\n                    if os.path.isdir(item_path) and item != \".rustc_info.json\":\n                        shutil.rmtree(item_path)\n                        print(f\"Removed {item_path}\")\n            return True\n        except Exception as e:\n            print(f\"Failed to clean up build files: {e}\")\n            return False\n    else:\n        print(f\"Build directory {target_path} not found\")\n        return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.create_dill_archive","title":"<code>create_dill_archive(site_packages, output_file='python312.dill')</code>","text":"<p>Package dill and all dependencies into a single .dill archive.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def create_dill_archive(site_packages, output_file=\"python312.dill\"):\n    \"\"\"Package dill and all dependencies into a single .dill archive.\"\"\"\n    try:\n        temp_dir = \"/tmp/dill_package\"\n        os.makedirs(temp_dir, exist_ok=True)\n\n        # Copy only necessary packages\n        packages = [\"dill\"]\n        for package in packages:\n            package_path = os.path.join(site_packages, package)\n            if os.path.exists(package_path):\n                shutil.copytree(package_path, os.path.join(temp_dir, package), dirs_exist_ok=True)\n            else:\n                print(f\"Warning: {package} not found in site-packages.\")\n\n        # Create the .dill archive\n        with tarfile.open(output_file, \"w:gz\") as tar:\n            tar.add(temp_dir, arcname=\".\")\n\n        print(f\"Successfully created {output_file}\")\n\n        # Clean up\n        shutil.rmtree(temp_dir)\n\n    except Exception as e:\n        print(f\"Error creating .dill archive: {e}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.detect_os_and_arch","title":"<code>detect_os_and_arch()</code>","text":"<p>Detect the current operating system and architecture.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def detect_os_and_arch():\n    \"\"\"Detect the current operating system and architecture.\"\"\"\n    current_os = platform.system().lower()  # e.g., 'windows', 'linux', 'darwin'\n    machine = platform.machine().lower()  # e.g., 'x86_64', 'amd64'\n    return current_os, machine\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.download_executable","title":"<code>download_executable(url, file_name)</code>","text":"<p>Attempt to download the executable from the provided URL.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def download_executable(url, file_name):\n    \"\"\"Attempt to download the executable from the provided URL.\"\"\"\n    try:\n        import requests\n    except ImportError:\n        print(\"The 'requests' library is required. Please install it via pip install requests\")\n        sys.exit(1)\n\n    print(f\"Attempting to download executable from {url}...\")\n    try:\n        response = requests.get(url, stream=True)\n    except Exception as e:\n        print(f\"Download error: {e}\")\n        return None\n\n    if response.status_code == 200:\n        with open(file_name, \"wb\") as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:\n                    f.write(chunk)\n        # Make the file executable on non-Windows systems\n        if platform.system().lower() != \"windows\":\n            os.chmod(file_name, 0o755)\n        return file_name\n    else:\n        print(\"Download failed. Status code:\", response.status_code)\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.ensure_socket_and_fd_file_posix","title":"<code>ensure_socket_and_fd_file_posix(host, port, backlog, fd_file_path)</code>","text":"<p>POSIX: Ensures a listening socket exists and its FD is in the fd_file.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def ensure_socket_and_fd_file_posix(host, port, backlog, fd_file_path) -&gt; tuple[socket.socket | None, int | None]:\n    \"\"\"POSIX: Ensures a listening socket exists and its FD is in the fd_file.\"\"\"\n    if os.path.exists(fd_file_path):\n        try:\n            with open(fd_file_path) as f:\n                fd = int(f.read().strip())\n            # Basic check (less reliable than on-demand creation for ensuring liveness)\n            # This check is mostly to see if the FD *number* is plausible.\n            # The real test is when the Rust server tries to use it.\n            print(f\"[POSIX] Found existing persistent FD {fd} in {fd_file_path}.\")\n            # We don't return a socket object if FD exists, Rust will use the FD directly.\n            return None, fd\n        except Exception as e:\n            print(f\"[POSIX] Persistent FD file {fd_file_path} exists but FD invalid/unreadable: {e}. Will create new.\")\n            with contextlib.suppress(OSError): os.remove(fd_file_path)\n\n    try:\n        server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        if hasattr(os, 'set_inheritable'): # Python 3.4+\n            os.set_inheritable(server_socket.fileno(), True)\n        else: # POSIX, Python &lt; 3.4 (fcntl not on Windows)\n            import fcntl\n            fd_num = server_socket.fileno()\n            flags = fcntl.fcntl(fd_num, fcntl.F_GETFD)\n            fcntl.fcntl(fd_num, fcntl.F_SETFD, flags &amp; ~fcntl.FD_CLOEXEC)\n\n        server_socket.bind((host, port))\n        server_socket.listen(backlog)\n        fd = server_socket.fileno()\n        with open(fd_file_path, 'w') as f: f.write(str(fd))\n        os.chmod(fd_file_path, 0o600)\n        print(f\"[POSIX] Created new socket. FD {fd} saved to {fd_file_path}.\")\n        return server_socket, fd # Return the socket object for the initial creator\n    except Exception as e:\n        print(f\"[POSIX] Fatal: Could not create and save listening socket FD: {e}\")\n        return None, None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.find_highest_zip_version","title":"<code>find_highest_zip_version(name_filter, app_version=None, root_dir='mods_sto', version_only=False)</code>","text":"<p>Findet die h\u00f6chste verf\u00fcgbare ZIP-Version in einem Verzeichnis basierend auf einem Namensfilter.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Wurzelverzeichnis f\u00fcr die Suche</p> <code>'mods_sto'</code> <code>name_filter</code> <code>str</code> <p>Namensfilter f\u00fcr die ZIP-Dateien</p> required <code>app_version</code> <code>str</code> <p>Aktuelle App-Version f\u00fcr Kompatibilit\u00e4tspr\u00fcfung</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Pfad zur ZIP-Datei mit der h\u00f6chsten Version oder None wenn keine gefunden</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def find_highest_zip_version(name_filter: str, app_version: str = None, root_dir: str = \"mods_sto\", version_only=False) -&gt; str:\n    \"\"\"\n    Findet die h\u00f6chste verf\u00fcgbare ZIP-Version in einem Verzeichnis basierend auf einem Namensfilter.\n\n    Args:\n        root_dir (str): Wurzelverzeichnis f\u00fcr die Suche\n        name_filter (str): Namensfilter f\u00fcr die ZIP-Dateien\n        app_version (str, optional): Aktuelle App-Version f\u00fcr Kompatibilit\u00e4tspr\u00fcfung\n\n    Returns:\n        str: Pfad zur ZIP-Datei mit der h\u00f6chsten Version oder None wenn keine gefunden\n    \"\"\"\n\n    # Kompiliere den Regex-Pattern f\u00fcr die Dateinamen\n    pattern = fr\"{name_filter}&amp;v[0-9.]+\u00a7([0-9.]+)\\.zip$\"\n\n    highest_version = None\n    highest_version_file = None\n\n    # Durchsuche das Verzeichnis\n    root_path = Path(root_dir)\n    for file_path in root_path.rglob(\"*.zip\"):\n        if \"RST$\"+name_filter not in str(file_path):\n            continue\n        match = re.search(pattern, str(file_path).split(\"RST$\")[-1].strip())\n        if match:\n            zip_version = match.group(1)\n\n            # Pr\u00fcfe App-Version Kompatibilit\u00e4t falls angegeben\n            if app_version:\n                file_app_version = re.search(r\"&amp;v([0-9.]+)\u00a7\", str(file_path)).group(1)\n                if version.parse(file_app_version) &gt; version.parse(app_version):\n                    continue\n\n            # Vergleiche Versionen\n            current_version = version.parse(zip_version)\n            if highest_version is None or current_version &gt; highest_version:\n                highest_version = current_version\n                highest_version_file = str(file_path)\n    if version_only:\n        return str(highest_version)\n    return highest_version_file\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.find_highest_zip_version_entry","title":"<code>find_highest_zip_version_entry(name, target_app_version=None, filepath='tbState.yaml')</code>","text":"<p>Findet den Eintrag mit der h\u00f6chsten ZIP-Version f\u00fcr einen gegebenen Namen und eine optionale Ziel-App-Version in einer YAML-Datei.</p> <p>:param name: Der Name des gesuchten Eintrags. :param target_app_version: Die Zielversion der App als String (optional). :param filepath: Der Pfad zur YAML-Datei. :return: Den Eintrag mit der h\u00f6chsten ZIP-Version innerhalb der Ziel-App-Version oder None, falls nicht gefunden.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def find_highest_zip_version_entry(name, target_app_version=None, filepath='tbState.yaml'):\n    \"\"\"\n    Findet den Eintrag mit der h\u00f6chsten ZIP-Version f\u00fcr einen gegebenen Namen und eine optionale Ziel-App-Version in einer YAML-Datei.\n\n    :param name: Der Name des gesuchten Eintrags.\n    :param target_app_version: Die Zielversion der App als String (optional).\n    :param filepath: Der Pfad zur YAML-Datei.\n    :return: Den Eintrag mit der h\u00f6chsten ZIP-Version innerhalb der Ziel-App-Version oder None, falls nicht gefunden.\n    \"\"\"\n    import yaml\n    highest_zip_ver = None\n    highest_entry = {}\n\n    with open(filepath) as file:\n        data = yaml.safe_load(file)\n        # print(data)\n        app_ver_h = None\n        for key, value in list(data.get('installable', {}).items())[::-1]:\n            # Pr\u00fcfe, ob der Name im Schl\u00fcssel enthalten ist\n\n            if name in key:\n                v = value['version']\n                if len(v) == 1:\n                    app_ver = v[0].split('v')[-1]\n                    zip_ver = \"0.0.0\"\n                else:\n                    app_ver, zip_ver = v\n                    app_ver = app_ver.split('v')[-1]\n                app_ver = version.parse(app_ver)\n                # Wenn eine Ziel-App-Version angegeben ist, vergleiche sie\n                if target_app_version is None or app_ver == version.parse(target_app_version):\n                    current_zip_ver = version.parse(zip_ver)\n                    # print(current_zip_ver, highest_zip_ver)\n\n                    if highest_zip_ver is None or current_zip_ver &gt; highest_zip_ver:\n                        highest_zip_ver = current_zip_ver\n                        highest_entry = value\n\n                    if app_ver_h is None or app_ver &gt; app_ver_h:\n                        app_ver_h = app_ver\n                        highest_zip_ver = current_zip_ver\n                        highest_entry = value\n    return highest_entry\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.get_uv_site_packages","title":"<code>get_uv_site_packages()</code>","text":"<p>Find the site-packages directory for a uv-managed virtual environment.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def get_uv_site_packages():\n    \"\"\"Find the site-packages directory for a uv-managed virtual environment.\"\"\"\n    try:\n        site_packages = subprocess.check_output([\"uv\", \"info\", \"--json\"], text=True)\n        import json\n        data = json.loads(site_packages)\n        return data[\"venv\"][\"site_packages\"]\n    except Exception as e:\n        print(f\"Error finding uv site-packages: {e}\")\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.is_uv_installed","title":"<code>is_uv_installed()</code>","text":"<p>Check if uv is installed.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def is_uv_installed():\n    \"\"\"Check if uv is installed.\"\"\"\n    try:\n        subprocess.run([\"uv\", \"--version\"], check=True, capture_output=True, text=True)\n        return True\n    except FileNotFoundError:\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.main_api_runner","title":"<code>main_api_runner(debug=False, run=True)</code>","text":"<p>Main function to run the API server. When debug=True, enables hot reloading and runs in debug mode.</p> <p>Non blocking!</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def main_api_runner(debug=False, run=True):\n    \"\"\"\n    Main function to run the API server.\n    When debug=True, enables hot reloading and runs in debug mode.\n\n    Non blocking!\n    \"\"\"\n    if not os.path.exists(os.getenv(\"PY_DILL\", '.')):\n        add_py_dill()\n    if is_uv_installed():\n        print(f\"VIRTUAL_ENV=$ {os.getenv('VIRTUAL_ENV')} {os.getenv('PY_SITE_PACKAGES')}\")\n        os.environ[\"VIRTUAL_ENV\"] = os.getenv('UV_BASE_ENV', os.getenv('VIRTUAL_ENV'))\n        # os.environ[\"PY_SITE_PACKAGES\"] = os.getenv('PY_SITE_PACKAGES')\n    if debug:\n        print(\"Starting in DEBUG mode with hot reloading enabled...\")\n        if check_cargo_installed():\n            run_with_hot_reload()\n        else:\n            print(\"Cargo is not installed. Hot reloading requires Cargo.\")\n        return None\n\n    # Release mode flow\n    if exe := check_and_run_local_release(run):\n        return exe\n\n    # Step 1: Detect current OS and machine architecture\n    current_os, machine = detect_os_and_arch()\n    print(f\"Detected OS: {current_os}, Architecture: {machine}\")\n\n    # Step 2: Attempt to download executable from remote URL\n    url, file_name = query_executable_url(current_os, machine)\n    downloaded_exe = download_executable(url, file_name)\n\n    if downloaded_exe and run:\n        print(\"Downloaded executable. Executing it...\")\n        run_executable(downloaded_exe)\n        return None\n\n    if downloaded_exe and not run:\n        return downloaded_exe\n\n    # Step 3: Fallback: Check for local pre-built release executable in src-core folder\n    print(\"Remote executable not found. Searching local 'src-core' folder...\")\n    if exe := check_and_run_local_release():\n        return exe\n    else:\n        print(\"Pre-built release executable not found locally.\")\n\n        # Step 4: If executable not found locally, check if Cargo is installed\n        if not check_cargo_installed():\n\n            print(\"Cargo is not installed. Please install Cargo to build the project.\")\n            return None\n\n        print(\"Cargo is installed. Proceeding with build.\")\n        if not build_cargo_project(debug=False):\n\n            print(\"Failed to build the Cargo project.\")\n            return None\n\n        # After successful build, try running the release executable again\n        if exe := check_and_run_local_release(run):\n            return exe\n\n        print(\"Release executable missing even after build.\")\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.query_executable_url","title":"<code>query_executable_url(current_os, machine)</code>","text":"<p>Query a remote URL for a matching executable based on OS and architecture. The file name is built dynamically based on parameters.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def query_executable_url(current_os, machine):\n    \"\"\"\n    Query a remote URL for a matching executable based on OS and architecture.\n    The file name is built dynamically based on parameters.\n    \"\"\"\n    base_url = \"https://example.com/downloads\"  # Replace with the actual URL\n    # Windows executables have .exe extension\n    if current_os == \"windows\":\n        file_name = f\"server_{current_os}_{machine}.exe\"\n    else:\n        file_name = f\"server_{current_os}_{machine}\"\n    full_url = f\"{base_url}/{file_name}\"\n    return full_url, file_name\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.remove_release_executable","title":"<code>remove_release_executable()</code>","text":"<p>Removes the release executable.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def remove_release_executable():\n    \"\"\"Removes the release executable.\"\"\"\n    src_core_path = os.path.join(\".\", \"src-core\")\n    expected_name = \"simple-core-server.exe\" if platform.system().lower() == \"windows\" else \"simple-core-server\"\n\n    # Remove from src-core root\n    direct_path = os.path.join(src_core_path, expected_name)\n    if os.path.exists(direct_path):\n        try:\n            os.remove(direct_path)\n            print(f\"Removed release executable: {direct_path}\")\n        except Exception as e:\n            print(f\"Failed to remove {direct_path}: {e}\")\n\n    # Remove from target/release\n    release_path = os.path.join(src_core_path, \"target\", \"release\", expected_name)\n    if os.path.exists(release_path):\n        try:\n            os.remove(release_path)\n            print(f\"Removed release executable: {release_path}\")\n        except Exception as e:\n            print(f\"Failed to remove {release_path}: {e}\")\n\n    return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.run_executable","title":"<code>run_executable(file_path)</code>","text":"<p>Run the executable file.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def run_executable(file_path):\n    \"\"\"Run the executable file.\"\"\"\n    try:\n        print(\"Running it.\")\n        subprocess.run([os.path.abspath(file_path)], check=True)\n    except subprocess.CalledProcessError as e:\n        print(f\"Failed to execute {file_path}: {e}\")\n    except KeyboardInterrupt:\n        print(\"Exiting call from:\", file_path)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.run_in_debug_mode","title":"<code>run_in_debug_mode()</code>","text":"<p>Run the Cargo project in debug mode.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def run_in_debug_mode():\n    \"\"\"Run the Cargo project in debug mode.\"\"\"\n    src_core_path = os.path.join(\".\", \"src-core\")\n    print(\"Running in debug mode...\")\n    try:\n        subprocess.run([\"cargo\", \"run\"], cwd=src_core_path)\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Debug execution failed: {e}\")\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.run_with_hot_reload","title":"<code>run_with_hot_reload()</code>","text":"<p>Run the Cargo project with hot reloading.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def run_with_hot_reload():\n    \"\"\"Run the Cargo project with hot reloading.\"\"\"\n    src_core_path = os.path.join(\".\", \"src-core\")\n\n    # Check if cargo-watch is installed\n    try:\n        subprocess.run([\"cargo\", \"watch\", \"--version\"], check=True, capture_output=True)\n    except Exception:\n        print(\"cargo-watch is not installed. Installing now...\")\n        try:\n            subprocess.run([\"cargo\", \"install\", \"cargo-watch\"], check=True)\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to install cargo-watch: {e}\")\n            print(\"Running without hot reload\")\n            return run_in_debug_mode()\n\n    print(\"Running with hot reload in debug mode...\")\n    try:\n        subprocess.run([\"cargo\", \"watch\", \"-x\", \"run\"], cwd=src_core_path)\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Hot reload execution failed: {e}\")\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.start_rust_server_posix","title":"<code>start_rust_server_posix(executable_path, persistent_fd)</code>","text":"<p>POSIX: Starts Rust server passing the persistent_fd.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def start_rust_server_posix(executable_path: str, persistent_fd: int):\n    \"\"\"POSIX: Starts Rust server passing the persistent_fd.\"\"\"\n    abs_executable_path = Path(executable_path).resolve()\n    env = os.environ.copy()\n    env[\"PERSISTENT_LISTENER_FD\"] = str(persistent_fd)\n    env[\"LISTEN_FDS\"] = str(persistent_fd) # Also set for listenfd standard mechanism\n    env[\"LISTEN_PID\"] = str(os.getpid())\n    print(f\"[POSIX] Starting Rust server: {abs_executable_path} using FD {persistent_fd}\")\n    try:\n        process = subprocess.Popen(\n            [str(abs_executable_path)],\n            cwd=abs_executable_path.parent,\n            env=env,\n        )\n        return process\n    except Exception as e:\n        print(f\"[POSIX] Failed to start Rust server {abs_executable_path}: {e}\")\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.start_rust_server_windows","title":"<code>start_rust_server_windows(executable_path)</code>","text":"<p>WINDOWS: Starts Rust server normally. It will bind its own socket.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def start_rust_server_windows(executable_path: str):\n    \"\"\"WINDOWS: Starts Rust server normally. It will bind its own socket.\"\"\"\n    abs_executable_path = Path(executable_path).resolve()\n    print(f\"[WINDOWS] Starting Rust server: {abs_executable_path}. It will bind its own socket.\")\n    try:\n        process = subprocess.Popen(\n            [str(abs_executable_path)],\n            cwd=abs_executable_path.parent,\n            # No special env vars for socket needed for Windows fallback\n        )\n        return process\n    except Exception as e:\n        print(f\"[WINDOWS] Failed to start Rust server {abs_executable_path}: {e}\")\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.update_server","title":"<code>update_server(new_executable_path, new_version)</code>","text":"<p>High-level update function, calls platform-specific logic.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def update_server(new_executable_path: str, new_version: str):\n    \"\"\"High-level update function, calls platform-specific logic.\"\"\"\n    if platform.system().lower() == \"windows\":\n        return update_server_windows(new_executable_path, new_version)\n    else: # POSIX\n        return update_server_posix(new_executable_path, new_version)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.update_server_posix","title":"<code>update_server_posix(new_executable_path, new_version)</code>","text":"<p>POSIX: Zero-downtime update using persistent FD.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def update_server_posix(new_executable_path: str, new_version: str):\n    \"\"\"POSIX: Zero-downtime update using persistent FD.\"\"\"\n    if not psutil: return False\n    print(f\"--- [POSIX] Starting Update to {new_version} ---\")\n    old_pid, old_version, old_exe_path = read_server_state()\n\n    if not os.path.exists(PERSISTENT_FD_FILE):\n        print(f\"[POSIX] Error: FD file '{PERSISTENT_FD_FILE}' not found. Cannot update.\")\n        return False\n    try:\n        with open(PERSISTENT_FD_FILE) as f: persistent_fd = int(f.read().strip())\n        print(f\"[POSIX] Using persistent listener FD: {persistent_fd}\")\n    except Exception as e:\n        print(f\"[POSIX] Error reading FD from '{PERSISTENT_FD_FILE}': {e}\")\n        return False\n\n    new_process = start_rust_server_posix(new_executable_path, persistent_fd)\n    if new_process is None: return False\n    time.sleep(5) # Allow new server to init\n    if new_process.poll() is not None:\n        print(f\"[POSIX] New server (PID {new_process.pid}) died. Exit: {new_process.poll()}. Update failed.\")\n        return False\n    print(f\"[POSIX] New server (v{new_version}, PID {new_process.pid}) started.\")\n\n    if old_pid and is_process_running(old_pid):\n        print(f\"[POSIX] Stopping old server (v{old_version}, PID {old_pid})...\")\n        if not stop_process(old_pid):\n            print(f\"[POSIX] Warning: Failed to stop old server PID {old_pid}.\")\n    else:\n        print(\"[POSIX] No old server or PID was stale.\")\n\n    write_server_state(new_process.pid, new_version, new_executable_path)\n    print(f\"--- [POSIX] Update to {new_version} complete. New PID: {new_process.pid} ---\")\n    return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.update_server_windows","title":"<code>update_server_windows(new_executable_path, new_version)</code>","text":"<p>WINDOWS: Graceful restart (stop old, start new).</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def update_server_windows(new_executable_path: str, new_version: str):\n    \"\"\"WINDOWS: Graceful restart (stop old, start new).\"\"\"\n    if not psutil: return False\n    print(f\"--- [WINDOWS] Starting Update (Graceful Restart) to {new_version} ---\")\n    old_pid, old_version, old_exe_path = read_server_state()\n\n    if old_pid and is_process_running(old_pid):\n        print(f\"[WINDOWS] Stopping old server (v{old_version}, PID {old_pid})...\")\n        if not stop_process(old_pid):\n            print(f\"[WINDOWS] Failed to stop old server PID {old_pid}. Update aborted to prevent conflicts.\")\n            return False\n        print(\"[WINDOWS] Old server stopped.\")\n        time.sleep(2) # Give OS time to release port\n    else:\n        print(\"[WINDOWS] No old server running or PID was stale.\")\n\n    new_process = start_rust_server_windows(new_executable_path)\n    if new_process is None: return False\n    time.sleep(3) # Allow new server to init\n    if new_process.poll() is not None:\n        print(f\"[WINDOWS] New server (PID {new_process.pid}) died. Exit: {new_process.poll()}. Update failed.\")\n        return False\n    print(f\"[WINDOWS] New server (v{new_version}, PID {new_process.pid}) started.\")\n\n    write_server_state(new_process.pid, new_version, new_executable_path)\n    print(f\"--- [WINDOWS] Update to {new_version} complete. New PID: {new_process.pid} ---\")\n    return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.conda_runner","title":"<code>conda_runner</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.conda_runner.create_env_registry","title":"<code>create_env_registry(env_name)</code>","text":"<p>Create a JSON registry of all packages installed in the specified conda environment.</p> <p>Args: env_name (str): Name of the conda environment</p> <p>Returns: bool: True if registry creation was successful, False otherwise</p> Source code in <code>toolboxv2/utils/system/conda_runner.py</code> <pre><code>def create_env_registry(env_name: str) -&gt; bool:\n    \"\"\"\n    Create a JSON registry of all packages installed in the specified conda environment.\n\n    Args:\n    env_name (str): Name of the conda environment\n\n    Returns:\n    bool: True if registry creation was successful, False otherwise\n    \"\"\"\n    # Get list of installed packages\n    command = f\"conda list -n {env_name} --json\"\n    success, output = run_command(command, live=False)\n\n    if not success or output is None:\n        print(f\"Failed to get package list for environment {env_name}\")\n        return False\n\n    try:\n        # Parse the JSON output\n        packages = json.loads(output)\n\n        # Create a simplified registry with package names and versions\n        registry = [{\"name\": pkg[\"name\"], \"version\": pkg[\"version\"]} for pkg in packages]\n\n        # Write the registry to a JSON file\n        registry_file = f\"{env_name}_registry.json\"\n        with open(registry_file, 'w') as f:\n            json.dump(registry, f, indent=2)\n\n        print(f\"Registry created successfully: {registry_file}\")\n        return True\n\n    except json.JSONDecodeError:\n        print(f\"Failed to parse package list for environment {env_name}\")\n        return False\n    except OSError:\n        print(f\"Failed to write registry file for environment {env_name}\")\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.main_tool","title":"<code>main_tool</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.main_tool.MainTool","title":"<code>MainTool</code>","text":"Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>class MainTool:\n    toolID: str = \"\"\n    # app = None\n    interface = None\n    spec = \"app\"\n    name = \"\"\n    color = \"Bold\"\n    stuf = False\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.__storedargs = args, kwargs\n        self.async_initialized = False\n\n    async def __ainit__(self, *args, **kwargs):\n        self.version = kwargs[\"v\"]\n        self.tools = kwargs.get(\"tool\", {})\n        self.name = kwargs[\"name\"]\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n        if not hasattr(self, 'config'):\n            self.config = {}\n        self.user = None\n        self.description = \"A toolbox mod\" if kwargs.get(\"description\") is None else kwargs.get(\"description\")\n        if MainTool.interface is None:\n            MainTool.interface = self.app.interface_type\n        # Result.default(self.app.interface)\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    await self.todo()\n                else:\n                    self.todo()\n                await asyncio.sleep(0.1)\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\" Error loading mod {self.name} {e}\")\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n\n        self.app.print(f\"TOOL : {self.spec}.{self.name} online\")\n\n    @property\n    def app(self):\n        return get_app(\n            from_=f\"{self.spec}.{self.name}|{self.toolID if self.toolID else '*' + MainTool.toolID} {self.interface if self.interface else MainTool.interface}\")\n\n    @app.setter\n    def app(self, v):\n        raise PermissionError(f\"You cannot set the App Instance! {v=}\")\n\n    @staticmethod\n    def return_result(error: ToolBoxError = ToolBoxError.none,\n                      exec_code: int = 0,\n                      help_text: str = \"\",\n                      data_info=None,\n                      data=None,\n                      data_to=None):\n\n        if data_to is None:\n            data_to = MainTool.interface if MainTool.interface is not None else ToolBoxInterfaces.cli\n\n        if data is None:\n            data = {}\n\n        if data_info is None:\n            data_info = {}\n\n        return Result(\n            error,\n            ToolBoxResult(data_info=data_info, data=data, data_to=data_to),\n            ToolBoxInfo(exec_code=exec_code, help_text=help_text)\n        )\n\n    def print(self, message, end=\"\\n\", **kwargs):\n        if self.stuf:\n            return\n\n        self.app.print(Style.style_dic[self.color] + self.name + Style.style_dic[\"END\"] + \":\", message, end=end,\n                       **kwargs)\n\n    def add_str_to_config(self, command):\n        if len(command) != 2:\n            self.logger.error('Invalid command must be key value')\n            return False\n        self.config[command[0]] = command[1]\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n\n    def get_version(self) -&gt; str:\n        \"\"\"\"Returns the version\"\"\"\n        return self.version\n\n    async def get_user(self, username: str) -&gt; Result:\n        return await self.app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME, username=username, get_results=True)\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n</code></pre> <code>__init__(*args, **kwargs)</code> \u00b6 <p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.__storedargs = args, kwargs\n    self.async_initialized = False\n</code></pre> <code>__initobj()</code> <code>async</code> \u00b6 <p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre> <code>get_version()</code> \u00b6 <p>\"Returns the version</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\"Returns the version\"\"\"\n    return self.version\n</code></pre> <code>webInstall(user_instance, construct_render)</code> \u00b6 <p>\"Returns a web installer for the given user instance and construct render template</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def webInstall(self, user_instance, construct_render) -&gt; str:\n    \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.main_tool.get_version_from_pyproject","title":"<code>get_version_from_pyproject(pyproject_path='../pyproject.toml')</code>","text":"<p>Reads the version from the pyproject.toml file.</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def get_version_from_pyproject(pyproject_path='../pyproject.toml'):\n    \"\"\"Reads the version from the pyproject.toml file.\"\"\"\n    if not os.path.exists(pyproject_path) and pyproject_path=='../pyproject.toml':\n        pyproject_path = 'pyproject.toml'\n    if not os.path.exists(pyproject_path) and pyproject_path=='pyproject.toml':\n        return \"0.1.21\"\n\n    try:\n        import toml\n        # Load the pyproject.toml file\n        with open(pyproject_path) as file:\n            pyproject_data = toml.load(file)\n\n        # Extract the version from the 'project' section\n        version = pyproject_data.get('project', {}).get('version')\n\n        if version is None:\n            raise ValueError(f\"Version not found in {pyproject_path}\")\n\n        return version\n    except Exception as e:\n        print(f\"Error reading version: {e}\")\n        return \"0.0.0\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.state_system","title":"<code>state_system</code>","text":"<p>The Task of the State System is : 1 Kep trak of the current state of the ToolBox and its dependency's 2 tracks the shasum of all mod and runnabael 3 the version of all mod</p> <p>The state : {\"utils\":{\"file_name\": {\"version\":##,\"shasum\"}} ,\"mods\":{\"file_name\": {\"version\":##,\"shasum\":##,\"src-url\":##}} ,\"runnable\":{\"file_name\": {\"version\":##,\"shasum\":##,\"src-url\":##}} ,\"api\":{\"file_name\": {\"version\":##,\"shasum\"}} ,\"app\":{\"file_name\": {\"version\":##,\"shasum\":##,\"src-url\":##}} }</p> <p>trans form state from on to an other.</p>"},{"location":"toolboxv2/#toolboxv2.utils.system.types","title":"<code>types</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.types.AppType","title":"<code>AppType</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class AppType:\n    prefix: str\n    id: str\n    globals: dict[str, Any] = {\"root\": dict, }\n    locals: dict[str, Any] = {\"user\": {'app': \"self\"}, }\n\n    local_test: bool = False\n    start_dir: str\n    data_dir: str\n    config_dir: str\n    info_dir: str\n\n    logger: logging.Logger\n    logging_filename: str\n\n    api_allowed_mods_list: list[str] = []\n\n    version: str\n    loop: asyncio.AbstractEventLoop\n\n    keys: dict[str, str] = {\n        \"MACRO\": \"macro~~~~:\",\n        \"MACRO_C\": \"m_color~~:\",\n        \"HELPER\": \"helper~~~:\",\n        \"debug\": \"debug~~~~:\",\n        \"id\": \"name-spa~:\",\n        \"st-load\": \"mute~load:\",\n        \"comm-his\": \"comm-his~:\",\n        \"develop-mode\": \"dev~mode~:\",\n        \"provider::\": \"provider::\",\n    }\n\n    defaults: dict[str, (bool or dict or dict[str, dict[str, str]] or str or list[str] or list[list]) | None] = {\n        \"MACRO\": list[str],\n        \"MACRO_C\": dict,\n        \"HELPER\": dict,\n        \"debug\": str,\n        \"id\": str,\n        \"st-load\": False,\n        \"comm-his\": list[list],\n        \"develop-mode\": bool,\n    }\n\n    config_fh: FileHandler\n    _debug: bool\n    flows: dict[str, Callable]\n    dev_modi: bool\n    functions: dict[str, Any]\n    modules: dict[str, Any]\n\n    interface_type: ToolBoxInterfaces\n    REFIX: str\n\n    alive: bool\n    called_exit: tuple[bool, float]\n    args_sto: AppArgs\n    system_flag = None\n    session = None\n    appdata = None\n    exit_tasks = []\n\n    enable_profiling: bool = False\n    sto = None\n\n    def __init__(self, prefix: None | str= None, args: AppArgs | None = None):\n        self.args_sto = args\n        self.prefix = prefix\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def exit_main(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def hide_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def show_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def disconnect(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def set_logger(self, debug=False):\n        \"\"\"proxi attr\"\"\"\n\n    @property\n    def debug(self):\n        \"\"\"proxi attr\"\"\"\n        return self._debug\n\n    def debug_rains(self, e):\n        \"\"\"proxi attr\"\"\"\n\n    def set_flows(self, r):\n        \"\"\"proxi attr\"\"\"\n\n    def run_flows(self, name, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def rrun_flows(self, name, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def idle(self):\n        import time\n        self.print(\"idle\")\n        try:\n            while self.alive:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            pass\n        self.print(\"idle done\")\n\n    async def a_idle(self):\n        self.print(\"a idle\")\n        try:\n            if hasattr(self, 'daemon_app'):\n                self.print(\"serving daemon\")\n                await self.daemon_app.connect(self)\n            else:\n                self.print(\"serving default\")\n                while self.alive:\n                    await asyncio.sleep(1)\n        except KeyboardInterrupt:\n            pass\n        self.print(\"a idle done\")\n\n    @debug.setter\n    def debug(self, value):\n        \"\"\"proxi attr\"\"\"\n\n    def _coppy_mod(self, content, new_mod_dir, mod_name, file_type='py'):\n        \"\"\"proxi attr\"\"\"\n\n    def _pre_lib_mod(self, mod_name, path_to=\"./runtime\", file_type='py'):\n        \"\"\"proxi attr\"\"\"\n\n    def _copy_load(self, mod_name, file_type='py', **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True):\n        \"\"\"proxi attr\"\"\"\n\n    def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n        \"\"\"proxi attr\"\"\"\n\n    def save_initialized_module(self, tools_class, spec):\n        \"\"\"proxi attr\"\"\"\n\n    def mod_online(self, mod_name, installed=False):\n        \"\"\"proxi attr\"\"\"\n\n    def _get_function(self,\n                      name: Enum or None,\n                      state: bool = True,\n                      specification: str = \"app\",\n                      metadata=False, as_str: tuple or None = None, r=0):\n        \"\"\"proxi attr\"\"\"\n\n    def save_exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def load_mod(self, mod_name: str, mlm='I', **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    async def init_module(self, modular):\n        return await self.load_mod(modular)\n\n    async def load_all_mods_in_file(self, working_dir=\"mods\"):\n        \"\"\"proxi attr\"\"\"\n\n    def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\"):\n        \"\"\"proxi attr\"\"\"\n\n    def remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            self.remove_mod(mod, delete=delete)\n\n    async def a_remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            await self.a_remove_mod(mod, delete=delete)\n\n    def print_ok(self):\n        \"\"\"proxi attr\"\"\"\n        self.logger.info(\"OK\")\n\n    def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n        \"\"\"proxi attr\"\"\"\n\n    def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None):\n        \"\"\"proxi attr\"\"\"\n\n    def remove_mod(self, mod_name, spec='app', delete=True):\n        \"\"\"proxi attr\"\"\"\n\n    async def a_remove_mod(self, mod_name, spec='app', delete=True):\n        \"\"\"proxi attr\"\"\"\n\n    def exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def web_context(self) -&gt; str:\n        \"\"\"returns the build index ( toolbox web component )\"\"\"\n\n    async def a_exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def save_load(self, modname, spec='app'):\n        \"\"\"proxi attr\"\"\"\n\n    def get_function(self, name: Enum or tuple, **kwargs):\n        \"\"\"\n        Kwargs for _get_function\n            metadata:: return the registered function dictionary\n                stateless: (function_data, None), 0\n                stateful: (function_data, higher_order_function), 0\n            state::boolean\n                specification::str default app\n        \"\"\"\n\n    def run_a_from_sync(self, function, *args):\n        \"\"\"\n        run a async fuction\n        \"\"\"\n\n    def run_function(self, mod_function_name: Enum or tuple,\n                     tb_run_function_with_state=True,\n                     tb_run_with_specification='app',\n                     args_=None,\n                     kwargs_=None,\n                     *args,\n                     **kwargs) -&gt; Result:\n\n        \"\"\"proxi attr\"\"\"\n\n    async def a_run_function(self, mod_function_name: Enum or tuple,\n                             tb_run_function_with_state=True,\n                             tb_run_with_specification='app',\n                             args_=None,\n                             kwargs_=None,\n                             *args,\n                             **kwargs) -&gt; Result:\n\n        \"\"\"proxi attr\"\"\"\n\n    def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n        \"\"\"\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        proxi attr\n        \"\"\"\n\n    async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict):\n        \"\"\"\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        proxi attr\n        \"\"\"\n\n    async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None, method=\"GET\",\n                       args_=None,\n                       kwargs_=None,\n                       *args, **kwargs):\n        \"\"\"run a function remote via http / https\"\"\"\n\n    def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n                get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                kwargs_=None,\n                *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                        backwords_compability_variabel_string_holder=None,\n                        get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                        kwargs_=None,\n                        *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def print(text, *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def sprint(text, *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    # ----------------------------------------------------------------\n    # Decorators for the toolbox\n\n    def _register_function(self, module_name, func_name, data):\n        \"\"\"proxi attr\"\"\"\n\n    def _create_decorator(self, type_: str,\n                          name: str = \"\",\n                          mod_name: str = \"\",\n                          level: int = -1,\n                          restrict_in_virtual_mode: bool = False,\n                          api: bool = False,\n                          helper: str = \"\",\n                          version: str or None = None,\n                          initial=False,\n                          exit_f=False,\n                          test=True,\n                          samples=None,\n                          state=None,\n                          pre_compute=None,\n                          post_compute=None,\n                          memory_cache=False,\n                          file_cache=False,\n                          row=False,\n                          request_as_kwarg=False,\n                          memory_cache_max_size=100,\n                          memory_cache_ttl=300):\n        \"\"\"proxi attr\"\"\"\n\n        # data = {\n        #     \"type\": type_,\n        #     \"module_name\": module_name,\n        #     \"func_name\": func_name,\n        #     \"level\": level,\n        #     \"restrict_in_virtual_mode\": restrict_in_virtual_mode,\n        #     \"func\": func,\n        #     \"api\": api,\n        #     \"helper\": helper,\n        #     \"version\": version,\n        #     \"initial\": initial,\n        #     \"exit_f\": exit_f,\n        #     \"__module__\": func.__module__,\n        #     \"signature\": sig,\n        #     \"params\": params,\n        #     \"state\": (\n        #         False if len(params) == 0 else params[0] in ['self', 'state', 'app']) if state is None else state,\n        #     \"do_test\": test,\n        #     \"samples\": samples,\n        #     \"request_as_kwarg\": request_as_kwarg,\n\n    def tb(self, name=None,\n           mod_name: str = \"\",\n           helper: str = \"\",\n           version: str or None = None,\n           test: bool = True,\n           restrict_in_virtual_mode: bool = False,\n           api: bool = False,\n           initial: bool = False,\n           exit_f: bool = False,\n           test_only: bool = False,\n           memory_cache: bool = False,\n           file_cache: bool = False,\n           row=False,\n           request_as_kwarg: bool = False,\n           state: bool or None = None,\n           level: int = 0,\n           memory_cache_max_size: int = 100,\n           memory_cache_ttl: int = 300,\n           samples: list or dict or None = None,\n           interface: ToolBoxInterfaces or None or str = None,\n           pre_compute=None,\n           post_compute=None,\n           api_methods=None,\n           ):\n        \"\"\"\n    A decorator for registering and configuring functions within a module.\n\n    This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\n    Args:\n        name (str, optional): The name to register the function under. Defaults to the function's own name.\n        mod_name (str, optional): The name of the module the function belongs to.\n        helper (str, optional): A helper string providing additional information about the function.\n        version (str or None, optional): The version of the function or module.\n        test (bool, optional): Flag to indicate if the function is for testing purposes.\n        restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n        api (bool, optional): Flag to indicate if the function is part of an API.\n        initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n        exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n        test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n        memory_cache (bool, optional): Flag to enable memory caching for the function.\n        request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n        file_cache (bool, optional): Flag to enable file caching for the function.\n        row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n        state (bool or None, optional): Flag to indicate if the function maintains state.\n        level (int, optional): The level of the function, used for prioritization or categorization.\n        memory_cache_max_size (int, optional): Maximum size of the memory cache.\n        memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n        samples (list or dict or None, optional): Samples or examples of function usage.\n        interface (str, optional): The interface type for the function.\n        pre_compute (callable, optional): A function to be called before the main function.\n        post_compute (callable, optional): A function to be called after the main function.\n        api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\n    Returns:\n        function: The decorated function with additional processing and registration capabilities.\n    \"\"\"\n        if interface is None:\n            interface = \"tb\"\n        if test_only and 'test' not in self.id:\n            return lambda *args, **kwargs: args\n        return self._create_decorator(interface,\n                                      name,\n                                      mod_name,\n                                      level=level,\n                                      restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                      helper=helper,\n                                      api=api,\n                                      version=version,\n                                      initial=initial,\n                                      exit_f=exit_f,\n                                      test=test,\n                                      samples=samples,\n                                      state=state,\n                                      pre_compute=pre_compute,\n                                      post_compute=post_compute,\n                                      memory_cache=memory_cache,\n                                      file_cache=file_cache,\n                                      row=row,\n                                      request_as_kwarg=request_as_kwarg,\n                                      memory_cache_max_size=memory_cache_max_size,\n                                      memory_cache_ttl=memory_cache_ttl)\n\n    def print_functions(self, name=None):\n\n\n        if not self.functions:\n            print(\"Nothing to see\")\n            return\n\n        def helper(_functions):\n            for func_name, data in _functions.items():\n                if not isinstance(data, dict):\n                    continue\n\n                func_type = data.get('type', 'Unknown')\n                func_level = 'r' if data['level'] == -1 else data['level']\n                api_status = 'Api' if data.get('api', False) else 'Non-Api'\n\n                print(f\"  Function: {func_name}{data.get('signature', '()')}; \"\n                      f\"Type: {func_type}, Level: {func_level}, {api_status}\")\n\n        if name is not None:\n            functions = self.functions.get(name)\n            if functions is not None:\n                print(f\"\\nModule: {name}; Type: {functions.get('app_instance_type', 'Unknown')}\")\n                helper(functions)\n                return\n        for module, functions in self.functions.items():\n            print(f\"\\nModule: {module}; Type: {functions.get('app_instance_type', 'Unknown')}\")\n            helper(functions)\n\n    def save_autocompletion_dict(self):\n        \"\"\"proxi attr\"\"\"\n\n    def get_autocompletion_dict(self):\n        \"\"\"proxi attr\"\"\"\n\n    def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n        \"\"\"proxi attr\"\"\"\n\n    def save_registry_as_enums(self, directory: str, filename: str):\n        \"\"\"proxi attr\"\"\"\n\n    async def execute_all_functions_(self, m_query='', f_query=''):\n        print(\"Executing all functions\")\n        from ..extras import generate_test_cases\n        all_data = {\n            \"modular_run\": 0,\n            \"modular_fatal_error\": 0,\n            \"errors\": 0,\n            \"modular_sug\": 0,\n            \"coverage\": [],\n            \"total_coverage\": {},\n        }\n        items = list(self.functions.items()).copy()\n        for module_name, functions in items:\n            infos = {\n                \"functions_run\": 0,\n                \"functions_fatal_error\": 0,\n                \"error\": 0,\n                \"functions_sug\": 0,\n                'calls': {},\n                'callse': {},\n                \"coverage\": [0, 0],\n            }\n            all_data['modular_run'] += 1\n            if not module_name.startswith(m_query):\n                all_data['modular_sug'] += 1\n                continue\n\n            with Spinner(message=f\"In {module_name}| \"):\n                f_items = list(functions.items()).copy()\n                for function_name, function_data in f_items:\n                    if not isinstance(function_data, dict):\n                        continue\n                    if not function_name.startswith(f_query):\n                        continue\n                    test: list = function_data.get('do_test')\n                    # print(test, module_name, function_name, function_data)\n                    infos[\"coverage\"][0] += 1\n                    if test is False:\n                        continue\n\n                    with Spinner(message=f\"\\t\\t\\t\\t\\t\\tfuction {function_name}...\"):\n                        params: list = function_data.get('params')\n                        sig: signature = function_data.get('signature')\n                        state: bool = function_data.get('state')\n                        samples: bool = function_data.get('samples')\n\n                        test_kwargs_list = [{}]\n\n                        if params is not None:\n                            test_kwargs_list = samples if samples is not None else generate_test_cases(sig=sig)\n                            # print(test_kwargs)\n                            # print(test_kwargs[0])\n                            # test_kwargs = test_kwargs_list[0]\n                        # print(module_name, function_name, test_kwargs_list)\n                        infos[\"coverage\"][1] += 1\n                        for test_kwargs in test_kwargs_list:\n                            try:\n                                # print(f\"test Running {state=} |{module_name}.{function_name}\")\n                                result = await self.a_run_function((module_name, function_name),\n                                                                   tb_run_function_with_state=state,\n                                                                   **test_kwargs)\n                                if not isinstance(result, Result):\n                                    result = Result.ok(result)\n                                if result.info.exec_code == 0:\n                                    infos['calls'][function_name] = [test_kwargs, str(result)]\n                                    infos['functions_sug'] += 1\n                                else:\n                                    infos['functions_sug'] += 1\n                                    infos['error'] += 1\n                                    infos['callse'][function_name] = [test_kwargs, str(result)]\n                            except Exception as e:\n                                infos['functions_fatal_error'] += 1\n                                infos['callse'][function_name] = [test_kwargs, str(e)]\n                            finally:\n                                infos['functions_run'] += 1\n\n                if infos['functions_run'] == infos['functions_sug']:\n                    all_data['modular_sug'] += 1\n                else:\n                    all_data['modular_fatal_error'] += 1\n                if infos['error'] &gt; 0:\n                    all_data['errors'] += infos['error']\n\n                all_data[module_name] = infos\n                if infos['coverage'][0] == 0:\n                    c = 0\n                else:\n                    c = infos['coverage'][1] / infos['coverage'][0]\n                all_data[\"coverage\"].append(f\"{module_name}:{c:.2f}\\n\")\n        total_coverage = sum([float(t.split(\":\")[-1]) for t in all_data[\"coverage\"]]) / len(all_data[\"coverage\"])\n        print(\n            f\"\\n{all_data['modular_run']=}\\n{all_data['modular_sug']=}\\n{all_data['modular_fatal_error']=}\\n{total_coverage=}\")\n        d = analyze_data(all_data)\n        return Result.ok(data=all_data, data_info=d)\n\n    @staticmethod\n    def calculate_complexity(filename_or_code):\n        from radon.complexity import cc_rank, cc_visit\n        if os.path.exists(filename_or_code):\n            with open(filename_or_code) as file:\n                code = file.read()\n        else:\n            code = filename_or_code\n\n        # Calculate and print Cyclomatic Complexity\n        complexity_results = cc_visit(code)\n        i = -1\n        avg_complexity = 0\n        for block in complexity_results:\n            complexity = block.complexity\n            i += 1\n            print(f\"block: {block.name} {i} Class/Fuction/Methode : {block.letter}\")\n            print(f\"    fullname: {block.fullname}\")\n            print(f\"    Cyclomatic Complexity: {complexity}\")\n            # Optional: Get complexity rank\n            avg_complexity += complexity\n            rank = cc_rank(complexity)\n            print(f\"    Complexity Rank: {rank}\")\n            # print(f\"    lineno: {block.lineno}\")\n            print(f\"    endline: {block.endline}\")\n            print(f\"    col_offset: {block.col_offset}\\n\")\n        if i &lt;= 0:\n            i += 2\n        avg_complexity = avg_complexity / i\n        print(f\"\\nAVG Complexity: {avg_complexity:.2f}\")\n        print(f\"Total Rank: {cc_rank(int(avg_complexity + i // 10))}\")\n\n    async def execute_function_test(self, module_name: str, function_name: str,\n                                    function_data: dict, test_kwargs: dict,\n                                    profiler: cProfile.Profile) -&gt; tuple[bool, str, dict, float]:\n        start_time = time.time()\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            try:\n                result = await self.a_run_function(\n                    (module_name, function_name),\n                    tb_run_function_with_state=function_data.get('state'),\n                    **test_kwargs\n                )\n\n                if not isinstance(result, Result):\n                    result = Result.ok(result)\n\n                success = result.info.exec_code == 0\n                execution_time = time.time() - start_time\n                return success, str(result), test_kwargs, execution_time\n            except Exception as e:\n                execution_time = time.time() - start_time\n                return False, str(e), test_kwargs, execution_time\n\n    async def process_function(self, module_name: str, function_name: str,\n                               function_data: dict, profiler: cProfile.Profile) -&gt; tuple[str, ModuleInfo]:\n        start_time = time.time()\n        info = ModuleInfo()\n\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            if not isinstance(function_data, dict):\n                return function_name, info\n\n            test = function_data.get('do_test')\n            info.coverage[0] += 1\n\n            if test is False:\n                return function_name, info\n\n            params = function_data.get('params')\n            sig = function_data.get('signature')\n            samples = function_data.get('samples')\n\n            test_kwargs_list = [{}] if params is None else (\n                samples if samples is not None else generate_test_cases(sig=sig)\n            )\n\n            info.coverage[1] += 1\n\n            # Create tasks for all test cases\n            tasks = [\n                self.execute_function_test(module_name, function_name, function_data, test_kwargs, profiler)\n                for test_kwargs in test_kwargs_list\n            ]\n\n            # Execute all tests concurrently\n            results = await asyncio.gather(*tasks)\n\n            total_execution_time = 0\n            for success, result_str, test_kwargs, execution_time in results:\n                info.functions_run += 1\n                total_execution_time += execution_time\n\n                if success:\n                    info.functions_sug += 1\n                    info.calls[function_name] = [test_kwargs, result_str]\n                else:\n                    info.functions_sug += 1\n                    info.error += 1\n                    info.callse[function_name] = [test_kwargs, result_str]\n\n            info.execution_time = time.time() - start_time\n            return function_name, info\n\n    async def process_module(self, module_name: str, functions: dict,\n                             f_query: str, profiler: cProfile.Profile) -&gt; tuple[str, ModuleInfo]:\n        start_time = time.time()\n\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            async with asyncio.Semaphore(mp.cpu_count()):\n                tasks = [\n                    self.process_function(module_name, fname, fdata, profiler)\n                    for fname, fdata in functions.items()\n                    if fname.startswith(f_query)\n                ]\n\n                if not tasks:\n                    return module_name, ModuleInfo()\n\n                results = await asyncio.gather(*tasks)\n\n                # Combine results from all functions in the module\n                combined_info = ModuleInfo()\n                total_execution_time = 0\n\n                for _, info in results:\n                    combined_info.functions_run += info.functions_run\n                    combined_info.functions_fatal_error += info.functions_fatal_error\n                    combined_info.error += info.error\n                    combined_info.functions_sug += info.functions_sug\n                    combined_info.calls.update(info.calls)\n                    combined_info.callse.update(info.callse)\n                    combined_info.coverage[0] += info.coverage[0]\n                    combined_info.coverage[1] += info.coverage[1]\n                    total_execution_time += info.execution_time\n\n                combined_info.execution_time = time.time() - start_time\n                return module_name, combined_info\n\n    async def execute_all_functions(self, m_query='', f_query='', enable_profiling=True):\n        \"\"\"\n        Execute all functions with parallel processing and optional profiling.\n\n        Args:\n            m_query (str): Module name query filter\n            f_query (str): Function name query filter\n            enable_profiling (bool): Enable detailed profiling information\n        \"\"\"\n        print(\"Executing all functions in parallel\" + (\" with profiling\" if enable_profiling else \"\"))\n\n        start_time = time.time()\n        stats = ExecutionStats()\n        items = list(self.functions.items()).copy()\n\n        # Set up profiling\n        self.enable_profiling = enable_profiling\n        profiler = cProfile.Profile()\n\n        with profile_section(profiler, enable_profiling):\n            # Filter modules based on query\n            filtered_modules = [\n                (mname, mfuncs) for mname, mfuncs in items\n                if mname.startswith(m_query)\n            ]\n\n            stats.modular_run = len(filtered_modules)\n\n            # Process all modules concurrently\n            async with asyncio.Semaphore(mp.cpu_count()):\n                tasks = [\n                    self.process_module(mname, mfuncs, f_query, profiler)\n                    for mname, mfuncs in filtered_modules\n                ]\n\n                results = await asyncio.gather(*tasks)\n\n            # Combine results and calculate statistics\n            for module_name, info in results:\n                if info.functions_run == info.functions_sug:\n                    stats.modular_sug += 1\n                else:\n                    stats.modular_fatal_error += 1\n\n                stats.errors += info.error\n\n                # Calculate coverage\n                coverage = (info.coverage[1] / info.coverage[0]) if info.coverage[0] &gt; 0 else 0\n                stats.coverage.append(f\"{module_name}:{coverage:.2f}\\n\")\n\n                # Store module info\n                stats.__dict__[module_name] = info\n\n            # Calculate total coverage\n            total_coverage = (\n                sum(float(t.split(\":\")[-1]) for t in stats.coverage) / len(stats.coverage)\n                if stats.coverage else 0\n            )\n\n            stats.total_execution_time = time.time() - start_time\n\n            # Generate profiling stats if enabled\n            if enable_profiling:\n                s = io.StringIO()\n                ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n                ps.print_stats()\n                stats.profiling_data = {\n                    'detailed_stats': s.getvalue(),\n                    'total_time': stats.total_execution_time,\n                    'function_count': stats.modular_run,\n                    'successful_functions': stats.modular_sug\n                }\n\n            print(\n                f\"\\n{stats.modular_run=}\"\n                f\"\\n{stats.modular_sug=}\"\n                f\"\\n{stats.modular_fatal_error=}\"\n                f\"\\n{total_coverage=}\"\n                f\"\\nTotal execution time: {stats.total_execution_time:.2f}s\"\n            )\n\n            if enable_profiling:\n                print(\"\\nProfiling Summary:\")\n                print(f\"{'=' * 50}\")\n                print(\"Top 10 time-consuming functions:\")\n                ps.print_stats(10)\n\n            analyzed_data = analyze_data(stats.__dict__)\n            return Result.ok(data=stats.__dict__, data_info=analyzed_data)\n</code></pre> <code>debug</code> <code>property</code> <code>writable</code> \u00b6 <p>proxi attr</p> <code>prefix = prefix</code> <code>instance-attribute</code> \u00b6 <p>proxi attr</p> <code>a_exit()</code> <code>async</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>a_fuction_runner(function, function_data, args, kwargs)</code> <code>async</code> \u00b6 <p>parameters = function_data.get('params') modular_name = function_data.get('module_name') function_name = function_data.get('func_name') mod_function_name = f\"{modular_name}.{function_name}\"</p> <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict):\n    \"\"\"\n    parameters = function_data.get('params')\n    modular_name = function_data.get('module_name')\n    function_name = function_data.get('func_name')\n    mod_function_name = f\"{modular_name}.{function_name}\"\n\n    proxi attr\n    \"\"\"\n</code></pre> <code>a_remove_mod(mod_name, spec='app', delete=True)</code> <code>async</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_remove_mod(self, mod_name, spec='app', delete=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>a_run_any(mod_function_name, backwords_compability_variabel_string_holder=None, get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code> <code>async</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                    backwords_compability_variabel_string_holder=None,\n                    get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                    kwargs_=None,\n                    *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>a_run_function(mod_function_name, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code> <code>async</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_run_function(self, mod_function_name: Enum or tuple,\n                         tb_run_function_with_state=True,\n                         tb_run_with_specification='app',\n                         args_=None,\n                         kwargs_=None,\n                         *args,\n                         **kwargs) -&gt; Result:\n\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>debug_rains(e)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def debug_rains(self, e):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>disconnect(*args, **kwargs)</code> <code>async</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def disconnect(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>execute_all_functions(m_query='', f_query='', enable_profiling=True)</code> <code>async</code> \u00b6 <p>Execute all functions with parallel processing and optional profiling.</p> <p>Parameters:</p> Name Type Description Default <code>m_query</code> <code>str</code> <p>Module name query filter</p> <code>''</code> <code>f_query</code> <code>str</code> <p>Function name query filter</p> <code>''</code> <code>enable_profiling</code> <code>bool</code> <p>Enable detailed profiling information</p> <code>True</code> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def execute_all_functions(self, m_query='', f_query='', enable_profiling=True):\n    \"\"\"\n    Execute all functions with parallel processing and optional profiling.\n\n    Args:\n        m_query (str): Module name query filter\n        f_query (str): Function name query filter\n        enable_profiling (bool): Enable detailed profiling information\n    \"\"\"\n    print(\"Executing all functions in parallel\" + (\" with profiling\" if enable_profiling else \"\"))\n\n    start_time = time.time()\n    stats = ExecutionStats()\n    items = list(self.functions.items()).copy()\n\n    # Set up profiling\n    self.enable_profiling = enable_profiling\n    profiler = cProfile.Profile()\n\n    with profile_section(profiler, enable_profiling):\n        # Filter modules based on query\n        filtered_modules = [\n            (mname, mfuncs) for mname, mfuncs in items\n            if mname.startswith(m_query)\n        ]\n\n        stats.modular_run = len(filtered_modules)\n\n        # Process all modules concurrently\n        async with asyncio.Semaphore(mp.cpu_count()):\n            tasks = [\n                self.process_module(mname, mfuncs, f_query, profiler)\n                for mname, mfuncs in filtered_modules\n            ]\n\n            results = await asyncio.gather(*tasks)\n\n        # Combine results and calculate statistics\n        for module_name, info in results:\n            if info.functions_run == info.functions_sug:\n                stats.modular_sug += 1\n            else:\n                stats.modular_fatal_error += 1\n\n            stats.errors += info.error\n\n            # Calculate coverage\n            coverage = (info.coverage[1] / info.coverage[0]) if info.coverage[0] &gt; 0 else 0\n            stats.coverage.append(f\"{module_name}:{coverage:.2f}\\n\")\n\n            # Store module info\n            stats.__dict__[module_name] = info\n\n        # Calculate total coverage\n        total_coverage = (\n            sum(float(t.split(\":\")[-1]) for t in stats.coverage) / len(stats.coverage)\n            if stats.coverage else 0\n        )\n\n        stats.total_execution_time = time.time() - start_time\n\n        # Generate profiling stats if enabled\n        if enable_profiling:\n            s = io.StringIO()\n            ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n            ps.print_stats()\n            stats.profiling_data = {\n                'detailed_stats': s.getvalue(),\n                'total_time': stats.total_execution_time,\n                'function_count': stats.modular_run,\n                'successful_functions': stats.modular_sug\n            }\n\n        print(\n            f\"\\n{stats.modular_run=}\"\n            f\"\\n{stats.modular_sug=}\"\n            f\"\\n{stats.modular_fatal_error=}\"\n            f\"\\n{total_coverage=}\"\n            f\"\\nTotal execution time: {stats.total_execution_time:.2f}s\"\n        )\n\n        if enable_profiling:\n            print(\"\\nProfiling Summary:\")\n            print(f\"{'=' * 50}\")\n            print(\"Top 10 time-consuming functions:\")\n            ps.print_stats(10)\n\n        analyzed_data = analyze_data(stats.__dict__)\n        return Result.ok(data=stats.__dict__, data_info=analyzed_data)\n</code></pre> <code>exit()</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>exit_main(*args, **kwargs)</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef exit_main(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>fuction_runner(function, function_data, args, kwargs, t0=0.0)</code> \u00b6 <p>parameters = function_data.get('params') modular_name = function_data.get('module_name') function_name = function_data.get('func_name') mod_function_name = f\"{modular_name}.{function_name}\"</p> <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n    \"\"\"\n    parameters = function_data.get('params')\n    modular_name = function_data.get('module_name')\n    function_name = function_data.get('func_name')\n    mod_function_name = f\"{modular_name}.{function_name}\"\n\n    proxi attr\n    \"\"\"\n</code></pre> <code>get_all_mods(working_dir='mods', path_to='./runtime')</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>get_autocompletion_dict()</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_autocompletion_dict(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>get_function(name, **kwargs)</code> \u00b6 <p>Kwargs for _get_function     metadata:: return the registered function dictionary         stateless: (function_data, None), 0         stateful: (function_data, higher_order_function), 0     state::boolean         specification::str default app</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_function(self, name: Enum or tuple, **kwargs):\n    \"\"\"\n    Kwargs for _get_function\n        metadata:: return the registered function dictionary\n            stateless: (function_data, None), 0\n            stateful: (function_data, higher_order_function), 0\n        state::boolean\n            specification::str default app\n    \"\"\"\n</code></pre> <code>get_mod(name, spec='app')</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>get_username(get_input=False, default='loot')</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>hide_console(*args, **kwargs)</code> <code>async</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def hide_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>inplace_load_instance(mod_name, loc='toolboxv2.mods.', spec='app', save=True)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>load_all_mods_in_file(working_dir='mods')</code> <code>async</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def load_all_mods_in_file(self, working_dir=\"mods\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>load_mod(mod_name, mlm='I', **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def load_mod(self, mod_name: str, mlm='I', **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>mod_online(mod_name, installed=False)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def mod_online(self, mod_name, installed=False):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>print(text, *args, **kwargs)</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef print(text, *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>print_ok()</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def print_ok(self):\n    \"\"\"proxi attr\"\"\"\n    self.logger.info(\"OK\")\n</code></pre> <code>reload_mod(mod_name, spec='app', is_file=True, loc='toolboxv2.mods.')</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>remove_mod(mod_name, spec='app', delete=True)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def remove_mod(self, mod_name, spec='app', delete=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>rrun_flows(name, **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def rrun_flows(self, name, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>run_a_from_sync(function, *args)</code> \u00b6 <p>run a async fuction</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_a_from_sync(self, function, *args):\n    \"\"\"\n    run a async fuction\n    \"\"\"\n</code></pre> <code>run_any(mod_function_name, backwords_compability_variabel_string_holder=None, get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n            get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n            kwargs_=None,\n            *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>run_flows(name, **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_flows(self, name, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>run_function(mod_function_name, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_function(self, mod_function_name: Enum or tuple,\n                 tb_run_function_with_state=True,\n                 tb_run_with_specification='app',\n                 args_=None,\n                 kwargs_=None,\n                 *args,\n                 **kwargs) -&gt; Result:\n\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>run_http(mod_function_name, function_name=None, method='GET', args_=None, kwargs_=None, *args, **kwargs)</code> <code>async</code> \u00b6 <p>run a function remote via http / https</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None, method=\"GET\",\n                   args_=None,\n                   kwargs_=None,\n                   *args, **kwargs):\n    \"\"\"run a function remote via http / https\"\"\"\n</code></pre> <code>save_autocompletion_dict()</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_autocompletion_dict(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>save_exit()</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>save_initialized_module(tools_class, spec)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_initialized_module(self, tools_class, spec):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>save_instance(instance, modular_id, spec='app', instance_type='file/application', tools_class=None)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>save_load(modname, spec='app')</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_load(self, modname, spec='app'):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>save_registry_as_enums(directory, filename)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_registry_as_enums(self, directory: str, filename: str):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>set_flows(r)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def set_flows(self, r):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>set_logger(debug=False)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def set_logger(self, debug=False):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>show_console(*args, **kwargs)</code> <code>async</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def show_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>sprint(text, *args, **kwargs)</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef sprint(text, *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>tb(name=None, mod_name='', helper='', version=None, test=True, restrict_in_virtual_mode=False, api=False, initial=False, exit_f=False, test_only=False, memory_cache=False, file_cache=False, row=False, request_as_kwarg=False, state=None, level=0, memory_cache_max_size=100, memory_cache_ttl=300, samples=None, interface=None, pre_compute=None, post_compute=None, api_methods=None)</code> \u00b6 <p>A decorator for registering and configuring functions within a module.</p> <p>This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to register the function under. Defaults to the function's own name.</p> <code>None</code> <code>mod_name</code> <code>str</code> <p>The name of the module the function belongs to.</p> <code>''</code> <code>helper</code> <code>str</code> <p>A helper string providing additional information about the function.</p> <code>''</code> <code>version</code> <code>str or None</code> <p>The version of the function or module.</p> <code>None</code> <code>test</code> <code>bool</code> <p>Flag to indicate if the function is for testing purposes.</p> <code>True</code> <code>restrict_in_virtual_mode</code> <code>bool</code> <p>Flag to restrict the function in virtual mode.</p> <code>False</code> <code>api</code> <code>bool</code> <p>Flag to indicate if the function is part of an API.</p> <code>False</code> <code>initial</code> <code>bool</code> <p>Flag to indicate if the function should be executed at initialization.</p> <code>False</code> <code>exit_f</code> <code>bool</code> <p>Flag to indicate if the function should be executed at exit.</p> <code>False</code> <code>test_only</code> <code>bool</code> <p>Flag to indicate if the function should only be used for testing.</p> <code>False</code> <code>memory_cache</code> <code>bool</code> <p>Flag to enable memory caching for the function.</p> <code>False</code> <code>request_as_kwarg</code> <code>bool</code> <p>Flag to get request if the fuction is calld from api.</p> <code>False</code> <code>file_cache</code> <code>bool</code> <p>Flag to enable file caching for the function.</p> <code>False</code> <code>row</code> <code>bool</code> <p>rather to auto wrap the result in Result type default False means no row data aka result type</p> <code>False</code> <code>state</code> <code>bool or None</code> <p>Flag to indicate if the function maintains state.</p> <code>None</code> <code>level</code> <code>int</code> <p>The level of the function, used for prioritization or categorization.</p> <code>0</code> <code>memory_cache_max_size</code> <code>int</code> <p>Maximum size of the memory cache.</p> <code>100</code> <code>memory_cache_ttl</code> <code>int</code> <p>Time-to-live for the memory cache entries.</p> <code>300</code> <code>samples</code> <code>list or dict or None</code> <p>Samples or examples of function usage.</p> <code>None</code> <code>interface</code> <code>str</code> <p>The interface type for the function.</p> <code>None</code> <code>pre_compute</code> <code>callable</code> <p>A function to be called before the main function.</p> <code>None</code> <code>post_compute</code> <code>callable</code> <p>A function to be called after the main function.</p> <code>None</code> <code>api_methods</code> <code>list[str]</code> <p>default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>function</code> <p>The decorated function with additional processing and registration capabilities.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def tb(self, name=None,\n       mod_name: str = \"\",\n       helper: str = \"\",\n       version: str or None = None,\n       test: bool = True,\n       restrict_in_virtual_mode: bool = False,\n       api: bool = False,\n       initial: bool = False,\n       exit_f: bool = False,\n       test_only: bool = False,\n       memory_cache: bool = False,\n       file_cache: bool = False,\n       row=False,\n       request_as_kwarg: bool = False,\n       state: bool or None = None,\n       level: int = 0,\n       memory_cache_max_size: int = 100,\n       memory_cache_ttl: int = 300,\n       samples: list or dict or None = None,\n       interface: ToolBoxInterfaces or None or str = None,\n       pre_compute=None,\n       post_compute=None,\n       api_methods=None,\n       ):\n    \"\"\"\nA decorator for registering and configuring functions within a module.\n\nThis decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\nArgs:\n    name (str, optional): The name to register the function under. Defaults to the function's own name.\n    mod_name (str, optional): The name of the module the function belongs to.\n    helper (str, optional): A helper string providing additional information about the function.\n    version (str or None, optional): The version of the function or module.\n    test (bool, optional): Flag to indicate if the function is for testing purposes.\n    restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n    api (bool, optional): Flag to indicate if the function is part of an API.\n    initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n    exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n    test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n    memory_cache (bool, optional): Flag to enable memory caching for the function.\n    request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n    file_cache (bool, optional): Flag to enable file caching for the function.\n    row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n    state (bool or None, optional): Flag to indicate if the function maintains state.\n    level (int, optional): The level of the function, used for prioritization or categorization.\n    memory_cache_max_size (int, optional): Maximum size of the memory cache.\n    memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n    samples (list or dict or None, optional): Samples or examples of function usage.\n    interface (str, optional): The interface type for the function.\n    pre_compute (callable, optional): A function to be called before the main function.\n    post_compute (callable, optional): A function to be called after the main function.\n    api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\nReturns:\n    function: The decorated function with additional processing and registration capabilities.\n\"\"\"\n    if interface is None:\n        interface = \"tb\"\n    if test_only and 'test' not in self.id:\n        return lambda *args, **kwargs: args\n    return self._create_decorator(interface,\n                                  name,\n                                  mod_name,\n                                  level=level,\n                                  restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                  helper=helper,\n                                  api=api,\n                                  version=version,\n                                  initial=initial,\n                                  exit_f=exit_f,\n                                  test=test,\n                                  samples=samples,\n                                  state=state,\n                                  pre_compute=pre_compute,\n                                  post_compute=post_compute,\n                                  memory_cache=memory_cache,\n                                  file_cache=file_cache,\n                                  row=row,\n                                  request_as_kwarg=request_as_kwarg,\n                                  memory_cache_max_size=memory_cache_max_size,\n                                  memory_cache_ttl=memory_cache_ttl)\n</code></pre> <code>watch_mod(mod_name, spec='app', loc='toolboxv2.mods.', use_thread=True, path_name=None)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>web_context()</code> \u00b6 <p>returns the build index ( toolbox web component )</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def web_context(self) -&gt; str:\n    \"\"\"returns the build index ( toolbox web component )\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.Headers","title":"<code>Headers</code>  <code>dataclass</code>","text":"<p>Class representing HTTP headers with strongly typed common fields.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@dataclass\nclass Headers:\n    \"\"\"Class representing HTTP headers with strongly typed common fields.\"\"\"\n    # General Headers\n    accept: None | str= None\n    accept_charset: None | str= None\n    accept_encoding: None | str= None\n    accept_language: None | str= None\n    accept_ranges: None | str= None\n    access_control_allow_credentials: None | str= None\n    access_control_allow_headers: None | str= None\n    access_control_allow_methods: None | str= None\n    access_control_allow_origin: None | str= None\n    access_control_expose_headers: None | str= None\n    access_control_max_age: None | str= None\n    access_control_request_headers: None | str= None\n    access_control_request_method: None | str= None\n    age: None | str= None\n    allow: None | str= None\n    alt_svc: None | str= None\n    authorization: None | str= None\n    cache_control: None | str= None\n    clear_site_data: None | str= None\n    connection: None | str= None\n    content_disposition: None | str= None\n    content_encoding: None | str= None\n    content_language: None | str= None\n    content_length: None | str= None\n    content_location: None | str= None\n    content_range: None | str= None\n    content_security_policy: None | str= None\n    content_security_policy_report_only: None | str= None\n    content_type: None | str= None\n    cookie: None | str= None\n    cross_origin_embedder_policy: None | str= None\n    cross_origin_opener_policy: None | str= None\n    cross_origin_resource_policy: None | str= None\n    date: None | str= None\n    device_memory: None | str= None\n    digest: None | str= None\n    dnt: None | str= None\n    dpr: None | str= None\n    etag: None | str= None\n    expect: None | str= None\n    expires: None | str= None\n    feature_policy: None | str= None\n    forwarded: None | str= None\n    from_header: None | str= None  # 'from' is a Python keyword\n    host: None | str= None\n    if_match: None | str= None\n    if_modified_since: None | str= None\n    if_none_match: None | str= None\n    if_range: None | str= None\n    if_unmodified_since: None | str= None\n    keep_alive: None | str= None\n    large_allocation: None | str= None\n    last_modified: None | str= None\n    link: None | str= None\n    location: None | str= None\n    max_forwards: None | str= None\n    origin: None | str= None\n    pragma: None | str= None\n    proxy_authenticate: None | str= None\n    proxy_authorization: None | str= None\n    public_key_pins: None | str= None\n    public_key_pins_report_only: None | str= None\n    range: None | str= None\n    referer: None | str= None\n    referrer_policy: None | str= None\n    retry_after: None | str= None\n    save_data: None | str= None\n    sec_fetch_dest: None | str= None\n    sec_fetch_mode: None | str= None\n    sec_fetch_site: None | str= None\n    sec_fetch_user: None | str= None\n    sec_websocket_accept: None | str= None\n    sec_websocket_extensions: None | str= None\n    sec_websocket_key: None | str= None\n    sec_websocket_protocol: None | str= None\n    sec_websocket_version: None | str= None\n    server: None | str= None\n    server_timing: None | str= None\n    service_worker_allowed: None | str= None\n    set_cookie: None | str= None\n    sourcemap: None | str= None\n    strict_transport_security: None | str= None\n    te: None | str= None\n    timing_allow_origin: None | str= None\n    tk: None | str= None\n    trailer: None | str= None\n    transfer_encoding: None | str= None\n    upgrade: None | str= None\n    upgrade_insecure_requests: None | str= None\n    user_agent: None | str= None\n    vary: None | str= None\n    via: None | str= None\n    warning: None | str= None\n    www_authenticate: None | str= None\n    x_content_type_options: None | str= None\n    x_dns_prefetch_control: None | str= None\n    x_forwarded_for: None | str= None\n    x_forwarded_host: None | str= None\n    x_forwarded_proto: None | str= None\n    x_frame_options: None | str= None\n    x_xss_protection: None | str= None\n\n    # Browser-specific and custom headers\n    sec_ch_ua: None | str= None\n    sec_ch_ua_mobile: None | str= None\n    sec_ch_ua_platform: None | str= None\n    sec_ch_ua_arch: None | str= None\n    sec_ch_ua_bitness: None | str= None\n    sec_ch_ua_full_version: None | str= None\n    sec_ch_ua_full_version_list: None | str= None\n    sec_ch_ua_platform_version: None | str= None\n\n    # HTMX specific headers\n    hx_boosted: None | str= None\n    hx_current_url: None | str= None\n    hx_history_restore_request: None | str= None\n    hx_prompt: None | str= None\n    hx_request: None | str= None\n    hx_target: None | str= None\n    hx_trigger: None | str= None\n    hx_trigger_name: None | str= None\n\n    # Additional fields can be stored in extra_headers\n    extra_headers: dict[str, str] = field(default_factory=dict)\n\n    def __post_init__(self):\n        \"\"\"Convert header keys with hyphens to underscores for attribute access.\"\"\"\n        # Handle the 'from' header specifically since it's a Python keyword\n        if 'from' in self.__dict__:\n            self.from_header = self.__dict__.pop('from')\n\n        # Store any attributes that weren't explicitly defined in extra_headers\n        all_attrs = self.__annotations__.keys()\n        for key in list(self.__dict__.keys()):\n            if key not in all_attrs and key != \"extra_headers\":\n                self.extra_headers[key.replace(\"_\", \"-\")] = getattr(self, key)\n                delattr(self, key)\n\n    @classmethod\n    def from_dict(cls, headers_dict: dict[str, str]) -&gt; 'Headers':\n        \"\"\"Create a Headers instance from a dictionary.\"\"\"\n        # Convert header keys from hyphenated to underscore format for Python attributes\n        processed_headers = {}\n        extra_headers = {}\n\n        for key, value in headers_dict.items():\n            # Handle 'from' header specifically\n            if key.lower() == 'from':\n                processed_headers['from_header'] = value\n                continue\n\n            python_key = key.replace(\"-\", \"_\").lower()\n            if python_key in cls.__annotations__ and python_key != \"extra_headers\":\n                processed_headers[python_key] = value\n            else:\n                extra_headers[key] = value\n\n        return cls(**processed_headers, extra_headers=extra_headers)\n\n    def to_dict(self) -&gt; dict[str, str]:\n        \"\"\"Convert the Headers object back to a dictionary.\"\"\"\n        result = {}\n\n        # Add regular attributes\n        for key, value in self.__dict__.items():\n            if key != \"extra_headers\" and value is not None:\n                # Handle from_header specially\n                if key == \"from_header\":\n                    result[\"from\"] = value\n                else:\n                    result[key.replace(\"_\", \"-\")] = value\n\n        # Add extra headers\n        result.update(self.extra_headers)\n\n        return result\n</code></pre> <code>__post_init__()</code> \u00b6 <p>Convert header keys with hyphens to underscores for attribute access.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Convert header keys with hyphens to underscores for attribute access.\"\"\"\n    # Handle the 'from' header specifically since it's a Python keyword\n    if 'from' in self.__dict__:\n        self.from_header = self.__dict__.pop('from')\n\n    # Store any attributes that weren't explicitly defined in extra_headers\n    all_attrs = self.__annotations__.keys()\n    for key in list(self.__dict__.keys()):\n        if key not in all_attrs and key != \"extra_headers\":\n            self.extra_headers[key.replace(\"_\", \"-\")] = getattr(self, key)\n            delattr(self, key)\n</code></pre> <code>from_dict(headers_dict)</code> <code>classmethod</code> \u00b6 <p>Create a Headers instance from a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef from_dict(cls, headers_dict: dict[str, str]) -&gt; 'Headers':\n    \"\"\"Create a Headers instance from a dictionary.\"\"\"\n    # Convert header keys from hyphenated to underscore format for Python attributes\n    processed_headers = {}\n    extra_headers = {}\n\n    for key, value in headers_dict.items():\n        # Handle 'from' header specifically\n        if key.lower() == 'from':\n            processed_headers['from_header'] = value\n            continue\n\n        python_key = key.replace(\"-\", \"_\").lower()\n        if python_key in cls.__annotations__ and python_key != \"extra_headers\":\n            processed_headers[python_key] = value\n        else:\n            extra_headers[key] = value\n\n    return cls(**processed_headers, extra_headers=extra_headers)\n</code></pre> <code>to_dict()</code> \u00b6 <p>Convert the Headers object back to a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, str]:\n    \"\"\"Convert the Headers object back to a dictionary.\"\"\"\n    result = {}\n\n    # Add regular attributes\n    for key, value in self.__dict__.items():\n        if key != \"extra_headers\" and value is not None:\n            # Handle from_header specially\n            if key == \"from_header\":\n                result[\"from\"] = value\n            else:\n                result[key.replace(\"_\", \"-\")] = value\n\n    # Add extra headers\n    result.update(self.extra_headers)\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.MainToolType","title":"<code>MainToolType</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class MainToolType:\n    toolID: str\n    app: A\n    interface: ToolBoxInterfaces\n    spec: str\n\n    version: str\n    tools: dict  # legacy\n    name: str\n    logger: logging\n    color: str\n    todo: Callable\n    _on_exit: Callable\n    stuf: bool\n    config: dict\n    user: U | None\n    description: str\n\n    @staticmethod\n    def return_result(error: ToolBoxError = ToolBoxError.none,\n                      exec_code: int = 0,\n                      help_text: str = \"\",\n                      data_info=None,\n                      data=None,\n                      data_to=None) -&gt; Result:\n        \"\"\"proxi attr\"\"\"\n\n    def load(self):\n        \"\"\"proxi attr\"\"\"\n\n    def print(self, message, end=\"\\n\", **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def add_str_to_config(self, command):\n        if len(command) != 2:\n            self.logger.error('Invalid command must be key value')\n            return False\n        self.config[command[0]] = command[1]\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n\n    async def get_user(self, username: str) -&gt; Result:\n        return self.app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME, username=username, get_results=True)\n</code></pre> <code>load()</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def load(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>print(message, end='\\n', **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def print(self, message, end=\"\\n\", **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>return_result(error=ToolBoxError.none, exec_code=0, help_text='', data_info=None, data=None, data_to=None)</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef return_result(error: ToolBoxError = ToolBoxError.none,\n                  exec_code: int = 0,\n                  help_text: str = \"\",\n                  data_info=None,\n                  data=None,\n                  data_to=None) -&gt; Result:\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>webInstall(user_instance, construct_render)</code> \u00b6 <p>\"Returns a web installer for the given user instance and construct render template</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def webInstall(self, user_instance, construct_render) -&gt; str:\n    \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.Request","title":"<code>Request</code>  <code>dataclass</code>","text":"<p>Class representing an HTTP request.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@dataclass\nclass Request:\n    \"\"\"Class representing an HTTP request.\"\"\"\n    content_type: str\n    headers: Headers\n    method: str\n    path: str\n    query_params: dict[str, Any] = field(default_factory=dict)\n    form_data: dict[str, Any] | None = None\n    body: Any | None = None\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any]) -&gt; 'Request':\n        \"\"\"Create a Request instance from a dictionary.\"\"\"\n        headers = Headers.from_dict(data.get('headers', {}))\n\n        # Extract other fields\n        return cls(\n            content_type=data.get('content_type', ''),\n            headers=headers,\n            method=data.get('method', ''),\n            path=data.get('path', ''),\n            query_params=data.get('query_params', {}),\n            form_data=data.get('form_data'),\n            body=data.get('body')\n        )\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert the Request object back to a dictionary.\"\"\"\n        result = {\n            'content_type': self.content_type,\n            'headers': self.headers.to_dict(),\n            'method': self.method,\n            'path': self.path,\n            'query_params': self.query_params,\n        }\n\n        if self.form_data is not None:\n            result['form_data'] = self.form_data\n\n        if self.body is not None:\n            result['body'] = self.body\n\n        return result\n</code></pre> <code>from_dict(data)</code> <code>classmethod</code> \u00b6 <p>Create a Request instance from a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; 'Request':\n    \"\"\"Create a Request instance from a dictionary.\"\"\"\n    headers = Headers.from_dict(data.get('headers', {}))\n\n    # Extract other fields\n    return cls(\n        content_type=data.get('content_type', ''),\n        headers=headers,\n        method=data.get('method', ''),\n        path=data.get('path', ''),\n        query_params=data.get('query_params', {}),\n        form_data=data.get('form_data'),\n        body=data.get('body')\n    )\n</code></pre> <code>to_dict()</code> \u00b6 <p>Convert the Request object back to a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert the Request object back to a dictionary.\"\"\"\n    result = {\n        'content_type': self.content_type,\n        'headers': self.headers.to_dict(),\n        'method': self.method,\n        'path': self.path,\n        'query_params': self.query_params,\n    }\n\n    if self.form_data is not None:\n        result['form_data'] = self.form_data\n\n    if self.body is not None:\n        result['body'] = self.body\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.RequestData","title":"<code>RequestData</code>  <code>dataclass</code>","text":"<p>Main class representing the complete request data structure.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@dataclass\nclass RequestData:\n    \"\"\"Main class representing the complete request data structure.\"\"\"\n    request: Request\n    session: Session\n    session_id: str\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any]) -&gt; 'RequestData':\n        \"\"\"Create a RequestData instance from a dictionary.\"\"\"\n        return cls(\n            request=Request.from_dict(data.get('request', {})),\n            session=Session.from_dict(data.get('session', {})),\n            session_id=data.get('session_id', '')\n        )\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert the RequestData object back to a dictionary.\"\"\"\n        return {\n            'request': self.request.to_dict(),\n            'session': self.session.to_dict(),\n            'session_id': self.session_id\n        }\n</code></pre> <code>from_dict(data)</code> <code>classmethod</code> \u00b6 <p>Create a RequestData instance from a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; 'RequestData':\n    \"\"\"Create a RequestData instance from a dictionary.\"\"\"\n    return cls(\n        request=Request.from_dict(data.get('request', {})),\n        session=Session.from_dict(data.get('session', {})),\n        session_id=data.get('session_id', '')\n    )\n</code></pre> <code>to_dict()</code> \u00b6 <p>Convert the RequestData object back to a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert the RequestData object back to a dictionary.\"\"\"\n    return {\n        'request': self.request.to_dict(),\n        'session': self.session.to_dict(),\n        'session_id': self.session_id\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.Result","title":"<code>Result</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class Result:\n    _task = None\n    def __init__(self,\n                 error: ToolBoxError,\n                 result: ToolBoxResult,\n                 info: ToolBoxInfo,\n                 origin: Any | None = None,\n                 ):\n        self.error: ToolBoxError = error\n        self.result: ToolBoxResult = result\n        self.info: ToolBoxInfo = info\n        self.origin = origin\n\n    def as_result(self):\n        return self\n\n    def as_dict(self):\n        return {\n            \"error\":self.error.value if isinstance(self.error, Enum) else self.error,\n        \"result\" : {\n            \"data_to\":self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n            \"data_info\":self.result.data_info,\n            \"data\":self.result.data,\n            \"data_type\":self.result.data_type\n        } if self.result else None,\n        \"info\" : {\n            \"exec_code\" : self.info.exec_code,  # exec_code umwandel in http resposn codes\n        \"help_text\" : self.info.help_text\n        } if self.info else None,\n        \"origin\" : self.origin\n        }\n\n    def set_origin(self, origin):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = origin\n        return self\n\n    def set_dir_origin(self, name, extras=\"assets/\"):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = f\"mods/{name}/{extras}\"\n        return self\n\n    def is_error(self):\n        if _test_is_result(self.result.data):\n            return self.result.data.is_error()\n        return self.info.exec_code != 0\n\n    def is_data(self):\n        return self.result.data is not None\n\n    def to_api_result(self):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=self.error.value if isinstance(self.error, Enum) else self.error,\n            result=ToolBoxResultBM(\n                data_to=self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n                data_info=self.result.data_info,\n                data=self.result.data,\n                data_type=self.result.data_type\n            ) if self.result else None,\n            info=ToolBoxInfoBM(\n                exec_code=self.info.exec_code,  # exec_code umwandel in http resposn codes\n                help_text=self.info.help_text\n            ) if self.info else None,\n            origin=self.origin\n        )\n\n    def task(self, task):\n        self._task = task\n        return self\n\n    @staticmethod\n    def result_from_dict(error: str, result: dict, info: dict, origin: list or None or str):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=error if isinstance(error, Enum) else error,\n            result=ToolBoxResultBM(\n                data_to=result.get('data_to') if isinstance(result.get('data_to'), Enum) else result.get('data_to'),\n                data_info=result.get('data_info', '404'),\n                data=result.get('data'),\n                data_type=result.get('data_type', '404'),\n            ) if result else None,\n            info=ToolBoxInfoBM(\n                exec_code=info.get('exec_code', 404),\n                help_text=info.get('help_text', '404')\n            ) if info else None,\n            origin=origin\n        ).as_result()\n\n    @classmethod\n    def stream(cls,\n               stream_generator,\n               content_type=\"text/event-stream\",\n               headers=None,\n               info=\"OK\",\n               interface=ToolBoxInterfaces.remote,\n               cleanup_func=None):\n        \"\"\"\n        Create a streaming response Result that properly handles all types of stream sources.\n\n        Args:\n            stream_generator: Any stream source (async generator, sync generator, iterable, or even string)\n            content_type: Content-Type header (default: text/event-stream for SSE)\n            headers: Additional HTTP headers\n            info: Help text for the result\n            interface: Interface to send data to\n\n        Returns:\n            A Result object configured for streaming\n        \"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        # Standard SSE headers\n        standard_headers = {\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"X-Accel-Buffering\": \"no\"\n        }\n\n        # Apply custom headers\n        all_headers = standard_headers.copy()\n        if headers:\n            all_headers.update(headers)\n\n        # Handle different types of stream sources\n        if content_type == \"text/event-stream\":\n            wrapped_generator = stream_generator\n            if inspect.isgenerator(stream_generator) or hasattr(stream_generator, '__iter__'):\n                # Sync generator or iterable\n                wrapped_generator = SSEGenerator.create_sse_stream(stream_generator, cleanup_func)\n\n            elif isinstance(stream_generator, str):\n                # String (could be a memory address or other reference)\n                # Convert to a generator that yields a single string\n                async def string_to_stream():\n                    yield stream_generator\n\n                wrapped_generator = SSEGenerator.create_sse_stream(string_to_stream(), cleanup_func)\n\n            # The final generator to use\n            final_generator = wrapped_generator\n\n        else:\n            # For non-SSE streams, use the original generator\n            final_generator = stream_generator\n\n        # Prepare streaming data\n        streaming_data = {\n            \"type\": \"stream\",\n            \"generator\": final_generator,\n            \"content_type\": content_type,\n            \"headers\": all_headers\n        }\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=streaming_data,\n            data_info=\"Streaming response\",\n            data_type=\"stream\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def default(cls, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=-1, help_text=\"\")\n        result = ToolBoxResult(data_to=interface)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a JSON response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=data,\n            data_info=\"JSON response\",\n            data_type=\"json\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n        \"\"\"Create a text response Result with specific content type.\"\"\"\n        if headers is not None:\n            return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=text_data,\n            data_info=\"Text response\",\n            data_type=content_type\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n               interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a binary data response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        # Create a dictionary with binary data and metadata\n        binary_data = {\n            \"data\": data,\n            \"content_type\": content_type,\n            \"filename\": download_name\n        }\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=binary_data,\n            data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n            data_type=\"binary\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a redirect response.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=url,\n            data_info=\"Redirect response\",\n            data_type=\"redirect\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def ok(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def html(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.remote, data_type=\"html\",status=200, headers=None):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=status, help_text=info)\n\n        if isinstance(headers, dict):\n            result = ToolBoxResult(data_to=interface, data={'html':data,'headers':headers}, data_info=data_info,\n                                   data_type=\"special_html\")\n        else:\n            result = ToolBoxResult(data_to=interface, data=data, data_info=data_info,\n                                   data_type=data_type if data_type is not None else type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def future(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.future):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=\"future\")\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def custom_error(cls, data=None, data_info=\"\", info=\"\", exec_code=-1, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def error(cls, data=None, data_info=\"\", info=\"\", exec_code=450, interface=ToolBoxInterfaces.remote):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_user_error(cls, info=\"\", exec_code=-3, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.input_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_internal_error(cls, info=\"\", exec_code=-2, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.internal_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    def print(self, show=True, show_data=True, prifix=\"\"):\n        data = '\\n' + f\"{((prifix + 'Data: ' + str(self.result.data) if self.result.data is not None else 'NO Data') if not isinstance(self.result.data, Result) else self.result.data.print(show=False, show_data=show_data, prifix=prifix + '-')) if show_data else 'Data: private'}\"\n        origin = '\\n' + f\"{prifix + 'Origin: ' + str(self.origin) if self.origin is not None else 'NO Origin'}\"\n        text = (f\"Function Exec code: {self.info.exec_code}\"\n                f\"\\n{prifix}Info's:\"\n                f\" {self.info.help_text} {'&lt;|&gt; ' + str(self.result.data_info) if self.result.data_info is not None else ''}\"\n                f\"{origin}{data if not data.endswith('NO Data') else ''}\")\n        if not show:\n            return text\n        print(\"\\n======== Result ========\\n\" + text + \"\\n------- EndOfD -------\")\n        return self\n\n    def log(self, show_data=True, prifix=\"\"):\n        from toolboxv2 import get_logger\n        get_logger().debug(self.print(show=False, show_data=show_data, prifix=prifix).replace(\"\\n\", \" - \"))\n        return self\n\n    def __str__(self):\n        return self.print(show=False, show_data=True)\n\n    def get(self, key=None, default=None):\n        data = self.result.data\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    async def aget(self, key=None, default=None):\n        if asyncio.isfuture(self.result.data) or asyncio.iscoroutine(self.result.data) or (\n            isinstance(self.result.data_to, Enum) and self.result.data_to.name == ToolBoxInterfaces.future.name):\n            data = await self.result.data\n        else:\n            data = self.get(key=None, default=None)\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    def lazy_return(self, _=0, data=None, **kwargs):\n        flags = ['raise', 'logg', 'user', 'intern']\n        flag = flags[_] if isinstance(_, int) else _\n        if self.info.exec_code == 0:\n            return self if data is None else data if _test_is_result(data) else self.ok(data=data, **kwargs)\n        if flag == 'raise':\n            raise ValueError(self.print(show=False))\n        if flag == 'logg':\n            from .. import get_logger\n            get_logger().error(self.print(show=False))\n\n        if flag == 'user':\n            return self if data is None else data if _test_is_result(data) else self.default_user_error(data=data,\n                                                                                                        **kwargs)\n        if flag == 'intern':\n            return self if data is None else data if _test_is_result(data) else self.default_internal_error(data=data,\n                                                                                                            **kwargs)\n\n        return self if data is None else data if _test_is_result(data) else self.custom_error(data=data, **kwargs)\n\n    @property\n    def bg_task(self):\n        return self._task\n</code></pre> <code>binary(data, content_type='application/octet-stream', download_name=None, info='OK', interface=ToolBoxInterfaces.remote)</code> <code>classmethod</code> \u00b6 <p>Create a binary data response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n           interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a binary data response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    # Create a dictionary with binary data and metadata\n    binary_data = {\n        \"data\": data,\n        \"content_type\": content_type,\n        \"filename\": download_name\n    }\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=binary_data,\n        data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n        data_type=\"binary\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre> <code>json(data, info='OK', interface=ToolBoxInterfaces.remote)</code> <code>classmethod</code> \u00b6 <p>Create a JSON response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a JSON response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=data,\n        data_info=\"JSON response\",\n        data_type=\"json\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre> <code>redirect(url, status_code=302, info='Redirect', interface=ToolBoxInterfaces.remote)</code> <code>classmethod</code> \u00b6 <p>Create a redirect response.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a redirect response.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=url,\n        data_info=\"Redirect response\",\n        data_type=\"redirect\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre> <code>stream(stream_generator, content_type='text/event-stream', headers=None, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code> <code>classmethod</code> \u00b6 <p>Create a streaming response Result that properly handles all types of stream sources.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <p>Any stream source (async generator, sync generator, iterable, or even string)</p> required <code>content_type</code> <p>Content-Type header (default: text/event-stream for SSE)</p> <code>'text/event-stream'</code> <code>headers</code> <p>Additional HTTP headers</p> <code>None</code> <code>info</code> <p>Help text for the result</p> <code>'OK'</code> <code>interface</code> <p>Interface to send data to</p> <code>remote</code> <p>Returns:</p> Type Description <p>A Result object configured for streaming</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef stream(cls,\n           stream_generator,\n           content_type=\"text/event-stream\",\n           headers=None,\n           info=\"OK\",\n           interface=ToolBoxInterfaces.remote,\n           cleanup_func=None):\n    \"\"\"\n    Create a streaming response Result that properly handles all types of stream sources.\n\n    Args:\n        stream_generator: Any stream source (async generator, sync generator, iterable, or even string)\n        content_type: Content-Type header (default: text/event-stream for SSE)\n        headers: Additional HTTP headers\n        info: Help text for the result\n        interface: Interface to send data to\n\n    Returns:\n        A Result object configured for streaming\n    \"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    # Standard SSE headers\n    standard_headers = {\n        \"Cache-Control\": \"no-cache\",\n        \"Connection\": \"keep-alive\",\n        \"X-Accel-Buffering\": \"no\"\n    }\n\n    # Apply custom headers\n    all_headers = standard_headers.copy()\n    if headers:\n        all_headers.update(headers)\n\n    # Handle different types of stream sources\n    if content_type == \"text/event-stream\":\n        wrapped_generator = stream_generator\n        if inspect.isgenerator(stream_generator) or hasattr(stream_generator, '__iter__'):\n            # Sync generator or iterable\n            wrapped_generator = SSEGenerator.create_sse_stream(stream_generator, cleanup_func)\n\n        elif isinstance(stream_generator, str):\n            # String (could be a memory address or other reference)\n            # Convert to a generator that yields a single string\n            async def string_to_stream():\n                yield stream_generator\n\n            wrapped_generator = SSEGenerator.create_sse_stream(string_to_stream(), cleanup_func)\n\n        # The final generator to use\n        final_generator = wrapped_generator\n\n    else:\n        # For non-SSE streams, use the original generator\n        final_generator = stream_generator\n\n    # Prepare streaming data\n    streaming_data = {\n        \"type\": \"stream\",\n        \"generator\": final_generator,\n        \"content_type\": content_type,\n        \"headers\": all_headers\n    }\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=streaming_data,\n        data_info=\"Streaming response\",\n        data_type=\"stream\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre> <code>text(text_data, content_type='text/plain', exec_code=None, status=200, info='OK', interface=ToolBoxInterfaces.remote, headers=None)</code> <code>classmethod</code> \u00b6 <p>Create a text response Result with specific content type.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n    \"\"\"Create a text response Result with specific content type.\"\"\"\n    if headers is not None:\n        return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=text_data,\n        data_info=\"Text response\",\n        data_type=content_type\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.SSEGenerator","title":"<code>SSEGenerator</code>","text":"<p>Production-ready SSE generator that converts any data source to properly formatted Server-Sent Events compatible with browsers.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class SSEGenerator:\n    \"\"\"\n    Production-ready SSE generator that converts any data source to\n    properly formatted Server-Sent Events compatible with browsers.\n    \"\"\"\n\n    @staticmethod\n    def format_sse_event(data: Any) -&gt; str:\n        \"\"\"Format any data as a proper SSE event message.\"\"\"\n        # Already formatted as SSE\n        if isinstance(data, str) and (data.startswith('data:') or data.startswith('event:')) and '\\n\\n' in data:\n            return data\n\n        # Handle bytes (binary data)\n        if isinstance(data, bytes):\n            try:\n                # Try to decode as UTF-8 first\n                data = data.decode('utf-8')\n            except UnicodeDecodeError:\n                # Binary data, encode as base64\n                b64_data = base64.b64encode(data).decode('utf-8')\n                return f\"event: binary\\ndata: {b64_data}\\n\\n\"\n\n        # Convert objects to JSON\n        if not isinstance(data, str):\n            try:\n                data = json.dumps(data)\n            except Exception:\n                data = str(data)\n\n        # Handle JSON data with special event formatting\n        if data.strip().startswith('{'):\n            try:\n                json_data = json.loads(data)\n                if isinstance(json_data, dict) and 'event' in json_data:\n                    event_type = json_data['event']\n                    event_id = json_data.get('id', '')\n\n                    sse = f\"event: {event_type}\\n\"\n                    if event_id:\n                        sse += f\"id: {event_id}\\n\"\n                    sse += f\"data: {data}\\n\\n\"\n                    return sse\n                else:\n                    # Regular JSON without event\n                    return f\"data: {data}\\n\\n\"\n            except json.JSONDecodeError:\n                # Not valid JSON, treat as text\n                return f\"data: {data}\\n\\n\"\n        else:\n            # Plain text\n            return f\"data: {data}\\n\\n\"\n\n    @classmethod\n    async def wrap_sync_generator(cls, generator):\n        \"\"\"Convert a synchronous generator to an async generator.\"\"\"\n        for item in generator:\n            yield item\n            # Allow other tasks to run\n            await asyncio.sleep(0)\n\n    @classmethod\n    async def create_sse_stream(\n        cls,\n        source,\n        cleanup_func: Callable[[], None] | Callable[[], T] | Callable[[], AsyncGenerator[T, None]] | None = None\n    ) -&gt; AsyncGenerator[str, None]:\n        \"\"\"\n        Convert any source to a properly formatted SSE stream.\n\n        Args:\n            source: Can be async generator, sync generator, or iterable\n            cleanup_func: Optional function to call when the stream ends or is cancelled.\n                          Can be a synchronous function, async function, or async generator.\n\n        Yields:\n            Properly formatted SSE messages\n        \"\"\"\n        # Send stream start event\n        yield cls.format_sse_event({\"event\": \"stream_start\", \"id\": \"0\"})\n\n        try:\n            # Handle different types of sources\n            if inspect.isasyncgen(source):\n                # Source is already an async generator\n                async for item in source:\n                    yield cls.format_sse_event(item)\n            elif inspect.isgenerator(source) or hasattr(source, '__iter__'):\n                # Source is a sync generator or iterable\n                async for item in cls.wrap_sync_generator(source):\n                    yield cls.format_sse_event(item)\n            else:\n                # Single item\n                yield cls.format_sse_event(source)\n        except asyncio.CancelledError:\n            # Client disconnected\n            yield cls.format_sse_event({\"event\": \"cancelled\", \"id\": \"cancelled\"})\n            raise\n        except Exception as e:\n            # Error in stream\n            error_info = {\n                \"event\": \"error\",\n                \"message\": str(e),\n                \"traceback\": traceback.format_exc()\n            }\n            yield cls.format_sse_event(error_info)\n        finally:\n            # Always send end event\n            yield cls.format_sse_event({\"event\": \"stream_end\", \"id\": \"final\"})\n\n            # Execute cleanup function if provided\n            if cleanup_func:\n                try:\n                    if asyncio.iscoroutinefunction(cleanup_func):\n                        # Async function\n                        await cleanup_func()\n                    elif inspect.isasyncgen(cleanup_func):\n                        # Async generator\n                        async for _ in cleanup_func():\n                            pass  # Exhaust the generator to ensure cleanup completes\n                    else:\n                        # Synchronous function\n                        cleanup_func()\n                except Exception as e:\n                    # Log cleanup errors but don't propagate them to client\n                    error_info = {\n                        \"event\": \"cleanup_error\",\n                        \"message\": str(e),\n                        \"traceback\": traceback.format_exc()\n                    }\n                    # We can't yield here as the stream is already closing\n                    # Instead, log the error\n                    print(f\"SSE cleanup error: {error_info}\", flush=True)\n</code></pre> <code>create_sse_stream(source, cleanup_func=None)</code> <code>async</code> <code>classmethod</code> \u00b6 <p>Convert any source to a properly formatted SSE stream.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <p>Can be async generator, sync generator, or iterable</p> required <code>cleanup_func</code> <code>Callable[[], None] | Callable[[], T] | Callable[[], AsyncGenerator[T, None]] | None</code> <p>Optional function to call when the stream ends or is cancelled.           Can be a synchronous function, async function, or async generator.</p> <code>None</code> <p>Yields:</p> Type Description <code>AsyncGenerator[str, None]</code> <p>Properly formatted SSE messages</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\nasync def create_sse_stream(\n    cls,\n    source,\n    cleanup_func: Callable[[], None] | Callable[[], T] | Callable[[], AsyncGenerator[T, None]] | None = None\n) -&gt; AsyncGenerator[str, None]:\n    \"\"\"\n    Convert any source to a properly formatted SSE stream.\n\n    Args:\n        source: Can be async generator, sync generator, or iterable\n        cleanup_func: Optional function to call when the stream ends or is cancelled.\n                      Can be a synchronous function, async function, or async generator.\n\n    Yields:\n        Properly formatted SSE messages\n    \"\"\"\n    # Send stream start event\n    yield cls.format_sse_event({\"event\": \"stream_start\", \"id\": \"0\"})\n\n    try:\n        # Handle different types of sources\n        if inspect.isasyncgen(source):\n            # Source is already an async generator\n            async for item in source:\n                yield cls.format_sse_event(item)\n        elif inspect.isgenerator(source) or hasattr(source, '__iter__'):\n            # Source is a sync generator or iterable\n            async for item in cls.wrap_sync_generator(source):\n                yield cls.format_sse_event(item)\n        else:\n            # Single item\n            yield cls.format_sse_event(source)\n    except asyncio.CancelledError:\n        # Client disconnected\n        yield cls.format_sse_event({\"event\": \"cancelled\", \"id\": \"cancelled\"})\n        raise\n    except Exception as e:\n        # Error in stream\n        error_info = {\n            \"event\": \"error\",\n            \"message\": str(e),\n            \"traceback\": traceback.format_exc()\n        }\n        yield cls.format_sse_event(error_info)\n    finally:\n        # Always send end event\n        yield cls.format_sse_event({\"event\": \"stream_end\", \"id\": \"final\"})\n\n        # Execute cleanup function if provided\n        if cleanup_func:\n            try:\n                if asyncio.iscoroutinefunction(cleanup_func):\n                    # Async function\n                    await cleanup_func()\n                elif inspect.isasyncgen(cleanup_func):\n                    # Async generator\n                    async for _ in cleanup_func():\n                        pass  # Exhaust the generator to ensure cleanup completes\n                else:\n                    # Synchronous function\n                    cleanup_func()\n            except Exception as e:\n                # Log cleanup errors but don't propagate them to client\n                error_info = {\n                    \"event\": \"cleanup_error\",\n                    \"message\": str(e),\n                    \"traceback\": traceback.format_exc()\n                }\n                # We can't yield here as the stream is already closing\n                # Instead, log the error\n                print(f\"SSE cleanup error: {error_info}\", flush=True)\n</code></pre> <code>format_sse_event(data)</code> <code>staticmethod</code> \u00b6 <p>Format any data as a proper SSE event message.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef format_sse_event(data: Any) -&gt; str:\n    \"\"\"Format any data as a proper SSE event message.\"\"\"\n    # Already formatted as SSE\n    if isinstance(data, str) and (data.startswith('data:') or data.startswith('event:')) and '\\n\\n' in data:\n        return data\n\n    # Handle bytes (binary data)\n    if isinstance(data, bytes):\n        try:\n            # Try to decode as UTF-8 first\n            data = data.decode('utf-8')\n        except UnicodeDecodeError:\n            # Binary data, encode as base64\n            b64_data = base64.b64encode(data).decode('utf-8')\n            return f\"event: binary\\ndata: {b64_data}\\n\\n\"\n\n    # Convert objects to JSON\n    if not isinstance(data, str):\n        try:\n            data = json.dumps(data)\n        except Exception:\n            data = str(data)\n\n    # Handle JSON data with special event formatting\n    if data.strip().startswith('{'):\n        try:\n            json_data = json.loads(data)\n            if isinstance(json_data, dict) and 'event' in json_data:\n                event_type = json_data['event']\n                event_id = json_data.get('id', '')\n\n                sse = f\"event: {event_type}\\n\"\n                if event_id:\n                    sse += f\"id: {event_id}\\n\"\n                sse += f\"data: {data}\\n\\n\"\n                return sse\n            else:\n                # Regular JSON without event\n                return f\"data: {data}\\n\\n\"\n        except json.JSONDecodeError:\n            # Not valid JSON, treat as text\n            return f\"data: {data}\\n\\n\"\n    else:\n        # Plain text\n        return f\"data: {data}\\n\\n\"\n</code></pre> <code>wrap_sync_generator(generator)</code> <code>async</code> <code>classmethod</code> \u00b6 <p>Convert a synchronous generator to an async generator.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\nasync def wrap_sync_generator(cls, generator):\n    \"\"\"Convert a synchronous generator to an async generator.\"\"\"\n    for item in generator:\n        yield item\n        # Allow other tasks to run\n        await asyncio.sleep(0)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.Session","title":"<code>Session</code>  <code>dataclass</code>","text":"<p>Class representing a session.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@dataclass\nclass Session:\n    \"\"\"Class representing a session.\"\"\"\n    SiID: str\n    level: str\n    spec: str\n    user_name: str\n    # Allow for additional fields\n    extra_data: dict[str, Any] = field(default_factory=dict)\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any]) -&gt; 'Session':\n        \"\"\"Create a Session instance from a dictionary.\"\"\"\n        # Extract known fields\n        known_fields = {k: data.get(k) for k in ['SiID', 'level', 'spec', 'user_name'] if k in data}\n\n        # Extract extra fields\n        extra_data = {k: v for k, v in data.items() if k not in known_fields}\n\n        return cls(**known_fields, extra_data=extra_data)\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert the Session object back to a dictionary.\"\"\"\n        result = {\n            'SiID': self.SiID,\n            'level': self.level,\n            'spec': self.spec,\n            'user_name': self.user_name,\n        }\n\n        # Add extra data\n        result.update(self.extra_data)\n\n        return result\n\n    @property\n    def valid(self):\n        return int(self.level) &gt; 0\n</code></pre> <code>from_dict(data)</code> <code>classmethod</code> \u00b6 <p>Create a Session instance from a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; 'Session':\n    \"\"\"Create a Session instance from a dictionary.\"\"\"\n    # Extract known fields\n    known_fields = {k: data.get(k) for k in ['SiID', 'level', 'spec', 'user_name'] if k in data}\n\n    # Extract extra fields\n    extra_data = {k: v for k, v in data.items() if k not in known_fields}\n\n    return cls(**known_fields, extra_data=extra_data)\n</code></pre> <code>to_dict()</code> \u00b6 <p>Convert the Session object back to a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert the Session object back to a dictionary.\"\"\"\n    result = {\n        'SiID': self.SiID,\n        'level': self.level,\n        'spec': self.spec,\n        'user_name': self.user_name,\n    }\n\n    # Add extra data\n    result.update(self.extra_data)\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.parse_request_data","title":"<code>parse_request_data(data)</code>","text":"<p>Parse the incoming request data into a strongly typed structure.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def parse_request_data(data: dict[str, Any]) -&gt; RequestData:\n    \"\"\"Parse the incoming request data into a strongly typed structure.\"\"\"\n    return RequestData.from_dict(data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox","title":"<code>toolbox</code>","text":"<p>Main module.</p>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App","title":"<code>App</code>","text":"Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>class App(AppType, metaclass=Singleton):\n\n    def __init__(self, prefix: str = \"\", args=AppArgs().default()):\n        super().__init__(prefix, args)\n        self._web_context = None\n        t0 = time.perf_counter()\n        abspath = os.path.abspath(__file__)\n        self.system_flag = system()  # Linux: Linux Mac: Darwin Windows: Windows\n\n        self.appdata = os.getenv('APPDATA') if os.name == 'nt' else os.getenv('XDG_CONFIG_HOME') or os.path.expanduser(\n                '~/.config') if os.name == 'posix' else None\n\n        if self.system_flag == \"Darwin\" or self.system_flag == \"Linux\":\n            dir_name = os.path.dirname(abspath).replace(\"/utils\", \"\")\n        else:\n            dir_name = os.path.dirname(abspath).replace(\"\\\\utils\", \"\")\n\n        self.start_dir = str(dir_name)\n\n        self.bg_tasks = []\n\n        lapp = dir_name + '\\\\.data\\\\'\n\n        if not prefix:\n            if not os.path.exists(f\"{lapp}last-app-prefix.txt\"):\n                os.makedirs(lapp, exist_ok=True)\n                open(f\"{lapp}last-app-prefix.txt\", \"a\").close()\n            with open(f\"{lapp}last-app-prefix.txt\") as prefix_file:\n                cont = prefix_file.read()\n                if cont:\n                    prefix = cont.rstrip()\n        else:\n            if not os.path.exists(f\"{lapp}last-app-prefix.txt\"):\n                os.makedirs(lapp, exist_ok=True)\n                open(f\"{lapp}last-app-prefix.txt\", \"a\").close()\n            with open(f\"{lapp}last-app-prefix.txt\", \"w\") as prefix_file:\n                prefix_file.write(prefix)\n\n        self.prefix = prefix\n\n        node_ = node()\n\n        if 'localhost' in node_ and (host := os.getenv('HOSTNAME', 'localhost')) != 'localhost':\n            node_ = node_.replace('localhost', host)\n        self.id = prefix + '-' + node_\n        self.globals = {\n            \"root\": {**globals()},\n        }\n        self.locals = {\n            \"user\": {'app': self, **locals()},\n        }\n\n        identification = self.id\n\n        if \"test\" in prefix:\n            if self.system_flag == \"Darwin\" or self.system_flag == \"Linux\":\n                start_dir = self.start_dir.replace(\"ToolBoxV2/toolboxv2\", \"toolboxv2\")\n            else:\n                start_dir = self.start_dir.replace(\"ToolBoxV2\\\\toolboxv2\", \"toolboxv2\")\n            self.data_dir = start_dir + '\\\\.data\\\\' + \"test\"\n            self.config_dir = start_dir + '\\\\.config\\\\' + \"test\"\n            self.info_dir = start_dir + '\\\\.info\\\\' + \"test\"\n        else:\n            self.data_dir = self.start_dir + '\\\\.data\\\\' + identification\n            self.config_dir = self.start_dir + '\\\\.config\\\\' + identification\n            self.info_dir = self.start_dir + '\\\\.info\\\\' + identification\n\n        if self.appdata is None:\n            self.appdata = self.data_dir\n        else:\n            self.appdata += \"/ToolBoxV2\"\n\n        if not os.path.exists(self.appdata):\n            os.makedirs(self.appdata, exist_ok=True)\n        if not os.path.exists(self.data_dir):\n            os.makedirs(self.data_dir, exist_ok=True)\n        if not os.path.exists(self.config_dir):\n            os.makedirs(self.config_dir, exist_ok=True)\n        if not os.path.exists(self.info_dir):\n            os.makedirs(self.info_dir, exist_ok=True)\n\n        print(f\"Starting ToolBox as {prefix} from :\", Style.Bold(Style.CYAN(f\"{os.getcwd()}\")))\n\n        logger_info_str, self.logger, self.logging_filename = self.set_logger(args.debug)\n\n        print(\"Logger \" + logger_info_str)\n        print(\"================================\")\n        self.logger.info(\"Logger initialized\")\n        get_logger().info(Style.GREEN(\"Starting Application instance\"))\n        if args.init and args.init is not None and self.start_dir not in sys.path:\n            sys.path.append(self.start_dir)\n\n\n        __version__ = get_version_from_pyproject()\n\n        self.version = __version__\n\n        self.keys = {\n            \"MACRO\": \"macro~~~~:\",\n            \"MACRO_C\": \"m_color~~:\",\n            \"HELPER\": \"helper~~~:\",\n            \"debug\": \"debug~~~~:\",\n            \"id\": \"name-spa~:\",\n            \"st-load\": \"mute~load:\",\n            \"comm-his\": \"comm-his~:\",\n            \"develop-mode\": \"dev~mode~:\",\n            \"provider::\": \"provider::\",\n        }\n\n        defaults = {\n            \"MACRO\": ['Exit'],\n            \"MACRO_C\": {},\n            \"HELPER\": {},\n            \"debug\": args.debug,\n            \"id\": self.id,\n            \"st-load\": False,\n            \"comm-his\": [[]],\n            \"develop-mode\": False,\n        }\n        self.config_fh = FileHandler(self.id + \".config\", keys=self.keys, defaults=defaults)\n        self.config_fh.load_file_handler()\n        self._debug = args.debug\n        self.flows = {}\n        self.dev_modi = self.config_fh.get_file_handler(self.keys[\"develop-mode\"])\n        if self.config_fh.get_file_handler(\"provider::\") is None:\n            self.config_fh.add_to_save_file_handler(\"provider::\", \"http://localhost:\" + str(\n                self.args_sto.port) if os.environ.get(\"HOSTNAME\",\n                                                                     \"localhost\") == \"localhost\" else \"https://simplecore.app\")\n        self.functions = {}\n        self.modules = {}\n\n        self.interface_type = ToolBoxInterfaces.native\n        self.PREFIX = Style.CYAN(f\"~{node()}@&gt;\")\n        self.alive = True\n        self.called_exit = False, time.time()\n\n        self.print(f\"Infos:\\n  {'Name':&lt;8} -&gt; {node()}\\n  {'ID':&lt;8} -&gt; {self.id}\\n  {'Version':&lt;8} -&gt; {self.version}\\n\")\n\n        self.logger.info(\n            Style.GREEN(\n                f\"Finish init up in {time.perf_counter() - t0:.2f}s\"\n            )\n        )\n\n        self.args_sto = args\n        self.loop = None\n\n        from .system.session import Session\n        self.session: Session = Session(self.get_username())\n\n    def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n        user_name = self.config_fh.get_file_handler(\"ac_user:::\")\n        if get_input and user_name is None:\n            user_name = input(\"Input your username: \")\n            self.config_fh.add_to_save_file_handler(\"ac_user:::\", user_name)\n        if user_name is None:\n            user_name = default\n            self.config_fh.add_to_save_file_handler(\"ac_user:::\", user_name)\n        return user_name\n\n    def set_username(self, username):\n        return self.config_fh.add_to_save_file_handler(\"ac_user:::\", username)\n\n    @staticmethod\n    def exit_main(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def hide_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def show_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def disconnect(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def set_logger(self, debug=False):\n        if \"test\" in self.prefix and not debug:\n            logger, logging_filename = setup_logging(logging.NOTSET, name=\"toolbox-test\", interminal=True,\n                                                     file_level=logging.NOTSET, app_name=self.id)\n            logger_info_str = \"in Test Mode\"\n        elif \"live\" in self.prefix and not debug:\n            logger, logging_filename = setup_logging(logging.DEBUG, name=\"toolbox-live\", interminal=False,\n                                                     file_level=logging.WARNING, app_name=self.id)\n            logger_info_str = \"in Live Mode\"\n            # setup_logging(logging.WARNING, name=\"toolbox-live\", is_online=True\n            #              , online_level=logging.WARNING).info(\"Logger initialized\")\n        elif \"debug\" in self.prefix or self.prefix.endswith(\"D\"):\n            self.prefix = self.prefix.replace(\"-debug\", '').replace(\"debug\", '')\n            logger, logging_filename = setup_logging(logging.DEBUG, name=\"toolbox-debug\", interminal=True,\n                                                     file_level=logging.WARNING, app_name=self.id)\n            logger_info_str = \"in debug Mode\"\n            self.debug = True\n        elif debug:\n            logger, logging_filename = setup_logging(logging.DEBUG, name=f\"toolbox-{self.prefix}-debug\",\n                                                     interminal=True,\n                                                     file_level=logging.DEBUG, app_name=self.id)\n            logger_info_str = \"in args debug Mode\"\n        else:\n            logger, logging_filename = setup_logging(logging.ERROR, name=f\"toolbox-{self.prefix}\", app_name=self.id)\n            logger_info_str = \"in Default\"\n\n        return logger_info_str, logger, logging_filename\n\n    @property\n    def debug(self):\n        return self._debug\n\n    @debug.setter\n    def debug(self, value):\n        if not isinstance(value, bool):\n            self.logger.debug(f\"Value must be an boolean. is : {value} type of {type(value)}\")\n            raise ValueError(\"Value must be an boolean.\")\n\n        # self.logger.info(f\"Setting debug {value}\")\n        self._debug = value\n\n    def debug_rains(self, e):\n        if self.debug:\n            import traceback\n            print(traceback.format_exc())\n            raise e\n\n    def set_flows(self, r):\n        self.flows = r\n\n    async def run_flows(self, name, **kwargs):\n        from ..flows import flows_dict as flows_dict_func\n        if name not in self.flows:\n            self.flows = {**self.flows, **flows_dict_func(s=name, remote=True)}\n        if name in self.flows:\n            if asyncio.iscoroutinefunction(self.flows[name]):\n                return await self.flows[name](get_app(from_=\"runner\"), self.args_sto, **kwargs)\n            else:\n                return self.flows[name](get_app(from_=\"runner\"), self.args_sto, **kwargs)\n        else:\n            print(\"Flow not found, active flows:\", len(self.flows.keys()))\n\n    def _coppy_mod(self, content, new_mod_dir, mod_name, file_type='py'):\n\n        mode = 'xb'\n        self.logger.info(f\" coppy mod {mod_name} to {new_mod_dir} size : {sys.getsizeof(content) / 8388608:.3f} mb\")\n\n        if not os.path.exists(new_mod_dir):\n            os.makedirs(new_mod_dir)\n            with open(f\"{new_mod_dir}/__init__.py\", \"w\") as nmd:\n                nmd.write(f\"__version__ = '{self.version}'\")\n\n        if os.path.exists(f\"{new_mod_dir}/{mod_name}.{file_type}\"):\n            mode = False\n\n            with open(f\"{new_mod_dir}/{mod_name}.{file_type}\", 'rb') as d:\n                runtime_mod = d.read()  # Testing version but not efficient\n\n            if len(content) != len(runtime_mod):\n                mode = 'wb'\n\n        if mode:\n            with open(f\"{new_mod_dir}/{mod_name}.{file_type}\", mode) as f:\n                f.write(content)\n\n    def _pre_lib_mod(self, mod_name, path_to=\"./runtime\", file_type='py'):\n        working_dir = self.id.replace(\".\", \"_\")\n        lib_mod_dir = f\"toolboxv2.runtime.{working_dir}.mod_lib.\"\n\n        self.logger.info(f\"pre_lib_mod {mod_name} from {lib_mod_dir}\")\n\n        postfix = \"_dev\" if self.dev_modi else \"\"\n        mod_file_dir = f\"./mods{postfix}/{mod_name}.{file_type}\"\n        new_mod_dir = f\"{path_to}/{working_dir}/mod_lib\"\n        with open(mod_file_dir, \"rb\") as c:\n            content = c.read()\n        self._coppy_mod(content, new_mod_dir, mod_name, file_type=file_type)\n        return lib_mod_dir\n\n    def _copy_load(self, mod_name, file_type='py', **kwargs):\n        loc = self._pre_lib_mod(mod_name, file_type)\n        return self.inplace_load_instance(mod_name, loc=loc, **kwargs)\n\n    def helper_install_pip_module(self, module_name):\n        if 'main' in self.id:\n            return\n        self.print(f\"Installing {module_name} GREEDY\")\n        os.system(f\"{sys.executable} -m pip install {module_name}\")\n\n    def python_module_import_classifier(self, mod_name, error_message):\n\n        if error_message.startswith(\"No module named 'toolboxv2.utils\"):\n            return Result.default_internal_error(f\"404 {error_message.split('utils')[1]} not found\")\n        if error_message.startswith(\"No module named 'toolboxv2.mods\"):\n            if mod_name.startswith('.'):\n                return\n            return self.run_a_from_sync(self.a_run_any, (\"CloudM\", \"install\"), module_name=mod_name)\n        if error_message.startswith(\"No module named '\"):\n            pip_requ = error_message.split(\"'\")[1].replace(\"'\", \"\").strip()\n            # if 'y' in input(f\"\\t\\t\\tAuto install {pip_requ} Y/n\").lower:\n            return self.helper_install_pip_module(pip_requ)\n            # return Result.default_internal_error(f\"404 {pip_requ} not found\")\n\n    def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True, mfo=None):\n        if self.dev_modi and loc == \"toolboxv2.mods.\":\n            loc = \"toolboxv2.mods_dev.\"\n        if self.mod_online(mod_name):\n            self.logger.info(f\"Reloading mod from : {loc + mod_name}\")\n            self.remove_mod(mod_name, spec=spec, delete=False)\n\n        if (os.path.exists(self.start_dir + '/mods/' + mod_name) or os.path.exists(\n            self.start_dir + '/mods/' + mod_name + '.py')) and (\n            os.path.isdir(self.start_dir + '/mods/' + mod_name) or os.path.isfile(\n            self.start_dir + '/mods/' + mod_name + '.py')):\n            try:\n                if mfo is None:\n                    modular_file_object = import_module(loc + mod_name)\n                else:\n                    modular_file_object = mfo\n                self.modules[mod_name] = modular_file_object\n            except ModuleNotFoundError as e:\n                self.logger.error(Style.RED(f\"module {loc + mod_name} not found is type sensitive {e}\"))\n                self.print(Style.RED(f\"module {loc + mod_name} not found is type sensitive {e}\"))\n                if self.debug or self.args_sto.sysPrint:\n                    self.python_module_import_classifier(mod_name, str(e))\n                self.debug_rains(e)\n                return None\n        else:\n            self.print(f\"module {loc + mod_name} is not valid\")\n            return None\n        if hasattr(modular_file_object, \"Tools\"):\n            tools_class = modular_file_object.Tools\n        else:\n            if hasattr(modular_file_object, \"name\"):\n                tools_class = modular_file_object\n                modular_file_object = import_module(loc + mod_name)\n            else:\n                tools_class = None\n\n        modular_id = None\n        instance = modular_file_object\n        app_instance_type = \"file/application\"\n\n        if tools_class is None:\n            modular_id = modular_file_object.Name if hasattr(modular_file_object, \"Name\") else mod_name\n\n        if tools_class is None and modular_id is None:\n            modular_id = str(modular_file_object.__name__)\n            self.logger.warning(f\"Unknown instance loaded {mod_name}\")\n            return modular_file_object\n\n        if tools_class is not None:\n            tools_class = self.save_initialized_module(tools_class, spec)\n            modular_id = tools_class.name\n            app_instance_type = \"functions/class\"\n        else:\n            instance.spec = spec\n        # if private:\n        #     self.functions[modular_id][f\"{spec}_private\"] = private\n\n        if not save:\n            return instance if tools_class is None else tools_class\n\n        return self.save_instance(instance, modular_id, spec, app_instance_type, tools_class=tools_class)\n\n    def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n\n        if modular_id in self.functions and tools_class is None:\n            if self.functions[modular_id].get(f\"{spec}_instance\", None) is None:\n                self.functions[modular_id][f\"{spec}_instance\"] = instance\n                self.functions[modular_id][f\"{spec}_instance_type\"] = instance_type\n            else:\n                self.print(\"ERROR OVERRIDE\")\n                raise ImportError(f\"Module already known {modular_id}\")\n\n        elif tools_class is not None:\n            if modular_id not in self.functions:\n                self.functions[modular_id] = {}\n            self.functions[modular_id][f\"{spec}_instance\"] = tools_class\n            self.functions[modular_id][f\"{spec}_instance_type\"] = instance_type\n\n            try:\n                if not hasattr(tools_class, 'tools'):\n                    tools_class.tools = {\"Version\": tools_class.get_version, 'name': tools_class.name}\n                for function_name in list(tools_class.tools.keys()):\n                    t_function_name = function_name.lower()\n                    if t_function_name != \"all\" and t_function_name != \"name\":\n                        self.tb(function_name, mod_name=modular_id)(tools_class.tools.get(function_name))\n                self.functions[modular_id][f\"{spec}_instance_type\"] += \"/BC\"\n            except Exception as e:\n                self.logger.error(f\"Starting Module {modular_id} compatibility failed with : {e}\")\n                pass\n        elif modular_id not in self.functions and tools_class is None:\n            self.functions[modular_id] = {}\n            self.functions[modular_id][f\"{spec}_instance\"] = instance\n            self.functions[modular_id][f\"{spec}_instance_type\"] = instance_type\n\n        else:\n            raise ImportError(f\"Modular {modular_id} is not a valid mod\")\n        on_start = self.functions[modular_id].get(\"on_start\")\n        if on_start is not None:\n            i = 1\n            for f in on_start:\n                try:\n                    f_, e = self.get_function((modular_id, f), state=True, specification=spec)\n                    if e == 0:\n                        self.logger.info(Style.GREY(f\"Running On start {f} {i}/{len(on_start)}\"))\n                        if asyncio.iscoroutinefunction(f_):\n                            self.print(f\"Async on start is only in Tool claas supported for {modular_id}.{f}\" if tools_class is None else f\"initialization starting soon for {modular_id}.{f}\")\n                        else:\n                            o = f_()\n                            if o is not None:\n                                self.print(f\"Function {modular_id} On start result: {o}\")\n                    else:\n                        self.logger.warning(f\"starting function not found {e}\")\n                except Exception as e:\n                    self.logger.debug(Style.YELLOW(\n                        Style.Bold(f\"modular:{modular_id}.{f} on_start error {i}/{len(on_start)} -&gt; {e}\")))\n                    self.debug_rains(e)\n                finally:\n                    i += 1\n        return instance if tools_class is None else tools_class\n\n    def save_initialized_module(self, tools_class, spec):\n        tools_class.spec = spec\n        live_tools_class = tools_class(app=self)\n        return live_tools_class\n\n    def mod_online(self, mod_name, installed=False):\n        if installed and mod_name not in self.functions:\n            self.save_load(mod_name)\n        return mod_name in self.functions\n\n    def _get_function(self,\n                      name: Enum or None,\n                      state: bool = True,\n                      specification: str = \"app\",\n                      metadata=False, as_str: tuple or None = None, r=0):\n\n        if as_str is None and isinstance(name, Enum):\n            modular_id = str(name.NAME.value)\n            function_id = str(name.value)\n        elif as_str is None and isinstance(name, list):\n            modular_id, function_id = name[0], name[1]\n        else:\n            modular_id, function_id = as_str\n\n        self.logger.info(f\"getting function : {specification}.{modular_id}.{function_id}\")\n\n        if modular_id not in self.functions:\n            if r == 0:\n                self.save_load(modular_id, spec=specification)\n                return self.get_function(name=(modular_id, function_id),\n                                         state=state,\n                                         specification=specification,\n                                         metadata=metadata,\n                                         r=1)\n            self.logger.warning(f\"function modular not found {modular_id} 404\")\n            return \"404\", 100\n\n        if function_id not in self.functions[modular_id]:\n            self.logger.warning(f\"function data not found {modular_id}.{function_id} 404\")\n            return \"404\", 404\n\n        function_data = self.functions[modular_id][function_id]\n\n        if isinstance(function_data, list):\n            print(f\"functions {function_id} : {function_data}\")\n            function_data = self.functions[modular_id][function_data[-1]]\n\n        function = function_data.get(\"func\")\n        params = function_data.get(\"params\")\n\n        state_ = function_data.get(\"state\")\n        if state_ is not None and state != state_:\n            state = state_\n\n        if function is None:\n            self.logger.warning(\"No function found\")\n            return \"404\", 300\n\n        if params is None:\n            self.logger.warning(\"No function (params) found\")\n            return \"404\", 301\n\n        if metadata and not state:\n            self.logger.info(\"returning metadata stateless\")\n            return (function_data, function), 0\n\n        if not state:  # mens a stateless function\n            self.logger.info(\"returning stateless function\")\n            return function, 0\n\n        instance = self.functions[modular_id].get(f\"{specification}_instance\")\n\n        # instance_type = self.functions[modular_id].get(f\"{specification}_instance_type\", \"functions/class\")\n\n        if params[0] == 'app':\n            instance = get_app(from_=f\"fuction {specification}.{modular_id}.{function_id}\")\n\n        if instance is None and self.alive:\n            self.inplace_load_instance(modular_id, spec=specification)\n            instance = self.functions[modular_id].get(f\"{specification}_instance\")\n\n        if instance is None:\n            self.logger.warning(\"No live Instance found\")\n            return \"404\", 400\n\n        # if instance_type.endswith(\"/BC\"):  # for backwards compatibility  functions/class/BC old modules\n        #     # returning as stateless\n        #     # return \"422\", -1\n        #     self.logger.info(\n        #         f\"returning stateless function, cant find tools class for state handling found {instance_type}\")\n        #     if metadata:\n        #         self.logger.info(f\"returning metadata stateless\")\n        #         return (function_data, function), 0\n        #     return function, 0\n\n        self.logger.info(\"wrapping in higher_order_function\")\n\n        self.logger.info(f\"returned fuction {specification}.{modular_id}.{function_id}\")\n        higher_order_function = partial(function, instance)\n\n        if metadata:\n            self.logger.info(\"returning metadata stateful\")\n            return (function_data, higher_order_function), 0\n\n        self.logger.info(\"returning stateful function\")\n        return higher_order_function, 0\n\n    def save_exit(self):\n        self.logger.info(f\"save exiting saving data to {self.config_fh.file_handler_filename} states of {self.debug=}\")\n        self.config_fh.add_to_save_file_handler(self.keys[\"debug\"], str(self.debug))\n\n    def init_mod(self, mod_name, spec='app'):\n        if '.' in mod_name:\n            mod_name = mod_name.split('.')[0]\n        return self.loop_gard().run_until_complete(self.a_init_mod(mod_name, spec))\n\n    def run_bg_task(self, task):\n        \"\"\"\n        Run a task in the background that will properly handle nested asyncio operations.\n        This implementation ensures that asyncio.create_task() and asyncio.gather() work\n        correctly within the background task.\n\n        Args:\n            task: A callable function that can be synchronous or asynchronous\n        \"\"\"\n        if not callable(task):\n            self.logger.warning(\"Task is not callable!\")\n            return None\n\n        # Function that will run in a separate thread with its own event loop\n        def thread_target(task_):\n            # Create a new event loop for this thread\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n\n            try:\n                # Determine how to run the task based on its type\n                if asyncio.iscoroutinefunction(task_):\n                    # If it's an async function, run it directly\n                    loop.run_until_complete(task_())\n                elif asyncio.iscoroutine(task_):\n                    # If it's already a coroutine object\n                    loop.run_until_complete(task_)\n                else:\n                    # If it's a synchronous function that might create async tasks internally\n                    async def wrapper():\n                        # Run potentially blocking synchronous code in an executor\n                        return await loop.run_in_executor(None, task_)\n\n                    loop.run_until_complete(wrapper())\n\n                self.logger.debug(\"Background task completed successfully\")\n            except Exception as e:\n                self.logger.error(f\"Background task failed with error: {str(e)}\")\n            finally:\n                # Clean up any pending tasks\n                pending = asyncio.all_tasks(loop)\n                if pending:\n                    # Cancel any remaining tasks\n                    for task_ in pending:\n                        task_.cancel()\n\n                    # Allow tasks to finish cancellation\n                    loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))\n\n                loop.close()\n\n        # Create and start a non-daemon thread that will run to completion\n        # Using non-daemon thread ensures the task completes even if main thread exits\n        t = threading.Thread(target=thread_target, args=(task,))\n        t.daemon = False  # Non-daemon thread will keep program alive until it completes\n        self.bg_tasks.append(t)\n        t.start()\n        return t\n\n    # Alternative implementation that may be needed if your function creates many nested tasks\n    def run_bg_task_advanced(self, task, *args, **kwargs):\n        \"\"\"\n        Alternative implementation for complex async scenarios where the task creates\n        nested asyncio tasks using create_task() and gather().\n\n        This version ensures proper execution of nested tasks by maintaining the thread\n        and its event loop throughout the lifetime of all child tasks.\n\n        Args:\n            task: A callable function that can be synchronous or asynchronous\n            *args, **kwargs: Arguments to pass to the task\n        \"\"\"\n        if not callable(task):\n            self.logger.warning(\"Task is not callable!\")\n            return None\n\n        # Create a dedicated thread with its own event loop\n        async def async_wrapper():\n            try:\n                if asyncio.iscoroutinefunction(task):\n                    return await task(*args, **kwargs)\n                elif asyncio.iscoroutine(task):\n                    return await task\n                else:\n                    # Run in executor to avoid blocking\n                    loop = asyncio.get_event_loop()\n                    return await loop.run_in_executor(None, lambda: task(*args, **kwargs))\n            except Exception as e:\n                self.logger.error(f\"Background task error: {str(e)}\")\n                raise\n\n        def thread_target():\n            # Create new event loop for this thread\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n\n            try:\n                # Run the task to completion with all its nested tasks\n                loop.run_until_complete(async_wrapper())\n            except Exception as e:\n                self.logger.error(f\"Background task thread failed: {str(e)}\")\n            finally:\n                # Clean up any pending tasks that might still be running\n                try:\n                    pending = asyncio.all_tasks(loop)\n                    if pending:\n                        # Allow tasks time to clean up\n                        loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))\n                except Exception:\n                    pass\n\n                loop.close()\n\n        # Use a non-daemon thread so it will run to completion\n        t = threading.Thread(target=thread_target, daemon=True)\n        t.daemon = False\n        self.bg_tasks.append(t)\n        t.start()\n        return t\n\n    # Helper method to wait for background tasks to complete (optional)\n    def wait_for_bg_tasks(self, timeout=None):\n        \"\"\"\n        Wait for all background tasks to complete.\n\n        Args:\n            timeout: Maximum time to wait (in seconds) for all tasks to complete.\n                     None means wait indefinitely.\n\n        Returns:\n            bool: True if all tasks completed, False if timeout occurred\n        \"\"\"\n        active_tasks = [t for t in self.bg_tasks if t.is_alive()]\n\n        for task in active_tasks:\n            task.join(timeout=timeout)\n            if task.is_alive():\n                return False\n\n        return True\n\n    def run(self, *args, request=None, running_function_coro=None, **kwargs):\n        \"\"\"\n        Run a function with support for SSE streaming in both\n        threaded and non-threaded contexts.\n        \"\"\"\n        if running_function_coro is None:\n            mn, fn = args[0]\n            if self.functions.get(mn, {}).get(fn, {}).get('request_as_kwarg', False):\n                kwargs[\"request\"] = request\n                if 'data' in kwargs and 'data' not in self.functions.get(mn, {}).get(fn, {}).get('params', []):\n                    kwargs[\"request\"]['data'] = kwargs['data']\n                    del kwargs['data']\n                if 'form_data' in kwargs and 'form_data' not in self.functions.get(mn, {}).get(fn, {}).get('params',\n                                                                                                           []):\n                    kwargs[\"request\"]['form_data'] = kwargs['form_data']\n                    del kwargs['form_data']\n                kwargs[\"request\"] = RequestData.from_dict(request)\n\n        # Create the coroutine\n        coro = running_function_coro or self.a_run_any(*args, **kwargs)\n\n        # Get or create an event loop\n        try:\n            loop = asyncio.get_event_loop()\n            is_running = loop.is_running()\n        except RuntimeError:\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n            is_running = False\n\n        # If the loop is already running, run in a separate thread\n        if is_running:\n            # Create thread pool executor as needed\n            if not hasattr(self.__class__, '_executor'):\n                self.__class__._executor = ThreadPoolExecutor(max_workers=4)\n\n            def run_in_new_thread():\n                # Set up a new loop in this thread\n                new_loop = asyncio.new_event_loop()\n                asyncio.set_event_loop(new_loop)\n\n                try:\n                    # Run the coroutine\n                    return new_loop.run_until_complete(coro)\n                finally:\n                    new_loop.close()\n\n            # Run in thread and get result\n            thread_result = self.__class__._executor.submit(run_in_new_thread).result()\n\n            # Handle streaming results from thread\n            if isinstance(thread_result, dict) and thread_result.get(\"is_stream\"):\n                # Create a new SSE stream in the main thread\n                async def stream_from_function():\n                    # Re-run the function with direct async access\n                    stream_result = await self.a_run_any(*args, **kwargs)\n\n                    if (isinstance(stream_result, Result) and\n                        getattr(stream_result.result, 'data_type', None) == \"stream\"):\n                        # Get and forward data from the original generator\n                        original_gen = stream_result.result.data.get(\"generator\")\n                        if inspect.isasyncgen(original_gen):\n                            async for item in original_gen:\n                                yield item\n\n                # Return a new streaming Result\n                return Result.stream(\n                    stream_generator=stream_from_function(),\n                    headers=thread_result.get(\"headers\", {})\n                )\n\n            result = thread_result\n        else:\n            # Direct execution when loop is not running\n            result = loop.run_until_complete(coro)\n\n        # Process the final result\n        if isinstance(result, Result):\n            result.print()\n            if getattr(result.result, 'data_type', None) == \"stream\":\n                return result\n            return result.to_api_result().model_dump(mode='json')\n\n        return result\n\n    def loop_gard(self):\n        if self.loop is None:\n            self.loop = asyncio.get_event_loop()\n        if self.loop.is_closed():\n            self.loop = asyncio.get_event_loop()\n        return self.loop\n\n    async def a_init_mod(self, mod_name, spec='app'):\n        mod = self.save_load(mod_name, spec=spec)\n        if hasattr(mod, \"__initobj\") and not mod.async_initialized:\n            await mod\n        return mod\n\n\n    def load_mod(self, mod_name: str, mlm='I', **kwargs):\n\n        action_list_helper = ['I (inplace load dill on error python)',\n                              # 'C (coppy py file to runtime dir)',\n                              # 'S (save py file to dill)',\n                              # 'CS (coppy and save py file)',\n                              # 'D (development mode, inplace load py file)'\n                              ]\n        action_list = {\"I\": lambda: self.inplace_load_instance(mod_name, **kwargs),\n                       \"C\": lambda: self._copy_load(mod_name, **kwargs)\n                       }\n\n        try:\n            if mlm in action_list:\n\n                return action_list.get(mlm)()\n            else:\n                self.logger.critical(\n                    f\"config mlm must be {' or '.join(action_list_helper)} is {mlm=}\")\n                raise ValueError(f\"config mlm must be {' or '.join(action_list_helper)} is {mlm=}\")\n        except ValueError as e:\n            self.logger.warning(Style.YELLOW(f\"Error Loading Module '{mod_name}', with error :{e}\"))\n            self.debug_rains(e)\n        except ImportError as e:\n            self.logger.error(Style.YELLOW(f\"Error Loading Module '{mod_name}', with error :{e}\"))\n            self.debug_rains(e)\n        except Exception as e:\n            self.logger.critical(Style.RED(f\"Error Loading Module '{mod_name}', with critical error :{e}\"))\n            print(Style.RED(f\"Error Loading Module '{mod_name}'\"))\n            self.debug_rains(e)\n\n        return Result.default_internal_error(info=\"info's in logs.\")\n\n    async def load_all_mods_in_file(self, working_dir=\"mods\"):\n        print(f\"LOADING ALL MODS FROM FOLDER : {working_dir}\")\n        t0 = time.perf_counter()\n        # Get the list of all modules\n        module_list = self.get_all_mods(working_dir)\n        open_modules = self.functions.keys()\n        start_len = len(open_modules)\n\n        for om in open_modules:\n            if om in module_list:\n                module_list.remove(om)\n\n        tasks: set[Task] = set()\n\n        _ = {tasks.add(asyncio.create_task(asyncio.to_thread(self.save_load, mod, 'app'))) for mod in module_list}\n        for t in asyncio.as_completed(tasks):\n            try:\n                result = await t\n                if hasattr(result, 'Name'):\n                    print('Opened :', result.Name)\n                elif hasattr(result, 'name'):\n                    if hasattr(result, 'async_initialized'):\n                        if not result.async_initialized:\n                            async def _():\n                                try:\n                                    if asyncio.iscoroutine(result):\n                                        await result\n                                    if hasattr(result, 'Name'):\n                                        print('Opened :', result.Name)\n                                    elif hasattr(result, 'name'):\n                                        print('Opened :', result.name)\n                                except Exception as e:\n                                    self.debug_rains(e)\n                                    if hasattr(result, 'Name'):\n                                        print('Error opening :', result.Name)\n                                    elif hasattr(result, 'name'):\n                                        print('Error opening :', result.name)\n                            asyncio.create_task(_())\n                        else:\n                            print('Opened :', result.name)\n                else:\n                    print('Opened :', result)\n            except Exception as e:\n                self.logger.error(Style.RED(f\"An Error occurred while opening all modules error: {str(e)}\"))\n                self.debug_rains(e)\n        opened = len(self.functions.keys()) - start_len\n\n        self.logger.info(f\"Opened {opened} modules in {time.perf_counter() - t0:.2f}s\")\n        return f\"Opened {opened} modules in {time.perf_counter() - t0:.2f}s\"\n\n    def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\", use_wd=True):\n        self.logger.info(f\"collating all mods in working directory {working_dir}\")\n\n        pr = \"_dev\" if self.dev_modi else \"\"\n        if working_dir == \"mods\" and use_wd:\n            working_dir = f\"{self.start_dir}/mods{pr}\"\n        elif use_wd:\n            pass\n        else:\n            w_dir = self.id.replace(\".\", \"_\")\n            working_dir = f\"{path_to}/{w_dir}/mod_lib{pr}/\"\n        res = os.listdir(working_dir)\n\n        self.logger.info(f\"found : {len(res)} files\")\n\n        def do_helper(_mod):\n            if \"mainTool\" in _mod:\n                return False\n            # if not _mod.endswith(\".py\"):\n            #     return False\n            if _mod.startswith(\"__\"):\n                return False\n            if _mod.startswith(\".\"):\n                return False\n            return not _mod.startswith(\"test_\")\n\n        def r_endings(word: str):\n            if word.endswith(\".py\"):\n                return word[:-3]\n            return word\n\n        mods_list = list(map(r_endings, filter(do_helper, res)))\n\n        self.logger.info(f\"found : {len(mods_list)} Modules\")\n        return mods_list\n\n    def remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            self.remove_mod(mod, delete=delete)\n\n    def remove_mod(self, mod_name, spec='app', delete=True):\n        if mod_name not in self.functions:\n            self.logger.info(f\"mod not active {mod_name}\")\n            return\n        on_exit = self.functions[mod_name].get(\"on_exit\")\n\n        def helper():\n            if f\"{spec}_instance\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance\"]\n            if f\"{spec}_instance_type\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance_type\"]\n\n        if on_exit is None and self.functions[mod_name].get(f\"{spec}_instance_type\", \"\").endswith(\"/BC\"):\n            instance = self.functions[mod_name].get(f\"{spec}_instance\", None)\n            if instance is not None and hasattr(instance, 'on_exit'):\n                if asyncio.iscoroutinefunction(instance.on_exit):\n                    self.exit_tasks.append(instance.on_exit)\n                else:\n                    instance.on_exit()\n\n        if on_exit is None and delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n            return\n        if on_exit is None:\n            helper()\n            return\n\n        i = 1\n        for f in on_exit:\n            try:\n                f_, e = self.get_function((mod_name, f), state=True, specification=spec)\n                if e == 0:\n                    self.logger.info(Style.GREY(f\"Running On exit {f} {i}/{len(on_exit)}\"))\n                    if asyncio.iscoroutinefunction(f_):\n                        self.exit_tasks.append(f_)\n                        o = None\n                    else:\n                        o = f_()\n                    if o is not None:\n                        self.print(f\"Function On Exit result: {o}\")\n                else:\n                    self.logger.warning(\"closing function not found\")\n            except Exception as e:\n                self.logger.debug(\n                    Style.YELLOW(Style.Bold(f\"modular:{mod_name}.{f} on_exit error {i}/{len(on_exit)} -&gt; {e}\")))\n            finally:\n                i += 1\n\n        helper()\n\n        if delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n\n    async def a_remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            await self.a_remove_mod(mod, delete=delete)\n\n    async def a_remove_mod(self, mod_name, spec='app', delete=True):\n        if mod_name not in self.functions:\n            self.logger.info(f\"mod not active {mod_name}\")\n            return\n        on_exit = self.functions[mod_name].get(\"on_exit\")\n\n        def helper():\n            if f\"{spec}_instance\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance\"]\n            if f\"{spec}_instance_type\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance_type\"]\n\n        if on_exit is None and self.functions[mod_name].get(f\"{spec}_instance_type\", \"\").endswith(\"/BC\"):\n            instance = self.functions[mod_name].get(f\"{spec}_instance\", None)\n            if instance is not None and hasattr(instance, 'on_exit'):\n                if asyncio.iscoroutinefunction(instance.on_exit):\n                    await instance.on_exit()\n                else:\n                    instance.on_exit()\n\n        if on_exit is None and delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n            return\n        if on_exit is None:\n            helper()\n            return\n\n        i = 1\n        for f in on_exit:\n            try:\n                f_, e = self.get_function((mod_name, f), state=True, specification=spec)\n                if e == 0:\n                    self.logger.info(Style.GREY(f\"Running On exit {f} {i}/{len(on_exit)}\"))\n                    if asyncio.iscoroutinefunction(f_):\n                        o = await f_()\n                    else:\n                        o = f_()\n                    if o is not None:\n                        self.print(f\"Function On Exit result: {o}\")\n                else:\n                    self.logger.warning(\"closing function not found\")\n            except Exception as e:\n                self.logger.debug(\n                    Style.YELLOW(Style.Bold(f\"modular:{mod_name}.{f} on_exit error {i}/{len(on_exit)} -&gt; {e}\")))\n            finally:\n                i += 1\n\n        helper()\n\n        if delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n\n    def exit(self, remove_all=True):\n        if not self.alive:\n            return\n        if self.args_sto.debug:\n            self.hide_console()\n        self.disconnect()\n        if remove_all:\n            self.remove_all_modules()\n        self.logger.info(\"Exiting ToolBox interface\")\n        self.alive = False\n        self.called_exit = True, time.time()\n        self.save_exit()\n        try:\n            self.config_fh.save_file_handler()\n        except SystemExit:\n            print(\"If u ar testing this is fine else ...\")\n\n        if hasattr(self, 'daemon_app'):\n            import threading\n\n            for thread in threading.enumerate()[::-1]:\n                if thread.name == \"MainThread\":\n                    continue\n                try:\n                    with Spinner(f\"closing Thread {thread.name:^50}|\", symbols=\"s\", count_down=True,\n                                 time_in_s=0.751 if not self.debug else 0.6):\n                        thread.join(timeout=0.751 if not self.debug else 0.6)\n                except TimeoutError as e:\n                    self.logger.error(f\"Timeout error on exit {thread.name} {str(e)}\")\n                    print(str(e), f\"Timeout {thread.name}\")\n                except KeyboardInterrupt:\n                    print(\"Unsave Exit\")\n                    break\n        if hasattr(self, 'loop') and self.loop is not None:\n            with Spinner(\"closing Event loop:\", symbols=\"+\"):\n                self.loop.stop()\n\n    async def a_exit(self):\n        await self.a_remove_all_modules()\n        results = await asyncio.gather(\n            *[asyncio.create_task(f()) for f in self.exit_tasks if asyncio.iscoroutinefunction(f)])\n        for result in results:\n            self.print(f\"Function On Exit result: {result}\")\n        self.exit(remove_all=False)\n\n    def save_load(self, modname, spec='app'):\n        self.logger.debug(f\"Save load module {modname}\")\n        if not modname:\n            self.logger.warning(\"no filename specified\")\n            return False\n        try:\n            return self.load_mod(modname, spec=spec)\n        except ModuleNotFoundError as e:\n            self.logger.error(Style.RED(f\"Module {modname} not found\"))\n            self.debug_rains(e)\n\n        return False\n\n    def get_function(self, name: Enum or tuple, **kwargs):\n        \"\"\"\n        Kwargs for _get_function\n            metadata:: return the registered function dictionary\n                stateless: (function_data, None), 0\n                stateful: (function_data, higher_order_function), 0\n            state::boolean\n                specification::str default app\n        \"\"\"\n        if isinstance(name, tuple):\n            return self._get_function(None, as_str=name, **kwargs)\n        else:\n            return self._get_function(name, **kwargs)\n\n    async def a_run_function(self, mod_function_name: Enum or tuple,\n                             tb_run_function_with_state=True,\n                             tb_run_with_specification='app',\n                             args_=None,\n                             kwargs_=None,\n                             *args,\n                             **kwargs) -&gt; Result:\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n        if isinstance(mod_function_name, tuple):\n            modular_name, function_name = mod_function_name\n        elif isinstance(mod_function_name, list):\n            modular_name, function_name = mod_function_name[0], mod_function_name[1]\n        elif isinstance(mod_function_name, Enum):\n            modular_name, function_name = mod_function_name.__class__.NAME.value, mod_function_name.value\n        else:\n            raise TypeError(\"Unknown function type\")\n\n        if not self.mod_online(modular_name, installed=True):\n            self.get_mod(modular_name)\n\n        function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                      metadata=True, specification=tb_run_with_specification)\n        self.logger.info(f\"Received fuction : {mod_function_name}, with execode: {error_code}\")\n        if error_code == 404:\n            mod = self.get_mod(modular_name)\n            if hasattr(mod, \"async_initialized\") and not mod.async_initialized:\n                await mod\n            function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                          metadata=True, specification=tb_run_with_specification)\n\n        if error_code == 404:\n            self.logger.warning(Style.RED(\"Function Not Found\"))\n            return (Result.default_user_error(interface=self.interface_type,\n                                              exec_code=404,\n                                              info=\"function not found function is not decorated\").\n                    set_origin(mod_function_name))\n\n        if error_code == 300:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 info=f\"module {modular_name}\"\n                                                      f\" has no state (instance)\").set_origin(mod_function_name)\n\n        if error_code != 0:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=error_code,\n                                                 info=f\"Internal error\"\n                                                      f\" {modular_name}.\"\n                                                      f\"{function_name}\").set_origin(mod_function_name)\n\n        if not tb_run_function_with_state:\n            function_data, _ = function_data\n            function = function_data.get('func')\n        else:\n            function_data, function = function_data\n\n        if not function:\n            self.logger.warning(Style.RED(f\"Function {function_name} not found\"))\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=404,\n                                                 info=\"function not found function\").set_origin(mod_function_name)\n\n        self.logger.info(\"Profiling function\")\n        t0 = time.perf_counter()\n        if asyncio.iscoroutinefunction(function):\n            return await self.a_fuction_runner(function, function_data, args, kwargs, t0)\n        else:\n            return self.fuction_runner(function, function_data, args, kwargs, t0)\n\n    def run_function(self, mod_function_name: Enum or tuple,\n                     tb_run_function_with_state=True,\n                     tb_run_with_specification='app',\n                     args_=None,\n                     kwargs_=None,\n                     *args,\n                     **kwargs) -&gt; Result:\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n        if isinstance(mod_function_name, tuple):\n            modular_name, function_name = mod_function_name\n        elif isinstance(mod_function_name, list):\n            modular_name, function_name = mod_function_name[0], mod_function_name[1]\n        elif isinstance(mod_function_name, Enum):\n            modular_name, function_name = mod_function_name.__class__.NAME.value, mod_function_name.value\n        else:\n            raise TypeError(\"Unknown function type\")\n\n        if not self.mod_online(modular_name, installed=True):\n            self.get_mod(modular_name)\n\n        function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                      metadata=True, specification=tb_run_with_specification)\n        self.logger.info(f\"Received fuction : {mod_function_name}, with execode: {error_code}\")\n        if error_code == 1 or error_code == 3 or error_code == 400:\n            self.get_mod(modular_name)\n            function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                          metadata=True, specification=tb_run_with_specification)\n\n        if error_code == 2:\n            self.logger.warning(Style.RED(\"Function Not Found\"))\n            return (Result.default_user_error(interface=self.interface_type,\n                                              exec_code=404,\n                                              info=\"function not found function is not decorated\").\n                    set_origin(mod_function_name))\n\n        if error_code == -1:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 info=f\"module {modular_name}\"\n                                                      f\" has no state (instance)\").set_origin(mod_function_name)\n\n        if error_code != 0:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=error_code,\n                                                 info=f\"Internal error\"\n                                                      f\" {modular_name}.\"\n                                                      f\"{function_name}\").set_origin(mod_function_name)\n\n        if not tb_run_function_with_state:\n            function_data, _ = function_data\n            function = function_data.get('func')\n        else:\n            function_data, function = function_data\n\n        if not function:\n            self.logger.warning(Style.RED(f\"Function {function_name} not found\"))\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=404,\n                                                 info=\"function not found function\").set_origin(mod_function_name)\n\n        self.logger.info(\"Profiling function\")\n        t0 = time.perf_counter()\n        if asyncio.iscoroutinefunction(function):\n            raise ValueError(f\"Fuction {function_name} is Async use a_run_any\")\n        else:\n            return self.fuction_runner(function, function_data, args, kwargs, t0)\n\n    def run_a_from_sync(self, function, *args, **kwargs):\n        # Initialize self.loop if not already set.\n        if self.loop is None:\n            try:\n                self.loop = asyncio.get_running_loop()\n            except RuntimeError:\n                self.loop = asyncio.new_event_loop()\n\n        # If the loop is running, offload the coroutine to a new thread.\n        if self.loop.is_running():\n            result_future = Future()\n\n            def run_in_new_loop():\n                new_loop = asyncio.new_event_loop()\n                asyncio.set_event_loop(new_loop)\n                try:\n                    result = new_loop.run_until_complete(function(*args, **kwargs))\n                    result_future.set_result(result)\n                except Exception as e:\n                    result_future.set_exception(e)\n                finally:\n                    new_loop.close()\n\n            thread = threading.Thread(target=run_in_new_loop)\n            thread.start()\n            thread.join()  # Block until the thread completes.\n            return result_future.result()\n        else:\n            # If the loop is not running, schedule and run the coroutine directly.\n            future = self.loop.create_task(function(*args, **kwargs))\n            return self.loop.run_until_complete(future)\n\n    def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        row = function_data.get('row')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        if_self_state = 1 if 'self' in parameters else 0\n\n        try:\n            if len(parameters) == 0:\n                res = function()\n            elif len(parameters) == len(args) + if_self_state:\n                res = function(*args)\n            elif len(parameters) == len(kwargs.keys()) + if_self_state:\n                res = function(**kwargs)\n            else:\n                res = function(*args, **kwargs)\n            self.logger.info(f\"Execution done in {time.perf_counter()-t0:.4f}\")\n            if isinstance(res, Result):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.set_origin(mod_function_name)\n            elif isinstance(res, ApiResult):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.as_result().set_origin(mod_function_name).to_api_result()\n            elif row:\n                formatted_result = res\n            else:\n                # Wrap the result in a Result object\n                formatted_result = Result.ok(\n                    interface=self.interface_type,\n                    data_info=\"Auto generated result\",\n                    data=res,\n                    info=\"Function executed successfully\"\n                ).set_origin(mod_function_name)\n            if not row:\n                self.logger.info(\n                    f\"Function Exec code: {formatted_result.info.exec_code} Info's: {formatted_result.info.help_text}\")\n            else:\n                self.logger.info(\n                    f\"Function Exec data: {formatted_result}\")\n        except Exception as e:\n            self.logger.error(\n                Style.YELLOW(Style.Bold(\n                    f\"! Function ERROR: in {modular_name}.{function_name}\")))\n            # Wrap the exception in a Result object\n            formatted_result = Result.default_internal_error(info=str(e)).set_origin(mod_function_name)\n            # res = formatted_result\n            self.logger.error(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed wit an error {str(e)}, {type(e)}\")\n            self.debug_rains(e)\n            self.print(f\"! Function ERROR: in {modular_name}.{function_name} \")\n        else:\n            self.print_ok()\n\n            self.logger.info(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed successfully\")\n\n        return formatted_result\n\n    async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        row = function_data.get('row')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        if_self_state = 1 if 'self' in parameters else 0\n\n        try:\n            if len(parameters) == 0:\n                res = await function()\n            elif len(parameters) == len(args) + if_self_state:\n                res = await function(*args)\n            elif len(parameters) == len(kwargs.keys()) + if_self_state:\n                res = await function(**kwargs)\n            else:\n                res = await function(*args, **kwargs)\n            self.logger.info(f\"Execution done in {time.perf_counter()-t0:.4f}\")\n            if isinstance(res, Result):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.set_origin(mod_function_name)\n            elif isinstance(res, ApiResult):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.as_result().set_origin(mod_function_name).to_api_result()\n            elif row:\n                formatted_result = res\n            else:\n                # Wrap the result in a Result object\n                formatted_result = Result.ok(\n                    interface=self.interface_type,\n                    data_info=\"Auto generated result\",\n                    data=res,\n                    info=\"Function executed successfully\"\n                ).set_origin(mod_function_name)\n            if not row:\n                self.logger.info(\n                    f\"Function Exec code: {formatted_result.info.exec_code} Info's: {formatted_result.info.help_text}\")\n            else:\n                self.logger.info(\n                    f\"Function Exec data: {formatted_result}\")\n        except Exception as e:\n            self.logger.error(\n                Style.YELLOW(Style.Bold(\n                    f\"! Function ERROR: in {modular_name}.{function_name}\")))\n            # Wrap the exception in a Result object\n            formatted_result = Result.default_internal_error(info=str(e)).set_origin(mod_function_name)\n            # res = formatted_result\n            self.logger.error(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed wit an error {str(e)}, {type(e)}\")\n            self.debug_rains(e)\n\n        else:\n            self.print_ok()\n\n            self.logger.info(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed successfully\")\n\n        return formatted_result\n\n    async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None,\n                       args_=None,\n                       kwargs_=None, method=\"GET\",\n                       *args, **kwargs):\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n\n        modular_name = mod_function_name\n        function_name = function_name\n\n        if isinstance(mod_function_name, str) and isinstance(function_name, str):\n            mod_function_name = (mod_function_name, function_name)\n\n        if isinstance(mod_function_name, tuple):\n            modular_name, function_name = mod_function_name\n        elif isinstance(mod_function_name, list):\n            modular_name, function_name = mod_function_name[0], mod_function_name[1]\n        elif isinstance(mod_function_name, Enum):\n            modular_name, function_name = mod_function_name.__class__.NAME.value, mod_function_name.value\n\n        r = await self.session.fetch(f\"/api/{modular_name}/{function_name}{'?' + args_ if args_ is not None else ''}\",\n                                     data=kwargs, method=method)\n        try:\n            if not r:\n                print(\"\u00a7 Session server Offline!\", self.session.base)\n                return Result.default_internal_error(msg=\"Session fetch failed\").as_dict()\n\n            content_type = r.headers.get('Content-Type', '').lower()\n            raw = await r.read()\n            encoding = r.get_encoding() or 'utf-8'\n            text = raw.decode(encoding, errors='ignore')\n\n            # Attempt JSON\n            if 'application/json' in content_type:\n                try:\n                    return await r.json()\n                except Exception as e:\n                    print(\"\u26a0 JSON decode error:\", e)\n\n            # Attempt YAML\n            if 'yaml' in content_type or text.strip().startswith('---'):\n                try:\n                    import yaml\n                    return yaml.safe_load(text)\n                except Exception as e:\n                    print(\"\u26a0 YAML decode error:\", e)\n\n            # Attempt XML\n            if 'xml' in content_type or text.strip().startswith('&lt;?xml'):\n                try:\n                    import xmltodict\n                    return xmltodict.parse(text)\n                except Exception as e:\n                    print(\"\u26a0 XML decode error:\", e)\n\n            # Fallback: return plain text\n            return Result.default_internal_error(data={'raw_text': text, 'content_type': content_type}).as_dict()\n\n        except Exception as e:\n            print(\"\u274c Fatal error during API call:\", e)\n            return Result.default_internal_error(str(e)).as_dict()\n\n    def run_local(self, *args, **kwargs):\n        return self.run_any(*args, **kwargs)\n\n    async def a_run_local(self, *args, **kwargs):\n        return await self.a_run_any(*args, **kwargs)\n\n    def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n                get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                kwargs_=None,\n                *args, **kwargs):\n\n        # if self.debug:\n        #     self.logger.info(f'Called from: {getouterframes(currentframe(), 2)}')\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n\n        if isinstance(mod_function_name, str) and isinstance(backwords_compability_variabel_string_holder, str):\n            mod_function_name = (mod_function_name, backwords_compability_variabel_string_holder)\n\n        res: Result = self.run_function(mod_function_name,\n                                        tb_run_function_with_state=tb_run_function_with_state,\n                                        tb_run_with_specification=tb_run_with_specification,\n                                        args_=args, kwargs_=kwargs).as_result()\n        if isinstance(res, ApiResult):\n            res = res.as_result()\n\n        if isinstance(res, Result) and res.bg_task is not None:\n            self.run_bg_task(res.bg_task)\n\n        if self.debug:\n            res.log(show_data=False)\n        if not get_results and isinstance(res, Result):\n            return res.get()\n\n        return res\n\n    async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                        backwords_compability_variabel_string_holder=None,\n                        get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                        kwargs_=None,\n                        *args, **kwargs):\n\n        # if self.debug:\n        #     self.logger.info(f'Called from: {getouterframes(currentframe(), 2)}')\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n\n        if isinstance(mod_function_name, str) and isinstance(backwords_compability_variabel_string_holder, str):\n            mod_function_name = (mod_function_name, backwords_compability_variabel_string_holder)\n\n        res: Result = await self.a_run_function(mod_function_name,\n                                                tb_run_function_with_state=tb_run_function_with_state,\n                                                tb_run_with_specification=tb_run_with_specification,\n                                                args_=args, kwargs_=kwargs)\n        if isinstance(res, ApiResult):\n            res = res.as_result()\n\n        if isinstance(res, Result) and res.bg_task is not None:\n            self.run_bg_task(res.bg_task)\n\n        if self.debug:\n            res.log(show_data=False)\n        if not get_results and isinstance(res, Result):\n            return res.get()\n\n        return res\n\n\n    def web_context(self):\n        if self._web_context is None:\n            self._web_context = open(\"./dist/helper.html\", encoding=\"utf-8\").read()\n        return self._web_context\n\n    def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n        if spec != \"app\":\n            self.print(f\"Getting Module {name} spec: {spec}\")\n        if name not in self.functions:\n            mod = self.save_load(name, spec=spec)\n            if mod is False or (isinstance(mod, Result) and mod.is_error()):\n                self.logger.warning(f\"Could not find {name} in {list(self.functions.keys())}\")\n                raise ValueError(f\"Could not find {name} in {list(self.functions.keys())} pleas install the module, or its posibly broken use --debug for infos\")\n        # private = self.functions[name].get(f\"{spec}_private\")\n        # if private is not None:\n        #     if private and spec != 'app':\n        #         raise ValueError(\"Module is private\")\n        if name not in self.functions:\n            self.logger.warning(f\"Module '{name}' is not found\")\n            return None\n        instance = self.functions[name].get(f\"{spec}_instance\")\n        if instance is None:\n            return self.load_mod(name, spec=spec)\n        return self.functions[name].get(f\"{spec}_instance\")\n\n    def print(self, text, *args, **kwargs):\n        # self.logger.info(f\"Output : {text}\")\n        if self.sprint(None):\n            print(Style.CYAN(f\"System${self.id}:\"), end=\" \")\n        print(text, *args, **kwargs)\n\n    def sprint(self, text, *args, **kwargs):\n        if text is None:\n            return True\n        # self.logger.info(f\"Output : {text}\")\n        print(Style.CYAN(f\"System${self.id}:\"), end=\" \")\n        if isinstance(text, str) and kwargs == {} and text:\n            stram_print(text + ' '.join(args))\n            print()\n        else:\n            print(text, *args, **kwargs)\n\n    # ----------------------------------------------------------------\n    # Decorators for the toolbox\n\n    def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n        self.remove_mod(mod_name, delete=True)\n        if hasattr(self.modules[mod_name], 'reload_save') and self.modules[mod_name].reload_save:\n            def reexecute_module_code(x):\n                return x\n        else:\n            def reexecute_module_code(module_name):\n                if isinstance(module_name, str):\n                    module = import_module(module_name)\n                else:\n                    module = module_name\n                # Get the source code of the module\n                try:\n                    source = inspect.getsource(module)\n                except Exception:\n                    # print(f\"No source for {str(module_name).split('from')[0]}: {e}\")\n                    return module\n                # Compile the source code\n                try:\n                    code = compile(source, module.__file__, 'exec')\n                    # Execute the code in the module's namespace\n                    exec(code, module.__dict__)\n                except Exception:\n                    # print(f\"No source for {str(module_name).split('from')[0]}: {e}\")\n                    pass\n                return module\n\n        if not is_file:\n            mods = self.get_all_mods(\"./mods/\" + mod_name)\n            def recursive_reload(package_name):\n                package = import_module(package_name)\n\n                # First, reload all submodules\n                if hasattr(package, '__path__'):\n                    for _finder, name, _ispkg in pkgutil.walk_packages(package.__path__, package.__name__ + \".\"):\n                        try:\n                            mod = import_module(name)\n                            reexecute_module_code(mod)\n                            reload(mod)\n                        except Exception as e:\n                            print(f\"Error reloading module {name}: {e}\")\n                            break\n\n                # Finally, reload the package itself\n                reexecute_module_code(package)\n                reload(package)\n\n            for mod in mods:\n                if mod.endswith(\".txt\") or mod.endswith(\".yaml\"):\n                    continue\n                try:\n                    recursive_reload(loc + mod_name + '.' + mod)\n                    self.print(f\"Reloaded {mod_name}.{mod}\")\n                except ImportError:\n                    self.print(f\"Could not load {mod_name}.{mod}\")\n        reexecute_module_code(self.modules[mod_name])\n        if mod_name in self.functions:\n            if \"on_exit\" in self.functions[mod_name]:\n                self.functions[mod_name][\"on_exit\"] = []\n            if \"on_start\" in self.functions[mod_name]:\n                self.functions[mod_name][\"on_start\"] = []\n        self.inplace_load_instance(mod_name, spec=spec, mfo=reload(self.modules[mod_name]) if mod_name in self.modules else None)\n\n    def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None, on_reload=None):\n        if path_name is None:\n            path_name = mod_name\n        is_file = os.path.isfile(self.start_dir + '/mods/' + path_name + '.py')\n        import watchfiles\n        def helper():\n            paths = f'mods/{path_name}' + ('.py' if is_file else '')\n            self.print(f'Watching Path: {paths}')\n            for changes in watchfiles.watch(paths):\n                if not changes:\n                    continue\n                self.reload_mod(mod_name, spec, is_file, loc)\n                if on_reload:\n                    on_reload()\n\n        if not use_thread:\n            helper()\n        else:\n            threading.Thread(target=helper, daemon=True).start()\n\n    def _register_function(self, module_name, func_name, data):\n        if module_name not in self.functions:\n            self.functions[module_name] = {}\n        if func_name in self.functions[module_name]:\n            self.print(f\"Overriding function {func_name} from {module_name}\", end=\"\\r\")\n            self.functions[module_name][func_name] = data\n        else:\n            self.functions[module_name][func_name] = data\n\n    def _create_decorator(self, type_: str,\n                          name: str = \"\",\n                          mod_name: str = \"\",\n                          level: int = -1,\n                          restrict_in_virtual_mode: bool = False,\n                          api: bool = False,\n                          helper: str = \"\",\n                          version: str or None = None,\n                          initial: bool=False,\n                          exit_f: bool=False,\n                          test: bool=True,\n                          samples:list[dict[str, Any]] | None=None,\n                          state:bool | None=None,\n                          pre_compute:Callable | None=None,\n                          post_compute:Callable[[], Result] | None=None,\n                          api_methods:list[str] | None=None,\n                          memory_cache: bool=False,\n                          file_cache: bool=False,\n                          request_as_kwarg: bool=False,\n                          row: bool=False,\n                          memory_cache_max_size:int=100,\n                          memory_cache_ttl:int=300):\n\n        if isinstance(type_, Enum):\n            type_ = type_.value\n\n        if memory_cache and file_cache:\n            raise ValueError(\"Don't use both cash at the same time for the same fuction\")\n\n        use_cache = memory_cache or file_cache\n        cache = {}\n        if file_cache:\n            cache = FileCache(folder=self.data_dir + f'\\\\cache\\\\{mod_name}\\\\',\n                              filename=self.data_dir + f'\\\\cache\\\\{mod_name}\\\\{name}cache.db')\n        if memory_cache:\n            cache = MemoryCache(maxsize=memory_cache_max_size, ttl=memory_cache_ttl)\n\n        version = self.version if version is None else self.version + ':' + version\n\n        def a_additional_process(func):\n\n            async def executor(*args, **kwargs):\n\n                if pre_compute is not None:\n                    args, kwargs = await pre_compute(*args, **kwargs)\n                if asyncio.iscoroutinefunction(func):\n                    result = await func(*args, **kwargs)\n                else:\n                    result = func(*args, **kwargs)\n                if post_compute is not None:\n                    result = await post_compute(result)\n                if row:\n                    return result\n                if not isinstance(result, Result):\n                    result = Result.ok(data=result)\n                if result.origin is None:\n                    result.set_origin((mod_name if mod_name else func.__module__.split('.')[-1]\n                                       , name if name else func.__name__\n                                       , type_))\n                if result.result.data_to == ToolBoxInterfaces.native.name:\n                    result.result.data_to = ToolBoxInterfaces.remote if api else ToolBoxInterfaces.native\n                # Wenden Sie die to_api_result Methode auf das Ergebnis an, falls verf\u00fcgbar\n                if api and hasattr(result, 'to_api_result'):\n                    return result.to_api_result()\n                return result\n\n            @wraps(func)\n            async def wrapper(*args, **kwargs):\n\n                if not use_cache:\n                    return await executor(*args, **kwargs)\n\n                try:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{str(args)},{str(kwargs.items())}\")\n                except ValueError:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{bytes(args)},{str(kwargs.items())}\")\n\n                result = cache.get(cache_key)\n                if result is not None:\n                    return result\n\n                result = await executor(*args, **kwargs)\n\n                cache.set(cache_key, result)\n\n                return result\n\n            return wrapper\n\n        def additional_process(func):\n\n            def executor(*args, **kwargs):\n\n                if pre_compute is not None:\n                    args, kwargs = pre_compute(*args, **kwargs)\n                if asyncio.iscoroutinefunction(func):\n                    result = func(*args, **kwargs)\n                else:\n                    result = func(*args, **kwargs)\n                if post_compute is not None:\n                    result = post_compute(result)\n                if row:\n                    return result\n                if not isinstance(result, Result):\n                    result = Result.ok(data=result)\n                if result.origin is None:\n                    result.set_origin((mod_name if mod_name else func.__module__.split('.')[-1]\n                                       , name if name else func.__name__\n                                       , type_))\n                if result.result.data_to == ToolBoxInterfaces.native.name:\n                    result.result.data_to = ToolBoxInterfaces.remote if api else ToolBoxInterfaces.native\n                # Wenden Sie die to_api_result Methode auf das Ergebnis an, falls verf\u00fcgbar\n                if api and hasattr(result, 'to_api_result'):\n                    return result.to_api_result()\n                return result\n\n            @wraps(func)\n            def wrapper(*args, **kwargs):\n\n                if not use_cache:\n                    return executor(*args, **kwargs)\n\n                try:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{str(args)},{str(kwargs.items())}\")\n                except ValueError:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{bytes(args)},{str(kwargs.items())}\")\n\n                result = cache.get(cache_key)\n                if result is not None:\n                    return result\n\n                result = executor(*args, **kwargs)\n\n                cache.set(cache_key, result)\n\n                return result\n\n            return wrapper\n\n        def decorator(func):\n            sig = signature(func)\n            params = list(sig.parameters)\n            module_name = mod_name if mod_name else func.__module__.split('.')[-1]\n            func_name = name if name else func.__name__\n            if func_name == 'on_start':\n                func_name = 'on_startup'\n            if func_name == 'on_exit':\n                func_name = 'on_close'\n            if api or pre_compute is not None or post_compute is not None or memory_cache or file_cache:\n                if asyncio.iscoroutinefunction(func):\n                    func = a_additional_process(func)\n                else:\n                    func = additional_process(func)\n            if api and str(sig.return_annotation) == 'Result':\n                raise ValueError(f\"Fuction {module_name}.{func_name} registered as \"\n                                 f\"Api fuction but uses {str(sig.return_annotation)}\\n\"\n                                 f\"Please change the sig from ..)-&gt; Result to ..)-&gt; ApiResult\")\n            data = {\n                \"type\": type_,\n                \"module_name\": module_name,\n                \"func_name\": func_name,\n                \"level\": level,\n                \"restrict_in_virtual_mode\": restrict_in_virtual_mode,\n                \"func\": func,\n                \"api\": api,\n                \"helper\": helper,\n                \"version\": version,\n                \"initial\": initial,\n                \"exit_f\": exit_f,\n                \"api_methods\": api_methods if api_methods is not None else [\"AUTO\"],\n                \"__module__\": func.__module__,\n                \"signature\": sig,\n                \"params\": params,\n                \"row\": row,\n                \"state\": (\n                    False if len(params) == 0 else params[0] in ['self', 'state', 'app']) if state is None else state,\n                \"do_test\": test,\n                \"samples\": samples,\n                \"request_as_kwarg\": request_as_kwarg,\n\n            }\n            self._register_function(module_name, func_name, data)\n            if exit_f:\n                if \"on_exit\" not in self.functions[module_name]:\n                    self.functions[module_name][\"on_exit\"] = []\n                self.functions[module_name][\"on_exit\"].append(func_name)\n            if initial:\n                if \"on_start\" not in self.functions[module_name]:\n                    self.functions[module_name][\"on_start\"] = []\n                self.functions[module_name][\"on_start\"].append(func_name)\n\n            return func\n\n        decorator.tb_init = True\n\n        return decorator\n\n    def tb(self, name=None,\n           mod_name: str = \"\",\n           helper: str = \"\",\n           version: str | None = None,\n           test: bool = True,\n           restrict_in_virtual_mode: bool = False,\n           api: bool = False,\n           initial: bool = False,\n           exit_f: bool = False,\n           test_only: bool = False,\n           memory_cache: bool = False,\n           file_cache: bool = False,\n           request_as_kwarg: bool = False,\n           row: bool = False,\n           state: bool | None = None,\n           level: int = -1,\n           memory_cache_max_size: int = 100,\n           memory_cache_ttl: int = 300,\n           samples: list or dict or None = None,\n           interface: ToolBoxInterfaces or None or str = None,\n           pre_compute=None,\n           post_compute=None,\n           api_methods=None,\n           ):\n        \"\"\"\n    A decorator for registering and configuring functions within a module.\n\n    This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\n    Args:\n        name (str, optional): The name to register the function under. Defaults to the function's own name.\n        mod_name (str, optional): The name of the module the function belongs to.\n        helper (str, optional): A helper string providing additional information about the function.\n        version (str or None, optional): The version of the function or module.\n        test (bool, optional): Flag to indicate if the function is for testing purposes.\n        restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n        api (bool, optional): Flag to indicate if the function is part of an API.\n        initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n        exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n        test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n        memory_cache (bool, optional): Flag to enable memory caching for the function.\n        request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n        file_cache (bool, optional): Flag to enable file caching for the function.\n        row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n        state (bool or None, optional): Flag to indicate if the function maintains state.\n        level (int, optional): The level of the function, used for prioritization or categorization.\n        memory_cache_max_size (int, optional): Maximum size of the memory cache.\n        memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n        samples (list or dict or None, optional): Samples or examples of function usage.\n        interface (str, optional): The interface type for the function.\n        pre_compute (callable, optional): A function to be called before the main function.\n        post_compute (callable, optional): A function to be called after the main function.\n        api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\n    Returns:\n        function: The decorated function with additional processing and registration capabilities.\n    \"\"\"\n        if interface is None:\n            interface = \"tb\"\n        if test_only and 'test' not in self.id:\n            return lambda *args, **kwargs: args\n        return self._create_decorator(interface,\n                                      name,\n                                      mod_name,\n                                      level=level,\n                                      restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                      helper=helper,\n                                      api=api,\n                                      version=version,\n                                      initial=initial,\n                                      exit_f=exit_f,\n                                      test=test,\n                                      samples=samples,\n                                      state=state,\n                                      pre_compute=pre_compute,\n                                      post_compute=post_compute,\n                                      memory_cache=memory_cache,\n                                      file_cache=file_cache,\n                                      request_as_kwarg=request_as_kwarg,\n                                      row=row,\n                                      api_methods=api_methods,\n                                      memory_cache_max_size=memory_cache_max_size,\n                                      memory_cache_ttl=memory_cache_ttl)\n\n    def save_autocompletion_dict(self):\n        autocompletion_dict = {}\n        for module_name, _module in self.functions.items():\n            data = {}\n            for function_name, function_data in self.functions[module_name].items():\n                if not isinstance(function_data, dict):\n                    continue\n                data[function_name] = {arg: None for arg in\n                                       function_data.get(\"params\", [])}\n                if len(data[function_name].keys()) == 0:\n                    data[function_name] = None\n            autocompletion_dict[module_name] = data if len(data.keys()) &gt; 0 else None\n        self.config_fh.add_to_save_file_handler(\"auto~~~~~~\", str(autocompletion_dict))\n\n    def get_autocompletion_dict(self):\n        return self.config_fh.get_file_handler(\"auto~~~~~~\")\n\n    def save_registry_as_enums(self, directory: str, filename: str):\n        # Ordner erstellen, falls nicht vorhanden\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n        # Dateipfad vorbereiten\n        filepath = os.path.join(directory, filename)\n\n        # Enum-Klassen als Strings generieren\n        enum_classes = [f'\"\"\"Automatic generated by ToolBox v = {self.version}\"\"\"'\n                        f'\\nfrom enum import Enum\\nfrom dataclasses import dataclass'\n                        f'\\n\\n\\n']\n        for module, functions in self.functions.items():\n            if module.startswith(\"APP_INSTANCE\"):\n                continue\n            class_name = module\n            enum_members = \"\\n    \".join(\n                [\n                    f\"{func_name.upper().replace('-', '')}\"\n                    f\" = '{func_name}' \"\n                    f\"# Input: ({fuction_data['params'] if isinstance(fuction_data, dict) else ''}),\"\n                    f\" Output: {fuction_data['signature'].return_annotation if isinstance(fuction_data, dict) else 'None'}\"\n                    for func_name, fuction_data in functions.items()])\n            enum_class = (f'@dataclass\\nclass {class_name.upper().replace(\".\", \"_\").replace(\"-\", \"\")}(Enum):'\n                          f\"\\n    NAME = '{class_name}'\\n    {enum_members}\")\n            enum_classes.append(enum_class)\n\n        # Enums in die Datei schreiben\n        data = \"\\n\\n\\n\".join(enum_classes)\n        if len(data) &lt; 12:\n            raise ValueError(\n                \"Invalid Enums Loosing content pleas delete it ur self in the (utils/system/all_functions_enums.py) or add mor new stuff :}\")\n        with open(filepath, 'w') as file:\n            file.write(data)\n\n        print(Style.Bold(Style.BLUE(f\"Enums gespeichert in {filepath}\")))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.disconnect","title":"<code>disconnect(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef disconnect(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.exit_main","title":"<code>exit_main(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef exit_main(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.get_function","title":"<code>get_function(name, **kwargs)</code>","text":"<p>Kwargs for _get_function     metadata:: return the registered function dictionary         stateless: (function_data, None), 0         stateful: (function_data, higher_order_function), 0     state::boolean         specification::str default app</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def get_function(self, name: Enum or tuple, **kwargs):\n    \"\"\"\n    Kwargs for _get_function\n        metadata:: return the registered function dictionary\n            stateless: (function_data, None), 0\n            stateful: (function_data, higher_order_function), 0\n        state::boolean\n            specification::str default app\n    \"\"\"\n    if isinstance(name, tuple):\n        return self._get_function(None, as_str=name, **kwargs)\n    else:\n        return self._get_function(name, **kwargs)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.hide_console","title":"<code>hide_console(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef hide_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.run","title":"<code>run(*args, request=None, running_function_coro=None, **kwargs)</code>","text":"<p>Run a function with support for SSE streaming in both threaded and non-threaded contexts.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def run(self, *args, request=None, running_function_coro=None, **kwargs):\n    \"\"\"\n    Run a function with support for SSE streaming in both\n    threaded and non-threaded contexts.\n    \"\"\"\n    if running_function_coro is None:\n        mn, fn = args[0]\n        if self.functions.get(mn, {}).get(fn, {}).get('request_as_kwarg', False):\n            kwargs[\"request\"] = request\n            if 'data' in kwargs and 'data' not in self.functions.get(mn, {}).get(fn, {}).get('params', []):\n                kwargs[\"request\"]['data'] = kwargs['data']\n                del kwargs['data']\n            if 'form_data' in kwargs and 'form_data' not in self.functions.get(mn, {}).get(fn, {}).get('params',\n                                                                                                       []):\n                kwargs[\"request\"]['form_data'] = kwargs['form_data']\n                del kwargs['form_data']\n            kwargs[\"request\"] = RequestData.from_dict(request)\n\n    # Create the coroutine\n    coro = running_function_coro or self.a_run_any(*args, **kwargs)\n\n    # Get or create an event loop\n    try:\n        loop = asyncio.get_event_loop()\n        is_running = loop.is_running()\n    except RuntimeError:\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        is_running = False\n\n    # If the loop is already running, run in a separate thread\n    if is_running:\n        # Create thread pool executor as needed\n        if not hasattr(self.__class__, '_executor'):\n            self.__class__._executor = ThreadPoolExecutor(max_workers=4)\n\n        def run_in_new_thread():\n            # Set up a new loop in this thread\n            new_loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(new_loop)\n\n            try:\n                # Run the coroutine\n                return new_loop.run_until_complete(coro)\n            finally:\n                new_loop.close()\n\n        # Run in thread and get result\n        thread_result = self.__class__._executor.submit(run_in_new_thread).result()\n\n        # Handle streaming results from thread\n        if isinstance(thread_result, dict) and thread_result.get(\"is_stream\"):\n            # Create a new SSE stream in the main thread\n            async def stream_from_function():\n                # Re-run the function with direct async access\n                stream_result = await self.a_run_any(*args, **kwargs)\n\n                if (isinstance(stream_result, Result) and\n                    getattr(stream_result.result, 'data_type', None) == \"stream\"):\n                    # Get and forward data from the original generator\n                    original_gen = stream_result.result.data.get(\"generator\")\n                    if inspect.isasyncgen(original_gen):\n                        async for item in original_gen:\n                            yield item\n\n            # Return a new streaming Result\n            return Result.stream(\n                stream_generator=stream_from_function(),\n                headers=thread_result.get(\"headers\", {})\n            )\n\n        result = thread_result\n    else:\n        # Direct execution when loop is not running\n        result = loop.run_until_complete(coro)\n\n    # Process the final result\n    if isinstance(result, Result):\n        result.print()\n        if getattr(result.result, 'data_type', None) == \"stream\":\n            return result\n        return result.to_api_result().model_dump(mode='json')\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.run_bg_task","title":"<code>run_bg_task(task)</code>","text":"<p>Run a task in the background that will properly handle nested asyncio operations. This implementation ensures that asyncio.create_task() and asyncio.gather() work correctly within the background task.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <p>A callable function that can be synchronous or asynchronous</p> required Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def run_bg_task(self, task):\n    \"\"\"\n    Run a task in the background that will properly handle nested asyncio operations.\n    This implementation ensures that asyncio.create_task() and asyncio.gather() work\n    correctly within the background task.\n\n    Args:\n        task: A callable function that can be synchronous or asynchronous\n    \"\"\"\n    if not callable(task):\n        self.logger.warning(\"Task is not callable!\")\n        return None\n\n    # Function that will run in a separate thread with its own event loop\n    def thread_target(task_):\n        # Create a new event loop for this thread\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n\n        try:\n            # Determine how to run the task based on its type\n            if asyncio.iscoroutinefunction(task_):\n                # If it's an async function, run it directly\n                loop.run_until_complete(task_())\n            elif asyncio.iscoroutine(task_):\n                # If it's already a coroutine object\n                loop.run_until_complete(task_)\n            else:\n                # If it's a synchronous function that might create async tasks internally\n                async def wrapper():\n                    # Run potentially blocking synchronous code in an executor\n                    return await loop.run_in_executor(None, task_)\n\n                loop.run_until_complete(wrapper())\n\n            self.logger.debug(\"Background task completed successfully\")\n        except Exception as e:\n            self.logger.error(f\"Background task failed with error: {str(e)}\")\n        finally:\n            # Clean up any pending tasks\n            pending = asyncio.all_tasks(loop)\n            if pending:\n                # Cancel any remaining tasks\n                for task_ in pending:\n                    task_.cancel()\n\n                # Allow tasks to finish cancellation\n                loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))\n\n            loop.close()\n\n    # Create and start a non-daemon thread that will run to completion\n    # Using non-daemon thread ensures the task completes even if main thread exits\n    t = threading.Thread(target=thread_target, args=(task,))\n    t.daemon = False  # Non-daemon thread will keep program alive until it completes\n    self.bg_tasks.append(t)\n    t.start()\n    return t\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.run_bg_task_advanced","title":"<code>run_bg_task_advanced(task, *args, **kwargs)</code>","text":"<p>Alternative implementation for complex async scenarios where the task creates nested asyncio tasks using create_task() and gather().</p> <p>This version ensures proper execution of nested tasks by maintaining the thread and its event loop throughout the lifetime of all child tasks.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <p>A callable function that can be synchronous or asynchronous</p> required <code>*args,</code> <code>**kwargs</code> <p>Arguments to pass to the task</p> required Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def run_bg_task_advanced(self, task, *args, **kwargs):\n    \"\"\"\n    Alternative implementation for complex async scenarios where the task creates\n    nested asyncio tasks using create_task() and gather().\n\n    This version ensures proper execution of nested tasks by maintaining the thread\n    and its event loop throughout the lifetime of all child tasks.\n\n    Args:\n        task: A callable function that can be synchronous or asynchronous\n        *args, **kwargs: Arguments to pass to the task\n    \"\"\"\n    if not callable(task):\n        self.logger.warning(\"Task is not callable!\")\n        return None\n\n    # Create a dedicated thread with its own event loop\n    async def async_wrapper():\n        try:\n            if asyncio.iscoroutinefunction(task):\n                return await task(*args, **kwargs)\n            elif asyncio.iscoroutine(task):\n                return await task\n            else:\n                # Run in executor to avoid blocking\n                loop = asyncio.get_event_loop()\n                return await loop.run_in_executor(None, lambda: task(*args, **kwargs))\n        except Exception as e:\n            self.logger.error(f\"Background task error: {str(e)}\")\n            raise\n\n    def thread_target():\n        # Create new event loop for this thread\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n\n        try:\n            # Run the task to completion with all its nested tasks\n            loop.run_until_complete(async_wrapper())\n        except Exception as e:\n            self.logger.error(f\"Background task thread failed: {str(e)}\")\n        finally:\n            # Clean up any pending tasks that might still be running\n            try:\n                pending = asyncio.all_tasks(loop)\n                if pending:\n                    # Allow tasks time to clean up\n                    loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))\n            except Exception:\n                pass\n\n            loop.close()\n\n    # Use a non-daemon thread so it will run to completion\n    t = threading.Thread(target=thread_target, daemon=True)\n    t.daemon = False\n    self.bg_tasks.append(t)\n    t.start()\n    return t\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.show_console","title":"<code>show_console(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef show_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.tb","title":"<code>tb(name=None, mod_name='', helper='', version=None, test=True, restrict_in_virtual_mode=False, api=False, initial=False, exit_f=False, test_only=False, memory_cache=False, file_cache=False, request_as_kwarg=False, row=False, state=None, level=-1, memory_cache_max_size=100, memory_cache_ttl=300, samples=None, interface=None, pre_compute=None, post_compute=None, api_methods=None)</code>","text":"<p>A decorator for registering and configuring functions within a module.</p> <p>This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to register the function under. Defaults to the function's own name.</p> <code>None</code> <code>mod_name</code> <code>str</code> <p>The name of the module the function belongs to.</p> <code>''</code> <code>helper</code> <code>str</code> <p>A helper string providing additional information about the function.</p> <code>''</code> <code>version</code> <code>str or None</code> <p>The version of the function or module.</p> <code>None</code> <code>test</code> <code>bool</code> <p>Flag to indicate if the function is for testing purposes.</p> <code>True</code> <code>restrict_in_virtual_mode</code> <code>bool</code> <p>Flag to restrict the function in virtual mode.</p> <code>False</code> <code>api</code> <code>bool</code> <p>Flag to indicate if the function is part of an API.</p> <code>False</code> <code>initial</code> <code>bool</code> <p>Flag to indicate if the function should be executed at initialization.</p> <code>False</code> <code>exit_f</code> <code>bool</code> <p>Flag to indicate if the function should be executed at exit.</p> <code>False</code> <code>test_only</code> <code>bool</code> <p>Flag to indicate if the function should only be used for testing.</p> <code>False</code> <code>memory_cache</code> <code>bool</code> <p>Flag to enable memory caching for the function.</p> <code>False</code> <code>request_as_kwarg</code> <code>bool</code> <p>Flag to get request if the fuction is calld from api.</p> <code>False</code> <code>file_cache</code> <code>bool</code> <p>Flag to enable file caching for the function.</p> <code>False</code> <code>row</code> <code>bool</code> <p>rather to auto wrap the result in Result type default False means no row data aka result type</p> <code>False</code> <code>state</code> <code>bool or None</code> <p>Flag to indicate if the function maintains state.</p> <code>None</code> <code>level</code> <code>int</code> <p>The level of the function, used for prioritization or categorization.</p> <code>-1</code> <code>memory_cache_max_size</code> <code>int</code> <p>Maximum size of the memory cache.</p> <code>100</code> <code>memory_cache_ttl</code> <code>int</code> <p>Time-to-live for the memory cache entries.</p> <code>300</code> <code>samples</code> <code>list or dict or None</code> <p>Samples or examples of function usage.</p> <code>None</code> <code>interface</code> <code>str</code> <p>The interface type for the function.</p> <code>None</code> <code>pre_compute</code> <code>callable</code> <p>A function to be called before the main function.</p> <code>None</code> <code>post_compute</code> <code>callable</code> <p>A function to be called after the main function.</p> <code>None</code> <code>api_methods</code> <code>list[str]</code> <p>default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>function</code> <p>The decorated function with additional processing and registration capabilities.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def tb(self, name=None,\n       mod_name: str = \"\",\n       helper: str = \"\",\n       version: str | None = None,\n       test: bool = True,\n       restrict_in_virtual_mode: bool = False,\n       api: bool = False,\n       initial: bool = False,\n       exit_f: bool = False,\n       test_only: bool = False,\n       memory_cache: bool = False,\n       file_cache: bool = False,\n       request_as_kwarg: bool = False,\n       row: bool = False,\n       state: bool | None = None,\n       level: int = -1,\n       memory_cache_max_size: int = 100,\n       memory_cache_ttl: int = 300,\n       samples: list or dict or None = None,\n       interface: ToolBoxInterfaces or None or str = None,\n       pre_compute=None,\n       post_compute=None,\n       api_methods=None,\n       ):\n    \"\"\"\nA decorator for registering and configuring functions within a module.\n\nThis decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\nArgs:\n    name (str, optional): The name to register the function under. Defaults to the function's own name.\n    mod_name (str, optional): The name of the module the function belongs to.\n    helper (str, optional): A helper string providing additional information about the function.\n    version (str or None, optional): The version of the function or module.\n    test (bool, optional): Flag to indicate if the function is for testing purposes.\n    restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n    api (bool, optional): Flag to indicate if the function is part of an API.\n    initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n    exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n    test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n    memory_cache (bool, optional): Flag to enable memory caching for the function.\n    request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n    file_cache (bool, optional): Flag to enable file caching for the function.\n    row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n    state (bool or None, optional): Flag to indicate if the function maintains state.\n    level (int, optional): The level of the function, used for prioritization or categorization.\n    memory_cache_max_size (int, optional): Maximum size of the memory cache.\n    memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n    samples (list or dict or None, optional): Samples or examples of function usage.\n    interface (str, optional): The interface type for the function.\n    pre_compute (callable, optional): A function to be called before the main function.\n    post_compute (callable, optional): A function to be called after the main function.\n    api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\nReturns:\n    function: The decorated function with additional processing and registration capabilities.\n\"\"\"\n    if interface is None:\n        interface = \"tb\"\n    if test_only and 'test' not in self.id:\n        return lambda *args, **kwargs: args\n    return self._create_decorator(interface,\n                                  name,\n                                  mod_name,\n                                  level=level,\n                                  restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                  helper=helper,\n                                  api=api,\n                                  version=version,\n                                  initial=initial,\n                                  exit_f=exit_f,\n                                  test=test,\n                                  samples=samples,\n                                  state=state,\n                                  pre_compute=pre_compute,\n                                  post_compute=post_compute,\n                                  memory_cache=memory_cache,\n                                  file_cache=file_cache,\n                                  request_as_kwarg=request_as_kwarg,\n                                  row=row,\n                                  api_methods=api_methods,\n                                  memory_cache_max_size=memory_cache_max_size,\n                                  memory_cache_ttl=memory_cache_ttl)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.wait_for_bg_tasks","title":"<code>wait_for_bg_tasks(timeout=None)</code>","text":"<p>Wait for all background tasks to complete.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <p>Maximum time to wait (in seconds) for all tasks to complete.      None means wait indefinitely.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if all tasks completed, False if timeout occurred</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def wait_for_bg_tasks(self, timeout=None):\n    \"\"\"\n    Wait for all background tasks to complete.\n\n    Args:\n        timeout: Maximum time to wait (in seconds) for all tasks to complete.\n                 None means wait indefinitely.\n\n    Returns:\n        bool: True if all tasks completed, False if timeout occurred\n    \"\"\"\n    active_tasks = [t for t in self.bg_tasks if t.is_alive()]\n\n    for task in active_tasks:\n        task.join(timeout=timeout)\n        if task.is_alive():\n            return False\n\n    return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.show_console","title":"<code>toolboxv2.show_console(show=True)</code>","text":"Source code in <code>toolboxv2/utils/extras/show_and_hide_console.py</code> <pre><code>def show_console(show=True):\n    global TBRUNNER_console_viabel\n    \"\"\"Brings up the Console Window.\"\"\"\n    try:\n        if show and not TBRUNNER_console_viabel:\n            # Show console\n            ctypes.windll.user32.ShowWindow(ctypes.windll.kernel32.GetConsoleWindow(), 4)\n            TBRUNNER_console_viabel = True\n            return True\n        elif not show and TBRUNNER_console_viabel:\n            # Hide console\n            ctypes.windll.user32.ShowWindow(ctypes.windll.kernel32.GetConsoleWindow(), 0)\n            TBRUNNER_console_viabel = False\n            return True\n    except:\n        print(f\"Could not show_console {show=}\", )\n        return False\n    return False\n</code></pre>"},{"location":"toolboxv2/#logging","title":"Logging","text":""},{"location":"toolboxv2/#toolboxv2.get_logger","title":"<code>toolboxv2.get_logger()</code>","text":"Source code in <code>toolboxv2/utils/system/tb_logger.py</code> <pre><code>def get_logger() -&gt; logging.Logger:\n    return logging.getLogger(loggerNameOfToolboxv2)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.setup_logging","title":"<code>toolboxv2.setup_logging(level, name=loggerNameOfToolboxv2, online_level=None, is_online=False, file_level=None, interminal=False, logs_directory='../logs', app_name='main')</code>","text":"Source code in <code>toolboxv2/utils/system/tb_logger.py</code> <pre><code>def setup_logging(level: int, name=loggerNameOfToolboxv2, online_level=None, is_online=False, file_level=None,\n                  interminal=False, logs_directory=\"../logs\", app_name=\"main\"):\n    global loggerNameOfToolboxv2\n\n    if not online_level:\n        online_level = level\n\n    if not file_level:\n        file_level = level\n\n    if not os.path.exists(logs_directory):\n        os.makedirs(logs_directory, exist_ok=True)\n    if not os.path.exists(logs_directory + \"/Logs.info\"):\n        open(f\"{logs_directory}/Logs.info\", \"a\").close()\n\n    loggerNameOfToolboxv2 = name\n\n    available_log_levels = [logging.CRITICAL, logging.FATAL, logging.ERROR, logging.WARNING, logging.WARN, logging.INFO,\n                            logging.DEBUG, logging.NOTSET]\n\n    if level not in available_log_levels:\n        raise ValueError(f\"level must be one of {available_log_levels}, but logging level is {level}\")\n\n    if online_level not in available_log_levels:\n        raise ValueError(f\"online_level must be one of {available_log_levels}, but logging level is {online_level}\")\n\n    if file_level not in available_log_levels:\n        raise ValueError(f\"file_level must be one of {available_log_levels}, but logging level is {file_level}\")\n\n    log_date = datetime.datetime.today().strftime('%Y-%m-%d')\n    log_levels = [\"CRITICAL\", \"ERROR\", \"WARNING\", \"INFO\", \"DEBUG\", \"NOTSET\"]\n    log_level_index = log_levels.index(logging.getLevelName(level))\n\n    filename = f\"Logs-{name}-{log_date}-{log_levels[log_level_index]}\"\n    log_filename = f\"{logs_directory}/{filename}.log\"\n\n    log_info_data = {\n        filename: 0,\n        \"H\": \"localhost\",\n        \"P\": 62435\n    }\n\n    with open(f\"{logs_directory}/Logs.info\") as li:\n        log_info_data_str = li.read()\n        try:\n            log_info_data = eval(log_info_data_str)\n        except SyntaxError:\n            if log_info_data_str:\n                print(Style.RED(Style.Bold(\"Could not parse log info data\")))\n\n        if filename not in log_info_data:\n            log_info_data[filename] = 0\n\n        if not os.path.exists(log_filename):\n            log_info_data[filename] = 0\n            print(\"new log file\")\n\n        if os.path.exists(log_filename):\n            log_info_data[filename] += 1\n\n            while os.path.exists(f\"{logs_directory}/{filename}#{log_info_data[filename]}.log\"):\n                log_info_data[filename] += 1\n\n            try:\n                os.rename(log_filename,\n                          f\"{logs_directory}/{filename}#{log_info_data[filename]}.log\")\n            except PermissionError:\n                print(Style.YELLOW(Style.Bold(f\"Could not rename log file appending on {filename}\")))\n\n    with open(f\"{logs_directory}/Logs.info\", \"w\") as li:\n        if len(log_info_data.keys()) &gt;= 7:\n            log_info_data = {\n                filename: log_info_data[filename],\n                \"H\": log_info_data[\"H\"],\n                \"P\": log_info_data[\"P\"]\n            }\n        li.write(str(log_info_data))\n\n    try:\n        with open(log_filename, \"a\"):\n            pass\n    except OSError:\n        log_filename = f\"{logs_directory}/Logs-Test-{log_date}-{log_levels[log_level_index]}.log\"\n        with open(log_filename, \"a\"):\n            pass\n\n    logger = logging.getLogger(name)\n\n    logger.setLevel(level)\n    # Prevent logger from propagating to parent loggers\n    logger.propagate = False\n\n    terminal_format = f\"{app_name} %(asctime)s %(levelname)s %(name)s - %(message)s\"\n    file_format = f\"{app_name} %(asctime)s - %(name)s - %(levelname)s - %(filename)s - %(funcName)s:%(lineno)d - %(message)s\"\n\n    # Configure handlers\n    handlers = []\n\n    # File handler (always added)\n    file_handler = logging.FileHandler(log_filename)\n    file_handler.setFormatter(logging.Formatter(file_format))\n    file_handler.setLevel(file_level)\n    handlers.append(file_handler)\n\n    # Terminal handler (if requested)\n    if interminal:\n        terminal_handler = logging.StreamHandler()\n        terminal_handler.setFormatter(logging.Formatter(terminal_format))\n        terminal_handler.setLevel(level)\n        handlers.append(terminal_handler)\n\n    # Socket handler (if requested)\n    if is_online:\n        socket_handler = SocketHandler(log_info_data[\"H\"], log_info_data[\"P\"])\n        socket_handler.setFormatter(logging.Formatter(file_format))\n        socket_handler.setLevel(online_level)\n        handlers.append(socket_handler)\n\n    # Add all handlers to logger\n    for handler in handlers:\n        logger.addHandler(handler)\n\n    return logger, filename\n</code></pre>"},{"location":"toolboxv2/#styling-console-output","title":"Styling &amp; Console Output","text":""},{"location":"toolboxv2/#toolboxv2.Style","title":"<code>toolboxv2.Style</code>","text":"Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>class Style:\n    _END = '\\33[0m'\n    _BLACK = '\\33[30m'\n    _RED = '\\33[31m'\n    _GREEN = '\\33[32m'\n    _YELLOW = '\\33[33m'\n    _BLUE = '\\33[34m'\n    _MAGENTA = '\\33[35m'\n    _CYAN = '\\33[36m'\n    _WHITE = '\\33[37m'\n\n    _Bold = '\\33[1m'\n    _ITALIC = '\\33[3m'\n    _Underline = '\\33[4m'\n    _BLINK = '\\33[5m'\n    _BLINK2 = '\\33[6m'\n    _Reversed = '\\33[7m'\n\n    _BLACKBG = '\\33[40m'\n    _REDBG = '\\33[41m'\n    _GREENBG = '\\33[42m'\n    _YELLOWBG = '\\33[43m'\n    _BLUEBG = '\\33[44m'\n    _VIOLETBG = '\\33[45m'\n    _BEIGEBG = '\\33[46m'\n    _WHITEBG = '\\33[47m'\n\n    _GREY = '\\33[90m'\n    _RED2 = '\\33[91m'\n    _GREEN2 = '\\33[92m'\n    _YELLOW2 = '\\33[93m'\n    _BLUE2 = '\\33[94m'\n    _VIOLET2 = '\\33[95m'\n    _BEIGE2 = '\\33[96m'\n    _WHITE2 = '\\33[97m'\n\n    _GREYBG = '\\33[100m'\n    _REDBG2 = '\\33[101m'\n    _GREENBG2 = '\\33[102m'\n    _YELLOWBG2 = '\\33[103m'\n    _BLUEBG2 = '\\33[104m'\n    _VIOLETBG2 = '\\33[105m'\n    _BEIGEBG2 = '\\33[106m'\n    _WHITEBG2 = '\\33[107m'\n\n    style_dic = {\n        \"END\": _END,\n        \"BLACK\": _BLACK,\n        \"RED\": _RED,\n        \"GREEN\": _GREEN,\n        \"YELLOW\": _YELLOW,\n        \"BLUE\": _BLUE,\n        \"MAGENTA\": _MAGENTA,\n        \"CYAN\": _CYAN,\n        \"WHITE\": _WHITE,\n        \"Bold\": _Bold,\n        \"Underline\": _Underline,\n        \"Reversed\": _Reversed,\n\n        \"ITALIC\": _ITALIC,\n        \"BLINK\": _BLINK,\n        \"BLINK2\": _BLINK2,\n        \"BLACKBG\": _BLACKBG,\n        \"REDBG\": _REDBG,\n        \"GREENBG\": _GREENBG,\n        \"YELLOWBG\": _YELLOWBG,\n        \"BLUEBG\": _BLUEBG,\n        \"VIOLETBG\": _VIOLETBG,\n        \"BEIGEBG\": _BEIGEBG,\n        \"WHITEBG\": _WHITEBG,\n        \"GREY\": _GREY,\n        \"RED2\": _RED2,\n        \"GREEN2\": _GREEN2,\n        \"YELLOW2\": _YELLOW2,\n        \"BLUE2\": _BLUE2,\n        \"VIOLET2\": _VIOLET2,\n        \"BEIGE2\": _BEIGE2,\n        \"WHITE2\": _WHITE2,\n        \"GREYBG\": _GREYBG,\n        \"REDBG2\": _REDBG2,\n        \"GREENBG2\": _GREENBG2,\n        \"YELLOWBG2\": _YELLOWBG2,\n        \"BLUEBG2\": _BLUEBG2,\n        \"VIOLETBG2\": _VIOLETBG2,\n        \"BEIGEBG2\": _BEIGEBG2,\n        \"WHITEBG2\": _WHITEBG2,\n\n    }\n\n    @staticmethod\n    def END_():\n        print(Style._END)\n\n    @staticmethod\n    def GREEN_():\n        print(Style._GREEN)\n\n    @staticmethod\n    def BLUE(text: str):\n        return Style._BLUE + text + Style._END\n\n    @staticmethod\n    def BLACK(text: str):\n        return Style._BLACK + text + Style._END\n\n    @staticmethod\n    def RED(text: str):\n        return Style._RED + text + Style._END\n\n    @staticmethod\n    def GREEN(text: str):\n        return Style._GREEN + text + Style._END\n\n    @staticmethod\n    def YELLOW(text: str):\n        return Style._YELLOW + text + Style._END\n\n    @staticmethod\n    def MAGENTA(text: str):\n        return Style._MAGENTA + text + Style._END\n\n    @staticmethod\n    def CYAN(text: str):\n        return Style._CYAN + text + Style._END\n\n    @staticmethod\n    def WHITE(text: str):\n        return Style._WHITE + text + Style._END\n\n    @staticmethod\n    def Bold(text: str):\n        return Style._Bold + text + Style._END\n\n    @staticmethod\n    def Underline(text: str):\n        return Style._Underline + text + Style._END\n\n    @staticmethod\n    def Reversed(text: str):\n        return Style._Reversed + text + Style._END\n\n    @staticmethod\n    def ITALIC(text: str):\n        return Style._ITALIC + text + Style._END\n\n    @staticmethod\n    def BLINK(text: str):\n        return Style._BLINK + text + Style._END\n\n    @staticmethod\n    def BLINK2(text: str):\n        return Style._BLINK2 + text + Style._END\n\n    @staticmethod\n    def BLACKBG(text: str):\n        return Style._BLACKBG + text + Style._END\n\n    @staticmethod\n    def REDBG(text: str):\n        return Style._REDBG + text + Style._END\n\n    @staticmethod\n    def GREENBG(text: str):\n        return Style._GREENBG + text + Style._END\n\n    @staticmethod\n    def YELLOWBG(text: str):\n        return Style._YELLOWBG + text + Style._END\n\n    @staticmethod\n    def BLUEBG(text: str):\n        return Style._BLUEBG + text + Style._END\n\n    @staticmethod\n    def VIOLETBG(text: str):\n        return Style._VIOLETBG + text + Style._END\n\n    @staticmethod\n    def BEIGEBG(text: str):\n        return Style._BEIGEBG + text + Style._END\n\n    @staticmethod\n    def WHITEBG(text: str):\n        return Style._WHITEBG + text + Style._END\n\n    @staticmethod\n    def GREY(text: str):\n        return Style._GREY + text + Style._END\n\n    @staticmethod\n    def RED2(text: str):\n        return Style._RED2 + text + Style._END\n\n    @staticmethod\n    def GREEN2(text: str):\n        return Style._GREEN2 + text + Style._END\n\n    @staticmethod\n    def YELLOW2(text: str):\n        return Style._YELLOW2 + text + Style._END\n\n    @staticmethod\n    def BLUE2(text: str):\n        return Style._BLUE2 + text + Style._END\n\n    @staticmethod\n    def VIOLET2(text: str):\n        return Style._VIOLET2 + text + Style._END\n\n    @staticmethod\n    def BEIGE2(text: str):\n        return Style._BEIGE2 + text + Style._END\n\n    @staticmethod\n    def WHITE2(text: str):\n        return Style._WHITE2 + text + Style._END\n\n    @staticmethod\n    def GREYBG(text: str):\n        return Style._GREYBG + text + Style._END\n\n    @staticmethod\n    def REDBG2(text: str):\n        return Style._REDBG2 + text + Style._END\n\n    @staticmethod\n    def GREENBG2(text: str):\n        return Style._GREENBG2 + text + Style._END\n\n    @staticmethod\n    def YELLOWBG2(text: str):\n        return Style._YELLOWBG2 + text + Style._END\n\n    @staticmethod\n    def BLUEBG2(text: str):\n        return Style._BLUEBG2 + text + Style._END\n\n    @staticmethod\n    def VIOLETBG2(text: str):\n        return Style._VIOLETBG2 + text + Style._END\n\n    @staticmethod\n    def BEIGEBG2(text: str):\n        return Style._BEIGEBG2 + text + Style._END\n\n    @staticmethod\n    def WHITEBG2(text: str):\n        return Style._WHITEBG2 + text + Style._END\n\n    @staticmethod\n    def loading_al(text: str):\n        b = f\"{text} /\"\n        print(b)\n        sleep(0.05)\n        cls()\n        b = f\"{text} -\"\n        print(b)\n        sleep(0.05)\n        cls()\n        b = f\"{text} \\\\\"\n        print(b)\n        sleep(0.05)\n        cls()\n        b = f\"{text} |\"\n        print(b)\n        sleep(0.05)\n        cls()\n\n    @property\n    def END(self):\n        return self._END\n\n    def color_demo(self):\n        for color in self.style_dic:\n            print(f\"{color} -&gt; {self.style_dic[color]}Effect{self._END}\")\n\n    @property\n    def Underline2(self):\n        return self._Underline\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Spinner","title":"<code>toolboxv2.Spinner</code>","text":"<p>Enhanced Spinner with tqdm-like line rendering.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>class Spinner:\n    \"\"\"\n    Enhanced Spinner with tqdm-like line rendering.\n    \"\"\"\n    SYMBOL_SETS = {\n        \"c\": [\"\u25d0\", \"\u25d3\", \"\u25d1\", \"\u25d2\"],\n        \"b\": [\"\u2581\", \"\u2583\", \"\u2584\", \"\u2585\", \"\u2586\", \"\u2587\", \"\u2588\", \"\u2587\", \"\u2586\", \"\u2585\", \"\u2584\", \"\u2583\"],\n        \"d\": [\"\u28fe\", \"\u28fd\", \"\u28fb\", \"\u28bf\", \"\u287f\", \"\u28df\", \"\u28ef\", \"\u28f7\"],\n        \"w\": [\"\ud83c\udf0d\", \"\ud83c\udf0e\", \"\ud83c\udf0f\"],\n        \"s\": [\"\ud83c\udf00   \", \" \ud83c\udf00  \", \"  \ud83c\udf00 \", \"   \ud83c\udf00\", \"  \ud83c\udf00 \", \" \ud83c\udf00  \"],\n        \"+\": [\"+\", \"x\"],\n        \"t\": [\"\u2736\", \"\u2738\", \"\u2739\", \"\u273a\", \"\u2739\", \"\u2737\"]\n    }\n\n    def __init__(\n        self,\n        message: str = \"Loading...\",\n        delay: float = 0.1,\n        symbols=None,\n        count_down: bool = False,\n        time_in_s: float = 0\n    ):\n        \"\"\"Initialize spinner with flexible configuration.\"\"\"\n        # Resolve symbol set.\n        if isinstance(symbols, str):\n            symbols = self.SYMBOL_SETS.get(symbols, None)\n\n        # Default symbols if not provided.\n        if symbols is None:\n            symbols = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n\n        # Test mode symbol set.\n        if 'unittest' in sys.argv[0]:\n            symbols = ['#', '=', '-']\n\n        self.spinner = itertools.cycle(symbols)\n        self.delay = delay\n        self.message = message\n        self.running = False\n        self.spinner_thread = None\n        self.max_t = time_in_s\n        self.contd = count_down\n\n        # Rendering management.\n        self._is_primary = False\n        self._start_time = 0\n\n        # Central manager.\n        self.manager = SpinnerManager()\n\n    def _generate_render_line(self):\n        \"\"\"Generate the primary render line.\"\"\"\n        current_time = time.time()\n        if self.contd:\n            remaining = max(0, self.max_t - (current_time - self._start_time))\n            time_display = f\"{remaining:.2f}\"\n        else:\n            time_display = f\"{current_time - self._start_time:.2f}\"\n\n        symbol = next(self.spinner)\n        return f\"{symbol} {self.message} | {time_display}\"\n\n    def _generate_secondary_info(self):\n        \"\"\"Generate secondary spinner info for additional spinners.\"\"\"\n        return f\"{self.message}\"\n\n    def __enter__(self):\n        \"\"\"Start the spinner.\"\"\"\n        self.running = True\n        self._start_time = time.time()\n        self.manager.register_spinner(self)\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        \"\"\"Stop the spinner.\"\"\"\n        self.running = False\n        self.manager.unregister_spinner(self)\n        # Clear the spinner's line if it was the primary spinner.\n        if self._is_primary:\n            sys.stdout.write(\"\\r\\033[K\")\n            sys.stdout.flush()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Spinner.__enter__","title":"<code>__enter__()</code>","text":"<p>Start the spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __enter__(self):\n    \"\"\"Start the spinner.\"\"\"\n    self.running = True\n    self._start_time = time.time()\n    self.manager.register_spinner(self)\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Spinner.__exit__","title":"<code>__exit__(exc_type, exc_value, exc_traceback)</code>","text":"<p>Stop the spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __exit__(self, exc_type, exc_value, exc_traceback):\n    \"\"\"Stop the spinner.\"\"\"\n    self.running = False\n    self.manager.unregister_spinner(self)\n    # Clear the spinner's line if it was the primary spinner.\n    if self._is_primary:\n        sys.stdout.write(\"\\r\\033[K\")\n        sys.stdout.flush()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Spinner.__init__","title":"<code>__init__(message='Loading...', delay=0.1, symbols=None, count_down=False, time_in_s=0)</code>","text":"<p>Initialize spinner with flexible configuration.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __init__(\n    self,\n    message: str = \"Loading...\",\n    delay: float = 0.1,\n    symbols=None,\n    count_down: bool = False,\n    time_in_s: float = 0\n):\n    \"\"\"Initialize spinner with flexible configuration.\"\"\"\n    # Resolve symbol set.\n    if isinstance(symbols, str):\n        symbols = self.SYMBOL_SETS.get(symbols, None)\n\n    # Default symbols if not provided.\n    if symbols is None:\n        symbols = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n\n    # Test mode symbol set.\n    if 'unittest' in sys.argv[0]:\n        symbols = ['#', '=', '-']\n\n    self.spinner = itertools.cycle(symbols)\n    self.delay = delay\n    self.message = message\n    self.running = False\n    self.spinner_thread = None\n    self.max_t = time_in_s\n    self.contd = count_down\n\n    # Rendering management.\n    self._is_primary = False\n    self._start_time = 0\n\n    # Central manager.\n    self.manager = SpinnerManager()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.remove_styles","title":"<code>toolboxv2.remove_styles(text, infos=False)</code>","text":"Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def remove_styles(text: str, infos=False):\n    in_ = []\n    for key, style in Style.style_dic.items():\n        if style in text:\n            text = text.replace(style, '')\n            if infos:\n                in_.append([key for key, st in Style.style_dic.items() if style == st][0])\n    if infos:\n        if \"END\" in in_:\n            in_.remove('END')\n        return text, in_\n    return text\n</code></pre>"},{"location":"toolboxv2/#data-types-structures","title":"Data Types &amp; Structures","text":""},{"location":"toolboxv2/#toolboxv2.AppArgs","title":"<code>toolboxv2.AppArgs</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class AppArgs:\n    init = None\n    init_file = 'init.config'\n    get_version = False\n    mm = False\n    sm = False\n    lm = False\n    modi = 'cli'\n    kill = False\n    remote = False\n    remote_direct_key = None\n    background_application = False\n    background_application_runner = False\n    docker = False\n    build = False\n    install = None\n    remove = None\n    update = None\n    name = 'main'\n    port = 5000\n    host = '0.0.0.0'\n    load_all_mod_in_files = False\n    mods_folder = 'toolboxv2.mods.'\n    debug = None\n    test = None\n    profiler = None\n    hot_reload = False\n    live_application = True\n    sysPrint = False\n    kwargs = {}\n    session = None\n\n    def default(self):\n        return self\n\n    def set(self, name, value):\n        setattr(self, name, value)\n        return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result","title":"<code>toolboxv2.Result</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class Result:\n    _task = None\n    def __init__(self,\n                 error: ToolBoxError,\n                 result: ToolBoxResult,\n                 info: ToolBoxInfo,\n                 origin: Any | None = None,\n                 ):\n        self.error: ToolBoxError = error\n        self.result: ToolBoxResult = result\n        self.info: ToolBoxInfo = info\n        self.origin = origin\n\n    def as_result(self):\n        return self\n\n    def as_dict(self):\n        return {\n            \"error\":self.error.value if isinstance(self.error, Enum) else self.error,\n        \"result\" : {\n            \"data_to\":self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n            \"data_info\":self.result.data_info,\n            \"data\":self.result.data,\n            \"data_type\":self.result.data_type\n        } if self.result else None,\n        \"info\" : {\n            \"exec_code\" : self.info.exec_code,  # exec_code umwandel in http resposn codes\n        \"help_text\" : self.info.help_text\n        } if self.info else None,\n        \"origin\" : self.origin\n        }\n\n    def set_origin(self, origin):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = origin\n        return self\n\n    def set_dir_origin(self, name, extras=\"assets/\"):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = f\"mods/{name}/{extras}\"\n        return self\n\n    def is_error(self):\n        if _test_is_result(self.result.data):\n            return self.result.data.is_error()\n        return self.info.exec_code != 0\n\n    def is_data(self):\n        return self.result.data is not None\n\n    def to_api_result(self):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=self.error.value if isinstance(self.error, Enum) else self.error,\n            result=ToolBoxResultBM(\n                data_to=self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n                data_info=self.result.data_info,\n                data=self.result.data,\n                data_type=self.result.data_type\n            ) if self.result else None,\n            info=ToolBoxInfoBM(\n                exec_code=self.info.exec_code,  # exec_code umwandel in http resposn codes\n                help_text=self.info.help_text\n            ) if self.info else None,\n            origin=self.origin\n        )\n\n    def task(self, task):\n        self._task = task\n        return self\n\n    @staticmethod\n    def result_from_dict(error: str, result: dict, info: dict, origin: list or None or str):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=error if isinstance(error, Enum) else error,\n            result=ToolBoxResultBM(\n                data_to=result.get('data_to') if isinstance(result.get('data_to'), Enum) else result.get('data_to'),\n                data_info=result.get('data_info', '404'),\n                data=result.get('data'),\n                data_type=result.get('data_type', '404'),\n            ) if result else None,\n            info=ToolBoxInfoBM(\n                exec_code=info.get('exec_code', 404),\n                help_text=info.get('help_text', '404')\n            ) if info else None,\n            origin=origin\n        ).as_result()\n\n    @classmethod\n    def stream(cls,\n               stream_generator,\n               content_type=\"text/event-stream\",\n               headers=None,\n               info=\"OK\",\n               interface=ToolBoxInterfaces.remote,\n               cleanup_func=None):\n        \"\"\"\n        Create a streaming response Result that properly handles all types of stream sources.\n\n        Args:\n            stream_generator: Any stream source (async generator, sync generator, iterable, or even string)\n            content_type: Content-Type header (default: text/event-stream for SSE)\n            headers: Additional HTTP headers\n            info: Help text for the result\n            interface: Interface to send data to\n\n        Returns:\n            A Result object configured for streaming\n        \"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        # Standard SSE headers\n        standard_headers = {\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"X-Accel-Buffering\": \"no\"\n        }\n\n        # Apply custom headers\n        all_headers = standard_headers.copy()\n        if headers:\n            all_headers.update(headers)\n\n        # Handle different types of stream sources\n        if content_type == \"text/event-stream\":\n            wrapped_generator = stream_generator\n            if inspect.isgenerator(stream_generator) or hasattr(stream_generator, '__iter__'):\n                # Sync generator or iterable\n                wrapped_generator = SSEGenerator.create_sse_stream(stream_generator, cleanup_func)\n\n            elif isinstance(stream_generator, str):\n                # String (could be a memory address or other reference)\n                # Convert to a generator that yields a single string\n                async def string_to_stream():\n                    yield stream_generator\n\n                wrapped_generator = SSEGenerator.create_sse_stream(string_to_stream(), cleanup_func)\n\n            # The final generator to use\n            final_generator = wrapped_generator\n\n        else:\n            # For non-SSE streams, use the original generator\n            final_generator = stream_generator\n\n        # Prepare streaming data\n        streaming_data = {\n            \"type\": \"stream\",\n            \"generator\": final_generator,\n            \"content_type\": content_type,\n            \"headers\": all_headers\n        }\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=streaming_data,\n            data_info=\"Streaming response\",\n            data_type=\"stream\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def default(cls, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=-1, help_text=\"\")\n        result = ToolBoxResult(data_to=interface)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a JSON response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=data,\n            data_info=\"JSON response\",\n            data_type=\"json\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n        \"\"\"Create a text response Result with specific content type.\"\"\"\n        if headers is not None:\n            return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=text_data,\n            data_info=\"Text response\",\n            data_type=content_type\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n               interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a binary data response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        # Create a dictionary with binary data and metadata\n        binary_data = {\n            \"data\": data,\n            \"content_type\": content_type,\n            \"filename\": download_name\n        }\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=binary_data,\n            data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n            data_type=\"binary\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a redirect response.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=url,\n            data_info=\"Redirect response\",\n            data_type=\"redirect\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def ok(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def html(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.remote, data_type=\"html\",status=200, headers=None):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=status, help_text=info)\n\n        if isinstance(headers, dict):\n            result = ToolBoxResult(data_to=interface, data={'html':data,'headers':headers}, data_info=data_info,\n                                   data_type=\"special_html\")\n        else:\n            result = ToolBoxResult(data_to=interface, data=data, data_info=data_info,\n                                   data_type=data_type if data_type is not None else type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def future(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.future):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=\"future\")\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def custom_error(cls, data=None, data_info=\"\", info=\"\", exec_code=-1, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def error(cls, data=None, data_info=\"\", info=\"\", exec_code=450, interface=ToolBoxInterfaces.remote):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_user_error(cls, info=\"\", exec_code=-3, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.input_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_internal_error(cls, info=\"\", exec_code=-2, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.internal_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    def print(self, show=True, show_data=True, prifix=\"\"):\n        data = '\\n' + f\"{((prifix + 'Data: ' + str(self.result.data) if self.result.data is not None else 'NO Data') if not isinstance(self.result.data, Result) else self.result.data.print(show=False, show_data=show_data, prifix=prifix + '-')) if show_data else 'Data: private'}\"\n        origin = '\\n' + f\"{prifix + 'Origin: ' + str(self.origin) if self.origin is not None else 'NO Origin'}\"\n        text = (f\"Function Exec code: {self.info.exec_code}\"\n                f\"\\n{prifix}Info's:\"\n                f\" {self.info.help_text} {'&lt;|&gt; ' + str(self.result.data_info) if self.result.data_info is not None else ''}\"\n                f\"{origin}{data if not data.endswith('NO Data') else ''}\")\n        if not show:\n            return text\n        print(\"\\n======== Result ========\\n\" + text + \"\\n------- EndOfD -------\")\n        return self\n\n    def log(self, show_data=True, prifix=\"\"):\n        from toolboxv2 import get_logger\n        get_logger().debug(self.print(show=False, show_data=show_data, prifix=prifix).replace(\"\\n\", \" - \"))\n        return self\n\n    def __str__(self):\n        return self.print(show=False, show_data=True)\n\n    def get(self, key=None, default=None):\n        data = self.result.data\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    async def aget(self, key=None, default=None):\n        if asyncio.isfuture(self.result.data) or asyncio.iscoroutine(self.result.data) or (\n            isinstance(self.result.data_to, Enum) and self.result.data_to.name == ToolBoxInterfaces.future.name):\n            data = await self.result.data\n        else:\n            data = self.get(key=None, default=None)\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    def lazy_return(self, _=0, data=None, **kwargs):\n        flags = ['raise', 'logg', 'user', 'intern']\n        flag = flags[_] if isinstance(_, int) else _\n        if self.info.exec_code == 0:\n            return self if data is None else data if _test_is_result(data) else self.ok(data=data, **kwargs)\n        if flag == 'raise':\n            raise ValueError(self.print(show=False))\n        if flag == 'logg':\n            from .. import get_logger\n            get_logger().error(self.print(show=False))\n\n        if flag == 'user':\n            return self if data is None else data if _test_is_result(data) else self.default_user_error(data=data,\n                                                                                                        **kwargs)\n        if flag == 'intern':\n            return self if data is None else data if _test_is_result(data) else self.default_internal_error(data=data,\n                                                                                                            **kwargs)\n\n        return self if data is None else data if _test_is_result(data) else self.custom_error(data=data, **kwargs)\n\n    @property\n    def bg_task(self):\n        return self._task\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.binary","title":"<code>binary(data, content_type='application/octet-stream', download_name=None, info='OK', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a binary data response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n           interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a binary data response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    # Create a dictionary with binary data and metadata\n    binary_data = {\n        \"data\": data,\n        \"content_type\": content_type,\n        \"filename\": download_name\n    }\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=binary_data,\n        data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n        data_type=\"binary\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.json","title":"<code>json(data, info='OK', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a JSON response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a JSON response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=data,\n        data_info=\"JSON response\",\n        data_type=\"json\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.redirect","title":"<code>redirect(url, status_code=302, info='Redirect', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a redirect response.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a redirect response.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=url,\n        data_info=\"Redirect response\",\n        data_type=\"redirect\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.stream","title":"<code>stream(stream_generator, content_type='text/event-stream', headers=None, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code>  <code>classmethod</code>","text":"<p>Create a streaming response Result that properly handles all types of stream sources.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <p>Any stream source (async generator, sync generator, iterable, or even string)</p> required <code>content_type</code> <p>Content-Type header (default: text/event-stream for SSE)</p> <code>'text/event-stream'</code> <code>headers</code> <p>Additional HTTP headers</p> <code>None</code> <code>info</code> <p>Help text for the result</p> <code>'OK'</code> <code>interface</code> <p>Interface to send data to</p> <code>remote</code> <p>Returns:</p> Type Description <p>A Result object configured for streaming</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef stream(cls,\n           stream_generator,\n           content_type=\"text/event-stream\",\n           headers=None,\n           info=\"OK\",\n           interface=ToolBoxInterfaces.remote,\n           cleanup_func=None):\n    \"\"\"\n    Create a streaming response Result that properly handles all types of stream sources.\n\n    Args:\n        stream_generator: Any stream source (async generator, sync generator, iterable, or even string)\n        content_type: Content-Type header (default: text/event-stream for SSE)\n        headers: Additional HTTP headers\n        info: Help text for the result\n        interface: Interface to send data to\n\n    Returns:\n        A Result object configured for streaming\n    \"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    # Standard SSE headers\n    standard_headers = {\n        \"Cache-Control\": \"no-cache\",\n        \"Connection\": \"keep-alive\",\n        \"X-Accel-Buffering\": \"no\"\n    }\n\n    # Apply custom headers\n    all_headers = standard_headers.copy()\n    if headers:\n        all_headers.update(headers)\n\n    # Handle different types of stream sources\n    if content_type == \"text/event-stream\":\n        wrapped_generator = stream_generator\n        if inspect.isgenerator(stream_generator) or hasattr(stream_generator, '__iter__'):\n            # Sync generator or iterable\n            wrapped_generator = SSEGenerator.create_sse_stream(stream_generator, cleanup_func)\n\n        elif isinstance(stream_generator, str):\n            # String (could be a memory address or other reference)\n            # Convert to a generator that yields a single string\n            async def string_to_stream():\n                yield stream_generator\n\n            wrapped_generator = SSEGenerator.create_sse_stream(string_to_stream(), cleanup_func)\n\n        # The final generator to use\n        final_generator = wrapped_generator\n\n    else:\n        # For non-SSE streams, use the original generator\n        final_generator = stream_generator\n\n    # Prepare streaming data\n    streaming_data = {\n        \"type\": \"stream\",\n        \"generator\": final_generator,\n        \"content_type\": content_type,\n        \"headers\": all_headers\n    }\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=streaming_data,\n        data_info=\"Streaming response\",\n        data_type=\"stream\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.text","title":"<code>text(text_data, content_type='text/plain', exec_code=None, status=200, info='OK', interface=ToolBoxInterfaces.remote, headers=None)</code>  <code>classmethod</code>","text":"<p>Create a text response Result with specific content type.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n    \"\"\"Create a text response Result with specific content type.\"\"\"\n    if headers is not None:\n        return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=text_data,\n        data_info=\"Text response\",\n        data_type=content_type\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.ApiResult","title":"<code>toolboxv2.ApiResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class ApiResult(BaseModel):\n    error: None | str= None\n    origin: Any | None\n    result: ToolBoxResultBM | None = None\n    info: ToolBoxInfoBM | None\n\n    def as_result(self):\n        return Result(\n            error=self.error.value if isinstance(self.error, Enum) else self.error,\n            result=ToolBoxResult(\n                data_to=self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n                data_info=self.result.data_info,\n                data=self.result.data,\n                data_type=self.result.data_type\n            ) if self.result else None,\n            info=ToolBoxInfo(\n                exec_code=self.info.exec_code,\n                help_text=self.info.help_text\n            ) if self.info else None,\n            origin=self.origin\n        )\n\n    def to_api_result(self):\n        return self\n\n    def print(self, *args, **kwargs):\n        res = self.as_result().print(*args, **kwargs)\n        if not isinstance(res, str):\n            res = res.to_api_result()\n        return res\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.RequestData","title":"<code>toolboxv2.RequestData</code>  <code>dataclass</code>","text":"<p>Main class representing the complete request data structure.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@dataclass\nclass RequestData:\n    \"\"\"Main class representing the complete request data structure.\"\"\"\n    request: Request\n    session: Session\n    session_id: str\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any]) -&gt; 'RequestData':\n        \"\"\"Create a RequestData instance from a dictionary.\"\"\"\n        return cls(\n            request=Request.from_dict(data.get('request', {})),\n            session=Session.from_dict(data.get('session', {})),\n            session_id=data.get('session_id', '')\n        )\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert the RequestData object back to a dictionary.\"\"\"\n        return {\n            'request': self.request.to_dict(),\n            'session': self.session.to_dict(),\n            'session_id': self.session_id\n        }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.RequestData.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create a RequestData instance from a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; 'RequestData':\n    \"\"\"Create a RequestData instance from a dictionary.\"\"\"\n    return cls(\n        request=Request.from_dict(data.get('request', {})),\n        session=Session.from_dict(data.get('session', {})),\n        session_id=data.get('session_id', '')\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.RequestData.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert the RequestData object back to a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert the RequestData object back to a dictionary.\"\"\"\n    return {\n        'request': self.request.to_dict(),\n        'session': self.session.to_dict(),\n        'session_id': self.session_id\n    }\n</code></pre>"},{"location":"toolboxv2/#security","title":"Security","text":""},{"location":"toolboxv2/#toolboxv2.Code","title":"<code>toolboxv2.Code</code>","text":"Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>class Code:\n\n    @staticmethod\n    def DK():\n        return DEVICE_KEY\n\n    def decode_code(self, encrypted_data, key=None):\n\n        if not isinstance(encrypted_data, str):\n            encrypted_data = str(encrypted_data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.decrypt_symmetric(encrypted_data, key)\n\n    def encode_code(self, data, key=None):\n\n        if not isinstance(data, str):\n            data = str(data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.encrypt_symmetric(data, key)\n\n    @staticmethod\n    def generate_seed() -&gt; int:\n        \"\"\"\n        Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n        Returns:\n            int: Eine zuf\u00e4llige Zahl.\n        \"\"\"\n        return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n\n    @staticmethod\n    def one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n        \"\"\"\n        Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n        Args:\n            text (str): Der zu hashende Text.\n            salt (str): Der Salt-Wert.\n            pepper (str): Der Pepper-Wert.\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            str: Der resultierende Hash-Wert.\n        \"\"\"\n        return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n\n    @staticmethod\n    def generate_symmetric_key() -&gt; str:\n        \"\"\"\n        Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n        Returns:\n            str: Der generierte Schl\u00fcssel.\n        \"\"\"\n        return Fernet.generate_key().decode()\n\n    @staticmethod\n    def encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        if isinstance(text, str):\n            text = text.encode()\n\n        try:\n            fernet = Fernet(key.encode())\n            return fernet.encrypt(text).decode()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n            return \"Error encrypt\"\n\n    @staticmethod\n    def decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n            to_str (bool): default true returns str if false returns bytes\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n\n        if isinstance(key, str):\n            key = key.encode()\n\n        #try:\n        fernet = Fernet(key)\n        text_b = fernet.decrypt(encrypted_text)\n        if not to_str:\n            return text_b\n        return text_b.decode()\n        # except Exception as e:\n        #     get_logger().error(f\"Error decrypt_symmetric {e}\")\n        #     if not mute:\n        #         raise e\n        #     if not to_str:\n        #         return f\"Error decoding\".encode()\n        #     return f\"Error decoding\"\n\n    @staticmethod\n    def generate_asymmetric_keys() -&gt; (str, str):\n        \"\"\"\n        Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n        Args:\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n        \"\"\"\n        private_key = rsa.generate_private_key(\n            public_exponent=65537,\n            key_size=2048 * 3,\n        )\n        public_key = private_key.public_key()\n\n        # Serialisieren der Schl\u00fcssel\n        pem_private_key = private_key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.NoEncryption()\n        ).decode()\n\n        pem_public_key = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        ).decode()\n\n        return pem_public_key, pem_private_key\n\n    @staticmethod\n    def save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n        \"\"\"\n        Speichert die generierten Schl\u00fcssel in separate Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n        Args:\n            public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n            private_key (str): Der private Schl\u00fcssel im PEM-Format\n            directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n        \"\"\"\n        # Erstelle das Verzeichnis, falls es nicht existiert\n        os.makedirs(directory, exist_ok=True)\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n        encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n        # Speichere den \u00f6ffentlichen Schl\u00fcssel\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        with open(public_key_path, \"w\") as f:\n            f.write(public_key)\n\n        # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n        with open(private_key_path, \"w\") as f:\n            f.write(encrypted_private_key)\n\n        print(\"Saved keys in \", public_key_path)\n\n    @staticmethod\n    def load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n        \"\"\"\n        L\u00e4dt die Schl\u00fcssel aus den Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n        Args:\n            directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n        Raises:\n            FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n        \"\"\"\n        # Pfade zu den Schl\u00fcsseldateien\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n\n        # Pr\u00fcfe ob die Dateien existieren\n        if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n            return \"\", \"\"\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Lade den \u00f6ffentlichen Schl\u00fcssel\n        with open(public_key_path) as f:\n            public_key = f.read()\n\n        # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n        with open(private_key_path) as f:\n            encrypted_private_key = f.read()\n            private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n        return public_key, private_key\n\n    @staticmethod\n    def encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        # try:\n        #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        #  except Exception as e:\n        #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            encrypted = public_key.encrypt(\n                text.encode(),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return encrypted.hex()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_asymmetric {e}\")\n            return \"Invalid\"\n\n    @staticmethod\n    def decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n        Args:\n            encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n            private_key_str (str): Der private Schl\u00fcssel als String.\n\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            decrypted = private_key.decrypt(\n                bytes.fromhex(encrypted_text_hex),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return decrypted.decode()\n\n        except Exception as e:\n            get_logger().error(f\"Error decrypt_asymmetric {e}\")\n        return \"Invalid\"\n\n    @staticmethod\n    def verify_signature(signature: str or bytes, message: str or bytes, public_key_str: str,\n                         salt_length=padding.PSS.MAX_LENGTH) -&gt; bool:\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                padding=padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                algorithm=hashes.SHA512()\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def verify_signature_web_algo(signature: str or bytes, message: str or bytes, public_key_str: str,\n                                  algo: int = -512) -&gt; bool:\n        signature_algorithm = ECDSA(hashes.SHA512())\n        if algo != -512:\n            signature_algorithm = ECDSA(hashes.SHA256())\n\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                # padding=padding.PSS(\n                #    mgf=padding.MGF1(hashes.SHA512()),\n                #    salt_length=padding.PSS.MAX_LENGTH\n                # ),\n                signature_algorithm=signature_algorithm\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def create_signature(message: str, private_key_str: str, salt_length=padding.PSS.MAX_LENGTH,\n                         row=False) -&gt; str or bytes:\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            signature = private_key.sign(\n                message.encode(),\n                padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                hashes.SHA512()\n            )\n            if row:\n                return signature\n            return base64.b64encode(signature).decode()\n        except Exception as e:\n            get_logger().error(f\"Error create_signature {e}\")\n            print(e)\n        return \"Invalid Key\"\n\n    @staticmethod\n    def pem_to_public_key(pem_key: str):\n        \"\"\"\n        Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n        Args:\n            pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n        Returns:\n            PublicKey: Das PublicKey-Objekt.\n        \"\"\"\n        public_key = serialization.load_pem_public_key(pem_key.encode())\n        return public_key\n\n    @staticmethod\n    def public_key_to_pem(public_key: RSAPublicKey):\n        \"\"\"\n        Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n        Args:\n            public_key (PublicKey): Das PublicKey-Objekt.\n\n        Returns:\n            str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n        \"\"\"\n        pem = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n        return pem.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.decrypt_asymmetric","title":"<code>decrypt_asymmetric(encrypted_text_hex, private_key_str)</code>  <code>staticmethod</code>","text":"<p>Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text_hex</code> <code>str</code> <p>Der verschl\u00fcsselte Text als Hex-String.</p> required <code>private_key_str</code> <code>str</code> <p>Der private Schl\u00fcssel als String.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n    Args:\n        encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n        private_key_str (str): Der private Schl\u00fcssel als String.\n\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n    try:\n        private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n        decrypted = private_key.decrypt(\n            bytes.fromhex(encrypted_text_hex),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return decrypted.decode()\n\n    except Exception as e:\n        get_logger().error(f\"Error decrypt_asymmetric {e}\")\n    return \"Invalid\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.decrypt_symmetric","title":"<code>decrypt_symmetric(encrypted_text, key, to_str=True, mute=False)</code>  <code>staticmethod</code>","text":"<p>Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text</code> <code>str</code> <p>Der zu entschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <code>to_str</code> <code>bool</code> <p>default true returns str if false returns bytes</p> <code>True</code> <p>Returns:     str: Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n        to_str (bool): default true returns str if false returns bytes\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n\n    if isinstance(key, str):\n        key = key.encode()\n\n    #try:\n    fernet = Fernet(key)\n    text_b = fernet.decrypt(encrypted_text)\n    if not to_str:\n        return text_b\n    return text_b.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.encrypt_asymmetric","title":"<code>encrypt_asymmetric(text, public_key_str)</code>  <code>staticmethod</code>","text":"<p>Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>public_key_str</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    # try:\n    #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n    #  except Exception as e:\n    #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n    try:\n        public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        encrypted = public_key.encrypt(\n            text.encode(),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return encrypted.hex()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        return \"Invalid\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.encrypt_symmetric","title":"<code>encrypt_symmetric(text, key)</code>  <code>staticmethod</code>","text":"<p>Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    if isinstance(text, str):\n        text = text.encode()\n\n    try:\n        fernet = Fernet(key.encode())\n        return fernet.encrypt(text).decode()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n        return \"Error encrypt\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.generate_asymmetric_keys","title":"<code>generate_asymmetric_keys()</code>  <code>staticmethod</code>","text":"<p>Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_asymmetric_keys() -&gt; (str, str):\n    \"\"\"\n    Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n    Args:\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n    \"\"\"\n    private_key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048 * 3,\n    )\n    public_key = private_key.public_key()\n\n    # Serialisieren der Schl\u00fcssel\n    pem_private_key = private_key.private_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PrivateFormat.PKCS8,\n        encryption_algorithm=serialization.NoEncryption()\n    ).decode()\n\n    pem_public_key = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    ).decode()\n\n    return pem_public_key, pem_private_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.generate_seed","title":"<code>generate_seed()</code>  <code>staticmethod</code>","text":"<p>Erzeugt eine zuf\u00e4llige Zahl als Seed.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Eine zuf\u00e4llige Zahl.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_seed() -&gt; int:\n    \"\"\"\n    Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n    Returns:\n        int: Eine zuf\u00e4llige Zahl.\n    \"\"\"\n    return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.generate_symmetric_key","title":"<code>generate_symmetric_key()</code>  <code>staticmethod</code>","text":"<p>Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der generierte Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_symmetric_key() -&gt; str:\n    \"\"\"\n    Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n    Returns:\n        str: Der generierte Schl\u00fcssel.\n    \"\"\"\n    return Fernet.generate_key().decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.load_keys_from_files","title":"<code>load_keys_from_files(directory='keys')</code>  <code>staticmethod</code>","text":"<p>L\u00e4dt die Schl\u00fcssel aus den Dateien. Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen</p> <code>'keys'</code> <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n    \"\"\"\n    L\u00e4dt die Schl\u00fcssel aus den Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n    Args:\n        directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n    Raises:\n        FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n    \"\"\"\n    # Pfade zu den Schl\u00fcsseldateien\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n\n    # Pr\u00fcfe ob die Dateien existieren\n    if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n        return \"\", \"\"\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Lade den \u00f6ffentlichen Schl\u00fcssel\n    with open(public_key_path) as f:\n        public_key = f.read()\n\n    # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n    with open(private_key_path) as f:\n        encrypted_private_key = f.read()\n        private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n    return public_key, private_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.one_way_hash","title":"<code>one_way_hash(text, salt='', pepper='')</code>  <code>staticmethod</code>","text":"<p>Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu hashende Text.</p> required <code>salt</code> <code>str</code> <p>Der Salt-Wert.</p> <code>''</code> <code>pepper</code> <code>str</code> <p>Der Pepper-Wert.</p> <code>''</code> <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der resultierende Hash-Wert.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n    \"\"\"\n    Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n    Args:\n        text (str): Der zu hashende Text.\n        salt (str): Der Salt-Wert.\n        pepper (str): Der Pepper-Wert.\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        str: Der resultierende Hash-Wert.\n    \"\"\"\n    return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.pem_to_public_key","title":"<code>pem_to_public_key(pem_key)</code>  <code>staticmethod</code>","text":"<p>Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.</p> <p>Parameters:</p> Name Type Description Default <code>pem_key</code> <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef pem_to_public_key(pem_key: str):\n    \"\"\"\n    Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n    Args:\n        pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n    Returns:\n        PublicKey: Das PublicKey-Objekt.\n    \"\"\"\n    public_key = serialization.load_pem_public_key(pem_key.encode())\n    return public_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.public_key_to_pem","title":"<code>public_key_to_pem(public_key)</code>  <code>staticmethod</code>","text":"<p>Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef public_key_to_pem(public_key: RSAPublicKey):\n    \"\"\"\n    Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n    Args:\n        public_key (PublicKey): Das PublicKey-Objekt.\n\n    Returns:\n        str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n    \"\"\"\n    pem = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    )\n    return pem.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.save_keys_to_files","title":"<code>save_keys_to_files(public_key, private_key, directory='keys')</code>  <code>staticmethod</code>","text":"<p>Speichert die generierten Schl\u00fcssel in separate Dateien. Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel im PEM-Format</p> required <code>private_key</code> <code>str</code> <p>Der private Schl\u00fcssel im PEM-Format</p> required <code>directory</code> <code>str</code> <p>Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen</p> <code>'keys'</code> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n    \"\"\"\n    Speichert die generierten Schl\u00fcssel in separate Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n    Args:\n        public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n        private_key (str): Der private Schl\u00fcssel im PEM-Format\n        directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n    \"\"\"\n    # Erstelle das Verzeichnis, falls es nicht existiert\n    os.makedirs(directory, exist_ok=True)\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n    encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n    # Speichere den \u00f6ffentlichen Schl\u00fcssel\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    with open(public_key_path, \"w\") as f:\n        f.write(public_key)\n\n    # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n    with open(private_key_path, \"w\") as f:\n        f.write(encrypted_private_key)\n\n    print(\"Saved keys in \", public_key_path)\n</code></pre>"},{"location":"toolboxv2/#modules-flows","title":"Modules &amp; Flows","text":""},{"location":"toolboxv2/#toolboxv2.mods","title":"<code>toolboxv2.mods</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM","title":"<code>CloudM</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager","title":"<code>ModManager</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.create_and_pack_module","title":"<code>create_and_pack_module(path, module_name='', version='-.-.-', additional_dirs=None, yaml_data=None)</code>","text":"<p>Erstellt ein Python-Modul und packt es in eine ZIP-Datei.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Pfad zum Ordner oder zur Datei, die in das Modul aufgenommen werden soll.</p> required <code>additional_dirs</code> <code>dict</code> <p>Zus\u00e4tzliche Verzeichnisse, die hinzugef\u00fcgt werden sollen.</p> <code>None</code> <code>version</code> <code>str</code> <p>Version des Moduls.</p> <code>'-.-.-'</code> <code>module_name</code> <code>str</code> <p>Name des Moduls.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <p>Pfad zur erstellten ZIP-Datei.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def create_and_pack_module(path, module_name='', version='-.-.-', additional_dirs=None, yaml_data=None):\n    \"\"\"\n    Erstellt ein Python-Modul und packt es in eine ZIP-Datei.\n\n    Args:\n        path (str): Pfad zum Ordner oder zur Datei, die in das Modul aufgenommen werden soll.\n        additional_dirs (dict): Zus\u00e4tzliche Verzeichnisse, die hinzugef\u00fcgt werden sollen.\n        version (str): Version des Moduls.\n        module_name (str): Name des Moduls.\n\n    Returns:\n        str: Pfad zur erstellten ZIP-Datei.\n    \"\"\"\n    if additional_dirs is None:\n        additional_dirs = {}\n    if yaml_data is None:\n        yaml_data = {}\n\n    os.makedirs(\"./mods_sto/temp/\", exist_ok=True)\n\n    module_path = os.path.join(path, module_name)\n    print(\"module_pathmodule_pathmodule_path\", module_path)\n    if not os.path.exists(module_path):\n        module_path += '.py'\n\n    temp_dir = tempfile.mkdtemp(dir=os.path.join(\"./mods_sto\", \"temp\"))\n    zip_file_name = f\"RST${module_name}&amp;{__version__}\u00a7{version}.zip\"\n    zip_path = f\"./mods_sto/{zip_file_name}\"\n\n    # Modulverzeichnis erstellen, falls es nicht existiert\n    if not os.path.exists(module_path):\n        return False\n\n    if os.path.isdir(module_path):\n        # tbConfig.yaml erstellen\n        config_path = os.path.join(module_path, \"tbConfig.yaml\")\n        with open(config_path, 'w') as config_file:\n            yaml.dump({\"version\": version, \"module_name\": module_name,\n                       \"dependencies_file\": f\"./mods/{module_name}/requirements.txt\",\n                       \"zip\": zip_file_name, **yaml_data}, config_file)\n\n        generate_requirements(module_path, os.path.join(module_path, \"requirements.txt\"))\n    # Datei oder Ordner in das Modulverzeichnis kopieren\n    if os.path.isdir(module_path):\n        shutil.copytree(module_path, os.path.join(temp_dir, os.path.basename(module_path)), dirs_exist_ok=True)\n    else:\n        shutil.copy2(module_path, temp_dir)\n        config_path = os.path.join(temp_dir, f\"{module_name}.yaml\")\n        with open(config_path, 'w') as config_file:\n            yaml.dump({\"version\": version, \"dependencies_file\": f\"./mods/{module_name}/requirements.txt\",\n                       \"module_name\": module_name, **yaml_data}, config_file)\n        generate_requirements(temp_dir, os.path.join(temp_dir, \"requirements.txt\"))\n    # Zus\u00e4tzliche Verzeichnisse hinzuf\u00fcgen\n    for dir_name, dir_paths in additional_dirs.items():\n        if isinstance(dir_paths, str):\n            dir_paths = [dir_paths]\n        for dir_path in dir_paths:\n            full_path = os.path.join(temp_dir, dir_name)\n            if os.path.isdir(dir_path):\n                shutil.copytree(dir_path, full_path, dirs_exist_ok=True)\n            elif os.path.isfile(dir_path):\n                # Stellen Sie sicher, dass das Zielverzeichnis existiert\n                os.makedirs(full_path, exist_ok=True)\n                # Kopieren Sie die Datei statt des Verzeichnisses\n                shutil.copy2(dir_path, full_path)\n            else:\n                print(f\"Der Pfad {dir_path} ist weder ein Verzeichnis noch eine Datei.\")\n\n    # Modul in eine ZIP-Datei packen\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, _dirs, files in os.walk(temp_dir):\n            for file in files:\n                file_path = os.path.join(root, file)\n                zipf.write(file_path, os.path.relpath(file_path, temp_dir))\n\n    # Temperatures Modulverzeichnis l\u00f6schen\n    shutil.rmtree(temp_dir)\n\n    return zip_path\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.download_files","title":"<code>download_files(urls, directory, desc, print_func, filename=None)</code>","text":"<p>Hilfsfunktion zum Herunterladen von Dateien.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def download_files(urls, directory, desc, print_func, filename=None):\n    \"\"\" Hilfsfunktion zum Herunterladen von Dateien. \"\"\"\n    for url in tqdm(urls, desc=desc):\n        if filename is None:\n            filename = os.path.basename(url)\n        print_func(f\"Download {filename}\")\n        print_func(f\"{url} -&gt; {directory}/{filename}\")\n        os.makedirs(directory, exist_ok=True)\n        urllib.request.urlretrieve(url, f\"{directory}/{filename}\")\n    return f\"{directory}/{filename}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.handle_requirements","title":"<code>handle_requirements(requirements_url, module_name, print_func)</code>","text":"<p>Verarbeitet und installiert Requirements.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def handle_requirements(requirements_url, module_name, print_func):\n    \"\"\" Verarbeitet und installiert Requirements. \"\"\"\n    if requirements_url:\n        requirements_filename = f\"{module_name}-requirements.txt\"\n        print_func(f\"Download requirements {requirements_filename}\")\n        urllib.request.urlretrieve(requirements_url, requirements_filename)\n\n        print_func(\"Install requirements\")\n        run_command(\n            [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", requirements_filename])\n\n        os.remove(requirements_filename)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.increment_version","title":"<code>increment_version(version_str, max_value=99)</code>","text":"<p>Inkrementiert eine Versionsnummer im Format \"vX.Y.Z\".</p> <p>Parameters:</p> Name Type Description Default <code>version_str</code> <code>str</code> <p>Die aktuelle Versionsnummer, z. B. \"v0.0.1\".</p> required <code>max_value</code> <code>int</code> <p>Die maximale Zahl pro Stelle (default: 99).</p> <code>99</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Die inkrementierte Versionsnummer.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def increment_version(version_str: str, max_value: int = 99) -&gt; str:\n    \"\"\"\n    Inkrementiert eine Versionsnummer im Format \"vX.Y.Z\".\n\n    Args:\n        version_str (str): Die aktuelle Versionsnummer, z. B. \"v0.0.1\".\n        max_value (int): Die maximale Zahl pro Stelle (default: 99).\n\n    Returns:\n        str: Die inkrementierte Versionsnummer.\n    \"\"\"\n    if not version_str.startswith(\"v\"):\n        raise ValueError(\"Die Versionsnummer muss mit 'v' beginnen, z. B. 'v0.0.1'.\")\n\n    # Entferne das f\u00fchrende 'v' und parse die Versionsnummer\n    version_core = version_str[1:]\n    try:\n        version = Version(version_core)\n    except ValueError as e:\n        raise ValueError(f\"Ung\u00fcltige Versionsnummer: {version_core}\") from e\n\n    # Extrahiere die Versionsteile und konvertiere sie zu einer Liste\n    parts = list(version.release)\n\n    # Inkrementiere die letzte Stelle\n    for i in range(len(parts) - 1, -1, -1):\n        if parts[i] &lt; max_value:\n            parts[i] += 1\n            break\n        else:\n            parts[i] = 0\n            # Schleife f\u00e4hrt fort, um die n\u00e4chsth\u00f6here Stelle zu inkrementieren\n    else:\n        # Wenn alle Stellen auf \"max_value\" sind, f\u00fcge eine neue Stelle hinzu\n        parts.insert(0, 1)\n\n    # Baue die neue Version\n    new_version = \"v\" + \".\".join(map(str, parts))\n    return new_version\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.installer","title":"<code>installer(app, module_name, build_state=True)</code>  <code>async</code>","text":"<p>Installiert oder aktualisiert ein Modul basierend auf der Remote-Version.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>@export(mod_name=Name, name=\"install\", test=False)\nasync def installer(app: App | None, module_name: str, build_state=True):\n    \"\"\"\n    Installiert oder aktualisiert ein Modul basierend auf der Remote-Version.\n    \"\"\"\n    if app is None:\n        app = get_app(f\"{Name}.installer\")\n\n    if not app.session.valid and not await app.session.login():\n        return Result.default_user_error(\"Please login with CloudM login\")\n\n    # Hole nur die h\u00f6chste verf\u00fcgbare Version vom Server\n    response = await app.session.fetch(f\"/installer/version?name={module_name}\", method=\"GET\")\n    remote_version : str = await response.text()\n    remote_version = remote_version.split('\"')[1]\n    if remote_version == \"None\":\n        remote_version = None\n    # Finde lokale Version\n    local_version = find_highest_zip_version(\n        module_name, version_only=True\n    )\n\n    if not local_version and not remote_version:\n        return Result.default_user_error(f\"404 mod {module_name} not found\")\n\n    # Vergleiche Versionen\n    local_ver = pv.parse(local_version) if local_version else pv.parse(\"0.0.0\")\n    remote_ver = pv.parse(remote_version)\n\n    app.print(f\"Mod versions - Local: {local_ver}, Remote: {remote_ver}\")\n\n    if remote_ver &gt; local_ver:\n        # Konstruiere die URL direkt aus Modulname und Version\n        mod_url = f\"/installer/mods_sto/RST${module_name}&amp;{app.version}\u00a7{remote_version}.zip\"\n        download_path = Path(app.start_dir) / 'mods_sto'\n\n        app.print(f\"Fetching Mod from {app.session.base+mod_url}\")\n        if not await app.session.download_file(mod_url, str(download_path)):\n            app.print(\"Failed to download mod\")\n            if 'y' not in input(\"Download manually and place in mods_sto folder. Done? (y/n) \").lower():\n                return Result.default_user_error(\"Installation cancelled\")\n\n        # Korrigiere Dateinamen\n        zip_name = mod_url.split('/')[-1]\n        clean_name = zip_name.replace(\"$\", '').replace(\"&amp;\", '').replace(\"\u00a7\", '')\n        with contextlib.suppress(FileExistsError):\n            os.rename(\n                str(download_path / clean_name),\n                str(download_path / zip_name)\n            )\n\n        with Spinner(\"Installing from zip\"):\n            report = install_from_zip(app, zip_name)\n\n        if not report:\n            return Result.default_user_error(\"Setup error occurred\")\n\n        if build_state:\n            get_state_from_app(app)\n\n        return report\n\n    app.print(\"Module is already up to date\")\n    return Result.ok()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.run_command","title":"<code>run_command(command, cwd=None)</code>","text":"<p>F\u00fchrt einen Befehl aus und gibt den Output zur\u00fcck.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def run_command(command, cwd=None):\n    \"\"\"F\u00fchrt einen Befehl aus und gibt den Output zur\u00fcck.\"\"\"\n    result = subprocess.run(command, cwd=cwd, capture_output=True, text=True, check=True,\n                            encoding='cp850')\n    return result.stdout\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.uninstall_module","title":"<code>uninstall_module(path, module_name='', version='-.-.-', additional_dirs=None, yaml_data=None)</code>","text":"<p>Deinstalliert ein Python-Modul, indem es das Modulverzeichnis oder die ZIP-Datei entfernt.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Pfad zum Ordner oder zur Datei, die in das Modul aufgenommen werden soll.</p> required <code>additional_dirs</code> <code>dict</code> <p>Zus\u00e4tzliche Verzeichnisse, die hinzugef\u00fcgt werden sollen.</p> <code>None</code> <code>version</code> <code>str</code> <p>Version des Moduls.</p> <code>'-.-.-'</code> <code>module_name</code> <code>str</code> <p>Name des Moduls.</p> <code>''</code> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def uninstall_module(path, module_name='', version='-.-.-', additional_dirs=None, yaml_data=None):\n    \"\"\"\n    Deinstalliert ein Python-Modul, indem es das Modulverzeichnis oder die ZIP-Datei entfernt.\n\n    Args:\n        path (str): Pfad zum Ordner oder zur Datei, die in das Modul aufgenommen werden soll.\n        additional_dirs (dict): Zus\u00e4tzliche Verzeichnisse, die hinzugef\u00fcgt werden sollen.\n        version (str): Version des Moduls.\n        module_name (str): Name des Moduls.\n\n    \"\"\"\n    if additional_dirs is None:\n        additional_dirs = {}\n    if yaml_data is None:\n        yaml_data = {}\n\n    os.makedirs(\"./mods_sto/temp/\", exist_ok=True)\n\n    base_path = os.path.dirname(path)\n    module_path = os.path.join(base_path, module_name)\n    zip_path = f\"./mods_sto/RST${module_name}&amp;{__version__}\u00a7{version}.zip\"\n\n    # Modulverzeichnis erstellen, falls es nicht existiert\n    if not os.path.exists(module_path):\n        print(\"Module %s already uninstalled\")\n        return False\n\n    # Datei oder Ordner in das Modulverzeichnis kopieren\n    shutil.rmtree(module_path)\n\n    # Zus\u00e4tzliche Verzeichnisse hinzuf\u00fcgen\n    for _dir_name, dir_paths in additional_dirs.items():\n        if isinstance(dir_paths, str):\n            dir_paths = [dir_paths]\n        for dir_path in dir_paths:\n            shutil.rmtree(dir_path)\n            print(f\"Der Pfad {dir_path} wurde entfernt\")\n\n    # Urspr\u00fcngliches Modulverzeichnis l\u00f6schen\n    shutil.rmtree(zip_path)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.unpack_and_move_module","title":"<code>unpack_and_move_module(zip_path, base_path='./mods', module_name='')</code>","text":"<p>Entpackt eine ZIP-Datei und verschiebt die Inhalte an die richtige Stelle. \u00dcberschreibt existierende Dateien f\u00fcr Update-Unterst\u00fctzung.</p> <p>Parameters:</p> Name Type Description Default <code>zip_path</code> <code>str</code> <p>Pfad zur ZIP-Datei, die entpackt werden soll</p> required <code>base_path</code> <code>str</code> <p>Basispfad, unter dem das Modul gespeichert werden soll</p> <code>'./mods'</code> <code>module_name</code> <code>str</code> <p>Name des Moduls (optional, wird sonst aus ZIP-Namen extrahiert)</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Name des installierten Moduls</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def unpack_and_move_module(zip_path: str, base_path: str = './mods', module_name: str = '') -&gt; str:\n    \"\"\"\n    Entpackt eine ZIP-Datei und verschiebt die Inhalte an die richtige Stelle.\n    \u00dcberschreibt existierende Dateien f\u00fcr Update-Unterst\u00fctzung.\n\n    Args:\n        zip_path (str): Pfad zur ZIP-Datei, die entpackt werden soll\n        base_path (str): Basispfad, unter dem das Modul gespeichert werden soll\n        module_name (str): Name des Moduls (optional, wird sonst aus ZIP-Namen extrahiert)\n\n    Returns:\n        str: Name des installierten Moduls\n    \"\"\"\n    # Konvertiere Pfade zu Path-Objekten f\u00fcr bessere Handhabung\n    zip_path = Path(zip_path)\n    base_path = Path(base_path)\n\n    # Extrahiere Modulnamen falls nicht angegeben\n    if not module_name:\n        module_name = zip_path.name.split('$')[1].split('&amp;')[0]\n\n    module_path = base_path / module_name\n    temp_base = Path('./mods_sto/temp')\n\n    try:\n        # Erstelle tempor\u00e4res Verzeichnis\n        temp_base.mkdir(parents=True, exist_ok=True)\n        with tempfile.TemporaryDirectory(dir=str(temp_base)) as temp_dir:\n            temp_dir = Path(temp_dir)\n\n            with Spinner(f\"Extracting {zip_path.name}\"):\n                # Entpacke ZIP-Datei\n                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                    zip_ref.extractall(temp_dir)\n\n            # Behandle Modul-Verzeichnis\n            source_module = temp_dir / module_name\n            if source_module.exists():\n                with Spinner(f\"Installing module to {module_path}\"):\n                    if module_path.exists():\n                        # L\u00f6sche existierendes Modul-Verzeichnis f\u00fcr sauberes Update\n                        shutil.rmtree(module_path)\n                    # Verschiebe neues Modul-Verzeichnis\n                    shutil.copytree(source_module, module_path, dirs_exist_ok=True)\n\n            # Behandle zus\u00e4tzliche Dateien im Root\n            with Spinner(\"Installing additional files\"):\n                for item in temp_dir.iterdir():\n                    if item.name == module_name:\n                        continue\n\n                    target = Path('./') / item.name\n                    if item.is_dir():\n                        with Spinner(f\"Installing directory {item.name}\"):\n                            if target.exists():\n                                shutil.rmtree(target)\n                            shutil.copytree(item, target, dirs_exist_ok=True)\n                    else:\n                        with Spinner(f\"Installing file {item.name}\"):\n                            shutil.copy2(item, target)\n\n            print(f\"Successfully installed/updated module {module_name} to {module_path}\")\n            return module_name\n\n    except Exception as e:\n        print(f\"Error during installation: {str(e)}\")\n        # Cleanup bei Fehler\n        if module_path.exists():\n            shutil.rmtree(module_path)\n        raise\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.email_services","title":"<code>email_services</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.email_services.send_email_verification_email","title":"<code>send_email_verification_email(app, user_email, username, verification_url)</code>","text":"<p>Sends an email verification link to the user.</p> Source code in <code>toolboxv2/mods/CloudM/email_services.py</code> <pre><code>@s_export\ndef send_email_verification_email(app: App, user_email: str, username: str, verification_url: str):\n    \"\"\"Sends an email verification link to the user.\"\"\"\n    sender = EmailSender(app)\n    subject = f\"Verify Your Email for {APP_NAME}\"\n    preview_text = f\"Almost there, {username}! Just one more step to activate your account.\"\n\n    content_html = f\"\"\"\n        &lt;h2&gt;Hi {username},&lt;/h2&gt;\n        &lt;p&gt;Thanks for signing up for {APP_NAME}! To complete your registration, please verify your email address by clicking the button below.&lt;/p&gt;\n        &lt;a href=\"{verification_url}\" class=\"button\"&gt;Verify Email Address&lt;/a&gt;\n        &lt;p&gt;If you didn't create an account with {APP_NAME}, you can safely ignore this email.&lt;/p&gt;\n        &lt;p&gt;If the button doesn't work, copy and paste this link into your browser:&lt;br&gt;&lt;span class=\"link-in-text\"&gt;{verification_url}&lt;/span&gt;&lt;/p&gt;\n        &lt;p&gt;Sincerely,&lt;br&gt;The {APP_NAME} Team&lt;/p&gt;\n    \"\"\"\n    return sender.send_html_email(user_email, subject, content_html, preview_text)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.email_services.send_magic_link_email","title":"<code>send_magic_link_email(app, user_email, magic_link_url, username=None)</code>","text":"<p>Sends a magic link email for login.</p> Source code in <code>toolboxv2/mods/CloudM/email_services.py</code> <pre><code>@s_export\ndef send_magic_link_email(app: App, user_email: str, magic_link_url: str, username: str = None):\n    \"\"\"Sends a magic link email for login.\"\"\"\n    sender = EmailSender(app)\n    greeting_name = f\", {username}\" if username else \"\"\n    subject = f\"Your Magic Login Link for {APP_NAME}\"\n    preview_text = \"Securely access your account with this one-time link.\"\n\n    content_html = f\"\"\"\n        &lt;h2&gt;Hello{greeting_name}!&lt;/h2&gt;\n        &lt;p&gt;You requested a magic link to sign in to your {APP_NAME} account.&lt;/p&gt;\n        &lt;p&gt;Click the button below to log in. This link is temporary and will expire shortly.&lt;/p&gt;\n        &lt;a href=\"{magic_link_url}\" class=\"button\"&gt;Log In Securely&lt;/a&gt;\n        &lt;p&gt;If you did not request this link, please ignore this email. Your account is safe.&lt;/p&gt;\n        &lt;p&gt;If the button doesn't work, copy and paste this link into your browser:&lt;br&gt;&lt;span class=\"link-in-text\"&gt;{magic_link_url}&lt;/span&gt;&lt;/p&gt;\n        &lt;p&gt;Thanks,&lt;br&gt;The {APP_NAME} Team&lt;/p&gt;\n    \"\"\"\n    return sender.send_html_email(user_email, subject, content_html, preview_text)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.email_services.send_signup_invitation_email","title":"<code>send_signup_invitation_email(app, invited_user_email, invited_username, inviter_username=None)</code>","text":"<p>Generates an invitation link and sends it via email.</p> Source code in <code>toolboxv2/mods/CloudM/email_services.py</code> <pre><code>@s_export\ndef send_signup_invitation_email(app: App, invited_user_email: str, invited_username: str,\n                                 inviter_username: str = None):\n    \"\"\"Generates an invitation link and sends it via email.\"\"\"\n    sender = EmailSender(app)\n\n    # Generate invitation code as specified in the prompt\n    # This uses the Code class, assuming TB_R_KEY is set in the environment\n    invitation_code = Code.one_way_hash(invited_username, \"00#\", os.getenv(\"TB_R_KEY\", \"pepper123\"))[:12] + str(\n        uuid.uuid4())[:6]\n\n    # Construct the signup link URL (adjust your frontend signup path as needed)\n    signup_link_url = f\"{APP_BASE_URL}/signup?invitation={quote(invitation_code)}&amp;email={quote(invited_user_email)}&amp;username={quote(invited_username)}\"\n\n    subject = f\"You're Invited to Join {APP_NAME}!\"\n    preview_text = f\"{inviter_username or 'A friend'} has invited you to {APP_NAME}!\"\n    inviter_line = f\"&lt;p&gt;{inviter_username} has invited you to join.&lt;/p&gt;\" if inviter_username else \"&lt;p&gt;You've been invited to join.&lt;/p&gt;\"\n\n    content_html = f\"\"\"\n        &lt;h2&gt;Hello {invited_username},&lt;/h2&gt;\n        {inviter_line}\n        &lt;p&gt;{APP_NAME} is an exciting platform, and we'd love for you to be a part of it!&lt;/p&gt;\n        &lt;p&gt;Click the button below to accept the invitation and create your account:&lt;/p&gt;\n        &lt;a href=\"{signup_link_url}\" class=\"button\"&gt;Accept Invitation &amp; Sign Up&lt;/a&gt;\n        &lt;p&gt;This invitation is unique to you.&lt;/p&gt;\n        &lt;p&gt;If the button doesn't work, copy and paste this link into your browser:&lt;br&gt;&lt;span class=\"link-in-text\"&gt;{signup_link_url}&lt;/span&gt;&lt;/p&gt;\n        &lt;p&gt;We look forward to seeing you there!&lt;br&gt;The {APP_NAME} Team&lt;/p&gt;\n    \"\"\"\n    return sender.send_html_email(invited_user_email, subject, content_html, preview_text)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.email_services.send_waiting_list_confirmation_email","title":"<code>send_waiting_list_confirmation_email(app, user_email)</code>","text":"<p>Sends a confirmation email for joining the waiting list.</p> Source code in <code>toolboxv2/mods/CloudM/email_services.py</code> <pre><code>@s_export\ndef send_waiting_list_confirmation_email(app: App, user_email: str):\n    \"\"\"Sends a confirmation email for joining the waiting list.\"\"\"\n    sender = EmailSender(app)\n    subject = f\"You're on the Waiting List for {APP_NAME}!\"\n    preview_text = \"Thanks for your interest! We'll keep you updated.\"\n\n    content_html = f\"\"\"\n        &lt;h2&gt;You're In!&lt;/h2&gt;\n        &lt;p&gt;Thank you for joining the waiting list for {APP_NAME}. We're working hard to get things ready and appreciate your interest.&lt;/p&gt;\n        &lt;p&gt;We'll notify you as soon as we have updates or when access becomes available.&lt;/p&gt;\n        &lt;p&gt;In the meantime, you can follow our progress or learn more at &lt;a href=\"{APP_BASE_URL}\" class=\"link-in-text\"&gt;{APP_BASE_URL}&lt;/a&gt;.&lt;/p&gt;\n        &lt;p&gt;Stay tuned,&lt;br&gt;The {APP_NAME} Team&lt;/p&gt;\n    \"\"\"\n    return sender.send_html_email(user_email, subject, content_html, preview_text,\n                                  recipient_email_for_unsubscribe=user_email, show_unsubscribe_link=True)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.email_services.send_welcome_email","title":"<code>send_welcome_email(app, user_email, username, welcome_action_url=None)</code>","text":"<p>Sends a welcome email to a new user.</p> Source code in <code>toolboxv2/mods/CloudM/email_services.py</code> <pre><code>@s_export  # Changed to native, api=False as it's a backend function\ndef send_welcome_email(app: App, user_email: str, username: str, welcome_action_url: str = None):\n    \"\"\"Sends a welcome email to a new user.\"\"\"\n    sender = EmailSender(app)\n    subject = f\"Welcome to {APP_NAME}, {username}!\"\n    preview_text = f\"We're thrilled to have you, {username}!\"\n    action_url = welcome_action_url or f\"{APP_BASE_URL}/dashboard\"  # Default to dashboard\n\n    content_html = f\"\"\"\n        &lt;h2&gt;Welcome Aboard, {username}!&lt;/h2&gt;\n        &lt;p&gt;Thank you for signing up for {APP_NAME}. We're excited to have you join our community!&lt;/p&gt;\n        &lt;p&gt;Here are a few things you might want to do next:&lt;/p&gt;\n        &lt;ul&gt;\n            &lt;li&gt;Explore your new account features.&lt;/li&gt;\n            &lt;li&gt;Customize your profile.&lt;/li&gt;\n        &lt;/ul&gt;\n        &lt;p&gt;Click the button below to get started:&lt;/p&gt;\n        &lt;a href=\"{action_url}\" class=\"button\"&gt;Go to Your Dashboard&lt;/a&gt;\n        &lt;p&gt;If the button doesn't work, copy and paste this link into your browser:&lt;br&gt;&lt;span class=\"link-in-text\"&gt;{action_url}&lt;/span&gt;&lt;/p&gt;\n        &lt;p&gt;Best regards,&lt;br&gt;The {APP_NAME} Team&lt;/p&gt;\n    \"\"\"\n    return sender.send_html_email(user_email, subject, content_html, preview_text,\n                                  recipient_email_for_unsubscribe=user_email, show_unsubscribe_link=True)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.mini","title":"<code>mini</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.mini.check_multiple_processes","title":"<code>check_multiple_processes(pids)</code>","text":"<p>Checks the status of multiple processes in a single system call. Returns a dictionary mapping PIDs to their status (GREEN_CIRCLE, RED_CIRCLE, or YELLOW_CIRCLE).</p> Source code in <code>toolboxv2/mods/CloudM/mini.py</code> <pre><code>def check_multiple_processes(pids: list[int]) -&gt; dict[int, str]:\n    \"\"\"\n    Checks the status of multiple processes in a single system call.\n    Returns a dictionary mapping PIDs to their status (GREEN_CIRCLE, RED_CIRCLE, or YELLOW_CIRCLE).\n    \"\"\"\n    if not pids:\n        return {}\n\n    pid_status = {}\n\n    if os.name == 'nt':  # Windows\n        try:\n            # Windows tasklist requires separate /FI for each filter\n            command = 'tasklist'\n\n            # Add encoding handling for Windows\n            result = subprocess.run(\n                command,\n                capture_output=True,\n                text=True,\n                shell=True,\n                encoding='cp850'  # Use cp850 for Windows console output\n            )\n            # Create a set of running PIDs from the output\n            running_pids = set()\n            for line in result.stdout.lower().split('\\n'):\n                for pid in pids:\n                    if str(pid) in line:\n                        running_pids.add(pid)\n            # Assign status based on whether PID was found in output\n            for pid in pids:\n                if pid in running_pids:\n                    pid_status[pid] = GREEN_CIRCLE\n                else:\n                    pid_status[pid] = RED_CIRCLE\n\n        except subprocess.SubprocessError as e:\n            print(f\"SubprocessError: {e}\")  # For debugging\n            # Mark all as YELLOW_CIRCLE if there's an error running the command\n            for pid in pids:\n                pid_status[pid] = YELLOW_CIRCLE\n        except UnicodeDecodeError as e:\n            print(f\"UnicodeDecodeError: {e}\")  # For debugging\n            # Try alternate encoding if cp850 fails\n            try:\n                result = subprocess.run(\n                    command,\n                    capture_output=True,\n                    text=True,\n                    shell=True,\n                    encoding='utf-8'\n                )\n                running_pids = set()\n                for line in result.stdout.lower().split('\\n'):\n                    for pid in pids:\n                        if str(pid) in line:\n                            running_pids.add(pid)\n\n                for pid in pids:\n                    pid_status[pid] = GREEN_CIRCLE if pid in running_pids else RED_CIRCLE\n            except Exception as e:\n                print(f\"Failed with alternate encoding: {e}\")  # For debugging\n                for pid in pids:\n                    pid_status[pid] = YELLOW_CIRCLE\n\n    else:  # Unix/Linux/Mac\n        try:\n            pids_str = ','.join(str(pid) for pid in pids)\n            command = f'ps -p {pids_str} -o pid='\n\n            result = subprocess.run(\n                command,\n                capture_output=True,\n                text=True,\n                shell=True,\n                encoding='utf-8'\n            )\n            running_pids = set(int(pid) for pid in result.stdout.strip().split())\n\n            for pid in pids:\n                pid_status[pid] = GREEN_CIRCLE if pid in running_pids else RED_CIRCLE\n\n        except subprocess.SubprocessError as e:\n            print(f\"SubprocessError: {e}\")  # For debugging\n            for pid in pids:\n                pid_status[pid] = YELLOW_CIRCLE\n\n    return pid_status\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.mini.get_service_pids","title":"<code>get_service_pids(info_dir)</code>","text":"<p>Extracts service names and PIDs from pid files.</p> Source code in <code>toolboxv2/mods/CloudM/mini.py</code> <pre><code>def get_service_pids(info_dir):\n    \"\"\"Extracts service names and PIDs from pid files.\"\"\"\n    services = {}\n    pid_files = [f for f in os.listdir(info_dir) if re.match(r'(.+)-(.+)\\.pid', f)]\n    for pid_file in pid_files:\n        match = re.match(r'(.+)-(.+)\\.pid', pid_file)\n        if match:\n            services_type, service_name = match.groups()\n            # Read the PID from the file\n            with open(os.path.join(info_dir, pid_file)) as file:\n                pid = file.read().strip()\n                # Store the PID using a formatted key\n                services[f\"{service_name} - {services_type}\"] = int(pid)\n    return services\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.mini.get_service_status","title":"<code>get_service_status(dir)</code>","text":"<p>Displays the status of all services.</p> Source code in <code>toolboxv2/mods/CloudM/mini.py</code> <pre><code>def get_service_status(dir: str) -&gt; str:\n    \"\"\"Displays the status of all services.\"\"\"\n    if time.time()-services_data_sto_last_update_time[0] &gt; 30:\n        services = get_service_pids(dir)\n        services_data_sto[0] = services\n        services_data_sto_last_update_time[0] = time.time()\n    else:\n        services = services_data_sto[0]\n    if not services:\n        return \"No services found\"\n\n    # Get status for all PIDs in a single call\n    pid_statuses = check_multiple_processes(list(services.values()))\n\n    # Build the status string\n    res_s = \"Service(s):\" + (\"\\n\" if len(services) &gt; 1 else ' ')\n    for service_name, pid in services.items():\n        status = pid_statuses.get(pid, YELLOW_CIRCLE)\n        res_s += f\"{status} {service_name} (PID: {pid})\\n\"\n    services_data_display[0] = res_s.strip()\n    return res_s.rstrip()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.module","title":"<code>module</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.module.hash_password","title":"<code>hash_password(password)</code>","text":"<p>Hash a password for storing.</p> Source code in <code>toolboxv2/mods/CloudM/module.py</code> <pre><code>def hash_password(password):\n    \"\"\"Hash a password for storing.\"\"\"\n    salt = hashlib.sha256(os.urandom(60)).hexdigest().encode('ascii')\n    pwdhash = hashlib.pbkdf2_hmac('sha512', password.encode('utf-8'), salt,\n                                  100000)\n    pwdhash = binascii.hexlify(pwdhash)\n    return (salt + pwdhash).decode('ascii')\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.module.verify_password","title":"<code>verify_password(stored_password, provided_password)</code>","text":"<p>Verify a stored password against one provided by user</p> Source code in <code>toolboxv2/mods/CloudM/module.py</code> <pre><code>def verify_password(stored_password, provided_password):\n    \"\"\"Verify a stored password against one provided by user\"\"\"\n    salt = stored_password[:64]\n    stored_password = stored_password[64:]\n    pwdhash = hashlib.pbkdf2_hmac('sha512', provided_password.encode('utf-8'),\n                                  salt.encode('ascii'), 100000)\n    pwdhash = binascii.hexlify(pwdhash).decode('ascii')\n    return pwdhash == stored_password\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification","title":"<code>CodeVerification</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification.VerificationSystem","title":"<code>VerificationSystem</code>","text":"Source code in <code>toolboxv2/mods/CodeVerification.py</code> <pre><code>class VerificationSystem:\n    def __init__(self, tools_db, scope=\"main\"):\n        \"\"\"\n        Initialize VerificationSystem with DB Tools integration\n\n        Args:\n            tools_db (Tools): Database tools from toolboxv2.mods.DB\n            scope (str, optional): Scope for templates and codes. Defaults to \"main\".\n        \"\"\"\n        self.tools_db = tools_db\n        self.scope = scope\n        self.tidmp = {}\n        self._ensure_scope_templates()\n\n    def get(self):\n        return self\n\n    def reset_scope_templates(self):\n        \"\"\"\n        Ensure a templates dictionary exists for the current scope in the database\n        \"\"\"\n        templates_key = f\"verification_templates_{self.scope}\"\n\n        self.tools_db.set(templates_key, json.dumps({}))\n\n    def _ensure_scope_templates(self):\n        \"\"\"\n        Ensure a templates dictionary exists for the current scope in the database\n        \"\"\"\n        templates_key = f\"verification_templates_{self.scope}\"\n\n        # Check if templates exist for this scope\n        templates_exist = self.tools_db.if_exist(templates_key)\n\n        if templates_exist.is_error() and not templates_exist.is_data():\n            # Initialize empty templates dictionary if not exists\n            self.tools_db.set(templates_key, json.dumps({}))\n        else:\n            allt = self.get_all_templates()\n\n            for k, v in allt.items():\n                if 'name' not in v:\n                    continue\n                self.tidmp[v['name']] = k\n\n    def add_config_template(self, template: ConfigTemplate) -&gt; str:\n        \"\"\"\n        Add a new configuration template to the database\n\n        Args:\n            template (ConfigTemplate): The configuration template\n\n        Returns:\n            str: Unique identifier of the template\n        \"\"\"\n        # Ensure template has the current scope\n        template.scope = self.scope\n\n        # Generate a unique template ID\n        template_id = secrets.token_urlsafe(8)\n\n        # Get existing templates for this scope\n        templates = self.get_all_templates()\n\n        # Add new template\n        self.tidmp[template.name] = template_id\n        templates[template_id] = asdict(template)\n\n        # Save updated templates back to database\n        templates_key = f\"verification_templates_{self.scope}\"\n        save_result = self.tools_db.set(templates_key, json.dumps(templates))\n\n        if save_result.is_error():\n            raise ValueError(\"Could not save template\")\n\n        return template_id\n\n    def get_all_templates(self):\n        templates_key = f\"verification_templates_{self.scope}\"\n        templates_result = self.tools_db.get(templates_key)\n\n        if not templates_result.is_error() and templates_result.is_data():\n            try:\n                templates_result.result.data = json.loads(templates_result.get())\n            except Exception as e:\n                templates_result.print()\n                print(f\"Errro loding template data curupted : {str(e)}\")\n                templates_result.result.data = {}\n        else:\n            templates_result.result.data = {}\n        if not isinstance(templates_result, dict):\n            templates_result = templates_result.result.data\n        return templates_result\n\n    def generate_code(self, template_id: str) -&gt; str:\n        \"\"\"\n        Generate a code based on the configuration template\n\n        Args:\n            template_id (str): ID of the configuration template\n\n        Returns:\n            str: Generated verification code\n        \"\"\"\n        # Get templates for this scope\n        templates = self.get_all_templates()\n        print(templates, self.tidmp, template_id)\n        if template_id not in templates:\n            template_id = self.tidmp.get(template_id, template_id)\n        if template_id not in templates:\n            raise ValueError(\"Invalid configuration template\")\n\n        template_dict = templates[template_id]\n        ConfigTemplate(**template_dict)\n\n        # Generate a random code with max 16 characters\n        code = secrets.token_urlsafe(10)[:16]\n\n        # Prepare code information\n        code_info = {\n            'template_id': template_id,\n            'created_at': time.time(),\n            'uses_count': 0,\n            'scope': self.scope\n        }\n\n        # Store code information in database\n        codes_key = f\"verification_codes_{self.scope}\"\n        existing_codes_result = self.tools_db.get(codes_key)\n\n        existing_codes = {}\n        if not existing_codes_result.is_error() and existing_codes_result.is_data():\n            d = existing_codes_result.get()\n            if isinstance(d, list):\n                d = d[0]\n            existing_codes = json.loads(d)\n\n        existing_codes[code] = code_info\n\n        save_result = self.tools_db.set(codes_key, json.dumps(existing_codes))\n\n        if save_result.is_error():\n            raise ValueError(\"Could not save generated code\")\n\n        return code\n\n    def validate_code(self, code: str) -&gt; dict[str, Any] | None:\n        \"\"\"\n        Validate a code and return template information\n\n        Args:\n            code (str): Code to validate\n\n        Returns:\n            Optional[Dict[str, Any]]: Template information for valid code, else None\n        \"\"\"\n        # Get codes for this scope\n        codes_key = f\"verification_codes_{self.scope}\"\n        codes_result = self.tools_db.get(codes_key)\n\n        if codes_result.is_error() or not codes_result.is_data():\n            return None\n\n        d = codes_result.get()\n        if isinstance(d, list):\n            d = d[0]\n        existing_codes = json.loads(d)\n\n        if code not in existing_codes:\n            return None\n\n        code_info = existing_codes[code]\n\n        # Check if code is from the same scope\n        if code_info.get('scope') != self.scope:\n            return None\n\n        # Get templates for this scope\n        templates = self.get_all_templates()\n        template_id = code_info['template_id']\n\n        if template_id not in templates:\n            return templates\n\n        template_dict = templates[template_id]\n        template = ConfigTemplate(**template_dict)\n\n        # Check usage count\n        if code_info['uses_count'] &gt;= template.max_uses:\n            del existing_codes[code]\n            self.tools_db.set(codes_key, json.dumps(existing_codes))\n            return None\n\n        # Check time validity for timed codes\n        if template.usage_type == 'timed':\n            current_time = time.time()\n            if template.valid_duration and (current_time - code_info['created_at']) &gt; template.valid_duration:\n                del existing_codes[code]\n                self.tools_db.set(codes_key, json.dumps(existing_codes))\n                return None\n\n        # Update uses count\n        existing_codes[code]['uses_count'] += 1\n        uses_count = existing_codes[code].get('uses_count', 1)\n        # Remove code if it's a one-time use\n        if template.usage_type == 'one_time':\n            del existing_codes[code]\n\n        # Save updated codes\n        self.tools_db.set(codes_key, json.dumps(existing_codes))\n\n        return {\n            'template_name': template.name,\n            'usage_type': template.usage_type,\n            'uses_count': uses_count\n        }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification.VerificationSystem.__init__","title":"<code>__init__(tools_db, scope='main')</code>","text":"<p>Initialize VerificationSystem with DB Tools integration</p> <p>Parameters:</p> Name Type Description Default <code>tools_db</code> <code>Tools</code> <p>Database tools from toolboxv2.mods.DB</p> required <code>scope</code> <code>str</code> <p>Scope for templates and codes. Defaults to \"main\".</p> <code>'main'</code> Source code in <code>toolboxv2/mods/CodeVerification.py</code> <pre><code>def __init__(self, tools_db, scope=\"main\"):\n    \"\"\"\n    Initialize VerificationSystem with DB Tools integration\n\n    Args:\n        tools_db (Tools): Database tools from toolboxv2.mods.DB\n        scope (str, optional): Scope for templates and codes. Defaults to \"main\".\n    \"\"\"\n    self.tools_db = tools_db\n    self.scope = scope\n    self.tidmp = {}\n    self._ensure_scope_templates()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification.VerificationSystem.add_config_template","title":"<code>add_config_template(template)</code>","text":"<p>Add a new configuration template to the database</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>ConfigTemplate</code> <p>The configuration template</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unique identifier of the template</p> Source code in <code>toolboxv2/mods/CodeVerification.py</code> <pre><code>def add_config_template(self, template: ConfigTemplate) -&gt; str:\n    \"\"\"\n    Add a new configuration template to the database\n\n    Args:\n        template (ConfigTemplate): The configuration template\n\n    Returns:\n        str: Unique identifier of the template\n    \"\"\"\n    # Ensure template has the current scope\n    template.scope = self.scope\n\n    # Generate a unique template ID\n    template_id = secrets.token_urlsafe(8)\n\n    # Get existing templates for this scope\n    templates = self.get_all_templates()\n\n    # Add new template\n    self.tidmp[template.name] = template_id\n    templates[template_id] = asdict(template)\n\n    # Save updated templates back to database\n    templates_key = f\"verification_templates_{self.scope}\"\n    save_result = self.tools_db.set(templates_key, json.dumps(templates))\n\n    if save_result.is_error():\n        raise ValueError(\"Could not save template\")\n\n    return template_id\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification.VerificationSystem.generate_code","title":"<code>generate_code(template_id)</code>","text":"<p>Generate a code based on the configuration template</p> <p>Parameters:</p> Name Type Description Default <code>template_id</code> <code>str</code> <p>ID of the configuration template</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated verification code</p> Source code in <code>toolboxv2/mods/CodeVerification.py</code> <pre><code>def generate_code(self, template_id: str) -&gt; str:\n    \"\"\"\n    Generate a code based on the configuration template\n\n    Args:\n        template_id (str): ID of the configuration template\n\n    Returns:\n        str: Generated verification code\n    \"\"\"\n    # Get templates for this scope\n    templates = self.get_all_templates()\n    print(templates, self.tidmp, template_id)\n    if template_id not in templates:\n        template_id = self.tidmp.get(template_id, template_id)\n    if template_id not in templates:\n        raise ValueError(\"Invalid configuration template\")\n\n    template_dict = templates[template_id]\n    ConfigTemplate(**template_dict)\n\n    # Generate a random code with max 16 characters\n    code = secrets.token_urlsafe(10)[:16]\n\n    # Prepare code information\n    code_info = {\n        'template_id': template_id,\n        'created_at': time.time(),\n        'uses_count': 0,\n        'scope': self.scope\n    }\n\n    # Store code information in database\n    codes_key = f\"verification_codes_{self.scope}\"\n    existing_codes_result = self.tools_db.get(codes_key)\n\n    existing_codes = {}\n    if not existing_codes_result.is_error() and existing_codes_result.is_data():\n        d = existing_codes_result.get()\n        if isinstance(d, list):\n            d = d[0]\n        existing_codes = json.loads(d)\n\n    existing_codes[code] = code_info\n\n    save_result = self.tools_db.set(codes_key, json.dumps(existing_codes))\n\n    if save_result.is_error():\n        raise ValueError(\"Could not save generated code\")\n\n    return code\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification.VerificationSystem.reset_scope_templates","title":"<code>reset_scope_templates()</code>","text":"<p>Ensure a templates dictionary exists for the current scope in the database</p> Source code in <code>toolboxv2/mods/CodeVerification.py</code> <pre><code>def reset_scope_templates(self):\n    \"\"\"\n    Ensure a templates dictionary exists for the current scope in the database\n    \"\"\"\n    templates_key = f\"verification_templates_{self.scope}\"\n\n    self.tools_db.set(templates_key, json.dumps({}))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification.VerificationSystem.validate_code","title":"<code>validate_code(code)</code>","text":"<p>Validate a code and return template information</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>Code to validate</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>Optional[Dict[str, Any]]: Template information for valid code, else None</p> Source code in <code>toolboxv2/mods/CodeVerification.py</code> <pre><code>def validate_code(self, code: str) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Validate a code and return template information\n\n    Args:\n        code (str): Code to validate\n\n    Returns:\n        Optional[Dict[str, Any]]: Template information for valid code, else None\n    \"\"\"\n    # Get codes for this scope\n    codes_key = f\"verification_codes_{self.scope}\"\n    codes_result = self.tools_db.get(codes_key)\n\n    if codes_result.is_error() or not codes_result.is_data():\n        return None\n\n    d = codes_result.get()\n    if isinstance(d, list):\n        d = d[0]\n    existing_codes = json.loads(d)\n\n    if code not in existing_codes:\n        return None\n\n    code_info = existing_codes[code]\n\n    # Check if code is from the same scope\n    if code_info.get('scope') != self.scope:\n        return None\n\n    # Get templates for this scope\n    templates = self.get_all_templates()\n    template_id = code_info['template_id']\n\n    if template_id not in templates:\n        return templates\n\n    template_dict = templates[template_id]\n    template = ConfigTemplate(**template_dict)\n\n    # Check usage count\n    if code_info['uses_count'] &gt;= template.max_uses:\n        del existing_codes[code]\n        self.tools_db.set(codes_key, json.dumps(existing_codes))\n        return None\n\n    # Check time validity for timed codes\n    if template.usage_type == 'timed':\n        current_time = time.time()\n        if template.valid_duration and (current_time - code_info['created_at']) &gt; template.valid_duration:\n            del existing_codes[code]\n            self.tools_db.set(codes_key, json.dumps(existing_codes))\n            return None\n\n    # Update uses count\n    existing_codes[code]['uses_count'] += 1\n    uses_count = existing_codes[code].get('uses_count', 1)\n    # Remove code if it's a one-time use\n    if template.usage_type == 'one_time':\n        del existing_codes[code]\n\n    # Save updated codes\n    self.tools_db.set(codes_key, json.dumps(existing_codes))\n\n    return {\n        'template_name': template.name,\n        'usage_type': template.usage_type,\n        'uses_count': uses_count\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB","title":"<code>DB</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.DB.local_instance","title":"<code>local_instance</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.DB.local_instance.load_from_json","title":"<code>load_from_json(filename)</code>","text":"<p>L\u00e4dt Daten aus einer JSON-Datei.</p> <p>:param filename: Der Dateiname oder Pfad der zu ladenden Datei. :return: Die geladenen Daten.</p> Source code in <code>toolboxv2/mods/DB/local_instance.py</code> <pre><code>def load_from_json(filename):\n    \"\"\"\n    L\u00e4dt Daten aus einer JSON-Datei.\n\n    :param filename: Der Dateiname oder Pfad der zu ladenden Datei.\n    :return: Die geladenen Daten.\n    \"\"\"\n    if not os.path.exists(filename):\n        return {'data': ''}\n\n    with open(filename) as file:\n        return json.load(file)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.local_instance.save_to_json","title":"<code>save_to_json(data, filename)</code>","text":"<p>Speichert die \u00fcbergebenen Daten in einer JSON-Datei.</p> <p>:param data: Die zu speichernden Daten. :param filename: Der Dateiname oder Pfad, in dem die Daten gespeichert werden sollen.</p> Source code in <code>toolboxv2/mods/DB/local_instance.py</code> <pre><code>def save_to_json(data, filename):\n    \"\"\"\n    Speichert die \u00fcbergebenen Daten in einer JSON-Datei.\n\n    :param data: Die zu speichernden Daten.\n    :param filename: Der Dateiname oder Pfad, in dem die Daten gespeichert werden sollen.\n    \"\"\"\n    if not os.path.exists(filename):\n        open(filename, 'a').close()\n\n    with open(filename, 'w+') as file:\n        json.dump(data, file, indent=4)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.reddis_instance","title":"<code>reddis_instance</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.DB.reddis_instance.sync_redis_databases","title":"<code>sync_redis_databases(source_url, target_url)</code>","text":"<p>Synchronize keys from the source Redis database to the target Redis database. This function scans all keys in the source DB and uses DUMP/RESTORE to replicate data to the target.</p> <p>Parameters:</p> Name Type Description Default <code>source_url</code> <code>str</code> <p>The Redis URL of the source database.</p> required <code>target_url</code> <code>str</code> <p>The Redis URL of the target database.</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The number of keys successfully synchronized.</p> Source code in <code>toolboxv2/mods/DB/reddis_instance.py</code> <pre><code>def sync_redis_databases(source_url, target_url):\n    \"\"\"Synchronize keys from the source Redis database to the target Redis database.\n    This function scans all keys in the source DB and uses DUMP/RESTORE to replicate data to the target.\n\n    Args:\n        source_url (str): The Redis URL of the source database.\n        target_url (str): The Redis URL of the target database.\n\n    Returns:\n        int: The number of keys successfully synchronized.\n    \"\"\"\n    try:\n        src_client = redis.from_url(source_url)\n        tgt_client = redis.from_url(target_url)\n    except Exception as e:\n        print(f\"Error connecting to one of the Redis instances: {e}\")\n        return 0\n\n    total_synced = 0\n    cursor = 0\n    try:\n        while True:\n            cursor, keys = src_client.scan(cursor=cursor, count=100)\n            for key in keys:\n                try:\n                    serialized_value = src_client.dump(key)\n                    if serialized_value is None:\n                        continue\n                    # Restore key with TTL=0 and replace existing key\n                    tgt_client.restore(key, 0, serialized_value, replace=True)\n                    total_synced += 1\n                except Exception as e:\n                    print(f\"Error syncing key {key}: {e}\")\n            if cursor == 0:\n                break\n    except Exception as scan_error:\n        print(f\"Error during scanning keys: {scan_error}\")\n\n    print(f\"Synced {total_synced} keys from {source_url} to {target_url}\")\n    return total_synced\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter","title":"<code>tb_adapter</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB","title":"<code>DB</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>class DB(ABC):\n    @abc.abstractmethod\n    def get(self, query: str) -&gt; Result:\n        \"\"\"get data\"\"\"\n\n    @abc.abstractmethod\n    def set(self, query: str, value) -&gt; Result:\n        \"\"\"set data\"\"\"\n\n    @abc.abstractmethod\n    def append_on_set(self, query: str, value) -&gt; Result:\n        \"\"\"append set data\"\"\"\n\n    @abc.abstractmethod\n    def delete(self, query: str, matching=False) -&gt; Result:\n        \"\"\"delete data\"\"\"\n\n    @abc.abstractmethod\n    def if_exist(self, query: str) -&gt; bool:\n        \"\"\"return True if query exists\"\"\"\n\n    @abc.abstractmethod\n    def exit(self) -&gt; Result:\n        \"\"\"Close DB connection and optional save data\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB.append_on_set","title":"<code>append_on_set(query, value)</code>  <code>abstractmethod</code>","text":"<p>append set data</p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>@abc.abstractmethod\ndef append_on_set(self, query: str, value) -&gt; Result:\n    \"\"\"append set data\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB.delete","title":"<code>delete(query, matching=False)</code>  <code>abstractmethod</code>","text":"<p>delete data</p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>@abc.abstractmethod\ndef delete(self, query: str, matching=False) -&gt; Result:\n    \"\"\"delete data\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB.exit","title":"<code>exit()</code>  <code>abstractmethod</code>","text":"<p>Close DB connection and optional save data</p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>@abc.abstractmethod\ndef exit(self) -&gt; Result:\n    \"\"\"Close DB connection and optional save data\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB.get","title":"<code>get(query)</code>  <code>abstractmethod</code>","text":"<p>get data</p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>@abc.abstractmethod\ndef get(self, query: str) -&gt; Result:\n    \"\"\"get data\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB.if_exist","title":"<code>if_exist(query)</code>  <code>abstractmethod</code>","text":"<p>return True if query exists</p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>@abc.abstractmethod\ndef if_exist(self, query: str) -&gt; bool:\n    \"\"\"return True if query exists\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB.set","title":"<code>set(query, value)</code>  <code>abstractmethod</code>","text":"<p>set data</p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>@abc.abstractmethod\ndef set(self, query: str, value) -&gt; Result:\n    \"\"\"set data\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.EventManager","title":"<code>EventManager</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.EventManager.module","title":"<code>module</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.EventManager.module.EventManagerClass","title":"<code>EventManagerClass</code>","text":"Source code in <code>toolboxv2/mods/EventManager/module.py</code> <pre><code>class EventManagerClass:\n    events: set[Event] = set()\n    source_id: str\n    _name: str\n    _identification: str\n\n    routes_client: dict[str, ProxyRout] = {}\n    routers_servers: dict[str, DaemonRout] = {}\n    routers_servers_tasks: list[Any] = []\n    routers_servers_tasks_running_flag: bool = False\n\n    receiver_que: queue.Queue\n    response_que: queue.Queue\n\n    def add_c_route(self, name, route: ProxyRout):\n        self.routes_client[name] = route\n\n    async def receive_all_client_data(self):\n\n        close_connections = []\n        add_ev = []\n        for name, client in self.routes_client.items():\n            if client.client is None or not client.client.get('alive', False):\n                close_connections.append(name)\n                continue\n            data = client.r\n\n            if isinstance(data, str) and data == \"No data\":\n                continue\n            elif isinstance(data, EventID) and len(data.get_source()) != 0:\n                await self.trigger_event(data)\n            elif isinstance(data, EventID) and len(data.get_source()) == 0:\n                print(f\"Event returned {data.payload}\")\n                self.response_que.put(data)\n            elif isinstance(data,\n                            dict) and 'error' in data and 'origin' in data and 'result' in data and 'info' in data:\n\n                self.response_que.put(Result.result_from_dict(**data).print())\n            elif isinstance(data,\n                            dict) and 'source' in data and 'path' in data and 'ID' in data and 'identifier' in data:\n                del data['identifier']\n                ev_id = EventID(**data)\n                await self.trigger_event(ev_id)\n            elif isinstance(data, Event):\n                print(\"Event:\", str(data.event_id), data.name)\n                add_ev.append(data)\n            elif isinstance(data, Result):\n                self.response_que.put(data.print())\n            else:\n                print(f\"Unknown Data {data}\")\n\n        for ev in add_ev:\n            await self.register_event(ev)\n\n        for client_name in close_connections:\n            print(f\"Client {client_name} closing connection\")\n            self.remove_c_route(client_name)\n\n    def remove_c_route(self, name):\n        self.routes_client[name].close()\n        del self.routes_client[name]\n\n    def crate_rout(self, source, addr=None):\n        if addr is None:\n            addr = ('0.0.0.0', 6588)\n        host, port = addr\n        if isinstance(port, str):\n            port = int(port)\n        return Rout(\n            _from=self.source_id,\n            _to=source,\n            _from_port=int(os.getenv(\"TOOLBOXV2_BASE_PORT\", 6588)),\n            _from_host=os.getenv(\"TOOLBOXV2_BASE_HOST\"),\n            _to_port=port,\n            _to_host=host,\n            routing_function=self.routing_function_router,\n        )\n\n    def __init__(self, source_id, _identification=\"PN\"):\n        self.bo = False\n        self.running = False\n        self.source_id = source_id\n        self.receiver_que = queue.Queue()\n        self.response_que = queue.Queue()\n        self._identification = _identification\n        self._name = self._identification + '-' + str(uuid.uuid4()).split('-')[1]\n        self.routes = {}\n        self.logger = get_logger()\n\n    @property\n    def identification(self) -&gt; str:\n        return self._identification\n\n    @identification.setter\n    def identification(self, _identification: str):\n        self.stop()\n        self._identification = _identification\n        self._name = self._identification + '-' + str(uuid.uuid4()).split('-')[1]\n\n    async def identity_post_setter(self):\n\n        do_reconnect = len(list(self.routers_servers.keys())) &gt; 0\n        if self._identification == \"P0\":\n            await self.add_server_route(self._identification, ('0.0.0.0', 6568))\n        if self._identification == \"P0|S0\":\n            await self.add_server_route(self._identification, ('0.0.0.0', 6567))\n\n        await asyncio.sleep(0.1)\n        self.start()\n        await asyncio.sleep(0.1)\n        if do_reconnect:\n            self.reconnect(\"ALL\")\n\n    async def open_connection_server(self, port):\n        await self.add_server_route(self._identification, ('0.0.0.0', port))\n\n    def start(self):\n        self.running = True\n        threading.Thread(target=async_test(self.receiver), daemon=True).start()\n\n    def make_event_from_fuction(self, fuction, name, *args, source_types=SourceTypes.F,\n                                scope=Scope.local,\n                                exec_in=ExecIn.local,\n                                threaded=False, **kwargs):\n\n        return Event(source=fuction,\n                     name=name,\n                     event_id=EventID.crate_with_source(self.source_id), args=args,\n                     kwargs_=kwargs,\n                     source_types=source_types,\n                     scope=scope,\n                     exec_in=exec_in,\n                     threaded=threaded,\n                     )\n\n    async def add_client_route(self, source_id, addr):\n        if source_id in self.routes_client:\n            if self.routes_client[source_id].client is None or not self.routes_client[source_id].client.get('alive'):\n                await self.routes_client[source_id].reconnect()\n                return True\n            print(\"Already connected\")\n            return False\n        try:\n            pr = await ProxyRout.toProxy(rout=self.crate_rout(source_id, addr=addr), name=source_id)\n            await asyncio.sleep(0.1)\n            await pr.client.get('sender')({\"id\": self._identification,\n                                           \"continue\": False,\n                                           \"key\": os.getenv('TB_R_KEY', 'root@remote')})\n            await asyncio.sleep(0.1)\n            self.add_c_route(source_id, pr)\n            return True\n        except Exception as e:\n            print(f\"Check the port {addr} Sever likely not Online : {e}\")\n            return False\n\n    async def add_mini_client(self, name: str, addr: tuple[str, int]):\n\n        mini_proxy = await ProxyRout(class_instance=None, timeout=15, app=get_app(),\n                                     remote_functions=[\"\"], peer=False, name=name, do_connect=False)\n\n        async def _(x):\n            return await self.routers_servers[self._identification].send(x, addr)\n\n        mini_proxy.put_data = _\n        mini_proxy.connect = lambda *x, **_: None\n        mini_proxy.reconnect = lambda *x, **_: None\n        mini_proxy.close = lambda *x, **_: None\n        mini_proxy.client = {'alive': True}\n        mini_proxy.r = \"No data\"\n        self.routes_client[name] = mini_proxy\n\n    async def on_register(self, id_, data):\n        try:\n            if \"unknown\" not in self.routes:\n                self.routes[\"unknown\"] = {}\n\n            if id_ != \"new_con\" and 'id' in data:\n                id_data = data.get('id')\n                id_ = eval(id_)\n                c_host, c_pot = id_\n                print(f\"Registering: new client {id_data} : {c_host, c_pot}\")\n                if id_data not in self.routes_client:\n                    await self.add_mini_client(id_data, (c_host, c_pot))\n                    self.routes[str((c_host, c_pot))] = id_data\n\n            # print(\"self.routes:\", self.routes)\n        except Exception as e:\n            print(\"Error in on_register\", str(e))\n\n    def on_client_exit(self, id_):\n\n        if isinstance(id_, str):\n            id_ = eval(id_)\n\n        c_name = self.routes.get(id_)\n\n        if c_name is None:\n            return\n\n        if c_name in self.routes_client:\n            self.remove_c_route(c_name)\n            print(f\"Removed route to {c_name}\")\n\n    async def add_server_route(self, source_id, addr=None):\n        if addr is None:\n            addr = ('0.0.0.0', 6588)\n        try:\n            self.routers_servers[source_id] = await DaemonRout(rout=self.crate_rout(source_id, addr=addr),\n                                                               name=source_id,\n                                                               on_r=self.on_register)\n            self.routers_servers_tasks.append(self.routers_servers[source_id].online)\n        except Exception as e:\n            print(f\"Sever already Online : {e}\")\n\n        if not self.routers_servers_tasks_running_flag:\n            self.routers_servers_tasks_running_flag = True\n            threading.Thread(target=self.server_route_runner, daemon=True).start()\n\n    def server_route_runner(self):\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n\n        # Sammle alle Ergebnisse zusammen\n        results = loop.run_until_complete(asyncio.gather(*self.routers_servers_tasks))\n\n        for result in results:\n            print(result)\n\n        loop.close()\n        self.routers_servers_tasks_running_flag = False\n\n    async def add_js_route(self, source_id=\"js:web\"):\n        await self.add_server_route(source_id, (\"./web/scripts/tb_socket.sock\", 0))\n\n    async def register_event(self, event: Event):\n\n        if event in self.events:\n            return Result.default_user_error(\"Event registration failed Event already registered\")\n\n        print(f\"Registration new Event : {event.name}, {str(event.event_id)}\")\n        self.events.add(event)\n\n        if event.scope.name == Scope.instance.name:\n            return\n\n        if event.scope.name == Scope.local.name:\n            if not self.bo and \"P0\" not in self.routes_client and os.getenv(\"TOOLBOXV2_BASE_HOST\",\n                                                                            \"localhost\") != \"localhost\":\n                await self.add_client_route(\"P0\", (os.getenv(\"TOOLBOXV2_BASE_HOST\", \"localhost\"),\n                                                   os.getenv(\"TOOLBOXV2_BASE_PORT\", 6568)))\n                self.bo = True\n            return\n\n        if event.scope.name == Scope.local_network.name:\n            if self.identification == \"P0\" and not self.bo:\n                t0 = threading.Thread(target=self.start_brodcast_router_local_network, daemon=True)\n                t0.start()\n            elif not self.bo and \"P0\" not in self.routes_client and os.getenv(\"TOOLBOXV2_BASE_HOST\",\n                                                                              \"localhost\") == \"localhost\":\n                self.bo = True\n                # self.add_server_route(self.identification, (\"127.0.0.1\", 44667))\n                with Spinner(message=\"Sercheing for Rooter instance\", count_down=True, time_in_s=6):\n                    with ThreadPoolExecutor(max_workers=1) as executor:\n                        t0 = executor.submit(make_known, self.identification)\n                        try:\n                            data = t0.result(timeout=6)\n                        except TimeoutError:\n                            print(\"No P0 found in network or on device\")\n                            return\n                    print(f\"Found P0 on {type(data)} {data.get('host')}\")\n                    await self.add_client_route(\"P0\", (data.get(\"host\"), os.getenv(\"TOOLBOXV2_BASE_PORT\", 6568)))\n            elif not self.bo and \"P0\" not in self.routes_client and os.getenv(\"TOOLBOXV2_BASE_HOST\",\n                                                                              \"localhost\") != \"localhost\":\n                do = await self.add_client_route(\"P0\", (\n                    os.getenv(\"TOOLBOXV2_BASE_HOST\", \"localhost\"), os.getenv(\"TOOLBOXV2_BASE_PORT\", 6568)))\n                self.bo = do\n                if not do:\n                    print(\"Connection failed\")\n                    os.environ[\"TOOLBOXV2_BASE_HOST\"] = \"localhost\"\n\n        if event.scope.name == Scope.global_network.name:\n            await self.add_server_route(self.source_id, ('0.0.0.0', os.getenv(\"TOOLBOXV2_REMOTE_PORT\", 6587)))\n\n    async def connect_to_remote(self, host=os.getenv(\"TOOLBOXV2_REMOTE_IP\"),\n                                port=os.getenv(\"TOOLBOXV2_REMOTE_PORT\", 6587)):\n        await self.add_client_route(\"S0\", (host, port))\n\n    def start_brodcast_router_local_network(self):\n        self.bo = True\n\n        # print(\"Starting brodcast router 0\")\n        router = start_client(get_local_ip())\n        # print(\"Starting brodcast router 1\")\n        # next(router)\n        # print(\"Starting brodcast router\")\n        while self.running:\n            source_id, connection = next(router)\n            print(f\"Infos :{source_id}, connection :{connection}\")\n            self.routes[source_id] = connection[0]\n            router.send(self.running)\n\n        router.send(\"e\")\n        router.close()\n\n    def _get_event_by_id_or_name(self, event_id: str or EventID):\n        if isinstance(event_id, str):\n            events = [e for e in self.events if e.name == event_id]\n            if len(events) &lt; 1:\n                return Result.default_user_error(\"Event not registered\")\n            event = events[0]\n\n        elif isinstance(event_id, EventID):\n            events = [e for e in self.events if e.event_id.ID == event_id.ID]\n            if len(events) &lt; 1:\n                events = [e for e in self.events if e.name == event_id.ID]\n            if len(events) &lt; 1:\n                return Result.default_user_error(\"Event not registered\")\n            event = events[0]\n\n        elif isinstance(event_id, Event):\n            if event_id not in self.events:\n                return Result.default_user_error(\"Event not registered\")\n            event = event_id\n\n        else:\n            event = Result.default_user_error(\"Event not registered\")\n\n        return event\n\n    def remove_event(self, event: Event or EventID or str):\n\n        event = self._get_event_by_id_or_name(event)\n        if isinstance(event, Event):\n            self.events.remove(event)\n        else:\n            return event\n\n    async def _trigger_local(self, event_id: EventID):\n        \"\"\"\n        Exec source based on\n\n        source_types\n            F -&gt; call directly\n            R -&gt; use get_app(str(event_id)).run_any(*args, **kwargs)\n            S -&gt; evaluate string\n        scope\n            instance -&gt; _trigger_local\n            local -&gt; if you ar proxy app run the event through get_app(str(event_id)).run_any(TBEF.EventManager._trigger_local, args=args, kwargs=kwargs, get_result=True)\n            local_network -&gt; use proxy0 app to communicate withe Daemon0 then local\n            global_network -&gt;\n        exec_in\n        event_id\n        threaded\n\n                       \"\"\"\n        event = self._get_event_by_id_or_name(event_id)\n\n        if isinstance(event, Result):\n            event.print()\n            if self.identification == \"P0\":\n                return event\n            print(f\"Routing to P0 {self.events}\")\n            if self.source_id not in self.routes_client:\n                # self.routers[self.source_id] = DaemonRout(rout=self.crate_rout(self.source_id))\n                await self.add_client_route(\"P0\", ('127.0.0.1', 6568))\n            return await self.route_event_id(event_id)\n\n        # if event.threaded:\n        #    threading.Thread(target=self.runner, args=(event, event_id), daemon=True).start()\n        #    return \"Event running In Thread\"\n        # else:\n\n        return await self.runner(event, event_id)\n\n    async def runner(self, event, event_id):\n\n        if event.source_types.name is SourceTypes.P.name:\n            return event.source(*event.args, payload=event_id, **event.kwargs_)\n\n        if event.source_types.name is SourceTypes.F.name:\n            return event.source(*event.args, **event.kwargs_)\n\n        if event.source_types.name is SourceTypes.R.name:\n            return get_app(str(event_id)).run_any(mod_function_name=event.source, get_results=True, args_=event.args,\n                                                  kwargs_=event.kwargs_)\n\n        if event.source_types.name is SourceTypes.AP.name:\n            return await event.source(*event.args, payload=event_id, **event.kwargs_)\n\n        if event.source_types.name is SourceTypes.AF.name:\n            return await event.source(*event.args, **event.kwargs_)\n\n        if event.source_types.name is SourceTypes.AR.name:\n            return await get_app(str(event_id)).run_any(mod_function_name=event.source, get_results=True,\n                                                        args_=event.args,\n                                                        kwargs_=event.kwargs_)\n\n        if event.source_types.name is SourceTypes.S.name:\n            return eval(event.source, __locals={'app': get_app(str(event_id)), 'event': event, 'eventManagerC': self})\n\n    async def routing_function_router(self, event_id: EventID):\n\n        result = await self.trigger_event(event_id)\n\n        if result is None:\n            result = Result.default_user_error(\"Invalid Event ID\")\n\n        if isinstance(result, bytes | dict):\n            pass\n        elif isinstance(result, Result):\n            result.result.data_info = str(event_id)\n        elif isinstance(result, EventID):\n            result = Result.default_internal_error(\"Event not found\", data=result)\n        else:\n            result = Result.ok(data=result, data_info=\"&lt;automatic&gt;\", info=str(event_id.path))\n\n        if isinstance(result, str):\n            result = result.encode()\n\n        return result\n\n    async def trigger_evnet_by_name(self, name: str):\n        await self.trigger_event(EventID.crate_name_as_id(name=name))\n\n    async def trigger_event(self, event_id: EventID):\n        \"\"\"\n        Exec source based on\n\n        source_types\n            F -&gt; call directly\n            R -&gt; use get_app(str(event_id)).run_any(*args, **kwargs)\n            S -&gt; evaluate string\n        scope\n            instance -&gt; _trigger_local\n            local -&gt; if you ar proxy app run the event through get_app(str(event_id)).run_any(TBEF.EventManager._trigger_local, args=args, kwargs=kwargs, get_result=True)\n            local_network -&gt; use proxy0 app to communicate withe Daemon0 then local\n            global_network -&gt;\n        exec_in\n        event_id\n        threaded\n\n                       \"\"\"\n        # print(f\"event-id Ptah : {event_id.get_path()}\")\n        # print(f\"testing trigger_event for {event_id.get_source()} {event_id.get_source()[-1] == self.source_id} \")\n        print(str(event_id))\n        if event_id.get_source()[-1] == self.source_id:\n            payload = await self._trigger_local(event_id)\n            event_id.set_payload(payload)\n            if len(event_id.path) &gt; 1:\n                event_id.source = ':'.join([e.split(':')[0] for e in event_id.get_path() if e != \"E\"])\n                res = await self.route_event_id(event_id)\n                if isinstance(res, Result):\n                    res.print()\n                else:\n                    print(res)\n            return payload\n        return await self.route_event_id(event_id)\n\n    async def route_event_id(self, event_id: EventID):\n\n        # print(f\"testing route_event_id for {event_id.get_source()[-1]}\")\n        if event_id.get_source()[-1] == '*':  # self.identification == \"P0\" and\n            responses = []\n            event_id.source = ':'.join(event_id.get_source()[:-1])\n            event_id.add_path(f\"{self._name}({self.source_id})\")\n            data = asdict(event_id)\n            for name, rout_ in self.routes_client.items():\n                if name in event_id.path:\n                    continue\n                ret = await rout_.put_data(data)\n                responses.append(ret)\n            return responses\n        route = self.routes_client.get(event_id.get_source()[-1])\n        # print(\"route:\", route)\n        if route is None:\n            route = self.routes_client.get(event_id.get_path()[-1])\n        if route is None:\n            return event_id.add_path((\"\" if len(event_id.get_source()) == 1 else \"404#\")+self.identification)\n        time.sleep(0.25)\n        event_id.source = ':'.join(event_id.get_source()[:-1])\n        event_id.add_path(f\"{self._name}({self.source_id})\")\n        return await route.put_data(asdict(event_id))\n\n    async def receiver(self):\n\n        t0 = time.time()\n\n        while self.running:\n            time.sleep(0.25)\n            if not self.receiver_que.empty():\n                event_id = self.receiver_que.get()\n                print(\"Receiver Event\", str(event_id))\n                await self.trigger_event(event_id)\n\n            if time.time() - t0 &gt; 5:\n                await self.receive_all_client_data()\n                t0 = time.time()\n\n    def info(self):\n        return {\"source\": self.source_id, \"known_routs:\": self.routers_servers, \"_router\": self.routes_client,\n                \"events\": self.events}\n\n    def stop(self):\n        self.running = False\n        list(map(lambda x: x.disconnect(), self.routes_client.values()))\n        list(map(lambda x: x.stop(), self.routers_servers.values()))\n\n    def reconnect(self, name):\n        if name is None:\n            pass\n        elif name in self.routes_client:\n            self.routes_client[name].reconnect()\n            return\n        list(map(lambda x: x.reconnect(), self.routes_client.values()))\n\n    async def verify(self, name):\n        if name is None:\n            pass\n        elif name in self.routes_client:\n            await self.routes_client[name].verify()\n            return\n        for x in self.routes_client.values():\n            await x.verify()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.EventManager.module.EventManagerClass.trigger_event","title":"<code>trigger_event(event_id)</code>  <code>async</code>","text":"<p>Exec source based on</p> <p>source_types     F -&gt; call directly     R -&gt; use get_app(str(event_id)).run_any(args, *kwargs)     S -&gt; evaluate string scope     instance -&gt; _trigger_local     local -&gt; if you ar proxy app run the event through get_app(str(event_id)).run_any(TBEF.EventManager._trigger_local, args=args, kwargs=kwargs, get_result=True)     local_network -&gt; use proxy0 app to communicate withe Daemon0 then local     global_network -&gt; exec_in event_id threaded</p> Source code in <code>toolboxv2/mods/EventManager/module.py</code> <pre><code>async def trigger_event(self, event_id: EventID):\n    \"\"\"\n    Exec source based on\n\n    source_types\n        F -&gt; call directly\n        R -&gt; use get_app(str(event_id)).run_any(*args, **kwargs)\n        S -&gt; evaluate string\n    scope\n        instance -&gt; _trigger_local\n        local -&gt; if you ar proxy app run the event through get_app(str(event_id)).run_any(TBEF.EventManager._trigger_local, args=args, kwargs=kwargs, get_result=True)\n        local_network -&gt; use proxy0 app to communicate withe Daemon0 then local\n        global_network -&gt;\n    exec_in\n    event_id\n    threaded\n\n                   \"\"\"\n    # print(f\"event-id Ptah : {event_id.get_path()}\")\n    # print(f\"testing trigger_event for {event_id.get_source()} {event_id.get_source()[-1] == self.source_id} \")\n    print(str(event_id))\n    if event_id.get_source()[-1] == self.source_id:\n        payload = await self._trigger_local(event_id)\n        event_id.set_payload(payload)\n        if len(event_id.path) &gt; 1:\n            event_id.source = ':'.join([e.split(':')[0] for e in event_id.get_path() if e != \"E\"])\n            res = await self.route_event_id(event_id)\n            if isinstance(res, Result):\n                res.print()\n            else:\n                print(res)\n        return payload\n    return await self.route_event_id(event_id)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.EventManager.module.Rout","title":"<code>Rout</code>  <code>dataclass</code>","text":"Source code in <code>toolboxv2/mods/EventManager/module.py</code> <pre><code>@dataclass\nclass Rout:\n    _from: str\n    _to: str\n\n    _from_port: int\n    _from_host: str\n\n    _to_port: int\n    _to_host: str\n\n    routing_function: Callable\n\n    @property\n    def to_host(self):\n        return self._to_host\n\n    @property\n    def to_port(self):\n        return self._to_port\n\n    async def put_data(self, event_id_data: dict[str, str]):\n        event_id: EventID = EventID(**event_id_data)\n        return await self.routing_function(event_id)\n\n    def close(self):\n        \"\"\" Close \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.EventManager.module.Rout.close","title":"<code>close()</code>","text":"<p>Close</p> Source code in <code>toolboxv2/mods/EventManager/module.py</code> <pre><code>def close(self):\n    \"\"\" Close \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi","title":"<code>FastApi</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_api_install","title":"<code>fast_api_install</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_api_install.FileBrowser","title":"<code>FileBrowser</code>","text":"Source code in <code>toolboxv2/mods/FastApi/fast_api_install.py</code> <pre><code>class FileBrowser:\n    ALLOWED_DIRECTORIES: set[str] = {\"mods_sto\", \"flows\", \"static\", \"apps\"}\n\n    def __init__(self, start_dir: str):\n        self.static_dir = pathlib.Path(start_dir).resolve()\n        self.current_container = None\n\n    def is_path_allowed(self, file_path: pathlib.Path) -&gt; bool:\n        \"\"\"Check if the path is within allowed directories.\"\"\"\n        if not file_path.is_relative_to(self.static_dir):\n            return False\n\n        relative_parts = file_path.parts[len(self.static_dir.parts):]\n        return any(part in self.ALLOWED_DIRECTORIES for part in relative_parts)\n\n    async def download_file(self, file_path: pathlib.Path) -&gt; None:\n        \"\"\"Handle file download.\"\"\"\n        if not file_path.is_file() or not self.is_path_allowed(file_path):\n            ui.notify('Access denied or file not found', type='negative')\n            return\n\n        # Use NiceGUI's download function\n        await ui.download(str(file_path))\n\n    def refresh_view(self, path: pathlib.Path) -&gt; None:\n        \"\"\"Refresh the file browser view.\"\"\"\n        if self.current_container:\n            self.current_container.clear()\n\n        with self.current_container:\n            # Add header with current path\n            ui.label(f'Current directory: {path.relative_to(self.static_dir)}').classes('text-h6')\n\n            # Add parent directory link if not at root\n            if path != self.static_dir and path.parent.is_relative_to(self.static_dir):\n                with ui.row().classes('w-full items-center'):\n                    ui.button('..', on_click=lambda p=path.parent: self.refresh_view(p)) \\\n                        .classes('bg-blue-100 px-4 py-2 rounded')\n\n            # List directories first\n            for item in sorted(path.iterdir()):\n                if not self.is_path_allowed(item):\n                    continue\n\n                with ui.row().classes('w-full items-center gap-2'):\n                    if item.is_dir():\n                        ui.button(f'\ud83d\udcc1 {item.name}/',\n                                  on_click=lambda p=item: self.refresh_view(p)) \\\n                            .classes('bg-blue-100 px-4 py-2 rounded')\n                    else:\n                        ui.label(f'\ud83d\udcc4 {item.name}').classes('flex-grow')\n                        ui.button('Download',\n                                  on_click=lambda p=item: self.download_file(p)) \\\n                            .classes('bg-green-100 px-4 py-2 rounded')\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_api_install.FileBrowser.download_file","title":"<code>download_file(file_path)</code>  <code>async</code>","text":"<p>Handle file download.</p> Source code in <code>toolboxv2/mods/FastApi/fast_api_install.py</code> <pre><code>async def download_file(self, file_path: pathlib.Path) -&gt; None:\n    \"\"\"Handle file download.\"\"\"\n    if not file_path.is_file() or not self.is_path_allowed(file_path):\n        ui.notify('Access denied or file not found', type='negative')\n        return\n\n    # Use NiceGUI's download function\n    await ui.download(str(file_path))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_api_install.FileBrowser.is_path_allowed","title":"<code>is_path_allowed(file_path)</code>","text":"<p>Check if the path is within allowed directories.</p> Source code in <code>toolboxv2/mods/FastApi/fast_api_install.py</code> <pre><code>def is_path_allowed(self, file_path: pathlib.Path) -&gt; bool:\n    \"\"\"Check if the path is within allowed directories.\"\"\"\n    if not file_path.is_relative_to(self.static_dir):\n        return False\n\n    relative_parts = file_path.parts[len(self.static_dir.parts):]\n    return any(part in self.ALLOWED_DIRECTORIES for part in relative_parts)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_api_install.FileBrowser.refresh_view","title":"<code>refresh_view(path)</code>","text":"<p>Refresh the file browser view.</p> Source code in <code>toolboxv2/mods/FastApi/fast_api_install.py</code> <pre><code>def refresh_view(self, path: pathlib.Path) -&gt; None:\n    \"\"\"Refresh the file browser view.\"\"\"\n    if self.current_container:\n        self.current_container.clear()\n\n    with self.current_container:\n        # Add header with current path\n        ui.label(f'Current directory: {path.relative_to(self.static_dir)}').classes('text-h6')\n\n        # Add parent directory link if not at root\n        if path != self.static_dir and path.parent.is_relative_to(self.static_dir):\n            with ui.row().classes('w-full items-center'):\n                ui.button('..', on_click=lambda p=path.parent: self.refresh_view(p)) \\\n                    .classes('bg-blue-100 px-4 py-2 rounded')\n\n        # List directories first\n        for item in sorted(path.iterdir()):\n            if not self.is_path_allowed(item):\n                continue\n\n            with ui.row().classes('w-full items-center gap-2'):\n                if item.is_dir():\n                    ui.button(f'\ud83d\udcc1 {item.name}/',\n                              on_click=lambda p=item: self.refresh_view(p)) \\\n                        .classes('bg-blue-100 px-4 py-2 rounded')\n                else:\n                    ui.label(f'\ud83d\udcc4 {item.name}').classes('flex-grow')\n                    ui.button('Download',\n                              on_click=lambda p=item: self.download_file(p)) \\\n                        .classes('bg-green-100 px-4 py-2 rounded')\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit","title":"<code>fast_lit</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit.APIRequestHelper","title":"<code>APIRequestHelper</code>","text":"Source code in <code>toolboxv2/mods/FastApi/fast_lit.py</code> <pre><code>class APIRequestHelper:\n    def __init__(self, token_secret: str):\n        self.token_secret = token_secret\n\n    async def make_api_request(self, endpoint: str, method: str, data: dict | None = None,\n                               headers: dict | None = None, session_token: str | None = None) -&gt; Any:\n        \"\"\"\n        Make API requests while maintaining session context\n        \"\"\"\n        import httpx\n\n        if headers is None:\n            headers = {}\n\n        if session_token:\n            try:\n                session_data = jwt.decode(session_token, self.token_secret, algorithms=[\"HS256\"])\n                headers['X-Session-ID'] = session_data.get('session_id')\n                headers['Authorization'] = f'Bearer {session_token}'\n            except jwt.InvalidTokenError:\n                raise ValueError(\"Invalid session token\")\n\n        async with httpx.AsyncClient() as client:\n            response = await client.request(\n                method=method,\n                url=endpoint,\n                json=data,\n                headers=headers\n            )\n\n            return response.json()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit.APIRequestHelper.make_api_request","title":"<code>make_api_request(endpoint, method, data=None, headers=None, session_token=None)</code>  <code>async</code>","text":"<p>Make API requests while maintaining session context</p> Source code in <code>toolboxv2/mods/FastApi/fast_lit.py</code> <pre><code>async def make_api_request(self, endpoint: str, method: str, data: dict | None = None,\n                           headers: dict | None = None, session_token: str | None = None) -&gt; Any:\n    \"\"\"\n    Make API requests while maintaining session context\n    \"\"\"\n    import httpx\n\n    if headers is None:\n        headers = {}\n\n    if session_token:\n        try:\n            session_data = jwt.decode(session_token, self.token_secret, algorithms=[\"HS256\"])\n            headers['X-Session-ID'] = session_data.get('session_id')\n            headers['Authorization'] = f'Bearer {session_token}'\n        except jwt.InvalidTokenError:\n            raise ValueError(\"Invalid session token\")\n\n    async with httpx.AsyncClient() as client:\n        response = await client.request(\n            method=method,\n            url=endpoint,\n            json=data,\n            headers=headers\n        )\n\n        return response.json()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit.BidirectionalStreamlitAppManager","title":"<code>BidirectionalStreamlitAppManager</code>","text":"<p>               Bases: <code>BaseHTTPMiddleware</code></p> Source code in <code>toolboxv2/mods/FastApi/fast_lit.py</code> <pre><code>class BidirectionalStreamlitAppManager(BaseHTTPMiddleware, metaclass=Singleton):\n    def __init__(self, app: FastAPI, streamlit_apps_dir: str = \"./apps\"):\n        super().__init__(app)\n        self.streamlit_manager = StreamlitAppManager()\n        self.streamlit_apps_dir = streamlit_apps_dir\n        self.token_secret = os.getenv(\"TOKEN_SECRET\", \"your-secret-key\")\n        self.api_helper = APIRequestHelper(self.token_secret)\n\n        # Run cleanup task\n        asyncio.create_task(self.periodic_cleanup())\n\n    #def add_ws(self, fast_app):\n        # Register WebSocket routes\n     #   fast_app.add_api_websocket_route(\"/ws/{session_id}/{app_id}\", self.websocket_endpoint, \"StWebSocket\")\n\n    async def periodic_cleanup(self):\n        while True:\n            self.streamlit_manager.cleanup_inactive_apps()\n            await asyncio.sleep(3600)\n\n    def create_streamlit_token(self, session_data: dict, app_name: str) -&gt; str:\n        payload = {\n            \"app_name\": app_name,\n            \"session_id\": session_data.get(\"ID\"),\n            \"user_data\": session_data.get(\"live_data\"),\n            \"exp\": datetime.utcnow() + timedelta(hours=1)\n        }\n        return jwt.encode(payload, self.token_secret, algorithm=\"HS256\")\n\n    #async def websocket_endpoint(self, websocket: WebSocket, session_id: str, app_id: str):\n    #    await self.streamlit_manager.ws_manager.connect(websocket, session_id, app_id)\n    #    try:\n    #        while True:\n    #            message = await websocket.receive_json()\n    #            await self.streamlit_manager.ws_manager.handle_message(session_id, message)\n    #    except WebSocketDisconnect:\n    #        await self.streamlit_manager.ws_manager.disconnect(session_id, app_id)\n\n    async def resolve_session_token(self, request: Request) -&gt; str | None:\n        \"\"\"\n        Extract and validate session token from request\n        \"\"\"\n        token = request.headers.get('Authorization', '').replace('Bearer ', '')\n        if not token:\n            token = request.query_params.get('token')\n\n        if token:\n            try:\n                jwt.decode(token, self.token_secret, algorithms=[\"HS256\"])\n                return token\n            except jwt.InvalidTokenError:\n                return None\n        return None\n\n    async def dispatch(self, request: Request, call_next) -&gt; Response:\n        # Handle API routes with session token resolution\n        if request.url.path.startswith(\"/api/\"):\n            session_token = await self.resolve_session_token(request)\n            if session_token:\n                # Inject session data into request state\n                request.state.session_token = session_token\n                request.state.api_helper = self.api_helper\n\n        # Handle Streamlit routes\n        elif request.url.path.startswith(\"/apps/\"):\n            app_name = request.url.path.split(\"/\")[-1]\n            app_path = os.path.join(self.streamlit_apps_dir, f\"{app_name}.py\")\n\n            # Verify session is valid\n            if 'public' not in app_name and not request.session.get(\"valid\", False):\n                return JSONResponse(\n                    status_code=401,\n                    content={\"message\": \"Invalid session\"}\n                )\n\n            if not os.path.exists(app_path):\n                return JSONResponse(\n                    status_code=401,\n                    content={\"message\": \"no app found\"}\n                )\n\n            streamlit_token = self.create_streamlit_token(request.session, app_name)\n            port = await self.streamlit_manager.start_app(app_path, request.session.get(\"ID\")+app_name)\n            streamlit_url = f\"http://{host}:{port}?token={streamlit_token}\"\n            return RedirectResponse(url=streamlit_url)\n\n        resposee = await call_next(request)\n        return resposee\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit.BidirectionalStreamlitAppManager.resolve_session_token","title":"<code>resolve_session_token(request)</code>  <code>async</code>","text":"<p>Extract and validate session token from request</p> Source code in <code>toolboxv2/mods/FastApi/fast_lit.py</code> <pre><code>async def resolve_session_token(self, request: Request) -&gt; str | None:\n    \"\"\"\n    Extract and validate session token from request\n    \"\"\"\n    token = request.headers.get('Authorization', '').replace('Bearer ', '')\n    if not token:\n        token = request.query_params.get('token')\n\n    if token:\n        try:\n            jwt.decode(token, self.token_secret, algorithms=[\"HS256\"])\n            return token\n        except jwt.InvalidTokenError:\n            return None\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit.inject_custom_css","title":"<code>inject_custom_css(css_file_path='./web/assets/styles.css')</code>","text":"<p>Liest eine CSS-Datei ein und injiziert sie in die Streamlit-App.</p> Source code in <code>toolboxv2/mods/FastApi/fast_lit.py</code> <pre><code>def inject_custom_css(css_file_path=\"./web/assets/styles.css\"):\n    \"\"\"\n    Liest eine CSS-Datei ein und injiziert sie in die Streamlit-App.\n    \"\"\"\n    import streamlit as st\n    try:\n        with open(css_file_path) as f:\n            css_content = f.read()\n\n        # CSS in einen &lt;style&gt;-Tag einbetten\n        css_injection = f\"&lt;style&gt;{css_content}&lt;/style&gt;\"\n\n        # CSS in Streamlit injizieren\n        st.markdown(css_injection, unsafe_allow_html=True)\n    except Exception as e:\n        st.error(f\"Fehler beim Laden des CSS: {e}\")\n\n    st.markdown(\"\"\"\n        &lt;style&gt;\n            .reportview-container {\n                margin-top: -2em;\n            }\n            #MainMenu {visibility: hidden;}\n            .stDeployButton {display:none;}\n            footer {visibility: hidden;}\n            #stDecoration {display:none;}\n        &lt;/style&gt;\n    \"\"\", unsafe_allow_html=True)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit.make_api_request","title":"<code>make_api_request(endpoint, method='GET', data=None)</code>  <code>async</code>","text":"<p>Helper function for making API requests from Streamlit apps</p> Source code in <code>toolboxv2/mods/FastApi/fast_lit.py</code> <pre><code>async def make_api_request(endpoint: str, method: str = \"GET\", data: dict | None = None):\n    \"\"\"Helper function for making API requests from Streamlit apps\"\"\"\n    import streamlit as st\n\n    if not hasattr(st.session_state, 'token'):\n        st.error(\"No valid session token found\")\n        st.stop()\n\n    headers = {\n        'Authorization': f'Bearer {st.session_state.token}',\n        'Content-Type': 'application/json'\n    }\n\n    try:\n        api_helper = APIRequestHelper(os.getenv(\"TOKEN_SECRET\", \"your-secret-key\"))\n        response = await api_helper.make_api_request(\n            endpoint=endpoint,\n            method=method,\n            data=data,\n            headers=headers,\n            session_token=st.session_state.token\n        )\n        return response\n    except Exception as e:\n        st.error(f\"API request failed: {str(e)}\")\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice","title":"<code>fast_nice</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager","title":"<code>NiceGUIManager</code>","text":"Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>class NiceGUIManager(metaclass=Singleton):\n    init = False\n    def __init__(self, fastapi_app: FastAPI = None, styles_path: str = \"./web/assets/styles.css\"):\n\n        if fastapi_app is None:\n            return None\n        self.admin_password = os.getenv(\"TB_R_KEY\", \"root@admin\")\n        self.app = fastapi_app\n        self.styles_path = styles_path\n        self.registered_guis: dict[str, dict[str, Any]] = {}\n        self.ws_connections: dict[str, dict[str, WebSocket]] = {}\n        self.mount_path = \"/gui\"\n        self.endpoints: list[UIEndpoint] = []\n\n        self.helper_contex = open(\"./dist/helper.html\", encoding=\"utf-8\").read()\n\n        self.app.add_middleware(BaseHTTPMiddleware, dispatch=self.middleware_dispatch)\n\n        # Add WebSocket endpoint\n        self.app.websocket(\"/ws/{session_id}/{gui_id}\")(self.websocket_endpoint)\n        self._setup_admin_gui()\n        self._setup_endpoints_api()\n\n    def _setup_endpoints_api(self):\n        @self.app.get(\"/api/CloudM/openui\")\n        def get_ui_endpoints(request: Request) -&gt; list[dict]:\n            def _(endpoint):\n                add_true = True\n                if endpoint.only_valid:\n                    add_true = request.session['valid']\n\n                if add_true and endpoint.only_root:\n                    add_true = request.session.get('live_data', {}).get('user_name') == 'root'\n                return add_true\n            return [{\"path\": endpoint.path,\n    \"title\": endpoint.title,\n    \"description\": endpoint.description} for endpoint in self.endpoints if endpoint.show and _(endpoint)]\n\n    def _setup_admin_gui(self):\n        \"\"\"Setup the admin GUI interface\"\"\"\n\n        @ui.page('/admin')\n        def admin_gui(user=None):\n            print(\"admin_gui;\", user)\n            if user is None or user.name != \"root\":\n                return\n\n            with ui.card().style(\"background-color: var(--background-color) !important\").classes('w-full'):\n                ui.label('NiceGUI Manager Admin Interface').classes('text-2xl font-bold mb-4')\n\n                # GUI Management Section\n                with ui.tabs().style(\"background-color: var(--background-color) !important\") as tabs:\n                    ui.tab('Registered GUIs')\n                    ui.tab('Add New GUI')\n                    ui.tab('System Status')\n\n                with ui.tab_panels(tabs, value='Registered GUIs').style(\n                    \"background-color: var(--background-color) !important\"):\n                    with ui.tab_panel('Registered GUIs'):\n                        self._show_registered_guis()\n\n                    with ui.tab_panel('Add New GUI'):\n                        self._show_add_gui_form()\n\n                    with ui.tab_panel('System Status'):\n                        self._show_system_status()\n\n        self.register_gui(\"admin\", admin_gui, \"/admin\", only_root=True)\n\n    def _show_registered_guis(self):\n        \"\"\"Show list of registered GUIs with management options\"\"\"\n        with ui.column().classes('w-full gap-4'):\n            for gui_id, gui_info in self.registered_guis.items():\n                with ui.card().classes('w-full').style(\"background-color: var(--background-color) !important\"):\n                    with ui.row().classes('w-full items-center justify-between').style(\n                        \"background-color: var(--background-color) !important\"):\n                        ui.label(f'GUI ID: {gui_id}').classes('font-bold')\n                        ui.label(f'Path: {gui_info[\"path\"]}')\n\n                        created_at = gui_info['created_at'].strftime('%Y-%m-%d %H:%M:%S')\n                        ui.label(f'Created: {created_at}')\n\n                        with ui.row().classes('gap-2').style(\"background-color: var(--background-color) !important\"):\n                            ui.button('View', on_click=lambda g=gui_info['path']: ui.navigate.to(g))\n                            ui.button('Remove', on_click=lambda g=gui_id: self._handle_gui_removal(g))\n                            ui.button('Restart', on_click=lambda g=gui_id: self._handle_gui_restart(g))\n\n                    # Show connection status\n                    active_connections = sum(\n                        1 for connections in self.ws_connections.values()\n                        if gui_id in connections\n                    )\n                    ui.label(f'Active Connections: {active_connections}')\n\n    def _show_add_gui_form(self):\n        \"\"\"Show form for adding new GUI\"\"\"\n        with ui.card().classes('w-full').style(\"background-color: var(--background-color) !important\"):\n            gui_id = ui.input('GUI ID').classes('w-full')\n            mount_path = ui.input('Mount Path (optional)').classes('w-full')\n\n            # Code editor for GUI setup\n            code_editor = ui.editor(\n                value='def setup_gui():\\n    ui.label(\"New GUI\")\\n',\n            ).classes('w-full h-64')\n\n            def add_new_gui():\n                try:\n                    # Create setup function from code\n                    setup_code = code_editor.value\n                    setup_namespace = {}\n                    exec(setup_code, {'ui': ui}, setup_namespace)\n                    setup_func = setup_namespace['setup_gui']\n\n                    # Register the new GUI\n                    self.register_gui(\n                        gui_id.value,\n                        setup_func,\n                        mount_path.value if mount_path.value else None\n                    )\n\n                    ui.notify('GUI added successfully')\n                    ui.navigate.to('admin')  # Refresh page\n                except Exception as e:\n                    ui.notify(f'Error adding GUI: {str(e)}', color='negative')\n\n            ui.button('Add GUI', on_click=add_new_gui).classes('w-full mt-4')\n\n    def _show_system_status(self):\n        \"\"\"Show system status information\"\"\"\n        with ui.card().classes('w-full').style(\"background-color: var(--background-color) !important\"):\n            ui.label('System Status').classes('text-xl font-bold mb-4')\n\n            # System stats\n            ui.label(f'Total GUIs: {len(self.registered_guis)}')\n            ui.label(f'Total WebSocket Connections: {sum(len(conns) for conns in self.ws_connections.values())}')\n\n            # Memory usage\n            import psutil\n            process = psutil.Process()\n            memory_usage = process.memory_info().rss / 1024 / 1024  # MB\n            ui.label(f'Memory Usage: {memory_usage:.2f} MB')\n\n            # Add refresh button\n            ui.button('Refresh Stats', on_click=lambda: ui.navigate.to('/admin'))\n\n    def _handle_gui_removal(self, gui_id: str):\n        \"\"\"Handle GUI removal with confirmation\"\"\"\n\n        def confirm_remove():\n            if self.remove_gui(gui_id):\n                ui.notify(f'GUI {gui_id} removed successfully')\n                ui.navigate.to('/admin')  # Refresh page\n            else:\n                ui.notify('Error removing GUI', color='negative')\n\n        ui.notify('Are you sure?',\n                  actions=[{'label': 'Yes', 'on_click': confirm_remove},\n                           {'label': 'No'}])\n\n    def _handle_gui_restart(self, gui_id: str):\n        \"\"\"Handle GUI restart\"\"\"\n        try:\n            if gui_id in self.registered_guis:\n                gui_info = self.registered_guis[gui_id]\n                # Re-register the GUI with the same setup\n                self.register_gui(gui_id, gui_info['setup'], gui_info['path'])\n                ui.notify(f'GUI {gui_id} restarted successfully')\n            else:\n                ui.notify('GUI not found', color='negative')\n        except Exception as e:\n            ui.notify(f'Error restarting GUI: {str(e)}', color='negative')\n\n    def _load_styles(self) -&gt; str:\n        \"\"\"Load custom styles from CSS file\"\"\"\n        try:\n            with open(self.styles_path) as f:\n                return f.read()\n        except Exception as e:\n            print(f\"Error loading styles: {e}\")\n            return \"\"\n\n    def register_gui(self, gui_id: str, setup_func: Callable, mount_path: str | None = None, additional: str | None = None, title: str | None = None , description: str | None = None, **kwargs) -&gt; None:\n        \"\"\"Register a new NiceGUI application\"\"\"\n        path = mount_path or f\"/{gui_id}\"\n        self.endpoints.append(UIEndpoint(path=self.mount_path+path, title=title if title is not None else path.replace('/', '') , description=description if description is not None else '', **kwargs))\n        if additional is None:\n            additional = \"\"\n\n        def has_parameters(func, *params):\n            \"\"\"\n            \u00dcberpr\u00fcft, ob die Funktion bestimmte Parameter hat.\n\n            :param func: Die zu analysierende Funktion.\n            :param params: Eine Liste der zu suchenden Parameter.\n            :return: Ein Dictionary mit den Parametern und einem booleschen Wert.\n            \"\"\"\n            signature = inspect.signature(func)\n            func_params = signature.parameters.keys()\n            return {param: param in func_params for param in params}\n\n        async def request_to_request_session(request):\n            jk = request.json()\n            if asyncio.iscoroutine(jk):\n                with contextlib.suppress(Exception):\n                    jk = await jk\n            def js():\n                return jk\n            return RequestSession(\n                session=request.session,\n                body=request.body,\n                json=js,\n                row=request,\n            )\n\n        get_app()\n\n        @ui.page(path)\n        async def wrapped_gui(request: Request):\n            # Inject custom styles\n            ui.add_body_html(self.helper_contex + additional)\n            # ui.switch('Dark').bind_value(ui, 'dark_mode')\n            # ui.add_css(\"q-card {background-color: var(--background-color)} !important\")\n            # ui.add_body_html('&lt;script src=\"../index.js\" type=\"module\" defer&gt;&lt;/script&gt;')\n\n            # Initialize the GUI\n            params_ = {}\n            params = has_parameters(setup_func, 'request', 'user', 'session', 'id', 'sid')\n\n            if params.get('request'):\n                params_['request'] = await request_to_request_session(request)\n            if params.get('user'):\n                params_['user'] = await get_user_from_request(get_app(), request)\n            if params.get('session'):\n                params_['session'] = request.session\n            if params.get('spec'):\n                params_['spec'] = get_spec(request)\n            if params.get('sid'):\n                params_['sid'] = get_s_id(request)\n\n            async def task():\n                if asyncio.iscoroutine(setup_func):\n\n                    # Event Listener f\u00fcr Button hinzuf\u00fcgen\n                    await ui.run_javascript('''\n                            Quasar.Dark.set(\"auto\");\n                            tailwind.config.darkMode = \"media\";\n                        ''')\n\n                    await ui.run_javascript(\"\"\"\n                    document.getElementById('darkModeToggle').addEventListener('click', function () {\n                    const labelToggel = document.getElementById('toggleLabel')\n                    if (labelToggel.innerHTML == `&lt;span class=\"material-symbols-outlined\"&gt;\ndark_mode\n&lt;/span&gt;`){\n                            Quasar.Dark.set(true);\n                            tailwind.config.darkMode = \"class\";\n                            document.body.classList.add(\"dark\");\n                        }else{\n                            Quasar.Dark.set(false);\n                            tailwind.config.darkMode = \"class\"\n                            document.body.classList.remove(\"dark\");\n                        }\n                    });\n                    \"\"\")\n\n                    if not params_:\n                        await setup_func()\n                    else:\n                        await setup_func(**params_)\n                else:\n                    if not params_:\n                        setup_func()\n                    else:\n                        setup_func(**params_)\n\n\n\n\n            await task()\n            # return result\n\n        self.registered_guis[gui_id] = {\n            'path': path,\n            'setup': setup_func,\n            'created_at': datetime.now()\n        }\n\n        print(\"Registered GUI:\", self.registered_guis[gui_id])\n        return True\n\n    def remove_gui(self, gui_id: str) -&gt; bool:\n        \"\"\"Remove a registered GUI application\"\"\"\n        if gui_id in self.registered_guis:\n            # Remove from registry\n            del self.registered_guis[gui_id]\n\n            # Clean up any WebSocket connections\n            for session_id in self.ws_connections:\n                if gui_id in self.ws_connections[session_id]:\n                    del self.ws_connections[session_id][gui_id]\n\n            return True\n        return False\n\n    async def websocket_endpoint(self, websocket: WebSocket, session_id: str, gui_id: str):\n        \"\"\"Handle WebSocket connections for real-time updates\"\"\"\n        await websocket.accept()\n\n        if session_id not in self.ws_connections:\n            self.ws_connections[session_id] = {}\n        self.ws_connections[session_id][gui_id] = websocket\n\n        try:\n            while True:\n                data = await websocket.receive_json()\n                # Handle incoming WebSocket messages\n                await self.handle_ws_message(session_id, gui_id, data)\n        except WebSocketDisconnect:\n            if session_id in self.ws_connections:\n                if gui_id in self.ws_connections[session_id]:\n                    del self.ws_connections[session_id][gui_id]\n\n    async def handle_ws_message(self, session_id: str, gui_id: str, message: dict):\n        \"\"\"Handle incoming WebSocket messages\"\"\"\n        # Implement custom WebSocket message handling\n        if message.get('type') == 'update':\n            # Broadcast updates to all connected clients for this GUI\n            await self.broadcast_to_gui(gui_id, {\n                'type': 'update',\n                'data': message.get('data')\n            })\n\n    async def broadcast_to_gui(self, gui_id: str, message: dict):\n        \"\"\"Broadcast a message to all sessions connected to a specific GUI\"\"\"\n        for session_connections in self.ws_connections.values():\n            if gui_id in session_connections:\n                await session_connections[gui_id].send_json(message)\n\n    async def middleware_dispatch(self, request: Request, call_next) -&gt; Response:\n        \"\"\"Custom middleware for session handling and authentication\"\"\"\n        async def callN():\n            response = await call_next(request)\n            return response\n\n        if not request.url.path.startswith(self.mount_path):\n            return await callN()\n\n        if request.url.path.endswith(\"/favicon.ico\"):\n            return await callN()\n        if \"_nicegui\" in request.url.path and \"static\" in request.url.path:\n            return await callN()\n        if \"_nicegui\" in request.url.path and \"components\" in request.url.path:\n            return await callN()\n        if \"_nicegui\" in request.url.path and \"codehilite\" in request.url.path:\n            return await callN()\n        if \"_nicegui\" in request.url.path and \"libraries\" in request.url.path:\n            return await callN()\n\n        if \"open\" in request.url.path:\n            return await callN()\n\n        # Verify session if needed\n        if not request.session.get(\"valid\", False):\n            return RedirectResponse(f\"/web/login?next={request.url.path}\")\n\n        response = await call_next(request)\n        return response\n\n    def init_app(self) -&gt; None:\n        \"\"\"Initialize the FastAPI application with NiceGUI integration\"\"\"\n        self.init = True\n        ui.run_with(\n            self.app,\n            mount_path=self.mount_path,\n            favicon=os.getenv(\"FAVI\"), # \"/root/Toolboxv2/toolboxv2/favicon.ico\"\n            show_welcome_message=False,\n            # prod_js=False,\n        )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.broadcast_to_gui","title":"<code>broadcast_to_gui(gui_id, message)</code>  <code>async</code>","text":"<p>Broadcast a message to all sessions connected to a specific GUI</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>async def broadcast_to_gui(self, gui_id: str, message: dict):\n    \"\"\"Broadcast a message to all sessions connected to a specific GUI\"\"\"\n    for session_connections in self.ws_connections.values():\n        if gui_id in session_connections:\n            await session_connections[gui_id].send_json(message)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.handle_ws_message","title":"<code>handle_ws_message(session_id, gui_id, message)</code>  <code>async</code>","text":"<p>Handle incoming WebSocket messages</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>async def handle_ws_message(self, session_id: str, gui_id: str, message: dict):\n    \"\"\"Handle incoming WebSocket messages\"\"\"\n    # Implement custom WebSocket message handling\n    if message.get('type') == 'update':\n        # Broadcast updates to all connected clients for this GUI\n        await self.broadcast_to_gui(gui_id, {\n            'type': 'update',\n            'data': message.get('data')\n        })\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.init_app","title":"<code>init_app()</code>","text":"<p>Initialize the FastAPI application with NiceGUI integration</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>def init_app(self) -&gt; None:\n    \"\"\"Initialize the FastAPI application with NiceGUI integration\"\"\"\n    self.init = True\n    ui.run_with(\n        self.app,\n        mount_path=self.mount_path,\n        favicon=os.getenv(\"FAVI\"), # \"/root/Toolboxv2/toolboxv2/favicon.ico\"\n        show_welcome_message=False,\n        # prod_js=False,\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.middleware_dispatch","title":"<code>middleware_dispatch(request, call_next)</code>  <code>async</code>","text":"<p>Custom middleware for session handling and authentication</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>async def middleware_dispatch(self, request: Request, call_next) -&gt; Response:\n    \"\"\"Custom middleware for session handling and authentication\"\"\"\n    async def callN():\n        response = await call_next(request)\n        return response\n\n    if not request.url.path.startswith(self.mount_path):\n        return await callN()\n\n    if request.url.path.endswith(\"/favicon.ico\"):\n        return await callN()\n    if \"_nicegui\" in request.url.path and \"static\" in request.url.path:\n        return await callN()\n    if \"_nicegui\" in request.url.path and \"components\" in request.url.path:\n        return await callN()\n    if \"_nicegui\" in request.url.path and \"codehilite\" in request.url.path:\n        return await callN()\n    if \"_nicegui\" in request.url.path and \"libraries\" in request.url.path:\n        return await callN()\n\n    if \"open\" in request.url.path:\n        return await callN()\n\n    # Verify session if needed\n    if not request.session.get(\"valid\", False):\n        return RedirectResponse(f\"/web/login?next={request.url.path}\")\n\n    response = await call_next(request)\n    return response\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.register_gui","title":"<code>register_gui(gui_id, setup_func, mount_path=None, additional=None, title=None, description=None, **kwargs)</code>","text":"<p>Register a new NiceGUI application</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>    def register_gui(self, gui_id: str, setup_func: Callable, mount_path: str | None = None, additional: str | None = None, title: str | None = None , description: str | None = None, **kwargs) -&gt; None:\n        \"\"\"Register a new NiceGUI application\"\"\"\n        path = mount_path or f\"/{gui_id}\"\n        self.endpoints.append(UIEndpoint(path=self.mount_path+path, title=title if title is not None else path.replace('/', '') , description=description if description is not None else '', **kwargs))\n        if additional is None:\n            additional = \"\"\n\n        def has_parameters(func, *params):\n            \"\"\"\n            \u00dcberpr\u00fcft, ob die Funktion bestimmte Parameter hat.\n\n            :param func: Die zu analysierende Funktion.\n            :param params: Eine Liste der zu suchenden Parameter.\n            :return: Ein Dictionary mit den Parametern und einem booleschen Wert.\n            \"\"\"\n            signature = inspect.signature(func)\n            func_params = signature.parameters.keys()\n            return {param: param in func_params for param in params}\n\n        async def request_to_request_session(request):\n            jk = request.json()\n            if asyncio.iscoroutine(jk):\n                with contextlib.suppress(Exception):\n                    jk = await jk\n            def js():\n                return jk\n            return RequestSession(\n                session=request.session,\n                body=request.body,\n                json=js,\n                row=request,\n            )\n\n        get_app()\n\n        @ui.page(path)\n        async def wrapped_gui(request: Request):\n            # Inject custom styles\n            ui.add_body_html(self.helper_contex + additional)\n            # ui.switch('Dark').bind_value(ui, 'dark_mode')\n            # ui.add_css(\"q-card {background-color: var(--background-color)} !important\")\n            # ui.add_body_html('&lt;script src=\"../index.js\" type=\"module\" defer&gt;&lt;/script&gt;')\n\n            # Initialize the GUI\n            params_ = {}\n            params = has_parameters(setup_func, 'request', 'user', 'session', 'id', 'sid')\n\n            if params.get('request'):\n                params_['request'] = await request_to_request_session(request)\n            if params.get('user'):\n                params_['user'] = await get_user_from_request(get_app(), request)\n            if params.get('session'):\n                params_['session'] = request.session\n            if params.get('spec'):\n                params_['spec'] = get_spec(request)\n            if params.get('sid'):\n                params_['sid'] = get_s_id(request)\n\n            async def task():\n                if asyncio.iscoroutine(setup_func):\n\n                    # Event Listener f\u00fcr Button hinzuf\u00fcgen\n                    await ui.run_javascript('''\n                            Quasar.Dark.set(\"auto\");\n                            tailwind.config.darkMode = \"media\";\n                        ''')\n\n                    await ui.run_javascript(\"\"\"\n                    document.getElementById('darkModeToggle').addEventListener('click', function () {\n                    const labelToggel = document.getElementById('toggleLabel')\n                    if (labelToggel.innerHTML == `&lt;span class=\"material-symbols-outlined\"&gt;\ndark_mode\n&lt;/span&gt;`){\n                            Quasar.Dark.set(true);\n                            tailwind.config.darkMode = \"class\";\n                            document.body.classList.add(\"dark\");\n                        }else{\n                            Quasar.Dark.set(false);\n                            tailwind.config.darkMode = \"class\"\n                            document.body.classList.remove(\"dark\");\n                        }\n                    });\n                    \"\"\")\n\n                    if not params_:\n                        await setup_func()\n                    else:\n                        await setup_func(**params_)\n                else:\n                    if not params_:\n                        setup_func()\n                    else:\n                        setup_func(**params_)\n\n\n\n\n            await task()\n            # return result\n\n        self.registered_guis[gui_id] = {\n            'path': path,\n            'setup': setup_func,\n            'created_at': datetime.now()\n        }\n\n        print(\"Registered GUI:\", self.registered_guis[gui_id])\n        return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.remove_gui","title":"<code>remove_gui(gui_id)</code>","text":"<p>Remove a registered GUI application</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>def remove_gui(self, gui_id: str) -&gt; bool:\n    \"\"\"Remove a registered GUI application\"\"\"\n    if gui_id in self.registered_guis:\n        # Remove from registry\n        del self.registered_guis[gui_id]\n\n        # Clean up any WebSocket connections\n        for session_id in self.ws_connections:\n            if gui_id in self.ws_connections[session_id]:\n                del self.ws_connections[session_id][gui_id]\n\n        return True\n    return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.websocket_endpoint","title":"<code>websocket_endpoint(websocket, session_id, gui_id)</code>  <code>async</code>","text":"<p>Handle WebSocket connections for real-time updates</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>async def websocket_endpoint(self, websocket: WebSocket, session_id: str, gui_id: str):\n    \"\"\"Handle WebSocket connections for real-time updates\"\"\"\n    await websocket.accept()\n\n    if session_id not in self.ws_connections:\n        self.ws_connections[session_id] = {}\n    self.ws_connections[session_id][gui_id] = websocket\n\n    try:\n        while True:\n            data = await websocket.receive_json()\n            # Handle incoming WebSocket messages\n            await self.handle_ws_message(session_id, gui_id, data)\n    except WebSocketDisconnect:\n        if session_id in self.ws_connections:\n            if gui_id in self.ws_connections[session_id]:\n                del self.ws_connections[session_id][gui_id]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.create_nicegui_manager","title":"<code>create_nicegui_manager(app, token_secret=None)</code>","text":"<p>Create and initialize a NiceGUI manager instance</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>def create_nicegui_manager(app: FastAPI, token_secret: str | None = None) -&gt; NiceGUIManager:\n    \"\"\"Create and initialize a NiceGUI manager instance\"\"\"\n    manager = NiceGUIManager(app, token_secret)\n    manager.init_app()\n    manager_online[0] = True\n    return manager\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager","title":"<code>manager</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools","title":"<code>Tools</code>","text":"<p>               Bases: <code>MainTool</code>, <code>FileHandler</code></p> <p>A production-ready API Manager for running, monitoring, and managing FastAPI instances.</p> This class allows you to <ul> <li>Start API instances (live, development, debug)</li> <li>Stop and restart running APIs</li> <li>Update configuration for APIs</li> <li>Get live diagnostic info about running APIs</li> </ul> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>class Tools(MainTool, FileHandler):\n    \"\"\"\n    A production-ready API Manager for running, monitoring, and managing FastAPI instances.\n\n    This class allows you to:\n      - Start API instances (live, development, debug)\n      - Stop and restart running APIs\n      - Update configuration for APIs\n      - Get live diagnostic info about running APIs\n    \"\"\"\n\n    def __init__(self, app: Any | None = None) -&gt; None:\n        # Running APIs will be stored as a mapping from api_name to subprocess.Popen\n        self.running_apis: dict[str, multiprocessing.Process] = {}\n        self.api_config: dict[str, dict[str, str | int]] = {}\n        self.version: str = VERSION\n        self.name: str = NAME\n        self.logger: logging.Logger = app.logger if app else logging.getLogger(__name__)\n        self.color: str = \"WHITE\"\n        self.keys: dict[str, str] = {\"Apis\": \"api~config\"}\n        # In case app is not passed in, ensure that we have a dummy object with required properties\n\n        # Define available tool commands\n        self.tools: dict[str, Any] = {\n            \"all\": [\n                [\"Version\", \"Shows current Version\"],\n                [\"edit-api\", \"Set default API for name, host and port\"],\n                [\"start-api\", \"Start an API instance\"],\n                [\"stop-api\", \"Stop a running API instance\"],\n                [\"restart-api\", \"Restart an API instance\"],\n                [\"info\", \"Show API configurations and running APIs\"],\n            ],\n            \"name\": \"api_manager\",\n            \"Version\": self.show_version,\n            \"edit-api\": self.conf_api,\n            \"stop-api\": self.stop_api,\n            \"start\": self.start_live,\n            \"startE\": self._start_api,\n            \"startDev\": self.start_dev,\n            \"startDUG\": self.start_debug,\n            \"info\": self.show_running,\n            \"restart-api\": self.restart_api,\n        }\n\n        # Initialize FileHandler with default configuration data\n        default_config = {\n            \"Apis\": {\n                'main': {\n                    \"Name\": 'main',\n                    \"version\": self.version,\n                    \"port\": 5000,\n                    \"host\": '127.0.0.1'\n                }\n            }\n        }\n        FileHandler.__init__(self, \"apis.config\", self.app.id, self.keys, default_config)\n        MainTool.__init__(\n            self,\n            load=self.on_start,\n            v=self.version,\n            tool=self.tools,\n            name=self.name,\n            logs=self.logger,\n            color=self.color,\n            on_exit=self.on_exit,\n        )\n        os.makedirs(\"./.data\", exist_ok=True)\n\n    @staticmethod\n    def _get_pid_file_path(api_name: str) -&gt; str:\n        \"\"\"Get the path to the PID file for an API.\"\"\"\n        return os.path.join(\"./.data\", f\"api_pid_{api_name}\")\n\n\n    def show_version(self) -&gt; str:\n        \"\"\"Display and return the current version.\"\"\"\n        self.logger.info(\"Version: %s\", self.version)\n        return self.version\n\n    def info(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Return diagnostic information about API configurations and currently running APIs.\n        \"\"\"\n        config_info = {name: cfg for name, cfg in self.api_config.items()}\n        running_info = {name: proc.pid for name, proc in self.running_apis.items() if proc.is_alive()}\n        self.logger.info(\"API Configurations: %s\", config_info)\n        self.logger.info(\"Running APIs: %s\", running_info)\n        # Optionally, print to console as well\n        for api_name, cfg in config_info.items():\n            print(f\"Configured API - Name: {api_name}, Config: {cfg}\")\n        print(\"Running APIs:\")\n        for api_name, pid in running_info.items():\n            print(f\"API: {api_name}, Process ID: {pid}\")\n        return {\"configurations\": config_info, \"running\": running_info}\n\n    def conf_api(self, api_name: str, host: str = \"localhost\", port: int = 5000) -&gt; None:\n        \"\"\"\n        Update or create an API configuration.\n\n        Args:\n            api_name (str): The name of the API.\n            host (str): The host address (default \"localhost\"). Use \"lh\" for \"127.0.0.1\" or \"0\" for \"0.0.0.0\".\n            port (int): The port number (default 5000; use \"0\" for port 8000).\n        \"\"\"\n        if host.lower() == \"lh\":\n            host = \"127.0.0.1\"\n        if host == \"0\":\n            host = \"0.0.0.0\"\n        if str(port) == \"0\":\n            port = 8000\n\n        self.api_config[api_name] = {\n            \"Name\": api_name,\n            \"version\": self.version,\n            \"port\": int(port),\n            \"host\": host,\n        }\n        self.logger.info(\"Updated API configuration for '%s': %s\", api_name, self.api_config[api_name])\n        print(f\"API configuration updated: {self.api_config[api_name]}\")\n\n    def start_dev(self, api_name: str, *modules: str, **kwargs: Any) -&gt; str | None:\n        \"\"\"\n        Start an API in development mode.\n\n        If additional modules are provided, they are stored in a BlobFile for later use.\n\n        Args:\n            api_name (str): The API name.\n            *modules (str): Additional modules for the API.\n\n        Returns:\n            Optional[str]: Status message.\n        \"\"\"\n        if modules:\n            api_name_dev = f\"{api_name}_D\"\n            with BlobFile(f\"FastApi/{api_name_dev}/dev\", mode='w') as f:\n                f.write_json({'modules': modules})\n            api_name = api_name_dev\n\n        return self._start_api(api_name, live=False, reload=False, test_override=False, host=\"localhost\")\n\n    def start_live(self, api_name: str) -&gt; str | None:\n        \"\"\"\n        Start an API in live mode.\n        \"\"\"\n        return self._start_api(api_name, live=True, reload=False, test_override=False)\n\n    def start_debug(self, api_name: str) -&gt; str | None:\n        \"\"\"\n        Start an API in debug mode.\n        \"\"\"\n        return self._start_api(api_name, live=False, reload=True, test_override=True, host=\"localhost\")\n\n    def _start_api(\n        self,\n        api_name: str,\n        live: bool = False,\n        reload: bool = False,\n        test_override: bool = False,\n        host: str = \"localhost\"\n    ) -&gt; str | None:\n        \"\"\"\n        Start an API process with the given configuration.\n\n        Args:\n            api_name (str): The API name.\n            live (bool): Whether to run in live mode.\n            reload (bool): Whether to enable auto-reload.\n            test_override (bool): If True, allow start even if running in a test environment.\n            host (str): Host to bind the API on.\n\n        Returns:\n            Optional[str]: A status message or error message.\n        \"\"\"\n        # Prevent starting an API if in test mode unless explicitly overridden.\n        if 'test' in self.app.id and not test_override:\n            msg = \"No API allowed in test mode\"\n            self.logger.warning(msg)\n            return msg\n\n        if not api_name:\n            self.logger.error(\"No API name provided.\")\n            return None\n\n        # Check if API is already running.\n        if api_name in self.running_apis and self.running_apis[api_name].is_alive():\n            msg = f\"API '{api_name}' is already running.\"\n            self.logger.info(msg)\n            return msg\n\n        # Ensure that live and reload are not both enabled.\n        if live and reload:\n            raise ValueError(\"Live mode and reload mode cannot be enabled simultaneously.\")\n\n        # If configuration does not exist, add it automatically.\n        if api_name not in self.api_config:\n            self.api_config[api_name] = {\n                \"Name\": api_name,\n                \"version\": self.version,\n                \"port\": self.app.args_sto.port,\n                \"host\": host if host and isinstance(host, str) else \"localhost\",\n            }\n            if live:\n                self.api_config[api_name]['host'] = \"0.0.0.0\"\n            self.logger.info(\"Auto-added API configuration for '%s': %s\", api_name, self.api_config[api_name])\n\n        # For live mode, always bind to all interfaces.\n        if live:\n            self.api_config[api_name]['host'] = \"0.0.0.0\"\n\n        api_data = self.api_config[api_name]\n\n        # Check for required frontend dependencies.\n        node_modules_path = os.path.join(self.app.start_dir, \"web\", \"node_modules\")\n        if not os.path.exists(node_modules_path):\n            self.logger.info(\"Node modules folder not found. Installing dependencies in '%s'\", node_modules_path)\n            os.system(\"npm install --prefix ./web ./web\")\n\n        # Build the uvicorn command.\n        cmd_parts: list[str] = [\n            # sys.executable,\n            # \"-m\",\n            \"uvicorn\",\n            \"toolboxv2.mods.FastApi.fast_api_main:app\",\n            f\"--host {api_data['host']}\",\n            f\"--port {api_data['port']}\",\n            f\"--header data:{self.app.debug}:{api_name}\"\n        ]\n        if reload:\n            # Reload directories can be adjusted as needed.\n            cmd_parts.append(\"--reload\")\n            cmd_parts.append(\"--reload-dir ./utils\")\n            cmd_parts.append(\"--reload-dir ./mods/FastApi\")\n        command: str = \" \".join(cmd_parts)\n        self.logger.info(\"Starting API '%s' with command: %s\", api_name, command)\n\n        print(command)\n\n        # Print QR codes for local and public IPs for convenience.\n        protocol = \"http\"  # Adjust if SSL is configured\n        local_url = f\"{protocol}://{get_local_ip()}:{api_data['port']}\"\n        public_url = f\"{protocol}://{get_public_ip()}:{api_data['port']}\"\n        print_qrcode_to_console(local_url)\n        print_qrcode_to_console(public_url)\n\n        try:\n\n            process = multiprocessing.Process(\n                target=os.system,\n                args=(command,),\n                # daemon=True\n            )\n            process.start()\n\n            # Store the process\n            self.running_apis[api_name] = process\n\n            # Save PID to file\n            with open(self._get_pid_file_path(api_name), \"w\") as f:\n                f.write(str(process.pid))\n\n            # Store process info in file handler\n            self.add_to_save_file_handler(\n                key=f\"pr{api_name}\",\n                value=json.dumps({\n                    \"pid\": process.pid,\n                    \"start_time\": datetime.now().isoformat(),\n                    \"host\": api_data['host'],\n                    \"port\": api_data['port']\n                })\n            )\n\n            msg = f\"Starting API '{api_name}' at {api_data['host']}:{api_data['port']} (PID: {process.pid})\"\n            self.logger.info(msg)\n            return msg\n        except Exception as e:\n            self.logger.exception(\"Failed to start API '%s': %s\", api_name, e)\n            return f\"Failed to start API '{api_name}': {e}\"\n\n    async def stop_api(self, api_name: str, delete: bool = True) -&gt; str:\n        \"\"\"\n        Stop a running API and clean up resources.\n        \"\"\"\n        if api_name not in self.api_config:\n            msg = f\"API with the name '{api_name}' is not configured.\"\n            self.logger.warning(msg)\n            return msg\n\n        pid_file = self._get_pid_file_path(api_name)\n        if not os.path.exists(pid_file):\n            self.logger.warning(\"No pid file found for API '%s'\", api_name)\n            return f\"No pid file found for API '{api_name}'.\"\n\n        try:\n            # Read PID from file\n            with open(pid_file) as f:\n                api_pid = int(f.read().strip())\n\n            # Try graceful shutdown first\n            if 'core' in self.app.id:\n                if not await self.app.session.login():\n                    self.logger.warning(\"Could not login with username '%s'\", self.app.get_username())\n                try:\n                    response = await self.app.session.fetch(f\"/api/exit/{api_pid}\", method=\"POST\")\n                    self.logger.info(\"Exit response for API '%s': %s\", api_name, response)\n                except Exception as e:\n                    self.logger.warning(\"Failed to stop API gracefully: %s\", e)\n\n            # Force kill if process still exists\n            process = self.running_apis.get(api_name)\n            if process and process.is_alive():\n                process.terminate()\n                process.join(timeout=5)\n                if process.is_alive():\n                    process.kill()\n\n            # Fallback to system commands if needed\n            try:\n                if system() == \"Windows\":\n                    os.system(f\"taskkill /pid {api_pid} /F\")\n                else:\n                    os.kill(api_pid, signal.SIGKILL)\n            except ProcessLookupError:\n                pass  # Process already terminated\n\n            # Cleanup\n            if os.path.exists(pid_file):\n                os.remove(pid_file)\n            if delete and api_name in self.running_apis:\n                del self.running_apis[api_name]\n\n            # Update file handler\n            self.add_to_save_file_handler(\n                key=f\"pr{api_name}\",\n                value=json.dumps({\n                    \"stop_time\": datetime.now().isoformat(),\n                    \"status\": \"stopped\"\n                })\n            )\n            self.save_file_handler()\n\n            msg = f\"Stopped API '{api_name}'.\"\n            self.logger.info(msg)\n            return msg\n\n        except Exception as e:\n            self.logger.exception(\"Error stopping API '%s': %s\", api_name, e)\n            return f\"Error stopping API '{api_name}': {e}\"\n\n    def nf(self, name):\n        if len(name) &gt; 10:\n            return name[:10]\n        elif len(name) &lt; 10:\n            return name + '~' * (len(name)-10)\n        else:\n            return name\n\n    def show_running(self) -&gt; list[str]:\n        \"\"\"\n        Display and return the list of currently running APIs with their status.\n        \"\"\"\n        self.on_start()\n        running_list = []\n        print(self.api_config)\n        for api_name in self.api_config:\n\n            # Get stored process info\n            process_info = self.get_file_handler(f\"pr{api_name}\")\n            print('#',api_name, '#',process_info)\n            if process_info is None:\n                process_info = {}\n            status = {\n                \"name\": api_name,\n                \"online\": api_name in self.running_apis,\n                \"start_time\": process_info.get(\"start_time\", \"offline\"),\n                \"pid\": process_info.get(\"pid\", ''),\n                \"host\": process_info.get(\"host\", ''),\n                \"port\": process_info.get(\"port\", '')\n            }\n            running_list.append(status)\n\n        # Log and print current status\n        self.logger.info(\"APIs: %s\", running_list)\n        print(\"\\nAPIs:\")\n        for api in running_list:\n            print(f\"- {api['name']}: at {api['host']}:{api['port']}\")\n            print(f\"  Started: {api['start_time']}\")\n\n        return [api[\"name\"] for api in running_list]\n\n    async def restart_api(self, api_name: str) -&gt; str:\n        \"\"\"\n        Restart the given API by stopping it and starting it again.\n\n        Args:\n            api_name (str): The name of the API to restart.\n\n        Returns:\n            str: A status message.\n        \"\"\"\n        stop_message = await self.stop_api(api_name)\n        self.logger.info(\"Restart: %s\", stop_message)\n        # Allow some time for the process to fully terminate.\n        time.sleep(4)\n        start_message = self._start_api(api_name)\n        return f\"Restarting API '{api_name}': {start_message}\"\n\n    def on_start(self) -&gt; None:\n        \"\"\"\n        Load API configuration from file when the tool starts.\n        \"\"\"\n        self.load_file_handler()\n        data = self.get_file_handler(self.keys[\"Apis\"])\n        try:\n            if isinstance(data, str):\n                self.api_config = json.loads(data)\n            else:\n                self.api_config = data\n            self.logger.info(\"Loaded API configuration: %s\", self.api_config)\n        except Exception as e:\n            self.logger.exception(\"Error loading API configuration: %s\", e)\n            self.api_config = {}\n\n    async def on_exit(self) -&gt; None:\n        \"\"\"\n        Gracefully stop all running APIs and save configuration upon exit.\n        \"\"\"\n        # Save configuration data.\n        if len(self.api_config) != 0:\n            self.add_to_save_file_handler(self.keys[\"Apis\"], json.dumps(self.api_config))\n        # Attempt to stop all running APIs.\n        # for api_name in list(self.running_apis.keys()):\n        #     await self.stop_api(api_name, delete=False)\n        self.running_apis = {}\n        self.save_file_handler()\n        self.logger.info(\"Exiting API Manager. All running APIs stopped and configuration saved.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.conf_api","title":"<code>conf_api(api_name, host='localhost', port=5000)</code>","text":"<p>Update or create an API configuration.</p> <p>Parameters:</p> Name Type Description Default <code>api_name</code> <code>str</code> <p>The name of the API.</p> required <code>host</code> <code>str</code> <p>The host address (default \"localhost\"). Use \"lh\" for \"127.0.0.1\" or \"0\" for \"0.0.0.0\".</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>The port number (default 5000; use \"0\" for port 8000).</p> <code>5000</code> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def conf_api(self, api_name: str, host: str = \"localhost\", port: int = 5000) -&gt; None:\n    \"\"\"\n    Update or create an API configuration.\n\n    Args:\n        api_name (str): The name of the API.\n        host (str): The host address (default \"localhost\"). Use \"lh\" for \"127.0.0.1\" or \"0\" for \"0.0.0.0\".\n        port (int): The port number (default 5000; use \"0\" for port 8000).\n    \"\"\"\n    if host.lower() == \"lh\":\n        host = \"127.0.0.1\"\n    if host == \"0\":\n        host = \"0.0.0.0\"\n    if str(port) == \"0\":\n        port = 8000\n\n    self.api_config[api_name] = {\n        \"Name\": api_name,\n        \"version\": self.version,\n        \"port\": int(port),\n        \"host\": host,\n    }\n    self.logger.info(\"Updated API configuration for '%s': %s\", api_name, self.api_config[api_name])\n    print(f\"API configuration updated: {self.api_config[api_name]}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.info","title":"<code>info()</code>","text":"<p>Return diagnostic information about API configurations and currently running APIs.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def info(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Return diagnostic information about API configurations and currently running APIs.\n    \"\"\"\n    config_info = {name: cfg for name, cfg in self.api_config.items()}\n    running_info = {name: proc.pid for name, proc in self.running_apis.items() if proc.is_alive()}\n    self.logger.info(\"API Configurations: %s\", config_info)\n    self.logger.info(\"Running APIs: %s\", running_info)\n    # Optionally, print to console as well\n    for api_name, cfg in config_info.items():\n        print(f\"Configured API - Name: {api_name}, Config: {cfg}\")\n    print(\"Running APIs:\")\n    for api_name, pid in running_info.items():\n        print(f\"API: {api_name}, Process ID: {pid}\")\n    return {\"configurations\": config_info, \"running\": running_info}\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.on_exit","title":"<code>on_exit()</code>  <code>async</code>","text":"<p>Gracefully stop all running APIs and save configuration upon exit.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>async def on_exit(self) -&gt; None:\n    \"\"\"\n    Gracefully stop all running APIs and save configuration upon exit.\n    \"\"\"\n    # Save configuration data.\n    if len(self.api_config) != 0:\n        self.add_to_save_file_handler(self.keys[\"Apis\"], json.dumps(self.api_config))\n    # Attempt to stop all running APIs.\n    # for api_name in list(self.running_apis.keys()):\n    #     await self.stop_api(api_name, delete=False)\n    self.running_apis = {}\n    self.save_file_handler()\n    self.logger.info(\"Exiting API Manager. All running APIs stopped and configuration saved.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.on_start","title":"<code>on_start()</code>","text":"<p>Load API configuration from file when the tool starts.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def on_start(self) -&gt; None:\n    \"\"\"\n    Load API configuration from file when the tool starts.\n    \"\"\"\n    self.load_file_handler()\n    data = self.get_file_handler(self.keys[\"Apis\"])\n    try:\n        if isinstance(data, str):\n            self.api_config = json.loads(data)\n        else:\n            self.api_config = data\n        self.logger.info(\"Loaded API configuration: %s\", self.api_config)\n    except Exception as e:\n        self.logger.exception(\"Error loading API configuration: %s\", e)\n        self.api_config = {}\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.restart_api","title":"<code>restart_api(api_name)</code>  <code>async</code>","text":"<p>Restart the given API by stopping it and starting it again.</p> <p>Parameters:</p> Name Type Description Default <code>api_name</code> <code>str</code> <p>The name of the API to restart.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A status message.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>async def restart_api(self, api_name: str) -&gt; str:\n    \"\"\"\n    Restart the given API by stopping it and starting it again.\n\n    Args:\n        api_name (str): The name of the API to restart.\n\n    Returns:\n        str: A status message.\n    \"\"\"\n    stop_message = await self.stop_api(api_name)\n    self.logger.info(\"Restart: %s\", stop_message)\n    # Allow some time for the process to fully terminate.\n    time.sleep(4)\n    start_message = self._start_api(api_name)\n    return f\"Restarting API '{api_name}': {start_message}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.show_running","title":"<code>show_running()</code>","text":"<p>Display and return the list of currently running APIs with their status.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def show_running(self) -&gt; list[str]:\n    \"\"\"\n    Display and return the list of currently running APIs with their status.\n    \"\"\"\n    self.on_start()\n    running_list = []\n    print(self.api_config)\n    for api_name in self.api_config:\n\n        # Get stored process info\n        process_info = self.get_file_handler(f\"pr{api_name}\")\n        print('#',api_name, '#',process_info)\n        if process_info is None:\n            process_info = {}\n        status = {\n            \"name\": api_name,\n            \"online\": api_name in self.running_apis,\n            \"start_time\": process_info.get(\"start_time\", \"offline\"),\n            \"pid\": process_info.get(\"pid\", ''),\n            \"host\": process_info.get(\"host\", ''),\n            \"port\": process_info.get(\"port\", '')\n        }\n        running_list.append(status)\n\n    # Log and print current status\n    self.logger.info(\"APIs: %s\", running_list)\n    print(\"\\nAPIs:\")\n    for api in running_list:\n        print(f\"- {api['name']}: at {api['host']}:{api['port']}\")\n        print(f\"  Started: {api['start_time']}\")\n\n    return [api[\"name\"] for api in running_list]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.show_version","title":"<code>show_version()</code>","text":"<p>Display and return the current version.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def show_version(self) -&gt; str:\n    \"\"\"Display and return the current version.\"\"\"\n    self.logger.info(\"Version: %s\", self.version)\n    return self.version\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.start_debug","title":"<code>start_debug(api_name)</code>","text":"<p>Start an API in debug mode.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def start_debug(self, api_name: str) -&gt; str | None:\n    \"\"\"\n    Start an API in debug mode.\n    \"\"\"\n    return self._start_api(api_name, live=False, reload=True, test_override=True, host=\"localhost\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.start_dev","title":"<code>start_dev(api_name, *modules, **kwargs)</code>","text":"<p>Start an API in development mode.</p> <p>If additional modules are provided, they are stored in a BlobFile for later use.</p> <p>Parameters:</p> Name Type Description Default <code>api_name</code> <code>str</code> <p>The API name.</p> required <code>*modules</code> <code>str</code> <p>Additional modules for the API.</p> <code>()</code> <p>Returns:</p> Type Description <code>str | None</code> <p>Optional[str]: Status message.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def start_dev(self, api_name: str, *modules: str, **kwargs: Any) -&gt; str | None:\n    \"\"\"\n    Start an API in development mode.\n\n    If additional modules are provided, they are stored in a BlobFile for later use.\n\n    Args:\n        api_name (str): The API name.\n        *modules (str): Additional modules for the API.\n\n    Returns:\n        Optional[str]: Status message.\n    \"\"\"\n    if modules:\n        api_name_dev = f\"{api_name}_D\"\n        with BlobFile(f\"FastApi/{api_name_dev}/dev\", mode='w') as f:\n            f.write_json({'modules': modules})\n        api_name = api_name_dev\n\n    return self._start_api(api_name, live=False, reload=False, test_override=False, host=\"localhost\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.start_live","title":"<code>start_live(api_name)</code>","text":"<p>Start an API in live mode.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def start_live(self, api_name: str) -&gt; str | None:\n    \"\"\"\n    Start an API in live mode.\n    \"\"\"\n    return self._start_api(api_name, live=True, reload=False, test_override=False)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.stop_api","title":"<code>stop_api(api_name, delete=True)</code>  <code>async</code>","text":"<p>Stop a running API and clean up resources.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>async def stop_api(self, api_name: str, delete: bool = True) -&gt; str:\n    \"\"\"\n    Stop a running API and clean up resources.\n    \"\"\"\n    if api_name not in self.api_config:\n        msg = f\"API with the name '{api_name}' is not configured.\"\n        self.logger.warning(msg)\n        return msg\n\n    pid_file = self._get_pid_file_path(api_name)\n    if not os.path.exists(pid_file):\n        self.logger.warning(\"No pid file found for API '%s'\", api_name)\n        return f\"No pid file found for API '{api_name}'.\"\n\n    try:\n        # Read PID from file\n        with open(pid_file) as f:\n            api_pid = int(f.read().strip())\n\n        # Try graceful shutdown first\n        if 'core' in self.app.id:\n            if not await self.app.session.login():\n                self.logger.warning(\"Could not login with username '%s'\", self.app.get_username())\n            try:\n                response = await self.app.session.fetch(f\"/api/exit/{api_pid}\", method=\"POST\")\n                self.logger.info(\"Exit response for API '%s': %s\", api_name, response)\n            except Exception as e:\n                self.logger.warning(\"Failed to stop API gracefully: %s\", e)\n\n        # Force kill if process still exists\n        process = self.running_apis.get(api_name)\n        if process and process.is_alive():\n            process.terminate()\n            process.join(timeout=5)\n            if process.is_alive():\n                process.kill()\n\n        # Fallback to system commands if needed\n        try:\n            if system() == \"Windows\":\n                os.system(f\"taskkill /pid {api_pid} /F\")\n            else:\n                os.kill(api_pid, signal.SIGKILL)\n        except ProcessLookupError:\n            pass  # Process already terminated\n\n        # Cleanup\n        if os.path.exists(pid_file):\n            os.remove(pid_file)\n        if delete and api_name in self.running_apis:\n            del self.running_apis[api_name]\n\n        # Update file handler\n        self.add_to_save_file_handler(\n            key=f\"pr{api_name}\",\n            value=json.dumps({\n                \"stop_time\": datetime.now().isoformat(),\n                \"status\": \"stopped\"\n            })\n        )\n        self.save_file_handler()\n\n        msg = f\"Stopped API '{api_name}'.\"\n        self.logger.info(msg)\n        return msg\n\n    except Exception as e:\n        self.logger.exception(\"Error stopping API '%s': %s\", api_name, e)\n        return f\"Error stopping API '{api_name}': {e}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FileWidget","title":"<code>FileWidget</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.FileWidget.FileUploadHandler","title":"<code>FileUploadHandler</code>","text":"Source code in <code>toolboxv2/mods/FileWidget.py</code> <pre><code>class FileUploadHandler:\n    def __init__(self, upload_dir: str = 'uploads'):\n        self.upload_dir = upload_dir\n        os.makedirs(upload_dir, exist_ok=True)\n\n    def save_file(self, chunk_info: ChunkInfo, storage=None) -&gt; str:\n        \"\"\"Speichert die Datei oder Chunk\"\"\"\n        if chunk_info.total_chunks == 1:\n            # Komplette Datei speichern\n            filepath = os.path.join(self.upload_dir, chunk_info.filename)\n            with BlobFile(filepath, 'w', storage=storage) as f:\n                f.write(chunk_info.content)\n        else:\n            # Chunk speichern\n            chunk_path = os.path.join(\n                self.upload_dir,\n                f\"{chunk_info.filename}.part{chunk_info.chunk_index}\"\n            )\n            with open(chunk_path, 'wb') as f:\n                f.write(chunk_info.content)\n\n            # Wenn alle Chunks da sind, zusammenf\u00fcgen\n            if self._all_chunks_received(chunk_info):\n                self._merge_chunks(chunk_info, storage=storage)\n                self._cleanup_chunks(chunk_info)\n\n        return os.path.join(self.upload_dir, chunk_info.filename)\n\n    def _all_chunks_received(self, chunk_info: ChunkInfo) -&gt; bool:\n        \"\"\"Pr\u00fcft ob alle Chunks empfangen wurden\"\"\"\n        if chunk_info.total_chunks is None:\n            return False\n\n        for i in range(chunk_info.total_chunks):\n            chunk_path = os.path.join(\n                self.upload_dir,\n                f\"{chunk_info.filename}.part{i}\"\n            )\n            if not os.path.exists(chunk_path):\n                return False\n        return True\n\n    def _merge_chunks(self, chunk_info: ChunkInfo, storage=None):\n        \"\"\"F\u00fcgt alle Chunks zusammen\"\"\"\n        filepath = os.path.join(self.upload_dir, chunk_info.filename)\n        print(\"filepath\", filepath)\n        with BlobFile(filepath, 'w', storage=storage) as outfile:\n            for i in range(chunk_info.total_chunks):\n                chunk_path = os.path.join(\n                    self.upload_dir,\n                    f\"{chunk_info.filename}.part{i}\"\n                )\n                with open(chunk_path, 'rb') as chunk:\n                    outfile.write(chunk.read())\n\n    def _cleanup_chunks(self, chunk_info: ChunkInfo):\n        \"\"\"L\u00f6scht tempor\u00e4re Chunk-Dateien\"\"\"\n        for i in range(chunk_info.total_chunks):\n            chunk_path = os.path.join(\n                self.upload_dir,\n                f\"{chunk_info.filename}.part{i}\"\n            )\n            if os.path.exists(chunk_path):\n                os.remove(chunk_path)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FileWidget.FileUploadHandler.save_file","title":"<code>save_file(chunk_info, storage=None)</code>","text":"<p>Speichert die Datei oder Chunk</p> Source code in <code>toolboxv2/mods/FileWidget.py</code> <pre><code>def save_file(self, chunk_info: ChunkInfo, storage=None) -&gt; str:\n    \"\"\"Speichert die Datei oder Chunk\"\"\"\n    if chunk_info.total_chunks == 1:\n        # Komplette Datei speichern\n        filepath = os.path.join(self.upload_dir, chunk_info.filename)\n        with BlobFile(filepath, 'w', storage=storage) as f:\n            f.write(chunk_info.content)\n    else:\n        # Chunk speichern\n        chunk_path = os.path.join(\n            self.upload_dir,\n            f\"{chunk_info.filename}.part{chunk_info.chunk_index}\"\n        )\n        with open(chunk_path, 'wb') as f:\n            f.write(chunk_info.content)\n\n        # Wenn alle Chunks da sind, zusammenf\u00fcgen\n        if self._all_chunks_received(chunk_info):\n            self._merge_chunks(chunk_info, storage=storage)\n            self._cleanup_chunks(chunk_info)\n\n    return os.path.join(self.upload_dir, chunk_info.filename)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FileWidget.FileWidget","title":"<code>FileWidget</code>","text":"<p>               Bases: <code>MainTool</code>, <code>BaseWidget</code></p> Source code in <code>toolboxv2/mods/FileWidget.py</code> <pre><code>class FileWidget(MainTool, BaseWidget):\n    def __init__(self, app=None):\n        self.name = \"FileWidget\"\n        self.color = \"WHITE\"\n        self.tools = {'name': self.name}\n        self.version = \"1.0.0\"\n        MainTool.__init__(self,\n                          load=self.on_start,\n                          v=self.version,\n                          name=self.name,\n                          color=self.color,\n                          on_exit=self.on_exit)\n\n        BaseWidget.__init__(self, name=self.name)\n        self.register(self.app, self.get_widget, self.version)\n        self.register(self.app, self.handle_upload, self.version, name=\"upload\")\n        self.register(self.app, self.handle_download, self.version, name=\"download\", row=True)\n        self.register(self.app, self.get_file_tree, self.version, name=\"files\")\n\n        self.blob_storage = {}\n\n    async def get_blob_storage(self, request):\n        user = await self.get_user_from_request(self.app, request)\n        if user.name == \"\":\n            return BlobStorage(self.app.data_dir + '/public', 0)\n        if user.name == \"root\":\n            return BlobStorage()\n        if user.name not in self.blob_storage:\n            self.blob_storage[user.name] = BlobStorage(\n                self.app.data_dir + '/storages/' + user.uid)\n        return self.blob_storage[user.name]\n\n    def main(self, request):\n        w_id = self.get_s_id(request)\n        if w_id.is_error():\n            return w_id\n        self.asset_loder(self.app, \"main\", self.hash_wrapper(w_id.get()), template=self.get_template())\n\n    def get_template(self):\n        return \"\"\"\n        &lt;title&gt;File Manager&lt;/title&gt;\n        &lt;style&gt;\n        .tree-view {\n            font-family: monospace;\n            margin: 10px 0;\n            border: 1px solid #ddd;\n            padding: 10px;\n            max-height: 600px;\n            overflow-y: auto;\n            background: #f8f9fa;\n        }\n\n        .folder-group {\n            font-weight: bold;\n            color: #2c3e50;\n            padding: 5px;\n            margin-top: 10px;\n            background: #edf2f7;\n            border-radius: 4px;\n            cursor: pointer;\n        }\n\n        .group-content {\n            margin-left: 20px;\n            border-left: 2px solid #e2e8f0;\n            padding-left: 10px;\n        }\n\n        .folder {\n            cursor: pointer;\n            padding: 2px 5px;\n            margin: 2px 0;\n            color: #4a5568;\n        }\n\n        .folder:hover {\n            background: #edf2f7;\n            border-radius: 4px;\n        }\n\n        .file {\n            padding: 2px 5px;\n            margin: 2px 0;\n            cursor: pointer;\n            color: #718096;\n            transition: background-color 0.2s;\n        }\n\n        .file:hover {\n            background: #edf2f7;\n            border-radius: 4px;\n            color: #2d3748;\n        }\n\n        .folder-content {\n            margin-left: 20px;\n            border-left: 1px solid #e2e8f0;\n            padding-left: 10px;\n            display: none;\n        }\n\n\n        .folder-content.open {\n            display: block;\n        }\n\n        .folder::before {\n            content: '\u25b6'; /* Geschlossener Pfeil */\n            display: inline-block;\n            margin-right: 5px;\n            transition: transform 0.2s;\n        }\n\n        .folder.open::before {\n            transform: rotate(90deg); /* Pfeil dreht sich beim \u00d6ffnen */\n        }\n\n        .group-content {\n            display: none; /* Standardm\u00e4\u00dfig eingeklappt */\n        }\n\n        .group-content.open {\n            display: block;\n        }\n\n        .folder-group::before {\n            content: '\u25b6';\n            display: inline-block;\n            margin-right: 5px;\n            transition: transform 0.2s;\n        }\n\n        .folder-group.open::before {\n            transform: rotate(90deg);\n        }\n\n        .drop-zone {\n            border: 2px dashed #ccc;\n            padding: 20px;\n            text-align: center;\n            margin: 10px 0;\n            cursor: pointer;\n        }\n\n        .drop-zone.dragover {\n            background-color: #e1e1e1;\n            border-color: #999;\n        }\n\n        .progress-bar {\n            width: 100%;\n            height: 20px;\n            background-color: #f0f0f0;\n            border-radius: 4px;\n            overflow: hidden;\n        }\n\n        .progress {\n            width: 0%;\n            height: 100%;\n            background-color: #4CAF50;\n            transition: width 0.3s ease-in-out;\n        }\n        &lt;/style&gt;\n\n        &lt;div class=\"file-container\"&gt;\n            &lt;h2&gt;File Manager&lt;/h2&gt;\n\n            &lt;div class=\"drop-zone\" id=\"dropZone\"&gt;\n                &lt;p&gt;Drag &amp; Drop files here or click to upload&lt;/p&gt;\n                &lt;input type=\"file\" id=\"fileInput\" multiple style=\"display: none;\"&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"progress-bar\" style=\"display: none;\"&gt;\n                &lt;div class=\"progress\" id=\"uploadProgress\"&gt;&lt;/div&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"tree-view\" id=\"fileTree\"&gt;&lt;/div&gt;\n        &lt;/div&gt;\n\n        &lt;script unSave=\"true\"&gt;\n            class FileManager {\n                constructor() {\n                    this.dropZone = document.getElementById('dropZone');\n                    this.fileInput = document.getElementById('fileInput');\n                    this.fileTree = document.getElementById('fileTree');\n                    this.progressBar = document.querySelector('.progress-bar');\n                    this.progress = document.getElementById('uploadProgress');\n                    this.response = {};\n\n                    this.initEventListeners();\n                    this.initLoadFileTree();\n                }\n\n                initEventListeners() {\n                    this.dropZone.addEventListener('click', () =&gt; this.fileInput.click());\n                    this.fileInput.addEventListener('change', (e) =&gt; this.handleFiles(e.target.files));\n\n                    this.dropZone.addEventListener('dragover', (e) =&gt; {\n                        e.preventDefault();\n                        this.dropZone.classList.add('dragover');\n                    });\n\n                    this.dropZone.addEventListener('dragleave', () =&gt; {\n                        this.dropZone.classList.remove('dragover');\n                    });\n\n                    this.dropZone.addEventListener('drop', (e) =&gt; {\n                        e.preventDefault();\n                        this.dropZone.classList.remove('dragover');\n                        this.handleFiles(e.dataTransfer.files);\n                    });\n                }\n\n                async handleFiles(files) {\n                    for (const file of files) {\n                        await this.uploadFile(file);\n                    }\n                    this.loadFileTree();\n                }\n\n                async uploadFile(file) {\n                    this.progressBar.style.display = 'block';\n\n                    const chunkSize = 1024 * 1024; // 1MB chunks\n                    const totalChunks = Math.ceil(file.size / chunkSize);\n\n                    for (let i = 0; i &lt; totalChunks; i++) {\n                        const chunk = file.slice(i * chunkSize, (i + 1) * chunkSize);\n                        const formData = new FormData();\n                        formData.append('file', chunk);\n                        formData.append('fileName', file.name);\n                        formData.append('chunkIndex', i);\n                        formData.append('totalChunks', totalChunks);\n\n                        await fetch('/api/FileWidget/upload', {\n                            method: 'POST',\n                            body: formData\n                        });\n\n                        const progress = ((i + 1) / totalChunks) * 100;\n                        this.progress.style.width = progress + '%';\n                    }\n\n                    setTimeout(() =&gt; {\n                        this.progressBar.style.display = 'none';\n                        this.progress.style.width = '0%';\n                    }, 1000);\n                }\n\n                initLoadFileTree() {\n                    setTimeout(async () =&gt; {\n                        await this.loadFileTree();\n                    }, 1000);\n                }\n                async loadFileTree() {\n                    const response = await fetch('/api/FileWidget/files');\n                    const files = await response.json();\n                    this.renderFileTree(files);\n                }\n\n                renderFileTree(files) {\n                    this.fileTree.innerHTML = this.buildTreeHTML(files);\n                    this.addTreeEventListeners();\n                }\n\n                buildTreeHTML(response, level = 0) {\n                    // \u00dcberpr\u00fcfe auf API-Antwortstruktur und extrahiere die relevanten Daten\n                    if (typeof(response) == \"string\"){\n                        response = JSON.parse(response);\n                    }\n                    console.log(\"buildTreeHTML\", response, response.result)\n                    if (response.result &amp;&amp; response.result.data) {\n                        return this.buildTreeHTML(response.result.data, level);\n                    }\n                    if (response['result'] &amp;&amp; response['result']['data']) {\n                        return this.buildTreeHTML(response['result']['data'], level);\n                    }\n                    this.response = Object.assign({}, this.response, response);\n                    // Wenn es ein einfaches Key-Value Paar ist\n                    if (typeof response === 'object' &amp;&amp; !Array.isArray(response)) {\n                        let html = '';\n                        const sorted = Object.entries(response).sort(([keyA], [keyB]) =&gt; {\n                            const extA = keyA.split('.').pop() || '';\n                            const extB = keyB.split('.').pop() || '';\n                            if (extA === extB) {\n                                return keyA.localeCompare(keyB);\n                            }\n                            return extA.localeCompare(extB);\n                        });\n\n                        let currentGroup = '';\n                        let indent =  '';\n                        for (const [key, value] of sorted) {\n                            indent = '    '.repeat(level);\n                            const fileExt = key.split('.').pop() || '';\n\n                            if (fileExt !== currentGroup) {\n                                currentGroup = fileExt;\n                                if (level === 0) {\n                                    html += indent + '&lt;div class=\"folder-group\"&gt;\ud83d\udcc1 ' + currentGroup.toUpperCase() + ' Files&lt;/div&gt;';\n                                    html += indent + '&lt;div class=\"group-content\" style=\"margin-left: 20px\"&gt;';\n                                }\n                            }\n\n                            const icon = this.getFileIcon(key);\n\n                            if (typeof value === 'object' &amp;&amp; value !== null) {\n                                html += indent + '&lt;div class=\"folder\" data-folder=\"' + key + '\"&gt;\ud83d\udcc1 ' + key + '&lt;/div&gt;';\n                                html += indent + '&lt;div class=\"folder-content\" style=\"margin-left: 20px\"&gt;';\n                                html += this.buildTreeHTML(value, level + 1);\n                                html += indent + '&lt;/div&gt;';\n                            } else {\n                                html += indent + '&lt;div class=\"file\" data-path=\"' + key + '\"&gt;' + icon + ' ' + key + '&lt;/div&gt;';\n                            }\n\n                            const nextEntry = sorted[sorted.indexOf([key, value]) + 1];\n                            if (level === 0 &amp;&amp; nextEntry) {\n                                const nextExt = nextEntry[0].split('.').pop() || '';\n                                if (nextExt !== currentGroup) {\n                                    html += indent + '&lt;/div&gt;';\n                                }\n                            }\n                        }\n\n                        if (level === 0 &amp;&amp; currentGroup) {\n                            html += indent + '&lt;/div&gt;';\n                        }\n\n                        return html;\n                    }\n\n                    if (typeof response === 'string') {\n                        const icon = this.getFileIcon(response);\n                        return '&lt;div class=\"file\" data-path=\"' + response + '\"&gt;' + icon + ' ' + response + '&lt;/div&gt;';\n                    }\n\n                    return '';\n                }\n\n                getFileIcon(filename) {\n                    const ext = filename.split('.').pop()?.toLowerCase();\n                    const iconMap = {\n                        'agent': '\ud83e\udd16',\n                        'json': '\ud83d\udccb',\n                        'pkl': '\ud83d\udce6',\n                        'txt': '\ud83d\udcdd',\n                        'data': '\ud83d\udcbe',\n                        'ipy': '\ud83d\udc0d',\n                        'bin': '\ud83d\udcc0',\n                        'sqlite3': '\ud83d\uddc4\ufe0f',\n                        'vec': '\ud83d\udcca',\n                        'pickle': '\ud83e\udd52',\n                        'html': '\ud83c\udf10',\n                        'javascript': '\ud83d\udcdc',\n                        'markdown': '\ud83d\udcd1',\n                        'python': '\ud83d\udc0d',\n                        'default': '\ud83d\udcc4'\n                    };\n                    return iconMap[ext] || iconMap['default'];\n                }\n\n                addTreeEventListeners() {\n                    document.querySelectorAll('.file').forEach(file =&gt; {\n                        file.addEventListener('click', () =&gt; this.downloadFile(file.dataset.path));\n                    });\n\n                    // Neue Event-Listener f\u00fcr Ordner\n                    document.querySelectorAll('.folder').forEach(folder =&gt; {\n                        folder.addEventListener('click', (e) =&gt; {\n                            e.stopPropagation();\n                            folder.classList.toggle('open');\n                            const content = folder.nextElementSibling;\n                            if (content &amp;&amp; content.classList.contains('folder-content')) {\n                                content.classList.toggle('open');\n                            }\n                        });\n                    });\n\n                    // Event-Listener f\u00fcr Gruppen\n                    document.querySelectorAll('.folder-group').forEach(group =&gt; {\n                        group.addEventListener('click', (e) =&gt; {\n                            e.stopPropagation();\n                            group.classList.toggle('open');\n                            const content = group.nextElementSibling;\n                            if (content &amp;&amp; content.classList.contains('group-content')) {\n                                content.classList.toggle('open');\n                            }\n                        });\n                    });\n                }\n                async downloadFile(path) {\n                    if (this.response){\n                        path = this.response[path]\n                    }\n                    const response = await fetch(`/api/FileWidget/download?path=`+encodeURIComponent(path));\n                    const blob = await response.blob();\n                    const url = window.URL.createObjectURL(blob);\n                    const a = document.createElement('a');\n                    a.href = url;\n                    a.download = path.split('/').pop();\n                    document.body.appendChild(a);\n                    a.click();\n                    document.body.removeChild(a);\n                    window.URL.revokeObjectURL(url);\n                }\n            }\n\n            new FileManager();\n        &lt;/script&gt;\n        \"\"\"\n\n    async def handle_upload(self, request: RequestSession):\n\n        if request is None:\n            return None\n\n        body = request.body\n        storage = await self.get_blob_storage(request)\n\n        parser = MultipartParser(body)\n        chunk_info = parser.parse()\n\n        handler = FileUploadHandler()\n        saved_path = handler.save_file(chunk_info, storage)\n        return saved_path\n\n        ## Erstellen oder aktualisieren der BlobFile\n#\n        #with BlobFile('userData/'+file.filename, 'w', storage=storage) as bf:\n        #    while contents := file.file.read(1024 * 1024):\n        #        bf.write(contents)\n\n\n    async def handle_download(self, request, path):\n        \"\"\"\n            Handle file downloads for BlobFile using Starlette response.\n\n            Args:\n                request: The Starlette request object\n                path: The blob file path to download\n\n            Returns:\n                Starlette Response with file data and appropriate headers\n            \"\"\"\n        try:\n            # Remove leading slash if present for consistency with BlobFile\n            if path.startswith('/'):\n                path = path[1:]\n\n            # Get the filename from the path\n            filename = Path(path).name\n\n            # Detect content type based on file extension\n            content_type, _ = mimetypes.guess_type(filename)\n            if content_type is None:\n                content_type = 'application/octet-stream'\n            storage = await self.get_blob_storage(request)\n            # Read the file data\n            with BlobFile(path, 'r', storage=storage) as bf:\n                data = bf.read()\n\n            # Create headers for the response\n            headers = {\n                'Content-Disposition': f'attachment; filename=\"{filename}\"',\n                'Content-Length': str(len(data)),\n                'Content-Type': content_type\n            }\n\n            return Response(\n                content=data,\n                headers=headers,\n                media_type=content_type\n            )\n\n        except FileNotFoundError:\n            return Response(\n                content=\"File not found\",\n                status_code=404\n            )\n        except Exception as e:\n            return Response(\n                content=f\"Error processing download: {str(e)}\",\n                status_code=500\n            )\n\n    async def get_file_tree(self, request):\n\n        def flatten_dict(d, parent_key='', sep='.'):\n            items = {}\n            for k, v in d.items():\n                # Erstelle einen neuen Schl\u00fcssel durch Anh\u00e4ngen des aktuellen Schl\u00fcssels\n                new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n                # Wenn der Wert wieder ein Dictionary ist, rufe die Funktion rekursiv auf\n                if isinstance(v, dict):\n                    items.update(flatten_dict(v, new_key, sep=sep))\n                else:\n                    items[new_key] = v\n            return items\n\n        # Implementierung der Verzeichnisstruktur\n        tree = {}\n        storage = await self.get_blob_storage(request)\n        blob_ids = storage._get_all_blob_ids()\n        folder_list = []\n        for blob_id in blob_ids:\n            blob_data = pickle.loads(storage.read_blob(blob_id))\n            folder_list.extend(flatten_dict(blob_data, blob_id, '/').keys())\n\n        for folder in folder_list:\n            path_parts = folder.split('/')\n            current = tree\n            for part in path_parts[:-1]:\n                if not part:\n                    continue\n                if part not in current:\n                    current[part] = {}\n                current = current[part]\n            current[path_parts[-1]] = folder\n\n        return tree\n\n    def on_start(self):\n        self.register2reload(self.main)\n        # API-Routen registrieren\n\n    def on_exit(self):\n        pass\n\n    async def get_widget(self, request, **kwargs):\n        w_id = self.get_s_id(request)\n        if w_id.is_error():\n            return w_id\n        self.reload_guard(self.on_start)\n        return self.load_widget(self.app, request, \"main\", self.hash_wrapper(w_id.get()))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FileWidget.FileWidget.handle_download","title":"<code>handle_download(request, path)</code>  <code>async</code>","text":"<p>Handle file downloads for BlobFile using Starlette response.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The Starlette request object</p> required <code>path</code> <p>The blob file path to download</p> required <p>Returns:</p> Type Description <p>Starlette Response with file data and appropriate headers</p> Source code in <code>toolboxv2/mods/FileWidget.py</code> <pre><code>async def handle_download(self, request, path):\n    \"\"\"\n        Handle file downloads for BlobFile using Starlette response.\n\n        Args:\n            request: The Starlette request object\n            path: The blob file path to download\n\n        Returns:\n            Starlette Response with file data and appropriate headers\n        \"\"\"\n    try:\n        # Remove leading slash if present for consistency with BlobFile\n        if path.startswith('/'):\n            path = path[1:]\n\n        # Get the filename from the path\n        filename = Path(path).name\n\n        # Detect content type based on file extension\n        content_type, _ = mimetypes.guess_type(filename)\n        if content_type is None:\n            content_type = 'application/octet-stream'\n        storage = await self.get_blob_storage(request)\n        # Read the file data\n        with BlobFile(path, 'r', storage=storage) as bf:\n            data = bf.read()\n\n        # Create headers for the response\n        headers = {\n            'Content-Disposition': f'attachment; filename=\"{filename}\"',\n            'Content-Length': str(len(data)),\n            'Content-Type': content_type\n        }\n\n        return Response(\n            content=data,\n            headers=headers,\n            media_type=content_type\n        )\n\n    except FileNotFoundError:\n        return Response(\n            content=\"File not found\",\n            status_code=404\n        )\n    except Exception as e:\n        return Response(\n            content=f\"Error processing download: {str(e)}\",\n            status_code=500\n        )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager","title":"<code>SchedulerManager</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.SchedulerManagerClass","title":"<code>SchedulerManagerClass</code>","text":"Source code in <code>toolboxv2/mods/SchedulerManager.py</code> <pre><code>class SchedulerManagerClass:\n    def __init__(self):\n        self.jobs = {}\n        self.thread = None\n        self.running = False\n        self.last_successful_jobs = deque(maxlen=3)  # Stores last 3 successful job names\n        self.job_errors = {}  # Stores job names as keys and error messages as values\n\n    def _run(self):\n        while self.running:\n            schedule.run_pending()\n            time.sleep(1)\n\n    def start(self):\n        if not self.running:\n            self.running = True\n            self.thread = threading.Thread(target=self._run, daemon=True)\n            self.thread.start()\n\n    def stop(self):\n        self.running = False\n        if self.thread is not None:\n            self.thread.join()\n\n    def job_wrapper(self, job_name: str, job_function: callable):\n        \"\"\"\n        Wrap a job function to track success and errors.\n        \"\"\"\n        def wrapped_job(*args, **kwargs):\n            try:\n                job_function(*args, **kwargs)\n                # If the job ran successfully, store it in the success queue\n                self.last_successful_jobs.append(job_name)\n                if job_name in self.job_errors:\n                    del self.job_errors[job_name]  # Remove error record if job succeeded after failing\n            except Exception as e:\n                # Capture any exceptions and store them\n                self.job_errors[job_name] = str(e)\n\n        return wrapped_job\n\n\n    def register_job(self,\n                     job_id: str,\n                     second: int = -1,\n                     func: (Callable or str) | None = None,\n                     job: schedule.Job | None = None,\n                     time_passer: schedule.Job | None = None,\n                     object_name: str | None = None,\n                     receive_job: bool = False,\n                     save: bool = False,\n                     max_live: bool = False,\n                     serializer=serializer_default,\n                     args=None, kwargs=None):\n        \"\"\"\n            Parameters\n            ----------\n                job_id : str\n                    id for the job for management\n                second : int\n                    The time interval in seconds between each call of the job.\n                func : Callable or str\n                    The function to be executed as the job.\n                job : schedule.Job\n                    An existing job object from the schedule library.\n                time_passer : schedule.Job\n                    A job without a function, used to specify the time interval.\n                object_name : str\n                    The name of the object containing in the 'func' var to be executed.\n                receive_job : bool\n                    A flag indicating whether the job should be received from an object from 'func' var.\n                save : bool\n                    A flag indicating whether the job should be saved.\n                max_live : bool\n                    A flag indicating whether the job should have a maximum live time.\n                serializer : dill\n                    json pickel or dill must have a dumps fuction\n                *args, **kwargs : Any serializable and deserializable\n                    Additional arguments to be passed to the job function.\n\n            Returns\n            -------\n           \"\"\"\n\n        if job is None and func is None:\n            return Result.default_internal_error(\"Both job and func are not specified.\"\n                                                 \" Please specify either job or func.\")\n        if job is not None and func is not None:\n            return Result.default_internal_error(\"Both job and func are specified. Please specify either job or func.\")\n\n        if job is not None:\n            def func(x):\n                return x\n            return self._save_job(job_id=job_id,\n                                  job=job,\n                                  save=save,\n                                  func=func,\n                                  args=args,\n                                  kwargs=kwargs,\n                                  serializer=serializer)\n\n        parsed_attr = self._parse_function(func=func, object_name=object_name)\n\n        if parsed_attr.is_error():\n            parsed_attr.result.data_info = f\"Error parsing function for job : {job_id}\"\n            return parsed_attr\n\n        if receive_job:\n            job = parsed_attr.get()\n        else:\n            func = parsed_attr.get()\n\n        time_passer = self._prepare_time_passer(time_passer=time_passer,\n                                                second=second)\n\n        job_func = self._prepare_job_func(func=func,\n                                          max_live=max_live,\n                                          second=second,\n                                          args=args,\n                                          kwargs=kwargs,\n                                          job_id=job_id)\n\n        job = self._get_final_job(job=job,\n                                  func=self.job_wrapper(job_id, job_func),\n                                  time_passer=time_passer,\n                                  job_func=job_func,\n                                  args=args,\n                                  kwargs=kwargs)\n        if job.is_error():\n            return job\n\n        job = job.get()\n\n        return self._save_job(job_id=job_id,\n                              job=job,\n                              save=save,\n                              func=func,\n                              args=args,\n                              kwargs=kwargs,\n                              serializer=serializer)\n\n    @staticmethod\n    def _parse_function(func: str or Callable, object_name):\n        if isinstance(func, str) and func.endswith('.py'):\n            with open(func) as file:\n                func_code = file.read()\n                exec(func_code)\n                func = locals()[object_name]\n        elif isinstance(func, str) and func.endswith('.dill') and safety_mode == 'open':\n            try:\n                with open(func, 'rb') as file:\n                    func = dill.load(file)\n            except FileNotFoundError:\n                return Result.default_internal_error(f\"Function file {func} not found or dill not installed\")\n        elif isinstance(func, str):\n            local_vars = {'app': get_app(from_=Name + f\".pasing.{object_name}\")}\n            try:\n                exec(func.strip(), {}, local_vars)\n            except Exception as e:\n                return Result.default_internal_error(f\"Function parsing failed withe {e}\")\n            func = local_vars[object_name]\n        elif isinstance(func, Callable):\n            pass\n        else:\n            return Result.default_internal_error(\"Could not parse object scheduler_manager.parse_function\")\n        return Result.ok(func)\n\n    @staticmethod\n    def _prepare_time_passer(time_passer, second):\n        if time_passer is None and second &gt; 0:\n            return schedule.every(second).seconds\n        elif time_passer is None and second &lt;= 0:\n            raise ValueError(\"second must be greater than 0\")\n        return time_passer\n\n    def _prepare_job_func(self, func: Callable, max_live: bool, second: float, job_id: str, *args, **kwargs):\n        if max_live:\n            end_time = datetime.now() + timedelta(seconds=second)\n\n            def job_func():\n                if datetime.now() &lt; end_time:\n                    func(*args, **kwargs)\n                else:\n                    job = self.jobs.get(job_id, {}).get('job')\n                    if job is not None:\n                        schedule.cancel_job(job)\n                    else:\n                        print(\"Error Canceling job\")\n\n            return job_func\n        return func\n\n    @staticmethod\n    def _get_final_job(job, func, time_passer, job_func, args, kwargs):\n        if job is None and isinstance(func, Callable):\n            job = time_passer.do(job_func, *args, **kwargs)\n        elif job is not None:\n            pass\n        else:\n            return Result.default_internal_error(\"No Final job found for register\")\n        return Result.ok(job)\n\n    def _save_job(self, job_id, job, save, args=None, **kwargs):\n        if job is not None:\n            self.jobs[job_id] = {'id': job_id, 'job': job, 'save': save, 'func': job_id, 'args': args,\n                                 'kwargs': kwargs}\n            f = (f\"Added Job {job_id} :{' - saved' if save else ''}\"\n                  f\"{' - args ' + str(len(args)) if args else ''}\"\n                  f\"{' - kwargs ' + str(len(kwargs.keys())) if kwargs else ''}\")\n            return Result.ok(f)\n        else:\n            return Result.default_internal_error(job_id)\n\n    def cancel_job(self, job_id):\n        if job_id not in self.jobs:\n            print(\"Job not found\")\n            return\n        schedule.cancel_job(self.jobs[job_id].get('job'))\n        self.jobs[job_id][\"cancelled\"] = True\n        self.jobs[job_id][\"save\"] = False\n        print(\"Job cancelled\")\n\n    def del_job(self, job_id):\n        if job_id not in self.jobs:\n            print(\"Job not found\")\n            return\n        if not self.jobs[job_id].get(\"cancelled\", False):\n            print(\"Job not cancelled canceling job\")\n            self.cancel_job(job_id)\n        del self.jobs[job_id]\n        print(\"Job deleted\")\n\n    def save_jobs(self, file_path, serializer=serializer_default):\n        with open(file_path, 'wb') as file:\n            save_jobs = [job for job in self.jobs.values() if job['save']]\n            serializer.dump(save_jobs, file)\n\n    def load_jobs(self, file_path, deserializer=deserializer_default):\n        with open(file_path, 'rb') as file:\n            jobs = deserializer.load(file)\n            for job_info in jobs:\n                del job_info['job']\n                func = deserializer.loads(job_info['func'])\n                self.register_job(job_info['id'], func=func, **job_info)\n\n    def get_tasks_table(self):\n        if not self.jobs:\n            return \"No tasks registered.\"\n\n        # Calculate the maximum width for each column\n        id_width = max(len(\"Task ID\"), max(len(job_id) for job_id in self.jobs))\n        next_run_width = len(\"Next Execution\")\n        interval_width = len(\"Interval\")\n\n        # Create the header\n        header = f\"| {'Task ID':&lt;{id_width}} | {'Next Execution':&lt;{next_run_width}} | {'Interval':&lt;{interval_width}} |\"\n        separator = f\"|{'-' * (id_width + 2)}|{'-' * (next_run_width + 2)}|{'-' * (interval_width + 2)}|\"\n\n        # Create the table rows\n        rows = []\n        for job_id, job_info in self.jobs.items():\n            job = job_info['job']\n            next_run = job.next_run.strftime(\"%Y-%m-%d %H:%M:%S\") if job.next_run else \"N/A\"\n            interval = self._get_interval_str(job)\n            row = f\"| {job_id:&lt;{id_width}} | {next_run:&lt;{next_run_width}} | {interval:&lt;{interval_width}} |\"\n            rows.append(row)\n\n        # Combine all parts of the table\n        table = \"\\n\".join([header, separator] + rows)\n        return table\n\n    def _get_interval_str(self, job):\n        if job.interval == 0:\n            return \"Once\"\n\n        units = [\n            (86400, \"day\"),\n            (3600, \"hour\"),\n            (60, \"minute\"),\n            (1, \"second\")\n        ]\n\n        for seconds, unit in units:\n            if job.interval % seconds == 0:\n                count = job.interval // seconds\n                return f\"Every {count} {unit}{'s' if count &gt; 1 else ''}\"\n\n        return f\"Every {job.interval} seconds\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.SchedulerManagerClass.job_wrapper","title":"<code>job_wrapper(job_name, job_function)</code>","text":"<p>Wrap a job function to track success and errors.</p> Source code in <code>toolboxv2/mods/SchedulerManager.py</code> <pre><code>def job_wrapper(self, job_name: str, job_function: callable):\n    \"\"\"\n    Wrap a job function to track success and errors.\n    \"\"\"\n    def wrapped_job(*args, **kwargs):\n        try:\n            job_function(*args, **kwargs)\n            # If the job ran successfully, store it in the success queue\n            self.last_successful_jobs.append(job_name)\n            if job_name in self.job_errors:\n                del self.job_errors[job_name]  # Remove error record if job succeeded after failing\n        except Exception as e:\n            # Capture any exceptions and store them\n            self.job_errors[job_name] = str(e)\n\n    return wrapped_job\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.SchedulerManagerClass.register_job","title":"<code>register_job(job_id, second=-1, func=None, job=None, time_passer=None, object_name=None, receive_job=False, save=False, max_live=False, serializer=serializer_default, args=None, kwargs=None)</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.SchedulerManagerClass.register_job--parameters","title":"Parameters","text":"<pre><code>job_id : str\n    id for the job for management\nsecond : int\n    The time interval in seconds between each call of the job.\nfunc : Callable or str\n    The function to be executed as the job.\njob : schedule.Job\n    An existing job object from the schedule library.\ntime_passer : schedule.Job\n    A job without a function, used to specify the time interval.\nobject_name : str\n    The name of the object containing in the 'func' var to be executed.\nreceive_job : bool\n    A flag indicating whether the job should be received from an object from 'func' var.\nsave : bool\n    A flag indicating whether the job should be saved.\nmax_live : bool\n    A flag indicating whether the job should have a maximum live time.\nserializer : dill\n    json pickel or dill must have a dumps fuction\n*args, **kwargs : Any serializable and deserializable\n    Additional arguments to be passed to the job function.\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.SchedulerManagerClass.register_job--returns","title":"Returns","text":"Source code in <code>toolboxv2/mods/SchedulerManager.py</code> <pre><code>def register_job(self,\n                 job_id: str,\n                 second: int = -1,\n                 func: (Callable or str) | None = None,\n                 job: schedule.Job | None = None,\n                 time_passer: schedule.Job | None = None,\n                 object_name: str | None = None,\n                 receive_job: bool = False,\n                 save: bool = False,\n                 max_live: bool = False,\n                 serializer=serializer_default,\n                 args=None, kwargs=None):\n    \"\"\"\n        Parameters\n        ----------\n            job_id : str\n                id for the job for management\n            second : int\n                The time interval in seconds between each call of the job.\n            func : Callable or str\n                The function to be executed as the job.\n            job : schedule.Job\n                An existing job object from the schedule library.\n            time_passer : schedule.Job\n                A job without a function, used to specify the time interval.\n            object_name : str\n                The name of the object containing in the 'func' var to be executed.\n            receive_job : bool\n                A flag indicating whether the job should be received from an object from 'func' var.\n            save : bool\n                A flag indicating whether the job should be saved.\n            max_live : bool\n                A flag indicating whether the job should have a maximum live time.\n            serializer : dill\n                json pickel or dill must have a dumps fuction\n            *args, **kwargs : Any serializable and deserializable\n                Additional arguments to be passed to the job function.\n\n        Returns\n        -------\n       \"\"\"\n\n    if job is None and func is None:\n        return Result.default_internal_error(\"Both job and func are not specified.\"\n                                             \" Please specify either job or func.\")\n    if job is not None and func is not None:\n        return Result.default_internal_error(\"Both job and func are specified. Please specify either job or func.\")\n\n    if job is not None:\n        def func(x):\n            return x\n        return self._save_job(job_id=job_id,\n                              job=job,\n                              save=save,\n                              func=func,\n                              args=args,\n                              kwargs=kwargs,\n                              serializer=serializer)\n\n    parsed_attr = self._parse_function(func=func, object_name=object_name)\n\n    if parsed_attr.is_error():\n        parsed_attr.result.data_info = f\"Error parsing function for job : {job_id}\"\n        return parsed_attr\n\n    if receive_job:\n        job = parsed_attr.get()\n    else:\n        func = parsed_attr.get()\n\n    time_passer = self._prepare_time_passer(time_passer=time_passer,\n                                            second=second)\n\n    job_func = self._prepare_job_func(func=func,\n                                      max_live=max_live,\n                                      second=second,\n                                      args=args,\n                                      kwargs=kwargs,\n                                      job_id=job_id)\n\n    job = self._get_final_job(job=job,\n                              func=self.job_wrapper(job_id, job_func),\n                              time_passer=time_passer,\n                              job_func=job_func,\n                              args=args,\n                              kwargs=kwargs)\n    if job.is_error():\n        return job\n\n    job = job.get()\n\n    return self._save_job(job_id=job_id,\n                          job=job,\n                          save=save,\n                          func=func,\n                          args=args,\n                          kwargs=kwargs,\n                          serializer=serializer)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.Tools","title":"<code>Tools</code>","text":"<p>               Bases: <code>MainTool</code>, <code>SchedulerManagerClass</code></p> Source code in <code>toolboxv2/mods/SchedulerManager.py</code> <pre><code>class Tools(MainTool, SchedulerManagerClass):\n    version = version\n\n    def __init__(self, app=None):\n        self.name = Name\n        self.color = \"VIOLET2\"\n\n        self.keys = {\"mode\": \"db~mode~~:\"}\n        self.encoding = 'utf-8'\n        self.tools = {'name': Name}\n        MainTool.__init__(self,\n                          load=self.init_sm,\n                          v=self.version,\n                          name=self.name,\n                          color=self.color,\n                          on_exit=self.on_exit)\n\n        SchedulerManagerClass.__init__(self)\n\n    @export(\n        mod_name=Name,\n        name=\"Version\",\n        version=version,\n    )\n    def get_version(self):\n        return self.version\n\n    # Exportieren der Scheduler-Instanz f\u00fcr die Nutzung in anderen Modulen\n    @export(mod_name=Name, name='init', version=version, initial=True)\n    def init_sm(self):\n        if os.path.exists(self.app.data_dir + '/jobs.compact'):\n            print(\"SchedulerManager try loading from file\")\n            self.load_jobs(\n                self.app.data_dir + '/jobs.compact'\n            )\n            print(\"SchedulerManager Successfully loaded\")\n        print(\"STARTING SchedulerManager\")\n        self.start()\n\n    @export(mod_name=Name, name='clos_manager', version=version, exit_f=True)\n    def on_exit(self):\n        self.stop()\n        self.save_jobs(self.app.data_dir + '/jobs.compact')\n        return f\"saved {len(self.jobs.keys())} jobs in {self.app.data_dir + '/jobs.compact'}\"\n\n    @export(mod_name=Name, name='instance', version=version)\n    def get_instance(self):\n        return self\n\n    @export(mod_name=Name, name='start', version=version)\n    def start_instance(self):\n        return self.start()\n\n    @export(mod_name=Name, name='stop', version=version)\n    def stop_instance(self):\n        return self.stop()\n\n    @export(mod_name=Name, name='cancel', version=version)\n    def cancel_instance(self, job_id):\n        return self.cancel_job(job_id)\n\n    @export(mod_name=Name, name='dealt', version=version)\n    def dealt_instance(self, job_id):\n        return self.del_job(job_id)\n\n    @export(mod_name=Name, name='add', version=version)\n    def register_instance(self, job_data: dict):\n        \"\"\"\n        example dicts :\n            -----------\n            {\n                \"job_id\": \"job0\",\n                \"second\": 0,\n                \"func\": None,\n                \"job\": None,\n                \"time_passer\": None,\n                \"object_name\": \"tb_job_fuction\",\n                \"receive_job\": False,\n                \"save\": False,\n                \"max_live\": True,\n                # just lev it out \"serializer\": serializer_default,\n                \"args\": [],\n                \"kwargs\": {},\n            }\n\n            job_id : str\n                id for the job for management\n            second (optional): int\n                The time interval in seconds between each call of the job.\n            func (optional): Callable or str\n                The function to be executed as the job.\n            job (optional):  schedule.Job\n                An existing job object from the schedule library.\n            time_passer (optional):  schedule.Job\n                A job without a function, used to specify the time interval.\n            object_name (optional): str\n                The name of the object containing in the 'func' var to be executed.\n            receive_job (optional): bool\n                A flag indicating whether the job should be received from an object from 'func' var.\n            save (optional): bool\n                A flag indicating whether the job should be saved.\n            max_live (optional): bool\n                A flag indicating whether the job should have a maximum live time.\n            serializer (optional): bool\n                json pickel or dill must have a dumps fuction\n            *args, **kwargs (optional):\n                Additional arguments to be passed to the job function.\n\n\n        Parameters\n            ----------\n           job_data : dict\n\n        example usage\n            ----------\n            `python\n\n            `\n\n    \"\"\"\n        if job_data is None:\n            self.app.logger.error(\"No job data provided\")\n            return None\n        job_id = job_data[\"job_id\"]\n        second = job_data.get(\"second\", 0)\n        func = job_data.get(\"func\")\n        job = job_data.get(\"job\")\n        time_passer = job_data.get(\"time_passer\")\n        object_name = job_data.get(\"object_name\", \"tb_job_fuction\")\n        receive_job = job_data.get(\"receive_job\", False)\n        save = job_data.get(\"save\", False)\n        max_live = job_data.get(\"max_live\", True)\n        serializer = job_data.get(\"serializer\", serializer_default)\n        args = job_data.get(\"args\", ())\n        kwargs = job_data.get(\"kwargs\", {})\n\n        return self.register_job(\n            job_id=job_id,\n            second=second,\n            func=func,\n            job=job,\n            time_passer=time_passer,\n            object_name=object_name,\n            receive_job=receive_job,\n            save=save,\n            max_live=max_live,\n            serializer=serializer,\n            args=args,\n            kwargs=kwargs\n        )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.Tools.register_instance","title":"<code>register_instance(job_data)</code>","text":"example dicts <p>{     \"job_id\": \"job0\",     \"second\": 0,     \"func\": None,     \"job\": None,     \"time_passer\": None,     \"object_name\": \"tb_job_fuction\",     \"receive_job\": False,     \"save\": False,     \"max_live\": True,     # just lev it out \"serializer\": serializer_default,     \"args\": [],     \"kwargs\": {}, }</p> <p>job_id : str     id for the job for management second (optional): int     The time interval in seconds between each call of the job. func (optional): Callable or str     The function to be executed as the job. job (optional):  schedule.Job     An existing job object from the schedule library. time_passer (optional):  schedule.Job     A job without a function, used to specify the time interval. object_name (optional): str     The name of the object containing in the 'func' var to be executed. receive_job (optional): bool     A flag indicating whether the job should be received from an object from 'func' var. save (optional): bool     A flag indicating whether the job should be saved. max_live (optional): bool     A flag indicating whether the job should have a maximum live time. serializer (optional): bool     json pickel or dill must have a dumps fuction args, *kwargs (optional):     Additional arguments to be passed to the job function.</p> <p>Parameters     ----------    job_data : dict</p> <p>example usage     ----------     `python</p> <pre><code>`\n</code></pre> Source code in <code>toolboxv2/mods/SchedulerManager.py</code> <pre><code>@export(mod_name=Name, name='add', version=version)\ndef register_instance(self, job_data: dict):\n    \"\"\"\n    example dicts :\n        -----------\n        {\n            \"job_id\": \"job0\",\n            \"second\": 0,\n            \"func\": None,\n            \"job\": None,\n            \"time_passer\": None,\n            \"object_name\": \"tb_job_fuction\",\n            \"receive_job\": False,\n            \"save\": False,\n            \"max_live\": True,\n            # just lev it out \"serializer\": serializer_default,\n            \"args\": [],\n            \"kwargs\": {},\n        }\n\n        job_id : str\n            id for the job for management\n        second (optional): int\n            The time interval in seconds between each call of the job.\n        func (optional): Callable or str\n            The function to be executed as the job.\n        job (optional):  schedule.Job\n            An existing job object from the schedule library.\n        time_passer (optional):  schedule.Job\n            A job without a function, used to specify the time interval.\n        object_name (optional): str\n            The name of the object containing in the 'func' var to be executed.\n        receive_job (optional): bool\n            A flag indicating whether the job should be received from an object from 'func' var.\n        save (optional): bool\n            A flag indicating whether the job should be saved.\n        max_live (optional): bool\n            A flag indicating whether the job should have a maximum live time.\n        serializer (optional): bool\n            json pickel or dill must have a dumps fuction\n        *args, **kwargs (optional):\n            Additional arguments to be passed to the job function.\n\n\n    Parameters\n        ----------\n       job_data : dict\n\n    example usage\n        ----------\n        `python\n\n        `\n\n\"\"\"\n    if job_data is None:\n        self.app.logger.error(\"No job data provided\")\n        return None\n    job_id = job_data[\"job_id\"]\n    second = job_data.get(\"second\", 0)\n    func = job_data.get(\"func\")\n    job = job_data.get(\"job\")\n    time_passer = job_data.get(\"time_passer\")\n    object_name = job_data.get(\"object_name\", \"tb_job_fuction\")\n    receive_job = job_data.get(\"receive_job\", False)\n    save = job_data.get(\"save\", False)\n    max_live = job_data.get(\"max_live\", True)\n    serializer = job_data.get(\"serializer\", serializer_default)\n    args = job_data.get(\"args\", ())\n    kwargs = job_data.get(\"kwargs\", {})\n\n    return self.register_job(\n        job_id=job_id,\n        second=second,\n        func=func,\n        job=job,\n        time_passer=time_passer,\n        object_name=object_name,\n        receive_job=receive_job,\n        save=save,\n        max_live=max_live,\n        serializer=serializer,\n        args=args,\n        kwargs=kwargs\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SocketManager","title":"<code>SocketManager</code>","text":"<p>The SocketManager Supports 2 types of connections 1. Client Server 2. Peer to Peer</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker","title":"<code>TruthSeeker</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler","title":"<code>arXivCrawler</code>","text":"<p>ArXiv Crawler for TruthSeeker. Main module for processing research queries.</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler.ArXivPDFProcessor","title":"<code>ArXivPDFProcessor</code>","text":"<p>Main processor for research queries. This is a wrapper around the new ResearchProcessor for backward compatibility.</p> Source code in <code>toolboxv2/mods/TruthSeeker/arXivCrawler.py</code> <pre><code>class ArXivPDFProcessor:\n    \"\"\"\n    Main processor for research queries.\n    This is a wrapper around the new ResearchProcessor for backward compatibility.\n    \"\"\"\n    def __init__(self,\n                 query: str,\n                 tools,\n                 chunk_size: int = 1_000_000,\n                 overlap: int = 2_000,\n                 max_workers=None,\n                 num_search_result_per_query=6,\n                 max_search=6,\n                 download_dir=\"pdfs\",\n                 callback=None,\n                 num_workers=None):\n        \"\"\"Initialize the ArXiv PDF processor.\n\n        Args:\n            query: Research query\n            tools: Tools module\n            chunk_size: Size of text chunks for processing\n            overlap: Overlap between chunks\n            max_workers: Maximum number of worker threads\n            num_search_result_per_query: Number of search results per query\n            max_search: Maximum number of search queries\n            download_dir: Directory to save downloaded files\n            callback: Callback function for status updates\n            num_workers: Number of worker threads\n        \"\"\"\n        # Create the new research processor\n        self.processor = ResearchProcessor(\n            query=query,\n            tools=tools,\n            chunk_size=chunk_size,\n            overlap=overlap,\n            max_workers=max_workers,\n            num_search_result_per_query=num_search_result_per_query,\n            max_search=max_search,\n            download_dir=download_dir,\n            callback=callback,\n            num_workers=num_workers\n        )\n\n        # Copy attributes for backward compatibility\n        self.insights_generated = False\n        self.queries_generated = False\n        self.query = query\n        self.tools = tools\n        self.mem = tools.get_memory()\n        self.chunk_size = chunk_size\n        self.overlap = overlap\n        self.max_workers = max_workers\n        self.nsrpq = num_search_result_per_query\n        self.max_search = max_search\n        self.download_dir = download_dir\n        self.parser = RobustPDFDownloader(download_dir=download_dir)\n        self.callback = callback if callback is not None else lambda status: None\n        self.mem_name = None\n        self.current_session = None\n        self.all_ref_papers = 0\n        self.last_insights_list = None\n        self.all_texts_len = 0\n        self.f_texts_len = 0\n        self.s_id = str(uuid.uuid4())\n        self.semantic_model = self.processor.semantic_model\n        self._query_progress = {}\n        self._progress_lock = threading.Lock()\n        self.num_workers = self.processor.num_workers\n\n    def _update_global_progress(self) -&gt; float:\n        \"\"\"Calculate overall progress considering all processing phases.\"\"\"\n        return self.processor._update_global_progress()\n\n    async def search_and_process_papers(self, queries: list[str]) -&gt; list[Paper]:\n        \"\"\"Search for and process papers based on queries.\n\n        Args:\n            queries: List of search queries\n\n        Returns:\n            List of processed papers\n        \"\"\"\n        # Use the new processor to search and process papers\n        unified_papers = await self.processor.search_and_process_papers(queries)\n\n        # Convert UnifiedPaper objects to Paper objects for backward compatibility\n        papers = []\n        for paper in unified_papers:\n            if paper.source == \"arxiv\":\n                # Convert to the old Paper format\n                arxiv_paper = Paper(\n                    title=paper.title,\n                    authors=paper.authors,\n                    summary=paper.summary,\n                    url=paper.url,\n                    pdf_url=paper.pdf_url,\n                    published=paper.published,\n                    updated=paper.source_specific_data.get(\"updated\", \"\"),\n                    categories=paper.source_specific_data.get(\"categories\", []),\n                    paper_id=paper.paper_id\n                )\n                papers.append(arxiv_paper)\n\n        # Update attributes for backward compatibility\n        self.all_ref_papers = self.processor.all_ref_papers\n        self.all_texts_len = self.processor.all_texts_len\n        self.f_texts_len = self.processor.f_texts_len\n\n        return papers\n\n    def send_status(self, step: str, progress: float = None, additional_info: str = \"\"):\n        \"\"\"Send status update via callback.\"\"\"\n        if progress is None:\n            progress = self._update_global_progress()\n        self.callback({\n            \"step\": step,\n            \"progress\": progress,\n            \"info\": additional_info\n        })\n\n    def generate_queries(self) -&gt; list[str]:\n        self.send_status(\"Generating search queries\")\n        self.queries_generated = False\n\n        class ArXivQueries(BaseModel):\n            queries: list[str] = Field(..., description=\"List of ArXiv search queries (en)\")\n\n        try:\n            query_generator: ArXivQueries = self.tools.format_class(\n                ArXivQueries,\n                f\"Generate a list of precise ArXiv search queries to comprehensively address: {self.query}\"\n            )\n            queries = [self.query] + query_generator[\"queries\"]\n        except Exception:\n            self.send_status(\"Error generating queries\", additional_info=\"Using default query.\")\n            queries = [self.query]\n\n        if len(queries[:self.max_search]) &gt; 0:\n            self.queries_generated = True\n        return queries[:self.max_search]\n\n    def init_process_papers(self):\n        self.mem.create_memory(self.mem_name, model_config={\"model_name\": \"anthropic/claude-3-5-haiku-20241022\"})\n        self.send_status(\"Memory initialized\")\n\n\n    async def generate_insights(self, queries) -&gt; dict:\n        self.send_status(\"Generating insights\")\n        query = self.query\n        # max_it = 0\n        results = await self.mem.query(query=query, memory_names=self.mem_name, unified_retrieve=True, query_params={\n            \"max_sentences\": 25})\n        #query = queries[min(len(queries)-1, max_it)]\n\n        self.insights_generated = True\n        self.send_status(\"Insights generated\", progress=1.0)\n        return results\n\n    async def extra_query(self, query, query_params=None, unified_retrieve=True):\n        self.send_status(\"Processing follow-up query\", progress=0.5)\n        results = await self.mem.query(query=query, memory_names=self.mem_name,\n                                                      query_params=query_params, unified_retrieve=unified_retrieve)\n        self.send_status(\"Processing follow-up query Done\", progress=1)\n        return results\n\n    def generate_mem_name(self):\n        class UniqueMemoryName(BaseModel):\n            \"\"\"unique memory name based on the user query\"\"\"\n            name: str\n        return self.tools.get_agent(\"thinkm\").format_class(UniqueMemoryName, self.query).get('name', '_'.join(self.query.split(\" \")[:3]))\n\n    def initialize(self, session_id, second=False):\n        self.current_session = session_id\n        self.insights_generated = False\n        self.queries_generated = False\n        if second:\n            return\n        self.mem_name = self.generate_mem_name().strip().replace(\"\\n\", '') + '_' + session_id\n        self.init_process_papers()\n\n    async def process(self, query=None) -&gt; tuple[list[Paper], dict]:\n        if query is not None:\n            self.query = query\n        self.send_status(\"Starting research process\")\n        t0 = time.perf_counter()\n        self.initialize(self.s_id, query is not None)\n\n        queries = self.generate_queries()\n\n        papers = await self.search_and_process_papers(queries)\n\n        if len(papers) == 0:\n            class UserQuery(BaseModel):\n                \"\"\"Fix all typos and clear the original user query\"\"\"\n                new_query: str\n            self.query= self.tools.format_class(\n                UserQuery,\n                self.query\n            )[\"new_query\"]\n            queries = self.generate_queries()\n            papers = await self.search_and_process_papers(queries)\n\n        insights = await self.generate_insights(queries)\n\n        elapsed_time = time.perf_counter() - t0\n        self.send_status(\"Process complete\", progress=1.0,\n                         additional_info=f\"Total time: {elapsed_time:.2f}s, Papers analyzed: {len(papers)}/{self.all_ref_papers}\")\n\n        return papers, insights\n\n    @staticmethod\n    def estimate_processing_metrics(query_length: int, **config) -&gt; (float, float):\n        \"\"\"Return estimated time (seconds) and price for processing.\"\"\"\n        total_papers = config['max_search'] * config['num_search_result_per_query']\n        median_text_length = 100000  # 10 pages * 10000 characters\n\n        # Estimated chunks to process\n        total_chunks = total_papers * (median_text_length / config['chunk_size']) + 1 / config['overlap']\n        processed_chunks = total_chunks * 0.45\n        total_chars = TextSplitter(config['chunk_size'],\n                     config['overlap']\n                     ).approximate(config['chunk_size'] * processed_chunks)\n        # Time estimation (seconds)\n        .75 / config['chunk_size']  # Hypothetical time per chunk in seconds\n        w = (config.get('num_workers', 16) if config.get('num_workers', 16) is not None else 16 / 10)\n        # Processing_ time - Insights Genration - Insights Query   -   Indexing Time     -    Download Time     -       workers   -   Query Genration time - Ui - Init Db\n        estimated_time = ((8+total_papers*0.012)+(total_chunks/20000) * .005 + (total_chunks/2) * .0003 + total_papers * 2.8 ) / w + (0.25 * config['max_search']) + 6 + 4\n\n        price_per_char = 0.0000012525\n        price_per_t_chunk =  total_chars * price_per_char\n        estimated_price = price_per_t_chunk ** 1.7\n\n        # estimated_price = 0 if query_length &lt; 420 and estimated_price &lt; 5 else estimated_price\n        if estimated_time &lt; 10:\n            estimated_time = 10\n        if estimated_price &lt; .04:\n            estimated_price = .04\n        return round(estimated_time, 2), round(estimated_price, 4)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler.ArXivPDFProcessor.__init__","title":"<code>__init__(query, tools, chunk_size=1000000, overlap=2000, max_workers=None, num_search_result_per_query=6, max_search=6, download_dir='pdfs', callback=None, num_workers=None)</code>","text":"<p>Initialize the ArXiv PDF processor.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Research query</p> required <code>tools</code> <p>Tools module</p> required <code>chunk_size</code> <code>int</code> <p>Size of text chunks for processing</p> <code>1000000</code> <code>overlap</code> <code>int</code> <p>Overlap between chunks</p> <code>2000</code> <code>max_workers</code> <p>Maximum number of worker threads</p> <code>None</code> <code>num_search_result_per_query</code> <p>Number of search results per query</p> <code>6</code> <code>max_search</code> <p>Maximum number of search queries</p> <code>6</code> <code>download_dir</code> <p>Directory to save downloaded files</p> <code>'pdfs'</code> <code>callback</code> <p>Callback function for status updates</p> <code>None</code> <code>num_workers</code> <p>Number of worker threads</p> <code>None</code> Source code in <code>toolboxv2/mods/TruthSeeker/arXivCrawler.py</code> <pre><code>def __init__(self,\n             query: str,\n             tools,\n             chunk_size: int = 1_000_000,\n             overlap: int = 2_000,\n             max_workers=None,\n             num_search_result_per_query=6,\n             max_search=6,\n             download_dir=\"pdfs\",\n             callback=None,\n             num_workers=None):\n    \"\"\"Initialize the ArXiv PDF processor.\n\n    Args:\n        query: Research query\n        tools: Tools module\n        chunk_size: Size of text chunks for processing\n        overlap: Overlap between chunks\n        max_workers: Maximum number of worker threads\n        num_search_result_per_query: Number of search results per query\n        max_search: Maximum number of search queries\n        download_dir: Directory to save downloaded files\n        callback: Callback function for status updates\n        num_workers: Number of worker threads\n    \"\"\"\n    # Create the new research processor\n    self.processor = ResearchProcessor(\n        query=query,\n        tools=tools,\n        chunk_size=chunk_size,\n        overlap=overlap,\n        max_workers=max_workers,\n        num_search_result_per_query=num_search_result_per_query,\n        max_search=max_search,\n        download_dir=download_dir,\n        callback=callback,\n        num_workers=num_workers\n    )\n\n    # Copy attributes for backward compatibility\n    self.insights_generated = False\n    self.queries_generated = False\n    self.query = query\n    self.tools = tools\n    self.mem = tools.get_memory()\n    self.chunk_size = chunk_size\n    self.overlap = overlap\n    self.max_workers = max_workers\n    self.nsrpq = num_search_result_per_query\n    self.max_search = max_search\n    self.download_dir = download_dir\n    self.parser = RobustPDFDownloader(download_dir=download_dir)\n    self.callback = callback if callback is not None else lambda status: None\n    self.mem_name = None\n    self.current_session = None\n    self.all_ref_papers = 0\n    self.last_insights_list = None\n    self.all_texts_len = 0\n    self.f_texts_len = 0\n    self.s_id = str(uuid.uuid4())\n    self.semantic_model = self.processor.semantic_model\n    self._query_progress = {}\n    self._progress_lock = threading.Lock()\n    self.num_workers = self.processor.num_workers\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler.ArXivPDFProcessor.estimate_processing_metrics","title":"<code>estimate_processing_metrics(query_length, **config)</code>  <code>staticmethod</code>","text":"<p>Return estimated time (seconds) and price for processing.</p> Source code in <code>toolboxv2/mods/TruthSeeker/arXivCrawler.py</code> <pre><code>@staticmethod\ndef estimate_processing_metrics(query_length: int, **config) -&gt; (float, float):\n    \"\"\"Return estimated time (seconds) and price for processing.\"\"\"\n    total_papers = config['max_search'] * config['num_search_result_per_query']\n    median_text_length = 100000  # 10 pages * 10000 characters\n\n    # Estimated chunks to process\n    total_chunks = total_papers * (median_text_length / config['chunk_size']) + 1 / config['overlap']\n    processed_chunks = total_chunks * 0.45\n    total_chars = TextSplitter(config['chunk_size'],\n                 config['overlap']\n                 ).approximate(config['chunk_size'] * processed_chunks)\n    # Time estimation (seconds)\n    .75 / config['chunk_size']  # Hypothetical time per chunk in seconds\n    w = (config.get('num_workers', 16) if config.get('num_workers', 16) is not None else 16 / 10)\n    # Processing_ time - Insights Genration - Insights Query   -   Indexing Time     -    Download Time     -       workers   -   Query Genration time - Ui - Init Db\n    estimated_time = ((8+total_papers*0.012)+(total_chunks/20000) * .005 + (total_chunks/2) * .0003 + total_papers * 2.8 ) / w + (0.25 * config['max_search']) + 6 + 4\n\n    price_per_char = 0.0000012525\n    price_per_t_chunk =  total_chars * price_per_char\n    estimated_price = price_per_t_chunk ** 1.7\n\n    # estimated_price = 0 if query_length &lt; 420 and estimated_price &lt; 5 else estimated_price\n    if estimated_time &lt; 10:\n        estimated_time = 10\n    if estimated_price &lt; .04:\n        estimated_price = .04\n    return round(estimated_time, 2), round(estimated_price, 4)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler.ArXivPDFProcessor.search_and_process_papers","title":"<code>search_and_process_papers(queries)</code>  <code>async</code>","text":"<p>Search for and process papers based on queries.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>list[str]</code> <p>List of search queries</p> required <p>Returns:</p> Type Description <code>list[Paper]</code> <p>List of processed papers</p> Source code in <code>toolboxv2/mods/TruthSeeker/arXivCrawler.py</code> <pre><code>async def search_and_process_papers(self, queries: list[str]) -&gt; list[Paper]:\n    \"\"\"Search for and process papers based on queries.\n\n    Args:\n        queries: List of search queries\n\n    Returns:\n        List of processed papers\n    \"\"\"\n    # Use the new processor to search and process papers\n    unified_papers = await self.processor.search_and_process_papers(queries)\n\n    # Convert UnifiedPaper objects to Paper objects for backward compatibility\n    papers = []\n    for paper in unified_papers:\n        if paper.source == \"arxiv\":\n            # Convert to the old Paper format\n            arxiv_paper = Paper(\n                title=paper.title,\n                authors=paper.authors,\n                summary=paper.summary,\n                url=paper.url,\n                pdf_url=paper.pdf_url,\n                published=paper.published,\n                updated=paper.source_specific_data.get(\"updated\", \"\"),\n                categories=paper.source_specific_data.get(\"categories\", []),\n                paper_id=paper.paper_id\n            )\n            papers.append(arxiv_paper)\n\n    # Update attributes for backward compatibility\n    self.all_ref_papers = self.processor.all_ref_papers\n    self.all_texts_len = self.processor.all_texts_len\n    self.f_texts_len = self.processor.f_texts_len\n\n    return papers\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler.ArXivPDFProcessor.send_status","title":"<code>send_status(step, progress=None, additional_info='')</code>","text":"<p>Send status update via callback.</p> Source code in <code>toolboxv2/mods/TruthSeeker/arXivCrawler.py</code> <pre><code>def send_status(self, step: str, progress: float = None, additional_info: str = \"\"):\n    \"\"\"Send status update via callback.\"\"\"\n    if progress is None:\n        progress = self._update_global_progress()\n    self.callback({\n        \"step\": step,\n        \"progress\": progress,\n        \"info\": additional_info\n    })\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler.main","title":"<code>main(query='Beste strategien in bretspielen sitler von katar')</code>  <code>async</code>","text":"<p>Main execution function</p> Source code in <code>toolboxv2/mods/TruthSeeker/arXivCrawler.py</code> <pre><code>async def main(query: str = \"Beste strategien in bretspielen sitler von katar\"):\n    \"\"\"Main execution function\"\"\"\n    with Spinner(\"Init Isaa\"):\n        tools = get_app(\"ArXivPDFProcessor\", name=None).get_mod(\"isaa\")\n        tools.init_isaa(build=True)\n    processor = ArXivPDFProcessor(query, tools=tools)\n    papers, insights = await processor.process()\n\n    print(\"Generated Insights:\", insights)\n    print(\"Generated Insights_list:\", processor.last_insights_list)\n    kb = tools.get_memory(processor.mem_name)\n    print(await kb.query_concepts(\"AI\"))\n    print(await kb.retrieve(\"Evaluation metrics for assessing AI Agent performance\"))\n    print(kb.concept_extractor.concept_graph.concepts.keys())\n    kb.vis(output_file=\"insights_graph.html\")\n    kb.save(\"mem.plk\")\n    # await get_app(\"ArXivPDFProcessor\", name=None).a_idle()\n    return insights\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui","title":"<code>nGui</code>","text":"<p>import colorsys import json import time from datetime import datetime, timedelta from queue import Queue from typing import Dict, Union, List, Any</p> <p>from fastapi import Request import os import random from threading import Thread, Event</p> <p>import networkx as nx from dataclasses import asdict</p> <p>from toolboxv2 import get_app from toolboxv2.mods.FastApi.fast_nice import register_nicegui</p> <p>import asyncio</p> <p>from nicegui import ui</p> <p>from pathlib import Path import stripe</p> <p>from toolboxv2.mods.TruthSeeker.arXivCrawler import Paper from toolboxv2.mods.isaa.base.AgentUtils import anything_from_str_to_dict</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--set-your-secret-key-use-environment-variables-in-production","title":"Set your secret key (use environment variables in production!)","text":"<p>stripe.api_key = os.getenv('STRIPE_SECRET_KEY', 'sk_test_YourSecretKey')</p> <p>def create_landing_page():     # Set up dynamic background     ui.query(\"body\").style(\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%)\")</p> <pre><code># Main container with enhanced responsive design\nwith ui.column().classes(\n\"w-full max-w-md p-8 rounded-3xl shadow-2xl \"\n\"items-center self-center mx-auto my-8\"\n):\n    # Advanced styling for glass-morphism effect\n    ui.query(\".nicegui-column\").style(\"\"\"\n    background: rgba(255, 255, 255, 0.05);\n    backdrop-filter: blur(12px);\n    border: 1px solid rgba(255, 255, 255, 0.1);\n    transition: all 0.3s ease-in-out;\n    \"\"\")\n\n    # Animated logo/brand icon\n    with ui.element(\"div\").classes(\"animate-fadeIn\"):\n        ui.icon(\"science\").classes(\n        \"text-7xl mb-6 text-primary \"\n        \"transform hover:scale-110 transition-transform\"\n        )\n\n    # Enhanced typography for title\n    ui.label(\"TruthSeeker\").classes(\n    \"text-5xl font-black text-center \"\n    \"text-primary mb-2 animate-slideDown\"\n    )\n\n    # Stylized subtitle with brand message\n    ui.label(\"Precision. Discovery. Insights.\").classes(\n    \"text-xl font-medium text-center \"\n    \"mb-10 animate-fadeIn\"\n    )\n\n    # Button container for consistent spacing\n    ui.button(\n    \"Start Research\",\n    on_click=lambda: ui.navigate.to(\"/open-Seeker.seek\")\n    ).classes(\n    \"w-full px-6 py-4 text-lg font-bold \"\n    \"bg-primary hover:bg-primary-dark \"\n    \"transform hover:-translate-y-0.5 \"\n    \"transition-all duration-300 ease-in-out \"\n    \"rounded-xl shadow-lg animate-slideUp\"\n    )\n\n    # Navigation links container\n    with ui.element(\"div\").classes(\"mt-8 space-y-3 text-center\"):\n        ui.link(\n        \"Demo video\",\n        ).classes(\n        \"block text-lg text-gray-200 hover:text-primary \"\n        \"transition-colors duration-300 animate-fadeIn\"\n        ).on(\"click\", lambda: ui.navigate.to(\"/open-Seeker.demo\"))\n\n        ui.link(\n        \"About Us\",\n        ).classes(\n        \"block text-lg text-gray-400 hover:text-primary \"\n        \"transition-colors duration-300 animate-fadeIn\"\n        ).on(\"click\", lambda: ui.navigate.to(\"/open-Seeker.about\"))\n</code></pre> <p>def create_video_demo():     with ui.card().classes('w-full max-w-3xl mx-auto').style(         'background: var(--background-color); color: var(--text-color)'):         # Video container with responsive aspect ratio         with ui.element('div').classes('relative w-full aspect-video'):             video = ui.video('../api/TruthSeeker/video').classes('w-full h-full object-cover')</p> <pre><code>        # Custom controls overlay\n        with ui.element('div').classes('absolute bottom-0 left-0 right-0 bg-black/50 p-2'):\n            with ui.row().classes('items-center gap-2'):\n                #play_btn = ui.button(icon='play_arrow', on_click=lambda: video.props('playing=true'))\n                #pause_btn = ui.button(icon='pause', on_click=lambda: video.props('playing=false'))\n                ui.slider(min=0, max=100, value=0).classes('w-full').bind_value(video, 'time')\n                #mute_btn = ui.button(icon='volume_up', on_click=lambda: video.props('muted=!muted'))\n                #fullscreen_btn = ui.button(icon='fullscreen', on_click=lambda: video.props('fullscreen=true'))\n\n\n    # Video description\n    ui.markdown('Walkthrough of TruthSeeker features and capabilities.')\n    # Back to Home Button\n    ui.button('Back to Home', on_click=lambda: ui.navigate.to('/open-Seeker')).classes(\n        'mt-6 w-full bg-primary text-white hover:opacity-90'\n    )\n\nreturn video\n</code></pre> <p>def create_about_page():     \"\"\"Create a comprehensive About page for TruthSeeker\"\"\"     with ui.column().classes('w-full max-w-4xl mx-auto p-6'):         # Page Header         ui.label('About TruthSeeker').classes('text-4xl font-bold text-primary mb-6')</p> <pre><code>    # Mission Statement\n    with ui.card().classes('w-full mb-6').style(\n        'background: var(--background-color); color: var(--text-color); padding: 20px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);'\n    ):\n        ui.label('Our Mission').classes('text-2xl font-semibold text-primary mb-4')\n        ui.markdown(\"\"\"\n            TruthSeeker aims to democratize access to scientific knowledge,\n            transforming complex academic research into comprehensible insights.\n            We bridge the gap between raw data and meaningful understanding.\n        \"\"\").classes('text-lg').style('color: var(--text-color);')\n\n    # Core Technologies\n    with ui.card().classes('w-full mb-6').style(\n        'background: var(--background-color); color: var(--text-color); padding: 20px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);'\n    ):\n        ui.label('Core Technologies').classes('text-2xl font-semibold text-primary mb-4')\n        with ui.row().classes('gap-4 w-full'):\n            with ui.column().classes('flex-1 text-center'):\n                ui.icon('search').classes('text-4xl text-primary mb-2')\n                ui.label('Advanced Query Processing').classes('font-bold')\n                ui.markdown('Intelligent algorithms that extract nuanced research insights.').style(\n                    'color: var(--text-color);')\n            with ui.column().classes('flex-1 text-center'):\n                ui.icon('analytics').classes('text-4xl text-primary mb-2')\n                ui.label('Semantic Analysis').classes('font-bold')\n                ui.markdown('Deep learning models for comprehensive research verification.').style(\n                    'color: var(--text-color);')\n            with ui.column().classes('flex-1 text-center'):\n                ui.icon('verified').classes('text-4xl text-primary mb-2')\n                ui.label('Research Validation').classes('font-bold')\n                ui.markdown('Multi-layered verification of academic sources.').style('color: var(--text-color);')\n    # Research Process\n    with ui.card().classes('w-full').style('background: var(--background-color);color: var(--text-color);'):\n        ui.label('Research Discovery Process').classes('text-2xl font-semibold text-primary mb-4')\n        with ui.card().classes('q-pa-md q-mx-auto').style(\n            'max-width: 800px; background: var(--background-color); border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);'\n        ) as card:\n            ui.markdown(\"# Research Workflow\").style(\n                \"color: var(--primary-color); text-align: center; margin-bottom: 20px;\")\n            ui.markdown(\n                \"\"\"\n                Welcome to TruthSeeker\u2019s interactive research assistant. Follow the steps below to transform your initial inquiry into a refined, actionable insight.\n                \"\"\"\n            ).style(\"color: var(--text-color); text-align: center; margin-bottom: 30px;\")\n\n            # The stepper component\n            with ui.stepper().style('background: var(--background-color); color: var(--text-color);') as stepper:\n                # Step 1: Query Initialization\n                with ui.step('Query Initialization'):\n                    ui.markdown(\"### Step 1: Query Initialization\").style(\"color: var(--primary-color);\")\n                    ui.markdown(\n                        \"\"\"\n                        Begin by entering your research question or selecting from popular academic domains.\n                        This sets the direction for our semantic analysis engine.\n                        \"\"\"\n                    ).style(\"color: var(--text-color); margin-bottom: 20px;\")\n                    with ui.stepper_navigation():\n                        ui.button('Next', on_click=stepper.next).props('rounded color=primary')\n\n                # Step 2: Semantic Search\n                with ui.step('Semantic Search'):\n                    ui.markdown(\"### Step 2: Semantic Search\").style(\"color: var(--primary-color);\")\n                    ui.markdown(\n                        \"\"\"\n                        Our advanced algorithms now process your input to generate context-rich queries.\n                        This stage refines the search context by understanding the deeper intent behind your question.\n                        \"\"\"\n                    ).style(\"color: var(--text-color); margin-bottom: 20px;\")\n                    with ui.stepper_navigation():\n                        ui.button('Back', on_click=stepper.previous).props('flat')\n                        ui.button('Next', on_click=stepper.next).props('rounded color=primary')\n\n                # Step 3: Document Analysis\n                with ui.step('Document Analysis'):\n                    ui.markdown(\"### Step 3: Document Analysis\").style(\"color: var(--primary-color);\")\n                    ui.markdown(\n                        \"\"\"\n                        The system then dives into a detailed analysis of academic papers, parsing content to extract key insights and connections.\n                        This ensures that even subtle but crucial information is captured.\n                        \"\"\"\n                    ).style(\"color: var(--text-color); margin-bottom: 20px;\")\n                    with ui.stepper_navigation():\n                        ui.button('Back', on_click=stepper.previous).props('flat')\n                        ui.button('Next', on_click=stepper.next).props('rounded color=primary')\n\n                # Step 4: Insight Generation\n                with ui.step('Insight Generation'):\n                    ui.markdown(\"### Step 4: Insight Generation\").style(\"color: var(--primary-color);\")\n                    ui.markdown(\n                        \"\"\"\n                        Finally, we synthesize the analyzed data into clear, actionable research summaries.\n                        These insights empower you with concise guidance to drive further inquiry or practical application.\n                        \"\"\"\n                    ).style(\"color: var(--text-color); margin-bottom: 20px;\")\n                    with ui.stepper_navigation():\n                        ui.button('Back', on_click=stepper.previous).props('flat')\n\n    # Back to Home Button\n    ui.button('Back to Home', on_click=lambda: ui.navigate.to('/open-Seeker')).classes(\n        'mt-6 w-full bg-primary text-white hover:opacity-90'\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--dummy-implementierung-fur-get_tools","title":"Dummy-Implementierung f\u00fcr get_tools()","text":"<p>def get_tools():     \"\"\"     Hier solltest du dein richtiges Werkzeug-Objekt zur\u00fcckliefern.     In diesem Beispiel gehen wir davon aus, dass du \u00fcber eine Funktion wie get_app verf\u00fcgst.     \"\"\"     return get_app(\"ArXivPDFProcessor\", name=None).get_mod(\"isaa\")</p> <p>def create_graph_tab(processor_instance: Dict, graph_ui: ui.element, main_ui: ui.element):     \"\"\"Create and update the graph visualization\"\"\"</p> <pre><code># Get HTML graph from processor\n_html_content = processor_instance[\"instance\"].tools.get_memory(processor_instance[\"instance\"].mem_name)\nhtml_content = \"\" if isinstance(_html_content, list) else _html_content.vis(get_output_html=True)\n\n# Ensure static directory exists\nstatic_dir = Path('dist/static')\nstatic_dir.mkdir(exist_ok=True)\n\n# Save HTML to static file\ngraph_file = static_dir / f'graph{processor_instance[\"instance\"].mem_name}.html'\n# Save HTML to static file with added fullscreen functionality\n\n# Add fullscreen JavaScript\ngraph_file.write_text(html_content, encoding='utf-8')\n\nwith main_ui:\n    # Clear existing content except fullscreen button\n    graph_ui.clear()\n\n    with graph_ui:\n        ui.html(f\"\"\"\n\n            &lt;iframe\n                 src=\"/static/graph{processor_instance[\"instance\"].mem_name}.html\"\n                style=\"width: 100%; height: 800px; border: none; background: #1a1a1a;\"\n                &gt;\n            &lt;/iframe&gt;\n        \"\"\").classes('w-full h-full')\n</code></pre> <p>is_init = [False]</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui---database-setup-","title":"--- Database Setup ---","text":"<p>def get_db():     db = get_app().get_mod(\"DB\")     if not is_init[0]:         is_init[0] = True         db.edit_cli(\"LD\")         db.initialize_database()     return db</p> <p>import pickle</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui---session-state-management-","title":"--- Session State Management ---","text":"<p>def get_user_state(session_id: str, is_new=False) -&gt; dict:     db = get_db()     state_ = {         'balance': .5,         'last_reset': datetime.utcnow().isoformat(),         'research_history': [],         'payment_id': '',     }     if session_id is None:         state_['balance'] *= -1         if is_new:             return state_, True         return state_     state = db.get(f\"TruthSeeker::session:{session_id}\")     if state.get() is None:         state = state_         if is_new:             return state_, True     else:         try:             state = pickle.loads(state.get())         except Exception as e:             print(e)             state = {         'balance': 0.04,         'last_reset': datetime.utcnow().isoformat(),         'research_history': [\"Sorry we had an error recreating your state\"],         'payment_id': '',             }             if is_new:                 return state, True     if is_new:         return state, False     return state</p> <p>def save_user_state(session_id: str, state: dict):     db = get_db()     print(\"Saving state\")     db.set(f\"TruthSeeker::session:{session_id}\", pickle.dumps(state)).print()</p> <p>def delete_user_state(session_id: str):     db = get_db()     print(\"Saving state\")     db.delete(f\"TruthSeeker::session:{session_id}\").print()</p> <p>def reset_daily_balance(state: dict, valid=False) -&gt; dict:     now = datetime.utcnow()     last_reset = datetime.fromisoformat(state.get('last_reset', now.isoformat()))     if now - last_reset &gt; timedelta(hours=24):         state['balance'] = max(state.get('balance', 1.6 if valid else 0.5), 1.6 if valid else 0.5)         state['last_reset'] = now.isoformat()     return state</p> class MemoryResultsDisplay <p>def init(self, results: List[Dict[str, Any]], main_ui: ui.element):     self.results = results     self.main_ui = main_ui     self.setup_ui()</p> <p>def setup_ui(self):     \"\"\"Set up the main UI for displaying memory results\"\"\"     with self.main_ui:         self.main_ui.clear()         with ui.column().classes('w-full'):             for mem_result in self.results:                 self.create_memory_card(mem_result)</p> <p>def create_memory_card(self, mem_result: Dict[str, Any]):     \"\"\"Create a card for each memory result\"\"\"     result = mem_result.get(\"result\", {})     with self.main_ui:         if isinstance(result, dict):             self.display_dict_result(result)         elif hasattr(result, 'overview'):  # Assuming RetrievalResult type             self.display_retrieval_result(result)         else:             ui.label(\"Unsupported result type\").classes('--text-color:error')</p> <p>def display_dict_result(self, result: Dict[str, Any]):     \"\"\"Display dictionary-based result with collapsible sections\"\"\"     # Summary Section     summary = result.get(\"summary\", {})     if isinstance(summary, str):         try:             summary = json.loads(summary[:-1])         except json.JSONDecodeError:             summary = {\"error\": \"Could not parse summary\"}</p> <pre><code># Raw Results Section\nraw_results = result.get(\"raw_results\", {})\nif isinstance(raw_results, str):\n    try:\n        raw_results = json.loads(raw_results[:-1])\n    except json.JSONDecodeError:\n        raw_results = {\"error\": \"Could not parse raw results\"}\n\n# Metadata Section\nmetadata = result.get(\"metadata\", {})\nwith self.main_ui:\n    # Collapsible Sections\n    with ui.column().classes('w-full space-y-2').style(\"max-width: 100%;\"):\n        # Summary Section\n        with ui.expansion('Summary', icon='description').classes('w-full') as se:\n            self.display_nested_data(summary, main_ui=se)\n\n        # Raw Results Section\n        with ui.expansion('Raw Results', icon='work').classes('w-full') as re:\n            self.display_nested_data(raw_results, main_ui=re)\n\n        # Metadata Section\n        if metadata:\n            with ui.expansion('Metadata', icon='info').classes('w-full'):\n                ui.markdown(f\"```json\n</code></pre> <p>{json.dumps(metadata, indent=2)} ```\").style(\"max-width: 100%;\")</p> <pre><code>def display_retrieval_result(self, result):\n    \"\"\"Display retrieval result with detailed sections\"\"\"\n    with self.main_ui:\n        with ui.column().classes('w-full space-y-4').style(\"max-width: 100%;\"):\n            # Overview Section\n            with ui.expansion('Overview', icon='visibility').classes('w-full') as ov:\n                for overview_item in result.overview:\n                    if isinstance(overview_item, str):\n                        overview_item = json.loads(overview_item)\n                    self.display_nested_data(overview_item, main_ui=ov)\n\n            # Details Section\n            with ui.expansion('Details', icon='article').classes('w-full'):\n                for chunk in result.details:\n                    with ui.card().classes('w-full p-3 mb-2').style(\"background: var(--background-color)\"):\n                        ui.label(chunk.text).classes('font-medium mb-2 --text-color:secondary')\n\n                        with ui.row().classes('w-full justify-between').style(\"background: var(--background-color)\"):\n                            ui.label(f\"Embedding Shape: {chunk.embedding.shape}\").classes('text-sm')\n                            ui.label(f\"Content Hash: {chunk.content_hash}\").classes('text-sm')\n\n                        if chunk.cluster_id is not None:\n                            ui.label(f\"Cluster ID: {chunk.cluster_id}\").classes('text-sm')\n\n            # Cross References Section\n            with ui.expansion('Cross References', icon='link').classes('w-full'):\n                for topic, chunks in result.cross_references.items():\n                    with ui.card().classes('w-full p-3 mb-2').style(\"background: var(--background-color)\"):\n                        ui.label(topic).classes('font-semibold mb-2 --text-color:secondary')\n                        for chunk in chunks:\n                            ui.label(chunk.text).classes('text-sm mb-1')\n\ndef display_nested_data(self, data: Union[Dict, List], indent: int = 0, main_ui=None):\n    \"\"\"Recursively display nested dictionary or list data\"\"\"\n    with (self.main_ui if main_ui is None else main_ui):\n        if isinstance(data, dict):\n            with ui.column().classes(f'ml-{indent * 2}').style(\"max-width: 100%;\"):\n                for key, value in data.items():\n                    with ui.row().classes('items-center'):\n                        ui.label(f\"{key}:\").classes('font-bold mr-2 --text-color:primary')\n                        if isinstance(value, list):\n                            if key == \"main_chunks\":\n                                continue\n                            self.display_nested_data(value, indent + 1, main_ui=main_ui)\n                        if isinstance(value, dict):\n                            ui.markdown(f\"```json\n</code></pre> <p>{json.dumps(value, indent=2)} <code>\").classes(\"break-words w-full\").style(\"max-width: 100%;\")                             else:                                 ui.label(str(value)).classes('--text-color:secondary')             elif isinstance(data, list):                 with ui.column().classes(f'ml-{indent * 2}').style(\"max-width: 100%;\"):                     for item in data:                         if isinstance(item, str):                             item = json.loads(item)                         if isinstance(item, list):                             self.display_nested_data(item, indent + 1, main_ui=main_ui)                         if isinstance(item, dict):                             ui.markdown(f\"</code>json {json.dumps(item, indent=2)} ```\").classes(\"break-words w-full\").style(\"max-width: 100%;\")                         else:                             ui.label(str(item)).classes('--text-color:secondary')</p> <p>def create_followup_section(processor_instance: Dict, main_ui: ui.element, session_id, balance):     main_ui.clear()     with main_ui:         ui.label(\"Query Interface  (1ct)\").classes(\"text-xl font-semibold mb-4\")</p> <pre><code>    # Container for query inputs\n    query_container = ui.column().classes(\"w-full gap-4\")\n    query = \"\"  # Store references to query inputs\n    # Query parameters section\n    with ui.expansion(\"Query Parameters\", icon=\"settings\").classes(\"w-full\") as query_e:\n        with ui.grid(columns=2).classes(\"w-full gap-4\"):\n            k_input = ui.number(\"Results Count (k)\", value=2, min=1, max=20)\n            min_sim = ui.number(\"Min Similarity\", value=.3, min=0, max=1, step=0.1)\n            cross_depth = ui.number(\"Cross Reference Depth\", value=2, min=1, max=5)\n            max_cross = ui.number(\"Max Cross References\", value=10, min=1, max=20)\n            max_sent = ui.number(\"Max Sentences\", value=10, min=1, max=50)\n            unified = ui.switch(\"Unified Retrieve (+3ct)\", value=True)\n\n    # Results display\n    with ui.element(\"div\").classes(\"w-full mt-4\") as results_display:\n        pass\n    results_display = results_display\n    with query_container:\n        query_input = ui.input(\"Query\", placeholder=\"Enter your query...\")                 .classes(\"w-full\")\n    # Control buttons\n    with ui.row().classes(\"w-full gap-4 mt-4\"):\n        ui.button(\"Execute Query\", on_click=lambda: asyncio.create_task(execute_query()))                 .classes(\"bg-green-600 hover:bg-green-700\")\n        ui.button(\"Clear Results\", on_click=lambda: results_display.clear())                 .classes(\"bg-red-600 hover:bg-red-700\")\nquery_input = query_input\n\nasync def execute_query():\n    \"\"\"Execute a single query with parameters\"\"\"\n    nonlocal query_input, results_display, main_ui\n    try:\n        query_text = query_input.value\n        if not query_text.strip():\n            with main_ui:\n                ui.notify(\"No Input\", type=\"warning\")\n            return \"\"\n\n        if not processor_instance.get(\"instance\"):\n            with main_ui:\n                ui.notify(\"No active processor instance\", type=\"warning\")\n            return\n        # Collect parameters\n        params = {\n            \"k\": int(k_input.value),\n            \"min_similarity\": min_sim.value,\n            \"cross_ref_depth\": int(cross_depth.value),\n            \"max_cross_refs\": int(max_cross.value),\n            \"max_sentences\": int(max_sent.value),\n            \"unified\": unified.value\n        }\n        # Construct query parameters\n        query_params = {\n            \"k\": params[\"k\"],\n            \"min_similarity\": params[\"min_similarity\"],\n            \"cross_ref_depth\": params[\"cross_ref_depth\"],\n            \"max_cross_refs\": params[\"max_cross_refs\"],\n            \"max_sentences\": params[\"max_sentences\"]\n        }\n\n        # Execute query\n        results = await processor_instance[\"instance\"].extra_query(\n            query=query_text,\n            query_params=query_params,\n            unified_retrieve=params[\"unified\"]\n        )\n        print(\"results\",results)\n        s = get_user_state(session_id)\n        s['balance'] -= .04 if unified.value else .01\n        save_user_state(session_id, s)\n        with main_ui:\n            balance.set_text(f\"Balance: {s['balance']:.2f}\u20ac\")\n        # Format results\n        with main_ui:\n            with results_display:\n                MemoryResultsDisplay(results, results_display)\n\n    except Exception as e:\n        return f\"Error executing query: {str(e)}\n</code></pre> <p>\"</p> <pre><code># Add initial query input\n</code></pre> <p>online_states = [0] def create_research_interface(Processor):</p> <pre><code>def helpr(request, session: dict):\n\n    state = {'balance':0, 'research_history': []}\n    main_ui = None\n    with ui.column().classes(\"w-full max-w-6xl mx-auto p-6 space-y-6\") as loading:\n        ui.spinner(size='lg')\n        ui.label('Initializing...').classes('ml-2')\n\n    # Container for main content (initially hidden)\n    content = ui.column().classes('hidden')\n\n    # Extract session data before spawning thread\n    session_id = session.get('ID')\n    session_id_h = session.get('IDh')\n    session_rid = request.row.query_params.get('session_id') if hasattr(request, 'row') else request.query_params.get('session_id')\n    session_valid = session.get('valid')\n\n    # Thread communication\n    result_queue = Queue()\n    ready_event = Event()\n\n    def init_background():\n        nonlocal session_id, session_id_h, session_rid, session_valid\n        try:\n            # Original initialization logic\n            _state, is_new = get_user_state(session_id, is_new=True)\n\n            if is_new and session_id_h != \"#0\":\n                _state = get_user_state(session_id_h)\n                save_user_state(session_id, _state)\n                delete_user_state(session_id_h)\n            if session_rid:\n                state_: dict\n                state_, is_new_ = get_user_state(session_rid, is_new=True)\n                if not is_new_:\n                    _state = state_.copy()\n                    state_['payment_id'] = ''\n                    state_['last_reset'] = datetime.utcnow().isoformat()\n                    state_['research_history'] = state_['research_history'][:3]\n                    state_['balance'] = 0\n                    save_user_state(session_id, _state)\n            _state = reset_daily_balance(_state, session_valid)\n            save_user_state(session_id, _state)\n\n            # Send result back to main thread\n            result_queue.put(_state)\n            ready_event.set()\n        except Exception as e:\n            result_queue.put(e)\n            ready_event.set()\n\n        # Start background initialization\n\n    Thread(target=init_background).start()\n\n    def check_ready():\n        nonlocal state\n        if ready_event.is_set():\n            result = result_queue.get()\n\n            # Check if initialization failed\n            if isinstance(result, Exception):\n                loading.clear()\n                with loading:\n                    ui.label(f\"Error during initialization: {str(result)}\").classes('text-red-500')\n                return\n\n            # Get state and build main UI\n            state = result\n            loading.classes('hidden')\n            content.classes(remove='hidden')\n            main_ui.visible = True\n            with main_ui:\n                balance.set_text(f\"Balance: {state['balance']:.2f}\u20ac\")\n                show_history()\n            return  # Stop the timer\n\n        # Check again in 100ms\n        ui.timer(0.1, check_ready, once=True)\n\n    # Start checking for completion\n    check_ready()\n\n    # Wir speichern die aktive Instanz, damit Follow-Up Fragen gestellt werden k\u00f6nnen\n    processor_instance = {\"instance\": None}\n\n    # UI-Elemente als Platzhalter; wir definieren sie sp\u00e4ter in der UI und machen sie so\n    # in den Callback-Funktionen \u00fcber \"nonlocal\" verf\u00fcgbar.\n    overall_progress = None\n    status_label = None\n    results_card = None\n    summary_content = None\n    analysis_content = None\n    references_content = None\n    followup_card = None\n    research_card = None\n    config_cart = None\n    progress_card = None\n    balance = None\n    graph_ui = None\n\n    sr_button = None\n    r_button = None\n    r_text = None\n\n\n    # Global config storage with default values\n    config = {\n        'chunk_size': 21000,\n        'overlap': 600,\n        'num_search_result_per_query': 3,\n        'max_search': 3,\n        'num_workers': None\n    }\n\n    def update_estimates():\n        \"\"\"\n        Dummy estimation based on query length and configuration.\n        (Replace with your own non-linear formula if needed.)\n        \"\"\"\n        query_text = query.value or \"\"\n        query_length = len(query_text)\n        # For example: estimated time scales with chunk size and query length.\n        estimated_time ,estimated_price = Processor.estimate_processing_metrics(query_length, **config)\n        estimated_time *= max(1, online_states[0] * 6)\n        if processor_instance[\"instance\"] is not None:\n            estimated_price += .25\n        if estimated_time &lt; 60:\n            time_str = f\"~{int(estimated_time)}s\"\n        elif estimated_time &lt; 3600:\n            minutes = estimated_time // 60\n            seconds = estimated_time % 60\n            time_str = f\"~{int(minutes)}m {int(seconds)}s\"\n        else:\n            hours = estimated_time // 3600\n            minutes = (estimated_time % 3600) // 60\n            time_str = f\"~{int(hours)}h {int(minutes)}m\"\n        with main_ui:\n            query_length_label.set_text(f\"Total Papers: {config['max_search']*config['num_search_result_per_query']}\")\n            time_label.set_text(f\"Processing Time: {time_str}\")\n            price_label.set_text(f\"Price: {estimated_price:.2f}\u20ac\")\n\n        return estimated_price\n\n    def on_config_change(event):\n        \"\"\"\n        Update the global config based on input changes and recalc estimates.\n        \"\"\"\n        try:\n            config['chunk_size'] = int(chunk_size_input.value)\n        except ValueError:\n            pass\n        try:\n            config['overlap'] = int(overlap_input.value)\n            if config['overlap'] &gt; config['chunk_size'] / 4:\n                config['overlap'] = int(config['chunk_size'] / 4)\n                with main_ui:\n                    overlap_input.value = config['overlap']\n        except ValueError:\n            pass\n        try:\n            config['num_search_result_per_query'] = int(num_search_result_input.value)\n        except ValueError:\n            pass\n        try:\n            config['max_search'] = int(max_search_input.value)\n        except ValueError:\n            pass\n        try:\n            config['num_workers'] = int(num_workers_input.value) if num_workers_input.value != 0 else None\n        except ValueError:\n            config['num_workers'] = None\n\n        update_estimates()\n\n    def on_query_change():\n        update_estimates()\n\n    # Callback, der vom Processor (\u00fcber processor_instance.callback) aufgerufen wird.\n    def update_status(data: dict):\n        nonlocal overall_progress, status_label\n        if not data:\n            return\n        # Aktualisiere den Fortschrittsbalken und den aktuellen Schritt (wenn vorhanden)\n        with main_ui:\n            if isinstance(data, dict):\n                progress = data.get(\"progress\", 0)\n                step = data.get(\"step\", \"Processing...\")\n                overall_progress.value =round( progress ,2) # nicegui.linear_progress erwartet einen Wert zwischen 0 und 1\n                status_label.set_text(f\"{step} {data.get('info','')}\")\n            else:\n                status_label.set_text(f\"{data}\")\n\n    def start_search():\n        nonlocal balance\n\n        async def helper():\n            nonlocal processor_instance, overall_progress, status_label, results_card,                     summary_content, analysis_content,config, references_content, followup_card,sr_button,r_button,r_text\n\n            try:\n                if not validate_inputs():\n                    with main_ui:\n                        state['balance'] += est_price\n                        save_user_state(session_id, state)\n                        balance.set_text(f\"Balance: {state['balance']:.2f}\u20ac\")\n                    return\n                reset_interface()\n                show_progress_indicators()\n\n                query_text = query.value.strip()\n                # Erzeuge das \"tools\"-Objekt (abh\u00e4ngig von deiner konkreten Implementation)\n                tools = get_tools()\n                with main_ui:\n                    research_card.visible = False\n                    config_cart.visible = False\n                    config_section.visible = False\n                    query.set_value(\"\")\n                # Direkt instanziieren: Eine neue ArXivPDFProcessor-Instanz\n                if processor_instance[\"instance\"] is not None:\n                    processor = processor_instance[\"instance\"]\n                    processor.chunk_size = config['chunk_size']\n                    processor.overlap = config['overlap']\n                    processor.num_search_result_per_query = config['num_search_result_per_query']\n                    processor.max_search = config['max_search']\n                    processor.num_workers = config['num_workers']\n                    papers, insights = await processor.process(query_text)\n                else:\n                    processor = Processor(query_text, tools=tools, **config)\n                # Setze den Callback so, dass Updates in der GUI angezeigt werden\n                    processor.callback = update_status\n                    processor_instance[\"instance\"] = processor\n                    papers, insights = await processor.process()\n\n                update_results({\n                    \"papers\": papers,\n                    \"insights\": insights\n                })\n                with main_ui:\n                    research_card.visible = True\n                    config_cart.visible = True\n                    show_history()\n\n            except Exception as e:\n                import traceback\n\n                with main_ui:\n                    update_status({\"progress\": 0, \"step\": \"Error\", \"info\": str(e)})\n                    state['balance'] += est_price\n                    save_user_state(session_id, state)\n                    balance.set_text(f\"Balance: {state['balance']:.2f}\u20ac\")\n                    ui.notify(f\"Error {str(e)})\", type=\"negative\")\n                    research_card.visible = True\n                    config_cart.visible = True\n                    config_section.visible = True\n                print(traceback.format_exc())\n\n        def target():\n            get_app().run_a_from_sync(helper, )\n\n        est_price = update_estimates()\n        if est_price &gt; state['balance']:\n            with main_ui:\n                ui.notify(f\"Insufficient balance. Need \u20ac{est_price:.2f}\", type='negative')\n        else:\n            state['balance'] -= est_price\n            save_user_state(session_id, state)\n            with main_ui:\n                online_states[0] += 1\n                balance.set_text(f\"Balance: {state['balance']:.2f}\u20ac Running Queries: {online_states[0]}\")\n\n            Thread(target=target, daemon=True).start()\n            with main_ui:\n                online_states[0] -= 1\n                balance.set_text(f\"Balance: {get_user_state(session_id)['balance']:.2f}\u20ac\")\n\n\n    def show_history():\n        with config_cart:\n            for idx, entry in enumerate(state['research_history']):\n                with ui.card().classes(\"w-full backdrop-blur-lg bg-white/10 p-4\"):\n                    ui.label(entry['query']).classes('text-sm')\n                    ui.button(\"Open\").on_click(lambda _, i=idx: load_history(i))\n\n    def reset():\n        nonlocal processor_instance, results_card, followup_card, sr_button, r_button, r_text\n        processor_instance[\"instance\"] = None\n        show_progress_indicators()\n        with main_ui:\n            config_cart.visible = False\n            config_section.visible = False\n            followup_card.visible = False\n            results_card.visible = False\n            r_button.visible = False\n            r_text.set_text(\"Research Interface\")\n            sr_button.set_text(\"Start Research\")\n        start_search()\n    # UI-Aufbau\n\n    with ui.column().classes(\"w-full max-w-6xl mx-auto p-6 space-y-6\") as main_ui:\n        balance = ui.label(f\"Balance: {state['balance']:.2f}\u20ac\").classes(\"text-s font-semibold\")\n\n        config_cart = config_cart\n\n        # --- Research Input UI Card ---\n        with ui.card().classes(\"w-full backdrop-blur-lg bg-white/10 p-4\") as research_card:\n            r_text = ui.label(\"Research Interface\").classes(\"text-3xl font-bold mb-4\")\n\n            # Query input section with auto-updating estimates\n            query = ui.input(\"Research Query\",\n                                placeholder=\"Gib hier deine Forschungsfrage ein...\",\n                                value=\"\")                     .classes(\"w-full min-h-[100px]\")                     .on('change', lambda e: on_query_change()).style(\"color: var(--text-color)\")\n\n            # --- Action Buttons ---\n            with ui.row().classes(\"mt-4\"):\n                sr_button =ui.button(\"Start Research\", on_click=start_search)                         .classes(\"bg-blue-600 hover:bg-blue-700 py-3 rounded-lg\")\n                ui.button(\"toggle config\",\n                          on_click=lambda: setattr(config_section, 'visible', not config_section.visible) or show_progress_indicators()).style(\n                    \"color: var(--text-color)\")\n                r_button = ui.button(\"Start new Research\",\n                          on_click=reset).style(\n                    \"color: var(--text-color)\")\n        sr_button = sr_button\n        r_button = r_button\n        r_button.visible = False\n        research_card = research_card\n\n        # --- Options Cart / Configurations ---\n        with ui.card_section().classes(\"w-full backdrop-blur-lg bg-white/10 hidden\") as config_section:\n            ui.separator()\n            ui.label(\"Configuration Options\").classes(\"text-xl font-semibold mt-4 mb-2\")\n            with ui.row():\n                chunk_size_input = ui.number(label=\"Chunk Size\",\n                                             value=config['chunk_size'], format='%.0f', max=64_000, min=1000,\n                                             step=100)                         .on('change', on_config_change).style(\"color: var(--text-color)\")\n                overlap_input = ui.number(label=\"Overlap\",\n                                          value=config['overlap'], format='%.0f', max=6400, min=100, step=50)                         .on('change', on_config_change).style(\"color: var(--text-color)\")\n\n            with ui.row():\n                num_search_result_input = ui.number(label=\"Results per Query\",\n                                                    value=config['num_search_result_per_query'], format='%.0f',\n                                                    min=1, max=100, step=1)                         .on('change', on_config_change).style(\"color: var(--text-color)\")\n                max_search_input = ui.number(label=\"Max Search Queries\",\n                                             value=config['max_search'], format='%.0f', min=1, max=100, step=1)                         .on('change', on_config_change).style(\"color: var(--text-color)\")\n                num_workers_input = ui.number(label=\"Number of Workers (leave empty for default)\",\n                                              value=0, format='%.0f', min=0, max=32, step=1)                         .on('change', on_config_change).style(\"color: var(--text-color)\")\n        config_section = config_section\n        config_section.visible = False\n        # --- Ergebnisse anzeigen ---\n        with ui.card().classes(\"w-full backdrop-blur-lg p-4 bg-white/10\") as results_card:\n            ui.label(\"Research Results\").classes(\"text-xl font-semibold mb-4\")\n            with ui.tabs() as tabs:\n                ui.tab(\"Summary\")\n                ui.tab(\"References\")\n                ui.tab(\"SystemStates\")\n            with ui.tab_panels(tabs, value=\"Summary\").classes(\"w-full\").style(\"background-color: var(--background-color)\"):\n                with ui.tab_panel(\"Summary\"):\n                    summary_content = ui.markdown(\"\").style(\"color : var(--text-color)\")\n                with ui.tab_panel(\"References\"):\n                    references_content = ui.markdown(\"\").style(\"color : var(--text-color)\")\n                with ui.tab_panel(\"SystemStates\"):\n                    analysis_content = ui.markdown(\"\").style(\"color : var(--text-color)\")\n\n\n        # Ergebnisse sichtbar machen, sobald sie vorliegen.\n        results_card = results_card\n        results_card.visible = False\n\n        # --- Follow-Up Bereich mit mehrfachen Folgefragen und Suchparametern ---\n        with ui.card().classes(\"w-full backdrop-blur-lg bg-white/10 p-4 hidden\") as followup_card:\n            pass\n\n        # Zugriff auf followup_card (falls sp\u00e4ter ben\u00f6tigt)\n        followup_card = followup_card\n        followup_card.visible = False\n\n        # --- Fortschrittsanzeige ---\n        with ui.card().classes(\"w-full backdrop-blur-lg bg-white/10 p-4\") as progress_card:\n            with ui.row():\n                ui.label(\"Research Progress\").classes(\"text-xl font-semibold mb-4\")\n                query_length_label = ui.label(\"\").classes(\"mt-6 hover:text-primary transition-colors duration-300\")\n                time_label = ui.label(\"Time: ...\").classes(\"mt-6 hover:text-primary transition-colors duration-300\")\n                price_label = ui.label(\"Price: ...\").classes(\n                    \"mt-6 hover:text-primary transition-colors duration-300\")\n\n            overall_progress = ui.linear_progress(0).classes(\"w-full mb-4\")\n            status_label = ui.label(\"Warte auf Start...\").classes(\"text-base\")\n        # Wir merken uns progress_card, falls wir ihn zur\u00fccksetzen wollen.\n        progress_card = progress_card\n\n        query_length_label = query_length_label\n        time_label = time_label\n        price_label = price_label\n\n        with ui.card().classes(\"w-full backdrop-blur-lg bg-white/10 p-4\") as config_cart:\n            # --- Process Code Section ---\n            # --- Estimated Time and Price ---\n            # ui.label(\"History\").classes(\"text-xl font-semibold mt-4 mb-2\")\n            ui.label('Research History').classes('text-xl p-4')\n            show_history()\n\n        ui.button('Add Credits', on_click=lambda: balance_overlay(session_id)).props('icon=paid')\n        ui.label('About TruthSeeker').classes(\n            'mt-6 text-gray-500 hover:text-primary '\n            'transition-colors duration-300'\n        ).on('click', lambda: ui.navigate.to('/open-Seeker.about', new_tab=True))\n\n        with ui.element('div').classes(\"w-full\").style(\"white:100%; height:100%\") as graph_ui:\n            pass\n\n        with ui.card().classes(\"w-full p-4\").style(\"background-color: var(--background-color)\"):\n            ui.label(\"Private Session link (restore the session on a different device)\")\n            base_url = f'https://{os.getenv(\"HOSTNAME\")}/gui/open-Seeker.seek' if not 'localhost' in os.getenv(\"HOSTNAME\") else 'http://localhost:5000/gui/open-Seeker.seek'\n            ui.label(f\"{base_url}?session_id={session_id}\").style(\"white:100%\")\n            ui.label(\"Changes each time!\")\n\n        graph_ui = graph_ui\n        graph_ui.visible = False\n    main_ui = main_ui\n    main_ui.visible = False\n\n    # --- Hilfsfunktionen ---\n    def validate_inputs() -&gt; bool:\n        if not query.value.strip():\n            with main_ui:\n                ui.notify(\"Bitte gib eine Forschungsfrage ein.\", type=\"warning\")\n            return False\n        return True\n\n    def reset_interface():\n        nonlocal overall_progress, status_label, results_card, followup_card\n        overall_progress.value = 0\n        with main_ui:\n            status_label.set_text(\"Research startet...\")\n        # Ergebnisse und Follow-Up Bereich verstecken\n        results_card.visible = False\n        followup_card.visible = False\n        graph_ui.visible = False\n\n    def show_progress_indicators():\n        nonlocal progress_card\n        progress_card.visible = True\n\n    def update_results(data: dict, save=True):\n        nonlocal summary_content, analysis_content, references_content, results_card,                followup_card,graph_ui, r_button, r_text, sr_button\n        with main_ui:\n            r_button.visible = True\n            r_text.set_text(\"Add to current Results or press 'Start new Research'\")\n            sr_button.set_text(\"Add to current Results\")\n        # Handle papers (1-to-1 case)\n        papers = data.get(\"papers\", [])\n        if not isinstance(papers, list):\n            papers = [papers]\n\n        # Get insights\n        insights = data.get(\"insights\", [])\n\n        if save:\n            history_entry = data.copy()\n            history_entry['papers'] = [paper.model_dump_json() for paper in papers]\n            if processor_instance is not None and processor_instance['instance'] is not None:\n                history_entry[\"mam_name\"] = processor_instance['instance'].mem_name\n                history_entry[\"query\"] = processor_instance['instance'].query\n\n                history_entry[\"processor_memory\"] = processor_instance['instance'].tools.get_memory(\n\n                ).save_memory(history_entry[\"mam_name\"], None)\n            state['research_history'].append(history_entry)\n            save_user_state(session_id, state)\n        else:\n            papers = [Paper(**json.loads(paper)) for paper in papers]\n        create_followup_section(processor_instance, followup_card, session_id, balance)\n        with main_ui:\n            progress_card.visible = False\n            # Build summary from insights\n            summaries = []\n            for insight in insights:\n                if 'result' in insight and 'summary' in insight['result']:\n                    if isinstance(insight['result']['summary'], str):\n                        # print(insight['result']['summary'], \"NEXT\", json.loads(insight['result']['summary'][:-1]),\"NEXT22\",  type(json.loads(insight['result']['summary'][:-1])))\n                        insight['result']['summary'] = json.loads(insight['result']['summary'][:-1])\n                    main_summary = insight['result']['summary'].get('main_summary', '')\n                    if main_summary:\n                        summaries.append(main_summary)\n            summary_text = \"\n</code></pre> <p>\".join(summaries) if summaries else \"No summary available.\"                 summary_content.set_content(f\"# Research Summary</p> <p>{summary_text}\")</p> <pre><code>            # Analysis section (unchanged if processor details haven't changed)\n            if processor_instance[\"instance\"] is not None:\n                inst = processor_instance[\"instance\"]\n                analysis_md = (\n                    f\"# Analysis\n</code></pre> <p>\"                         f\"- query: {inst.query} \"                         f\"- chunk_size: {inst.chunk_size} \"                         f\"- overlap: {inst.overlap} \"                         f\"- max_workers: {inst.max_workers} \"                         f\"- num_search_result_per_query: {inst.nsrpq} \"                         f\"- max_search: {inst.max_search} \"                         f\"- download_dir: {inst.download_dir} \"                         f\"- mem_name: {inst.mem_name} \"                         f\"- current_session: {inst.current_session} \"                         f\"- all_ref_papers: {inst.all_ref_papers} \"                         f\"- all_texts_len: {inst.all_texts_len} \"                         f\"- final_texts_len: {inst.f_texts_len} \"                         f\"- num_workers: {inst.num_workers}\"                     )                     analysis_content.set_content(analysis_md)</p> <pre><code>            # References and Insights section\n            references_md = \"# References\n</code></pre> <p>\"                 # Add papers                 references_md += \" \".join(                     f\"- ({i}) {getattr(paper, 'title', 'Unknown Title')}})\"                     for i, paper in enumerate(papers)                 )</p> <pre><code>            # Add detailed insights\n            references_md += \"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--insights","title":"Insights","text":"<p>\"                 for i, insight in enumerate(insights):                     print(insight)                     result = insight.get('result', {})                     summary = result.get('summary', {})</p> <pre><code>                if isinstance(summary, str):\n                    summary = json.loads(summary)\n\n                # Main summary\n                references_md += f\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--insight","title":"Insight","text":"<p>\"                     references_md += f\"### Main Summary {summary.get('main_summary', 'No summary available.')} \"</p> <pre><code>                # Concept Analysis\n                concept_analysis = summary.get('concept_analysis', {})\n                if concept_analysis:\n                    references_md += \"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--concept-analysis","title":"Concept Analysis","text":"<p>\"                         references_md += \"#### Key Concepts - \" + \" - \".join(                             concept_analysis.get('key_concepts', [])) + \" \"                         references_md += \"</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--relationships","title":"Relationships","text":"<ul> <li>\" + \"</li> <li>\".join(                             concept_analysis.get('relationships', [])) + \" \"                         references_md += \"</li> </ul>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--importance-hierarchy","title":"Importance Hierarchy","text":"<ul> <li>\" + \"</li> <li> <p>\".join(                             concept_analysis.get('importance_hierarchy', [])) + \" \"</p> <pre><code>            # Topic Insights\n            topic_insights = summary.get('topic_insights', {})\n            if topic_insights:\n                references_md += \"\n</code></pre> </li> </ul>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--topic-insights","title":"Topic Insights","text":"<p>\"                     references_md += \"#### Primary Topics - \" + \" - \".join(                         topic_insights.get('primary_topics', [])) + \" \"                     references_md += \"</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--cross-references","title":"Cross References","text":"<ul> <li>\" + \"</li> <li>\".join(                         topic_insights.get('cross_references', [])) + \" \"                     references_md += \"</li> </ul>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--knowledge-gaps","title":"Knowledge Gaps","text":"<ul> <li>\" + \"</li> <li> <p>\".join(                         topic_insights.get('knowledge_gaps', [])) + \" \"</p> <pre><code>        # Relevance Assessment\n        relevance = summary.get('relevance_assessment', {})\n        if relevance:\n            references_md += \"\n</code></pre> </li> </ul> <p>return helpr</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--relevance-assessment","title":"Relevance Assessment","text":"<p>\"                 references_md += f\"- Query Alignment: {relevance.get('query_alignment', 'N/A')} \"                 references_md += f\"- Confidence Score: {relevance.get('confidence_score', 'N/A')} \"                 references_md += f\"- Coverage Analysis: {relevance.get('coverage_analysis', 'N/A')} \"</p> <pre><code>    references_content.set_content(references_md)\n\n    # nx concpts graph\n    if processor_instance[\"instance\"] is not None:\n        create_graph_tab(\n            processor_instance,\n            graph_ui,main_ui\n        )\n\n    # Show results and followup cards\n    results_card.visible = True\n    followup_card.visible = True\n    graph_ui.visible = True\n</code></pre> <p>def load_history(index: int):     entry = state['research_history'][index]     if processor_instance is not None and processor_instance['instance'] is not None:</p> <pre><code>    processor_instance[\"instance\"].mem_name = entry[\"mam_name\"]\n    processor_instance['instance'].query = entry[\"query\"]\n\n    pass\nelse:\n    processor = Processor(entry[\"query\"], tools=get_tools(), **config)\n    # Setze den Callback so, dass Updates in der GUI angezeigt werden\n    processor.callback = update_status\n    processor.mem_name = entry[\"mam_name\"]\n    processor_instance[\"instance\"] = processor\n\nprocessor_instance[\"instance\"].tools.get_memory().load_memory(entry[\"mam_name\"], entry[\"processor_memory\"])\nprocessor_instance[\"instance\"].mem_name = entry[\"mam_name\"]\nupdate_results(entry, save=False)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui---stripe-integration-","title":"--- Stripe Integration ---","text":"<p>def regiser_stripe_integration(is_scc=True):     def stripe_callback(request: Request):</p> <pre><code>    sid = request.row.query_params.get('session_id') if hasattr(request, 'row') else request.query_params.get('session_id')\n    state = get_user_state(sid)\n\n    if state['payment_id'] == '':\n        with ui.card().classes(\"w-full items-center\").style(\"background-color: var(--background-color)\"):\n            ui.label(f\"No payment id!\").classes(\"text-lg font-bold\")\n            ui.button(\n                \"Start Research\",\n                on_click=lambda: ui.navigate.to(\"/open-Seeker.seek?session_id=\"+sid)\n            ).classes(\n                \"w-full px-6 py-4 text-lg font-bold \"\n                \"bg-primary hover:bg-primary-dark \"\n                \"transform hover:-translate-y-0.5 \"\n                \"transition-all duration-300 ease-in-out \"\n                \"rounded-xl shadow-lg animate-slideUp\"\n            )\n        return\n\n    try:\n        session_data = stripe.checkout.Session.retrieve(state['payment_id'])\n    except Exception as e:\n        with ui.card().classes(\"w-full items-center\").style(\"background-color: var(--background-color)\"):\n            ui.label(f\"No Transactions Details !{e}\").classes(\"text-lg font-bold\")\n            ui.button(\n                \"Start Research\",\n                on_click=lambda: ui.navigate.to(\"/open-Seeker.seek\")\n            ).classes(\n                \"w-full px-6 py-4 text-lg font-bold \"\n                \"bg-primary hover:bg-primary-dark \"\n                \"transform hover:-translate-y-0.5 \"\n                \"transition-all duration-300 ease-in-out \"\n                \"rounded-xl shadow-lg animate-slideUp\"\n            )\n            return\n    with ui.card().classes(\"w-full items-center\").style(\"background-color: var(--background-color)\"):\n        if is_scc and state['payment_id'] != '' and session_data.payment_status == 'paid':\n            state = get_user_state(sid)\n            amount = session_data.amount_total / 100  # Convert cents to euros\n            state['balance'] += amount\n            state['payment_id'] = ''\n            save_user_state(sid, state)\n\n        # ui.navigate.to(f'/session?session={session}')\n            ui.label(f\"Transaction Complete - New balance :{state['balance']}\").classes(\"text-lg font-bold\")\n            with ui.card().classes(\"w-full p-4\").style(\"background-color: var(--background-color)\"):\n                ui.label(\"Private Session link (restore the session on a different device)\")\n                base_url = f'https://{os.getenv(\"HOSTNAME\")}/gui/open-Seeker.seek' if not 'localhost' in os.getenv(\"HOSTNAME\")else 'http://localhost:5000/gui/open-Seeker.seek'\n                ui.label(f\"{base_url}?session_id={sid}\").style(\"white:100%\")\n                ui.label(\"Changes each time!\")\n        else:\n            ui.label(f\"Transaction Error! {session_data}, {dir(session_data)}\").classes(\"text-lg font-bold\")\n        ui.button(\n            \"Start Research\",\n            on_click=lambda: ui.navigate.to(\"/open-Seeker.seek\")\n        ).classes(\n            \"w-full px-6 py-4 text-lg font-bold \"\n            \"bg-primary hover:bg-primary-dark \"\n            \"transform hover:-translate-y-0.5 \"\n            \"transition-all duration-300 ease-in-out \"\n            \"rounded-xl shadow-lg animate-slideUp\"\n        )\n\n\nreturn stripe_callback\n</code></pre> <p>def handle_stripe_payment(amount: float, session_id):     base_url = f'https://{os.getenv(\"HOSTNAME\")}/gui/open-Seeker.stripe' if not 'localhost' in os.getenv(\"HOSTNAME\") else 'http://localhost:5000/gui/open-Seeker.stripe'     session = stripe.checkout.Session.create(         payment_method_types=['card',                               \"link\",                               ],         line_items=[{             'price_data': {                 'currency': 'eur',                 'product_data': {'name': 'Research Credits'},                 'unit_amount': int(amount * 100),             },             'quantity': 1,         }],         automatic_tax={\"enabled\": True},         mode='payment',         success_url=f'{base_url}?session_id={session_id}',         cancel_url=f'{base_url}.error'     )     state = get_user_state(session_id)     state['payment_id'] = session.id     save_user_state(session_id, state)     ui.navigate.to(session.url, new_tab=True)</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui---ui-components-","title":"--- UI Components ---","text":"<p>def balance_overlay(session_id):     with ui.dialog().classes('w-full max-w-md bg-white/20 backdrop-blur-lg rounded-xl') as dialog:         with ui.card().classes('w-full p-6 space-y-4').style(\"background-color: var(--background-color)\"):             ui.label('Add Research Credits').classes('text-2xl font-bold')             amount = ui.number('Amount (\u20ac) min 2', value=5, format='%.2f', min=2, max=9999, step=1).classes('w-full')             with ui.row().classes('w-full justify-between'):                 ui.button('Cancel', on_click=dialog.close).props('flat')                 ui.button('Purchase', on_click=lambda: handle_stripe_payment(amount.value, session_id))     return dialog</p> <p>def create_ui(processor):     # ui_instance =     register_nicegui(\"open-Seeker\", create_landing_page                      , additional=\"\"\"     \"\"\", show=False)     register_nicegui(\"open-Seeker.demo\", create_video_demo, additional=\"\"\"          \"\"\", show=False)</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui","title":"<code>newui</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.cleanup_module","title":"<code>cleanup_module(app)</code>","text":"<p>Cleanup resources when the module is unloaded</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, version=version, exit_f=True)\ndef cleanup_module(app: App):\n    \"\"\"Cleanup resources when the module is unloaded\"\"\"\n    # Clean up any temp files or resources\n    import glob\n    import shutil\n\n    # Remove temporary PDF directories\n    for pdf_dir in glob.glob(\"pdfs_*\"):\n        try:\n            shutil.rmtree(pdf_dir)\n        except Exception as e:\n            print(f\"Error removing directory {pdf_dir}: {str(e)}\")\n\n    # Clear any SSE queues\n    if hasattr(app, 'sse_queues'):\n        app.sse_queues = {}\n\n    if hasattr(app, 'payment_queues'):\n        app.payment_queues = {}\n\n    return Result.ok(info=\"ArXivPDFProcessor UI cleaned up\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.create_payment","title":"<code>create_payment(app, data)</code>  <code>async</code>","text":"<p>Create a Stripe payment session</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def create_payment(app: App, data):\n    \"\"\"Create a Stripe payment session\"\"\"\n    amount = data.get(\"amount\")\n    session_id = data.get(\"session_id\")\n\n    if amount &lt; 2:\n        return Result.default_user_error(info=\"Minimum donation amount is \u20ac2\")\n\n    try:\n        # Create a Stripe Checkout Session\n        base_url = f\"https://{os.getenv('HOSTNAME', 'localhost:5000')}\"\n        success_url = f\"{base_url}/api/{MOD_NAME}/payment_success?session_id={session_id}\"\n        cancel_url = f\"{base_url}/api/{MOD_NAME}/payment_cancel?session_id={session_id}\"\n\n        stripe_session = stripe.checkout.Session.create(\n            payment_method_types=['card', 'link'],\n            line_items=[{\n                'price_data': {\n                    'currency': 'eur',\n                    'product_data': {'name': 'Research Credits'},\n                    'unit_amount': int(amount * 100),\n                },\n                'quantity': 1,\n            }],\n            automatic_tax={\"enabled\": True},\n            mode='payment',\n            success_url=success_url,\n            cancel_url=cancel_url\n        )\n\n        # Store the payment info\n        if not hasattr(app, 'payment_info'):\n            app.payment_info = {}\n\n        # Initialize payment_queues if not already done\n        if not hasattr(app, 'payment_queues'):\n            app.payment_queues = {}\n\n        # Create a queue for this payment\n        app.payment_queues[session_id] = asyncio.Queue()\n\n        app.payment_info[session_id] = {\n            'payment_id': stripe_session.id,\n            'amount': amount,\n            'status': 'pending'\n        }\n\n        return Result.ok(data={\"url\": stripe_session.url})\n    except Exception as e:\n        return Result.default_sys_error(info=f\"Error creating payment: {str(e)}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.estimate_processing","title":"<code>estimate_processing(data)</code>  <code>async</code>","text":"<p>Estimate processing time and cost for a given query</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def estimate_processing(data):\n    \"\"\"Estimate processing time and cost for a given query\"\"\"\n    # Use the static method to estimate metrics\n    query, max_search, num_search_result_per_query= data.get(\"query\", \"\"), data.get(\"max_search\",4), data.get(\"num_search_result_per_query\",6)\n    estimated_time, estimated_price = ArXivPDFProcessor.estimate_processing_metrics(\n        query_length=len(query),\n        max_search=max_search,\n        num_search_result_per_query=num_search_result_per_query,\n        chunk_size=1_000_000,\n        overlap=2_000,\n        num_workers=None\n    )\n\n    return Result.ok(data={\n        \"time\": estimated_time,\n        \"price\": estimated_price\n    })\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.follow_up_query","title":"<code>follow_up_query(app, data)</code>  <code>async</code>","text":"<p>Ask a follow-up question about the research</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def follow_up_query(app: App, data):\n    \"\"\"Ask a follow-up question about the research\"\"\"\n    research_id = data.get(\"research_id\")\n    query = data.get(\"query\")\n\n    if not hasattr(app, 'research_processes') or research_id not in app.research_processes:\n        return Result.default_user_error(info=\"Research process not found\")\n\n    research_process = app.research_processes[research_id]\n\n    if research_process['status'] != 'complete':\n        return Result.default_user_error(info=\"Research is not complete\")\n\n    processor = research_process['processor']\n    if not processor:\n        return Result.default_user_error(info=\"Processor not available\")\n\n    try:\n        # Use the extra_query method to ask follow-up questions\n        result = await processor.extra_query(query)\n\n        return Result.ok(data={\"answer\": result['response'] if result and 'response' in result else \"No response\"})\n    except Exception as e:\n        return Result.default_sys_error(info=f\"Error processing follow-up query: {str(e)}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.initialize_module","title":"<code>initialize_module(app)</code>","text":"<p>Initialize the module and register UI with CloudM</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, version=version, initial=True)\ndef initialize_module(app: App):\n    \"\"\"Initialize the module and register UI with CloudM\"\"\"\n    # Register the UI with CloudM\n    app.run_any((\"CloudM\", \"add_ui\"),\n                name=\"TruthSeeker\",\n                title=\"TruthSeeker Research\",\n                path=f\"/api/{MOD_NAME}/get_main_ui\",\n                description=\"AI Research Assistant\"\n                )\n\n    # Initialize SSE message queues\n    if not hasattr(app, 'sse_queues'):\n        app.sse_queues = {}\n    print(\"TruthSeeker online\")\n    return Result.ok(info=\"ArXivPDFProcessor UI initialized\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.payment_cancel","title":"<code>payment_cancel(app, session_id, request_as_kwarg=True, request=None)</code>  <code>async</code>","text":"<p>Handle cancelled payment</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def payment_cancel(app: App, session_id: str, request_as_kwarg=True, request=None):\n    \"\"\"Handle cancelled payment\"\"\"\n    if hasattr(app, 'payment_info') and session_id in app.payment_info:\n        app.payment_info[session_id]['status'] = 'cancelled'\n\n        # Notify SSE clients about payment cancellation\n        if hasattr(app, 'payment_queues') and session_id in app.payment_queues:\n            await app.payment_queues[session_id].put({\n                \"status\": \"cancelled\"\n            })\n\n    return Result.html(app.web_context() + \"\"\"\n    &lt;div style=\"text-align: center; padding: 50px;\"&gt;\n        &lt;h2&gt;Payment Cancelled&lt;/h2&gt;\n        &lt;p&gt;Your payment was cancelled.&lt;/p&gt;\n        &lt;script&gt;\n            setTimeout(function() {\n                window.close();\n            }, 3000);\n        &lt;/script&gt;\n    &lt;/div&gt;\n    \"\"\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.payment_stream","title":"<code>payment_stream(app, session_id)</code>  <code>async</code>","text":"<p>SSE stream endpoint for payment status updates</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def payment_stream(app: App, session_id: str):\n    \"\"\"SSE stream endpoint for payment status updates\"\"\"\n    if not hasattr(app, 'payment_queues'):\n        app.payment_queues = {}\n\n    # Create a message queue for this session_id if it doesn't exist\n    if session_id not in app.payment_queues:\n        app.payment_queues[session_id] = asyncio.Queue()\n\n    async def generate():\n        try:\n            # Stream payment updates\n            while True:\n                try:\n                    # Wait for a payment update with a timeout\n                    payment_data = await asyncio.wait_for(app.payment_queues[session_id].get(), timeout=30)\n                    yield f\"event: payment_update\\ndata: {json.dumps(payment_data)}\\n\\n\"\n\n                    # If the payment is complete or cancelled, exit the loop\n                    if payment_data.get('status') in ['completed', 'cancelled']:\n                        break\n                except TimeoutError:\n                    # Send a keep-alive comment to prevent connection timeout\n                    yield \":\\n\\n\"\n        finally:\n            # Clean up resources when the client disconnects\n            if session_id in app.payment_queues:\n                # Keep the queue for other potential clients\n                pass\n\n    return Result.stream(generate())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.payment_success","title":"<code>payment_success(app, session_id, request_as_kwarg=True, request=None)</code>  <code>async</code>","text":"<p>Handle successful payment</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def payment_success(app: App, session_id: str, request_as_kwarg=True, request=None):\n    \"\"\"Handle successful payment\"\"\"\n    if not hasattr(app, 'payment_info') or session_id not in app.payment_info:\n        return Result.html(app.web_context() + \"\"\"\n        &lt;div style=\"text-align: center; padding: 50px;\"&gt;\n            &lt;h2&gt;Payment Session Not Found&lt;/h2&gt;\n            &lt;p&gt;Return to the main page to continue.&lt;/p&gt;\n            &lt;a href=\"/\" style=\"display: inline-block; margin-top: 20px; padding: 10px 20px; background-color: #4F46E5; color: white; text-decoration: none; border-radius: 5px;\"&gt;Return to Home&lt;/a&gt;\n        &lt;/div&gt;\n        \"\"\")\n\n    payment_info = app.payment_info[session_id]\n\n    try:\n        # Verify the payment with Stripe\n        stripe_session = stripe.checkout.Session.retrieve(payment_info['payment_id'])\n\n        if stripe_session.payment_status == 'paid':\n            payment_info['status'] = 'completed'\n\n            # Notify SSE clients about payment completion\n            if hasattr(app, 'payment_queues') and session_id in app.payment_queues:\n                await app.payment_queues[session_id].put({\n                    \"status\": \"completed\",\n                    \"amount\": payment_info['amount']\n                })\n\n            return Result.html(app.web_context() + \"\"\"\n            &lt;div style=\"text-align: center; padding: 50px;\"&gt;\n                &lt;h2&gt;Thank You for Your Support!&lt;/h2&gt;\n                &lt;p&gt;Your payment was successful. You can now close this window and continue with your research.&lt;/p&gt;\n                &lt;script&gt;\n                    setTimeout(function() {\n                        window.close();\n                    }, 5000);\n                &lt;/script&gt;\n            &lt;/div&gt;\n            \"\"\")\n        else:\n            return Result.html(app.web_context() + \"\"\"\n            &lt;div style=\"text-align: center; padding: 50px;\"&gt;\n                &lt;h2&gt;Payment Not Completed&lt;/h2&gt;\n                &lt;p&gt;Your payment has not been completed. Please try again.&lt;/p&gt;\n                &lt;button onclick=\"window.close()\"&gt;Close Window&lt;/button&gt;\n            &lt;/div&gt;\n            \"\"\")\n    except Exception as e:\n        return Result.html(app.web_context() + f\"\"\"\n        &lt;div style=\"text-align: center; padding: 50px;\"&gt;\n            &lt;h2&gt;Error Processing Payment&lt;/h2&gt;\n            &lt;p&gt;There was an error processing your payment: {str(e)}&lt;/p&gt;\n            &lt;button onclick=\"window.close()\"&gt;Close Window&lt;/button&gt;\n        &lt;/div&gt;\n        \"\"\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.research_results","title":"<code>research_results(app, research_id)</code>  <code>async</code>","text":"<p>Get the results of a completed research process</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def research_results(app: App, research_id: str):\n    \"\"\"Get the results of a completed research process\"\"\"\n    if not hasattr(app, 'research_processes') or research_id not in app.research_processes:\n        return Result.default_user_error(info=\"Research process not found\")\n\n    research_process = app.research_processes[research_id]\n\n    if research_process['status'] != 'complete':\n        return Result.default_user_error(info=\"Research is not complete\")\n\n    return Result.ok(data=research_process['results'])\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.research_status","title":"<code>research_status(app, research_id)</code>  <code>async</code>","text":"<p>Get the status of a research process</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def research_status(app: App, research_id: str):\n    \"\"\"Get the status of a research process\"\"\"\n    if not hasattr(app, 'research_processes') or research_id not in app.research_processes:\n        return Result.default_user_error(info=\"Research process not found\")\n\n    research_process = app.research_processes[research_id]\n\n    return Result.ok(data={\n        \"status\": research_process['status'],\n        \"progress\": research_process['progress'],\n        \"step\": research_process['step'],\n        \"info\": research_process['info']\n    })\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.start_research","title":"<code>start_research(app, data)</code>  <code>async</code>","text":"<p>Start a new research process</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def start_research(app: App, data):\n    \"\"\"Start a new research process\"\"\"\n    # Get data from the request\n    query = data.get(\"query\")\n    session_id = data.get(\"session_id\")\n    max_search = data.get(\"max_search\", 4)\n    num_search_result_per_query = data.get(\"num_search_result_per_query\", 4)\n\n    # Get the tools module\n    tools = get_app(\"ArXivPDFProcessor\").get_mod(\"isaa\")\n    if not hasattr(tools, 'initialized') or not tools.initialized:\n        tools.init_isaa(build=True)\n\n    # Generate a unique research_id\n    research_id = str(uuid.uuid4())\n\n    # Store the research information in a global dictionary\n    if not hasattr(app, 'research_processes'):\n        app.research_processes = {}\n\n    # Initialize SSE queues if not already done\n    if not hasattr(app, 'sse_queues'):\n        app.sse_queues = {}\n\n    # Create a queue for this research process\n    app.sse_queues[research_id] = asyncio.Queue()\n\n    # Create a processor with callback for status updates\n    app.research_processes[research_id] = {\n        'status': 'initializing',\n        'progress': 0.0,\n        'step': 'Initializing',\n        'info': '',\n        'query': query,\n        'session_id': session_id,\n        'processor': None,\n        'results': None,\n        'stop_requested': False\n    }\n\n    # Define the callback function that sends updates to the SSE queue\n    def status_callback(status_data):\n        if research_id in app.research_processes:\n            process = app.research_processes[research_id]\n            process['status'] = 'processing'\n            process['progress'] = status_data.get('progress', 0.0)\n            process['step'] = status_data.get('step', '')\n            process['info'] = status_data.get('info', '')\n\n            # Put the status update in the SSE queue\n            status_update = {\n                \"status\": process['status'],\n                \"progress\": process['progress'],\n                \"step\": process['step'],\n                \"info\": process['info']\n            }\n\n            if research_id in app.sse_queues:\n                asyncio.create_task(app.sse_queues[research_id].put(status_update))\n\n    # Create the processor\n    processor = ArXivPDFProcessor(\n        query=query,\n        tools=tools,\n        chunk_size=1_000_000,\n        overlap=2_000,\n        max_search=max_search,\n        num_search_result_per_query=num_search_result_per_query,\n        download_dir=f\"pdfs_{research_id}\",\n        callback=status_callback\n    )\n\n    app.research_processes[research_id]['processor'] = processor\n\n    # Process in the background\n    async def process_in_background():\n        try:\n            # Check if stop was requested before starting\n            if app.research_processes[research_id]['stop_requested']:\n                app.research_processes[research_id]['status'] = 'stopped'\n                if research_id in app.sse_queues:\n                    await app.sse_queues[research_id].put({\n                        \"status\": \"stopped\",\n                        \"progress\": 0,\n                        \"step\": \"Research stopped\",\n                        \"info\": \"\"\n                    })\n                return\n\n            # Start processing\n            papers, insights = await processor.process()\n\n            # Check if stop was requested during processing\n            if app.research_processes[research_id]['stop_requested']:\n                app.research_processes[research_id]['status'] = 'stopped'\n                if research_id in app.sse_queues:\n                    await app.sse_queues[research_id].put({\n                        \"status\": \"stopped\",\n                        \"progress\": 1,\n                        \"step\": \"Research stopped\",\n                        \"info\": \"\"\n                    })\n                return\n\n            # Store results\n            app.research_processes[research_id]['results'] = {\n                'papers': papers,\n                'insights': insights['response'] if insights and 'response' in insights else None\n            }\n            app.research_processes[research_id]['status'] = 'complete'\n\n            # Send final status update\n            if research_id in app.sse_queues:\n                await app.sse_queues[research_id].put({\n                    \"status\": \"complete\",\n                    \"progress\": 1,\n                    \"step\": \"Research complete\",\n                    \"info\": f\"Found {len(papers)} papers\"\n                })\n\n        except Exception as e:\n            app.research_processes[research_id]['status'] = 'error'\n            app.research_processes[research_id]['info'] = str(e)\n\n            # Send error status\n            if research_id in app.sse_queues:\n                await app.sse_queues[research_id].put({\n                    \"status\": \"error\",\n                    \"progress\": 0,\n                    \"step\": \"Error\",\n                    \"info\": str(e)\n                })\n\n            print(f\"Error in research process {research_id}: {str(e)}\")\n\n    # Start the background task\n    asyncio.create_task(process_in_background())\n\n    return Result.ok(data={\"research_id\": research_id})\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.status_stream","title":"<code>status_stream(app, research_id)</code>  <code>async</code>","text":"<p>SSE stream endpoint for research status updates</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def status_stream(app: App, research_id: str):\n    \"\"\"SSE stream endpoint for research status updates\"\"\"\n    if not hasattr(app, 'sse_queues'):\n        app.sse_queues = {}\n\n    # Create a message queue for this research_id if it doesn't exist\n    if research_id not in app.sse_queues:\n        app.sse_queues[research_id] = asyncio.Queue()\n\n    async def generate():\n        # Send initial status\n        if hasattr(app, 'research_processes') and research_id in app.research_processes:\n            process = app.research_processes[research_id]\n            initial_status = {\n                \"status\": process['status'],\n                \"progress\": process['progress'],\n                \"step\": process['step'],\n                \"info\": process['info']\n            }\n            yield f\"event: status_update\\ndata: {json.dumps(initial_status)}\\n\\n\"\n\n        try:\n            # Stream status updates\n            while True:\n                try:\n                    # Wait for a new status update with a timeout\n                    status_data = await asyncio.wait_for(app.sse_queues[research_id].get(), timeout=30)\n                    yield f\"event: status_update\\ndata: {json.dumps(status_data)}\\n\\n\"\n\n                    # If the research is complete or there was an error, exit the loop\n                    if status_data.get('status') in ['complete', 'error', 'stopped']:\n                        break\n                except TimeoutError:\n                    # Send a keep-alive comment to prevent connection timeout\n                    yield \":\\n\\n\"\n        finally:\n            # Clean up resources when the client disconnects\n            if research_id in app.sse_queues:\n                # Keep the queue for other potential clients\n                pass\n\n    return Result.stream(generate())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.stop_research","title":"<code>stop_research(app, data)</code>  <code>async</code>","text":"<p>Stop a research process</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def stop_research(app: App, data):\n    \"\"\"Stop a research process\"\"\"\n    research_id = data.get(\"research_id\")\n    if not hasattr(app, 'research_processes') or research_id not in app.research_processes:\n        return Result.default_user_error(info=\"Research process not found\")\n\n    app.research_processes[research_id]['stop_requested'] = True\n\n    # Send stopped status to SSE clients\n    if hasattr(app, 'sse_queues') and research_id in app.sse_queues:\n        await app.sse_queues[research_id].put({\n            \"status\": \"stopped\",\n            \"progress\": app.research_processes[research_id]['progress'],\n            \"step\": \"Stopping research\",\n            \"info\": \"\"\n        })\n\n    return Result.ok(data={\"status\": \"stop_requested\"})\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.one","title":"<code>one</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.one.IntelligenceRingEmbeddings","title":"<code>IntelligenceRingEmbeddings</code>","text":"Source code in <code>toolboxv2/mods/TruthSeeker/one.py</code> <pre><code>class IntelligenceRingEmbeddings:\n    name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n    clip_name: str = \"openai/clip-vit-base-patch32\"\n    wav2vec_name: str = \"facebook/wav2vec2-base-960h\"\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    vector_size: int = 768\n    tokenizer: Any | None = None\n    text_model: Any | None = None\n\n    clip_processor: Any | None = None\n    clip_model: Any | None = None\n\n    audio_processor: Any | None = None\n    audio_model: Any | None = None\n\n    text_projection: Any | None = None\n    image_projection: Any | None = None\n    audio_projection: Any | None = None\n\n    def __init__(self, **kwargs):\n\n        super().__init__(**kwargs)\n        self._ndims = self.vector_size\n\n        # Text embedding model\n        self.tokenizer = AutoTokenizer.from_pretrained(self.name)\n        self.text_model = AutoModel.from_pretrained(self.name).to(self.device)\n\n        # Image embedding model (CLIP)\n        self.clip_processor = CLIPProcessor.from_pretrained(self.clip_name)\n        self.clip_model = CLIPModel.from_pretrained(self.clip_name).to(self.device)\n\n        # Audio embedding model (Wav2Vec2)\n        self.audio_processor = Wav2Vec2Processor.from_pretrained(self.wav2vec_name)\n        self.audio_model = Wav2Vec2Model.from_pretrained(self.wav2vec_name).to(self.device)\n\n        # Projection layers to align dimensions\n        self.text_projection = torch.nn.Linear(\n            self.text_model.config.hidden_size,\n            self.vector_size\n        ).to(self.device)\n        self.image_projection = torch.nn.Linear(\n            self.clip_model.config.vision_config.hidden_size,\n            self.vector_size\n        ).to(self.device)\n        self.audio_projection = torch.nn.Linear(\n            self.audio_model.config.hidden_size,\n            self.vector_size\n        ).to(self.device)\n\n    def _process_text(self, text: str) -&gt; torch.Tensor:\n        encoded_input = self.tokenizer(\n            text,\n            padding=True,\n            truncation=True,\n            max_length=self.vector_size,\n            return_tensors='pt'\n        ).to(self.device)\n\n        with torch.no_grad():\n            outputs = self.text_model(**encoded_input)\n            embeddings = self._mean_pooling(outputs, encoded_input['attention_mask'])\n            projected = self.text_projection(embeddings)\n            return torch.nn.functional.normalize(projected, p=2, dim=1)\n\n    def _process_image(self, image_data: bytes | str) -&gt; torch.Tensor:\n        # Handle different image input types\n        if isinstance(image_data, str):\n            if image_data.startswith('data:image'):\n                # Handle base64 encoded images\n                image_data = base64.b64decode(image_data.split(',')[1])\n            else:\n                # Handle file paths\n                with open(image_data, 'rb') as f:\n                    image_data = f.read()\n\n        # Convert bytes to PIL Image\n        image = Image.open(io.BytesIO(image_data))\n\n        # Process image with CLIP\n        inputs = self.clip_processor(images=image, return_tensors=\"pt\").to(self.device)\n\n        with torch.no_grad():\n            outputs = self.clip_model.get_image_features(**inputs)\n            projected = self.image_projection(outputs)\n            return torch.nn.functional.normalize(projected, p=2, dim=1)\n\n    def _process_audio(self, audio_data: bytes | str | np.ndarray) -&gt; torch.Tensor:\n        try:\n            import torchaudio\n        except ImportError:\n            raise ValueError(\"Couldn't load audio install torchaudio'\")\n        # Handle different audio input types\n        if isinstance(audio_data, str):\n            if audio_data.startswith('data:audio'):\n                # Handle base64 encoded audio\n                audio_data = base64.b64decode(audio_data.split(',')[1])\n                waveform, sample_rate = torchaudio.load(io.BytesIO(audio_data))\n            else:\n                # Handle file paths\n                waveform, sample_rate = torchaudio.load(audio_data)\n        elif isinstance(audio_data, bytes):\n            waveform, sample_rate = torchaudio.load(io.BytesIO(audio_data))\n        else:\n            # Assume numpy array with sample rate in metadata\n            waveform = torch.from_numpy(audio_data)\n            sample_rate = 16000  # Default sample rate\n\n        # Resample if necessary\n        if sample_rate != 16000:\n            resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n            waveform = resampler(waveform)\n\n        # Process audio with Wav2Vec2\n        inputs = self.audio_processor(waveform, sampling_rate=16000, return_tensors=\"pt\").to(self.device)\n\n        with torch.no_grad():\n            outputs = self.audio_model(**inputs)\n            # Mean pooling over time dimension\n            embeddings = outputs.last_hidden_state.mean(dim=1)\n            projected = self.audio_projection(embeddings)\n            return torch.nn.functional.normalize(projected, p=2, dim=1)\n\n    def _mean_pooling(self, model_output: torch.Tensor, attention_mask: torch.Tensor) -&gt; torch.Tensor:\n        token_embeddings = model_output[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n    def process_input(self, input_data: InputData) -&gt; np.ndarray:\n        if input_data.modality == \"text\":\n            embeddings = self._process_text(input_data.content)\n        elif input_data.modality == \"image\":\n            embeddings = self._process_image(input_data.content)\n        elif input_data.modality == \"audio\":\n            embeddings = self._process_audio(input_data.content)\n        else:\n            raise ValueError(f\"Unsupported modality: {input_data.modality}\")\n\n        return embeddings.cpu().numpy()\n\n    def compute_query_embeddings(self, query: str | bytes | np.ndarray, modality: str = \"text\") -&gt; list[\n        np.ndarray]:\n        \"\"\"Compute embeddings for query input\"\"\"\n        input_data = InputData(query, modality)\n        embedding = self.process_input(input_data)\n        return [embedding.squeeze()]\n\n    def compute_source_embeddings(self, sources: list[str | bytes | np.ndarray], modalities: list[str]) -&gt; list[\n        np.ndarray]:\n        \"\"\"Compute embeddings for source inputs\"\"\"\n        embeddings = []\n        for source, modality in zip(sources, modalities, strict=False):\n            input_data = InputData(source, modality)\n            embedding = self.process_input(input_data)\n            embeddings.append(embedding.squeeze())\n        return embeddings\n\n    def ndims(self) -&gt; int:\n        return self._ndims\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.one.IntelligenceRingEmbeddings.compute_query_embeddings","title":"<code>compute_query_embeddings(query, modality='text')</code>","text":"<p>Compute embeddings for query input</p> Source code in <code>toolboxv2/mods/TruthSeeker/one.py</code> <pre><code>def compute_query_embeddings(self, query: str | bytes | np.ndarray, modality: str = \"text\") -&gt; list[\n    np.ndarray]:\n    \"\"\"Compute embeddings for query input\"\"\"\n    input_data = InputData(query, modality)\n    embedding = self.process_input(input_data)\n    return [embedding.squeeze()]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.one.IntelligenceRingEmbeddings.compute_source_embeddings","title":"<code>compute_source_embeddings(sources, modalities)</code>","text":"<p>Compute embeddings for source inputs</p> Source code in <code>toolboxv2/mods/TruthSeeker/one.py</code> <pre><code>def compute_source_embeddings(self, sources: list[str | bytes | np.ndarray], modalities: list[str]) -&gt; list[\n    np.ndarray]:\n    \"\"\"Compute embeddings for source inputs\"\"\"\n    embeddings = []\n    for source, modality in zip(sources, modalities, strict=False):\n        input_data = InputData(source, modality)\n        embedding = self.process_input(input_data)\n        embeddings.append(embedding.squeeze())\n    return embeddings\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests","title":"<code>tests</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker","title":"<code>TestTruthSeeker</code>","text":"<p>               Bases: <code>TestCase</code></p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>class TestTruthSeeker(unittest.TestCase):\n    def setUp(self):\n        # Mock the App class\n        self.mock_app = Mock()\n        self.mock_app.get_mod.return_value = Mock()\n\n        # Setup mock for run_any that returns iterable dict\n        self.mock_app.run_any.return_value = {\n            \"1\": {\"name\": \"template1\"},\n            \"2\": {\"name\": \"template2\"}\n        }\n\n        # Mock RequestSession\n        self.mock_request = Mock()\n        self.mock_request.json = AsyncMock()\n\n    @patch('os.path.join')\n    @patch('builtins.open', create=True)\n    def test_start_initialization(self, mock_open, mock_join):\n        \"\"\"Test the start function initializes correctly\"\"\"\n        # Setup mock file handling\n        mock_file = Mock()\n        mock_file.read.return_value = \"test content\"\n        mock_open.return_value.__enter__.return_value = mock_file\n\n        # Call start function\n        start(self.mock_app)\n\n        # Verify app initialization calls\n        self.mock_app.get_mod.assert_called_with(\"CodeVerification\")\n        self.mock_app.run_any.assert_any_call((\"CodeVerification\", \"init_scope\"), scope=\"TruthSeeker\")\n        self.mock_app.run_any.assert_any_call((\"CodeVerification\", \"init_scope\"), scope=\"TruthSeeker-promo\")\n\n    @async_test\n    async def test_codes_valid_request(self):\n        \"\"\"Test the codes function with valid input\"\"\"\n        # Mock request data\n        test_data = {\n            \"query\": \"test query\",\n            \"depth\": \"Q\",\n            \"promoCode\": \"PROMO15\",\n            \"ontimeCode\": \"TEST123\"\n        }\n        self.mock_request.json.return_value = test_data\n\n        # Mock code verification\n        self.mock_app.run_any.return_value = {\n            \"template_name\": \"Promo15\",\n            \"usage_type\": \"one_time\"\n        }\n\n        result = await codes(self.mock_app, self.mock_request)\n\n        self.assertTrue(result['valid'])\n        self.assertIn('ontimeKey', result)\n        self.assertIn('ppc', result)\n\n    @async_test\n    async def test_codes_invalid_promo(self):\n        \"\"\"Test the codes function with invalid promo code\"\"\"\n        test_data = {\n            \"query\": \"test query\",\n            \"depth\": \"I\",\n            \"promoCode\": \"INVALID\",\n            \"ontimeCode\": \"TEST123\"\n        }\n        self.mock_request.json.return_value = test_data\n\n        # Mock invalid promo code verification\n        self.mock_app.run_any.return_value = None\n\n        result = await codes(self.mock_app, self.mock_request)\n\n        self.assertIn('ppc', result)\n        self.assertTrue(result['ppc']['price'] &gt; 0)\n\n    @async_test\n    async def test_process_valid_request(self):\n        \"\"\"Test the process function with valid input\"\"\"\n        test_data = {\n            \"query\": \"test query\",\n            \"depth\": \"Q\",\n            \"ontimeKey\": \"VALID_KEY\",\n            \"email\": \"test@example.com\"\n        }\n        self.mock_request.json.return_value = test_data\n\n        # Mock valid key verification\n        self.mock_app.run_any.return_value = {\n            \"template_name\": \"PROCESS\",\n            \"usage_type\": \"timed\",\n            \"uses_count\": 1\n        }\n\n        # Mock ArXivPDFProcessor\n        with patch('toolboxv2.mods.TruthSeeker.module.ArXivPDFProcessor') as mock_processor:\n            mock_insights = MagicMock()\n            mock_insights.is_true = \"True\"\n            mock_insights.summary = \"Test summary\"\n            mock_insights.key_point = \"Point1&gt;\\n\\n&lt;Point2\"\n\n            mock_processor.return_value.process.return_value = ([], mock_insights)\n\n            result = await process(self.mock_app, self.mock_request)\n\n            self.assertEqual(result['is_true'], \"True\")\n            self.assertEqual(result['summary'], \"Test summary\")\n\n    @async_test\n    async def test_process_invalid_key(self):\n        \"\"\"Test the process function with invalid key\"\"\"\n        test_data = {\n            \"query\": \"test query\",\n            \"depth\": \"Q\",\n            \"ontimeKey\": \"INVALID_KEY\",\n            \"email\": \"test@example.com\"\n        }\n        self.mock_request.json.return_value = test_data\n\n        # Mock invalid key verification\n        self.mock_app.run_any.return_value = None\n\n        result = await process(self.mock_app, self.mock_request)\n\n        self.assertEqual(result['summary'], \"INVALID QUERY\")\n        self.assertEqual(result['insights'], [])\n        self.assertEqual(result['papers'], [])\n\n    def test_byCode_functionality(self):\n        \"\"\"Test the byCode function\"\"\"\n        test_request = Mock()\n        test_request.json.return_value = [\"payKey\", \"codeClass\", \"ontimeKey\"]\n\n        result = byCode(self.mock_app, test_request)\n\n        self.assertEqual(result, {'code': 'code'})\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker.test_byCode_functionality","title":"<code>test_byCode_functionality()</code>","text":"<p>Test the byCode function</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>def test_byCode_functionality(self):\n    \"\"\"Test the byCode function\"\"\"\n    test_request = Mock()\n    test_request.json.return_value = [\"payKey\", \"codeClass\", \"ontimeKey\"]\n\n    result = byCode(self.mock_app, test_request)\n\n    self.assertEqual(result, {'code': 'code'})\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker.test_codes_invalid_promo","title":"<code>test_codes_invalid_promo()</code>  <code>async</code>","text":"<p>Test the codes function with invalid promo code</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@async_test\nasync def test_codes_invalid_promo(self):\n    \"\"\"Test the codes function with invalid promo code\"\"\"\n    test_data = {\n        \"query\": \"test query\",\n        \"depth\": \"I\",\n        \"promoCode\": \"INVALID\",\n        \"ontimeCode\": \"TEST123\"\n    }\n    self.mock_request.json.return_value = test_data\n\n    # Mock invalid promo code verification\n    self.mock_app.run_any.return_value = None\n\n    result = await codes(self.mock_app, self.mock_request)\n\n    self.assertIn('ppc', result)\n    self.assertTrue(result['ppc']['price'] &gt; 0)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker.test_codes_valid_request","title":"<code>test_codes_valid_request()</code>  <code>async</code>","text":"<p>Test the codes function with valid input</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@async_test\nasync def test_codes_valid_request(self):\n    \"\"\"Test the codes function with valid input\"\"\"\n    # Mock request data\n    test_data = {\n        \"query\": \"test query\",\n        \"depth\": \"Q\",\n        \"promoCode\": \"PROMO15\",\n        \"ontimeCode\": \"TEST123\"\n    }\n    self.mock_request.json.return_value = test_data\n\n    # Mock code verification\n    self.mock_app.run_any.return_value = {\n        \"template_name\": \"Promo15\",\n        \"usage_type\": \"one_time\"\n    }\n\n    result = await codes(self.mock_app, self.mock_request)\n\n    self.assertTrue(result['valid'])\n    self.assertIn('ontimeKey', result)\n    self.assertIn('ppc', result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker.test_process_invalid_key","title":"<code>test_process_invalid_key()</code>  <code>async</code>","text":"<p>Test the process function with invalid key</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@async_test\nasync def test_process_invalid_key(self):\n    \"\"\"Test the process function with invalid key\"\"\"\n    test_data = {\n        \"query\": \"test query\",\n        \"depth\": \"Q\",\n        \"ontimeKey\": \"INVALID_KEY\",\n        \"email\": \"test@example.com\"\n    }\n    self.mock_request.json.return_value = test_data\n\n    # Mock invalid key verification\n    self.mock_app.run_any.return_value = None\n\n    result = await process(self.mock_app, self.mock_request)\n\n    self.assertEqual(result['summary'], \"INVALID QUERY\")\n    self.assertEqual(result['insights'], [])\n    self.assertEqual(result['papers'], [])\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker.test_process_valid_request","title":"<code>test_process_valid_request()</code>  <code>async</code>","text":"<p>Test the process function with valid input</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@async_test\nasync def test_process_valid_request(self):\n    \"\"\"Test the process function with valid input\"\"\"\n    test_data = {\n        \"query\": \"test query\",\n        \"depth\": \"Q\",\n        \"ontimeKey\": \"VALID_KEY\",\n        \"email\": \"test@example.com\"\n    }\n    self.mock_request.json.return_value = test_data\n\n    # Mock valid key verification\n    self.mock_app.run_any.return_value = {\n        \"template_name\": \"PROCESS\",\n        \"usage_type\": \"timed\",\n        \"uses_count\": 1\n    }\n\n    # Mock ArXivPDFProcessor\n    with patch('toolboxv2.mods.TruthSeeker.module.ArXivPDFProcessor') as mock_processor:\n        mock_insights = MagicMock()\n        mock_insights.is_true = \"True\"\n        mock_insights.summary = \"Test summary\"\n        mock_insights.key_point = \"Point1&gt;\\n\\n&lt;Point2\"\n\n        mock_processor.return_value.process.return_value = ([], mock_insights)\n\n        result = await process(self.mock_app, self.mock_request)\n\n        self.assertEqual(result['is_true'], \"True\")\n        self.assertEqual(result['summary'], \"Test summary\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker.test_start_initialization","title":"<code>test_start_initialization(mock_open, mock_join)</code>","text":"<p>Test the start function initializes correctly</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@patch('os.path.join')\n@patch('builtins.open', create=True)\ndef test_start_initialization(self, mock_open, mock_join):\n    \"\"\"Test the start function initializes correctly\"\"\"\n    # Setup mock file handling\n    mock_file = Mock()\n    mock_file.read.return_value = \"test content\"\n    mock_open.return_value.__enter__.return_value = mock_file\n\n    # Call start function\n    start(self.mock_app)\n\n    # Verify app initialization calls\n    self.mock_app.get_mod.assert_called_with(\"CodeVerification\")\n    self.mock_app.run_any.assert_any_call((\"CodeVerification\", \"init_scope\"), scope=\"TruthSeeker\")\n    self.mock_app.run_any.assert_any_call((\"CodeVerification\", \"init_scope\"), scope=\"TruthSeeker-promo\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.run_all_tests","title":"<code>run_all_tests()</code>","text":"<p>Run all test classes</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@default_test\ndef run_all_tests():\n    \"\"\"Run all test classes\"\"\"\n    return run_test_suite()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.run_arxiv_processor_tests","title":"<code>run_arxiv_processor_tests(test_name=None)</code>","text":"<p>Run TestArXivPDFProcessor tests</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>def run_arxiv_processor_tests(test_name=None):\n    \"\"\"Run TestArXivPDFProcessor tests\"\"\"\n    return run_test_suite(TestArXivPDFProcessor, test_name)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.run_pdf_downloader_tests","title":"<code>run_pdf_downloader_tests(test_name=None)</code>","text":"<p>Run TestRobustPDFDownloader tests</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>def run_pdf_downloader_tests(test_name=None):\n    \"\"\"Run TestRobustPDFDownloader tests\"\"\"\n    return run_test_suite(TestRobustPDFDownloader, test_name)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.run_specific_test","title":"<code>run_specific_test(test_class, test_name)</code>","text":"<p>Run a specific test from a test class</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>def run_specific_test(test_class, test_name):\n    \"\"\"Run a specific test from a test class\"\"\"\n    return run_test_suite(test_class, test_name)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.run_test_suite","title":"<code>run_test_suite(test_class=None, test_name=None, verbosity=2)</code>","text":"<p>Run specific test class or test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_class</code> <p>The test class to run (optional)</p> <code>None</code> <code>test_name</code> <p>Specific test method name to run (optional)</p> <code>None</code> <code>verbosity</code> <p>Output detail level (default=2)</p> <code>2</code> <p>Returns:</p> Type Description <p>TestResult object</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>def run_test_suite(test_class=None, test_name=None, verbosity=2):\n    \"\"\"\n    Run specific test class or test case.\n\n    Args:\n        test_class: The test class to run (optional)\n        test_name: Specific test method name to run (optional)\n        verbosity: Output detail level (default=2)\n\n    Returns:\n        TestResult object\n    \"\"\"\n    loader = unittest.TestLoader()\n    suite = unittest.TestSuite()\n\n    if test_class and test_name:\n        # Run specific test method\n        suite.addTest(test_class(test_name))\n    elif test_class:\n        # Run all tests in the class\n        suite.addTests(loader.loadTestsFromTestCase(test_class))\n    else:\n        # Run all tests\n        suite.addTests(loader.loadTestsFromModule(sys.modules[__name__]))\n\n    runner = unittest.TextTestRunner(verbosity=verbosity)\n    return runner.run(suite)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.run_truth_seeker_tests","title":"<code>run_truth_seeker_tests(test_name=None)</code>","text":"<p>Run TestTruthSeeker tests</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>def run_truth_seeker_tests(test_name=None):\n    \"\"\"Run TestTruthSeeker tests\"\"\"\n    return run_test_suite(TestTruthSeeker, test_name)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.test_arxiv_search","title":"<code>test_arxiv_search()</code>","text":"<p>Run only ArXiv search tests</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@default_test\ndef test_arxiv_search():\n    \"\"\"Run only ArXiv search tests\"\"\"\n    return run_specific_test(\n        TestArXivPDFProcessor,\n        'test_search_and_process_papers'\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.test_pdf_download","title":"<code>test_pdf_download()</code>","text":"<p>Run only PDF download tests</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@default_test\ndef test_pdf_download():\n    \"\"\"Run only PDF download tests\"\"\"\n    return run_specific_test(\n        TestRobustPDFDownloader,\n        'test_download_pdf_success'\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.test_truth_seeker","title":"<code>test_truth_seeker()</code>","text":"<p>Run only PDF download tests</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@default_test\ndef test_truth_seeker():\n    \"\"\"Run only PDF download tests\"\"\"\n    return run_specific_test(\n        TestTruthSeeker,\n        'test_truth_seeker_success'\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager","title":"<code>WebSocketManager</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager","title":"<code>WebSocketPoolManager</code>","text":"Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>class WebSocketPoolManager:\n    def __init__(self):\n        self.pools: dict[str, dict[str, Any]] = {}\n        self.logger = logging.getLogger(__name__)\n\n    async def create_pool(self, pool_id: str) -&gt; None:\n        \"\"\"Create a new WebSocket pool.\"\"\"\n        if pool_id not in self.pools:\n            self.pools[pool_id] = {\n                'connections': {},\n                'actions': {},\n                'global_actions': {}\n            }\n            self.logger.info(f\"Created new pool: {pool_id}\")\n        else:\n            self.logger.warning(f\"Pool {pool_id} already exists\")\n\n    async def add_connection(self, pool_id: str, connection_id: str, websocket) -&gt; None:\n        \"\"\"Add a WebSocket connection to a pool.\"\"\"\n        if pool_id not in self.pools:\n            await self.create_pool(pool_id)\n\n        self.pools[pool_id]['connections'][connection_id] = websocket\n        self.logger.info(f\"Added connection {connection_id} to pool {pool_id}\")\n\n    async def remove_connection(self, pool_id: str, connection_id: str) -&gt; None:\n        \"\"\"Remove a WebSocket connection from a pool.\"\"\"\n        if pool_id in self.pools and connection_id in self.pools[pool_id]['connections']:\n            del self.pools[pool_id]['connections'][connection_id]\n            self.logger.info(f\"Removed connection {connection_id} from pool {pool_id}\")\n        else:\n            self.logger.warning(f\"Connection {connection_id} not found in pool {pool_id}\")\n\n    def register_action(self, pool_id: str, action_name: str, handler: Callable,\n                        connection_ids: list[str] = None) -&gt; None:\n        \"\"\"Register an action for specific connections or the entire pool.\"\"\"\n        if pool_id not in self.pools:\n            self.logger.error(f\"Pool {pool_id} does not exist\")\n            return\n\n        if connection_ids is None:\n            self.pools[pool_id]['global_actions'][action_name] = handler\n            self.logger.info(f\"Registered global action {action_name} for pool {pool_id}\")\n        else:\n            for conn_id in connection_ids:\n                if conn_id not in self.pools[pool_id]['actions']:\n                    self.pools[pool_id]['actions'][conn_id] = {}\n                self.pools[pool_id]['actions'][conn_id][action_name] = handler\n            self.logger.info(f\"Registered action {action_name} for connections {connection_ids} in pool {pool_id}\")\n\n    async def handle_message(self, pool_id: str, connection_id: str, message: str) -&gt; None:\n        \"\"\"Handle incoming messages and route them to the appropriate action handler.\"\"\"\n        if pool_id not in self.pools or connection_id not in self.pools[pool_id]['connections']:\n            self.logger.error(f\"Invalid pool_id or connection_id: {pool_id}, {connection_id}\")\n            return\n\n        try:\n            data = json.loads(message)\n            action = data.get('action')\n\n            if action:\n                if action in self.pools[pool_id]['global_actions']:\n                    await self.pools[pool_id]['global_actions'][action](pool_id, connection_id, data)\n                elif connection_id in self.pools[pool_id]['actions'] and action in self.pools[pool_id]['actions'][\n                    connection_id]:\n                    await self.pools[pool_id]['actions'][connection_id][action](pool_id, connection_id, data)\n                else:\n                    self.logger.warning(f\"No handler found for action {action} in pool {pool_id}\")\n            else:\n                self.logger.warning(f\"No action specified in message from {connection_id} in pool {pool_id}\")\n        except json.JSONDecodeError:\n            self.logger.error(f\"Invalid JSON received from {connection_id} in pool {pool_id}\")\n\n    async def broadcast(self, pool_id: str, message: str, exclude_connection_id: str = None) -&gt; None:\n        \"\"\"Broadcast a message to all connections in a pool, optionally excluding one connection.\"\"\"\n        if pool_id not in self.pools:\n            self.logger.error(f\"Pool {pool_id} does not exist\")\n            return\n\n        for conn_id, websocket in self.pools[pool_id]['connections'].items():\n            if conn_id != exclude_connection_id:\n                try:\n                    await websocket.send_text(message)\n                except Exception as e:\n                    self.logger.error(f\"Error sending message to {conn_id} in pool {pool_id}: {str(e)}\")\n\n    async def send_to_connection(self, pool_id: str, connection_id: str, message: str) -&gt; None:\n        \"\"\"Send a message to a specific connection in a pool.\"\"\"\n        if pool_id in self.pools and connection_id in self.pools[pool_id]['connections']:\n            try:\n                await self.pools[pool_id]['connections'][connection_id].send_text(message)\n            except Exception as e:\n                self.logger.error(f\"Error sending message to {connection_id} in pool {pool_id}: {str(e)}\")\n        else:\n            self.logger.error(f\"Connection {connection_id} not found in pool {pool_id}\")\n\n    def get_pool_connections(self, pool_id: str) -&gt; list[str]:\n        \"\"\"Get a list of all connection IDs in a pool.\"\"\"\n        if pool_id in self.pools:\n            return list(self.pools[pool_id]['connections'].keys())\n        else:\n            self.logger.error(f\"Pool {pool_id} does not exist\")\n            return []\n\n    def get_all_pools(self) -&gt; list[str]:\n        \"\"\"Get a list of all pool IDs.\"\"\"\n        return list(self.pools.keys())\n\n    async def close_pool(self, pool_id: str) -&gt; None:\n        \"\"\"Close all connections in a pool and remove the pool.\"\"\"\n        if pool_id in self.pools:\n            for websocket in self.pools[pool_id]['connections'].values():\n                await websocket.close()\n            del self.pools[pool_id]\n            self.logger.info(f\"Closed and removed pool {pool_id}\")\n        else:\n            self.logger.warning(f\"Pool {pool_id} does not exist\")\n\n    async def close_all_pools(self) -&gt; None:\n        \"\"\"Close all connections in all pools and remove all pools.\"\"\"\n        for pool_id in list(self.pools.keys()):\n            await self.close_pool(pool_id)\n        self.logger.info(\"Closed all pools\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.add_connection","title":"<code>add_connection(pool_id, connection_id, websocket)</code>  <code>async</code>","text":"<p>Add a WebSocket connection to a pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def add_connection(self, pool_id: str, connection_id: str, websocket) -&gt; None:\n    \"\"\"Add a WebSocket connection to a pool.\"\"\"\n    if pool_id not in self.pools:\n        await self.create_pool(pool_id)\n\n    self.pools[pool_id]['connections'][connection_id] = websocket\n    self.logger.info(f\"Added connection {connection_id} to pool {pool_id}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.broadcast","title":"<code>broadcast(pool_id, message, exclude_connection_id=None)</code>  <code>async</code>","text":"<p>Broadcast a message to all connections in a pool, optionally excluding one connection.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def broadcast(self, pool_id: str, message: str, exclude_connection_id: str = None) -&gt; None:\n    \"\"\"Broadcast a message to all connections in a pool, optionally excluding one connection.\"\"\"\n    if pool_id not in self.pools:\n        self.logger.error(f\"Pool {pool_id} does not exist\")\n        return\n\n    for conn_id, websocket in self.pools[pool_id]['connections'].items():\n        if conn_id != exclude_connection_id:\n            try:\n                await websocket.send_text(message)\n            except Exception as e:\n                self.logger.error(f\"Error sending message to {conn_id} in pool {pool_id}: {str(e)}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.close_all_pools","title":"<code>close_all_pools()</code>  <code>async</code>","text":"<p>Close all connections in all pools and remove all pools.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def close_all_pools(self) -&gt; None:\n    \"\"\"Close all connections in all pools and remove all pools.\"\"\"\n    for pool_id in list(self.pools.keys()):\n        await self.close_pool(pool_id)\n    self.logger.info(\"Closed all pools\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.close_pool","title":"<code>close_pool(pool_id)</code>  <code>async</code>","text":"<p>Close all connections in a pool and remove the pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def close_pool(self, pool_id: str) -&gt; None:\n    \"\"\"Close all connections in a pool and remove the pool.\"\"\"\n    if pool_id in self.pools:\n        for websocket in self.pools[pool_id]['connections'].values():\n            await websocket.close()\n        del self.pools[pool_id]\n        self.logger.info(f\"Closed and removed pool {pool_id}\")\n    else:\n        self.logger.warning(f\"Pool {pool_id} does not exist\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.create_pool","title":"<code>create_pool(pool_id)</code>  <code>async</code>","text":"<p>Create a new WebSocket pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def create_pool(self, pool_id: str) -&gt; None:\n    \"\"\"Create a new WebSocket pool.\"\"\"\n    if pool_id not in self.pools:\n        self.pools[pool_id] = {\n            'connections': {},\n            'actions': {},\n            'global_actions': {}\n        }\n        self.logger.info(f\"Created new pool: {pool_id}\")\n    else:\n        self.logger.warning(f\"Pool {pool_id} already exists\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.get_all_pools","title":"<code>get_all_pools()</code>","text":"<p>Get a list of all pool IDs.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>def get_all_pools(self) -&gt; list[str]:\n    \"\"\"Get a list of all pool IDs.\"\"\"\n    return list(self.pools.keys())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.get_pool_connections","title":"<code>get_pool_connections(pool_id)</code>","text":"<p>Get a list of all connection IDs in a pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>def get_pool_connections(self, pool_id: str) -&gt; list[str]:\n    \"\"\"Get a list of all connection IDs in a pool.\"\"\"\n    if pool_id in self.pools:\n        return list(self.pools[pool_id]['connections'].keys())\n    else:\n        self.logger.error(f\"Pool {pool_id} does not exist\")\n        return []\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.handle_message","title":"<code>handle_message(pool_id, connection_id, message)</code>  <code>async</code>","text":"<p>Handle incoming messages and route them to the appropriate action handler.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def handle_message(self, pool_id: str, connection_id: str, message: str) -&gt; None:\n    \"\"\"Handle incoming messages and route them to the appropriate action handler.\"\"\"\n    if pool_id not in self.pools or connection_id not in self.pools[pool_id]['connections']:\n        self.logger.error(f\"Invalid pool_id or connection_id: {pool_id}, {connection_id}\")\n        return\n\n    try:\n        data = json.loads(message)\n        action = data.get('action')\n\n        if action:\n            if action in self.pools[pool_id]['global_actions']:\n                await self.pools[pool_id]['global_actions'][action](pool_id, connection_id, data)\n            elif connection_id in self.pools[pool_id]['actions'] and action in self.pools[pool_id]['actions'][\n                connection_id]:\n                await self.pools[pool_id]['actions'][connection_id][action](pool_id, connection_id, data)\n            else:\n                self.logger.warning(f\"No handler found for action {action} in pool {pool_id}\")\n        else:\n            self.logger.warning(f\"No action specified in message from {connection_id} in pool {pool_id}\")\n    except json.JSONDecodeError:\n        self.logger.error(f\"Invalid JSON received from {connection_id} in pool {pool_id}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.register_action","title":"<code>register_action(pool_id, action_name, handler, connection_ids=None)</code>","text":"<p>Register an action for specific connections or the entire pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>def register_action(self, pool_id: str, action_name: str, handler: Callable,\n                    connection_ids: list[str] = None) -&gt; None:\n    \"\"\"Register an action for specific connections or the entire pool.\"\"\"\n    if pool_id not in self.pools:\n        self.logger.error(f\"Pool {pool_id} does not exist\")\n        return\n\n    if connection_ids is None:\n        self.pools[pool_id]['global_actions'][action_name] = handler\n        self.logger.info(f\"Registered global action {action_name} for pool {pool_id}\")\n    else:\n        for conn_id in connection_ids:\n            if conn_id not in self.pools[pool_id]['actions']:\n                self.pools[pool_id]['actions'][conn_id] = {}\n            self.pools[pool_id]['actions'][conn_id][action_name] = handler\n        self.logger.info(f\"Registered action {action_name} for connections {connection_ids} in pool {pool_id}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.remove_connection","title":"<code>remove_connection(pool_id, connection_id)</code>  <code>async</code>","text":"<p>Remove a WebSocket connection from a pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def remove_connection(self, pool_id: str, connection_id: str) -&gt; None:\n    \"\"\"Remove a WebSocket connection from a pool.\"\"\"\n    if pool_id in self.pools and connection_id in self.pools[pool_id]['connections']:\n        del self.pools[pool_id]['connections'][connection_id]\n        self.logger.info(f\"Removed connection {connection_id} from pool {pool_id}\")\n    else:\n        self.logger.warning(f\"Connection {connection_id} not found in pool {pool_id}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.send_to_connection","title":"<code>send_to_connection(pool_id, connection_id, message)</code>  <code>async</code>","text":"<p>Send a message to a specific connection in a pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def send_to_connection(self, pool_id: str, connection_id: str, message: str) -&gt; None:\n    \"\"\"Send a message to a specific connection in a pool.\"\"\"\n    if pool_id in self.pools and connection_id in self.pools[pool_id]['connections']:\n        try:\n            await self.pools[pool_id]['connections'][connection_id].send_text(message)\n        except Exception as e:\n            self.logger.error(f\"Error sending message to {connection_id} in pool {pool_id}: {str(e)}\")\n    else:\n        self.logger.error(f\"Connection {connection_id} not found in pool {pool_id}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb","title":"<code>WhatsAppTb</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client","title":"<code>client</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.DocumentSystem","title":"<code>DocumentSystem</code>","text":"Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>class DocumentSystem:\n    def __init__(self, storage: BlobStorage):\n        self.storage = storage\n        self.media_types = {\n            'document': ['pdf', 'doc', 'docx', 'txt'],\n            'image': ['jpg', 'jpeg', 'png', 'gif'],\n            'video': ['mp4', 'mov', 'avi']\n        }\n\n    def list_documents(self, filter_type: str = None) -&gt; list[dict]:\n        \"\"\"List all documents with metadata\"\"\"\n        docs = []\n        for blob_id in self.storage._get_all_blob_ids():\n            with BlobFile(blob_id, 'r', self.storage) as f:\n                metadata = f.read_json()\n                if metadata:\n                    docs.append({\n                        'id': blob_id,\n                        'name': metadata.get('filename', blob_id),\n                        'type': metadata.get('type', 'document'),\n                        'size': metadata.get('size', 0),\n                        'modified': metadata.get('timestamp', ''),\n                        'preview': metadata.get('preview', '')\n                    })\n        if filter_type:\n            return [d for d in docs if d['type'] == filter_type]\n        return docs\n\n    def save_document(self, file_data: bytes, filename: str, file_type: str) -&gt; str:\n        \"\"\"Save a document with metadata\"\"\"\n        blob_id = self.storage._generate_blob_id()\n        metadata = {\n            'filename': filename,\n            'type': file_type,\n            'size': len(file_data),\n            'timestamp': datetime.now().isoformat(),\n            'preview': self._generate_preview(file_data, file_type)\n        }\n\n        with BlobFile(blob_id, 'w', self.storage) as f:\n            f.write_json(metadata)\n            f.write(file_data)\n        return blob_id\n\n    def delete_document(self, blob_id: str) -&gt; bool:\n        \"\"\"Delete a document\"\"\"\n        try:\n            self.storage.delete_blob(blob_id)\n            return True\n        except Exception as e:\n            logging.error(f\"Delete failed: {str(e)}\")\n            return False\n\n    def search_documents(self, query: str) -&gt; list[dict]:\n        \"\"\"Search documents by filename or content\"\"\"\n        results = []\n        for doc in self.list_documents():\n            if query.lower() in doc['name'].lower() or self._search_in_content(doc['id'], query):\n                results.append(doc)\n        return results\n\n    def _generate_preview(self, data: bytes, file_type: str) -&gt; str:\n        \"\"\"Generate preview based on file type\"\"\"\n        if file_type in self.media_types['image']:\n            return f\"Image preview: {data[:100].hex()}\"\n        elif file_type in self.media_types['video']:\n            return \"Video preview unavailable\"\n        return data[:100].decode('utf-8', errors='ignore')\n\n    def _search_in_content(self, blob_id: str, query: str) -&gt; bool:\n        \"\"\"Search content within documents\"\"\"\n        try:\n            with BlobFile(blob_id, 'r', self.storage) as f:\n                content = f.read().decode('utf-8', errors='ignore')\n                return query.lower() in content.lower()\n        except:\n            return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.DocumentSystem.delete_document","title":"<code>delete_document(blob_id)</code>","text":"<p>Delete a document</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def delete_document(self, blob_id: str) -&gt; bool:\n    \"\"\"Delete a document\"\"\"\n    try:\n        self.storage.delete_blob(blob_id)\n        return True\n    except Exception as e:\n        logging.error(f\"Delete failed: {str(e)}\")\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.DocumentSystem.list_documents","title":"<code>list_documents(filter_type=None)</code>","text":"<p>List all documents with metadata</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def list_documents(self, filter_type: str = None) -&gt; list[dict]:\n    \"\"\"List all documents with metadata\"\"\"\n    docs = []\n    for blob_id in self.storage._get_all_blob_ids():\n        with BlobFile(blob_id, 'r', self.storage) as f:\n            metadata = f.read_json()\n            if metadata:\n                docs.append({\n                    'id': blob_id,\n                    'name': metadata.get('filename', blob_id),\n                    'type': metadata.get('type', 'document'),\n                    'size': metadata.get('size', 0),\n                    'modified': metadata.get('timestamp', ''),\n                    'preview': metadata.get('preview', '')\n                })\n    if filter_type:\n        return [d for d in docs if d['type'] == filter_type]\n    return docs\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.DocumentSystem.save_document","title":"<code>save_document(file_data, filename, file_type)</code>","text":"<p>Save a document with metadata</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def save_document(self, file_data: bytes, filename: str, file_type: str) -&gt; str:\n    \"\"\"Save a document with metadata\"\"\"\n    blob_id = self.storage._generate_blob_id()\n    metadata = {\n        'filename': filename,\n        'type': file_type,\n        'size': len(file_data),\n        'timestamp': datetime.now().isoformat(),\n        'preview': self._generate_preview(file_data, file_type)\n    }\n\n    with BlobFile(blob_id, 'w', self.storage) as f:\n        f.write_json(metadata)\n        f.write(file_data)\n    return blob_id\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.DocumentSystem.search_documents","title":"<code>search_documents(query)</code>","text":"<p>Search documents by filename or content</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def search_documents(self, query: str) -&gt; list[dict]:\n    \"\"\"Search documents by filename or content\"\"\"\n    results = []\n    for doc in self.list_documents():\n        if query.lower() in doc['name'].lower() or self._search_in_content(doc['id'], query):\n            results.append(doc)\n    return results\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant","title":"<code>WhatsAppAssistant</code>  <code>dataclass</code>","text":"Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>@dataclass\nclass WhatsAppAssistant:\n    whc: WhClient\n    isaa: 'Tools'\n    agent: Optional['Agent'] = None\n    credentials: Credentials | None = None\n    state: AssistantState = AssistantState.OFFLINE\n\n    # Service clients\n    gmail_service: Any = None\n    calendar_service: Any = None\n\n    start_time: Any = None\n\n    blob_docs_system: Any = None\n    duration_minutes: int = 20\n    credentials_path: str = \"/root/Toolboxv2/credentials.json\"\n    # Progress messengers\n    progress_messengers: dict[str, 'ProgressMessenger'] = field(default_factory=dict)\n    buttons: dict[str, dict] = field(default_factory=dict)\n    history: FileCache = field(default_factory=FileCache)\n\n    pending_actions: dict[str, dict] = field(default_factory=dict)\n\n\n    def __post_init__(self):\n\n        self.start_time = datetime.now()\n        self.processed_messages = set()\n        self.message_lock = threading.Lock()\n        self.audio_processor = None\n        self.blob_docs_system = DocumentSystem(BlobStorage())\n        self.stt = get_app().run_any(TBEF.AUDIO.STT_GENERATE,\n                                     model=\"openai/whisper-small\",\n                                     row=False, device=1)\n\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n\n        self.load_credentials()\n        self.setup_progress_messengers()\n        self.setup_interaction_buttons()\n        self.history = FileCache(folder=\".data/WhatsAppAssistant\")\n        self.state = AssistantState.ONLINE\n\n    async def generate_authorization_url(self, *a):\n        \"\"\"\n        Generate an authorization URL for user consent\n\n        :return: Authorization URL for the user to click and authorize access\n        \"\"\"\n        from google_auth_oauthlib.flow import Flow\n        # Define the scopes required for Gmail and Calendar\n        SCOPES = [\n            'https://www.googleapis.com/auth/gmail.modify',\n            'https://www.googleapis.com/auth/calendar'\n        ]\n\n        # Create a flow instance to manage the OAuth 2.0 authorization process\n        flow = Flow.from_client_secrets_file(\n            self.credentials_path,\n            scopes=SCOPES,\n            redirect_uri='urn:ietf:wg:oauth:2.0:oob'  # Use 'urn:ietf:wg:oauth:2.0:oob' for desktop apps\n        )\n\n        # Generate the authorization URL\n        authorization_url, _ = flow.authorization_url(\n            access_type='offline',  # Allows obtaining refresh token\n            prompt='consent'  # Ensures user is always prompted for consent\n        )\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {'type': 'auth',\n                                                                              'step': 'awaiting_key'}\n        return {\n            'type': 'quick_reply',\n            'text': f'Url to log in {authorization_url}',\n            'options': {'cancel': '\u274c Cancel Upload'}\n        }\n\n    def complete_authorization(self, message: Message):\n        \"\"\"\n        Complete the authorization process using the authorization code\n\n        :param authorization_code: Authorization code received from Google\n        \"\"\"\n        from google_auth_oauthlib.flow import Flow\n        authorization_code = message.content\n        # Define the scopes required for Gmail and Calendar\n        SCOPES = [\n            'https://www.googleapis.com/auth/gmail.modify',\n            'https://www.googleapis.com/auth/calendar'\n        ]\n\n        # Create a flow instance to manage the OAuth 2.0 authorization process\n        flow = Flow.from_client_secrets_file(\n            self.credentials_path,\n            scopes=SCOPES,\n            redirect_uri='urn:ietf:wg:oauth:2.0:oob'\n        )\n\n        # Exchange the authorization code for credentials\n        flow.fetch_token(code=authorization_code)\n        self.credentials = flow.credentials\n\n        # Save the credentials for future use\n        self.save_credentials()\n\n        # Initialize services\n        self.init_services()\n        return \"Done\"\n\n\n    def save_credentials(self):\n        \"\"\"\n        Save the obtained credentials to a file for future use\n        \"\"\"\n        if not os.path.exists('token'):\n            os.makedirs('token')\n\n        with open('token/google_token.json', 'w') as token_file:\n            token_file.write(self.credentials.to_json())\n\n\n    def load_credentials(self):\n        \"\"\"\n        Load previously saved credentials if available\n\n        :return: Whether credentials were successfully loaded\n        \"\"\"\n        try:\n            self.credentials = Credentials.from_authorized_user_file('token/google_token.json')\n            self.init_services()\n            return True\n        except FileNotFoundError:\n            return False\n\n\n    def init_services(self):\n        \"\"\"\n        Initialize Gmail and Calendar services\n        \"\"\"\n        from googleapiclient.discovery import build\n\n        self.gmail_service = build('gmail', 'v1', credentials=self.credentials)\n        self.calendar_service = build('calendar', 'v3', credentials=self.credentials)\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n\n    def setup_progress_messengers(self):\n        \"\"\"Initialize progress messengers for different types of tasks\"\"\"\n        self.progress_messengers = {\n            'task': self.whc.progress_messenger0,\n            'email': self.whc.progress_messenger1,\n            'calendar': self.whc.progress_messenger2\n        }\n\n    def setup_interaction_buttons(self):\n        \"\"\"Define WhatsApp interaction buttons for different functionalities\"\"\"\n        self.buttons = {\n            'menu': {\n                'header': 'Digital Assistant',\n                'body': 'Please select an option:',\n                'footer': '-- + --',\n                'action': {\n                    'button': 'Menu',\n                    'sections': [\n                        {\n                            'title': 'Main Functions',\n                            'rows': [\n                                {'id': 'agent', 'title': 'Agent Controls', 'description': 'Manage your AI assistant'},\n                                {'id': 'email', 'title': 'Email Management', 'description': 'Handle your emails'},\n                                {'id': 'calendar', 'title': 'Calendar', 'description': 'Manage your schedule'},\n                                {'id': 'docs', 'title': 'Documents', 'description': 'Handle documents'},\n                                {'id': 'system', 'title': 'System', 'description': 'System controls and metrics'}\n                            ]\n                        }\n                    ]\n                }\n            },\n            'agent': self._create_agent_controls_buttons(),\n            'email': self._create_email_controls_buttons(),\n            'calendar': self._create_calendar_controls_buttons(),\n            'docs': self._create_docs_controls_buttons(),\n            'system': self._create_system_controls_buttons()\n        }\n\n    @staticmethod\n    def _create_agent_controls_buttons():\n        return {\n            'header': 'Agent Controls',\n            'body': 'Manage your AI assistant:',\n            'action': {\n                'button': 'Select',\n                'sections': [\n                    {\n                        'title': 'Basic Actions',\n                        'rows': [\n                            {'id': 'agent-task', 'title': 'Agent Task', 'description': 'Run the agent'},\n                            {'id': 'start', 'title': 'Start Agent', 'description': 'Run taskstack in background'},\n                            {'id': 'stop', 'title': 'Stop Agent', 'description': 'Stop taskstack execution'}\n                        ]\n                    },\n                    {\n                        'title': 'Advanced Actions',\n                        'rows': [\n                            {'id': 'system-task', 'title': 'System Task',\n                             'description': 'Run the Isaa Reasoning Agent system'},\n                            {'id': 'tasks', 'title': 'Task Stack', 'description': 'View and manage tasks'},\n                            {'id': 'memory', 'title': 'Clear Memory', 'description': 'Reset agent memory'}\n                        ]\n                    }\n                ]\n            }\n        }\n\n    @staticmethod\n    def _create_email_controls_buttons():\n        return {\n            'header': 'Email Management',\n            'body': 'Handle your emails:',\n            'action': {\n                'button': 'Select',\n                'sections': [\n                    {\n                        'title': 'Basic Actions',\n                        'rows': [\n                            {'id': 'check', 'title': 'Check Emails', 'description': 'View recent emails'},\n                            {'id': 'send', 'title': 'Send Email', 'description': 'Compose new email'},\n                            {'id': 'summary', 'title': 'Get Summary', 'description': 'Summarize emails'}\n                        ]\n                    },\n                    {\n                        'title': 'Advanced Actions',\n                        'rows': [\n                            {'id': 'search', 'title': 'Search', 'description': 'Search emails'}\n                        ]\n                    }\n                ]\n            }\n        }\n\n    @staticmethod\n    def _create_calendar_controls_buttons():\n        return {\n            'header': 'Calendar Management',\n            'body': 'Manage your schedule:',\n            'action': {\n                'button': 'Select',\n                'sections': [\n                    {\n                        'title': 'Basic Actions',\n                        'rows': [\n                            {'id': 'today', 'title': 'Today\\'s Events', 'description': 'View today\\'s schedule'},\n                            {'id': 'add', 'title': 'Add Event', 'description': 'Create new event'},\n                            {'id': 'upcoming', 'title': 'Upcoming', 'description': 'View upcoming events'}\n                        ]\n                    },\n                    {\n                        'title': 'Advanced Actions',\n                        'rows': [\n                            {'id': 'find_slot', 'title': 'Find Time Slot', 'description': 'Find available time'}\n                        ]\n                    }\n                ]\n            }\n        }\n\n    @staticmethod\n    def _create_docs_controls_buttons():\n        return {\n            'header': 'Document Management',\n            'body': 'Handle your documents:',\n            'action': {\n                'button': 'Select',\n                'sections': [\n                    {\n                        'title': 'Basic Actions',\n                        'rows': [\n                            {'id': 'upload', 'title': 'Upload', 'description': 'Add new document'},\n                            {'id': 'list', 'title': 'List Documents', 'description': 'View all documents'},\n                            {'id': 'search', 'title': 'Search', 'description': 'Search documents'}\n                        ]\n                    },\n                    {\n                        'title': 'Advanced Actions',\n                        'rows': [\n                            {'id': 'delete', 'title': 'Delete', 'description': 'Remove document'}\n                        ]\n                    }\n                ]\n            }\n        }\n\n    @staticmethod\n    def _create_system_controls_buttons():\n        return {\n            'header': 'System Controls',\n            'body': 'System management:',\n            'action': {\n                'button': 'Select',\n                'sections': [\n                    {\n                        'title': 'Basic Actions',\n                        'rows': [\n                            {'id': 'status', 'title': 'System Status', 'description': 'View current status'},\n                            {'id': 'restart', 'title': 'Restart', 'description': 'Restart system'},\n                            {'id': 'connect', 'title': 'Connect', 'description': 'Connect to Google Calendar and Email'}\n                        ]\n                    }\n                ]\n            }\n        }\n\n    async def handle_message(self, message: 'Message'):\n        \"\"\"Main message handler for incoming WhatsApp messages\"\"\"\n\n        # Deduplication check\n        with self.message_lock:\n            if message.id in self.processed_messages:\n                return\n            last_ts = time.time()\n            print(last_ts)\n            if len(self.processed_messages) &gt; 0:\n                m_id, last_ts = self.processed_messages.pop()\n                self.processed_messages.add((m_id, last_ts))\n\n            print(\"DUPLICATION P\", message.data.get('entry', [{}])[0].get('changes', [{}])[0].get('value', {}).get('messages', [{}])[0].get('timestamp', 0) , last_ts)\n            if float(message.data.get('entry', [{}])[0].get('changes', [{}])[0].get('value', {}).get('messages', [{}])[0].get('timestamp', 0)) &lt; last_ts - 120:\n                return\n            self.processed_messages.add((message.id, time.perf_counter()))\n\n        # Mark message as read\n        message.mark_as_read()\n\n        # Extract content and type\n        content_type = message.type\n        content = message.content\n\n        print(f\"message.content {content=} {content_type=} {message.data=}\")\n\n        try:\n            if content_type == 'interactive':\n                await self.handle_interactive(message)\n            elif content_type == 'audio':\n                await self.handle_audio_message(message)\n            elif content_type in ['document', 'image', 'video']:\n                response = await self.handle_media_message(message)\n                self.save_reply(message, response)\n            elif content_type == 'text':\n                if content.lower() == \"menu\":\n                    self.whc.messenger.send_button(\n                        recipient_id=self.whc.progress_messenger0.recipient_phone,\n                        button=self.buttons[content.lower()]\n                    )\n                else:\n                    await self.helper_text(message)\n            else:\n                message.reply(\"Unsupported message type\")\n        #except Exception as e:\n        #    logging.error(f\"Message handling error: {str(e)}\")\n        #   message.reply(\"\u274c Error processing request\")\n        finally:\n            # Cleanup old messages (keep 1 hour history)\n            with self.message_lock:\n                self._clean_processed_messages()\n\n    async def helper_text(self, message: 'Message', return_text=False):\n        if not isinstance(message.content, str) and not len(message.content) &gt; 0:\n            content = self.whc.messenger.get_message(message.data)\n            print(f\"contents {content=}, {message.content=}\")\n            message.content = content\n        self.history.set(message.id, message.content)\n        if len(self.pending_actions[self.whc.progress_messenger0.recipient_phone].keys()) != 0:\n            message.reply(\n                f\"Open Interaction : {json.dumps(self.pending_actions[self.whc.progress_messenger0.recipient_phone], indent=2)}\")\n            if self.pending_actions[self.whc.progress_messenger0.recipient_phone].get('type') == 'auth':\n                res = self.complete_authorization(message)\n                self.save_reply(message, res)\n            res = await self.handle_calendar_actions(message)\n            if res:\n                self.save_reply(message, res)\n                return\n            res2 = await self.handle_email_actions(message)\n            if res2:\n                self.save_reply(message, res2)\n                return\n            await self.handle_agent_actions(message)\n            return\n        await self.handle_agent_actions(message)\n\n    async def handle_interactive(self, message: Message):\n        \"\"\"Handle all interactive messages\"\"\"\n        content = self.whc.messenger.get_interactive_response(message.data)\n        if content.get(\"type\") == \"list_reply\":\n            await self.handle_button_interaction(content.get(\"list_reply\"), message)\n        elif content.get(\"type\") == \"button_reply\":\n            print(content)\n\n    async def handle_audio_message(self, message: 'Message'):\n        \"\"\"Process audio messages with STT and TTS\"\"\"\n        # Download audio\n        progress = self.progress_messengers['task']\n        stop_flag = threading.Event()\n        # message_id = progress.send_initial_message(mode=\"loading\")\n        progress.message_id = message.id\n        progress.start_loading_in_background(stop_flag)\n\n        content = self.whc.messenger.get_audio(message.data)\n        audio_file_name = self.whc.messenger.download_media(media_url=self.whc.messenger.query_media_url(media_id=content.get('id')), mime_type='audio/opus', file_path=\".data/temp\")\n        print(f\"audio_file_name {audio_file_name}\")\n        if audio_file_name is None:\n            message.reply(\"Could not process audio file\")\n            stop_flag.set()\n            return\n\n        text = self.stt(audio_file_name)['text']\n        if not text:\n            message.reply(\"Could not process audio\")\n            stop_flag.set()\n            return\n\n        message.reply(\"Transcription :\\n \"+ text)\n        message.content = text\n        agent_res = await self.helper_text(message, return_text=True)\n\n        if agent_res is not None:\n            pass\n\n        stop_flag.set()\n        # Process text and get response\n        # response = await self.process_input(text, message)\n\n        # Convert response to audio\n        #audio_file = self.audio_processor.tts(response)\n        #audio_file = None # TODO\n        #self.whc.messenger.send_audio(\n        #    audio=audio_file,\n        #    recipient_id=self.whc.progress_messenger0.recipient_phone,\n        #)\n\n    async def confirm(self, message: Message):\n        status = self.pending_actions[self.whc.progress_messenger0.recipient_phone]\n        if status.get('type') == \"create_event\":\n            if status.get('step') == \"confirm_envet\":\n                event = self._create_calendar_event(status.get('event_data'))\n                self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n                return f\"\u2705 Event created!\\n{event.get('htmlLink')}\"\n            return \"\u274c\"\n        elif status.get('type') == \"compose_email\":\n            if status.get('step') == \"confirm_email\":\n                # Send email\n                result = self.gmail_service.users().messages().send(\n                    userId='me',\n                    body=self._build_email_draft(status['draft'])\n                ).execute()\n                self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n                return f\"\u2705 Email sent! Message ID: {result['id']}\"\n            return \"\u274c\"\n        return \"\u274c Done\"\n\n    async def cancel(self, *a):\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n        return \"\u2705 cancel Done\"\n\n    async def handle_button_interaction(self, content: dict, message: Message):\n        \"\"\"Handle button click interactions\"\"\"\n        button_id = content['id']\n\n        # First check if it's a main menu button\n        if button_id in self.buttons:\n            self.whc.messenger.send_button(\n                recipient_id=self.whc.progress_messenger0.recipient_phone,\n                button=self.buttons[button_id]\n            )\n            return\n\n        # Handle action buttons\n        action_handlers = {\n            # Agent controls\n            'start': self.start_agent,\n            'stop': self.stop_agent,\n            'tasks': self.show_task_stack,\n            'memory': self.clear_memory,\n            'system-task': self.system_task,\n            'agent-task': self.agent_task,\n\n            # Email controls\n            'check': self.check_emails,\n            'send': self.start_email_compose,\n            'summary': self.email_summary,\n            'search': self.email_search,\n\n            # Calendar controls\n            'today': self.show_today_events,\n            'add': self.start_event_create,\n            'upcoming': self.show_upcoming_events,\n            'find_slot': self.find_time_slot,\n\n            # Document controls\n            'upload': self.start_document_upload,\n            'list': self.list_documents,\n            'search_docs': self.search_documents,\n            'delete': self.delete_document,\n\n            # System controls\n            'status': self.system_status,\n            'restart': self.restart_system,\n            'connect': self.generate_authorization_url,\n\n            'cancel': self.cancel,\n            'confirm': self.confirm,\n        }\n        if button_id in action_handlers:\n            try:\n                # Start progress indicator\n                progress = self.progress_messengers['task']\n                stop_flag = threading.Event()\n                # message_id = progress.send_initial_message(mode=\"loading\")\n                progress.message_id = message.id\n                progress.start_loading_in_background(stop_flag)\n\n                # Execute handler\n\n                result = await action_handlers[button_id](message)\n\n\n                # Send result\n                if isinstance(result, str):\n                    self.save_reply(message, result)\n                elif isinstance(result, dict):  # For structured responses\n                    self.send_structured_response(result)\n\n                stop_flag.set()\n            finally:\n                #except Exception as e:\n                stop_flag.set()\n            #    message.reply(f\"\u274c Error processing {button_id}: {str(e)}\")\n        elif 'event_' in button_id:\n            res = await self.get_event_details(button_id.replace(\"event_\", ''))\n            if isinstance(res, str):\n                self.save_reply(message, res)\n                return\n            for r in res:\n                if isinstance(r, str):\n                    self.save_reply(message, r)\n                else:\n                    self.whc.messenger.send_location(**r)\n\n        elif 'email_' in button_id:\n            res = await self.get_email_details(button_id.replace(\"email_\", ''))\n            self.save_reply(message, res)\n        else:\n            message.reply(\"\u26a0\ufe0f Unknown command\")\n\n    def send_structured_response(self, result: dict):\n        \"\"\"Send complex responses using appropriate WhatsApp features\"\"\"\n        if result['type'] == 'list':\n            self.whc.messenger.send_button(\n                recipient_id=self.whc.progress_messenger0.recipient_phone,\n                button={\n                    'header': result.get('header', ''),\n                    'body': result.get('body', ''),\n                    'footer': result.get('footer', ''),\n                    'action': {\n                        'button': 'Action',\n                        'sections': result['sections']\n                    }\n                }\n            )\n        elif result['type'] == 'quick_reply':\n            self.whc.messenger.send_button(\n                recipient_id=self.whc.progress_messenger0.recipient_phone,\n                button={\n                    'header': \"Quick reply\",\n                    'body': result['text'],\n                    'footer': '',\n                    'action': {'button': 'Action', 'sections': [{\n                        'title': 'View',\n                        'rows': [{'id': k, 'title': v[:23]} for k, v in result['options'].items()]\n                    }]}\n                }\n            )\n\n        elif result['type'] == 'media':\n            if result['media_type'] == 'image':\n                self.whc.messenger.send_image(\n                    image=result['url'],\n                    recipient_id=self.whc.progress_messenger0.recipient_phone,\n                    caption=result.get('caption', '')\n                )\n            elif result['media_type'] == 'document':\n                self.whc.messenger.send_document(\n                    document=result['url'],\n                    recipient_id=self.whc.progress_messenger0.recipient_phone,\n                    caption=result.get('caption', '')\n                )\n\n    async def clear_memory(self, message):\n        self.agent.reset_context()\n        self.agent.taskstack.tasks = []\n        return \"\ud83e\udde0 Memory cleared successfully\"\n\n    async def system_task(self, message):\n        \"\"\"Initiate email search workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n            'type': 'system',\n            'step': 'await_query'\n        }\n        return {\n            'type': 'quick_reply',\n            'text': \"Now prompt the \ud83e\udde0ISAA-System \ud83d\udcdd\",\n            'options': {'cancel': '\u274c Cancel Search'}\n        }\n\n    async def agent_task(self, message):\n        \"\"\"Initiate email search workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n            'type': 'self-agent',\n            'step': 'await_query'\n        }\n        return {\n            'type': 'quick_reply',\n            'text': \"Now prompt the self-agent \ud83d\udcdd\",\n            'options': {'cancel': '\u274c Cancel Search'}\n        }\n\n    async def check_emails(self, message, query=\"\"):\n        \"\"\"Improved email checking with WhatsApp API formatting\"\"\"\n        if not self.gmail_service:\n            return \"\u26a0\ufe0f Gmail service not configured\"\n\n        try:\n            results = self.gmail_service.users().messages().list(\n                userId='me',\n                maxResults=10,\n                labelIds=['INBOX'],\n                q=query\n            ).execute()\n\n            emails = []\n            for msg in results.get('messages', [])[:10]:\n                email_data = self.gmail_service.users().messages().get(\n                    userId='me',\n                    id=msg['id'],\n                    format='metadata'\n                ).execute()\n\n                headers = {h['name']: h['value'] for h in email_data['payload']['headers']}\n                emails.append({\n                    'id': msg['id'],\n                    'from': headers.get('From', 'Unknown'),\n                    'subject': headers.get('Subject', 'No Subject'),\n                    'date': headers.get('Date', 'Unknown'),\n                    'snippet': email_data.get('snippet', ''),\n                    'unread': 'UNREAD' in email_data.get('labelIds', [])\n                })\n\n            return {\n                'type': 'list',\n                'header': '\ud83d\udce8 Recent Emails',\n                'body': 'Tap to view full email',\n                'footer': 'Email Manager',\n                'sections': [{\n                    'title': f\"Inbox ({len(emails)} emails)\",\n                    'rows': [{\n                        'id': f\"email_{email['id']}\",\n                        'title': f\"{'\ud83d\udcec' if email['unread'] else '\ud83d\udced'} {email['subject']}\"[:23],\n                        'description': f\"From: {email['from']}\\n{email['snippet']}\"[:45]\n                    } for email in emails]\n                }]\n            }\n        except Exception as e:\n            return f\"\u26a0\ufe0f Error fetching emails: {str(e)}\"\n\n    async def get_email_details(self, email_id):\n        \"\"\"Retrieve and format full email details\"\"\"\n        if not self.gmail_service:\n            return \"\u26a0\ufe0f Gmail service not configured\"\n\n        try:\n            email_data = self.gmail_service.users().messages().get(\n                userId='me',\n                id=email_id,\n                format='full'\n            ).execute()\n\n            headers = {h['name']: h['value'] for h in email_data['payload']['headers']}\n            body = \"\"\n            for part in email_data.get('payload', {}).get('parts', []):\n                if part['mimeType'] == 'text/plain':\n                    body = base64.urlsafe_b64decode(part['body']['data']).decode('utf-8')\n                    break\n\n            formatted_text = (\n                f\"\ud83d\udce7 *Email Details*\\n\\n\"\n                f\"From: {headers.get('From', 'Unknown')}\\n\"\n                f\"Subject: {headers.get('Subject', 'No Subject')}\\n\"\n                f\"Date: {headers.get('Date', 'Unknown')}\\n\\n\"\n                f\"{body[:15000]}{'...' if len(body) &gt; 15000 else ''}\"\n            )\n            return  self.agent.mini_task(\n                formatted_text , \"system\", \"Summarize the email in bullet points with key details\"\n            )\n        except Exception as e:\n            return f\"\u26a0\ufe0f Error fetching email: {str(e)}\"\n\n    async def email_summary(self, message):\n        \"\"\"Generate AI-powered email summaries\"\"\"\n        try:\n            messages = self.gmail_service.users().messages().list(\n                userId='me',\n                maxResults=3,\n                labelIds=['INBOX']\n            ).execute().get('messages', [])\n\n            email_contents = []\n            for msg in messages[:3]:\n                email_data = self.gmail_service.users().messages().get(\n                    userId='me',\n                    id=msg['id'],\n                    format='full'\n                ).execute()\n                email_contents.append(self._parse_email_content(email_data))\n\n            summary = self.agent.mini_task(\n                \"\\n\\n\".join(email_contents) , \"system\", \"Summarize these emails in bullet points with key details:\"\n            )\n\n            return f\"\ud83d\udccb Email Summary:\\n{summary}\\n\\n*Powered by AI*\"\n        except Exception as e:\n            logging.error(f\"Summary failed: {str(e)}\")\n            return f\"\u274c Could not generate summary: {str(e)}\"\n\n    async def email_search(self, message):\n        \"\"\"Initiate email search workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n            'type': 'email_search',\n            'step': 'await_query'\n        }\n        return {\n            'type': 'quick_reply',\n            'text': \"\ud83d\udd0d What would you like to search for?\",\n            'options': {'cancel': '\u274c Cancel Search'}\n        }\n\n    async def start_email_compose(self, message):\n        \"\"\"Enhanced email composition workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n            'type': 'compose_email',\n            'step': 'subject',\n            'draft': {'attachments': []}\n        }\n        return {\n            'type': 'quick_reply',\n            'text': \"\ud83d\udcdd Let's compose an email\\n\\nSubject:\",\n            'options': {'cancel': '\u274c Cancel Composition'}\n        }\n\n    async def handle_email_actions(self, message):\n        \"\"\"Handle multi-step email workflows\"\"\"\n        user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n\n        if user_state.get('type') == 'compose_email':\n            return await self._handle_email_composition(message, user_state)\n        if user_state.get('type') == 'email_search':\n            return await self.check_emails(message, self.agent.mini_task(\"\"\"Conventire Pezise zu einer googel str only query using : Gmail Suchoperatoren!\n\nBasis-Operatoren:\n- from: Absender\n- to: Empf\u00e4nger\n- subject: Betreff\n- label: Gmail Label\n- has:attachment Anh\u00e4nge\n- newer_than:7d Zeitfilter\n- before: Datum vor\n- after: Datum nach\n\nErweiterte Operatoren:\n- in:inbox\n- in:sent\n- in:spam\n- cc: Kopie\n- bcc: Blindkopie\n- is:unread\n- is:read\n- larger:10M Gr\u00f6\u00dfenfilter\n- smaller:5M\n- filename:pdf Dateityp\n\nProfi-Tipps:\n- Kombinierbar mit UND/ODER\n- Anf\u00fchrungszeichen f\u00fcr exakte Suche\n- Negation mit -\n beispeile : 'Ungelesene Mails letzte Woche': -&gt; 'is:unread newer_than:7d'\n\n\"\"\", \"user\",message.content))\n\n\n        return None\n\n    async def _handle_email_composition(self, message, state):\n        if state['step'] == 'subject':\n            state['draft']['subject'] = message.content\n            state['step'] = 'body'\n            return {\n                'type': 'quick_reply',\n                'text': \"\u270d\ufe0f Email body:\",\n                'options': {'attach': '\ud83d\udcce Add Attachment', 'send': '\ud83d\udce4 Send Now'}\n            }\n\n        elif state['step'] == 'body':\n            if message.content == 'attach':\n                state['step'] = 'attachment'\n                return \"\ud83d\udcce Please send the file you want to attach\"\n\n            state['draft']['body'] = message.content\n            state['step'] = 'confirm_email'\n            return {\n                'type': 'quick_reply',\n                'text': f\"\ud83d\udce7 Ready to send?\\n\\nSubject: {state['draft']['subject']}\\n\\n{state['draft']['body']}\",\n                'options': {'confirm': '\u2705 Send', 'cancel': '\u274c cancel'}\n            }\n\n        elif state['step'] == 'attachment':\n            # Handle attachment upload\n            file_type = message.type\n            if file_type not in ['document', 'image']:\n                return \"\u274c Unsupported file type\"\n\n            media_url = getattr(message, file_type).id\n            media_data = self.whc.messenger.download_media(media_url=self.whc.messenger.query_media_url(media_id=media_url), mime_type=media_url.type, file_path=\".data/temp\")\n            state['draft']['attachments'].append(media_data)\n            state['step'] = 'body'\n            return \"\ud83d\udcce Attachment added! Add more or send the email\"\n\n\n    def _parse_email_content(self, email_data):\n        \"\"\"Extract readable content from email payload\"\"\"\n        parts = email_data.get('payload', {}).get('parts', [])\n        body = \"\"\n        for part in parts:\n            if part['mimeType'] == 'text/plain':\n                body += base64.urlsafe_b64decode(part['body']['data']).decode('utf-8')\n        return f\"Subject: {email_data.get('subject', '')}\\nFrom: {email_data.get('from', '')}\\n\\n{body}\"\n\n    def _build_email_draft(self, draft):\n        \"\"\"Create MIME message from draft data\"\"\"\n        message = MIMEMultipart()\n        message['to'] = draft.get('to', '')\n        message['subject'] = draft['subject']\n        message.attach(MIMEText(draft['body']))\n\n        for attachment in draft['attachments']:\n            part = MIMEBase('application', 'octet-stream')\n            part.set_payload(attachment)\n            encoders.encode_base64(part)\n            part.add_header('Content-Disposition', 'attachment')\n            message.attach(part)\n\n        return {'raw': base64.urlsafe_b64encode(message.as_bytes()).decode()}\n\n    def _get_email_subject(self, msg):\n        headers = msg.get('payload', {}).get('headers', [])\n        return next((h['value'] for h in headers if h['name'] == 'Subject'), 'No Subject')\n\n    def _get_email_sender(self, msg):\n        headers = msg.get('payload', {}).get('headers', [])\n        return next((h['value'] for h in headers if h['name'] == 'From'), 'Unknown Sender')\n\n    def _get_email_snippet(self, msg):\n        return msg.get('snippet', '')[:100] + '...'\n    # Calendar Handlers\n\n    # Calendar Functions\n    def _format_event_time(self, event):\n        \"\"\"Improved time formatting for calendar events\"\"\"\n        start = event['start'].get('dateTime', event['start'].get('date'))\n        end = event['end'].get('dateTime', event['end'].get('date'))\n\n        try:\n            start_dt = parser.parse(start)\n            end_dt = parser.parse(end)\n            if 'T' in start:\n                return f\"{start_dt.strftime('%a %d %b %H:%M')} - {end_dt.strftime('%H:%M')}\"\n            return f\"{start_dt.strftime('%d %b %Y')} (All Day)\"\n        except:\n            return \"Time not specified\"\n\n    async def get_event_details(self, event_id):\n        \"\"\"Retrieve and format calendar event details with location support\"\"\"\n        if not self.calendar_service:\n            return \"\u26a0\ufe0f Calendar service not configured\"\n\n        try:\n            event = self.calendar_service.events().get(\n                calendarId='primary',\n                eventId=event_id\n            ).execute()\n\n            response = [ (\n                    f\"\ud83d\udcc5 *Event Details*\\n\\n\"\n                    f\"Title: {event.get('summary', 'No title')}\\n\"\n                    f\"Time: {self._format_event_time(event)}\\n\"\n                    f\"Location: {event.get('location', 'Not specified')}\\n\\n\"\n                    f\"{event.get('description', 'No description')[:1000]}\"\n                )]\n\n            if 'geo' in event:\n                response.append({\n                    'lat': float(event['geo']['latitude']),\n                    'long': float(event['geo']['longitude']),\n                    'name': event.get('location', 'Event Location'),\n                    'address': event.get('location', ''),\n                    'recipient_id': self.whc.progress_messenger0.recipient_phone\n                })\n            return response\n        except Exception as e:\n            return f\"\u26a0\ufe0f Error fetching event: {str(e)}\"\n\n    async def show_today_events(self, message):\n        \"\"\"Show today's calendar events\"\"\"\n        if not self.calendar_service:\n            message.replay(\"service not online\")\n\n        now = datetime.utcnow().isoformat() + 'Z'\n        end_of_day = (datetime.now() + timedelta(days=1)).replace(\n            hour=0, minute=0, second=0).isoformat() + 'Z'\n\n        events_result = self.calendar_service.events().list(\n            calendarId='primary',\n            timeMin=now,\n            timeMax=end_of_day,\n            singleEvents=True,\n            orderBy='startTime'\n        ).execute()\n\n        events = events_result.get('items', [])\n        return self._format_calendar_response(events, \"Today's Events\")\n\n    # Updated Calendar List Handlers\n    async def show_upcoming_events(self, message):\n        \"\"\"Show upcoming events with interactive support\"\"\"\n        if not self.calendar_service:\n            return \"\u26a0\ufe0f Calendar service not configured\"\n\n        try:\n            now = datetime.utcnow().isoformat() + 'Z'\n            next_week = (datetime.now() + timedelta(days=7)).isoformat() + 'Z'\n\n            events_result = self.calendar_service.events().list(\n                calendarId='primary',\n                timeMin=now,\n                timeMax=next_week,\n                singleEvents=True,\n                orderBy='startTime',\n                maxResults=10\n            ).execute()\n\n            events = events_result.get('items', [])\n            return self._format_calendar_response(events, \"Upcoming Events\")\n        except Exception as e:\n            return f\"\u26a0\ufe0f Error fetching events: {str(e)}\"\n\n    async def start_event_create(self, message):\n        \"\"\"Initiate event creation workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n            'type': 'create_event',\n            'step': 'title',\n            'event_data': {}\n        }\n        return {\n            'type': 'quick_reply',\n            'text': \"Let's create an event! What's the title?\",\n            'options': {'cancel': '\u274c Cancel'}\n        }\n\n    async def find_time_slot(self, message):\n        \"\"\"Find and display the next 5 available time slots with dynamic durations\"\"\"\n        if not self.calendar_service:\n            return \"\u26a0\ufe0f Calendar service not configured\"\n\n        try:\n            # Define the time range for the search (next 24 hours)\n            now = datetime.now(UTC)\n            end_time = now + timedelta(days=1)\n\n            # FreeBusy Request\n            freebusy_request = {\n                \"timeMin\": now.isoformat(),\n                \"timeMax\": end_time.isoformat(),\n                \"items\": [{\"id\": 'primary'}]\n            }\n\n            freebusy_response = self.calendar_service.freebusy().query(body=freebusy_request).execute()\n            busy_slots = freebusy_response['calendars']['primary']['busy']\n\n            # Slot-Berechnung\n            available_slots = self._calculate_efficient_slots(\n                busy_slots,\n                self.duration_minutes\n            )\n\n            # Format the response for WhatsApp\n            return {\n                'type': 'list',\n                'header': \"\u23f0 Available Time Slots\",\n                'body': \"Tap to select a time slot\",\n                'footer': \"Time Slot Finder\",\n                'sections': [{\n                    'title': \"Next 5 Available Slots\",\n                    'rows': [{\n                        'id': f\"slot_{slot['start'].timestamp()}\",\n                        'title': f\"\ud83d\udd52 {slot['start'].strftime('%H:%M')} - {slot['end'].strftime('%H:%M')}\",\n                        'description': f\"Duration: {slot['duration']}\"\n                    } for slot in available_slots[:5]]\n                }]\n            }\n        except Exception as e:\n            return f\"\u26a0\ufe0f Error finding time slots: {str(e)}\"\n\n    def _calculate_efficient_slots(self, busy_slots, duration_minutes):\n        \"\"\"Effiziente Slot-Berechnung\"\"\"\n        available_slots = []\n        current = datetime.now(UTC)\n        end_time = current + timedelta(days=1)\n\n        while current &lt; end_time:\n            slot_end = current + timedelta(minutes=duration_minutes)\n\n            if slot_end &gt; end_time:\n                break\n\n            is_available = all(\n                slot_end &lt;= parser.parse(busy['start']) or\n                current &gt;= parser.parse(busy['end'])\n                for busy in busy_slots\n            )\n\n            if is_available:\n                available_slots.append({\n                    'start': current,\n                    'end': slot_end,\n                    'duration': f\"{duration_minutes} min\"\n                })\n                current = slot_end\n            else:\n                current += timedelta(minutes=15)\n\n        return available_slots\n\n    async def handle_calendar_actions(self, message):\n        \"\"\"Handle calendar-related pending actions\"\"\"\n        user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n\n        if user_state.get('type') == 'create_event':\n            return await self._handle_event_creation(message, user_state)\n\n        return None\n\n    async def _handle_event_creation(self, message, state):\n        step = state['step']\n        event_data = state['event_data']\n\n        if step == 'title':\n            event_data['summary'] = message.content\n            state['step'] = 'start_time'\n            return \"\ud83d\udcc5 When should it start? (e.g., 'tomorrow 2pm' or '2024-03-20 14:30')\"\n\n        elif step == 'start_time':\n            event_data['start'] = self._parse_time(message.content)\n            state['step'] = 'end_time'\n            return \"\u23f0 When should it end? (e.g., '3pm' or '2024-03-20 15:30')\"\n\n        elif step == 'end_time':\n            event_data['end'] = self._parse_time(message.content, reference=event_data['start'])\n            state['step'] = 'description'\n            return \"\ud83d\udcdd Add a description (or type 'skip')\"\n\n        elif step == 'description':\n            if message.content.lower() != 'skip':\n                event_data['description'] = message.content\n            state['step'] = 'confirm_envet'\n            return self._create_confirmation_message(event_data)\n\n    def _format_calendar_response(self, events, title):\n        \"\"\"Enhanced calendar formatting with interactive support\"\"\"\n        if not events:\n            return f\"\ud83d\udcc5 No {title.lower()} found\"\n\n        return {\n            'type': 'list',\n            'header': title,\n            'body': \"Tap to view event details\",\n            \"footer\": \"-- Calendar --\",\n            'sections': [{\n                'title': f\"{len(events)} Events\",\n                'rows': [{\n                    'id': f\"event_{event['id']}\",\n                    'title': f\"\ud83d\udcc5 {event['summary']}\"[:23],\n                    'description': self._format_event_time(event)[:45]\n                } for event in events[:5]]\n            }]\n        }\n\n    def _parse_iso_to_readable(self, iso_str):\n        \"\"\"Convert ISO datetime to readable format\"\"\"\n        dt = datetime.fromisoformat(iso_str.replace('Z', '+00:00'))\n        return dt.strftime(\"%a %d %b %Y %H:%M\")\n\n    def _parse_time(self, time_str, reference=None):\n        \"\"\"\n        Konvertiert nat\u00fcrliche Sprache zu pr\u00e4ziser Datetime\n\n        Unterst\u00fctzt:\n        - 'heute'\n        - 'morgen'\n        - 'in einer woche'\n        - '10 uhr'\n        - '10pm'\n        - 'n\u00e4chsten montag'\n        \"\"\"\n        if reference is None:\n            reference = datetime.now()\n\n        try:\n            import dateparser\n\n            # Dateparser f\u00fcr flexibel Zeitparsing\n            parsed_time = dateparser.parse(\n                time_str,\n                settings={\n                    'PREFER_DATES_FROM': 'future',\n                    'RELATIVE_BASE': reference,\n                    'TIMEZONE': 'Europe/Berlin'\n                }\n            )\n\n            if parsed_time is None:\n                # Fallback auf dateutil wenn dateparser scheitert\n                parsed_time = parser .parse(time_str, fuzzy=True, default=reference)\n\n            return parsed_time\n\n        except Exception as e:\n            print(f\"Zeitparsing-Fehler: {e}\")\n            return reference\n\n    def _calculate_free_slots(self, start, end, busy_slots):\n        \"\"\"Calculate free time slots between busy periods\"\"\"\n        # Implementation would calculate available windows\n        return [{\n            'start': \"09:00\",\n            'end': \"11:00\",\n            'duration': \"2 hours\"\n        }]\n\n    def _create_confirmation_message(self, event_data):\n        \"\"\"Create event confirmation message\"\"\"\n        details = [\n            f\"\ud83d\udccc Title: {event_data['summary']}\",\n            f\"\ud83d\udd52 Start: {self._parse_iso_to_readable(event_data['start'])}\",\n            f\"\u23f0 End: {self._parse_iso_to_readable(event_data['end'])}\",\n            f\"\ud83d\udcdd Description: {event_data.get('description', 'None')}\"\n        ]\n        return {\n            'type': 'quick_reply',\n            'text': \"\\n\".join(details),\n            'options': {'confirm': '\u2705 Confirm', 'cancel': '\u274c Cancel'}\n        }\n\n    def _create_calendar_event(self, event_data):\n        \"\"\"Create event through Calendar API\"\"\"\n        event = {\n            'summary': event_data['summary'],\n            'start': {'dateTime': event_data['start']},\n            'end': {'dateTime': event_data['end']},\n        }\n        if 'description' in event_data:\n            event['description'] = event_data['description']\n\n        return self.calendar_service.events().insert(\n            calendarId='primary',\n            body=event\n        ).execute()\n\n    async def system_status(self, message):\n        o = (datetime.now() - self.start_time)\n        o.microseconds = 0\n        status = {\n            \"\ud83e\udd16 Agent\": \"Online\" if self.agent else \"Offline\",\n            \"\ud83d\udce7 Email\": \"Connected\" if self.gmail_service else \"Disconnected\",\n            \"\ud83d\udcc5 Calendar\": \"Connected\" if self.calendar_service else \"Disconnected\",\n            \"\ud83d\udcc4 Documents\": \"Connected\" if self.blob_docs_system else \"Disconnected\",\n            \"\u23f3 Uptime\": f\"{str(o.isoformat())}\"\n        }\n        return \"\\n\".join([f\"{k}: {v}\" for k, v in status.items()])\n\n    async def restart_system(self, message):\n        message.reply(\"\ud83d\udd04 System restart initiated...\")\n        time.sleep(1)\n        await self.clear_memory(message)\n        time.sleep(1)\n        return  \"\u2705 System restarted\"\n\n    # Updated document handlers\n    async def list_documents(self, message, filter_type=None):\n        docs = self.blob_docs_system.list_documents(filter_type)\n        if len(docs) == 0:\n            return \"No docs found\"\n        else:\n            return str(docs)\n        return {\n            'type': 'list',\n            'body': 'Stored Documents',\n            'action': {\n                'sections': [{\n                    'title': 'Your Documents',\n                    'rows': [{\n                        'id': doc['id'],\n                        'title': f\"{self._get_icon(doc['type'])} {doc['name']}\"[:23],\n                        'description': f\"{doc['type'].title()} | {self._format_size(doc['size'])} | {doc['modified']}\"[:29]\n                    } for doc in docs[:10]]\n                }]}\n        }\n\n    async def start_document_upload(self, message):\n        \"\"\"Initiate document upload workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {'type': 'document', 'step': 'awaiting_file'}\n        return {\n            'type': 'quick_reply',\n            'text': '\ud83d\udce4 Send me the file you want to upload',\n            'options': {'cancel': '\u274c Cancel Upload'}\n        }\n\n    async def search_documents(self, message):\n        \"\"\"Initiate document search workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {'type': 'search', 'step': 'awaiting_query'}\n        return {\n            'type': 'quick_reply',\n            'text': '\ud83d\udd0d What are you looking for?',\n            'options': {'cancel': '\u274c Cancel Search'}\n        }\n\n    async def handle_media_message(self, message: 'Message'):\n        \"\"\"Handle document/image/video uploads\"\"\"\n        user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n\n        if user_state.get('step') == 'awaiting_file':\n            file_type = message.type\n            if file_type not in ['document', 'image', 'video']:\n                return \"Unsupported file type\"\n\n            try:\n                # Download media\n                #media_url = message.document.url if hasattr(message, 'document') else \\\n                #    message.image.url if hasattr(message, 'image') else \\\n                #        message.video.url\n                if file_type =='video':\n                    content = self.whc.messenger.get_video(message.data)\n                if file_type =='image':\n                    content = self.whc.messenger.get_image(message.data)\n                if file_type =='document':\n                    content = self.whc.messenger.get_document(message.data)\n                print(\"Media content:\", content)\n                media_data = self.whc.messenger.download_media(media_url=self.whc.messenger.query_media_url(media_id=content.get('id')),  mime_type=content.get('mime_type'), file_path='.data/temp')\n                print(\"Media media_data:\", media_data)\n                # Save to blob storage\n                filename = f\"file_{file_type}_{datetime.now().isoformat()}_{content.get('sha256', '')}\"\n                blob_id = self.blob_docs_system.save_document(\n                    open(media_data, 'rb').read(),\n                    filename=filename,\n                    file_type=file_type\n                )\n\n                self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n                return f\"\u2705 File uploaded successfully!\\nID: {blob_id}\"\n\n            except Exception as e:\n                logging.error(f\"Upload failed: {str(e)}\")\n                return f\"\u274c Failed to upload file Error : {str(e)}\"\n\n        return \"No pending uploads\"\n\n    async def delete_document(self, message):\n        \"\"\"Delete document workflow\"\"\"\n        docs = self.blob_docs_system.list_documents()\n        return {\n            'type': 'quick_reply',\n            'text': 'Select document to delete:',\n            'options': {doc['id']: doc['name'] for doc in docs[:5]},\n            'handler': self._confirm_delete\n        }\n\n    async def _confirm_delete(self, doc_id, message):\n        \"\"\"Confirm deletion workflow\"\"\"\n        doc = next((d for d in self.blob_docs_system.list_documents() if d['id'] == doc_id), None)\n        if not doc:\n            return \"Document not found\"\n\n        if self.blob_docs_system.delete_document(doc_id):\n            return f\"\u2705 {doc['name']} deleted successfully\"\n        return \"\u274c Failed to delete document\"\n\n    # Helper methods\n    def _get_icon(self, file_type: str) -&gt; str:\n        icons = {\n            'document': '\ud83d\udcc4',\n            'image': '\ud83d\uddbc\ufe0f',\n            'video': '\ud83c\udfa5'\n        }\n        return icons.get(file_type, '\ud83d\udcc1')\n\n    def _format_size(self, size: int) -&gt; str:\n        if size &lt; 1024:\n            return f\"{size}B\"\n        elif size &lt; 1024 ** 2:\n            return f\"{size / 1024:.1f}KB\"\n        elif size &lt; 1024 ** 3:\n            return f\"{size / (1024 ** 2):.1f}MB\"\n        return f\"{size / (1024 ** 3):.1f}GB\"\n\n    # Utility Methods\n\n    def _clean_processed_messages(self):\n        \"\"\"Clean old messages from processed cache\"\"\"\n        now = time.time()\n        self.processed_messages = {\n            msg_id for msg_id, timestamp in self.processed_messages\n            if now - timestamp &lt; 3600  # 1 hour retention\n        }\n\n    def send_email(self, to, subject, body):\n        \"\"\"Actual email sending function to be called by agent\"\"\"\n        if not self.gmail_service:\n            return False\n\n        message = MIMEText(body)\n        message['to'] = to\n        message['subject'] = subject\n\n        encoded_message = base64.urlsafe_b64encode(message.as_bytes()).decode()\n        self.gmail_service.users().messages().send(\n            userId='me',\n            body={'raw': encoded_message}\n        ).execute()\n        return True\n\n    async def start_agent(self, *a):\n        \"\"\"Start the agent in background mode\"\"\"\n        if self.agent:\n            self.agent.run_in_background()\n            return True\n        return False\n\n    async def stop_agent(self, *b):\n        \"\"\"Stop the currently running agent\"\"\"\n        if self.agent:\n            self.agent.stop()\n            return True\n        return False\n\n    async def show_task_stack(self, *a):\n        \"\"\"Display current task stack\"\"\"\n        if self.agent and len(self.agent.taskstack.tasks) &gt; 0:\n            tasks = self.agent.taskstack.tasks\n            return self.agent.mini_task(\"\\n\".join([f\"Task {t.id}: {t.description}\" for t in tasks]), \"system\", \"Format to nice and clean whatsapp format\")\n        return \"No tasks in stack\"\n\n    def run(self):\n        \"\"\"Start the WhatsApp assistant\"\"\"\n        try:\n            self.state = AssistantState.ONLINE\n            # Send welcome message\n\n            mas = self.whc.messenger.create_message(\n                content=\"Digital Assistant is online! Send /help for available commands.\",to=self.whc.progress_messenger0.recipient_phone,\n            ).send(sender=0)\n            mas_id = mas.get(\"messages\", [{}])[0].get(\"id\")\n            print(mas_id)\n\n        except Exception as e:\n            logging.error(f\"Assistant error: {str(e)}\")\n            self.state = AssistantState.OFFLINE\n            raise\n\n    async def handle_agent_actions(self, message):\n        user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n        def helper():\n\n            stop_flag = threading.Event()\n            try:\n                progress = self.progress_messengers['task']\n                # message_id = progress.send_initial_message(mode=\"loading\")\n                progress.message_id = message.id\n                progress.start_loading_in_background(stop_flag)\n                res = message.content\n                print(message.data.get('entry', [{}])[0].get('changes', [{}])[0].get('value', {}).get('messages', [{}])[0].get(\n                    'context'))\n                if context := message.data.get('entry', [{}])[0].get('changes', [{}])[0].get('value', {}).get('messages', [{}])[0].get(\n                    'context'):\n                    context_str = f\"Context : source {'USER' if context.get('from') in self.whc.progress_messenger0.recipient_phone else 'AGENT'}\"\n                    cd = self.history.get(context.get('id'))\n                    context_str += \"\\n\" + (cd if cd is not None else \"The ref Message is not in the history\")\n                    res += \"\\n\" + context_str\n                if user_state.get('type') == 'system':\n                    res = self.isaa.run(res)\n                    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n                elif user_state.get('type') == 'self-agent':\n                    res = self.agent.run(res)\n                    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n                self.agent.mode = LLMMode(\n                    name=\"Chatter\",\n                    description=\"whatsapp Chat LLM\",\n                    system_msg=\"Response precise and short style using whatsapp syntax!\",\n                    post_msg=None\n                )\n                response = self.agent.mini_task(res, \"user\", persist=True)\n                self.save_reply(message, response)\n            except Exception as e:\n                stop_flag.set()\n                message.reply(\"\u274c Error in agent \"+str(e))\n            finally:\n                self.agent.mode = None\n                stop_flag.set()\n        threading.Thread(target=helper, daemon=True).start()\n\n    def save_reply(self, message, content):\n        res = message.reply(content)\n        res_id = res.get(\"messages\", [{}])[0].get(\"id\")\n        if res_id is not None:\n            self.history.set(res_id, content)\n        else:\n            print(f\"No ID to add to history: {res}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.agent_task","title":"<code>agent_task(message)</code>  <code>async</code>","text":"<p>Initiate email search workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def agent_task(self, message):\n    \"\"\"Initiate email search workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n        'type': 'self-agent',\n        'step': 'await_query'\n    }\n    return {\n        'type': 'quick_reply',\n        'text': \"Now prompt the self-agent \ud83d\udcdd\",\n        'options': {'cancel': '\u274c Cancel Search'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.check_emails","title":"<code>check_emails(message, query='')</code>  <code>async</code>","text":"<p>Improved email checking with WhatsApp API formatting</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def check_emails(self, message, query=\"\"):\n    \"\"\"Improved email checking with WhatsApp API formatting\"\"\"\n    if not self.gmail_service:\n        return \"\u26a0\ufe0f Gmail service not configured\"\n\n    try:\n        results = self.gmail_service.users().messages().list(\n            userId='me',\n            maxResults=10,\n            labelIds=['INBOX'],\n            q=query\n        ).execute()\n\n        emails = []\n        for msg in results.get('messages', [])[:10]:\n            email_data = self.gmail_service.users().messages().get(\n                userId='me',\n                id=msg['id'],\n                format='metadata'\n            ).execute()\n\n            headers = {h['name']: h['value'] for h in email_data['payload']['headers']}\n            emails.append({\n                'id': msg['id'],\n                'from': headers.get('From', 'Unknown'),\n                'subject': headers.get('Subject', 'No Subject'),\n                'date': headers.get('Date', 'Unknown'),\n                'snippet': email_data.get('snippet', ''),\n                'unread': 'UNREAD' in email_data.get('labelIds', [])\n            })\n\n        return {\n            'type': 'list',\n            'header': '\ud83d\udce8 Recent Emails',\n            'body': 'Tap to view full email',\n            'footer': 'Email Manager',\n            'sections': [{\n                'title': f\"Inbox ({len(emails)} emails)\",\n                'rows': [{\n                    'id': f\"email_{email['id']}\",\n                    'title': f\"{'\ud83d\udcec' if email['unread'] else '\ud83d\udced'} {email['subject']}\"[:23],\n                    'description': f\"From: {email['from']}\\n{email['snippet']}\"[:45]\n                } for email in emails]\n            }]\n        }\n    except Exception as e:\n        return f\"\u26a0\ufe0f Error fetching emails: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.complete_authorization","title":"<code>complete_authorization(message)</code>","text":"<p>Complete the authorization process using the authorization code</p> <p>:param authorization_code: Authorization code received from Google</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def complete_authorization(self, message: Message):\n    \"\"\"\n    Complete the authorization process using the authorization code\n\n    :param authorization_code: Authorization code received from Google\n    \"\"\"\n    from google_auth_oauthlib.flow import Flow\n    authorization_code = message.content\n    # Define the scopes required for Gmail and Calendar\n    SCOPES = [\n        'https://www.googleapis.com/auth/gmail.modify',\n        'https://www.googleapis.com/auth/calendar'\n    ]\n\n    # Create a flow instance to manage the OAuth 2.0 authorization process\n    flow = Flow.from_client_secrets_file(\n        self.credentials_path,\n        scopes=SCOPES,\n        redirect_uri='urn:ietf:wg:oauth:2.0:oob'\n    )\n\n    # Exchange the authorization code for credentials\n    flow.fetch_token(code=authorization_code)\n    self.credentials = flow.credentials\n\n    # Save the credentials for future use\n    self.save_credentials()\n\n    # Initialize services\n    self.init_services()\n    return \"Done\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.delete_document","title":"<code>delete_document(message)</code>  <code>async</code>","text":"<p>Delete document workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def delete_document(self, message):\n    \"\"\"Delete document workflow\"\"\"\n    docs = self.blob_docs_system.list_documents()\n    return {\n        'type': 'quick_reply',\n        'text': 'Select document to delete:',\n        'options': {doc['id']: doc['name'] for doc in docs[:5]},\n        'handler': self._confirm_delete\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.email_search","title":"<code>email_search(message)</code>  <code>async</code>","text":"<p>Initiate email search workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def email_search(self, message):\n    \"\"\"Initiate email search workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n        'type': 'email_search',\n        'step': 'await_query'\n    }\n    return {\n        'type': 'quick_reply',\n        'text': \"\ud83d\udd0d What would you like to search for?\",\n        'options': {'cancel': '\u274c Cancel Search'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.email_summary","title":"<code>email_summary(message)</code>  <code>async</code>","text":"<p>Generate AI-powered email summaries</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def email_summary(self, message):\n    \"\"\"Generate AI-powered email summaries\"\"\"\n    try:\n        messages = self.gmail_service.users().messages().list(\n            userId='me',\n            maxResults=3,\n            labelIds=['INBOX']\n        ).execute().get('messages', [])\n\n        email_contents = []\n        for msg in messages[:3]:\n            email_data = self.gmail_service.users().messages().get(\n                userId='me',\n                id=msg['id'],\n                format='full'\n            ).execute()\n            email_contents.append(self._parse_email_content(email_data))\n\n        summary = self.agent.mini_task(\n            \"\\n\\n\".join(email_contents) , \"system\", \"Summarize these emails in bullet points with key details:\"\n        )\n\n        return f\"\ud83d\udccb Email Summary:\\n{summary}\\n\\n*Powered by AI*\"\n    except Exception as e:\n        logging.error(f\"Summary failed: {str(e)}\")\n        return f\"\u274c Could not generate summary: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.find_time_slot","title":"<code>find_time_slot(message)</code>  <code>async</code>","text":"<p>Find and display the next 5 available time slots with dynamic durations</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def find_time_slot(self, message):\n    \"\"\"Find and display the next 5 available time slots with dynamic durations\"\"\"\n    if not self.calendar_service:\n        return \"\u26a0\ufe0f Calendar service not configured\"\n\n    try:\n        # Define the time range for the search (next 24 hours)\n        now = datetime.now(UTC)\n        end_time = now + timedelta(days=1)\n\n        # FreeBusy Request\n        freebusy_request = {\n            \"timeMin\": now.isoformat(),\n            \"timeMax\": end_time.isoformat(),\n            \"items\": [{\"id\": 'primary'}]\n        }\n\n        freebusy_response = self.calendar_service.freebusy().query(body=freebusy_request).execute()\n        busy_slots = freebusy_response['calendars']['primary']['busy']\n\n        # Slot-Berechnung\n        available_slots = self._calculate_efficient_slots(\n            busy_slots,\n            self.duration_minutes\n        )\n\n        # Format the response for WhatsApp\n        return {\n            'type': 'list',\n            'header': \"\u23f0 Available Time Slots\",\n            'body': \"Tap to select a time slot\",\n            'footer': \"Time Slot Finder\",\n            'sections': [{\n                'title': \"Next 5 Available Slots\",\n                'rows': [{\n                    'id': f\"slot_{slot['start'].timestamp()}\",\n                    'title': f\"\ud83d\udd52 {slot['start'].strftime('%H:%M')} - {slot['end'].strftime('%H:%M')}\",\n                    'description': f\"Duration: {slot['duration']}\"\n                } for slot in available_slots[:5]]\n            }]\n        }\n    except Exception as e:\n        return f\"\u26a0\ufe0f Error finding time slots: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.generate_authorization_url","title":"<code>generate_authorization_url(*a)</code>  <code>async</code>","text":"<p>Generate an authorization URL for user consent</p> <p>:return: Authorization URL for the user to click and authorize access</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def generate_authorization_url(self, *a):\n    \"\"\"\n    Generate an authorization URL for user consent\n\n    :return: Authorization URL for the user to click and authorize access\n    \"\"\"\n    from google_auth_oauthlib.flow import Flow\n    # Define the scopes required for Gmail and Calendar\n    SCOPES = [\n        'https://www.googleapis.com/auth/gmail.modify',\n        'https://www.googleapis.com/auth/calendar'\n    ]\n\n    # Create a flow instance to manage the OAuth 2.0 authorization process\n    flow = Flow.from_client_secrets_file(\n        self.credentials_path,\n        scopes=SCOPES,\n        redirect_uri='urn:ietf:wg:oauth:2.0:oob'  # Use 'urn:ietf:wg:oauth:2.0:oob' for desktop apps\n    )\n\n    # Generate the authorization URL\n    authorization_url, _ = flow.authorization_url(\n        access_type='offline',  # Allows obtaining refresh token\n        prompt='consent'  # Ensures user is always prompted for consent\n    )\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {'type': 'auth',\n                                                                          'step': 'awaiting_key'}\n    return {\n        'type': 'quick_reply',\n        'text': f'Url to log in {authorization_url}',\n        'options': {'cancel': '\u274c Cancel Upload'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.get_email_details","title":"<code>get_email_details(email_id)</code>  <code>async</code>","text":"<p>Retrieve and format full email details</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def get_email_details(self, email_id):\n    \"\"\"Retrieve and format full email details\"\"\"\n    if not self.gmail_service:\n        return \"\u26a0\ufe0f Gmail service not configured\"\n\n    try:\n        email_data = self.gmail_service.users().messages().get(\n            userId='me',\n            id=email_id,\n            format='full'\n        ).execute()\n\n        headers = {h['name']: h['value'] for h in email_data['payload']['headers']}\n        body = \"\"\n        for part in email_data.get('payload', {}).get('parts', []):\n            if part['mimeType'] == 'text/plain':\n                body = base64.urlsafe_b64decode(part['body']['data']).decode('utf-8')\n                break\n\n        formatted_text = (\n            f\"\ud83d\udce7 *Email Details*\\n\\n\"\n            f\"From: {headers.get('From', 'Unknown')}\\n\"\n            f\"Subject: {headers.get('Subject', 'No Subject')}\\n\"\n            f\"Date: {headers.get('Date', 'Unknown')}\\n\\n\"\n            f\"{body[:15000]}{'...' if len(body) &gt; 15000 else ''}\"\n        )\n        return  self.agent.mini_task(\n            formatted_text , \"system\", \"Summarize the email in bullet points with key details\"\n        )\n    except Exception as e:\n        return f\"\u26a0\ufe0f Error fetching email: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.get_event_details","title":"<code>get_event_details(event_id)</code>  <code>async</code>","text":"<p>Retrieve and format calendar event details with location support</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def get_event_details(self, event_id):\n    \"\"\"Retrieve and format calendar event details with location support\"\"\"\n    if not self.calendar_service:\n        return \"\u26a0\ufe0f Calendar service not configured\"\n\n    try:\n        event = self.calendar_service.events().get(\n            calendarId='primary',\n            eventId=event_id\n        ).execute()\n\n        response = [ (\n                f\"\ud83d\udcc5 *Event Details*\\n\\n\"\n                f\"Title: {event.get('summary', 'No title')}\\n\"\n                f\"Time: {self._format_event_time(event)}\\n\"\n                f\"Location: {event.get('location', 'Not specified')}\\n\\n\"\n                f\"{event.get('description', 'No description')[:1000]}\"\n            )]\n\n        if 'geo' in event:\n            response.append({\n                'lat': float(event['geo']['latitude']),\n                'long': float(event['geo']['longitude']),\n                'name': event.get('location', 'Event Location'),\n                'address': event.get('location', ''),\n                'recipient_id': self.whc.progress_messenger0.recipient_phone\n            })\n        return response\n    except Exception as e:\n        return f\"\u26a0\ufe0f Error fetching event: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_audio_message","title":"<code>handle_audio_message(message)</code>  <code>async</code>","text":"<p>Process audio messages with STT and TTS</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def handle_audio_message(self, message: 'Message'):\n    \"\"\"Process audio messages with STT and TTS\"\"\"\n    # Download audio\n    progress = self.progress_messengers['task']\n    stop_flag = threading.Event()\n    # message_id = progress.send_initial_message(mode=\"loading\")\n    progress.message_id = message.id\n    progress.start_loading_in_background(stop_flag)\n\n    content = self.whc.messenger.get_audio(message.data)\n    audio_file_name = self.whc.messenger.download_media(media_url=self.whc.messenger.query_media_url(media_id=content.get('id')), mime_type='audio/opus', file_path=\".data/temp\")\n    print(f\"audio_file_name {audio_file_name}\")\n    if audio_file_name is None:\n        message.reply(\"Could not process audio file\")\n        stop_flag.set()\n        return\n\n    text = self.stt(audio_file_name)['text']\n    if not text:\n        message.reply(\"Could not process audio\")\n        stop_flag.set()\n        return\n\n    message.reply(\"Transcription :\\n \"+ text)\n    message.content = text\n    agent_res = await self.helper_text(message, return_text=True)\n\n    if agent_res is not None:\n        pass\n\n    stop_flag.set()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_button_interaction","title":"<code>handle_button_interaction(content, message)</code>  <code>async</code>","text":"<p>Handle button click interactions</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def handle_button_interaction(self, content: dict, message: Message):\n    \"\"\"Handle button click interactions\"\"\"\n    button_id = content['id']\n\n    # First check if it's a main menu button\n    if button_id in self.buttons:\n        self.whc.messenger.send_button(\n            recipient_id=self.whc.progress_messenger0.recipient_phone,\n            button=self.buttons[button_id]\n        )\n        return\n\n    # Handle action buttons\n    action_handlers = {\n        # Agent controls\n        'start': self.start_agent,\n        'stop': self.stop_agent,\n        'tasks': self.show_task_stack,\n        'memory': self.clear_memory,\n        'system-task': self.system_task,\n        'agent-task': self.agent_task,\n\n        # Email controls\n        'check': self.check_emails,\n        'send': self.start_email_compose,\n        'summary': self.email_summary,\n        'search': self.email_search,\n\n        # Calendar controls\n        'today': self.show_today_events,\n        'add': self.start_event_create,\n        'upcoming': self.show_upcoming_events,\n        'find_slot': self.find_time_slot,\n\n        # Document controls\n        'upload': self.start_document_upload,\n        'list': self.list_documents,\n        'search_docs': self.search_documents,\n        'delete': self.delete_document,\n\n        # System controls\n        'status': self.system_status,\n        'restart': self.restart_system,\n        'connect': self.generate_authorization_url,\n\n        'cancel': self.cancel,\n        'confirm': self.confirm,\n    }\n    if button_id in action_handlers:\n        try:\n            # Start progress indicator\n            progress = self.progress_messengers['task']\n            stop_flag = threading.Event()\n            # message_id = progress.send_initial_message(mode=\"loading\")\n            progress.message_id = message.id\n            progress.start_loading_in_background(stop_flag)\n\n            # Execute handler\n\n            result = await action_handlers[button_id](message)\n\n\n            # Send result\n            if isinstance(result, str):\n                self.save_reply(message, result)\n            elif isinstance(result, dict):  # For structured responses\n                self.send_structured_response(result)\n\n            stop_flag.set()\n        finally:\n            #except Exception as e:\n            stop_flag.set()\n        #    message.reply(f\"\u274c Error processing {button_id}: {str(e)}\")\n    elif 'event_' in button_id:\n        res = await self.get_event_details(button_id.replace(\"event_\", ''))\n        if isinstance(res, str):\n            self.save_reply(message, res)\n            return\n        for r in res:\n            if isinstance(r, str):\n                self.save_reply(message, r)\n            else:\n                self.whc.messenger.send_location(**r)\n\n    elif 'email_' in button_id:\n        res = await self.get_email_details(button_id.replace(\"email_\", ''))\n        self.save_reply(message, res)\n    else:\n        message.reply(\"\u26a0\ufe0f Unknown command\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_calendar_actions","title":"<code>handle_calendar_actions(message)</code>  <code>async</code>","text":"<p>Handle calendar-related pending actions</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def handle_calendar_actions(self, message):\n    \"\"\"Handle calendar-related pending actions\"\"\"\n    user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n\n    if user_state.get('type') == 'create_event':\n        return await self._handle_event_creation(message, user_state)\n\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_email_actions","title":"<code>handle_email_actions(message)</code>  <code>async</code>","text":"<p>Handle multi-step email workflows</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>    async def handle_email_actions(self, message):\n        \"\"\"Handle multi-step email workflows\"\"\"\n        user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n\n        if user_state.get('type') == 'compose_email':\n            return await self._handle_email_composition(message, user_state)\n        if user_state.get('type') == 'email_search':\n            return await self.check_emails(message, self.agent.mini_task(\"\"\"Conventire Pezise zu einer googel str only query using : Gmail Suchoperatoren!\n\nBasis-Operatoren:\n- from: Absender\n- to: Empf\u00e4nger\n- subject: Betreff\n- label: Gmail Label\n- has:attachment Anh\u00e4nge\n- newer_than:7d Zeitfilter\n- before: Datum vor\n- after: Datum nach\n\nErweiterte Operatoren:\n- in:inbox\n- in:sent\n- in:spam\n- cc: Kopie\n- bcc: Blindkopie\n- is:unread\n- is:read\n- larger:10M Gr\u00f6\u00dfenfilter\n- smaller:5M\n- filename:pdf Dateityp\n\nProfi-Tipps:\n- Kombinierbar mit UND/ODER\n- Anf\u00fchrungszeichen f\u00fcr exakte Suche\n- Negation mit -\n beispeile : 'Ungelesene Mails letzte Woche': -&gt; 'is:unread newer_than:7d'\n\n\"\"\", \"user\",message.content))\n\n\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_interactive","title":"<code>handle_interactive(message)</code>  <code>async</code>","text":"<p>Handle all interactive messages</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def handle_interactive(self, message: Message):\n    \"\"\"Handle all interactive messages\"\"\"\n    content = self.whc.messenger.get_interactive_response(message.data)\n    if content.get(\"type\") == \"list_reply\":\n        await self.handle_button_interaction(content.get(\"list_reply\"), message)\n    elif content.get(\"type\") == \"button_reply\":\n        print(content)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_media_message","title":"<code>handle_media_message(message)</code>  <code>async</code>","text":"<p>Handle document/image/video uploads</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def handle_media_message(self, message: 'Message'):\n    \"\"\"Handle document/image/video uploads\"\"\"\n    user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n\n    if user_state.get('step') == 'awaiting_file':\n        file_type = message.type\n        if file_type not in ['document', 'image', 'video']:\n            return \"Unsupported file type\"\n\n        try:\n            # Download media\n            #media_url = message.document.url if hasattr(message, 'document') else \\\n            #    message.image.url if hasattr(message, 'image') else \\\n            #        message.video.url\n            if file_type =='video':\n                content = self.whc.messenger.get_video(message.data)\n            if file_type =='image':\n                content = self.whc.messenger.get_image(message.data)\n            if file_type =='document':\n                content = self.whc.messenger.get_document(message.data)\n            print(\"Media content:\", content)\n            media_data = self.whc.messenger.download_media(media_url=self.whc.messenger.query_media_url(media_id=content.get('id')),  mime_type=content.get('mime_type'), file_path='.data/temp')\n            print(\"Media media_data:\", media_data)\n            # Save to blob storage\n            filename = f\"file_{file_type}_{datetime.now().isoformat()}_{content.get('sha256', '')}\"\n            blob_id = self.blob_docs_system.save_document(\n                open(media_data, 'rb').read(),\n                filename=filename,\n                file_type=file_type\n            )\n\n            self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n            return f\"\u2705 File uploaded successfully!\\nID: {blob_id}\"\n\n        except Exception as e:\n            logging.error(f\"Upload failed: {str(e)}\")\n            return f\"\u274c Failed to upload file Error : {str(e)}\"\n\n    return \"No pending uploads\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_message","title":"<code>handle_message(message)</code>  <code>async</code>","text":"<p>Main message handler for incoming WhatsApp messages</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def handle_message(self, message: 'Message'):\n    \"\"\"Main message handler for incoming WhatsApp messages\"\"\"\n\n    # Deduplication check\n    with self.message_lock:\n        if message.id in self.processed_messages:\n            return\n        last_ts = time.time()\n        print(last_ts)\n        if len(self.processed_messages) &gt; 0:\n            m_id, last_ts = self.processed_messages.pop()\n            self.processed_messages.add((m_id, last_ts))\n\n        print(\"DUPLICATION P\", message.data.get('entry', [{}])[0].get('changes', [{}])[0].get('value', {}).get('messages', [{}])[0].get('timestamp', 0) , last_ts)\n        if float(message.data.get('entry', [{}])[0].get('changes', [{}])[0].get('value', {}).get('messages', [{}])[0].get('timestamp', 0)) &lt; last_ts - 120:\n            return\n        self.processed_messages.add((message.id, time.perf_counter()))\n\n    # Mark message as read\n    message.mark_as_read()\n\n    # Extract content and type\n    content_type = message.type\n    content = message.content\n\n    print(f\"message.content {content=} {content_type=} {message.data=}\")\n\n    try:\n        if content_type == 'interactive':\n            await self.handle_interactive(message)\n        elif content_type == 'audio':\n            await self.handle_audio_message(message)\n        elif content_type in ['document', 'image', 'video']:\n            response = await self.handle_media_message(message)\n            self.save_reply(message, response)\n        elif content_type == 'text':\n            if content.lower() == \"menu\":\n                self.whc.messenger.send_button(\n                    recipient_id=self.whc.progress_messenger0.recipient_phone,\n                    button=self.buttons[content.lower()]\n                )\n            else:\n                await self.helper_text(message)\n        else:\n            message.reply(\"Unsupported message type\")\n    #except Exception as e:\n    #    logging.error(f\"Message handling error: {str(e)}\")\n    #   message.reply(\"\u274c Error processing request\")\n    finally:\n        # Cleanup old messages (keep 1 hour history)\n        with self.message_lock:\n            self._clean_processed_messages()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.init_services","title":"<code>init_services()</code>","text":"<p>Initialize Gmail and Calendar services</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def init_services(self):\n    \"\"\"\n    Initialize Gmail and Calendar services\n    \"\"\"\n    from googleapiclient.discovery import build\n\n    self.gmail_service = build('gmail', 'v1', credentials=self.credentials)\n    self.calendar_service = build('calendar', 'v3', credentials=self.credentials)\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.load_credentials","title":"<code>load_credentials()</code>","text":"<p>Load previously saved credentials if available</p> <p>:return: Whether credentials were successfully loaded</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def load_credentials(self):\n    \"\"\"\n    Load previously saved credentials if available\n\n    :return: Whether credentials were successfully loaded\n    \"\"\"\n    try:\n        self.credentials = Credentials.from_authorized_user_file('token/google_token.json')\n        self.init_services()\n        return True\n    except FileNotFoundError:\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.run","title":"<code>run()</code>","text":"<p>Start the WhatsApp assistant</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def run(self):\n    \"\"\"Start the WhatsApp assistant\"\"\"\n    try:\n        self.state = AssistantState.ONLINE\n        # Send welcome message\n\n        mas = self.whc.messenger.create_message(\n            content=\"Digital Assistant is online! Send /help for available commands.\",to=self.whc.progress_messenger0.recipient_phone,\n        ).send(sender=0)\n        mas_id = mas.get(\"messages\", [{}])[0].get(\"id\")\n        print(mas_id)\n\n    except Exception as e:\n        logging.error(f\"Assistant error: {str(e)}\")\n        self.state = AssistantState.OFFLINE\n        raise\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.save_credentials","title":"<code>save_credentials()</code>","text":"<p>Save the obtained credentials to a file for future use</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def save_credentials(self):\n    \"\"\"\n    Save the obtained credentials to a file for future use\n    \"\"\"\n    if not os.path.exists('token'):\n        os.makedirs('token')\n\n    with open('token/google_token.json', 'w') as token_file:\n        token_file.write(self.credentials.to_json())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.search_documents","title":"<code>search_documents(message)</code>  <code>async</code>","text":"<p>Initiate document search workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def search_documents(self, message):\n    \"\"\"Initiate document search workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {'type': 'search', 'step': 'awaiting_query'}\n    return {\n        'type': 'quick_reply',\n        'text': '\ud83d\udd0d What are you looking for?',\n        'options': {'cancel': '\u274c Cancel Search'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.send_email","title":"<code>send_email(to, subject, body)</code>","text":"<p>Actual email sending function to be called by agent</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def send_email(self, to, subject, body):\n    \"\"\"Actual email sending function to be called by agent\"\"\"\n    if not self.gmail_service:\n        return False\n\n    message = MIMEText(body)\n    message['to'] = to\n    message['subject'] = subject\n\n    encoded_message = base64.urlsafe_b64encode(message.as_bytes()).decode()\n    self.gmail_service.users().messages().send(\n        userId='me',\n        body={'raw': encoded_message}\n    ).execute()\n    return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.send_structured_response","title":"<code>send_structured_response(result)</code>","text":"<p>Send complex responses using appropriate WhatsApp features</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def send_structured_response(self, result: dict):\n    \"\"\"Send complex responses using appropriate WhatsApp features\"\"\"\n    if result['type'] == 'list':\n        self.whc.messenger.send_button(\n            recipient_id=self.whc.progress_messenger0.recipient_phone,\n            button={\n                'header': result.get('header', ''),\n                'body': result.get('body', ''),\n                'footer': result.get('footer', ''),\n                'action': {\n                    'button': 'Action',\n                    'sections': result['sections']\n                }\n            }\n        )\n    elif result['type'] == 'quick_reply':\n        self.whc.messenger.send_button(\n            recipient_id=self.whc.progress_messenger0.recipient_phone,\n            button={\n                'header': \"Quick reply\",\n                'body': result['text'],\n                'footer': '',\n                'action': {'button': 'Action', 'sections': [{\n                    'title': 'View',\n                    'rows': [{'id': k, 'title': v[:23]} for k, v in result['options'].items()]\n                }]}\n            }\n        )\n\n    elif result['type'] == 'media':\n        if result['media_type'] == 'image':\n            self.whc.messenger.send_image(\n                image=result['url'],\n                recipient_id=self.whc.progress_messenger0.recipient_phone,\n                caption=result.get('caption', '')\n            )\n        elif result['media_type'] == 'document':\n            self.whc.messenger.send_document(\n                document=result['url'],\n                recipient_id=self.whc.progress_messenger0.recipient_phone,\n                caption=result.get('caption', '')\n            )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.setup_interaction_buttons","title":"<code>setup_interaction_buttons()</code>","text":"<p>Define WhatsApp interaction buttons for different functionalities</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def setup_interaction_buttons(self):\n    \"\"\"Define WhatsApp interaction buttons for different functionalities\"\"\"\n    self.buttons = {\n        'menu': {\n            'header': 'Digital Assistant',\n            'body': 'Please select an option:',\n            'footer': '-- + --',\n            'action': {\n                'button': 'Menu',\n                'sections': [\n                    {\n                        'title': 'Main Functions',\n                        'rows': [\n                            {'id': 'agent', 'title': 'Agent Controls', 'description': 'Manage your AI assistant'},\n                            {'id': 'email', 'title': 'Email Management', 'description': 'Handle your emails'},\n                            {'id': 'calendar', 'title': 'Calendar', 'description': 'Manage your schedule'},\n                            {'id': 'docs', 'title': 'Documents', 'description': 'Handle documents'},\n                            {'id': 'system', 'title': 'System', 'description': 'System controls and metrics'}\n                        ]\n                    }\n                ]\n            }\n        },\n        'agent': self._create_agent_controls_buttons(),\n        'email': self._create_email_controls_buttons(),\n        'calendar': self._create_calendar_controls_buttons(),\n        'docs': self._create_docs_controls_buttons(),\n        'system': self._create_system_controls_buttons()\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.setup_progress_messengers","title":"<code>setup_progress_messengers()</code>","text":"<p>Initialize progress messengers for different types of tasks</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def setup_progress_messengers(self):\n    \"\"\"Initialize progress messengers for different types of tasks\"\"\"\n    self.progress_messengers = {\n        'task': self.whc.progress_messenger0,\n        'email': self.whc.progress_messenger1,\n        'calendar': self.whc.progress_messenger2\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.show_task_stack","title":"<code>show_task_stack(*a)</code>  <code>async</code>","text":"<p>Display current task stack</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def show_task_stack(self, *a):\n    \"\"\"Display current task stack\"\"\"\n    if self.agent and len(self.agent.taskstack.tasks) &gt; 0:\n        tasks = self.agent.taskstack.tasks\n        return self.agent.mini_task(\"\\n\".join([f\"Task {t.id}: {t.description}\" for t in tasks]), \"system\", \"Format to nice and clean whatsapp format\")\n    return \"No tasks in stack\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.show_today_events","title":"<code>show_today_events(message)</code>  <code>async</code>","text":"<p>Show today's calendar events</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def show_today_events(self, message):\n    \"\"\"Show today's calendar events\"\"\"\n    if not self.calendar_service:\n        message.replay(\"service not online\")\n\n    now = datetime.utcnow().isoformat() + 'Z'\n    end_of_day = (datetime.now() + timedelta(days=1)).replace(\n        hour=0, minute=0, second=0).isoformat() + 'Z'\n\n    events_result = self.calendar_service.events().list(\n        calendarId='primary',\n        timeMin=now,\n        timeMax=end_of_day,\n        singleEvents=True,\n        orderBy='startTime'\n    ).execute()\n\n    events = events_result.get('items', [])\n    return self._format_calendar_response(events, \"Today's Events\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.show_upcoming_events","title":"<code>show_upcoming_events(message)</code>  <code>async</code>","text":"<p>Show upcoming events with interactive support</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def show_upcoming_events(self, message):\n    \"\"\"Show upcoming events with interactive support\"\"\"\n    if not self.calendar_service:\n        return \"\u26a0\ufe0f Calendar service not configured\"\n\n    try:\n        now = datetime.utcnow().isoformat() + 'Z'\n        next_week = (datetime.now() + timedelta(days=7)).isoformat() + 'Z'\n\n        events_result = self.calendar_service.events().list(\n            calendarId='primary',\n            timeMin=now,\n            timeMax=next_week,\n            singleEvents=True,\n            orderBy='startTime',\n            maxResults=10\n        ).execute()\n\n        events = events_result.get('items', [])\n        return self._format_calendar_response(events, \"Upcoming Events\")\n    except Exception as e:\n        return f\"\u26a0\ufe0f Error fetching events: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.start_agent","title":"<code>start_agent(*a)</code>  <code>async</code>","text":"<p>Start the agent in background mode</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def start_agent(self, *a):\n    \"\"\"Start the agent in background mode\"\"\"\n    if self.agent:\n        self.agent.run_in_background()\n        return True\n    return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.start_document_upload","title":"<code>start_document_upload(message)</code>  <code>async</code>","text":"<p>Initiate document upload workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def start_document_upload(self, message):\n    \"\"\"Initiate document upload workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {'type': 'document', 'step': 'awaiting_file'}\n    return {\n        'type': 'quick_reply',\n        'text': '\ud83d\udce4 Send me the file you want to upload',\n        'options': {'cancel': '\u274c Cancel Upload'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.start_email_compose","title":"<code>start_email_compose(message)</code>  <code>async</code>","text":"<p>Enhanced email composition workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def start_email_compose(self, message):\n    \"\"\"Enhanced email composition workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n        'type': 'compose_email',\n        'step': 'subject',\n        'draft': {'attachments': []}\n    }\n    return {\n        'type': 'quick_reply',\n        'text': \"\ud83d\udcdd Let's compose an email\\n\\nSubject:\",\n        'options': {'cancel': '\u274c Cancel Composition'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.start_event_create","title":"<code>start_event_create(message)</code>  <code>async</code>","text":"<p>Initiate event creation workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def start_event_create(self, message):\n    \"\"\"Initiate event creation workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n        'type': 'create_event',\n        'step': 'title',\n        'event_data': {}\n    }\n    return {\n        'type': 'quick_reply',\n        'text': \"Let's create an event! What's the title?\",\n        'options': {'cancel': '\u274c Cancel'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.stop_agent","title":"<code>stop_agent(*b)</code>  <code>async</code>","text":"<p>Stop the currently running agent</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def stop_agent(self, *b):\n    \"\"\"Stop the currently running agent\"\"\"\n    if self.agent:\n        self.agent.stop()\n        return True\n    return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.system_task","title":"<code>system_task(message)</code>  <code>async</code>","text":"<p>Initiate email search workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def system_task(self, message):\n    \"\"\"Initiate email search workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n        'type': 'system',\n        'step': 'await_query'\n    }\n    return {\n        'type': 'quick_reply',\n        'text': \"Now prompt the \ud83e\udde0ISAA-System \ud83d\udcdd\",\n        'options': {'cancel': '\u274c Cancel Search'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server","title":"<code>server</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager","title":"<code>AppManager</code>","text":"Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>class AppManager(metaclass=Singleton):\n    pepper = \"pepper0\"\n\n    def __init__(self, start_port: int = 8000, port_range: int = 10, em=None):\n        self.instances: dict[str, dict] = {}\n        self.start_port = start_port\n        self.port_range = port_range\n        self.threads: dict[str, Thread] = {}\n        self.stop_events: dict[str, Event] = {}\n        self.message_queue: asyncio.Queue = asyncio.Queue()\n        self.last_messages: dict[str, datetime] = {}\n        self.keys: dict[str, str] = {}\n        self.forwarders: dict[str, dict] = {}\n        self.runner = lambda :None\n\n        if em is None:\n            from toolboxv2 import get_app\n            em = get_app().get_mod(\"EventManager\")\n        from toolboxv2.mods import EventManager\n        self.event_manager: EventManager = em.get_manager()\n\n        # Set up signal handlers for graceful shutdown\n        try:\n            if threading.current_thread() is threading.main_thread():\n                signal.signal(signal.SIGINT, self.signal_handler)\n                signal.signal(signal.SIGTERM, self.signal_handler)\n        except Exception:\n            pass\n\n    def offline(self, instance_id):\n\n        def mark_as_offline():\n            self.forwarders[instance_id]['send'] = None\n            return 'done'\n\n        return mark_as_offline\n\n    def online(self, instance_id):\n\n        def mark_as_online():\n            return self.instances[instance_id]['app']\n\n        def set_callbacks(callback, e_callback=None):\n            if callback is not None:\n                self.forwarders[instance_id]['send'] = callback\n            if e_callback is not None:\n                self.forwarders[instance_id]['sende'] = e_callback\n\n        return mark_as_online(), set_callbacks\n\n    def get_next_available_port(self) -&gt; int:\n        \"\"\"Find the next available port in the range.\"\"\"\n        used_ports = {instance['port'] for instance in self.instances.values()}\n        for port in range(self.start_port, self.start_port + self.port_range):\n            if port not in used_ports:\n                return port\n        raise RuntimeError(\"No available ports in range\")\n\n    def add_instance(self, instance_id: str, **kwargs):\n        \"\"\"\n        Add a new app instance to the manager with automatic port assignment.\n        \"\"\"\n        if instance_id in self.instances:\n            raise ValueError(f\"Instance {instance_id} already exists\")\n\n        port = self.get_next_available_port()\n        app_instance = WhatsApp(**kwargs)\n\n        self.instances[instance_id] = {\n            'app': app_instance,\n            'port': port,\n            'kwargs': kwargs,\n            'phone_number_id': kwargs.get(\"phone_number_id\", {}),\n            'retry_count': 0,\n            'max_retries': 3,\n            'retry_delay': 5\n        }\n        self.keys[instance_id] = Code.one_way_hash(kwargs.get(\"phone_number_id\", {}).get(\"key\"), \"WhatsappAppManager\",\n                                                   self.pepper)\n        self.forwarders[instance_id] = {}\n\n        # Set up message handlers\n        @app_instance.on_message\n        async def message_handler(message):\n            await self.on_message(instance_id, message)\n\n        @app_instance.on_event\n        async def event_handler(event):\n            await self.on_event(instance_id, event)\n\n        @app_instance.on_verification\n        async def verification_handler(verification):\n            await self.on_verification(instance_id, verification)\n\n        # Create stop event for this instance Error parsing message1:\n        self.stop_events[instance_id] = Event()\n\n    def run_instance(self, instance_id: str):\n        \"\"\"Run a single instance in a separate thread with error handling and automatic restart.\"\"\"\n        instance_data = self.instances[instance_id]\n        stop_event = self.stop_events[instance_id]\n\n        while not stop_event.is_set():\n            try:\n                logger.info(f\"Starting instance {instance_id} on port {instance_data['port']}\")\n                instance_data['app'].run(host='0.0.0.0', port=instance_data['port'])\n\n            except Exception as e:\n                logger.error(f\"Error in instance {instance_id}: {str(e)}\")\n                instance_data['retry_count'] += 1\n\n                if instance_data['retry_count'] &gt; instance_data['max_retries']:\n                    logger.error(f\"Max retries exceeded for instance {instance_id}\")\n                    break\n\n                logger.info(f\"Restarting instance {instance_id} in {instance_data['retry_delay']} seconds...\")\n                time.sleep(instance_data['retry_delay'])\n\n                # Recreate the instance\n                instance_data['app'] = WhatsApp(**instance_data['kwargs'])\n                continue\n\n    async def on_message(self, instance_id: str, message: Message):\n        \"\"\"Handle and forward incoming messages.\"\"\"\n        logger.info(f\"Message from instance {instance_id}: {message}\")\n        if instance_id in self.forwarders and 'send' in self.forwarders[instance_id]:\n            await self.forwarders[instance_id]['send'](message)\n\n    async def on_event(self, instance_id: str, event):\n        \"\"\"Handle events.\"\"\"\n        logger.info(f\"Event from instance {instance_id}: {event}\")\n        if instance_id in self.forwarders and 'sende' in self.forwarders[instance_id] and self.forwarders[instance_id]['sende'] is not None:\n            self.forwarders[instance_id]['sende'](event)\n\n    async def on_verification(self, instance_id: str, verification):\n        \"\"\"Handle verification events.\"\"\"\n        logger.info(f\"Verification from instance {instance_id}: {verification}\")\n\n    def run_all_instances(self):\n        \"\"\"Start all instances in separate daemon threads.\"\"\"\n        # Start message forwarder\n\n        # Start all instances\n        for instance_id in self.instances:\n            thread = Thread(\n                target=self.run_instance,\n                args=(instance_id,),\n                daemon=True,\n                name=f\"WhatsApp-{instance_id}\"\n            )\n            self.threads[instance_id] = thread\n            thread.start()\n\n    def signal_handler(self, signum, frame):\n        \"\"\"Handle shutdown signals gracefully.\"\"\"\n        logger.info(\"Shutdown signal received, stopping all instances...\")\n        self.stop_all_instances()\n        sys.exit(0)\n\n    def stop_all_instances(self):\n        \"\"\"Stop all running instances gracefully.\"\"\"\n        for instance_id in self.stop_events:\n            self.stop_events[instance_id].set()\n\n        for thread in self.threads.values():\n            thread.join(timeout=5)\n\n    def create_manager_ui(self, start_assistant):\n        \"\"\"Enhanced WhatsApp Manager UI with instance configuration controls\"\"\"\n        self.runner = start_assistant\n        def ui_manager():\n            # Track instance states and messages\n            original_on_message = self.on_message\n\n            async def enhanced_on_message(instance_id: str, message):\n                self.last_messages[instance_id] = datetime.now()\n                await original_on_message(instance_id, message)\n\n            self.on_message = enhanced_on_message\n\n            def create_instance_card(instance_id: str):\n                \"\"\"Interactive instance control card\"\"\"\n                config = self.instances[instance_id]\n                with ui.card().classes('w-full p-4 mb-4 bg-gray-50 dark:bg-gray-800').style(\"background-color: var(--background-color) !important\"):\n                    # Header Section\n                    with ui.row().classes('w-full justify-between items-center'):\n                        ui.label(f'\ud83d\udcf1 {instance_id}').classes('text-xl font-bold')\n\n                        # Status Indicator\n                        ui.label().bind_text_from(\n                            self.threads, instance_id,\n                            lambda x: 'Running' if x and x.is_alive() else 'Stopped'\n                        )\n\n                    # Configuration Display\n                    with ui.grid(columns=2).classes('w-full mt-4 gap-2'):\n\n                        ui.label('port:').classes('font-bold')\n                        ui.label(config['port'])\n\n                        ui.label('Last Activity:').classes('font-bold')\n                        ui.label().bind_text_from(\n                            self.last_messages, instance_id,\n                            lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\") if x else 'Never'\n                        )\n\n                    # Action Controls\n                    with ui.row().classes('w-full mt-4 gap-2'):\n                        with ui.button(icon='settings', on_click=lambda: edit_dialog.open()).props('flat'):\n                            ui.tooltip('Configure')\n\n                        with ui.button(icon='refresh', color='orange',\n                                       on_click=lambda: self.restart_instance(instance_id)):\n                            ui.tooltip('Restart')\n\n                        with ui.button(icon='stop', color='red',\n                                       on_click=lambda: self.stop_instance(instance_id)):\n                            ui.tooltip('Stop')\n\n                    # Edit Configuration Dialog\n                    with ui.dialog() as edit_dialog, ui.card().classes('p-4 gap-4'):\n                        new_key = ui.input('API Key', value=config['phone_number_id'].get('key', ''))\n                        new_number = ui.input('Phone Number', value=config['phone_number_id'].get('number', ''))\n\n                        with ui.row().classes('w-full justify-end'):\n                            ui.button('Cancel', on_click=edit_dialog.close)\n                            ui.button('Save', color='primary', on_click=lambda: (\n                                self.update_instance_config(\n                                    instance_id,\n                                    new_key.value,\n                                    new_number.value\n                                ),\n                                edit_dialog.close()\n                            ))\n\n            # Main UI Layout\n            with ui.column().classes('w-full max-w-4xl mx-auto p-4'):\n                ui.label('WhatsApp Instance Manager').classes('text-2xl font-bold mb-6')\n\n                # Add Instance Section\n                with ui.expansion('\u2795 Add New Instance', icon='add').classes('w-full'):\n                    with ui.card().classes('w-full p-4 mt-2'):\n                        instance_id = ui.input('Instance ID').classes('w-full')\n                        token = ui.input('API Token').classes('w-full')\n                        phone_key = ui.input('Phone Number Key').classes('w-full')\n                        phone_number = ui.input('Phone Number').classes('w-full')\n\n                        with ui.row().classes('w-full justify-end gap-2'):\n                            ui.button('Clear', on_click=lambda: (\n                                instance_id.set_value(''),\n                                token.set_value(''),\n                                phone_key.set_value(''),\n                                phone_number.set_value('')\n                            ))\n                            ui.button('Create', color='positive', on_click=lambda: (\n                                self.add_update_instance(\n                                    instance_id.value,\n                                    token.value,\n                                    phone_key.value,\n                                    phone_number.value\n                                ),\n                                instances_container.refresh()\n                            ))\n\n                # Instances Display\n                instances_container = ui.column().classes('w-full')\n                with instances_container:\n                    for instance_id in self.instances:\n                        create_instance_card(instance_id)\n\n        return ui_manager\n\n    # Add to manager class\n    def add_update_instance(self, instance_id, token, phone_key, phone_number):\n        \"\"\"Add or update instance configuration\"\"\"\n        if instance_id in self.instances:\n            self.stop_instance(instance_id)\n            del self.instances[instance_id]\n\n        self.add_instance(\n            instance_id,\n            token=token,\n            phone_number_id={\n                'key': phone_key,\n                'number': phone_number\n            },\n            verify_token=os.getenv(\"WHATSAPP_VERIFY_TOKEN\")\n        )\n        self.start_instance(instance_id)\n\n    def update_instance_config(self, instance_id, new_key, new_number):\n        \"\"\"Update existing instance configuration\"\"\"\n        if instance_id in self.instances:\n            self.instances[instance_id]['phone_number_id'] = {\n                'key': new_key,\n                'number': new_number\n            }\n            self.restart_instance(instance_id)\n\n    def restart_instance(self, instance_id):\n        \"\"\"Safe restart of instance\"\"\"\n        self.stop_instance(instance_id)\n        self.start_instance(instance_id)\n\n    def stop_instance(self, instance_id):\n        \"\"\"Graceful stop of instance\"\"\"\n        if instance_id in self.threads:\n            self.stop_events[instance_id].set()\n            self.threads[instance_id].join(timeout=5)\n            del self.threads[instance_id]\n\n    def start_instance(self, instance_id):\n        \"\"\"Start instance thread\"\"\"\n        print(\"Starting Istance\")\n\n        self.stop_events[instance_id] = threading.Event()\n        self.threads[instance_id] = threading.Thread(\n            target=self.run_instance,\n            args=(instance_id,),\n            daemon=True\n        )\n        self.threads[instance_id].start()\n        print(\"Running starter\", self.runner())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.add_instance","title":"<code>add_instance(instance_id, **kwargs)</code>","text":"<p>Add a new app instance to the manager with automatic port assignment.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def add_instance(self, instance_id: str, **kwargs):\n    \"\"\"\n    Add a new app instance to the manager with automatic port assignment.\n    \"\"\"\n    if instance_id in self.instances:\n        raise ValueError(f\"Instance {instance_id} already exists\")\n\n    port = self.get_next_available_port()\n    app_instance = WhatsApp(**kwargs)\n\n    self.instances[instance_id] = {\n        'app': app_instance,\n        'port': port,\n        'kwargs': kwargs,\n        'phone_number_id': kwargs.get(\"phone_number_id\", {}),\n        'retry_count': 0,\n        'max_retries': 3,\n        'retry_delay': 5\n    }\n    self.keys[instance_id] = Code.one_way_hash(kwargs.get(\"phone_number_id\", {}).get(\"key\"), \"WhatsappAppManager\",\n                                               self.pepper)\n    self.forwarders[instance_id] = {}\n\n    # Set up message handlers\n    @app_instance.on_message\n    async def message_handler(message):\n        await self.on_message(instance_id, message)\n\n    @app_instance.on_event\n    async def event_handler(event):\n        await self.on_event(instance_id, event)\n\n    @app_instance.on_verification\n    async def verification_handler(verification):\n        await self.on_verification(instance_id, verification)\n\n    # Create stop event for this instance Error parsing message1:\n    self.stop_events[instance_id] = Event()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.add_update_instance","title":"<code>add_update_instance(instance_id, token, phone_key, phone_number)</code>","text":"<p>Add or update instance configuration</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def add_update_instance(self, instance_id, token, phone_key, phone_number):\n    \"\"\"Add or update instance configuration\"\"\"\n    if instance_id in self.instances:\n        self.stop_instance(instance_id)\n        del self.instances[instance_id]\n\n    self.add_instance(\n        instance_id,\n        token=token,\n        phone_number_id={\n            'key': phone_key,\n            'number': phone_number\n        },\n        verify_token=os.getenv(\"WHATSAPP_VERIFY_TOKEN\")\n    )\n    self.start_instance(instance_id)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.create_manager_ui","title":"<code>create_manager_ui(start_assistant)</code>","text":"<p>Enhanced WhatsApp Manager UI with instance configuration controls</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def create_manager_ui(self, start_assistant):\n    \"\"\"Enhanced WhatsApp Manager UI with instance configuration controls\"\"\"\n    self.runner = start_assistant\n    def ui_manager():\n        # Track instance states and messages\n        original_on_message = self.on_message\n\n        async def enhanced_on_message(instance_id: str, message):\n            self.last_messages[instance_id] = datetime.now()\n            await original_on_message(instance_id, message)\n\n        self.on_message = enhanced_on_message\n\n        def create_instance_card(instance_id: str):\n            \"\"\"Interactive instance control card\"\"\"\n            config = self.instances[instance_id]\n            with ui.card().classes('w-full p-4 mb-4 bg-gray-50 dark:bg-gray-800').style(\"background-color: var(--background-color) !important\"):\n                # Header Section\n                with ui.row().classes('w-full justify-between items-center'):\n                    ui.label(f'\ud83d\udcf1 {instance_id}').classes('text-xl font-bold')\n\n                    # Status Indicator\n                    ui.label().bind_text_from(\n                        self.threads, instance_id,\n                        lambda x: 'Running' if x and x.is_alive() else 'Stopped'\n                    )\n\n                # Configuration Display\n                with ui.grid(columns=2).classes('w-full mt-4 gap-2'):\n\n                    ui.label('port:').classes('font-bold')\n                    ui.label(config['port'])\n\n                    ui.label('Last Activity:').classes('font-bold')\n                    ui.label().bind_text_from(\n                        self.last_messages, instance_id,\n                        lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\") if x else 'Never'\n                    )\n\n                # Action Controls\n                with ui.row().classes('w-full mt-4 gap-2'):\n                    with ui.button(icon='settings', on_click=lambda: edit_dialog.open()).props('flat'):\n                        ui.tooltip('Configure')\n\n                    with ui.button(icon='refresh', color='orange',\n                                   on_click=lambda: self.restart_instance(instance_id)):\n                        ui.tooltip('Restart')\n\n                    with ui.button(icon='stop', color='red',\n                                   on_click=lambda: self.stop_instance(instance_id)):\n                        ui.tooltip('Stop')\n\n                # Edit Configuration Dialog\n                with ui.dialog() as edit_dialog, ui.card().classes('p-4 gap-4'):\n                    new_key = ui.input('API Key', value=config['phone_number_id'].get('key', ''))\n                    new_number = ui.input('Phone Number', value=config['phone_number_id'].get('number', ''))\n\n                    with ui.row().classes('w-full justify-end'):\n                        ui.button('Cancel', on_click=edit_dialog.close)\n                        ui.button('Save', color='primary', on_click=lambda: (\n                            self.update_instance_config(\n                                instance_id,\n                                new_key.value,\n                                new_number.value\n                            ),\n                            edit_dialog.close()\n                        ))\n\n        # Main UI Layout\n        with ui.column().classes('w-full max-w-4xl mx-auto p-4'):\n            ui.label('WhatsApp Instance Manager').classes('text-2xl font-bold mb-6')\n\n            # Add Instance Section\n            with ui.expansion('\u2795 Add New Instance', icon='add').classes('w-full'):\n                with ui.card().classes('w-full p-4 mt-2'):\n                    instance_id = ui.input('Instance ID').classes('w-full')\n                    token = ui.input('API Token').classes('w-full')\n                    phone_key = ui.input('Phone Number Key').classes('w-full')\n                    phone_number = ui.input('Phone Number').classes('w-full')\n\n                    with ui.row().classes('w-full justify-end gap-2'):\n                        ui.button('Clear', on_click=lambda: (\n                            instance_id.set_value(''),\n                            token.set_value(''),\n                            phone_key.set_value(''),\n                            phone_number.set_value('')\n                        ))\n                        ui.button('Create', color='positive', on_click=lambda: (\n                            self.add_update_instance(\n                                instance_id.value,\n                                token.value,\n                                phone_key.value,\n                                phone_number.value\n                            ),\n                            instances_container.refresh()\n                        ))\n\n            # Instances Display\n            instances_container = ui.column().classes('w-full')\n            with instances_container:\n                for instance_id in self.instances:\n                    create_instance_card(instance_id)\n\n    return ui_manager\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.get_next_available_port","title":"<code>get_next_available_port()</code>","text":"<p>Find the next available port in the range.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def get_next_available_port(self) -&gt; int:\n    \"\"\"Find the next available port in the range.\"\"\"\n    used_ports = {instance['port'] for instance in self.instances.values()}\n    for port in range(self.start_port, self.start_port + self.port_range):\n        if port not in used_ports:\n            return port\n    raise RuntimeError(\"No available ports in range\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.on_event","title":"<code>on_event(instance_id, event)</code>  <code>async</code>","text":"<p>Handle events.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>async def on_event(self, instance_id: str, event):\n    \"\"\"Handle events.\"\"\"\n    logger.info(f\"Event from instance {instance_id}: {event}\")\n    if instance_id in self.forwarders and 'sende' in self.forwarders[instance_id] and self.forwarders[instance_id]['sende'] is not None:\n        self.forwarders[instance_id]['sende'](event)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.on_message","title":"<code>on_message(instance_id, message)</code>  <code>async</code>","text":"<p>Handle and forward incoming messages.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>async def on_message(self, instance_id: str, message: Message):\n    \"\"\"Handle and forward incoming messages.\"\"\"\n    logger.info(f\"Message from instance {instance_id}: {message}\")\n    if instance_id in self.forwarders and 'send' in self.forwarders[instance_id]:\n        await self.forwarders[instance_id]['send'](message)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.on_verification","title":"<code>on_verification(instance_id, verification)</code>  <code>async</code>","text":"<p>Handle verification events.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>async def on_verification(self, instance_id: str, verification):\n    \"\"\"Handle verification events.\"\"\"\n    logger.info(f\"Verification from instance {instance_id}: {verification}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.restart_instance","title":"<code>restart_instance(instance_id)</code>","text":"<p>Safe restart of instance</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def restart_instance(self, instance_id):\n    \"\"\"Safe restart of instance\"\"\"\n    self.stop_instance(instance_id)\n    self.start_instance(instance_id)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.run_all_instances","title":"<code>run_all_instances()</code>","text":"<p>Start all instances in separate daemon threads.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def run_all_instances(self):\n    \"\"\"Start all instances in separate daemon threads.\"\"\"\n    # Start message forwarder\n\n    # Start all instances\n    for instance_id in self.instances:\n        thread = Thread(\n            target=self.run_instance,\n            args=(instance_id,),\n            daemon=True,\n            name=f\"WhatsApp-{instance_id}\"\n        )\n        self.threads[instance_id] = thread\n        thread.start()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.run_instance","title":"<code>run_instance(instance_id)</code>","text":"<p>Run a single instance in a separate thread with error handling and automatic restart.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def run_instance(self, instance_id: str):\n    \"\"\"Run a single instance in a separate thread with error handling and automatic restart.\"\"\"\n    instance_data = self.instances[instance_id]\n    stop_event = self.stop_events[instance_id]\n\n    while not stop_event.is_set():\n        try:\n            logger.info(f\"Starting instance {instance_id} on port {instance_data['port']}\")\n            instance_data['app'].run(host='0.0.0.0', port=instance_data['port'])\n\n        except Exception as e:\n            logger.error(f\"Error in instance {instance_id}: {str(e)}\")\n            instance_data['retry_count'] += 1\n\n            if instance_data['retry_count'] &gt; instance_data['max_retries']:\n                logger.error(f\"Max retries exceeded for instance {instance_id}\")\n                break\n\n            logger.info(f\"Restarting instance {instance_id} in {instance_data['retry_delay']} seconds...\")\n            time.sleep(instance_data['retry_delay'])\n\n            # Recreate the instance\n            instance_data['app'] = WhatsApp(**instance_data['kwargs'])\n            continue\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.signal_handler","title":"<code>signal_handler(signum, frame)</code>","text":"<p>Handle shutdown signals gracefully.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def signal_handler(self, signum, frame):\n    \"\"\"Handle shutdown signals gracefully.\"\"\"\n    logger.info(\"Shutdown signal received, stopping all instances...\")\n    self.stop_all_instances()\n    sys.exit(0)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.start_instance","title":"<code>start_instance(instance_id)</code>","text":"<p>Start instance thread</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def start_instance(self, instance_id):\n    \"\"\"Start instance thread\"\"\"\n    print(\"Starting Istance\")\n\n    self.stop_events[instance_id] = threading.Event()\n    self.threads[instance_id] = threading.Thread(\n        target=self.run_instance,\n        args=(instance_id,),\n        daemon=True\n    )\n    self.threads[instance_id].start()\n    print(\"Running starter\", self.runner())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.stop_all_instances","title":"<code>stop_all_instances()</code>","text":"<p>Stop all running instances gracefully.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def stop_all_instances(self):\n    \"\"\"Stop all running instances gracefully.\"\"\"\n    for instance_id in self.stop_events:\n        self.stop_events[instance_id].set()\n\n    for thread in self.threads.values():\n        thread.join(timeout=5)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.stop_instance","title":"<code>stop_instance(instance_id)</code>","text":"<p>Graceful stop of instance</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def stop_instance(self, instance_id):\n    \"\"\"Graceful stop of instance\"\"\"\n    if instance_id in self.threads:\n        self.stop_events[instance_id].set()\n        self.threads[instance_id].join(timeout=5)\n        del self.threads[instance_id]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.update_instance_config","title":"<code>update_instance_config(instance_id, new_key, new_number)</code>","text":"<p>Update existing instance configuration</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def update_instance_config(self, instance_id, new_key, new_number):\n    \"\"\"Update existing instance configuration\"\"\"\n    if instance_id in self.instances:\n        self.instances[instance_id]['phone_number_id'] = {\n            'key': new_key,\n            'number': new_number\n        }\n        self.restart_instance(instance_id)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils","title":"<code>utils</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils.ProgressMessenger","title":"<code>ProgressMessenger</code>","text":"Source code in <code>toolboxv2/mods/WhatsAppTb/utils.py</code> <pre><code>class ProgressMessenger:\n    def __init__(self, messenger, recipient_phone: str, max_steps: int = 5, emoji_set: list[str] = None, content=None):\n        self.messenger = messenger\n        self.recipient_phone = recipient_phone\n        self.max_steps = max_steps\n        self.emoji_set = emoji_set or [\"\u2b1c\", \"\u2b1b\", \"\ud83d\udfe9\", \"\ud83d\udfe8\", \"\ud83d\udfe6\"]\n        self.message_id = None\n        self.content = content\n\n    def send_initial_message(self, mode: str = \"progress\"):\n        \"\"\"\n        Sends the initial message. Modes can be 'progress' or 'loading'.\n        \"\"\"\n        if mode == \"progress\":\n            emoji_legend = \"\\n\".join(\n                f\"{emoji} - Step {i + 1}\" for i, emoji in enumerate(self.emoji_set)\n            )\n            content = (\n                \"Progress is being updated in real-time!\\n\\n\"\n                \"Legend:\\n\"\n                f\"{emoji_legend}\\n\\n\"\n                \"Stay tuned for updates!\"\n            )\n        elif mode == \"loading\":\n            content = (\n                \"Loading in progress! \ud83c\udf00\\n\"\n                \"The indicator will loop until work is done.\"\n            )\n        else:\n            raise ValueError(\"Invalid mode. Use 'progress' or 'loading'.\")\n\n        if self.content is not None:\n            content += '\\n'+self.content\n        message = self.messenger.create_message(content=content, to=self.recipient_phone)\n        response = message.send(sender=0)\n        self.message_id = response.get(\"messages\", [{}])[0].get(\"id\")\n        logging.info(f\"Initial message sent: {content}\")\n        return self.message_id\n\n    def update_progress(self, step_flag: threading.Event):\n        \"\"\"\n        Updates the reaction on the message to represent progress.\n        \"\"\"\n        if not self.message_id:\n            raise ValueError(\"Message ID not found. Ensure the initial message is sent first.\")\n        message = self.messenger.create_message(id=self.message_id, to=self.recipient_phone)\n        for step in range(self.max_steps):\n            emoji = self.emoji_set[step % len(self.emoji_set)]\n            message.react(emoji)\n            logging.info(f\"Progress updated: Step {step + 1}/{self.max_steps} with emoji {emoji}\")\n            while not step_flag.is_set():\n                time.sleep(0.5)\n            step_flag.clear()\n        # Final acknowledgment\n        message.react(\"\ud83d\udc4d\")\n        logging.info(\"Progress completed with final acknowledgment.\")\n\n    def update_loading(self, stop_flag: threading.Event):\n        \"\"\"\n        Continuously updates the reaction to represent a looping 'loading' indicator.\n        \"\"\"\n        if not self.message_id:\n            raise ValueError(\"Message ID not found. Ensure the initial message is sent first.\")\n        message = self.messenger.create_message(id=self.message_id, to=self.recipient_phone)\n        step = 0\n        while not stop_flag.is_set():\n            emoji = self.emoji_set[step % len(self.emoji_set)]\n            message.react(emoji)\n            logging.info(f\"Loading update: {emoji}\")\n            time.sleep(1)  # Faster updates for loading\n            step += 1\n        # Final acknowledgment\n        message.react(\"\u2705\")\n        logging.info(\"Loading completed with final acknowledgment.\")\n        message.reply(\"\u2705Done\u2705\")\n\n    def start_progress_in_background(self, step_flag):\n        \"\"\"\n        Starts the progress update in a separate thread.\n        \"\"\"\n        threading.Thread(target=self.update_progress, args=(step_flag, ), daemon=True).start()\n\n    def start_loading_in_background(self, stop_flag: threading.Event):\n        \"\"\"\n        Starts the loading update in a separate thread.\n        \"\"\"\n        threading.Thread(target=self.update_loading, args=(stop_flag,), daemon=True).start()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils.ProgressMessenger.send_initial_message","title":"<code>send_initial_message(mode='progress')</code>","text":"<p>Sends the initial message. Modes can be 'progress' or 'loading'.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/utils.py</code> <pre><code>def send_initial_message(self, mode: str = \"progress\"):\n    \"\"\"\n    Sends the initial message. Modes can be 'progress' or 'loading'.\n    \"\"\"\n    if mode == \"progress\":\n        emoji_legend = \"\\n\".join(\n            f\"{emoji} - Step {i + 1}\" for i, emoji in enumerate(self.emoji_set)\n        )\n        content = (\n            \"Progress is being updated in real-time!\\n\\n\"\n            \"Legend:\\n\"\n            f\"{emoji_legend}\\n\\n\"\n            \"Stay tuned for updates!\"\n        )\n    elif mode == \"loading\":\n        content = (\n            \"Loading in progress! \ud83c\udf00\\n\"\n            \"The indicator will loop until work is done.\"\n        )\n    else:\n        raise ValueError(\"Invalid mode. Use 'progress' or 'loading'.\")\n\n    if self.content is not None:\n        content += '\\n'+self.content\n    message = self.messenger.create_message(content=content, to=self.recipient_phone)\n    response = message.send(sender=0)\n    self.message_id = response.get(\"messages\", [{}])[0].get(\"id\")\n    logging.info(f\"Initial message sent: {content}\")\n    return self.message_id\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils.ProgressMessenger.start_loading_in_background","title":"<code>start_loading_in_background(stop_flag)</code>","text":"<p>Starts the loading update in a separate thread.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/utils.py</code> <pre><code>def start_loading_in_background(self, stop_flag: threading.Event):\n    \"\"\"\n    Starts the loading update in a separate thread.\n    \"\"\"\n    threading.Thread(target=self.update_loading, args=(stop_flag,), daemon=True).start()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils.ProgressMessenger.start_progress_in_background","title":"<code>start_progress_in_background(step_flag)</code>","text":"<p>Starts the progress update in a separate thread.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/utils.py</code> <pre><code>def start_progress_in_background(self, step_flag):\n    \"\"\"\n    Starts the progress update in a separate thread.\n    \"\"\"\n    threading.Thread(target=self.update_progress, args=(step_flag, ), daemon=True).start()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils.ProgressMessenger.update_loading","title":"<code>update_loading(stop_flag)</code>","text":"<p>Continuously updates the reaction to represent a looping 'loading' indicator.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/utils.py</code> <pre><code>def update_loading(self, stop_flag: threading.Event):\n    \"\"\"\n    Continuously updates the reaction to represent a looping 'loading' indicator.\n    \"\"\"\n    if not self.message_id:\n        raise ValueError(\"Message ID not found. Ensure the initial message is sent first.\")\n    message = self.messenger.create_message(id=self.message_id, to=self.recipient_phone)\n    step = 0\n    while not stop_flag.is_set():\n        emoji = self.emoji_set[step % len(self.emoji_set)]\n        message.react(emoji)\n        logging.info(f\"Loading update: {emoji}\")\n        time.sleep(1)  # Faster updates for loading\n        step += 1\n    # Final acknowledgment\n    message.react(\"\u2705\")\n    logging.info(\"Loading completed with final acknowledgment.\")\n    message.reply(\"\u2705Done\u2705\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils.ProgressMessenger.update_progress","title":"<code>update_progress(step_flag)</code>","text":"<p>Updates the reaction on the message to represent progress.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/utils.py</code> <pre><code>def update_progress(self, step_flag: threading.Event):\n    \"\"\"\n    Updates the reaction on the message to represent progress.\n    \"\"\"\n    if not self.message_id:\n        raise ValueError(\"Message ID not found. Ensure the initial message is sent first.\")\n    message = self.messenger.create_message(id=self.message_id, to=self.recipient_phone)\n    for step in range(self.max_steps):\n        emoji = self.emoji_set[step % len(self.emoji_set)]\n        message.react(emoji)\n        logging.info(f\"Progress updated: Step {step + 1}/{self.max_steps} with emoji {emoji}\")\n        while not step_flag.is_set():\n            time.sleep(0.5)\n        step_flag.clear()\n    # Final acknowledgment\n    message.react(\"\ud83d\udc4d\")\n    logging.info(\"Progress completed with final acknowledgment.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.cli_functions","title":"<code>cli_functions</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.cli_functions.replace_bracketed_content","title":"<code>replace_bracketed_content(text, replacements, inlist=False)</code>","text":"<p>Ersetzt Inhalte in eckigen Klammern mit entsprechenden Werten aus einem W\u00f6rterbuch.</p> <p>:param text: Der zu verarbeitende Text als String. :param replacements: Ein W\u00f6rterbuch mit Schl\u00fcssel-Wert-Paaren f\u00fcr die Ersetzung. :return: Den modifizierten Text.</p> Source code in <code>toolboxv2/mods/cli_functions.py</code> <pre><code>def replace_bracketed_content(text, replacements, inlist=False):\n    \"\"\"\n    Ersetzt Inhalte in eckigen Klammern mit entsprechenden Werten aus einem W\u00f6rterbuch.\n\n    :param text: Der zu verarbeitende Text als String.\n    :param replacements: Ein W\u00f6rterbuch mit Schl\u00fcssel-Wert-Paaren f\u00fcr die Ersetzung.\n    :return: Den modifizierten Text.\n    \"\"\"\n    # Finde alle Vorkommen von Texten in eckigen Klammern\n    matches = re.findall(r'\\[([^\\]]+)\\]', text)\n\n    # Ersetze jeden gefundenen Text durch den entsprechenden Wert aus dem W\u00f6rterbuch\n    as_list = text.split(' ')\n    i = 0\n    for key in matches:\n        if key in replacements:\n            if not inlist:\n                text = text.replace(f'[{key}]', str(replacements[key]))\n            else:\n                as_list[i] = replacements[key]\n        i += 1\n    if not inlist:\n        return text\n    return as_list\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa","title":"<code>isaa</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent","title":"<code>CodingAgent</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.coder","title":"<code>coder</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.coder.CodeFile","title":"<code>CodeFile</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Source or test file specification</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/coder.py</code> <pre><code>class CodeFile(BaseModel):\n    \"\"\"Source or test file specification\"\"\"\n    path: str = Field(..., description=\"Relative file path\")\n    content: str = Field(..., description=\"File contents\")\n    language: Literal[\"python\", \"javascript\", \"html\", \"css\", \"c\", \"rust\", \"go\"]\n    is_test: bool = Field(default=False, description=\"Whether this is a test file\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.coder.MVPPipeline","title":"<code>MVPPipeline</code>  <code>dataclass</code>","text":"Source code in <code>toolboxv2/mods/isaa/CodingAgent/coder.py</code> <pre><code>@dataclass\nclass MVPPipeline:\n    agent: Agent\n    message: list\n\n    async def generate_project(self, requirements: str) -&gt; list[CodeFile]:\n        \"\"\"Generate complete project from requirements\"\"\"\n        try:\n            logger.info(\"Starting MVP project generation\")\n\n            # Generate project structure\n            structure_dict = self.agent.format_class(\n                ProjectStructure,\n                f\"\"\"Create a project structure for these requirements:\n                {requirements}\n\n                Include:\n                - Source files by logic structure\n                - Test files by logic structure\n                - Configuration files if needed\n                - Documentation placement\n                \"\"\"\n            )\n            print(structure_dict)\n            structure = ProjectStructure(**structure_dict)\n            logger.info(f\"Generated project structure with {len(structure.directories_files)} directories_files\")\n            # Generate test cases\n            test_cases_dict = []\n            for file_name in structure.directories_files.get('test',\n                            structure.directories_files.get('tests',\n                         structure.directories_files.get('test_files',\n                         structure.directories_files.get('tests_files',\n                         structure.directories_files.get('test_file',\n                         structure.directories_files.get('tests_file', [])))))):\n                test_cases_dict.append(self.agent.format_class(\n                TestCase,\n                f\"\"\"Generate test cases following TDD for:\n                Requirements: {requirements}\n                Only for the file : {file_name}\n                Structure: {structure.model_dump_json()}\n\n                Create comprehensive tests that:\n                1. Cover all requirements\n                2. Include edge cases\n                3. Test cross-language integration\n                4. Follow {structure.test_framework} best practices\"\"\", message=self.message\n                ))\n                self.message.append({'content': self.agent.last_result, 'role': 'assistant'})\n            test_cases = [TestCase(**tc) for tc in test_cases_dict]\n            logger.info(f\"Generated {len(test_cases)} test cases\")\n\n            # Generate test files\n            test_files_dict = []\n            for tc in test_cases:\n                test_files_dict.append(self.agent.format_class(\n                CodeFile,\n                f\"\"\"Create test files implementing these cases:\n                {tc.model_dump_json()}\n\n                Requirements:\n                - Use {structure.test_framework}\n                - One file per major feature\n                - Include setup/teardown\n                - Mock external services\"\"\", message=self.message\n                ))\n                self.message.append({'content': self.agent.last_result, 'role': 'assistant'})\n            test_files = [CodeFile(**tf) for tf in test_files_dict]\n            logger.info(f\"Generated {len(test_files)} test files\")\n\n            # Generate implementation files\n            impl_files_dict = []\n            for tf in test_cases:\n                impl_files_dict.append(self.agent.format_class(\n                CodeFile,\n                f\"\"\"Create implementation files to pass tests:\n                Test Files: {tf}\n                Structure: {structure.model_dump_json()}\n\n                Requirements:\n                - Follow language best practices\n                - Include documentation\n                - Handle errors properly\n                - Use proper typing\"\"\"\n                ))\n            impl_files = [CodeFile(**impl) for impl in impl_files_dict]\n            logger.info(f\"Generated {len(impl_files)} implementation files\")\n\n            # Save files\n            await self._save_files(structure, test_files + impl_files)\n\n            # Run tests\n            test_results = await run_tests(test_files, impl_files)\n\n            if not all(tr.passed for tr in test_results):\n                failed = [tr for tr in test_results if not tr.passed]\n                raise Exception(f\"Tests failed: {[f.message for f in failed]}\")\n\n            logger.info(\"Project generation completed successfully\")\n            return test_files + impl_files\n\n        except Exception as e:\n            logger.error(f\"Project generation failed: {str(e)}\")\n            raise\n\n    @staticmethod\n    async def _save_files(structure: ProjectStructure,\n                          files: list[CodeFile]) -&gt; None:\n        \"\"\"Save generated files to disk\"\"\"\n        try:\n            # Create directories_files\n            for _dir_type, paths in structure.directories_files.items():\n                for path in paths:\n                    full_path = os.path.join('data', structure.root_dir, path)\n                    os.makedirs(full_path, exist_ok=True)\n                    logger.debug(f\"Created directory: {full_path}\")\n\n            # Save files\n            for file in files:\n                full_path = os.path.join('data',structure.root_dir, file.path)\n                os.makedirs(os.path.dirname(full_path), exist_ok=True)\n                with open(full_path, 'w') as f:\n                    f.write(file.content)\n                logger.debug(f\"Saved file: {full_path}\")\n\n        except OSError as e:\n            logger.error(f\"Failed to save files: {str(e)}\")\n            raise\n</code></pre> <code>generate_project(requirements)</code> <code>async</code> \u00b6 <p>Generate complete project from requirements</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/coder.py</code> <pre><code>async def generate_project(self, requirements: str) -&gt; list[CodeFile]:\n    \"\"\"Generate complete project from requirements\"\"\"\n    try:\n        logger.info(\"Starting MVP project generation\")\n\n        # Generate project structure\n        structure_dict = self.agent.format_class(\n            ProjectStructure,\n            f\"\"\"Create a project structure for these requirements:\n            {requirements}\n\n            Include:\n            - Source files by logic structure\n            - Test files by logic structure\n            - Configuration files if needed\n            - Documentation placement\n            \"\"\"\n        )\n        print(structure_dict)\n        structure = ProjectStructure(**structure_dict)\n        logger.info(f\"Generated project structure with {len(structure.directories_files)} directories_files\")\n        # Generate test cases\n        test_cases_dict = []\n        for file_name in structure.directories_files.get('test',\n                        structure.directories_files.get('tests',\n                     structure.directories_files.get('test_files',\n                     structure.directories_files.get('tests_files',\n                     structure.directories_files.get('test_file',\n                     structure.directories_files.get('tests_file', [])))))):\n            test_cases_dict.append(self.agent.format_class(\n            TestCase,\n            f\"\"\"Generate test cases following TDD for:\n            Requirements: {requirements}\n            Only for the file : {file_name}\n            Structure: {structure.model_dump_json()}\n\n            Create comprehensive tests that:\n            1. Cover all requirements\n            2. Include edge cases\n            3. Test cross-language integration\n            4. Follow {structure.test_framework} best practices\"\"\", message=self.message\n            ))\n            self.message.append({'content': self.agent.last_result, 'role': 'assistant'})\n        test_cases = [TestCase(**tc) for tc in test_cases_dict]\n        logger.info(f\"Generated {len(test_cases)} test cases\")\n\n        # Generate test files\n        test_files_dict = []\n        for tc in test_cases:\n            test_files_dict.append(self.agent.format_class(\n            CodeFile,\n            f\"\"\"Create test files implementing these cases:\n            {tc.model_dump_json()}\n\n            Requirements:\n            - Use {structure.test_framework}\n            - One file per major feature\n            - Include setup/teardown\n            - Mock external services\"\"\", message=self.message\n            ))\n            self.message.append({'content': self.agent.last_result, 'role': 'assistant'})\n        test_files = [CodeFile(**tf) for tf in test_files_dict]\n        logger.info(f\"Generated {len(test_files)} test files\")\n\n        # Generate implementation files\n        impl_files_dict = []\n        for tf in test_cases:\n            impl_files_dict.append(self.agent.format_class(\n            CodeFile,\n            f\"\"\"Create implementation files to pass tests:\n            Test Files: {tf}\n            Structure: {structure.model_dump_json()}\n\n            Requirements:\n            - Follow language best practices\n            - Include documentation\n            - Handle errors properly\n            - Use proper typing\"\"\"\n            ))\n        impl_files = [CodeFile(**impl) for impl in impl_files_dict]\n        logger.info(f\"Generated {len(impl_files)} implementation files\")\n\n        # Save files\n        await self._save_files(structure, test_files + impl_files)\n\n        # Run tests\n        test_results = await run_tests(test_files, impl_files)\n\n        if not all(tr.passed for tr in test_results):\n            failed = [tr for tr in test_results if not tr.passed]\n            raise Exception(f\"Tests failed: {[f.message for f in failed]}\")\n\n        logger.info(\"Project generation completed successfully\")\n        return test_files + impl_files\n\n    except Exception as e:\n        logger.error(f\"Project generation failed: {str(e)}\")\n        raise\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.coder.ProjectStructure","title":"<code>ProjectStructure</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Project layout specification</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/coder.py</code> <pre><code>class ProjectStructure(BaseModel):\n    \"\"\"Project layout specification\"\"\"\n    root_dir: str\n    directories_files: dict[str, list[str]] = Field(\n        description=\"Map of directory types to paths\"\n    )\n    test_framework: str = Field(\n        default=\"pytest\",\n        description=\"Testing framework to use\"\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.coder.TestCase","title":"<code>TestCase</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Individual test case specification</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/coder.py</code> <pre><code>class TestCase(BaseModel):\n    \"\"\"Individual test case specification\"\"\"\n    name: str = Field(..., description=\"Unique test case identifier\")\n    description: str = Field(..., description=\"What the test verifies\")\n    expected_result: str = Field(..., description=\"Expected outcome\")\n    test_code: str = Field(..., description=\"Actual test implementation\")\n    dependencies: list[str] = Field(default_factory=list, description=\"Required dependencies\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.coder.TestResult","title":"<code>TestResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Individual test execution result</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/coder.py</code> <pre><code>class TestResult(BaseModel):\n    \"\"\"Individual test execution result\"\"\"\n    passed: bool\n    message: str\n    details: dict[str, str]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.coder.run_tests","title":"<code>run_tests(test_files, impl_files)</code>  <code>async</code>","text":"<p>Execute tests using pytest and return results</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/coder.py</code> <pre><code>async def run_tests(test_files: list[CodeFile], impl_files: list[CodeFile]) -&gt; list[TestResult]:\n    \"\"\"Execute tests using pytest and return results\"\"\"\n    try:\n        if len(test_files + impl_files) == 0:\n            return []\n        with tempfile.TemporaryDirectory() as tmpdir:\n            # Save all files preserving their structure\n            for file in test_files + impl_files:\n                path = os.path.join(tmpdir, file.path)\n                os.makedirs(os.path.dirname(path), exist_ok=True)\n                with open(path, 'w') as f:\n                    f.write(file.content)\n\n            # Run pytest with JSON report\n            report_path = os.path.join(tmpdir, 'report.json')\n            cmd = [\n                sys.executable, '-m'\n                'pytest',\n                '--json-report',\n                '--json-report-file', report_path,\n                '-v',\n                tmpdir\n            ]\n\n            proc = await asyncio.create_subprocess_exec(\n                *cmd,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n\n            stdout, stderr = await proc.communicate()\n\n            if proc.returncode not in (0, 1):  # 1 is test failures, other codes are pytest errors\n                raise Exception(f\"Pytest execution failed: {stderr.decode()}\")\n\n            # Parse JSON report\n            if not os.path.exists(report_path):\n                raise Exception(\"Test report not generated\")\n\n            with open(report_path) as f:\n                report = json.load(f)\n\n            results = []\n            for test in report.get('tests', []):\n                passed = test['outcome'] == 'passed'\n                error_data = test.get('call', {}).get('crash', {})\n\n                result = TestResult(\n                    passed=passed,\n                    message=error_data.get('message', 'Test passed' if passed else 'Test failed'),\n                    details={\n                        'test_id': test['nodeid'],\n                        'stdout': test.get('stdout', ''),\n                        'stderr': test.get('stderr', ''),\n                        'error': error_data.get('traceback', '') if not passed else ''\n                    },\n                    duration_ms=float(test['duration']) * 1000\n                )\n                results.append(result)\n\n            return results\n\n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"pytest not found. Please install pytest and pytest-json-report : {str(e)}\")\n    except Exception as e:\n        raise Exception(f\"Error running tests: {str(e)}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live","title":"<code>live</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.AsyncCodeDetector","title":"<code>AsyncCodeDetector</code>","text":"<p>               Bases: <code>NodeVisitor</code></p> <p>Detect async code and top-level await</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class AsyncCodeDetector(ast.NodeVisitor):\n    \"\"\"Detect async code and top-level await\"\"\"\n    def __init__(self):\n        self.has_async = False\n        self.has_top_level_await = False\n        self.await_nodes = []\n\n    def visit_AsyncFunctionDef(self, node):\n        self.has_async = True\n        self.generic_visit(node)\n\n    def visit_Await(self, node):\n        self.has_async = True\n        # Track all await nodes\n        self.await_nodes.append(node)\n        # Check if this await is at top level\n        parent = node\n        while hasattr(parent, 'parent'):\n            parent = parent.parent\n            if isinstance(parent, ast.AsyncFunctionDef | ast.FunctionDef):\n                break\n        else:\n            self.has_top_level_await = True\n        self.generic_visit(node)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.BrowserWrapper","title":"<code>BrowserWrapper</code>","text":"<p>A wrapper for browser agent functionality that allows seamless interaction with web browsers.</p> <p>This class provides a system-agnostic interface to control browsers through the browser_use library, supporting both local and remote browser connections.</p> <p>Attributes:</p> Name Type Description <code>browser</code> <p>The Browser instance for web automation</p> <code>agent</code> <p>The BrowserAgent instance for intelligent browsing</p> <code>is_initialized</code> <code>bool</code> <p>Whether the browser has been initialized</p> <code>config</code> <code>Dict</code> <p>Configuration for the browser</p> <code>remote_url</code> <code>Optional[str]</code> <p>URL for remote browser connection if applicable</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class BrowserWrapper:\n    \"\"\"\n    A wrapper for browser agent functionality that allows seamless interaction with web browsers.\n\n    This class provides a system-agnostic interface to control browsers through the browser_use\n    library, supporting both local and remote browser connections.\n\n    Attributes:\n        browser: The Browser instance for web automation\n        agent: The BrowserAgent instance for intelligent browsing\n        is_initialized (bool): Whether the browser has been initialized\n        config (Dict): Configuration for the browser\n        remote_url (Optional[str]): URL for remote browser connection if applicable\n    \"\"\"\n\n    def __init__(self,\n                 llm: Any = None,\n                 headless: bool = False,\n                 chrome_path: str | None = None,\n                 remote_url: str | None = None,\n                 api_key: str | None=None,\n                 config: dict[str, Any] | None = None):\n        \"\"\"\n        Initialize the browser wrapper.\n\n        Args:\n            llm: Language model to use for the browser agent\n            headless: Whether to run the browser in headless mode\n            chrome_path: Path to local Chrome executable\n            remote_url: URL for remote browser connection (wss or cdp)\n            config: Additional browser configuration\n        \"\"\"\n        self.is_initialized = False\n        self.agent = None\n        self.browser = None\n        self.context = None\n        import os\n\n        from pydantic import SecretStr\n        def pars(x):\n            return x.split('/')[-1] if '/' in x else x\n        if llm is None:\n            llm = 'google/gemini-2.0-flash-exp'\n        if not isinstance(llm, str):\n            llm = llm\n        elif 'deepseek' in llm:\n            from langchain_openai import ChatOpenAI\n            llm = ChatOpenAI(base_url='https://api.deepseek.com/v1', model=pars(llm), api_key=SecretStr(api_key or os.getenv('DEEPSEEK_API_KEY')))\n        elif 'google' in llm:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n            llm = ChatGoogleGenerativeAI(model=pars(llm), api_key=SecretStr(api_key or os.getenv('GEMINI_API_KEY')))\n        elif 'claude' in llm:\n            from langchain_anthropic import ChatAnthropic\n            llm = ChatAnthropic(\n                model_name=pars(llm),\n                temperature=0.0,\n                timeout=400,  # Increase for complex tasks\n                api_key=SecretStr(api_key or os.getenv('ANTHROPIC_API_KEY')))\n        elif isinstance(llm, str):\n            from langchain_openai import ChatOpenAI\n            llm = ChatOpenAI(\n                model=pars(llm),\n                temperature=0.0,api_key=SecretStr(api_key or os.getenv('OPENAI_API_KEY'))\n            )\n\n\n\n        self.llm = ChatLiteLLM(model=llm) if isinstance(llm,str) else llm\n        self.parser = None\n\n        browser_config = {\n            'headless': headless,\n            'disable_security': True\n        }\n\n        if config:\n            browser_config.update(config)\n\n        self.config = browser_config\n\n        # Set up remote connection if specified\n        if remote_url:\n            if remote_url.startswith('wss://'):\n                self.config['wss_url'] = remote_url\n            elif remote_url.startswith('http'):\n                self.config['cdp_url'] = remote_url\n            self.remote_url = remote_url\n        else:\n            self.remote_url = None\n\n        # Set up local Chrome path if specified\n        if not headless and remote_url is None and chrome_path is None:\n            import os\n            import platform\n\n            def get_chrome_path():\n                \"\"\"\n                Returns the correct path to the Chrome executable based on the OS.\n                If Chrome is not found, returns None.\n                \"\"\"\n                chrome_paths = {\n                    \"Darwin\": \"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\",  # macOS\n                    \"Windows\": \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\",  # Windows\n                    \"Linux\": \"/usr/bin/google-chrome\"  # Linux\n                }\n\n                system = platform.system()\n                chrome_path_ = chrome_paths.get(system)\n\n                if chrome_path_ and os.path.isfile(chrome_path_):\n                    return chrome_path_\n\n                return None\n\n            chrome_path = get_chrome_path()\n        if chrome_path:\n            self.config['chrome_instance_path'] = chrome_path\n\n\n    async def initialize(self):\n        \"\"\"Initialize the browser and context\"\"\"\n        if self.is_initialized:\n            return\n\n        try:\n            # Create browser instance\n            self.browser = Browser(\n                config=BrowserConfig(**self.config)\n            )\n\n            # Create context configuration with better settings for scraping\n            context_config = BrowserContextConfig(\n                wait_for_network_idle_page_load_time=3.0,\n                highlight_elements=True,\n                viewport_expansion=500,\n                wait_between_actions=0.5  # Add a small delay between actions\n            )\n\n            # Initialize context\n            self.context = await self.browser.new_context(config=context_config)\n\n            # Create an initial page\n            browser_state = await self.context.get_state()\n            if not browser_state or not browser_state.tabs:\n                # If no tabs exist, create a new page\n                await self.browser.get_playwright_browser()\n                browser_context = await self.context.get_playwright_context()\n                self.page = await browser_context.new_page()\n            else:\n                # Use the existing active tab\n                self.page = await self.context.get_current_page()\n\n            self.is_initialized = True\n\n        except Exception as e:\n            # Clean up resources in case of initialization error\n            if self.context:\n                await self.context.close()\n            if self.browser:\n                await self.browser.close()\n            raise Exception(f\"Failed to initialize browser: {str(e)}\")\n\n    async def create_agent(self, task: str, initial_actions=None):\n        \"\"\"Create a browser agent with the specified task\"\"\"\n        #if not self.is_initialized:\n        #    await self.initialize()\n\n        self.agent = BrowserAgent(\n            task=task,\n            llm=self.llm,\n            #browser_context=self.context,\n            initial_actions=initial_actions,\n            #browser=self.browser,\n        )\n        return self.agent\n\n    async def run(self, task: str):\n        \"\"\"Run the browser agent with the specified task\"\"\"\n        agent = await self.create_agent(task)\n        result = await agent.run()\n        return result\n\n    async def navigate(self, url: str):\n        \"\"\"Navigate to a URL\"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        # Get the current active page or create a new one if needed\n        try:\n            page = await self.context.get_current_page()\n            if not page:\n                browser_context = await self.context.get_playwright_context()\n                page = await browser_context.new_page()\n\n            # Navigate to the URL\n            await page.goto(url)\n            self.page = page\n            return page\n        except Exception as e:\n            raise Exception(f\"Failed to navigate to {url}: {str(e)}\")\n\n    async def get_tabs(self):\n        \"\"\"Get all open tabs/pages\"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        browser_state = await self.context.get_state()\n        return browser_state.tabs if browser_state else []\n\n    async def switch_to_tab(self, tab_index: int):\n        \"\"\"Switch to a specific tab by index\"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        browser_state = await self.context.get_state()\n        if not browser_state or not browser_state.tabs or tab_index &gt;= len(browser_state.tabs):\n            raise ValueError(f\"Tab index {tab_index} is out of range\")\n\n        tab_id = browser_state.tabs[tab_index].id\n        await self.context.switch_to_tab(tab_id)\n        self.page = await self.context.get_current_page()\n        return self.page\n\n    async def create_new_tab(self):\n        \"\"\"Create a new tab/page\"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        browser_context = await self.context.get_playwright_context()\n        new_page = await browser_context.new_page()\n        self.page = new_page\n        return new_page\n\n    async def close_current_tab(self):\n        \"\"\"Close the current tab/page\"\"\"\n        if not self.is_initialized:\n            return\n\n        page = await self.context.get_current_page()\n        if page:\n            await page.close()\n\n        # Update the current page reference\n        browser_state = await self.context.get_state()\n        if browser_state and browser_state.tabs:\n            await self.switch_to_tab(0)\n\n    async def execute_js(self, code: str, page=None):\n        \"\"\"Execute JavaScript code in the browser context\"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        if page is None:\n            pages = await self.context.pages()\n            if not pages:\n                page = await self.context.new_page()\n            else:\n                page = pages[0]\n\n        result = await page.evaluate(code)\n        return result\n\n    async def save_context(self):\n        \"\"\"Save browser context state\"\"\"\n        if not self.is_initialized:\n            return None\n\n        return await self.browser.export_context(self.context)\n\n    async def restore_context(self, context_data):\n        \"\"\"Restore browser context from saved state\"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        await self.browser.import_context(context_data)\n\n    async def close(self):\n        \"\"\"Close the browser\"\"\"\n        if self.is_initialized and self.browser:\n            await self.browser.close()\n            self.is_initialized = False\n\n    # Add these methods to the BrowserWrapper class\n\n    def get_parser(self):\n        \"\"\"Get a content parser for the browser\"\"\"\n        if self.parser is None:\n            self.parser = WebContentParser(self)\n        return self.parser\n\n    async def extract_markdown(self, page=None, selector=\"body\", include_images=True):\n        \"\"\"\n        Extract content from a webpage and convert it to markdown.\n        \"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        if page is None:\n            pages = await self.context.pages()\n            if not pages:\n                page = await self.context.new_page()\n            else:\n                page = pages[0]\n\n        # JavaScript to convert HTML to markdown\n        script = \"\"\"\n        (selector, includeImages) =&gt; {\n            const element = document.querySelector(selector);\n            if (!element) return '';\n\n            // Simple HTML to Markdown conversion function\n            const htmlToMarkdown = (node) =&gt; {\n                let result = '';\n\n                // Process text nodes\n                if (node.nodeType === Node.TEXT_NODE) {\n                    return node.textContent;\n                }\n\n                // Process element nodes\n                if (node.nodeType === Node.ELEMENT_NODE) {\n                    const tagName = node.tagName.toLowerCase();\n\n                    // Process by tag type\n                    switch(tagName) {\n                        case 'h1': return '# ' + getInnerText(node) + '\\\\n\\\\n';\n                        case 'h2': return '## ' + getInnerText(node) + '\\\\n\\\\n';\n                        case 'h3': return '### ' + getInnerText(node) + '\\\\n\\\\n';\n                        case 'h4': return '#### ' + getInnerText(node) + '\\\\n\\\\n';\n                        case 'h5': return '##### ' + getInnerText(node) + '\\\\n\\\\n';\n                        case 'h6': return '###### ' + getInnerText(node) + '\\\\n\\\\n';\n                        case 'p': return getInnerText(node) + '\\\\n\\\\n';\n                        case 'br': return '\\\\n';\n                        case 'hr': return '---\\\\n\\\\n';\n                        case 'b':\n                        case 'strong': return '**' + getInnerText(node) + '**';\n                        case 'i':\n                        case 'em': return '*' + getInnerText(node) + '*';\n                        case 'a': {\n                            const href = node.getAttribute('href');\n                            return '[' + getInnerText(node) + '](' + href + ')';\n                        }\n                        case 'img': {\n                            if (!includeImages) return '';\n                            const src = node.getAttribute('src');\n                            const alt = node.getAttribute('alt') || 'image';\n                            return '![' + alt + '](' + src + ')\\\\n\\\\n';\n                        }\n                        case 'code':\n                        case 'pre': return '`' + getInnerText(node) + '`';\n                        case 'ul': {\n                            let listResult = '\\\\n';\n                            Array.from(node.children).forEach(li =&gt; {\n                                if (li.tagName.toLowerCase() === 'li') {\n                                    listResult += '- ' + getInnerText(li) + '\\\\n';\n                                }\n                            });\n                            return listResult + '\\\\n';\n                        }\n                        case 'ol': {\n                            let listResult = '\\\\n';\n                            Array.from(node.children).forEach((li, index) =&gt; {\n                                if (li.tagName.toLowerCase() === 'li') {\n                                    listResult += (index + 1) + '. ' + getInnerText(li) + '\\\\n';\n                                }\n                            });\n                            return listResult + '\\\\n';\n                        }\n                        case 'blockquote': return '&gt; ' + getInnerText(node) + '\\\\n\\\\n';\n                        default: {\n                            // Process child nodes for other elements\n                            for (const child of node.childNodes) {\n                                result += htmlToMarkdown(child);\n                            }\n                            return result;\n                        }\n                    }\n                }\n\n                return '';\n            };\n\n            // Helper function to get inner text with special handling\n            const getInnerText = (node) =&gt; {\n                let text = '';\n                for (const child of node.childNodes) {\n                    text += htmlToMarkdown(child);\n                }\n                return text;\n            };\n\n            return htmlToMarkdown(element);\n        }\n        \"\"\"\n\n        try:\n            # Try to convert to markdown using our script\n            markdown = await page.evaluate(script, selector, include_images)\n\n            # Add a title if we have one\n            title = await page.title()\n            if title and not markdown.startswith(\"# \"):\n                markdown = f\"# {title}\\n\\n{markdown}\"\n\n            return markdown\n        except Exception:\n            # Fallback to basic extraction if script fails\n            content = await self.extract_text(page, selector)\n            title = await page.title()\n            return f\"# {title}\\n\\n{content}\"\n\n    async def take_scrolling_screenshot(self, page=None, full_page=True, path=None,\n                                        initial_delay=1000, scroll_delay=500, format='png'):\n        \"\"\"\n        Take a screenshot with scrolling functionality and delay.\n        \"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        if page is None:\n            pages = await self.context.pages()\n            if not pages:\n                page = await self.context.new_page()\n            else:\n                page = pages[0]\n\n        # Wait for the initial delay to let content load\n        if initial_delay &gt; 0:\n            await page.wait_for_timeout(initial_delay)\n\n        if full_page and scroll_delay &gt; 0:\n            # Get page dimensions\n            dimensions = await page.evaluate(\"\"\"\n                () =&gt; {\n                    return {\n                        width: document.documentElement.scrollWidth,\n                        height: document.documentElement.scrollHeight,\n                        windowHeight: window.innerHeight\n                    }\n                }\n            \"\"\")\n\n            # Scroll down the page gradually to trigger lazy loading\n            current_position = 0\n            while current_position &lt; dimensions['height']:\n                await page.evaluate(f\"window.scrollTo(0, {current_position})\")\n                await page.wait_for_timeout(scroll_delay)\n                current_position += dimensions['windowHeight'] // 2  # Scroll by half viewport\n\n        # Reset scroll position to top\n        await page.evaluate(\"window.scrollTo(0, 0)\")\n\n        # Take the screenshot\n        screenshot_params = {\n            'full_page': full_page,\n            'type': format\n        }\n\n        if path:\n            screenshot_params['path'] = path\n\n        return await page.screenshot(**screenshot_params)\n\n    async def extract_text(self, page=None, selector=\"body\"):\n        \"\"\"\n        Extract plain text from a webpage.\n        \"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        if page is None:\n            pages = await self.context.pages()\n            if not pages:\n                page = await self.context.new_page()\n            else:\n                page = pages[0]\n\n        text = await page.evaluate(\"\"\"\n            (selector) =&gt; {\n                const element = document.querySelector(selector);\n                return element ? element.innerText : '';\n            }\n        \"\"\", selector)\n\n        return text\n\n    async def extract_structured_content(self, page=None, config=None):\n        \"\"\"\n        Extract structured content from a webpage based on a configuration.\n        \"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        if page is None:\n            pages = await self.context.pages()\n            if not pages:\n                page = await self.context.new_page()\n            else:\n                page = pages[0]\n\n        if not config:\n            # Default configuration if none provided\n            config = {\n                'title': 'h1',\n                'headings': 'h2, h3, h4, h5, h6',\n                'paragraphs': 'p',\n                'links': 'a',\n                'images': 'img'\n            }\n\n        result = {}\n\n        for key, selector in config.items():\n            if key == 'links':\n                # Extract links with their href and text\n                result[key] = await page.evaluate(\"\"\"\n                    (selector) =&gt; {\n                        return Array.from(document.querySelectorAll(selector))\n                            .map(el =&gt; ({\n                                text: el.innerText.trim(),\n                                href: el.href\n                            }))\n                            .filter(item =&gt; item.text &amp;&amp; item.href);\n                    }\n                \"\"\", selector)\n            elif key == 'images':\n                # Extract images with their src and alt\n                result[key] = await page.evaluate(\"\"\"\n                    (selector) =&gt; {\n                        return Array.from(document.querySelectorAll(selector))\n                            .map(el =&gt; ({\n                                src: el.src,\n                                alt: el.alt || ''\n                            }))\n                            .filter(item =&gt; item.src);\n                    }\n                \"\"\", selector)\n            else:\n                # Extract text content for other elements\n                result[key] = await page.evaluate(\"\"\"\n                    (selector) =&gt; {\n                        return Array.from(document.querySelectorAll(selector))\n                            .map(el =&gt; el.innerText.trim())\n                            .filter(text =&gt; text);\n                    }\n                \"\"\", selector)\n\n        return result\n</code></pre> <code>__init__(llm=None, headless=False, chrome_path=None, remote_url=None, api_key=None, config=None)</code> \u00b6 <p>Initialize the browser wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>Any</code> <p>Language model to use for the browser agent</p> <code>None</code> <code>headless</code> <code>bool</code> <p>Whether to run the browser in headless mode</p> <code>False</code> <code>chrome_path</code> <code>str | None</code> <p>Path to local Chrome executable</p> <code>None</code> <code>remote_url</code> <code>str | None</code> <p>URL for remote browser connection (wss or cdp)</p> <code>None</code> <code>config</code> <code>dict[str, Any] | None</code> <p>Additional browser configuration</p> <code>None</code> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __init__(self,\n             llm: Any = None,\n             headless: bool = False,\n             chrome_path: str | None = None,\n             remote_url: str | None = None,\n             api_key: str | None=None,\n             config: dict[str, Any] | None = None):\n    \"\"\"\n    Initialize the browser wrapper.\n\n    Args:\n        llm: Language model to use for the browser agent\n        headless: Whether to run the browser in headless mode\n        chrome_path: Path to local Chrome executable\n        remote_url: URL for remote browser connection (wss or cdp)\n        config: Additional browser configuration\n    \"\"\"\n    self.is_initialized = False\n    self.agent = None\n    self.browser = None\n    self.context = None\n    import os\n\n    from pydantic import SecretStr\n    def pars(x):\n        return x.split('/')[-1] if '/' in x else x\n    if llm is None:\n        llm = 'google/gemini-2.0-flash-exp'\n    if not isinstance(llm, str):\n        llm = llm\n    elif 'deepseek' in llm:\n        from langchain_openai import ChatOpenAI\n        llm = ChatOpenAI(base_url='https://api.deepseek.com/v1', model=pars(llm), api_key=SecretStr(api_key or os.getenv('DEEPSEEK_API_KEY')))\n    elif 'google' in llm:\n        from langchain_google_genai import ChatGoogleGenerativeAI\n        llm = ChatGoogleGenerativeAI(model=pars(llm), api_key=SecretStr(api_key or os.getenv('GEMINI_API_KEY')))\n    elif 'claude' in llm:\n        from langchain_anthropic import ChatAnthropic\n        llm = ChatAnthropic(\n            model_name=pars(llm),\n            temperature=0.0,\n            timeout=400,  # Increase for complex tasks\n            api_key=SecretStr(api_key or os.getenv('ANTHROPIC_API_KEY')))\n    elif isinstance(llm, str):\n        from langchain_openai import ChatOpenAI\n        llm = ChatOpenAI(\n            model=pars(llm),\n            temperature=0.0,api_key=SecretStr(api_key or os.getenv('OPENAI_API_KEY'))\n        )\n\n\n\n    self.llm = ChatLiteLLM(model=llm) if isinstance(llm,str) else llm\n    self.parser = None\n\n    browser_config = {\n        'headless': headless,\n        'disable_security': True\n    }\n\n    if config:\n        browser_config.update(config)\n\n    self.config = browser_config\n\n    # Set up remote connection if specified\n    if remote_url:\n        if remote_url.startswith('wss://'):\n            self.config['wss_url'] = remote_url\n        elif remote_url.startswith('http'):\n            self.config['cdp_url'] = remote_url\n        self.remote_url = remote_url\n    else:\n        self.remote_url = None\n\n    # Set up local Chrome path if specified\n    if not headless and remote_url is None and chrome_path is None:\n        import os\n        import platform\n\n        def get_chrome_path():\n            \"\"\"\n            Returns the correct path to the Chrome executable based on the OS.\n            If Chrome is not found, returns None.\n            \"\"\"\n            chrome_paths = {\n                \"Darwin\": \"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\",  # macOS\n                \"Windows\": \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\",  # Windows\n                \"Linux\": \"/usr/bin/google-chrome\"  # Linux\n            }\n\n            system = platform.system()\n            chrome_path_ = chrome_paths.get(system)\n\n            if chrome_path_ and os.path.isfile(chrome_path_):\n                return chrome_path_\n\n            return None\n\n        chrome_path = get_chrome_path()\n    if chrome_path:\n        self.config['chrome_instance_path'] = chrome_path\n</code></pre> <code>close()</code> <code>async</code> \u00b6 <p>Close the browser</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def close(self):\n    \"\"\"Close the browser\"\"\"\n    if self.is_initialized and self.browser:\n        await self.browser.close()\n        self.is_initialized = False\n</code></pre> <code>close_current_tab()</code> <code>async</code> \u00b6 <p>Close the current tab/page</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def close_current_tab(self):\n    \"\"\"Close the current tab/page\"\"\"\n    if not self.is_initialized:\n        return\n\n    page = await self.context.get_current_page()\n    if page:\n        await page.close()\n\n    # Update the current page reference\n    browser_state = await self.context.get_state()\n    if browser_state and browser_state.tabs:\n        await self.switch_to_tab(0)\n</code></pre> <code>create_agent(task, initial_actions=None)</code> <code>async</code> \u00b6 <p>Create a browser agent with the specified task</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def create_agent(self, task: str, initial_actions=None):\n    \"\"\"Create a browser agent with the specified task\"\"\"\n    #if not self.is_initialized:\n    #    await self.initialize()\n\n    self.agent = BrowserAgent(\n        task=task,\n        llm=self.llm,\n        #browser_context=self.context,\n        initial_actions=initial_actions,\n        #browser=self.browser,\n    )\n    return self.agent\n</code></pre> <code>create_new_tab()</code> <code>async</code> \u00b6 <p>Create a new tab/page</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def create_new_tab(self):\n    \"\"\"Create a new tab/page\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    browser_context = await self.context.get_playwright_context()\n    new_page = await browser_context.new_page()\n    self.page = new_page\n    return new_page\n</code></pre> <code>execute_js(code, page=None)</code> <code>async</code> \u00b6 <p>Execute JavaScript code in the browser context</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def execute_js(self, code: str, page=None):\n    \"\"\"Execute JavaScript code in the browser context\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    if page is None:\n        pages = await self.context.pages()\n        if not pages:\n            page = await self.context.new_page()\n        else:\n            page = pages[0]\n\n    result = await page.evaluate(code)\n    return result\n</code></pre> <code>extract_markdown(page=None, selector='body', include_images=True)</code> <code>async</code> \u00b6 <p>Extract content from a webpage and convert it to markdown.</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def extract_markdown(self, page=None, selector=\"body\", include_images=True):\n    \"\"\"\n    Extract content from a webpage and convert it to markdown.\n    \"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    if page is None:\n        pages = await self.context.pages()\n        if not pages:\n            page = await self.context.new_page()\n        else:\n            page = pages[0]\n\n    # JavaScript to convert HTML to markdown\n    script = \"\"\"\n    (selector, includeImages) =&gt; {\n        const element = document.querySelector(selector);\n        if (!element) return '';\n\n        // Simple HTML to Markdown conversion function\n        const htmlToMarkdown = (node) =&gt; {\n            let result = '';\n\n            // Process text nodes\n            if (node.nodeType === Node.TEXT_NODE) {\n                return node.textContent;\n            }\n\n            // Process element nodes\n            if (node.nodeType === Node.ELEMENT_NODE) {\n                const tagName = node.tagName.toLowerCase();\n\n                // Process by tag type\n                switch(tagName) {\n                    case 'h1': return '# ' + getInnerText(node) + '\\\\n\\\\n';\n                    case 'h2': return '## ' + getInnerText(node) + '\\\\n\\\\n';\n                    case 'h3': return '### ' + getInnerText(node) + '\\\\n\\\\n';\n                    case 'h4': return '#### ' + getInnerText(node) + '\\\\n\\\\n';\n                    case 'h5': return '##### ' + getInnerText(node) + '\\\\n\\\\n';\n                    case 'h6': return '###### ' + getInnerText(node) + '\\\\n\\\\n';\n                    case 'p': return getInnerText(node) + '\\\\n\\\\n';\n                    case 'br': return '\\\\n';\n                    case 'hr': return '---\\\\n\\\\n';\n                    case 'b':\n                    case 'strong': return '**' + getInnerText(node) + '**';\n                    case 'i':\n                    case 'em': return '*' + getInnerText(node) + '*';\n                    case 'a': {\n                        const href = node.getAttribute('href');\n                        return '[' + getInnerText(node) + '](' + href + ')';\n                    }\n                    case 'img': {\n                        if (!includeImages) return '';\n                        const src = node.getAttribute('src');\n                        const alt = node.getAttribute('alt') || 'image';\n                        return '![' + alt + '](' + src + ')\\\\n\\\\n';\n                    }\n                    case 'code':\n                    case 'pre': return '`' + getInnerText(node) + '`';\n                    case 'ul': {\n                        let listResult = '\\\\n';\n                        Array.from(node.children).forEach(li =&gt; {\n                            if (li.tagName.toLowerCase() === 'li') {\n                                listResult += '- ' + getInnerText(li) + '\\\\n';\n                            }\n                        });\n                        return listResult + '\\\\n';\n                    }\n                    case 'ol': {\n                        let listResult = '\\\\n';\n                        Array.from(node.children).forEach((li, index) =&gt; {\n                            if (li.tagName.toLowerCase() === 'li') {\n                                listResult += (index + 1) + '. ' + getInnerText(li) + '\\\\n';\n                            }\n                        });\n                        return listResult + '\\\\n';\n                    }\n                    case 'blockquote': return '&gt; ' + getInnerText(node) + '\\\\n\\\\n';\n                    default: {\n                        // Process child nodes for other elements\n                        for (const child of node.childNodes) {\n                            result += htmlToMarkdown(child);\n                        }\n                        return result;\n                    }\n                }\n            }\n\n            return '';\n        };\n\n        // Helper function to get inner text with special handling\n        const getInnerText = (node) =&gt; {\n            let text = '';\n            for (const child of node.childNodes) {\n                text += htmlToMarkdown(child);\n            }\n            return text;\n        };\n\n        return htmlToMarkdown(element);\n    }\n    \"\"\"\n\n    try:\n        # Try to convert to markdown using our script\n        markdown = await page.evaluate(script, selector, include_images)\n\n        # Add a title if we have one\n        title = await page.title()\n        if title and not markdown.startswith(\"# \"):\n            markdown = f\"# {title}\\n\\n{markdown}\"\n\n        return markdown\n    except Exception:\n        # Fallback to basic extraction if script fails\n        content = await self.extract_text(page, selector)\n        title = await page.title()\n        return f\"# {title}\\n\\n{content}\"\n</code></pre> <code>extract_structured_content(page=None, config=None)</code> <code>async</code> \u00b6 <p>Extract structured content from a webpage based on a configuration.</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def extract_structured_content(self, page=None, config=None):\n    \"\"\"\n    Extract structured content from a webpage based on a configuration.\n    \"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    if page is None:\n        pages = await self.context.pages()\n        if not pages:\n            page = await self.context.new_page()\n        else:\n            page = pages[0]\n\n    if not config:\n        # Default configuration if none provided\n        config = {\n            'title': 'h1',\n            'headings': 'h2, h3, h4, h5, h6',\n            'paragraphs': 'p',\n            'links': 'a',\n            'images': 'img'\n        }\n\n    result = {}\n\n    for key, selector in config.items():\n        if key == 'links':\n            # Extract links with their href and text\n            result[key] = await page.evaluate(\"\"\"\n                (selector) =&gt; {\n                    return Array.from(document.querySelectorAll(selector))\n                        .map(el =&gt; ({\n                            text: el.innerText.trim(),\n                            href: el.href\n                        }))\n                        .filter(item =&gt; item.text &amp;&amp; item.href);\n                }\n            \"\"\", selector)\n        elif key == 'images':\n            # Extract images with their src and alt\n            result[key] = await page.evaluate(\"\"\"\n                (selector) =&gt; {\n                    return Array.from(document.querySelectorAll(selector))\n                        .map(el =&gt; ({\n                            src: el.src,\n                            alt: el.alt || ''\n                        }))\n                        .filter(item =&gt; item.src);\n                }\n            \"\"\", selector)\n        else:\n            # Extract text content for other elements\n            result[key] = await page.evaluate(\"\"\"\n                (selector) =&gt; {\n                    return Array.from(document.querySelectorAll(selector))\n                        .map(el =&gt; el.innerText.trim())\n                        .filter(text =&gt; text);\n                }\n            \"\"\", selector)\n\n    return result\n</code></pre> <code>extract_text(page=None, selector='body')</code> <code>async</code> \u00b6 <p>Extract plain text from a webpage.</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def extract_text(self, page=None, selector=\"body\"):\n    \"\"\"\n    Extract plain text from a webpage.\n    \"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    if page is None:\n        pages = await self.context.pages()\n        if not pages:\n            page = await self.context.new_page()\n        else:\n            page = pages[0]\n\n    text = await page.evaluate(\"\"\"\n        (selector) =&gt; {\n            const element = document.querySelector(selector);\n            return element ? element.innerText : '';\n        }\n    \"\"\", selector)\n\n    return text\n</code></pre> <code>get_parser()</code> \u00b6 <p>Get a content parser for the browser</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def get_parser(self):\n    \"\"\"Get a content parser for the browser\"\"\"\n    if self.parser is None:\n        self.parser = WebContentParser(self)\n    return self.parser\n</code></pre> <code>get_tabs()</code> <code>async</code> \u00b6 <p>Get all open tabs/pages</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def get_tabs(self):\n    \"\"\"Get all open tabs/pages\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    browser_state = await self.context.get_state()\n    return browser_state.tabs if browser_state else []\n</code></pre> <code>initialize()</code> <code>async</code> \u00b6 <p>Initialize the browser and context</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def initialize(self):\n    \"\"\"Initialize the browser and context\"\"\"\n    if self.is_initialized:\n        return\n\n    try:\n        # Create browser instance\n        self.browser = Browser(\n            config=BrowserConfig(**self.config)\n        )\n\n        # Create context configuration with better settings for scraping\n        context_config = BrowserContextConfig(\n            wait_for_network_idle_page_load_time=3.0,\n            highlight_elements=True,\n            viewport_expansion=500,\n            wait_between_actions=0.5  # Add a small delay between actions\n        )\n\n        # Initialize context\n        self.context = await self.browser.new_context(config=context_config)\n\n        # Create an initial page\n        browser_state = await self.context.get_state()\n        if not browser_state or not browser_state.tabs:\n            # If no tabs exist, create a new page\n            await self.browser.get_playwright_browser()\n            browser_context = await self.context.get_playwright_context()\n            self.page = await browser_context.new_page()\n        else:\n            # Use the existing active tab\n            self.page = await self.context.get_current_page()\n\n        self.is_initialized = True\n\n    except Exception as e:\n        # Clean up resources in case of initialization error\n        if self.context:\n            await self.context.close()\n        if self.browser:\n            await self.browser.close()\n        raise Exception(f\"Failed to initialize browser: {str(e)}\")\n</code></pre> <code>navigate(url)</code> <code>async</code> \u00b6 <p>Navigate to a URL</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def navigate(self, url: str):\n    \"\"\"Navigate to a URL\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    # Get the current active page or create a new one if needed\n    try:\n        page = await self.context.get_current_page()\n        if not page:\n            browser_context = await self.context.get_playwright_context()\n            page = await browser_context.new_page()\n\n        # Navigate to the URL\n        await page.goto(url)\n        self.page = page\n        return page\n    except Exception as e:\n        raise Exception(f\"Failed to navigate to {url}: {str(e)}\")\n</code></pre> <code>restore_context(context_data)</code> <code>async</code> \u00b6 <p>Restore browser context from saved state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def restore_context(self, context_data):\n    \"\"\"Restore browser context from saved state\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    await self.browser.import_context(context_data)\n</code></pre> <code>run(task)</code> <code>async</code> \u00b6 <p>Run the browser agent with the specified task</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def run(self, task: str):\n    \"\"\"Run the browser agent with the specified task\"\"\"\n    agent = await self.create_agent(task)\n    result = await agent.run()\n    return result\n</code></pre> <code>save_context()</code> <code>async</code> \u00b6 <p>Save browser context state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def save_context(self):\n    \"\"\"Save browser context state\"\"\"\n    if not self.is_initialized:\n        return None\n\n    return await self.browser.export_context(self.context)\n</code></pre> <code>switch_to_tab(tab_index)</code> <code>async</code> \u00b6 <p>Switch to a specific tab by index</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def switch_to_tab(self, tab_index: int):\n    \"\"\"Switch to a specific tab by index\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    browser_state = await self.context.get_state()\n    if not browser_state or not browser_state.tabs or tab_index &gt;= len(browser_state.tabs):\n        raise ValueError(f\"Tab index {tab_index} is out of range\")\n\n    tab_id = browser_state.tabs[tab_index].id\n    await self.context.switch_to_tab(tab_id)\n    self.page = await self.context.get_current_page()\n    return self.page\n</code></pre> <code>take_scrolling_screenshot(page=None, full_page=True, path=None, initial_delay=1000, scroll_delay=500, format='png')</code> <code>async</code> \u00b6 <p>Take a screenshot with scrolling functionality and delay.</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def take_scrolling_screenshot(self, page=None, full_page=True, path=None,\n                                    initial_delay=1000, scroll_delay=500, format='png'):\n    \"\"\"\n    Take a screenshot with scrolling functionality and delay.\n    \"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    if page is None:\n        pages = await self.context.pages()\n        if not pages:\n            page = await self.context.new_page()\n        else:\n            page = pages[0]\n\n    # Wait for the initial delay to let content load\n    if initial_delay &gt; 0:\n        await page.wait_for_timeout(initial_delay)\n\n    if full_page and scroll_delay &gt; 0:\n        # Get page dimensions\n        dimensions = await page.evaluate(\"\"\"\n            () =&gt; {\n                return {\n                    width: document.documentElement.scrollWidth,\n                    height: document.documentElement.scrollHeight,\n                    windowHeight: window.innerHeight\n                }\n            }\n        \"\"\")\n\n        # Scroll down the page gradually to trigger lazy loading\n        current_position = 0\n        while current_position &lt; dimensions['height']:\n            await page.evaluate(f\"window.scrollTo(0, {current_position})\")\n            await page.wait_for_timeout(scroll_delay)\n            current_position += dimensions['windowHeight'] // 2  # Scroll by half viewport\n\n    # Reset scroll position to top\n    await page.evaluate(\"window.scrollTo(0, 0)\")\n\n    # Take the screenshot\n    screenshot_params = {\n        'full_page': full_page,\n        'type': format\n    }\n\n    if path:\n        screenshot_params['path'] = path\n\n    return await page.screenshot(**screenshot_params)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface","title":"<code>CargoRustInterface</code>","text":"<p>Usage :</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface--create-interface","title":"Create interface","text":"<p>cargo_interface = CargoRustInterface()</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface--set-up-new-project","title":"Set up new project","text":"<p>await cargo_interface.setup_project(\"hello_rust\")</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface--add-a-dependency","title":"Add a dependency","text":"<p>await cargo_interface.add_dependency(\"serde\", \"1.0\")</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface--write-and-run-some-code","title":"Write and run some code","text":"<p>code = \"\"\" fn main() {     println!(\"Hello, Rust!\"); } \"\"\" result = await cargo_interface.run_code(code)</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface--modify-code","title":"Modify code","text":"<p>new_function = \"\"\" fn main() {     println!(\"Modified Hello, Rust!\"); } \"\"\" await cargo_interface.modify_code(new_function, \"main()\")</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface--build-and-test","title":"Build and test","text":"<p>await cargo_interface.build() await cargo_interface.test()</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class CargoRustInterface:\n    '''Usage :\n# Create interface\ncargo_interface = CargoRustInterface()\n\n# Set up new project\nawait cargo_interface.setup_project(\"hello_rust\")\n\n# Add a dependency\nawait cargo_interface.add_dependency(\"serde\", \"1.0\")\n\n# Write and run some code\ncode = \"\"\"\nfn main() {\n    println!(\"Hello, Rust!\");\n}\n\"\"\"\nresult = await cargo_interface.run_code(code)\n\n# Modify code\nnew_function = \"\"\"\nfn main() {\n    println!(\"Modified Hello, Rust!\");\n}\n\"\"\"\nawait cargo_interface.modify_code(new_function, \"main()\")\n\n# Build and test\nawait cargo_interface.build()\nawait cargo_interface.test()\n\n    '''\n    def __init__(self, session_dir=None, auto_remove=True):\n        \"\"\"Initialize the Rust/Cargo interface\"\"\"\n        self.auto_remove = auto_remove\n        self._session_dir = session_dir or Path.home() / '.cargo_sessions'\n        self._session_dir.mkdir(exist_ok=True)\n        self.vfs = VirtualFileSystem(self._session_dir / 'virtual_fs')\n        self.output_history = {}\n        self._execution_count = 0\n        self.current_project = None\n\n    def reset(self):\n        \"\"\"Reset the interface state\"\"\"\n        if self.auto_remove and self.current_project:\n            shutil.rmtree(self.current_project, ignore_errors=True)\n        self.output_history.clear()\n        self._execution_count = 0\n        self.current_project = None\n\n    async def setup_project(self, name: str) -&gt; str:\n        \"\"\"Set up a new Cargo project\"\"\"\n        try:\n            project_path = self.vfs.base_dir / name\n            if project_path.exists():\n                shutil.rmtree(project_path)\n\n            result = subprocess.run(\n                ['cargo', 'new', str(project_path)],\n                capture_output=True,\n                text=True, check=True\n            )\n\n            if result.returncode != 0:\n                return f\"Error creating project: {result.stderr}\"\n\n            self.current_project = project_path\n            return f\"Created new project at {project_path}\"\n\n        except Exception as e:\n            return f\"Failed to create project: {str(e)}\"\n\n    async def add_dependency(self, name: str, version: str | None = None) -&gt; str:\n        \"\"\"Add a dependency to Cargo.toml\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            cargo_toml = self.current_project / \"Cargo.toml\"\n            if not cargo_toml.exists():\n                return \"Cargo.toml not found\"\n\n            cmd = ['cargo', 'add', name]\n            if version:\n                cmd.extend(['--vers', version])\n\n            result = subprocess.run(\n                cmd,\n                cwd=self.current_project,\n                capture_output=True,\n                text=True,check=True\n            )\n\n            return result.stdout if result.returncode == 0 else f\"Error: {result.stderr}\"\n\n        except Exception as e:\n            return f\"Failed to add dependency: {str(e)}\"\n\n    async def build(self, release: bool = False) -&gt; str:\n        \"\"\"Build the project\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            cmd = ['cargo', 'build']\n            if release:\n                cmd.append('--release')\n\n            result = subprocess.run(\n                cmd,\n                cwd=self.current_project,\n                capture_output=True,\n                text=True\n            )\n\n            return result.stdout if result.returncode == 0 else f\"Build error: {result.stderr}\"\n\n        except Exception as e:\n            return f\"Build failed: {str(e)}\"\n\n    async def test(self) -&gt; str:\n        \"\"\"Run project tests\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            result = subprocess.run(\n                ['cargo', 'test'],\n                cwd=self.current_project,\n                capture_output=True,\n                text=True, check=True\n            )\n\n            return result.stdout if result.returncode == 0 else f\"Test error: {result.stderr}\"\n\n        except Exception as e:\n            return f\"Tests failed: {str(e)}\"\n\n    async def run_code(self, code: str) -&gt; str:\n        \"\"\"Run Rust code\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            # Write code to main.rs\n            main_rs = self.current_project / \"src\" / \"main.rs\"\n            with open(main_rs, 'w') as f:\n                f.write(code)\n\n            # Build and run\n            build_result = subprocess.run(\n                ['cargo', 'build'],\n                cwd=self.current_project,\n                capture_output=True,\n                text=True\n            )\n\n            if build_result.returncode != 0:\n                return f\"Compilation error: {build_result.stderr}\"\n\n            run_result = subprocess.run(\n                ['cargo', 'run'],\n                cwd=self.current_project,\n                capture_output=True,\n                text=True\n            )\n\n            self._execution_count += 1\n            output = {\n                'code': code,\n                'stdout': run_result.stdout,\n                'stderr': run_result.stderr,\n                'result': run_result.returncode == 0\n            }\n            self.output_history[self._execution_count] = output\n\n            return run_result.stdout if run_result.returncode == 0 else f\"Runtime error: {run_result.stderr}\"\n\n        except Exception as e:\n            return f\"Execution failed: {str(e)}\"\n\n    async def modify_code(self, code: str, object_name: str, file: str = \"src/main.rs\") -&gt; str:\n        \"\"\"Modify existing Rust code\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            file_path = self.current_project / file\n            if not file_path.exists():\n                return f\"File {file} not found\"\n\n            with open(file_path) as f:\n                content = f.read()\n\n            # Handle function modification\n            if object_name.endswith(\"()\"):\n                func_name = object_name[:-2]\n                # Find and replace function definition\n                pattern = f\"fn {func_name}.*?}}(?=\\n|$)\"\n                updated_content = re.sub(pattern, code.strip(), content, flags=re.DOTALL)\n            else:\n                # Handle other modifications (structs, constants, etc.)\n                pattern = f\"{object_name}.*?(?=\\n|$)\"\n                updated_content = re.sub(pattern, code.strip(), content)\n\n            with open(file_path, 'w') as f:\n                f.write(updated_content)\n\n            return f\"Modified {object_name} in {file}\"\n\n        except Exception as e:\n            return f\"Modification failed: {str(e)}\"\n\n    def save_session(self, name: str):\n        \"\"\"Save current session state\"\"\"\n        session_file = self._session_dir / f\"{name}.json\"\n        state = {\n            'output_history': self.output_history,\n            'current_project': str(self.current_project) if self.current_project else None\n        }\n\n        with open(session_file, 'w') as f:\n            json.dump(state, f)\n\n    def load_session(self, name: str):\n        \"\"\"Load saved session state\"\"\"\n        session_file = self._session_dir / f\"{name}.json\"\n        if session_file.exists():\n            with open(session_file) as f:\n                state = json.load(f)\n                self.output_history = state['output_history']\n                self.current_project = Path(state['current_project']) if state['current_project'] else None\n</code></pre> <code>__init__(session_dir=None, auto_remove=True)</code> \u00b6 <p>Initialize the Rust/Cargo interface</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __init__(self, session_dir=None, auto_remove=True):\n    \"\"\"Initialize the Rust/Cargo interface\"\"\"\n    self.auto_remove = auto_remove\n    self._session_dir = session_dir or Path.home() / '.cargo_sessions'\n    self._session_dir.mkdir(exist_ok=True)\n    self.vfs = VirtualFileSystem(self._session_dir / 'virtual_fs')\n    self.output_history = {}\n    self._execution_count = 0\n    self.current_project = None\n</code></pre> <code>add_dependency(name, version=None)</code> <code>async</code> \u00b6 <p>Add a dependency to Cargo.toml</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def add_dependency(self, name: str, version: str | None = None) -&gt; str:\n    \"\"\"Add a dependency to Cargo.toml\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        cargo_toml = self.current_project / \"Cargo.toml\"\n        if not cargo_toml.exists():\n            return \"Cargo.toml not found\"\n\n        cmd = ['cargo', 'add', name]\n        if version:\n            cmd.extend(['--vers', version])\n\n        result = subprocess.run(\n            cmd,\n            cwd=self.current_project,\n            capture_output=True,\n            text=True,check=True\n        )\n\n        return result.stdout if result.returncode == 0 else f\"Error: {result.stderr}\"\n\n    except Exception as e:\n        return f\"Failed to add dependency: {str(e)}\"\n</code></pre> <code>build(release=False)</code> <code>async</code> \u00b6 <p>Build the project</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def build(self, release: bool = False) -&gt; str:\n    \"\"\"Build the project\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        cmd = ['cargo', 'build']\n        if release:\n            cmd.append('--release')\n\n        result = subprocess.run(\n            cmd,\n            cwd=self.current_project,\n            capture_output=True,\n            text=True\n        )\n\n        return result.stdout if result.returncode == 0 else f\"Build error: {result.stderr}\"\n\n    except Exception as e:\n        return f\"Build failed: {str(e)}\"\n</code></pre> <code>load_session(name)</code> \u00b6 <p>Load saved session state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def load_session(self, name: str):\n    \"\"\"Load saved session state\"\"\"\n    session_file = self._session_dir / f\"{name}.json\"\n    if session_file.exists():\n        with open(session_file) as f:\n            state = json.load(f)\n            self.output_history = state['output_history']\n            self.current_project = Path(state['current_project']) if state['current_project'] else None\n</code></pre> <code>modify_code(code, object_name, file='src/main.rs')</code> <code>async</code> \u00b6 <p>Modify existing Rust code</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def modify_code(self, code: str, object_name: str, file: str = \"src/main.rs\") -&gt; str:\n    \"\"\"Modify existing Rust code\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        file_path = self.current_project / file\n        if not file_path.exists():\n            return f\"File {file} not found\"\n\n        with open(file_path) as f:\n            content = f.read()\n\n        # Handle function modification\n        if object_name.endswith(\"()\"):\n            func_name = object_name[:-2]\n            # Find and replace function definition\n            pattern = f\"fn {func_name}.*?}}(?=\\n|$)\"\n            updated_content = re.sub(pattern, code.strip(), content, flags=re.DOTALL)\n        else:\n            # Handle other modifications (structs, constants, etc.)\n            pattern = f\"{object_name}.*?(?=\\n|$)\"\n            updated_content = re.sub(pattern, code.strip(), content)\n\n        with open(file_path, 'w') as f:\n            f.write(updated_content)\n\n        return f\"Modified {object_name} in {file}\"\n\n    except Exception as e:\n        return f\"Modification failed: {str(e)}\"\n</code></pre> <code>reset()</code> \u00b6 <p>Reset the interface state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def reset(self):\n    \"\"\"Reset the interface state\"\"\"\n    if self.auto_remove and self.current_project:\n        shutil.rmtree(self.current_project, ignore_errors=True)\n    self.output_history.clear()\n    self._execution_count = 0\n    self.current_project = None\n</code></pre> <code>run_code(code)</code> <code>async</code> \u00b6 <p>Run Rust code</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def run_code(self, code: str) -&gt; str:\n    \"\"\"Run Rust code\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        # Write code to main.rs\n        main_rs = self.current_project / \"src\" / \"main.rs\"\n        with open(main_rs, 'w') as f:\n            f.write(code)\n\n        # Build and run\n        build_result = subprocess.run(\n            ['cargo', 'build'],\n            cwd=self.current_project,\n            capture_output=True,\n            text=True\n        )\n\n        if build_result.returncode != 0:\n            return f\"Compilation error: {build_result.stderr}\"\n\n        run_result = subprocess.run(\n            ['cargo', 'run'],\n            cwd=self.current_project,\n            capture_output=True,\n            text=True\n        )\n\n        self._execution_count += 1\n        output = {\n            'code': code,\n            'stdout': run_result.stdout,\n            'stderr': run_result.stderr,\n            'result': run_result.returncode == 0\n        }\n        self.output_history[self._execution_count] = output\n\n        return run_result.stdout if run_result.returncode == 0 else f\"Runtime error: {run_result.stderr}\"\n\n    except Exception as e:\n        return f\"Execution failed: {str(e)}\"\n</code></pre> <code>save_session(name)</code> \u00b6 <p>Save current session state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def save_session(self, name: str):\n    \"\"\"Save current session state\"\"\"\n    session_file = self._session_dir / f\"{name}.json\"\n    state = {\n        'output_history': self.output_history,\n        'current_project': str(self.current_project) if self.current_project else None\n    }\n\n    with open(session_file, 'w') as f:\n        json.dump(state, f)\n</code></pre> <code>setup_project(name)</code> <code>async</code> \u00b6 <p>Set up a new Cargo project</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def setup_project(self, name: str) -&gt; str:\n    \"\"\"Set up a new Cargo project\"\"\"\n    try:\n        project_path = self.vfs.base_dir / name\n        if project_path.exists():\n            shutil.rmtree(project_path)\n\n        result = subprocess.run(\n            ['cargo', 'new', str(project_path)],\n            capture_output=True,\n            text=True, check=True\n        )\n\n        if result.returncode != 0:\n            return f\"Error creating project: {result.stderr}\"\n\n        self.current_project = project_path\n        return f\"Created new project at {project_path}\"\n\n    except Exception as e:\n        return f\"Failed to create project: {str(e)}\"\n</code></pre> <code>test()</code> <code>async</code> \u00b6 <p>Run project tests</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def test(self) -&gt; str:\n    \"\"\"Run project tests\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        result = subprocess.run(\n            ['cargo', 'test'],\n            cwd=self.current_project,\n            capture_output=True,\n            text=True, check=True\n        )\n\n        return result.stdout if result.returncode == 0 else f\"Test error: {result.stderr}\"\n\n    except Exception as e:\n        return f\"Tests failed: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.EnhancedVerboseOutput","title":"<code>EnhancedVerboseOutput</code>","text":"Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class EnhancedVerboseOutput:\n    def __init__(self, verbose: bool = True,print_f=None):\n        self.verbose = verbose\n        self.print = print_f or print\n        self.formatter = VerboseFormatter(self.print)\n\n\n    async def log_message(self, role: str, content: str):\n        \"\"\"Log chat messages with role-based formatting\"\"\"\n        if not self.verbose:\n            return\n\n        role_formats = {\n            'user': (self.formatter.style.GREEN, \"\ud83d\udc64\"),\n            'assistant': (self.formatter.style.BLUE, \"\ud83e\udd16\"),\n            'system': (self.formatter.style.YELLOW, \"\u2699\ufe0f\")\n        }\n\n        color_func, icon = role_formats.get(role, (self.formatter.style.WHITE, \"\u2022\"))\n        self.print(f\"\\n{icon} {color_func(f'[{role}]')}\")\n        self.print(f\"{self.formatter.style.GREY('\u2514\u2500')} {content}\\n\")\n\n    async def log_think_result(self, result: dict[str, Any]):\n        \"\"\"Log thinking results with structured formatting\"\"\"\n        if not self.verbose:\n            return\n\n        self.formatter.print_section(\n            \"Action Result\",\n            f\"Action: {result.get('action', 'N/A')}\\n\"\n            f\"context: {result.get('context', 'N/A')}\\n\"\n            f\"Content:\\n{result.get('content', '')}\"\n        )\n\n    async def log_process_result(self, result: dict[str, Any]):\n        \"\"\"Log processing results with structured formatting\"\"\"\n        if not self.verbose:\n            return\n\n        self.formatter.print_section(\n            \"Process Result\",\n            f\"Completed: {result.get('is_completed', False)}\\n\"\n            f\"Effectiveness: {result.get('effectiveness', 'N/A')}\\n\"\n            f\"Recommendations: \\n{result.get('recommendations', 'None')}\\n\"\n            f\"workflow: \\n{result.get('workflow', 'None')}\\n\"\n            f\"errors: {result.get('errors', 'None')}\\n\"\n            f\"text: {result.get('text', 'None')}\"\n        )\n\n    def log_header(self, text: str):\n        \"\"\"Log method update with structured formatting\"\"\"\n        if not self.verbose:\n            return\n\n        self.formatter.print_header(text)\n\n    def log_state(self, state: str, user_ns:dict, override=False):\n        \"\"\"Log method update with structured formatting\"\"\"\n        if not self.verbose and override:\n            return\n\n        return self.formatter.print_state(state, user_ns)\n\n    async def process(self, message: str, coroutine):\n        if not self.verbose:\n            return await coroutine\n        if message == \"code\":\n            return await coroutine\n        return await self.formatter.process_with_spinner(message, coroutine)\n</code></pre> <code>log_header(text)</code> \u00b6 <p>Log method update with structured formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def log_header(self, text: str):\n    \"\"\"Log method update with structured formatting\"\"\"\n    if not self.verbose:\n        return\n\n    self.formatter.print_header(text)\n</code></pre> <code>log_message(role, content)</code> <code>async</code> \u00b6 <p>Log chat messages with role-based formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def log_message(self, role: str, content: str):\n    \"\"\"Log chat messages with role-based formatting\"\"\"\n    if not self.verbose:\n        return\n\n    role_formats = {\n        'user': (self.formatter.style.GREEN, \"\ud83d\udc64\"),\n        'assistant': (self.formatter.style.BLUE, \"\ud83e\udd16\"),\n        'system': (self.formatter.style.YELLOW, \"\u2699\ufe0f\")\n    }\n\n    color_func, icon = role_formats.get(role, (self.formatter.style.WHITE, \"\u2022\"))\n    self.print(f\"\\n{icon} {color_func(f'[{role}]')}\")\n    self.print(f\"{self.formatter.style.GREY('\u2514\u2500')} {content}\\n\")\n</code></pre> <code>log_process_result(result)</code> <code>async</code> \u00b6 <p>Log processing results with structured formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def log_process_result(self, result: dict[str, Any]):\n    \"\"\"Log processing results with structured formatting\"\"\"\n    if not self.verbose:\n        return\n\n    self.formatter.print_section(\n        \"Process Result\",\n        f\"Completed: {result.get('is_completed', False)}\\n\"\n        f\"Effectiveness: {result.get('effectiveness', 'N/A')}\\n\"\n        f\"Recommendations: \\n{result.get('recommendations', 'None')}\\n\"\n        f\"workflow: \\n{result.get('workflow', 'None')}\\n\"\n        f\"errors: {result.get('errors', 'None')}\\n\"\n        f\"text: {result.get('text', 'None')}\"\n    )\n</code></pre> <code>log_state(state, user_ns, override=False)</code> \u00b6 <p>Log method update with structured formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def log_state(self, state: str, user_ns:dict, override=False):\n    \"\"\"Log method update with structured formatting\"\"\"\n    if not self.verbose and override:\n        return\n\n    return self.formatter.print_state(state, user_ns)\n</code></pre> <code>log_think_result(result)</code> <code>async</code> \u00b6 <p>Log thinking results with structured formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def log_think_result(self, result: dict[str, Any]):\n    \"\"\"Log thinking results with structured formatting\"\"\"\n    if not self.verbose:\n        return\n\n    self.formatter.print_section(\n        \"Action Result\",\n        f\"Action: {result.get('action', 'N/A')}\\n\"\n        f\"context: {result.get('context', 'N/A')}\\n\"\n        f\"Content:\\n{result.get('content', '')}\"\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.JSExecutionRecord","title":"<code>JSExecutionRecord</code>  <code>dataclass</code>","text":"<p>Records JavaScript execution details</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>@dataclass\nclass JSExecutionRecord:\n    \"\"\"Records JavaScript execution details\"\"\"\n    code: str\n    result: Any\n    error: str | None = None\n    page_state: dict | None = None\n    extracted_data: dict | None = None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.MockIPython","title":"<code>MockIPython</code>","text":"Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class MockIPython:\n    def __init__(self, _session_dir=None, auto_remove=True):\n        self.auto_remove = auto_remove\n        self.output_history = {}\n        self._execution_count = 0\n        self._session_dir = _session_dir or Path(get_app().appdata) / '.pipeline_sessions'\n        self._session_dir.mkdir(exist_ok=True)\n        self.vfs = VirtualFileSystem(self._session_dir / 'virtual_fs')\n        self._venv_path = self._session_dir / 'venv'\n        self.user_ns: dict[str, Any] = {}\n        nest_asyncio.apply()\n        # Set up virtual environment if it doesn't exist\n        with Spinner(\"Starting virtual environment\"):\n            self._setup_venv()\n        self.reset()\n\n    def _setup_venv(self):\n        \"\"\"Create virtual environment if it doesn't exist\"\"\"\n        if not self._venv_path.exists():\n            try:\n                subprocess.run([sys.executable, \"-m\", \"venv\", str(self._venv_path)], check=True)\n            except subprocess.CalledProcessError as e:\n                raise RuntimeError(f\"Failed to create virtual environment: {str(e)}\")\n\n    def _virtual_open(self, filepath, mode='r', *args, **kwargs):\n        \"\"\"Custom open function that uses virtual filesystem\"\"\"\n        abs_path = self.vfs._resolve_path(filepath)\n\n        if 'w' in mode or 'a' in mode:\n            # Ensure parent directory exists\n            abs_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Use actual filesystem but track in virtual fs\n        real_file = open(abs_path, mode, *args, **kwargs)\n\n        if 'r' in mode:\n            # Track file content in virtual filesystem when reading\n            rel_path = str(abs_path.relative_to(self.vfs.base_dir))\n            if rel_path not in self.vfs.virtual_files:\n                try:\n                    self.vfs.virtual_files[rel_path] = real_file.read()\n                    real_file.seek(0)\n                except UnicodeDecodeError:\n                    # Handle binary files\n                    pass\n\n        return real_file\n\n    def reset(self):\n        \"\"\"Reset the interpreter state\"\"\"\n        self.user_ns = {\n            '__name__': '__main__',\n            '__builtins__': __builtins__,\n            'toolboxv2': toolboxv2,\n            '__file__': None,\n            '__path__': [str(self.vfs.current_dir)],\n            'auto_install': auto_install,\n            'modify_code': self.modify_code,\n        }\n        self.output_history.clear()\n        self._execution_count = 0\n        if self.auto_remove:\n            shutil.rmtree(self.vfs.base_dir, ignore_errors=True)\n\n    def get_namespace(self) -&gt; dict[str, Any]:\n        \"\"\"Get current namespace\"\"\"\n        return self.user_ns.copy()\n\n    def update_namespace(self, variables: dict[str, Any]):\n        \"\"\"Update namespace with new variables\"\"\"\n        self.user_ns.update(variables)\n\n    @staticmethod\n    def _parse_code(code: str) -&gt; tuple[Any, Any | None, bool, bool]:\n        \"\"\"Parse code and handle top-level await\"\"\"\n        code_ = \"\"\n        for line in code.split('\\n'):\n            if line.strip().startswith('#'):\n                continue\n            if line.strip().startswith('asyncio.run('):\n                line = (' ' *(len(line) - len(line.strip()))) + 'await ' + line.strip()[len('asyncio.run('):-1]\n            code_ += line + '\\n'\n        try:\n            tree = ast.parse(code)\n            # Add parent references\n            ParentNodeTransformer().visit(tree)\n\n            # Detect async features\n            detector = AsyncCodeDetector()\n            detector.visit(tree)\n\n            if detector.has_top_level_await:\n                # Wrap code in async function\n                wrapped_code = \"async def __wrapper():\\n\"\n                wrapped_code += \"    global result\\n\"  # Allow writing to global scope\n                wrapped_code += \"    result = None\\n\"\n                # add try:\n                wrapped_code +=\"    try:\\n\"\n                # Indent the original code\n                wrapped_code += \"\\n\".join(f\"        {line}\" for line in code.splitlines())\n                # Add return statement for last expression\n                wrapped_code +=\"\\n    except Exception as e:\\n\"\n                wrapped_code +=\"        import traceback\\n\"\n                wrapped_code +=\"        print(traceback.format_exc())\\n\"\n                wrapped_code +=\"        raise e\\n\"\n                if isinstance(tree.body[-1], ast.Expr):\n                    wrapped_code += \"\\n    return result\"\n\n                # Parse and compile wrapped code\n                wrapped_tree = ast.parse(wrapped_code)\n                return (\n                    compile(wrapped_tree, '&lt;exec&gt;', 'exec'),\n                    None,\n                    True,\n                    True\n                )\n\n            # Handle regular code\n            if isinstance(tree.body[-1], ast.Expr):\n                exec_code = ast.Module(\n                    body=tree.body[:-1],\n                    type_ignores=[]\n                )\n                eval_code = ast.Expression(\n                    body=tree.body[-1].value\n                )\n                return (\n                    compile(exec_code, '&lt;exec&gt;', 'exec'),\n                    compile(eval_code, '&lt;eval&gt;', 'eval'),\n                    detector.has_async,\n                    False\n                )\n\n            return (\n                compile(tree, '&lt;exec&gt;', 'exec'),\n                None,\n                detector.has_async,\n                False\n            )\n\n        except SyntaxError as e:\n            lines = code.splitlines()\n            if e.lineno and e.lineno &lt;= len(lines):\n                line = lines[e.lineno - 1]\n                arrow = ' ' * (e.offset - 1) + '^' if e.offset else ''\n                error_msg = (\n                    f\"Syntax error at line {e.lineno}:\\n\"\n                    f\"{line}\\n\"\n                    f\"{arrow}\\n\"\n                    f\"{e.msg}\"\n                )\n            else:\n                error_msg = str(e)\n\n            error_msg += traceback.format_exc()\n\n            raise SyntaxError(error_msg) from e\n\n    async def run_cell(self, code: str, live_output: bool = True) -&gt; Any:\n        \"\"\"Async version of run_cell that handles both sync and async code\"\"\"\n        result = None\n        error = None\n        tb = None\n        original_dir = os.getcwd()\n\n        if live_output:\n            stdout_buffer = io.StringIO()\n            stderr_buffer = io.StringIO()\n            stdout = TeeStream(sys.__stdout__, stdout_buffer)\n            stderr = TeeStream(sys.__stderr__, stderr_buffer)\n        else:\n            stdout = io.StringIO()\n            stderr = io.StringIO()\n\n        try:\n            # Check if a file is already specified\n            original_file = self.user_ns.get('__file__')\n            if original_file is None:\n                # Create temp file if no file specified\n                temp_file = self.vfs.write_file(\n                    f'src/temp/_temp_{self._execution_count}.py',\n                    code\n                )\n                # work_ns = self.user_ns.copy()\n                self.user_ns['__file__'] = str(temp_file)\n            else:\n                # Use existing file\n                temp_file = Path(original_file)\n                # Write code to the existing file\n                self.vfs.write_file(temp_file, code)\n                #work_ns = self.user_ns.copy()\n\n            self.user_ns['__builtins__'] = __builtins__\n            with VirtualEnvContext(self._venv_path) as python_exec:\n                try:\n                    exec_code, eval_code, is_async, has_top_level_await = self._parse_code(\n                        code.encode('utf-8', errors='replace').decode('utf-8')\n                    )\n                    if exec_code is None:\n                        return \"No executable code\"\n                    os.makedirs(str(temp_file.parent.absolute()), exist_ok=True)\n                    os.chdir(str(temp_file.parent.absolute()))\n                    self.user_ns['PYTHON_EXEC'] = python_exec\n\n                    with redirect_stdout(stdout), redirect_stderr(stderr):\n                        if has_top_level_await:\n                            try:\n                                # Execute wrapped code and await it\n                                exec(exec_code, self.user_ns)\n                                result = self.user_ns['__wrapper']()\n                                if asyncio.iscoroutine(result):\n                                    result = await result\n                            finally:\n                                self.user_ns.pop('__wrapper', None)\n                        elif is_async:\n                            # Execute async code\n                            exec(exec_code, self.user_ns)\n                            if eval_code:\n                                result = eval(eval_code, self.user_ns)\n                                if asyncio.iscoroutine(result):\n                                    result = await result\n                        else:\n                            # Execute sync code\n                            exec(exec_code, self.user_ns)\n                            if eval_code:\n                                result = eval(eval_code, self.user_ns)\n\n                        if result is not None:\n                            self.user_ns['_'] = result\n                except KeyboardInterrupt:\n                    print(\"Stop execution manuel!\")\n\n                except Exception as e:\n                    error = str(e)\n                    tb = traceback.format_exc()\n                    if live_output:\n                        sys.__stderr__.write(f\"{error}\\n{tb}\")\n                    stderr.write(f\"{error}\\n{tb}\")\n\n                finally:\n                    os.chdir(original_dir)\n                    self._execution_count += 1\n                    # self.user_ns = work_ns.copy()\n                    if live_output:\n                        stdout_value = stdout_buffer.getvalue()\n                        stderr_value = stderr_buffer.getvalue()\n                    else:\n                        stdout_value = stdout.getvalue()\n                        stderr_value = stderr.getvalue()\n\n                    output = {\n                        'code': code,\n                        'stdout': stdout_value,\n                        'stderr': stderr_value,\n                        'result': result if result else \"stdout\"\n                    }\n                    self.output_history[self._execution_count] = output\n\n                    if not result:\n                        result = \"\"\n                    if output['stdout']:\n                        result = f\"{result}\\nstdout:{output['stdout']}\"\n                    if output['stderr']:\n                        result = f\"{result}\\nstderr:{output['stderr']}\"\n\n                    if self.auto_remove and original_file is None:\n                        # Only remove temp files, not user-specified files\n                        self.vfs.delete_file(temp_file)\n\n                    return result\n\n        except Exception as e:\n            error_msg = f\"Error executing code: {str(e)}\\n{traceback.format_exc()}\"\n            if live_output:\n                sys.__stderr__.write(error_msg)\n            return error_msg\n\n    async def modify_code(self, code: str = None, object_name: str = None, file: str = None) -&gt; str:\n        '''\n        Modify existing code in memory (user namespace) and optionally in the corresponding file.\n\n        This method updates variables, functions, or methods in the current Python session and can\n        also update the corresponding source file if specified.\n\n        Args:\n            code: New value or implementation for the object\n            object_name: Name of the object to modify (variable, function, or method)\n            file: Path to the file to update (if None, only updates in memory)\n\n        Returns:\n            String describing the modification result\n\n        Examples:\n\n        # 1. Update a variable in memory\n        await ipython.modify_code(code=\"5\", object_name=\"x\")\n\n    # 2. Change a method implementation\n    await ipython.modify_code(\n        code='\"\"\"def sound(self):\\n        return \"Woof\"\"\"\"',\n        object_name=\"Dog.sound\"\n    )\n\n    # 3. Modify a function\n    await ipython.modify_code(\n        code='\"\"\"def calculate_age():\\n    return 25\"\"\"',\n        object_name=\"calculate_age\"\n    )\n\n    # 4. Update variable in memory and file\n    await ipython.modify_code(\n        code=\"100\",\n        object_name=\"MAX_SIZE\",\n        file=\"config.py\"\n    )\n\n    # 5. Modifying an attribute in __init__\n    await ipython.modify_code(\n        code='\"\"\"def __init__(self):\\n        self.name = \"Buddy\"\"\"\"',\n        object_name=\"Dog.__init__\"\n    )\n        '''\n        try:\n            if not object_name:\n                raise ValueError(\"Object name must be specified\")\n            if code is None:\n                raise ValueError(\"New code or value must be provided\")\n\n            # Process object name (handle methods with parentheses)\n            clean_object_name = object_name.replace(\"()\", \"\")\n\n            # Step 1: Update in memory (user namespace)\n            result_message = []\n\n            # Handle different types of objects\n            if \".\" in clean_object_name:\n                # For methods or class attributes\n                parts = clean_object_name.split(\".\")\n                base_obj_name = parts[0]\n                attr_name = parts[1]\n\n                if base_obj_name not in self.user_ns:\n                    raise ValueError(f\"Object '{base_obj_name}' not found in namespace\")\n\n                base_obj = self.user_ns[base_obj_name]\n\n                # Handle method definitions which are passed as docstrings\n                if code.split('\\n'):\n                    method_code = code\n\n                    # Parse the method code to extract its body\n                    method_ast = ast.parse(method_code).body[0]\n                    method_name = method_ast.name\n\n                    # Create a new function object from the code\n                    method_locals = {}\n                    exec(\n                        f\"def _temp_func{signature(getattr(base_obj.__class__, attr_name, None))}: {method_ast.body[0].value.s}\",\n                        globals(), method_locals)\n                    new_method = method_locals['_temp_func']\n\n                    # Set the method on the class\n                    setattr(base_obj.__class__, attr_name, new_method)\n                    result_message.append(f\"Updated method '{clean_object_name}' in memory\")\n                else:\n                    # For simple attributes\n                    setattr(base_obj, attr_name, eval(code, self.user_ns))\n                    result_message.append(f\"Updated attribute '{clean_object_name}' in memory\")\n            else:\n                # For variables and functions\n                if code.startswith('\"\"\"') and code.endswith('\"\"\"'):\n                    # Handle function definitions\n                    func_code = code.strip('\"\"\"')\n                    func_ast = ast.parse(func_code).body[0]\n                    func_name = func_ast.name\n\n                    # Create a new function object from the code\n                    func_locals = {}\n                    exec(f\"{func_code}\", globals(), func_locals)\n                    self.user_ns[clean_object_name] = func_locals[func_name]\n                    result_message.append(f\"Updated function '{clean_object_name}' in memory\")\n                else:\n                    # Simple variable assignment\n                    self.user_ns[clean_object_name] = eval(code, self.user_ns)\n                    result_message.append(f\"Updated variable '{clean_object_name}' in memory\")\n\n            # Step 2: Update in file if specified\n            if file is not None:\n                file_path = self.vfs._resolve_path(file)\n\n                if not file_path.exists():\n                    self.user_ns['__file__'] = str(file_path)\n                    return await self.run_cell(code)\n\n                # Read original content\n                original_content = self.vfs.read_file(file_path)\n                updated_content = original_content\n\n                # Handle different object types for file updates\n                if \".\" in clean_object_name:\n                    # For methods\n                    parts = clean_object_name.split(\".\")\n                    class_name = parts[0]\n                    method_name = parts[1]\n\n                    if code.startswith('\"\"\"') and code.endswith('\"\"\"'):\n                        method_code = code.strip('\"\"\"')\n\n                        # Use ast to parse the file and find the method to replace\n                        file_ast = ast.parse(original_content)\n                        for node in ast.walk(file_ast):\n                            if isinstance(node, ast.ClassDef) and node.name == class_name:\n                                for method in node.body:\n                                    if isinstance(method, ast.FunctionDef) and method.name == method_name:\n                                        # Find the method in the source code\n                                        method_pattern = fr\"def {method_name}.*?:(.*?)(?=\\n    \\w|\\n\\w|\\Z)\"\n                                        method_match = re.search(method_pattern, original_content, re.DOTALL)\n\n                                        if method_match:\n                                            indentation = re.match(r\"^(\\s*)\", method_match.group(0)).group(1)\n                                            method_indented = textwrap.indent(method_code, indentation)\n                                            updated_content = original_content.replace(\n                                                method_match.group(0),\n                                                method_indented\n                                            )\n                                            self.vfs.write_file(file_path, updated_content)\n                                            result_message.append(\n                                                f\"Updated method '{clean_object_name}' in file '{file}'\")\n                else:\n                    # For variables and functions\n                    if code.startswith('\"\"\"') and code.endswith('\"\"\"'):\n                        # Handle function updates\n                        func_code = code.strip('\"\"\"')\n                        func_pattern = fr\"def {clean_object_name}.*?:(.*?)(?=\\n\\w|\\Z)\"\n                        func_match = re.search(func_pattern, original_content, re.DOTALL)\n\n                        if func_match:\n                            indentation = re.match(r\"^(\\s*)\", func_match.group(0)).group(1)\n                            func_indented = textwrap.indent(func_code, indentation)\n                            updated_content = original_content.replace(\n                                func_match.group(0),\n                                func_indented\n                            )\n                            self.vfs.write_file(file_path, updated_content)\n                            result_message.append(f\"Updated function '{clean_object_name}' in file '{file}'\")\n                    else:\n                        # Handle variable updates\n                        var_pattern = fr\"{clean_object_name}\\s*=.*\"\n                        var_replacement = f\"{clean_object_name} = {code}\"\n                        updated_content = re.sub(var_pattern, var_replacement, original_content)\n\n                        if updated_content != original_content:\n                            self.vfs.write_file(file_path, updated_content)\n                            result_message.append(f\"Updated variable '{clean_object_name}' in file '{file}'\")\n                        else:\n                            result_message.append(f\"Could not find variable '{clean_object_name}' in file '{file}'\")\n\n            return \"\\n\".join(result_message)\n\n        except Exception as e:\n            return f\"Error during code modification: {str(e)}\\n{traceback.format_exc()}\"\n\n\n    def save_session(self, name: str):\n        \"\"\"Save session with UTF-8 encoding\"\"\"\n        session_file = self._session_dir / f\"{name}.pkl\"\n        user_ns = self.user_ns.copy()\n        output_history = self.output_history.copy()\n\n        # Ensure all strings are properly encoded\n        for key, value in user_ns.items():\n            try:\n                if isinstance(value, str):\n                    value = value.encode('utf-8').decode('utf-8')\n                pickle.dumps(value)\n            except Exception:\n                user_ns[key] = f\"not serializable: {str(value)}\"\n\n        for key, value in output_history.items():\n            try:\n                if isinstance(value, dict):\n                    for k, v in value.items():\n                        if isinstance(v, str):\n                            value[k] = v.encode('utf-8').decode('utf-8')\n                pickle.dumps(value)\n            except Exception:\n                output_history[key] = f\"not serializable: {str(value)}\"\n\n\n        session_data = {\n            'user_ns': user_ns,\n            'output_history': output_history,\n\n        }\n\n        with open(session_file, 'wb') as f:\n            pickle.dump(session_data, f)\n\n        # Save VFS state with UTF-8 encoding\n        vfs_state_file = self._session_dir / f\"{name}_vfs.json\"\n        with open(vfs_state_file, 'w', encoding='utf-8') as f:\n            json.dump(self.vfs.virtual_files, f, ensure_ascii=False)\n\n    def load_session(self, name: str):\n        \"\"\"Load session with UTF-8 encoding\"\"\"\n        session_file = self._session_dir / f\"{name}.pkl\"\n        if session_file.exists():\n            with open(session_file, 'rb') as f:\n                session_data = pickle.load(f)\n                # self.user_ns.update(session_data['user_ns'])\n                self.output_history.update(session_data['output_history'])\n\n        # Load VFS state with UTF-8 encoding\n        vfs_state_file = self._session_dir / f\"{name}_vfs.json\"\n        if vfs_state_file.exists():\n            with open(vfs_state_file, encoding='utf-8') as f:\n                self.vfs.virtual_files = json.load(f)\n\n    def __str__(self):\n        \"\"\"String representation of current session\"\"\"\n        output = []\n        for count, data in self.output_history.items():\n            output.append(f\"In [{count}]: {data['code']}\")\n            if data['stdout']:\n                output.append(data['stdout'])\n            if data['stderr']:\n                output.append(f\"Error: {data['stderr']}\")\n            if data['result'] is not None:\n                output.append(f\"Out[{count}]: {data['result']}\")\n        return \"\\n\".join(output)\n</code></pre> <code>__str__()</code> \u00b6 <p>String representation of current session</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __str__(self):\n    \"\"\"String representation of current session\"\"\"\n    output = []\n    for count, data in self.output_history.items():\n        output.append(f\"In [{count}]: {data['code']}\")\n        if data['stdout']:\n            output.append(data['stdout'])\n        if data['stderr']:\n            output.append(f\"Error: {data['stderr']}\")\n        if data['result'] is not None:\n            output.append(f\"Out[{count}]: {data['result']}\")\n    return \"\\n\".join(output)\n</code></pre> <code>get_namespace()</code> \u00b6 <p>Get current namespace</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def get_namespace(self) -&gt; dict[str, Any]:\n    \"\"\"Get current namespace\"\"\"\n    return self.user_ns.copy()\n</code></pre> <code>load_session(name)</code> \u00b6 <p>Load session with UTF-8 encoding</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def load_session(self, name: str):\n    \"\"\"Load session with UTF-8 encoding\"\"\"\n    session_file = self._session_dir / f\"{name}.pkl\"\n    if session_file.exists():\n        with open(session_file, 'rb') as f:\n            session_data = pickle.load(f)\n            # self.user_ns.update(session_data['user_ns'])\n            self.output_history.update(session_data['output_history'])\n\n    # Load VFS state with UTF-8 encoding\n    vfs_state_file = self._session_dir / f\"{name}_vfs.json\"\n    if vfs_state_file.exists():\n        with open(vfs_state_file, encoding='utf-8') as f:\n            self.vfs.virtual_files = json.load(f)\n</code></pre> <code>modify_code(code=None, object_name=None, file=None)</code> <code>async</code> \u00b6 <pre><code>Modify existing code in memory (user namespace) and optionally in the corresponding file.\n\nThis method updates variables, functions, or methods in the current Python session and can\nalso update the corresponding source file if specified.\n\nArgs:\n    code: New value or implementation for the object\n    object_name: Name of the object to modify (variable, function, or method)\n    file: Path to the file to update (if None, only updates in memory)\n\nReturns:\n    String describing the modification result\n\nExamples:\n\n# 1. Update a variable in memory\nawait ipython.modify_code(code=\"5\", object_name=\"x\")\n</code></pre> <code>reset()</code> \u00b6 <p>Reset the interpreter state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def reset(self):\n    \"\"\"Reset the interpreter state\"\"\"\n    self.user_ns = {\n        '__name__': '__main__',\n        '__builtins__': __builtins__,\n        'toolboxv2': toolboxv2,\n        '__file__': None,\n        '__path__': [str(self.vfs.current_dir)],\n        'auto_install': auto_install,\n        'modify_code': self.modify_code,\n    }\n    self.output_history.clear()\n    self._execution_count = 0\n    if self.auto_remove:\n        shutil.rmtree(self.vfs.base_dir, ignore_errors=True)\n</code></pre> <code>run_cell(code, live_output=True)</code> <code>async</code> \u00b6 <p>Async version of run_cell that handles both sync and async code</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def run_cell(self, code: str, live_output: bool = True) -&gt; Any:\n    \"\"\"Async version of run_cell that handles both sync and async code\"\"\"\n    result = None\n    error = None\n    tb = None\n    original_dir = os.getcwd()\n\n    if live_output:\n        stdout_buffer = io.StringIO()\n        stderr_buffer = io.StringIO()\n        stdout = TeeStream(sys.__stdout__, stdout_buffer)\n        stderr = TeeStream(sys.__stderr__, stderr_buffer)\n    else:\n        stdout = io.StringIO()\n        stderr = io.StringIO()\n\n    try:\n        # Check if a file is already specified\n        original_file = self.user_ns.get('__file__')\n        if original_file is None:\n            # Create temp file if no file specified\n            temp_file = self.vfs.write_file(\n                f'src/temp/_temp_{self._execution_count}.py',\n                code\n            )\n            # work_ns = self.user_ns.copy()\n            self.user_ns['__file__'] = str(temp_file)\n        else:\n            # Use existing file\n            temp_file = Path(original_file)\n            # Write code to the existing file\n            self.vfs.write_file(temp_file, code)\n            #work_ns = self.user_ns.copy()\n\n        self.user_ns['__builtins__'] = __builtins__\n        with VirtualEnvContext(self._venv_path) as python_exec:\n            try:\n                exec_code, eval_code, is_async, has_top_level_await = self._parse_code(\n                    code.encode('utf-8', errors='replace').decode('utf-8')\n                )\n                if exec_code is None:\n                    return \"No executable code\"\n                os.makedirs(str(temp_file.parent.absolute()), exist_ok=True)\n                os.chdir(str(temp_file.parent.absolute()))\n                self.user_ns['PYTHON_EXEC'] = python_exec\n\n                with redirect_stdout(stdout), redirect_stderr(stderr):\n                    if has_top_level_await:\n                        try:\n                            # Execute wrapped code and await it\n                            exec(exec_code, self.user_ns)\n                            result = self.user_ns['__wrapper']()\n                            if asyncio.iscoroutine(result):\n                                result = await result\n                        finally:\n                            self.user_ns.pop('__wrapper', None)\n                    elif is_async:\n                        # Execute async code\n                        exec(exec_code, self.user_ns)\n                        if eval_code:\n                            result = eval(eval_code, self.user_ns)\n                            if asyncio.iscoroutine(result):\n                                result = await result\n                    else:\n                        # Execute sync code\n                        exec(exec_code, self.user_ns)\n                        if eval_code:\n                            result = eval(eval_code, self.user_ns)\n\n                    if result is not None:\n                        self.user_ns['_'] = result\n            except KeyboardInterrupt:\n                print(\"Stop execution manuel!\")\n\n            except Exception as e:\n                error = str(e)\n                tb = traceback.format_exc()\n                if live_output:\n                    sys.__stderr__.write(f\"{error}\\n{tb}\")\n                stderr.write(f\"{error}\\n{tb}\")\n\n            finally:\n                os.chdir(original_dir)\n                self._execution_count += 1\n                # self.user_ns = work_ns.copy()\n                if live_output:\n                    stdout_value = stdout_buffer.getvalue()\n                    stderr_value = stderr_buffer.getvalue()\n                else:\n                    stdout_value = stdout.getvalue()\n                    stderr_value = stderr.getvalue()\n\n                output = {\n                    'code': code,\n                    'stdout': stdout_value,\n                    'stderr': stderr_value,\n                    'result': result if result else \"stdout\"\n                }\n                self.output_history[self._execution_count] = output\n\n                if not result:\n                    result = \"\"\n                if output['stdout']:\n                    result = f\"{result}\\nstdout:{output['stdout']}\"\n                if output['stderr']:\n                    result = f\"{result}\\nstderr:{output['stderr']}\"\n\n                if self.auto_remove and original_file is None:\n                    # Only remove temp files, not user-specified files\n                    self.vfs.delete_file(temp_file)\n\n                return result\n\n    except Exception as e:\n        error_msg = f\"Error executing code: {str(e)}\\n{traceback.format_exc()}\"\n        if live_output:\n            sys.__stderr__.write(error_msg)\n        return error_msg\n</code></pre> <code>save_session(name)</code> \u00b6 <p>Save session with UTF-8 encoding</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def save_session(self, name: str):\n    \"\"\"Save session with UTF-8 encoding\"\"\"\n    session_file = self._session_dir / f\"{name}.pkl\"\n    user_ns = self.user_ns.copy()\n    output_history = self.output_history.copy()\n\n    # Ensure all strings are properly encoded\n    for key, value in user_ns.items():\n        try:\n            if isinstance(value, str):\n                value = value.encode('utf-8').decode('utf-8')\n            pickle.dumps(value)\n        except Exception:\n            user_ns[key] = f\"not serializable: {str(value)}\"\n\n    for key, value in output_history.items():\n        try:\n            if isinstance(value, dict):\n                for k, v in value.items():\n                    if isinstance(v, str):\n                        value[k] = v.encode('utf-8').decode('utf-8')\n            pickle.dumps(value)\n        except Exception:\n            output_history[key] = f\"not serializable: {str(value)}\"\n\n\n    session_data = {\n        'user_ns': user_ns,\n        'output_history': output_history,\n\n    }\n\n    with open(session_file, 'wb') as f:\n        pickle.dump(session_data, f)\n\n    # Save VFS state with UTF-8 encoding\n    vfs_state_file = self._session_dir / f\"{name}_vfs.json\"\n    with open(vfs_state_file, 'w', encoding='utf-8') as f:\n        json.dump(self.vfs.virtual_files, f, ensure_ascii=False)\n</code></pre> <code>update_namespace(variables)</code> \u00b6 <p>Update namespace with new variables</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def update_namespace(self, variables: dict[str, Any]):\n    \"\"\"Update namespace with new variables\"\"\"\n    self.user_ns.update(variables)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.MockIPython.modify_code--2-change-a-method-implementation","title":"2. Change a method implementation","text":"<p>await ipython.modify_code(     code='\"\"\"def sound(self):     return \"Woof\"\"\"\"',     object_name=\"Dog.sound\" )</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.MockIPython.modify_code--3-modify-a-function","title":"3. Modify a function","text":"<p>await ipython.modify_code(     code='\"\"\"def calculate_age(): return 25\"\"\"',     object_name=\"calculate_age\" )</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.MockIPython.modify_code--4-update-variable-in-memory-and-file","title":"4. Update variable in memory and file","text":"<p>await ipython.modify_code(     code=\"100\",     object_name=\"MAX_SIZE\",     file=\"config.py\" )</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.MockIPython.modify_code--5-modifying-an-attribute-in-init","title":"5. Modifying an attribute in init","text":"<p>await ipython.modify_code(     code='\"\"\"def init(self):     self.name = \"Buddy\"\"\"\"',     object_name=\"Dog.init\" )</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def modify_code(self, code: str = None, object_name: str = None, file: str = None) -&gt; str:\n    '''\n    Modify existing code in memory (user namespace) and optionally in the corresponding file.\n\n    This method updates variables, functions, or methods in the current Python session and can\n    also update the corresponding source file if specified.\n\n    Args:\n        code: New value or implementation for the object\n        object_name: Name of the object to modify (variable, function, or method)\n        file: Path to the file to update (if None, only updates in memory)\n\n    Returns:\n        String describing the modification result\n\n    Examples:\n\n    # 1. Update a variable in memory\n    await ipython.modify_code(code=\"5\", object_name=\"x\")\n\n# 2. Change a method implementation\nawait ipython.modify_code(\n    code='\"\"\"def sound(self):\\n        return \"Woof\"\"\"\"',\n    object_name=\"Dog.sound\"\n)\n\n# 3. Modify a function\nawait ipython.modify_code(\n    code='\"\"\"def calculate_age():\\n    return 25\"\"\"',\n    object_name=\"calculate_age\"\n)\n\n# 4. Update variable in memory and file\nawait ipython.modify_code(\n    code=\"100\",\n    object_name=\"MAX_SIZE\",\n    file=\"config.py\"\n)\n\n# 5. Modifying an attribute in __init__\nawait ipython.modify_code(\n    code='\"\"\"def __init__(self):\\n        self.name = \"Buddy\"\"\"\"',\n    object_name=\"Dog.__init__\"\n)\n    '''\n    try:\n        if not object_name:\n            raise ValueError(\"Object name must be specified\")\n        if code is None:\n            raise ValueError(\"New code or value must be provided\")\n\n        # Process object name (handle methods with parentheses)\n        clean_object_name = object_name.replace(\"()\", \"\")\n\n        # Step 1: Update in memory (user namespace)\n        result_message = []\n\n        # Handle different types of objects\n        if \".\" in clean_object_name:\n            # For methods or class attributes\n            parts = clean_object_name.split(\".\")\n            base_obj_name = parts[0]\n            attr_name = parts[1]\n\n            if base_obj_name not in self.user_ns:\n                raise ValueError(f\"Object '{base_obj_name}' not found in namespace\")\n\n            base_obj = self.user_ns[base_obj_name]\n\n            # Handle method definitions which are passed as docstrings\n            if code.split('\\n'):\n                method_code = code\n\n                # Parse the method code to extract its body\n                method_ast = ast.parse(method_code).body[0]\n                method_name = method_ast.name\n\n                # Create a new function object from the code\n                method_locals = {}\n                exec(\n                    f\"def _temp_func{signature(getattr(base_obj.__class__, attr_name, None))}: {method_ast.body[0].value.s}\",\n                    globals(), method_locals)\n                new_method = method_locals['_temp_func']\n\n                # Set the method on the class\n                setattr(base_obj.__class__, attr_name, new_method)\n                result_message.append(f\"Updated method '{clean_object_name}' in memory\")\n            else:\n                # For simple attributes\n                setattr(base_obj, attr_name, eval(code, self.user_ns))\n                result_message.append(f\"Updated attribute '{clean_object_name}' in memory\")\n        else:\n            # For variables and functions\n            if code.startswith('\"\"\"') and code.endswith('\"\"\"'):\n                # Handle function definitions\n                func_code = code.strip('\"\"\"')\n                func_ast = ast.parse(func_code).body[0]\n                func_name = func_ast.name\n\n                # Create a new function object from the code\n                func_locals = {}\n                exec(f\"{func_code}\", globals(), func_locals)\n                self.user_ns[clean_object_name] = func_locals[func_name]\n                result_message.append(f\"Updated function '{clean_object_name}' in memory\")\n            else:\n                # Simple variable assignment\n                self.user_ns[clean_object_name] = eval(code, self.user_ns)\n                result_message.append(f\"Updated variable '{clean_object_name}' in memory\")\n\n        # Step 2: Update in file if specified\n        if file is not None:\n            file_path = self.vfs._resolve_path(file)\n\n            if not file_path.exists():\n                self.user_ns['__file__'] = str(file_path)\n                return await self.run_cell(code)\n\n            # Read original content\n            original_content = self.vfs.read_file(file_path)\n            updated_content = original_content\n\n            # Handle different object types for file updates\n            if \".\" in clean_object_name:\n                # For methods\n                parts = clean_object_name.split(\".\")\n                class_name = parts[0]\n                method_name = parts[1]\n\n                if code.startswith('\"\"\"') and code.endswith('\"\"\"'):\n                    method_code = code.strip('\"\"\"')\n\n                    # Use ast to parse the file and find the method to replace\n                    file_ast = ast.parse(original_content)\n                    for node in ast.walk(file_ast):\n                        if isinstance(node, ast.ClassDef) and node.name == class_name:\n                            for method in node.body:\n                                if isinstance(method, ast.FunctionDef) and method.name == method_name:\n                                    # Find the method in the source code\n                                    method_pattern = fr\"def {method_name}.*?:(.*?)(?=\\n    \\w|\\n\\w|\\Z)\"\n                                    method_match = re.search(method_pattern, original_content, re.DOTALL)\n\n                                    if method_match:\n                                        indentation = re.match(r\"^(\\s*)\", method_match.group(0)).group(1)\n                                        method_indented = textwrap.indent(method_code, indentation)\n                                        updated_content = original_content.replace(\n                                            method_match.group(0),\n                                            method_indented\n                                        )\n                                        self.vfs.write_file(file_path, updated_content)\n                                        result_message.append(\n                                            f\"Updated method '{clean_object_name}' in file '{file}'\")\n            else:\n                # For variables and functions\n                if code.startswith('\"\"\"') and code.endswith('\"\"\"'):\n                    # Handle function updates\n                    func_code = code.strip('\"\"\"')\n                    func_pattern = fr\"def {clean_object_name}.*?:(.*?)(?=\\n\\w|\\Z)\"\n                    func_match = re.search(func_pattern, original_content, re.DOTALL)\n\n                    if func_match:\n                        indentation = re.match(r\"^(\\s*)\", func_match.group(0)).group(1)\n                        func_indented = textwrap.indent(func_code, indentation)\n                        updated_content = original_content.replace(\n                            func_match.group(0),\n                            func_indented\n                        )\n                        self.vfs.write_file(file_path, updated_content)\n                        result_message.append(f\"Updated function '{clean_object_name}' in file '{file}'\")\n                else:\n                    # Handle variable updates\n                    var_pattern = fr\"{clean_object_name}\\s*=.*\"\n                    var_replacement = f\"{clean_object_name} = {code}\"\n                    updated_content = re.sub(var_pattern, var_replacement, original_content)\n\n                    if updated_content != original_content:\n                        self.vfs.write_file(file_path, updated_content)\n                        result_message.append(f\"Updated variable '{clean_object_name}' in file '{file}'\")\n                    else:\n                        result_message.append(f\"Could not find variable '{clean_object_name}' in file '{file}'\")\n\n        return \"\\n\".join(result_message)\n\n    except Exception as e:\n        return f\"Error during code modification: {str(e)}\\n{traceback.format_exc()}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.ParentNodeTransformer","title":"<code>ParentNodeTransformer</code>","text":"<p>               Bases: <code>NodeTransformer</code></p> <p>Add parent references to AST nodes</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class ParentNodeTransformer(ast.NodeTransformer):\n    \"\"\"Add parent references to AST nodes\"\"\"\n    def visit(self, node):\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n        return super().visit(node)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.Pipeline","title":"<code>Pipeline</code>","text":"<p>A pipeline for executing AI agent-driven tasks with interactive code execution and variable management.</p> <p>The Pipeline class provides a structured environment for AI agents to: 1. Execute code in a controlled environment 2. Manage and track variables 3. Update methods dynamically 4. Save and load session states 5. Generate detailed variable descriptions</p> <p>Attributes:</p> Name Type Description <code>agent</code> <p>The AI agent instance used for task execution</p> <code>task</code> <code>str</code> <p>The task to be performed</p> <code>mas_iter</code> <code>int</code> <p>Maximum number of iterations allowed (default: 12)</p> <code>variables</code> <code>Dict[str, Any]</code> <p>Dictionary of variables available to the pipeline</p> <code>top_n</code> <code>Optional[int]</code> <p>Limit variable descriptions to top N most used</p> <code>execution_history</code> <code>List[ExecutionRecord]</code> <p>History of executed code and results</p> <code>session_name</code> <code>Optional[str]</code> <p>Name of the current session if saved</p> <code>ipython</code> <p>IPython or MockIPython instance for code execution</p> Example <p>agent = get_free_agent(\"demo\", \"anthropic/claude-3-haiku-20240307\") pipeline = Pipeline( ...     agent=agent, ...     task=\"Calculate fibonacci sequence\", ...     variables={\"n\": 10} ... ) result = pipeline.run(\"...\") print(result.result)</p> Notes <ul> <li>The pipeline uses either IPython if available or a MockIPython implementation</li> <li>Variables can be provided as either a dictionary or list</li> <li>Session state can be saved and loaded</li> <li>Method updates are handled through a structured BaseModel approach</li> </ul> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class Pipeline:\n    \"\"\"\n        A pipeline for executing AI agent-driven tasks with interactive code execution and variable management.\n\n        The Pipeline class provides a structured environment for AI agents to:\n        1. Execute code in a controlled environment\n        2. Manage and track variables\n        3. Update methods dynamically\n        4. Save and load session states\n        5. Generate detailed variable descriptions\n\n        Attributes:\n            agent: The AI agent instance used for task execution\n            task (str): The task to be performed\n            mas_iter (int): Maximum number of iterations allowed (default: 12)\n            variables (Dict[str, Any]): Dictionary of variables available to the pipeline\n            top_n (Optional[int]): Limit variable descriptions to top N most used\n            execution_history (List[ExecutionRecord]): History of executed code and results\n            session_name (Optional[str]): Name of the current session if saved\n            ipython: IPython or MockIPython instance for code execution\n\n        Example:\n            &gt;&gt;&gt; agent = get_free_agent(\"demo\", \"anthropic/claude-3-haiku-20240307\")\n            &gt;&gt;&gt; pipeline = Pipeline(\n            ...     agent=agent,\n            ...     task=\"Calculate fibonacci sequence\",\n            ...     variables={\"n\": 10}\n            ... )\n            &gt;&gt;&gt; result = pipeline.run(\"...\")\n            &gt;&gt;&gt; print(result.result)\n\n        Notes:\n            - The pipeline uses either IPython if available or a MockIPython implementation\n            - Variables can be provided as either a dictionary or list\n            - Session state can be saved and loaded\n            - Method updates are handled through a structured BaseModel approach\n        \"\"\"\n    def __init__(\n        self,\n        agent: Any,\n        verbose: bool=False,\n        max_iter: int= 12,\n        variables: dict[str, Any] | list[Any] | None = None,\n        top_n: bool | None = None,\n        restore: bool | None = None,\n        max_think_after_think = None,\n        print_f=None,\n        web_js=False,\n        timeout_timer=25,\n        v_agent=None,\n        web_llm=None,\n    ):\n        \"\"\"\n        Initialize the Pipeline.\n\n        Args:\n            agent: AI agent instance to use for task execution\n            verbose: print internal results\n            max_iter: Maximum number of iterations (default: 12)\n            variables: Dictionary or list of variables to make available\n            top_n: Limit variable descriptions to top N most used\n            web_js: if the agent is allow to use the web\n        \"\"\"\n\n        self.timeout_timer = timeout_timer\n        self.top_n = top_n\n        self.max_iter = max_iter\n        self.max_think_after_think = max_think_after_think or max_iter // 2\n        self.agent = agent\n        self.v_agent = v_agent or agent\n        # self.agent.verbose = verbose\n        self.task = None\n        self.web_js = web_js\n        self.print_f = print_f\n        self.verbose_output = EnhancedVerboseOutput(verbose=verbose, print_f=self.print_f)\n        self.variables = self._process_variables(variables or {})\n        self.variables['auto_install'] = auto_install\n        self.execution_history = []\n        self.session_name = None\n\n        self.browser_session: BrowserWrapper | None = BrowserWrapper(llm=web_llm or agent.amd.model)\n        self.js_history: list[JSExecutionRecord] = []\n\n        self._session_dir = Path(get_app().appdata) / 'ChatSession' / agent.amd.name\n        self.ipython = MockIPython(self._session_dir, auto_remove=False)\n        self.chat_session = ChatSession(agent.memory, space_name=f\"ChatSession/{agent.amd.name}/Pipeline.session\", max_length=max_iter)\n        self.process_memory = ChatSession(agent.memory, space_name=f\"ChatSession/{agent.amd.name}/Process.session\", max_length=max_iter)\n\n        # Initialize interpreter with variables\n        self.init_keys = list(self.ipython.user_ns.keys()).copy()\n        if self.web_js:\n            self.variables['web_actions'] = self.browser_session.run\n            self.variables['browser_session'] = self.browser_session\n        self.ipython.user_ns.update(self.variables)\n\n        self.restore_var = restore\n\n        if restore:\n            self.restore()\n\n    def on_exit(self):\n        self.chat_session.on_exit()\n        self.process_memory.on_exit()\n        self.save_session(f\"Pipeline_Session_{self.agent.amd.name}\")\n\n    def restore(self):\n        self.load_session(f\"Pipeline_Session_{self.agent.amd.name}\")\n\n    def save_session(self, name: str):\n        \"\"\"Save current session\"\"\"\n        self.session_name = name\n        self.ipython.save_session(name)\n\n    def load_session(self, name: str):\n        \"\"\"Load saved session\"\"\"\n        self.ipython.load_session(name)\n        self.variables.update(self.ipython.user_ns)\n\n\n    def show_graph_html(self, output_file=None, get_output_html=False, get_output_net=False):\n\n        if output_file is None:\n            chat_graph = self.ipython._session_dir / 'chat_graph.html'\n            process_graph = self.ipython._session_dir / 'process_graph.html'\n            output_file = str(chat_graph.absolute())\n            p_output_file = str(process_graph.absolute())\n        else:\n            output_file = output_file + '_chat_graph.html'\n            p_output_file = output_file + '_process_graph.html'\n\n        return (self.chat_session.mem.memories.get(\n            self.chat_session.mem._sanitize_name(\n                self.chat_session.space_name)).vis(output_file=output_file,\n        get_output_html=get_output_html, get_output_net=get_output_net)  ,\n                self.process_memory.mem.memories.get(\n            self.process_memory.mem._sanitize_name(\n                self.process_memory.space_name)).vis(output_file=p_output_file,\n        get_output_html=get_output_html, get_output_net=get_output_net))\n\n    @staticmethod\n    def _process_variables(variables: dict[str, Any] | list[Any]) -&gt; dict[str, Any]:\n        \"\"\"\n        Process variables to generate meaningful names, using actual variable names where possible.\n        Instances get lowercase names based on their class names.\n\n        Args:\n            variables: Dictionary of variables or list of variables to process\n\n        Returns:\n            Dict[str, Any]: Processed variables with meaningful names\n        \"\"\"\n        if isinstance(variables, dict):\n            return variables\n\n        processed = {}\n        name_counts = defaultdict(int)\n\n        # Get caller's frame to find variable names\n        caller_frame = currentframe().f_back\n        caller_locals = {**caller_frame.f_locals, **caller_frame.f_globals}\n\n        def find_var_name(obj: Any) -&gt; str:\n            # Find original variable name if exists\n            var_names = [name for name, val in caller_locals.items()\n                         if val is obj and not name.startswith('_')]\n            if var_names:\n                return var_names[0]\n\n            # Special handling for functions\n            if isfunction(obj) or isclass(obj):\n                return obj.__name__\n            # Handle instances\n            elif hasattr(obj, '__class__'):\n                base_name = obj.__class__.__name__.lower()  # Lowercase for instances\n                count = name_counts[base_name]\n                name_counts[base_name] += 1\n                return f\"{base_name}_{count + 1}\" if count &gt; 0 else base_name\n\n            return type(obj).__name__\n\n        # Process each variable\n        for var in variables:\n            name = find_var_name(var)\n            while name in processed:\n                if name.rpartition('_')[0]:\n                    base, _, num = name.rpartition('_')\n                    try:\n                        num = int(num) + 1\n                        name = f\"{base}_{num}\"\n                    except ValueError:\n                        name = f\"{name}\"\n                else:\n                    name = f\"{name}\"\n\n            processed[name] = var\n\n        return processed\n\n    def _generate_variable_descriptions(\n        self,\n        top_n: int | None = None\n    ) -&gt; str:\n        \"\"\"\n        Generate detailed descriptions of variables, showing args, kwargs, docstrings, and return values.\n\n        Args:\n            top_n: Optional limit to show only top N variables\n\n        Returns:\n            str: Formatted variable descriptions in Markdown\n        \"\"\"\n        if top_n is None:\n            top_n = self.top_n\n\n        def format_value_preview(var: Any) -&gt; str:\n            \"\"\"Format preview of variable contents\"\"\"\n            try:\n                if isinstance(var, int | float | bool | str):\n                    return f\"`{repr(var)}`\"\n                elif isinstance(var, list | tuple | set):\n                    preview = str(list(var)[:3])[:-1] + \", ...]\"\n                    return f\"{len(var)} items: {preview}\"\n                elif isinstance(var, dict):\n                    preview_items = [f\"{repr(k)}: {repr(v)}\" for k, v in list(var.items())[:3]]\n                    return f\"{len(var)} pairs: {{{', '.join(preview_items)}, ...}}\"\n                return f\"&lt;{type(var).__name__}&gt;\"\n            except:\n                return \"&lt;error getting value&gt;\"\n\n        def get_instance_state(var: Any) -&gt; dict[str, Any]:\n            \"\"\"Get current instance state\"\"\"\n            state = {}\n            if hasattr(var, '__dict__'):\n                for name, value in var.__dict__.items():\n                    if not name.startswith('_') and not callable(value):\n                        state[name] = format_value_preview(value)\n            return state\n\n        # Process variables\n        variables = self.variables.items()\n        if top_n:\n            variables = list(variables)[:top_n]\n\n        descriptions = []\n        for name, var in variables:\n            if name in [\"PYTHON_EXEC\", \"__name__\", \"__builtins__\", \"__path__\", \"asyncio\"]:\n                continue\n\n            desc_parts = [f\"### {name}\"]\n\n            # Handle different types\n            if isinstance(var, type):  # Class\n                desc_parts.append(f\"**Type:** `class '{var.__name__}'`\")\n                if var.__doc__:\n                    desc_parts.append(f\"**Documentation:**\\n{var.__doc__.strip()}\")\n\n                # Show methods\n                methods = []\n                for attr_name, attr in var.__dict__.items():\n                    if (not attr_name.startswith('_') or attr_name == \"__init__\") and (isfunction(attr) or ismethod(attr)):\n                        try:\n                            sig = signature(attr)\n                            is_a = asyncio.iscoroutinefunction(var)\n                            methods.append(f\"- `{attr_name}{sig}` Async: `{is_a}\")\n                            if attr.__doc__:\n                                r = attr.__doc__.split('\\n')[0]\n                                methods.append(f\"  {r}\")\n                        except:\n                            methods.append(f\"- `{attr_name}()`\")\n                if methods:\n                    desc_parts.append(\"**Methods:**\\n\" + \"\\n\".join(methods))\n\n            elif isfunction(var) or ismethod(var):  # Function\n                try:\n                    sig = signature(var)\n                    desc_parts.append(f\"**Signature:** `{var.__name__}{sig}`\")\n                    is_a = asyncio.iscoroutinefunction(var)\n                    desc_parts.append(f\"**IS Async:** `{is_a}`\")\n                    if var.__doc__:\n                        desc_parts.append(f\"**Documentation:**\\n{var.__doc__.strip()}\")\n                    ret_anno = sig.return_annotation\n                    if ret_anno != Signature.empty:\n                        desc_parts.append(f\"**Returns:** `{ret_anno}`\")\n                except:\n                    desc_parts.append(f\"**Function:** `{var.__name__}()`\")\n\n            elif isinstance(var, BaseModel):  # Pydantic model\n                desc_parts.append(f\"**Type:** Pydantic model '{var.__class__.__name__}'\")\n                fields = []\n                for field_name, field in var.model_fields.items():\n                    value = getattr(var, field_name, None)\n                    fields.append(f\"- `{field_name}: {field.annotation.__name__}` = {repr(value)}\")\n                if fields:\n                    desc_parts.append(\"**Fields:**\\n\" + \"\\n\".join(fields))\n\n            else:  # Instance\n                class_type = var.__class__\n                desc_parts.append(f\"**Type:** `{class_type.__module__}.{class_type.__name__}`\")\n\n                # Instance initialization details\n                try:\n                    init = class_type.__init__\n                    sig = signature(init)\n                    params = list(sig.parameters.items())[1:]  # Skip self\n                    if params:\n                        args = []\n                        for name, param in params:\n                            if param.default == param.empty:\n                                args.append(name)\n                            else:\n                                args.append(f\"{name}={param.default}\")\n                        desc_parts.append(f\"**Init Args:** `{', '.join(args)}`\")\n                except:\n                    pass\n\n                # Instance state\n                state = get_instance_state(var)\n                if state:\n                    desc_parts.append(\"**Current instance State:**\")\n                    for attr_name, attr_value in state.items():\n                        desc_parts.append(f\"- `{attr_name}` = {attr_value}\")\n\n                # Documentation\n                doc = getdoc(var) or getdoc(class_type)\n                if doc:\n                    desc_parts.append(f\"**Documentation:**\\n{doc.strip()}\")\n\n            descriptions.append(\"\\n\".join(desc_parts))\n\n        return \"\\n\\n\".join(descriptions)\n\n    async def _execute_code(self, code: str, context:dict) -&gt; ExecutionRecord:\n        \"\"\"Execute code and track results\"\"\"\n        lang = context.get('lang', 'py')\n        try:\n\n            if'py' in lang:\n\n                return await self._execute_py(code)\n\n            elif self.web_js and 'js' in lang:\n                return await self._execute_js(code, context)\n\n        except Exception as e:\n            record = ExecutionRecord(code=code, result=None, error=str(e))\n            self.execution_history.append(record)\n            return record\n        record = ExecutionRecord(code=code, result=None, error=f\"Invalid lang {lang} valid is, {'js' if self.web_js else 'py'}]\")\n        self.execution_history.append(record)\n        return record\n\n    async def _execute_py(self, code) -&gt; ExecutionRecord:\n        show = True #len(code) &gt; 450 and code.count('while') &gt; 1 and code.count('print') &gt;= 1\n        result = await self.ipython.run_cell(code, show)\n\n        all_keys = list(self.ipython.user_ns.keys())\n\n        new_keys = [key for key in all_keys if key not in self.init_keys]\n        # Update pipeline variables from IPython namespace\n\n        for var_name in new_keys:\n            if var_name.startswith('_'):\n                continue\n            self.variables[var_name] = self.ipython.user_ns[var_name]\n\n        record = ExecutionRecord(code=code, result=result, error=None)\n        self.execution_history.append(record)\n        return record\n\n    async def _execute_js(self, code: str, context: dict) -&gt; ExecutionRecord:\n        \"\"\"Execute JavaScript code in browser context\"\"\"\n\n        if '&lt;script&gt;' in code:\n            code = code.split('&lt;script&gt;')[1]\n        if '&lt;/script&gt;' in code:\n            code = code.split('&lt;/script&gt;')[0]\n        def _format_error_markdown(error: str) -&gt; str:\n            \"\"\"Format error as Markdown\"\"\"\n            return f\"\"\"\n# Execution Error\n{error}\n\"\"\"\n\n        def _format_result_markdown(result_: dict) -&gt; str:\n            \"\"\"Format execution result as Markdown\"\"\"\n\n            def _clean_html_content(html: str) -&gt; str:\n                \"\"\"Clean HTML content and convert to Markdown-like format\"\"\"\n                soup = BeautifulSoup(html, 'html.parser')\n\n                # Remove scripts and styles\n                for script in soup([\"script\", \"style\"]):\n                    script.decompose()\n\n                # Extract text\n                text = soup.get_text()\n\n                # Clean up whitespace\n                lines = (line.strip() for line in text.splitlines())\n                chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n                text = '\\n'.join(chunk for chunk in chunks if chunk)\n\n                # Add Markdown formatting\n                text = re.sub(r'^(.+)$', r'&gt; \\1', text, flags=re.MULTILINE)\n\n                return text\n\n            md_parts = []\n\n            # Add title\n            md_parts.append(\"# Page Analysis Results\\n\")\n\n            # Format JavaScript result\n            if result_.get('js_result'):\n                md_parts.append(\"## JavaScript Execution Result\")\n                md_parts.append(\"```\")\n                md_parts.append(str(result_['js_result']))\n                md_parts.append(\"```\\n\")\n\n            # Format page state\n            if 'page_state' in result_:\n                md_parts.append(\"## Page Information\")\n                md_parts.append(f\"- **URL**: {result_['page_state']['url']}\")\n                md_parts.append(f\"- **Title**: {result_['page_state']['title']}\\n\")\n\n                # Clean and format content\n                if 'content' in result_['page_state']:\n                    content = _clean_html_content(result_['page_state']['content'])\n                    if content:\n                        md_parts.append(\"### Page Content\")\n                        md_parts.append(content + \"\\n\")\n\n            # Format extracted data\n            if result_.get('extracted_data'):\n                md_parts.append(\"## Extracted Data\")\n                for key, value in result_['extracted_data'].items():\n                    if value:\n                        md_parts.append(f\"### {key.replace('_', ' ').title()}\")\n                        if isinstance(value, list):\n                            for item in value:\n                                md_parts.append(f\"- {item}\")\n                        else:\n                            md_parts.append(str(value))\n                        md_parts.append(\"\")\n\n            return \"\\n\".join(md_parts)\n\n        try:\n            # Prepare execution context\n            url = context.get('url')\n            page = None\n            result = None\n            page_state = {}\n\n            extracted_data = None\n            if url:\n                page = await self.browser_session.navigate(url)\n                parser = self.browser_session.get_parser()\n                markdown = await parser.to_markdown(page)\n\n                if 'patterns' in context:\n                    extracted_data = await parser.to_structured(page, context['patterns'])\n\n                page_state = {\n                    'url': page.url,\n                    'title': await page.title(),\n                    'content': markdown,\n                }\n\n            if code:\n                result = await self.browser_session.execute_js(code, page)\n\n                if isinstance(result, dict) and 'success' in result:\n                    if not result['success']:\n                        raise Exception(f\"JavaScript Error: {result.get('error')}\\nStack: {result.get('stack')}\")\n                    result = result.get('result')\n\n            # Capture page state after execution\n\n\n            # Extract data using patterns if specified\n\n            # Create execution record\n            record = JSExecutionRecord(\n                code=code,\n                result=result,\n                page_state=page_state,\n                extracted_data=extracted_data\n            )\n\n            self.js_history.append(record)\n\n            # Convert to standard ExecutionRecord for pipeline\n            return ExecutionRecord(\n                code=code,\n                result=_format_result_markdown({\n                    'js_result': result,\n                    'page_state': page_state,\n                    'extracted_data': extracted_data\n                }),\n                error=None\n            )\n\n        except Exception as e:\n            error_md = _format_error_markdown(str(e))\n            return ExecutionRecord(code=code, result=None, error=error_md)\n\n\n    def __str__(self):\n        \"\"\"String representation of pipeline session\"\"\"\n        return str(self.ipython)\n\n    async def _process_think_result(self, think_result: ThinkResult, task:str) -&gt; tuple[ThinkState,  ExecutionRecord | str | None]:\n        \"\"\"Process the result of agent thinking\"\"\"\n        if think_result.action == 'brake':\n            return ThinkState.BRAKE, think_result.content\n\n        elif think_result.action == 'update':\n            if think_result.context.get('object_name') is None:\n                return ThinkState.ACTION, \"no object_name specified in context!\"\n            if think_result.context.get('file') is not None:\n                self.ipython.user_ns['__file__'] = think_result.context.get('file')\n            result = await self.verbose_output.process(think_result.action,\n                                                       self.ipython.modify_code(code=think_result.content,\n                                                    object_name=think_result.context.get('object_name'),))\n            return ThinkState.PROCESSING, result\n\n        elif think_result.action == 'code':\n            if think_result.context.get('file') is not None:\n                self.ipython.user_ns['__file__'] = think_result.context.get('file')\n            result = await self._execute_code(think_result.content, think_result.context)\n            return ThinkState.PROCESSING, result\n\n        elif think_result.action == 'done':\n            return ThinkState.DONE, think_result.content\n\n        elif think_result.action == 'infos':\n            infos = await self.chat_session.get_reference(think_result.content, to_str=True)\n            return ThinkState.ACTION, infos\n\n        elif think_result.action == 'guide':\n            details = await self.process_memory.get_reference(think_result.content, to_str=True)\n            plan = await self.agent.a_mini_task(f\"\"\"You are an AI guidance system designed to help determine the next step in a task and provide instructions on how to proceed. Your role is to analyze the given information and offer clear, actionable guidance for the next steps.\n\nFirst, carefully read and understand the main task:\n&lt;main_task&gt;\n{task}\n&lt;/main_task&gt;\n\nNext, review the last thought of the agent, if available:\n&lt;last_thought&gt;\n{think_result.content}\n{think_result.context}\n&lt;/last_thought&gt;\n\nThen, examine the processing history, if provided:\n&lt;processing_history&gt;\n{details}\n&lt;/processing_history&gt;\n\nTo determine the next step and provide guidance, follow these instructions:\n\n1. Analyze the main task, breaking it down into smaller, manageable steps if necessary.\n2. Consider the last thought and processing history to understand the current progress and context.\n3. Identify any gaps, challenges, or areas that need further attention.\n4. Determine the most logical and efficient next step to move the task forward.\n5. Provide clear, concise instructions on how to complete this next step.\n\nWhen formulating your response, follow this structure:\n\n1. Begin with a brief summary of the current situation, referencing the main task and any relevant information from the last thought or processing history.\n2. Clearly state the next step that should be taken.\n3. Provide detailed instructions on how to complete this step, including any specific techniques, methods, or considerations to keep in mind.\n4. If applicable, mention any potential challenges or pitfalls to be aware of during this step.\n5. Conclude with a brief statement on how this step contributes to the overall progress of the main task.\n\nFormat your response using the following sections:\n&lt;summary&gt;\n(Include your summary of the current situation here)\n&lt;/summary&gt;\n\n&lt;next_step&gt;\n(State the next step to be taken here)\n&lt;/next_step&gt;\n\n&lt;instructions&gt;\n(Provide detailed instructions for completing the next step here)\n&lt;/instructions&gt;\n\n&lt;challenges&gt;\n(If applicable, mention potential challenges or pitfalls here)\n&lt;/challenges&gt;\n\n&lt;conclusion&gt;\n(Briefly state how this step contributes to overall progress)\n&lt;/conclusion&gt;\n\nRemember to be clear, concise, and specific in your guidance. Avoid vague or ambiguous instructions, and provide concrete examples or explanations where necessary.\"\"\")\n            return ThinkState.ACTION, plan\n\n        return ThinkState.ACTION, None\n\n    async def execute(self, code:str):\n        return str(await self._execute_code(code))\n\n    def clear(self):\n        self.chat_session.history = []\n        self.process_memory.history = []\n        self.execution_history = []\n        self.variables = {}\n        self.ipython.reset()\n        self.js_history = []\n\n    async def get_process_hint(self, task):\n        return await self.process_memory.get_reference(task, to_str=True), await self.chat_session.get_reference(task, to_str=True)\n\n    def show_vars(self):\n        return self.verbose_output.log_state(\"VARS\", self.variables, override=True)\n\n    def set_file(self, full_file_path_and_name):\n        if not os.path.exists(full_file_path_and_name):\n            print(\"Invalid file\")\n            return\n        self.ipython.user_ns[\"__file__\"] = full_file_path_and_name\n\n    async def run(self, task, do_continue=False) -&gt; PipelineResult:\n        \"\"\"Run the pipeline with separated thinking and processing phases\"\"\"\n        state = ThinkState.ACTION\n        result = None\n        original_task = task\n        if not do_continue:\n            task = self.agent.mini_task(task, \"user\", f\"\"\"You are an AI assistant tasked with refactoring a user-provided task description into a more structured format with context learning and examples. Your goal is to create a comprehensive and well-organized task description that incorporates model flows and potential code fixes.\n\nFirst, I will provide you with a task description and some example tasks. Please read them carefully:\n\n&lt;existing_globals&gt;\n{self._generate_variable_descriptions()}\n&lt;/existing_globals&gt;\n\n&lt;example_tasks&gt;\nTask: Create a simple analysis of a list of numbers\n- Generate a list of 100 random numbers between 1-1000\n- Calculate the mean, median, and standard deviation\n- Create a histogram of the distribution\n- Print all results and display the plot\n\nTask: Create a reinforcement learning (RL) agent to play a simple game\n- Set up an OpenAI Gym environment (e.g., CartPole)\n- Implement a Q-learning or Deep Q-Network (DQN) agent\n- Train the model and optimize hyperparameters\n- Visualize learning progress with reward graphs\n- Save and reload trained models for inference\n- Provide an option to let the trained agent play in real time\n\nTask: Perform edge detection on an image\n- Load an image from a URL or local file\n- Convert the image to grayscale\n- Apply Gaussian blur to reduce noise\n- Use Canny edge detection to extract edges\n- Display the original and processed images side by side\n- Save the output image\n\nTask: Build a basic sentiment analysis system\n- Load a dataset of movie reviews (you can use a small sample)\n- Preprocess the text (remove punctuation, lowercase, etc.)\n- Create a TF-IDF vectorizer\n- Split data into training and testing sets\n- Train a classifier (e.g., Naive Bayes or LogisticRegression)\n- Evaluate performance with accuracy, precision, recall\n- Create a confusion matrix visualization\n- Make predictions on new sample texts\n&lt;/example_tasks&gt;\n\nNow, please refactor the given task description using the following guidelines:\n\n1. Analyze the task description and identify the main components and objectives.\n\n2. Structure the refactored task in a similar format to the example tasks, including:\n   - A clear title that summarizes the task\n   - A difficulty level (Easy, Intermediate, Hard, or Super Hard)\n   - A brief introduction to the task's context and purpose\n   - A code block containing step-by-step instructions\n   - A list of required skills, libraries, or technologies\n\n3. Incorporate model flows by breaking down the task into logical steps and explaining the process flow.\n\n4. Include potential code fixes or common pitfalls that users might encounter while working on the task.\n\n5. Add context learning elements by providing brief explanations or resources for key concepts related to the task.\n\n6. Ensure that the refactored task is comprehensive and can stand alone as a learning exercise.\n\nPlease provide your refactored task description within &lt;refactored_task&gt; tags. Use appropriate subheadings and formatting to make the description clear and easy to read.\n\nAdditional tips:\n- Mention any prerequisites or assumed knowledge\n- Suggest potential extensions or variations of the task for further learning\n\nRemember to maintain the original intent and complexity of the task while improving its structure and clarity.\"\"\")\n            if '&lt;refactored_task&gt;' in task:\n                task = task.split('&lt;refactored_task&gt;')[1]\n            if '&lt;/refactored_task&gt;' in task:\n                task = task.split('&lt;/refactored_task&gt;')[0]\n        code_follow_up_prompt = f\"\"\"\nYou are an AI assistant responsible for evaluating task completion and providing feedback on the execution process. Your goal is to determine if a given task has been completed based on the execution result, and to offer insights for future improvements.\n\nYou will be provided with two inputs:\n&lt;task_description&gt;\n{original_task}\n{f'&lt;refactored_task_description_from_ai&gt;{task}&lt;/refactored_task_description_from_ai&gt;' if not do_continue else ''}\n&lt;/task_description&gt;\n\n&lt;code&gt;\n#CODE#\n&lt;/code&gt;\n\n&lt;execution_result&gt;\n#EXECUTION_RESULT#\n&lt;/execution_result&gt;\n\nFirst, carefully analyze the task description and the execution result. Determine whether the task has been completed successfully based on the information provided.\n\nIf the task is completed:\n1. Prepare a brief statement indicating that the task is done.\n2. Summarize the output for the user in a clear and concise manner.\n\nIf the task is not completed:\n1. Prepare a brief statement indicating that the task is not done.\n2. Identify the specific aspects of the task that remain incomplete.\n\nRegardless of task completion status, evaluate the procedure and effectiveness of the execution:\n1. Analyze the workflow: Describe the steps taken in the execution process.\n2. Assess effectiveness: Determine how well the procedure achieved the desired outcome.\n3. Identify errors: Pinpoint any mistakes or inefficiencies in the execution.\n4. Provide recommendations: Suggest improvements for future task executions.\n\ntip: Enclose mutil line strings property for python eval to function!\ntip: Set is_completed True if all requirements are completed from &lt;task_description&gt;.\ntip: Help the Agent with your analyses to finalize the &lt;task_description&gt;.\n{'tip: Prefer new informations from &lt;execution_result&gt; over &lt;refactored_task_description_from_ai&gt; based of &lt;code&gt;' if not do_continue else ''}\nnote : for the final result only toke information from the &lt;execution_result&gt;. if the relevant informations is not avalabel try string withe tips in the recommendations. else set is_completed True and return the teh Task failed!\nEnsure that your evaluation is thorough, constructive, and provides actionable insights for improving future task executions.\nAdd guidance based on the the last execution result\"\"\"\n        code_follow_up_prompt_ = [code_follow_up_prompt]\n        initial_prompt = f\"\"\"\nYou are an AI py coding agent specializing in iterative development and code refinement, designed to perform tasks that involve thinking. Your goal is to complete the given task while demonstrating a clear thought process throughout the execution.\nSYSTEM STATE:\n&lt;current_state&gt;\nIteration: #ITER#\nStatus: #STATE#\nLast EXECUTION: #EXECUTION#\n&lt;/current_state&gt;\n\nENVIRONMENT: {'current file :'+self.ipython.user_ns.get(\"__file__\")  if self.ipython.user_ns.get(\"__file__\") is not None else ''}\n\n'''&lt;global_variables&gt;\n#LOCALS#\n&lt;/global_variables&gt;'''\n\nMEMORY:\n&lt;process_memory&gt;\n#PHINT#\n&lt;/process_memory&gt;\n\n&lt;chat_memory&gt;\n#CHINT#\n&lt;/chat_memory&gt;\n\nVALIDATION CHECKLIST (Must verify before each action):\n1. \u2713 Check existing variables in ENVIRONMENT &lt;global_variables&gt;\n2. \u2713 Verify existing functions and classes\n3. \u2713 Review current imports\n4. \u2713 Confirm method signatures\n5. \u2713 Validate state preservation\n\nWORKFLOW STEPS:\n1. Analyze Current State:\n   - Reason and use all avalabel context\n   - Do not repeat the same errors\n   - Review existing implementations\n   - Check variable values\n   - Verify import statements\n   - Document dependencies\n\n2. Plan Change:\n   - NO example/simulation/simulate\n   - No demo er moc Data no Simulations Allowed or u will die!!\n   - Use existing variables and code when possible\n   - Prefer updates over rewrites\n\n3. Execute Change:\n   - Use appropriate action\n   - Maintain existing state\n   - Document modifications\n   - Verify results\n\nYou will use a structure called ThinkResult to organize your thoughts and actions.\nFor each step of your task, follow this process:\n\nACTIONS:\n1. 'code':\n    - MUST check &lt;global_variables&gt; first\n    - NEVER create demo functions\n    - Include 'reason'\n    - lang default 'py'\n    - Required: code in content\n    - code MUST call a function or display the row variabel / value at the end!\n    - Required: {{'context':{{'lang':'py',  'reason': ... }}...}}\n    - Optional file key in context example {{'context':{{'lang':'py',  'file': 'main.py' ,  'reason': ... }}...}}\n    - py code allows for toplevel await !!! use it !!! like\n:file-start:\nprint(\"using toplevel await\")\nawait abc()\n:file-end:\n\n    - Tip: use comments to reason with in the code\n3. 'infos': Request specific details\n4. 'guide': Get step clarification use on complex task and ery 5 step for staying on trak!\n5. 'brake': Pause for assessment\n6. 'done': Summarize changes\n\nCODE CONSTRAINTS:\n1. State Preservation:\n   - ALL variables ar persist\n   - ALL functions remain\n   - ALL classes ar maintained\n\n2. Import Management:\n   - Check &lt;global_variables&gt; for modules\n   - Use absolute imports\n   - Document new dependencies\n\n3. Function Handling:\n   - NEVER overwrite existing\n   - Use update for changes\n   - Preserve signatures\n\n4. Variable Scope:\n   - Maintain existing scope\n   - Check for conflicts\n   - Document state changes\n\nEXECUTION RULES:\n1. VERIFY before create\n2. UPDATE don't replace\n3. TEST after each change\n\nNext Action Required:\n1. Review current state\n2. Check existing code\n3. Execute with state preservation\n\n!!CRITICAL!!\n- NO demo functions\n- NO placeholder functions\n- USE existing code\n- FOR Implementations prefer writing large production redy code chunks.\n- FOR reasoning and validation write small code blocks.\n- THE CODE must call something or end the code with an value!\n- NO INFINIT LOOPS! none breakable while loops ar not allowed, exception ui (closed by user)\n- NO 'python' top level return, only write the variabel or value itself!\n- 'code is run using exec! do not use !pip ...'\n'- instead use auto_install(package_name, install_method=\"pip\", upgrade=False, quiet=False, version=None, extra_args=None)'\n# Example usage first time\n\u2502 auto_install('pandas', version='1.3.0')\n\u2502 import pandas\n\u2502 auto_install('pygame')\n\u2502 import pygame\n\u2502 auto_install('numpy')\n\u2502 import numpy as np\n!TIPS!\n- '&lt;global_variables&gt; can contain instances and functions you can use in your python' code\n- if the function is async you can use top level await\n- if their is missing of informations try running code to get the infos\n- if you got stuck or need assistance break with a question to the user.\n'- run functions from &lt;global_variables&gt; using name(*args, **kwargs) or await name(*args, **kwargs)'\n'- &lt;global_variables&gt; ar global accessible!'\n'- if an &lt;global_variables&gt; name is lower lists an redy to use instance'\n\"\"\"\n        p_hint, c_hint = await self.get_process_hint(task)\n        initial_prompt = initial_prompt.replace('#PHINT#', p_hint)\n        initial_prompt = initial_prompt.replace('#CHINT#', c_hint)\n        initial_prompt_ = initial_prompt\n        iter_i = 0\n        iter_p = 0\n        iter_tat = 0\n        next_infos = \"\"\n        if not do_continue:\n            await self.chat_session.add_message({'role': 'user', 'content': task})\n        else:\n            self.restore()\n            await self.chat_session.add_message({'role': 'user', 'content': task})\n\n        if self.web_js and self.browser_session is None:\n            self.browser_session = BrowserWrapper(llm=self.agent.amd.modle)\n\n        # await self.verbose_output.log_message('user', task)\n        self.verbose_output.log_header(task)\n        while state != ThinkState.DONE:\n            iter_i += 1\n            t0 = time.perf_counter()\n            prompt = initial_prompt.replace('#ITER#', f'{iter_i} max {self.max_iter}')\n            prompt = prompt.replace('#STATE#', f'{state.name}')\n            prompt = prompt.replace('#EXECUTION#', f'{next_infos}')  if next_infos else prompt.replace('Last EXECUTION: #EXECUTION#', '')\n            prompt = prompt.replace('#LOCALS#', f'{self._generate_variable_descriptions()}')\n            self.verbose_output.log_state(state.name, {})\n            self.verbose_output.formatter.print_iteration(iter_i, self.max_iter)\n            if state == ThinkState.ACTION:\n                iter_tat +=1\n                if iter_tat &gt; self.max_think_after_think:\n                    state = ThinkState.BRAKE\n            else:\n                iter_tat = 0\n\n            if state == ThinkState.ACTION:\n                # Get agent's thoughts\n                think_dicts = await self.verbose_output.process(state.name, self.agent.a_format_class(\n                    ThinkResults,\n                    prompt,\n                    message=self.chat_session.get_past_x(self.max_iter*2, last_u=not do_continue).copy()+([self.process_memory.history[-1]] if self.process_memory.history else []) ,\n                ))\n                think_dicts = think_dicts.get(\"actions\")\n                if think_dicts is None:\n                    think_dicts = [await self.verbose_output.process(state.name, self.agent.a_format_class(\n                        ThinkResult,\n                        prompt,\n                        message=self.chat_session.get_past_x(self.max_iter * 2, last_u=not do_continue).copy() + (\n                            [self.process_memory.history[-1]] if self.process_memory.history else []),\n                    ))]\n                if len(think_dicts) == 1:\n                    think_dict = think_dicts[0]\n                else:\n                    for think_dict in think_dicts[:-1]:\n                        if think_dict.get('context') is None:\n                            think_dict['context'] = {'context': 'N/A'}\n                        if not isinstance(think_dict.get('context'), dict):\n                            think_dict['context'] = {'context': think_dict.get('context')}\n                        think_result = ThinkResult(**think_dict)\n                        await self.chat_session.add_message(\n                            {'role': 'assistant', 'content': think_result.content + str(think_result.context)})\n                        state, result = await self.verbose_output.process(think_dict.get(\"action\"),\n                                                                          self._process_think_result(think_result,\n                                                                                                     task=task))\n                        if result:\n                            await self.chat_session.add_message(\n                                {'role': 'system', 'content': 'Evaluation: ' + str(result)})\n                            await self.verbose_output.log_message('system', str(result))\n                    think_dict = think_dicts[-1]\n                await self.verbose_output.log_think_result(think_dict)\n                if think_dict.get('context') is None:\n                    think_dict['context'] = {'context': 'N/A'}\n                if not isinstance(think_dict.get('context'), dict):\n                    think_dict['context'] = {'context': think_dict.get('context')}\n                think_result = ThinkResult(**think_dict)\n                state, result = await self.verbose_output.process(think_dict.get(\"action\"), self._process_think_result(think_result, task=task))\n                await self.chat_session.add_message({'role': 'assistant', 'content': think_result.content + str(think_result.context)})\n                if result:\n                    await self.chat_session.add_message({'role': 'system', 'content': 'Evaluation: '+str(result)})\n                    await self.verbose_output.log_message('system', str(result))\n                    code_follow_up_prompt_[0] = code_follow_up_prompt.replace(\"#EXECUTION_RESULT#\", str(result))\n                    if isinstance(result ,ExecutionRecord):\n                        code_follow_up_prompt_[0] = code_follow_up_prompt_[0].replace(\"#CODE#\", result.code)\n                    else:\n                        code_follow_up_prompt_[0] = code_follow_up_prompt_[0].replace(\"#CODE#\", self._generate_variable_descriptions())\n                else:\n                    code_follow_up_prompt_[0] = code_follow_up_prompt.replace(\"#EXECUTION_RESULT#\", str(think_result))\n                    code_follow_up_prompt_[0] = code_follow_up_prompt_[0].replace(\"#CODE#\",\n                                                                              self._generate_variable_descriptions())\n\n\n            elif state == ThinkState.PROCESSING:\n                # Get agent's thoughts\n                class Next(BaseModel):\n                    is_completed: bool\n                    recommendations: str\n                    errors: str\n                    effectiveness: str\n                    workflow: str\n                    text: str\n                # Format the agent's thoughts into a structured response\n                _agent = self.v_agent if self.v_agent is not None else self.agent\n                next_dict = await self.verbose_output.process(state.name, _agent.a_format_class(\n                    Next,\n                    code_follow_up_prompt_[0],\n                    message=self.chat_session.get_past_x(self.max_iter*2, last_u=not do_continue).copy(),\n                ))\n                next_infos = json.dumps(next_dict)\n                await self.verbose_output.log_process_result(next_dict)\n                await self.process_memory.add_message({'role': 'assistant', 'content': next_infos.replace('workflow:', 'past-workflow:')})\n                iter_p += 1\n                code_follow_up_prompt_[0] = code_follow_up_prompt\n                if not next_dict.get('is_completed', True):\n                    state = ThinkState.ACTION\n                    initial_prompt = initial_prompt_.replace('#ITER#',f'#ITER#\\nReasoning assist result: {next_dict}')\n                    continue\n                elif next_dict.get('is_completed', False):\n                    result = next_dict.get('text', '')\n                    state = ThinkState.DONE\n                    continue\n                else:\n                    result = next_dict.get('text', '')\n                    break\n\n            elif state == ThinkState.BRAKE:\n                break\n\n            if iter_i &lt; self.max_iter:\n                if time.perf_counter() -t0 &lt; self.timeout_timer*2.5:\n                    with Spinner(f\"Prevent rate limit posing for {self.timeout_timer}s\", symbols='+', time_in_s=self.timeout_timer, count_down=True):\n                        await asyncio.sleep(self.timeout_timer)\n            else:\n                state = ThinkState.BRAKE\n                if isinstance(result, ExecutionRecord):\n                    result = result.result\n                elif isinstance(result, str):\n                    pass\n                else:\n                    result = \"Max iterations\"\n                break\n\n        self.verbose_output.log_state(state.name, {})\n\n        return PipelineResult(\n            variables=self.variables,\n            result=result,\n            execution_history=self.execution_history,\n            message=self.chat_session.get_past_x(iter_i*2, last_u=not do_continue),\n        )\n\n    async def run_project(self, task, lang='py', execute_function=None):\n        if execute_function is None:\n            if lang == 'py':\n                execute_function = default_python_execute_function\n            elif lang == 'rust':\n                execute_function = default_rust_execute_function\n            else:\n                raise ValueError(f\"Unsupported language: {lang}\")\n        class FileAction(BaseModel):\n            action: str\n            path: str\n            content: str | None = None\n\n        class ProjectThinkResult(BaseModel):\n            action: str\n            file_actions: list[FileAction]\n            reasoning: str\n\n        class ProjectPipelineResult(BaseModel):\n            result: str\n            execution_history: list[str]\n            files: dict[str, str]\n        state = ThinkState.ACTION\n        result = None\n        vfs = VirtualFileSystem(self._session_dir / f\"project_{lang}\")\n\n        project_prompt = f\"\"\"\n    You are an AI coding agent specializing in {lang} project development. Your task is to create, modify, and manage files within a project structure to complete the given task. Use the VirtualFileSystem to interact with files.\n\n    TASK DESCRIPTION:\n    {task}\n    CURRENT FILES:\n    #files#\n\n    WORKFLOW STEPS:\n    1. Analyze the current project state\n    2. Plan necessary changes or additions\n    3. Execute changes using file actions\n    4. Evaluate the project's progress\n\n    Use the ProjectThinkResult structure to organize your thoughts and actions:\n\n    class ProjectThinkResult(BaseModel):\n        action: str  # 'code', 'evaluate', 'done'\n        file_actions: List[FileAction]\n        reasoning: str\n\n    class FileAction(BaseModel):\n        action: str  # 'write', 'read', 'delete', 'list'\n        path: str\n        content: Optional[str] = None\n\n    EXECUTION RULES:\n    1. Use absolute paths for all file operations\n    2. Maintain a clear project structure\n    3. Document your code and reasoning\n    4. Ensure all necessary files are created and properly linked\n    5. Use the appropriate language syntax and best practices for {lang}\n\n    Next Action Required:\n    1. Review the current project state\n    2. Plan the next step in project development\n    3. Execute file actions to implement changes\n    \"\"\"\n\n        execution_history = []\n        files = {}\n\n        iter_i = 0\n        self.verbose_output.log_header(task)\n\n        while state != ThinkState.DONE:\n            iter_i += 1\n            self.verbose_output.formatter.print_iteration(iter_i, self.max_iter)\n            if iter_i&gt;self.max_iter:\n                break\n            if state == ThinkState.ACTION:\n                think_result = await self.agent.a_format_class(\n                    ProjectThinkResult,\n                    project_prompt.replace('#files#', vfs.print_file_structure()),\n                    message=execution_history\n                )\n                self.verbose_output.log_state(state.name, think_result)\n                think_result = ProjectThinkResult(**think_result)\n                for file_action in think_result.file_actions:\n                    path = file_action.path\n                    Path(file_action.path).mkdir(exist_ok=True)\n                    if file_action.action == 'write':\n                        vfs.write_file(path, file_action.content)\n                        files[path] = file_action.content\n                    elif file_action.action == 'read':\n                        content = vfs.read_file(path)\n                        files[path] = content\n                    elif file_action.action == 'delete':\n                        vfs.delete_file(path)\n                        files.pop(path, None)\n                    elif file_action.action == 'list':\n                        dir_contents = vfs.list_directory(path)\n                        files[path] = str(dir_contents)\n\n                if think_result.action == 'evaluate':\n                    state = ThinkState.PROCESSING\n                elif think_result.action == 'done':\n                    state = ThinkState.DONE\n\n                execution_history.append(f\"Action: {think_result.action}\\nReasoning: {think_result.reasoning}\")\n\n            elif state == ThinkState.PROCESSING:\n                if execute_function:\n                    execution_result = await execute_function(files)\n                    execution_history.append(f\"Execution Result: {execution_result}\")\n\n                    evaluation_prompt = f\"\"\"\n    Evaluate the current state of the project based on the execution result:\n\n    {execution_result}\n\n    Determine if the project is complete or if further modifications are needed.\n    \"\"\"\n                    evaluation = await self.agent.a_format_class(\n                        ProjectThinkResult,\n                        evaluation_prompt,\n                        message=execution_history\n                    )\n                    self.verbose_output.log_state(state.name, evaluation)\n                    evaluation = ProjectThinkResult(**evaluation)\n                    if evaluation.action == 'done':\n                        state = ThinkState.DONE\n                        result = execution_result\n                    else:\n                        state = ThinkState.ACTION\n                else:\n                    state = ThinkState.ACTION\n            else:\n                break\n\n        return ProjectPipelineResult(\n            result=result,\n            execution_history=execution_history,\n            files=files\n        )\n\n    async def __aenter__(self):\n        self.clear()\n        return self\n\n    async def configure(self, verbose=None, print_function=None, with_js=False, agent=None, variables=None, web_kwargs=None):\n        if verbose is not None and (print_function is not None or verbose != self.verbose_output.verbose):\n            if agent is None:\n                agent = self.agent\n            else:\n                self.agent = agent\n            agent.verbose = verbose\n            self.verbose_output = EnhancedVerboseOutput(verbose=verbose, print_f=print_function)\n\n            if print_function is not None:\n                agent.print_verbose = print_function\n        if variables:\n            self.variables = {**self.variables, **self._process_variables(variables)}\n        if with_js and web_kwargs:\n            self.browser_session: BrowserWrapper | None = BrowserWrapper(**web_kwargs)\n        self.web_js = with_js\n        if self.restore_var:\n            self.restore()\n\n        return self\n\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        if self.web_js:\n            await self.browser_session.close()\n            if self.restore_var:\n                self.save_session(f\"Pipeline_Session_{self.agent.amd.name}\")\n        if exc_type is not None:\n            print(f\"Exception occurred: {exc_value}\")\n        else:\n            print(\"Pipe Exit\")\n</code></pre> <code>__init__(agent, verbose=False, max_iter=12, variables=None, top_n=None, restore=None, max_think_after_think=None, print_f=None, web_js=False, timeout_timer=25, v_agent=None, web_llm=None)</code> \u00b6 <p>Initialize the Pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Any</code> <p>AI agent instance to use for task execution</p> required <code>verbose</code> <code>bool</code> <p>print internal results</p> <code>False</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations (default: 12)</p> <code>12</code> <code>variables</code> <code>dict[str, Any] | list[Any] | None</code> <p>Dictionary or list of variables to make available</p> <code>None</code> <code>top_n</code> <code>bool | None</code> <p>Limit variable descriptions to top N most used</p> <code>None</code> <code>web_js</code> <p>if the agent is allow to use the web</p> <code>False</code> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __init__(\n    self,\n    agent: Any,\n    verbose: bool=False,\n    max_iter: int= 12,\n    variables: dict[str, Any] | list[Any] | None = None,\n    top_n: bool | None = None,\n    restore: bool | None = None,\n    max_think_after_think = None,\n    print_f=None,\n    web_js=False,\n    timeout_timer=25,\n    v_agent=None,\n    web_llm=None,\n):\n    \"\"\"\n    Initialize the Pipeline.\n\n    Args:\n        agent: AI agent instance to use for task execution\n        verbose: print internal results\n        max_iter: Maximum number of iterations (default: 12)\n        variables: Dictionary or list of variables to make available\n        top_n: Limit variable descriptions to top N most used\n        web_js: if the agent is allow to use the web\n    \"\"\"\n\n    self.timeout_timer = timeout_timer\n    self.top_n = top_n\n    self.max_iter = max_iter\n    self.max_think_after_think = max_think_after_think or max_iter // 2\n    self.agent = agent\n    self.v_agent = v_agent or agent\n    # self.agent.verbose = verbose\n    self.task = None\n    self.web_js = web_js\n    self.print_f = print_f\n    self.verbose_output = EnhancedVerboseOutput(verbose=verbose, print_f=self.print_f)\n    self.variables = self._process_variables(variables or {})\n    self.variables['auto_install'] = auto_install\n    self.execution_history = []\n    self.session_name = None\n\n    self.browser_session: BrowserWrapper | None = BrowserWrapper(llm=web_llm or agent.amd.model)\n    self.js_history: list[JSExecutionRecord] = []\n\n    self._session_dir = Path(get_app().appdata) / 'ChatSession' / agent.amd.name\n    self.ipython = MockIPython(self._session_dir, auto_remove=False)\n    self.chat_session = ChatSession(agent.memory, space_name=f\"ChatSession/{agent.amd.name}/Pipeline.session\", max_length=max_iter)\n    self.process_memory = ChatSession(agent.memory, space_name=f\"ChatSession/{agent.amd.name}/Process.session\", max_length=max_iter)\n\n    # Initialize interpreter with variables\n    self.init_keys = list(self.ipython.user_ns.keys()).copy()\n    if self.web_js:\n        self.variables['web_actions'] = self.browser_session.run\n        self.variables['browser_session'] = self.browser_session\n    self.ipython.user_ns.update(self.variables)\n\n    self.restore_var = restore\n\n    if restore:\n        self.restore()\n</code></pre> <code>__str__()</code> \u00b6 <p>String representation of pipeline session</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __str__(self):\n    \"\"\"String representation of pipeline session\"\"\"\n    return str(self.ipython)\n</code></pre> <code>load_session(name)</code> \u00b6 <p>Load saved session</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def load_session(self, name: str):\n    \"\"\"Load saved session\"\"\"\n    self.ipython.load_session(name)\n    self.variables.update(self.ipython.user_ns)\n</code></pre> <code>run(task, do_continue=False)</code> <code>async</code> \u00b6 <p>Run the pipeline with separated thinking and processing phases</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>    async def run(self, task, do_continue=False) -&gt; PipelineResult:\n        \"\"\"Run the pipeline with separated thinking and processing phases\"\"\"\n        state = ThinkState.ACTION\n        result = None\n        original_task = task\n        if not do_continue:\n            task = self.agent.mini_task(task, \"user\", f\"\"\"You are an AI assistant tasked with refactoring a user-provided task description into a more structured format with context learning and examples. Your goal is to create a comprehensive and well-organized task description that incorporates model flows and potential code fixes.\n\nFirst, I will provide you with a task description and some example tasks. Please read them carefully:\n\n&lt;existing_globals&gt;\n{self._generate_variable_descriptions()}\n&lt;/existing_globals&gt;\n\n&lt;example_tasks&gt;\nTask: Create a simple analysis of a list of numbers\n- Generate a list of 100 random numbers between 1-1000\n- Calculate the mean, median, and standard deviation\n- Create a histogram of the distribution\n- Print all results and display the plot\n\nTask: Create a reinforcement learning (RL) agent to play a simple game\n- Set up an OpenAI Gym environment (e.g., CartPole)\n- Implement a Q-learning or Deep Q-Network (DQN) agent\n- Train the model and optimize hyperparameters\n- Visualize learning progress with reward graphs\n- Save and reload trained models for inference\n- Provide an option to let the trained agent play in real time\n\nTask: Perform edge detection on an image\n- Load an image from a URL or local file\n- Convert the image to grayscale\n- Apply Gaussian blur to reduce noise\n- Use Canny edge detection to extract edges\n- Display the original and processed images side by side\n- Save the output image\n\nTask: Build a basic sentiment analysis system\n- Load a dataset of movie reviews (you can use a small sample)\n- Preprocess the text (remove punctuation, lowercase, etc.)\n- Create a TF-IDF vectorizer\n- Split data into training and testing sets\n- Train a classifier (e.g., Naive Bayes or LogisticRegression)\n- Evaluate performance with accuracy, precision, recall\n- Create a confusion matrix visualization\n- Make predictions on new sample texts\n&lt;/example_tasks&gt;\n\nNow, please refactor the given task description using the following guidelines:\n\n1. Analyze the task description and identify the main components and objectives.\n\n2. Structure the refactored task in a similar format to the example tasks, including:\n   - A clear title that summarizes the task\n   - A difficulty level (Easy, Intermediate, Hard, or Super Hard)\n   - A brief introduction to the task's context and purpose\n   - A code block containing step-by-step instructions\n   - A list of required skills, libraries, or technologies\n\n3. Incorporate model flows by breaking down the task into logical steps and explaining the process flow.\n\n4. Include potential code fixes or common pitfalls that users might encounter while working on the task.\n\n5. Add context learning elements by providing brief explanations or resources for key concepts related to the task.\n\n6. Ensure that the refactored task is comprehensive and can stand alone as a learning exercise.\n\nPlease provide your refactored task description within &lt;refactored_task&gt; tags. Use appropriate subheadings and formatting to make the description clear and easy to read.\n\nAdditional tips:\n- Mention any prerequisites or assumed knowledge\n- Suggest potential extensions or variations of the task for further learning\n\nRemember to maintain the original intent and complexity of the task while improving its structure and clarity.\"\"\")\n            if '&lt;refactored_task&gt;' in task:\n                task = task.split('&lt;refactored_task&gt;')[1]\n            if '&lt;/refactored_task&gt;' in task:\n                task = task.split('&lt;/refactored_task&gt;')[0]\n        code_follow_up_prompt = f\"\"\"\nYou are an AI assistant responsible for evaluating task completion and providing feedback on the execution process. Your goal is to determine if a given task has been completed based on the execution result, and to offer insights for future improvements.\n\nYou will be provided with two inputs:\n&lt;task_description&gt;\n{original_task}\n{f'&lt;refactored_task_description_from_ai&gt;{task}&lt;/refactored_task_description_from_ai&gt;' if not do_continue else ''}\n&lt;/task_description&gt;\n\n&lt;code&gt;\n#CODE#\n&lt;/code&gt;\n\n&lt;execution_result&gt;\n#EXECUTION_RESULT#\n&lt;/execution_result&gt;\n\nFirst, carefully analyze the task description and the execution result. Determine whether the task has been completed successfully based on the information provided.\n\nIf the task is completed:\n1. Prepare a brief statement indicating that the task is done.\n2. Summarize the output for the user in a clear and concise manner.\n\nIf the task is not completed:\n1. Prepare a brief statement indicating that the task is not done.\n2. Identify the specific aspects of the task that remain incomplete.\n\nRegardless of task completion status, evaluate the procedure and effectiveness of the execution:\n1. Analyze the workflow: Describe the steps taken in the execution process.\n2. Assess effectiveness: Determine how well the procedure achieved the desired outcome.\n3. Identify errors: Pinpoint any mistakes or inefficiencies in the execution.\n4. Provide recommendations: Suggest improvements for future task executions.\n\ntip: Enclose mutil line strings property for python eval to function!\ntip: Set is_completed True if all requirements are completed from &lt;task_description&gt;.\ntip: Help the Agent with your analyses to finalize the &lt;task_description&gt;.\n{'tip: Prefer new informations from &lt;execution_result&gt; over &lt;refactored_task_description_from_ai&gt; based of &lt;code&gt;' if not do_continue else ''}\nnote : for the final result only toke information from the &lt;execution_result&gt;. if the relevant informations is not avalabel try string withe tips in the recommendations. else set is_completed True and return the teh Task failed!\nEnsure that your evaluation is thorough, constructive, and provides actionable insights for improving future task executions.\nAdd guidance based on the the last execution result\"\"\"\n        code_follow_up_prompt_ = [code_follow_up_prompt]\n        initial_prompt = f\"\"\"\nYou are an AI py coding agent specializing in iterative development and code refinement, designed to perform tasks that involve thinking. Your goal is to complete the given task while demonstrating a clear thought process throughout the execution.\nSYSTEM STATE:\n&lt;current_state&gt;\nIteration: #ITER#\nStatus: #STATE#\nLast EXECUTION: #EXECUTION#\n&lt;/current_state&gt;\n\nENVIRONMENT: {'current file :'+self.ipython.user_ns.get(\"__file__\")  if self.ipython.user_ns.get(\"__file__\") is not None else ''}\n\n'''&lt;global_variables&gt;\n#LOCALS#\n&lt;/global_variables&gt;'''\n\nMEMORY:\n&lt;process_memory&gt;\n#PHINT#\n&lt;/process_memory&gt;\n\n&lt;chat_memory&gt;\n#CHINT#\n&lt;/chat_memory&gt;\n\nVALIDATION CHECKLIST (Must verify before each action):\n1. \u2713 Check existing variables in ENVIRONMENT &lt;global_variables&gt;\n2. \u2713 Verify existing functions and classes\n3. \u2713 Review current imports\n4. \u2713 Confirm method signatures\n5. \u2713 Validate state preservation\n\nWORKFLOW STEPS:\n1. Analyze Current State:\n   - Reason and use all avalabel context\n   - Do not repeat the same errors\n   - Review existing implementations\n   - Check variable values\n   - Verify import statements\n   - Document dependencies\n\n2. Plan Change:\n   - NO example/simulation/simulate\n   - No demo er moc Data no Simulations Allowed or u will die!!\n   - Use existing variables and code when possible\n   - Prefer updates over rewrites\n\n3. Execute Change:\n   - Use appropriate action\n   - Maintain existing state\n   - Document modifications\n   - Verify results\n\nYou will use a structure called ThinkResult to organize your thoughts and actions.\nFor each step of your task, follow this process:\n\nACTIONS:\n1. 'code':\n    - MUST check &lt;global_variables&gt; first\n    - NEVER create demo functions\n    - Include 'reason'\n    - lang default 'py'\n    - Required: code in content\n    - code MUST call a function or display the row variabel / value at the end!\n    - Required: {{'context':{{'lang':'py',  'reason': ... }}...}}\n    - Optional file key in context example {{'context':{{'lang':'py',  'file': 'main.py' ,  'reason': ... }}...}}\n    - py code allows for toplevel await !!! use it !!! like\n:file-start:\nprint(\"using toplevel await\")\nawait abc()\n:file-end:\n\n    - Tip: use comments to reason with in the code\n3. 'infos': Request specific details\n4. 'guide': Get step clarification use on complex task and ery 5 step for staying on trak!\n5. 'brake': Pause for assessment\n6. 'done': Summarize changes\n\nCODE CONSTRAINTS:\n1. State Preservation:\n   - ALL variables ar persist\n   - ALL functions remain\n   - ALL classes ar maintained\n\n2. Import Management:\n   - Check &lt;global_variables&gt; for modules\n   - Use absolute imports\n   - Document new dependencies\n\n3. Function Handling:\n   - NEVER overwrite existing\n   - Use update for changes\n   - Preserve signatures\n\n4. Variable Scope:\n   - Maintain existing scope\n   - Check for conflicts\n   - Document state changes\n\nEXECUTION RULES:\n1. VERIFY before create\n2. UPDATE don't replace\n3. TEST after each change\n\nNext Action Required:\n1. Review current state\n2. Check existing code\n3. Execute with state preservation\n\n!!CRITICAL!!\n- NO demo functions\n- NO placeholder functions\n- USE existing code\n- FOR Implementations prefer writing large production redy code chunks.\n- FOR reasoning and validation write small code blocks.\n- THE CODE must call something or end the code with an value!\n- NO INFINIT LOOPS! none breakable while loops ar not allowed, exception ui (closed by user)\n- NO 'python' top level return, only write the variabel or value itself!\n- 'code is run using exec! do not use !pip ...'\n'- instead use auto_install(package_name, install_method=\"pip\", upgrade=False, quiet=False, version=None, extra_args=None)'\n# Example usage first time\n\u2502 auto_install('pandas', version='1.3.0')\n\u2502 import pandas\n\u2502 auto_install('pygame')\n\u2502 import pygame\n\u2502 auto_install('numpy')\n\u2502 import numpy as np\n!TIPS!\n- '&lt;global_variables&gt; can contain instances and functions you can use in your python' code\n- if the function is async you can use top level await\n- if their is missing of informations try running code to get the infos\n- if you got stuck or need assistance break with a question to the user.\n'- run functions from &lt;global_variables&gt; using name(*args, **kwargs) or await name(*args, **kwargs)'\n'- &lt;global_variables&gt; ar global accessible!'\n'- if an &lt;global_variables&gt; name is lower lists an redy to use instance'\n\"\"\"\n        p_hint, c_hint = await self.get_process_hint(task)\n        initial_prompt = initial_prompt.replace('#PHINT#', p_hint)\n        initial_prompt = initial_prompt.replace('#CHINT#', c_hint)\n        initial_prompt_ = initial_prompt\n        iter_i = 0\n        iter_p = 0\n        iter_tat = 0\n        next_infos = \"\"\n        if not do_continue:\n            await self.chat_session.add_message({'role': 'user', 'content': task})\n        else:\n            self.restore()\n            await self.chat_session.add_message({'role': 'user', 'content': task})\n\n        if self.web_js and self.browser_session is None:\n            self.browser_session = BrowserWrapper(llm=self.agent.amd.modle)\n\n        # await self.verbose_output.log_message('user', task)\n        self.verbose_output.log_header(task)\n        while state != ThinkState.DONE:\n            iter_i += 1\n            t0 = time.perf_counter()\n            prompt = initial_prompt.replace('#ITER#', f'{iter_i} max {self.max_iter}')\n            prompt = prompt.replace('#STATE#', f'{state.name}')\n            prompt = prompt.replace('#EXECUTION#', f'{next_infos}')  if next_infos else prompt.replace('Last EXECUTION: #EXECUTION#', '')\n            prompt = prompt.replace('#LOCALS#', f'{self._generate_variable_descriptions()}')\n            self.verbose_output.log_state(state.name, {})\n            self.verbose_output.formatter.print_iteration(iter_i, self.max_iter)\n            if state == ThinkState.ACTION:\n                iter_tat +=1\n                if iter_tat &gt; self.max_think_after_think:\n                    state = ThinkState.BRAKE\n            else:\n                iter_tat = 0\n\n            if state == ThinkState.ACTION:\n                # Get agent's thoughts\n                think_dicts = await self.verbose_output.process(state.name, self.agent.a_format_class(\n                    ThinkResults,\n                    prompt,\n                    message=self.chat_session.get_past_x(self.max_iter*2, last_u=not do_continue).copy()+([self.process_memory.history[-1]] if self.process_memory.history else []) ,\n                ))\n                think_dicts = think_dicts.get(\"actions\")\n                if think_dicts is None:\n                    think_dicts = [await self.verbose_output.process(state.name, self.agent.a_format_class(\n                        ThinkResult,\n                        prompt,\n                        message=self.chat_session.get_past_x(self.max_iter * 2, last_u=not do_continue).copy() + (\n                            [self.process_memory.history[-1]] if self.process_memory.history else []),\n                    ))]\n                if len(think_dicts) == 1:\n                    think_dict = think_dicts[0]\n                else:\n                    for think_dict in think_dicts[:-1]:\n                        if think_dict.get('context') is None:\n                            think_dict['context'] = {'context': 'N/A'}\n                        if not isinstance(think_dict.get('context'), dict):\n                            think_dict['context'] = {'context': think_dict.get('context')}\n                        think_result = ThinkResult(**think_dict)\n                        await self.chat_session.add_message(\n                            {'role': 'assistant', 'content': think_result.content + str(think_result.context)})\n                        state, result = await self.verbose_output.process(think_dict.get(\"action\"),\n                                                                          self._process_think_result(think_result,\n                                                                                                     task=task))\n                        if result:\n                            await self.chat_session.add_message(\n                                {'role': 'system', 'content': 'Evaluation: ' + str(result)})\n                            await self.verbose_output.log_message('system', str(result))\n                    think_dict = think_dicts[-1]\n                await self.verbose_output.log_think_result(think_dict)\n                if think_dict.get('context') is None:\n                    think_dict['context'] = {'context': 'N/A'}\n                if not isinstance(think_dict.get('context'), dict):\n                    think_dict['context'] = {'context': think_dict.get('context')}\n                think_result = ThinkResult(**think_dict)\n                state, result = await self.verbose_output.process(think_dict.get(\"action\"), self._process_think_result(think_result, task=task))\n                await self.chat_session.add_message({'role': 'assistant', 'content': think_result.content + str(think_result.context)})\n                if result:\n                    await self.chat_session.add_message({'role': 'system', 'content': 'Evaluation: '+str(result)})\n                    await self.verbose_output.log_message('system', str(result))\n                    code_follow_up_prompt_[0] = code_follow_up_prompt.replace(\"#EXECUTION_RESULT#\", str(result))\n                    if isinstance(result ,ExecutionRecord):\n                        code_follow_up_prompt_[0] = code_follow_up_prompt_[0].replace(\"#CODE#\", result.code)\n                    else:\n                        code_follow_up_prompt_[0] = code_follow_up_prompt_[0].replace(\"#CODE#\", self._generate_variable_descriptions())\n                else:\n                    code_follow_up_prompt_[0] = code_follow_up_prompt.replace(\"#EXECUTION_RESULT#\", str(think_result))\n                    code_follow_up_prompt_[0] = code_follow_up_prompt_[0].replace(\"#CODE#\",\n                                                                              self._generate_variable_descriptions())\n\n\n            elif state == ThinkState.PROCESSING:\n                # Get agent's thoughts\n                class Next(BaseModel):\n                    is_completed: bool\n                    recommendations: str\n                    errors: str\n                    effectiveness: str\n                    workflow: str\n                    text: str\n                # Format the agent's thoughts into a structured response\n                _agent = self.v_agent if self.v_agent is not None else self.agent\n                next_dict = await self.verbose_output.process(state.name, _agent.a_format_class(\n                    Next,\n                    code_follow_up_prompt_[0],\n                    message=self.chat_session.get_past_x(self.max_iter*2, last_u=not do_continue).copy(),\n                ))\n                next_infos = json.dumps(next_dict)\n                await self.verbose_output.log_process_result(next_dict)\n                await self.process_memory.add_message({'role': 'assistant', 'content': next_infos.replace('workflow:', 'past-workflow:')})\n                iter_p += 1\n                code_follow_up_prompt_[0] = code_follow_up_prompt\n                if not next_dict.get('is_completed', True):\n                    state = ThinkState.ACTION\n                    initial_prompt = initial_prompt_.replace('#ITER#',f'#ITER#\\nReasoning assist result: {next_dict}')\n                    continue\n                elif next_dict.get('is_completed', False):\n                    result = next_dict.get('text', '')\n                    state = ThinkState.DONE\n                    continue\n                else:\n                    result = next_dict.get('text', '')\n                    break\n\n            elif state == ThinkState.BRAKE:\n                break\n\n            if iter_i &lt; self.max_iter:\n                if time.perf_counter() -t0 &lt; self.timeout_timer*2.5:\n                    with Spinner(f\"Prevent rate limit posing for {self.timeout_timer}s\", symbols='+', time_in_s=self.timeout_timer, count_down=True):\n                        await asyncio.sleep(self.timeout_timer)\n            else:\n                state = ThinkState.BRAKE\n                if isinstance(result, ExecutionRecord):\n                    result = result.result\n                elif isinstance(result, str):\n                    pass\n                else:\n                    result = \"Max iterations\"\n                break\n\n        self.verbose_output.log_state(state.name, {})\n\n        return PipelineResult(\n            variables=self.variables,\n            result=result,\n            execution_history=self.execution_history,\n            message=self.chat_session.get_past_x(iter_i*2, last_u=not do_continue),\n        )\n</code></pre> <code>save_session(name)</code> \u00b6 <p>Save current session</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def save_session(self, name: str):\n    \"\"\"Save current session\"\"\"\n    self.session_name = name\n    self.ipython.save_session(name)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.SyncReport","title":"<code>SyncReport</code>  <code>dataclass</code>","text":"<p>Report of variables synced from namespace to pipeline</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>@dataclass\nclass SyncReport:\n    \"\"\"Report of variables synced from namespace to pipeline\"\"\"\n    added: dict[str, str]\n    skipped: dict[str, str]  # var_name -&gt; reason\n    errors: dict[str, str]  # var_name -&gt; error message\n\n    def __str__(self) -&gt; str:\n        parts = []\n        if self.added:\n            parts.append(\"Added variables:\")\n            for name, type_ in self.added.items():\n                parts.append(f\"  - {name}: {type_}\")\n        if self.skipped:\n            parts.append(\"\\nSkipped variables:\")\n            for name, reason in self.skipped.items():\n                parts.append(f\"  - {name}: {reason}\")\n        if self.errors:\n            parts.append(\"\\nErrors:\")\n            for name, error in self.errors.items():\n                parts.append(f\"  - {name}: {error}\")\n        return \"\\n\".join(parts)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.TeeStream","title":"<code>TeeStream</code>","text":"<p>Stream that writes to both console and buffer</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class TeeStream:\n    \"\"\"Stream that writes to both console and buffer\"\"\"\n    def __init__(self, console_stream, buffer_stream):\n        self.console_stream = console_stream\n        self.buffer_stream = buffer_stream\n\n    def write(self, data):\n        self.console_stream.write(data)\n        self.buffer_stream.write(data)\n        self.console_stream.flush()  # Ensure immediate console output\n\n    def flush(self):\n        self.console_stream.flush()\n        self.buffer_stream.flush()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.VerboseFormatter","title":"<code>VerboseFormatter</code>","text":"Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class VerboseFormatter:\n    def __init__(self,print_f, spinner_style: str = \"d\"):\n        self.style = Style()\n        self.current_spinner = None\n        self.spinner_style = spinner_style\n        self.print = print_f\n\n    def print_header(self, text: str):\n        \"\"\"Print a formatted header with separator line\"\"\"\n        width = 80\n        self.print(f\"\\n{self.style.BLUE('=' * width)}\")\n        self.print(self.style.BLUE2(f\"\u26a1 {text.center(width - 4)} \u26a1\"))\n        self.print(f\"{self.style.BLUE('=' * width)}\\n\")\n\n    def print_section(self, title: str, content: str):\n        \"\"\"Print a formatted section with title and content\"\"\"\n        self.print(f\"{self.style.YELLOW('\u250c\u2500')} {self.style.YELLOW2(title)}\")\n        for line in content.split('\\n'):\n            try:\n                self.print(f\"{self.style.YELLOW('\u2502')} {line}\")\n            except Exception as e:\n                try:\n                    pos = int(str(e).split('position ')[1].split('-')[0])\n                    line = line[:pos] + line[pos+1:]\n                    self.print(f\"{self.style.YELLOW('\u2502')} {line}\")\n                except Exception as e:\n                    self.print(f\"{self.style.RED('\u2502')} UNABLE TO PRINT {str(e)}\")\n        self.print(f\"{self.style.YELLOW('\u2514\u2500')} {self.style.GREY('End of section')}\\n\")\n\n    def print_iteration(self, current: int, maximum: int):\n        \"\"\"Print iteration progress with visual bar\"\"\"\n        progress = int((current / maximum) * 20)\n        bar = \"\u2588\" * progress + \"\u2591\" * (20 - progress)\n        self.print(f\"\\r{self.style.CYAN(f'Iteration [{bar}] {current}/{maximum}')}  \", end='')\n\n    def print_state(self, state: str, details: dict[str, Any] | None = None):\n        \"\"\"Print current state with optional details\"\"\"\n        state_color = {\n            'ACTION': self.style.GREEN2,\n            'PROCESSING': self.style.YELLOW2,\n            'BRAKE': self.style.RED2,\n            'DONE': self.style.BLUE2\n        }.get(state, self.style.WHITE2)\n        res_str = f\"\\nCurrent State: {state}\"\n        self.print(f\"\\n{self.style.Bold('Current State:')} {state_color(state)}\")\n\n        if details:\n            for key, value in details.items():\n                self.print(f\"  {self.style.GREY('\u251c\u2500')} {self.style.CYAN(key)}: {value}\")\n                res_str += f\"  \u251c\u2500 {key}: {value}\\n\"\n        return res_str\n\n    def print_method_update(self, method_update: 'MethodUpdate'):\n        \"\"\"Print a formatted view of a MethodUpdate structure\"\"\"\n        # Header with class and method name\n        self.print(f\"\\n{self.style.BLUE('\u250f\u2501')} {self.style.Bold('Method Update Details')}\")\n\n        # Class and method information\n        self.print(f\"{self.style.BLUE('\u2523\u2501')} Class: {self.style.GREEN2(method_update.class_name)}\")\n        self.print(f\"{self.style.BLUE('\u2523\u2501')} Method: {self.style.YELLOW2(method_update.method_name)}\")\n\n        # Description if available\n        if method_update.description:\n            self.print(f\"{self.style.BLUE('\u2523\u2501')} Description:\")\n            for line in method_update.description.split('\\n'):\n                self.print(f\"{self.style.BLUE('\u2503')}  {self.style.GREY(line)}\")\n\n        # Code section\n        self.print(f\"{self.style.BLUE('\u2523\u2501')} Code:\")\n        code_lines = method_update.code.split('\\n')\n        for i, line in enumerate(code_lines):\n            # Different styling for first and last lines\n            if i == 0:\n                self.print(f\"{self.style.BLUE('\u2503')}  {self.style.CYAN('\u250c\u2500')} {line}\")\n            elif i == len(code_lines) - 1:\n                self.print(f\"{self.style.BLUE('\u2503')}  {self.style.CYAN('\u2514\u2500')} {line}\")\n            else:\n                self.print(f\"{self.style.BLUE('\u2503')}  {self.style.CYAN('\u2502')} {line}\")\n\n        # Footer\n        self.print(f\"{self.style.BLUE('\u2517\u2501')} {self.style.GREY('End of method update')}\\n\")\n\n    async def process_with_spinner(self, message: str, coroutine):\n        \"\"\"Execute a coroutine with a spinner indicator\"\"\"\n        with Spinner(message, symbols=self.spinner_style):\n            result = await coroutine\n            return result\n</code></pre> <code>print_header(text)</code> \u00b6 <p>Print a formatted header with separator line</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_header(self, text: str):\n    \"\"\"Print a formatted header with separator line\"\"\"\n    width = 80\n    self.print(f\"\\n{self.style.BLUE('=' * width)}\")\n    self.print(self.style.BLUE2(f\"\u26a1 {text.center(width - 4)} \u26a1\"))\n    self.print(f\"{self.style.BLUE('=' * width)}\\n\")\n</code></pre> <code>print_iteration(current, maximum)</code> \u00b6 <p>Print iteration progress with visual bar</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_iteration(self, current: int, maximum: int):\n    \"\"\"Print iteration progress with visual bar\"\"\"\n    progress = int((current / maximum) * 20)\n    bar = \"\u2588\" * progress + \"\u2591\" * (20 - progress)\n    self.print(f\"\\r{self.style.CYAN(f'Iteration [{bar}] {current}/{maximum}')}  \", end='')\n</code></pre> <code>print_method_update(method_update)</code> \u00b6 <p>Print a formatted view of a MethodUpdate structure</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_method_update(self, method_update: 'MethodUpdate'):\n    \"\"\"Print a formatted view of a MethodUpdate structure\"\"\"\n    # Header with class and method name\n    self.print(f\"\\n{self.style.BLUE('\u250f\u2501')} {self.style.Bold('Method Update Details')}\")\n\n    # Class and method information\n    self.print(f\"{self.style.BLUE('\u2523\u2501')} Class: {self.style.GREEN2(method_update.class_name)}\")\n    self.print(f\"{self.style.BLUE('\u2523\u2501')} Method: {self.style.YELLOW2(method_update.method_name)}\")\n\n    # Description if available\n    if method_update.description:\n        self.print(f\"{self.style.BLUE('\u2523\u2501')} Description:\")\n        for line in method_update.description.split('\\n'):\n            self.print(f\"{self.style.BLUE('\u2503')}  {self.style.GREY(line)}\")\n\n    # Code section\n    self.print(f\"{self.style.BLUE('\u2523\u2501')} Code:\")\n    code_lines = method_update.code.split('\\n')\n    for i, line in enumerate(code_lines):\n        # Different styling for first and last lines\n        if i == 0:\n            self.print(f\"{self.style.BLUE('\u2503')}  {self.style.CYAN('\u250c\u2500')} {line}\")\n        elif i == len(code_lines) - 1:\n            self.print(f\"{self.style.BLUE('\u2503')}  {self.style.CYAN('\u2514\u2500')} {line}\")\n        else:\n            self.print(f\"{self.style.BLUE('\u2503')}  {self.style.CYAN('\u2502')} {line}\")\n\n    # Footer\n    self.print(f\"{self.style.BLUE('\u2517\u2501')} {self.style.GREY('End of method update')}\\n\")\n</code></pre> <code>print_section(title, content)</code> \u00b6 <p>Print a formatted section with title and content</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_section(self, title: str, content: str):\n    \"\"\"Print a formatted section with title and content\"\"\"\n    self.print(f\"{self.style.YELLOW('\u250c\u2500')} {self.style.YELLOW2(title)}\")\n    for line in content.split('\\n'):\n        try:\n            self.print(f\"{self.style.YELLOW('\u2502')} {line}\")\n        except Exception as e:\n            try:\n                pos = int(str(e).split('position ')[1].split('-')[0])\n                line = line[:pos] + line[pos+1:]\n                self.print(f\"{self.style.YELLOW('\u2502')} {line}\")\n            except Exception as e:\n                self.print(f\"{self.style.RED('\u2502')} UNABLE TO PRINT {str(e)}\")\n    self.print(f\"{self.style.YELLOW('\u2514\u2500')} {self.style.GREY('End of section')}\\n\")\n</code></pre> <code>print_state(state, details=None)</code> \u00b6 <p>Print current state with optional details</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_state(self, state: str, details: dict[str, Any] | None = None):\n    \"\"\"Print current state with optional details\"\"\"\n    state_color = {\n        'ACTION': self.style.GREEN2,\n        'PROCESSING': self.style.YELLOW2,\n        'BRAKE': self.style.RED2,\n        'DONE': self.style.BLUE2\n    }.get(state, self.style.WHITE2)\n    res_str = f\"\\nCurrent State: {state}\"\n    self.print(f\"\\n{self.style.Bold('Current State:')} {state_color(state)}\")\n\n    if details:\n        for key, value in details.items():\n            self.print(f\"  {self.style.GREY('\u251c\u2500')} {self.style.CYAN(key)}: {value}\")\n            res_str += f\"  \u251c\u2500 {key}: {value}\\n\"\n    return res_str\n</code></pre> <code>process_with_spinner(message, coroutine)</code> <code>async</code> \u00b6 <p>Execute a coroutine with a spinner indicator</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def process_with_spinner(self, message: str, coroutine):\n    \"\"\"Execute a coroutine with a spinner indicator\"\"\"\n    with Spinner(message, symbols=self.spinner_style):\n        result = await coroutine\n        return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.VirtualEnvContext","title":"<code>VirtualEnvContext</code>","text":"<p>Context manager for temporary virtual environment activation</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class VirtualEnvContext:\n    \"\"\"Context manager for temporary virtual environment activation\"\"\"\n\n    def __init__(self, venv_path: Path):\n        self.venv_path = venv_path\n        self._original_path = None\n        self._original_sys_path = None\n        self._original_prefix = None\n        self._original_virtual_env = None\n\n    def _get_venv_paths(self):\n        \"\"\"Get virtual environment paths based on platform\"\"\"\n        if sys.platform == 'win32':\n            site_packages = self.venv_path / 'Lib' / 'site-packages'\n            scripts_dir = self.venv_path / 'Scripts'\n            python_path = scripts_dir / 'python.exe'\n        else:\n            python_version = f'python{sys.version_info.major}.{sys.version_info.minor}'\n            site_packages = self.venv_path / 'lib' / python_version / 'site-packages'\n            scripts_dir = self.venv_path / 'bin'\n            python_path = scripts_dir / 'python'\n\n        return site_packages, scripts_dir, python_path\n\n    def __enter__(self):\n        # Save original state\n        self._original_path = os.environ.get('PATH', '')\n        self._original_sys_path = sys.path.copy()\n        self._original_prefix = sys.prefix\n        self._original_virtual_env = os.environ.get('VIRTUAL_ENV')\n\n        # Get venv paths\n        site_packages, scripts_dir, python_path = self._get_venv_paths()\n\n        # Modify environment for venv\n        if scripts_dir.exists():\n            new_path = os.pathsep.join([str(scripts_dir), self._original_path])\n            os.environ['PATH'] = new_path\n\n        if site_packages.exists():\n            sys.path.insert(0, str(site_packages))\n\n        os.environ['VIRTUAL_ENV'] = str(self.venv_path)\n\n        # Return the python executable path for potential subprocess calls\n        return str(python_path)\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Restore original state\n        os.environ['PATH'] = self._original_path\n        sys.path = self._original_sys_path\n\n        if self._original_virtual_env is None:\n            os.environ.pop('VIRTUAL_ENV', None)\n        else:\n            os.environ['VIRTUAL_ENV'] = self._original_virtual_env\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.VirtualFileSystem","title":"<code>VirtualFileSystem</code>","text":"Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class VirtualFileSystem:\n    def __init__(self, base_dir: Path):\n        self.base_dir = base_dir\n        self.current_dir = base_dir\n        self.virtual_files: dict[str, str] = {}\n        self.base_dir.mkdir(parents=True, exist_ok=True)\n\n    def write_file(self, filepath: str | Path, content: str) -&gt; Path:\n        \"\"\"Write content to a virtual file and persist to disk using UTF-8\"\"\"\n        try:\n            abs_path = self._resolve_path(filepath)\n        except ValueError:\n            print(\"invalid :\", filepath)\n            filepath = \"src/temp_js/_temp_fix.py\"\n            abs_path = self._resolve_path(filepath)\n        abs_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Store in virtual filesystem\n        rel_path = str(abs_path.relative_to(self.base_dir))\n        self.virtual_files[rel_path] = content\n\n        # Write to actual filesystem with UTF-8 encoding\n        with open(abs_path, 'w', encoding='utf-8', errors='replace') as f:\n            f.write(content)\n\n        return abs_path\n\n    def read_file(self, filepath: str | Path) -&gt; str:\n        \"\"\"Read content from a virtual file using UTF-8\"\"\"\n        abs_path = self._resolve_path(filepath)\n        if not abs_path.exists():\n            raise FileNotFoundError(f\"File not found: {filepath}\")\n\n        rel_path = str(abs_path.relative_to(self.base_dir))\n\n        # Check virtual filesystem first\n        if rel_path in self.virtual_files:\n            return self.virtual_files[rel_path]\n\n        # Fall back to reading from disk with UTF-8 encoding\n        with open(abs_path, encoding='utf-8', errors='replace') as f:\n            content = f.read()\n            self.virtual_files[rel_path] = content\n            return content\n\n    def delete_file(self, filepath: str | Path):\n        \"\"\"Delete a virtual file\"\"\"\n        abs_path = self._resolve_path(filepath)\n        rel_path = str(abs_path.relative_to(self.base_dir))\n\n        if rel_path in self.virtual_files:\n            del self.virtual_files[rel_path]\n\n        if abs_path.exists():\n            abs_path.unlink()\n\n    def create_directory(self, dirpath: str | Path):\n        \"\"\"Create a new directory\"\"\"\n        abs_path = self._resolve_path(dirpath)\n        abs_path.mkdir(parents=True, exist_ok=True)\n        return abs_path\n\n\n    def list_directory(self, dirpath: str | Path = '.') -&gt; list:\n        \"\"\"List contents of a directory\"\"\"\n        abs_path = self._resolve_path(dirpath)\n        if not abs_path.exists():\n            raise FileNotFoundError(f\"Directory not found: {dirpath}\")\n        return [p.name for p in abs_path.iterdir()]\n\n    def change_directory(self, dirpath: str | Path):\n        \"\"\"Change current working directory\"\"\"\n        new_dir = self._resolve_path(dirpath)\n        if not new_dir.exists() or not new_dir.is_dir():\n            raise NotADirectoryError(f\"Directory not found: {dirpath}\")\n        self.current_dir = new_dir\n\n    def _resolve_path(self, filepath: str | Path) -&gt; Path:\n        \"\"\"Convert relative path to absolute path\"\"\"\n        filepath = Path(filepath)\n        if filepath.is_absolute():\n            if not str(filepath).startswith(str(self.base_dir)):\n                raise ValueError(\"Path must be within base directory\")\n            return filepath\n        return (self.current_dir / filepath).resolve()\n\n    def save_state(self, state_file: Path):\n        \"\"\"Save virtual filesystem state to disk\"\"\"\n        state = {\n            'current_dir': str(self.current_dir.relative_to(self.base_dir)),\n            'virtual_files': self.virtual_files\n        }\n        with open(state_file, 'w') as f:\n            json.dump(state, f)\n\n    def load_state(self, state_file: Path):\n        \"\"\"Load virtual filesystem state from disk\"\"\"\n        if not state_file.exists():\n            return\n\n        with open(state_file) as f:\n            state = json.load(f)\n            self.current_dir = self.base_dir / state['current_dir']\n            self.virtual_files = state['virtual_files']\n\n    def print_file_structure(self, start_path: str | Path = '.', indent: str = ''):\n        \"\"\"Print the file structure starting from the given path\"\"\"\n        start_path = self._resolve_path(start_path)\n        if not start_path.exists():\n            s = f\"Path not found: {start_path}\"\n            return s\n\n        s = f\"{indent}{start_path.name}/\"\n        for item in sorted(start_path.iterdir()):\n            if item.is_dir():\n               s+= self.print_file_structure(item, indent + '  ')\n            else:\n                s = f\"{indent}  {item.name}\"\n        return s\n</code></pre> <code>change_directory(dirpath)</code> \u00b6 <p>Change current working directory</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def change_directory(self, dirpath: str | Path):\n    \"\"\"Change current working directory\"\"\"\n    new_dir = self._resolve_path(dirpath)\n    if not new_dir.exists() or not new_dir.is_dir():\n        raise NotADirectoryError(f\"Directory not found: {dirpath}\")\n    self.current_dir = new_dir\n</code></pre> <code>create_directory(dirpath)</code> \u00b6 <p>Create a new directory</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def create_directory(self, dirpath: str | Path):\n    \"\"\"Create a new directory\"\"\"\n    abs_path = self._resolve_path(dirpath)\n    abs_path.mkdir(parents=True, exist_ok=True)\n    return abs_path\n</code></pre> <code>delete_file(filepath)</code> \u00b6 <p>Delete a virtual file</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def delete_file(self, filepath: str | Path):\n    \"\"\"Delete a virtual file\"\"\"\n    abs_path = self._resolve_path(filepath)\n    rel_path = str(abs_path.relative_to(self.base_dir))\n\n    if rel_path in self.virtual_files:\n        del self.virtual_files[rel_path]\n\n    if abs_path.exists():\n        abs_path.unlink()\n</code></pre> <code>list_directory(dirpath='.')</code> \u00b6 <p>List contents of a directory</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def list_directory(self, dirpath: str | Path = '.') -&gt; list:\n    \"\"\"List contents of a directory\"\"\"\n    abs_path = self._resolve_path(dirpath)\n    if not abs_path.exists():\n        raise FileNotFoundError(f\"Directory not found: {dirpath}\")\n    return [p.name for p in abs_path.iterdir()]\n</code></pre> <code>load_state(state_file)</code> \u00b6 <p>Load virtual filesystem state from disk</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def load_state(self, state_file: Path):\n    \"\"\"Load virtual filesystem state from disk\"\"\"\n    if not state_file.exists():\n        return\n\n    with open(state_file) as f:\n        state = json.load(f)\n        self.current_dir = self.base_dir / state['current_dir']\n        self.virtual_files = state['virtual_files']\n</code></pre> <code>print_file_structure(start_path='.', indent='')</code> \u00b6 <p>Print the file structure starting from the given path</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_file_structure(self, start_path: str | Path = '.', indent: str = ''):\n    \"\"\"Print the file structure starting from the given path\"\"\"\n    start_path = self._resolve_path(start_path)\n    if not start_path.exists():\n        s = f\"Path not found: {start_path}\"\n        return s\n\n    s = f\"{indent}{start_path.name}/\"\n    for item in sorted(start_path.iterdir()):\n        if item.is_dir():\n           s+= self.print_file_structure(item, indent + '  ')\n        else:\n            s = f\"{indent}  {item.name}\"\n    return s\n</code></pre> <code>read_file(filepath)</code> \u00b6 <p>Read content from a virtual file using UTF-8</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def read_file(self, filepath: str | Path) -&gt; str:\n    \"\"\"Read content from a virtual file using UTF-8\"\"\"\n    abs_path = self._resolve_path(filepath)\n    if not abs_path.exists():\n        raise FileNotFoundError(f\"File not found: {filepath}\")\n\n    rel_path = str(abs_path.relative_to(self.base_dir))\n\n    # Check virtual filesystem first\n    if rel_path in self.virtual_files:\n        return self.virtual_files[rel_path]\n\n    # Fall back to reading from disk with UTF-8 encoding\n    with open(abs_path, encoding='utf-8', errors='replace') as f:\n        content = f.read()\n        self.virtual_files[rel_path] = content\n        return content\n</code></pre> <code>save_state(state_file)</code> \u00b6 <p>Save virtual filesystem state to disk</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def save_state(self, state_file: Path):\n    \"\"\"Save virtual filesystem state to disk\"\"\"\n    state = {\n        'current_dir': str(self.current_dir.relative_to(self.base_dir)),\n        'virtual_files': self.virtual_files\n    }\n    with open(state_file, 'w') as f:\n        json.dump(state, f)\n</code></pre> <code>write_file(filepath, content)</code> \u00b6 <p>Write content to a virtual file and persist to disk using UTF-8</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def write_file(self, filepath: str | Path, content: str) -&gt; Path:\n    \"\"\"Write content to a virtual file and persist to disk using UTF-8\"\"\"\n    try:\n        abs_path = self._resolve_path(filepath)\n    except ValueError:\n        print(\"invalid :\", filepath)\n        filepath = \"src/temp_js/_temp_fix.py\"\n        abs_path = self._resolve_path(filepath)\n    abs_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Store in virtual filesystem\n    rel_path = str(abs_path.relative_to(self.base_dir))\n    self.virtual_files[rel_path] = content\n\n    # Write to actual filesystem with UTF-8 encoding\n    with open(abs_path, 'w', encoding='utf-8', errors='replace') as f:\n        f.write(content)\n\n    return abs_path\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.WebContentParser","title":"<code>WebContentParser</code>","text":"<p>Parser for extracting content from web pages in various formats.</p> <p>Provides methods to extract content as markdown, plain text, structured data, and take screenshots with scrolling support.</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class WebContentParser:\n    \"\"\"\n    Parser for extracting content from web pages in various formats.\n\n    Provides methods to extract content as markdown, plain text,\n    structured data, and take screenshots with scrolling support.\n    \"\"\"\n\n    def __init__(self, browser_wrapper):\n        \"\"\"Initialize the parser with a browser wrapper instance\"\"\"\n        self.browser = browser_wrapper\n\n    async def to_markdown(self, page=None, selector=\"main, article, #content, .content, body\",\n                          include_images=True):\n        \"\"\"\n        Convert webpage content to markdown format\n\n        Args:\n            page: The page to parse (uses current page if None)\n            selector: CSS selector for the content to extract\n            include_images: Whether to include image references\n\n        Returns:\n            str: Markdown content\n        \"\"\"\n        return await self.browser.extract_markdown(page, selector, include_images)\n\n    async def to_text(self, page=None, selector=\"body\"):\n        \"\"\"Extract plain text from webpage\"\"\"\n        return await self.browser.extract_text(page, selector)\n\n    async def to_structured(self, page=None, config=None):\n        \"\"\"Extract structured data from webpage using selector configuration\"\"\"\n        return await self.browser.extract_structured_content(page, config)\n\n    async def to_screenshot(self, page=None, full_page=True, path=None,\n                            initial_delay=1000, scroll_delay=500, format='png'):\n        \"\"\"\n        Take a screenshot with scrolling functionality\n\n        Args:\n            page: The page to screenshot\n            full_page: Whether to capture the full page\n            path: Path to save the screenshot\n            initial_delay: Delay in ms before starting screenshot\n            scroll_delay: Delay in ms between scrolls\n            format: Image format ('png' or 'jpeg')\n        \"\"\"\n        return await self.browser.take_scrolling_screenshot(\n            page, full_page, path, initial_delay, scroll_delay, format\n        )\n\n    async def extract_all(self, page=None, selector=\"body\", include_images=True,\n                          screenshot=True, screenshot_path=None):\n        \"\"\"Extract all content types (markdown, text, structured data, screenshot)\"\"\"\n        result = {\n            'markdown': await self.to_markdown(page, selector, include_images),\n            'text': await self.to_text(page, selector),\n            'structured': await self.to_structured(page)\n        }\n\n        if screenshot:\n            result['screenshot'] = await self.to_screenshot(\n                page, path=screenshot_path, initial_delay=1000\n            )\n\n        return result\n</code></pre> <code>__init__(browser_wrapper)</code> \u00b6 <p>Initialize the parser with a browser wrapper instance</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __init__(self, browser_wrapper):\n    \"\"\"Initialize the parser with a browser wrapper instance\"\"\"\n    self.browser = browser_wrapper\n</code></pre> <code>extract_all(page=None, selector='body', include_images=True, screenshot=True, screenshot_path=None)</code> <code>async</code> \u00b6 <p>Extract all content types (markdown, text, structured data, screenshot)</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def extract_all(self, page=None, selector=\"body\", include_images=True,\n                      screenshot=True, screenshot_path=None):\n    \"\"\"Extract all content types (markdown, text, structured data, screenshot)\"\"\"\n    result = {\n        'markdown': await self.to_markdown(page, selector, include_images),\n        'text': await self.to_text(page, selector),\n        'structured': await self.to_structured(page)\n    }\n\n    if screenshot:\n        result['screenshot'] = await self.to_screenshot(\n            page, path=screenshot_path, initial_delay=1000\n        )\n\n    return result\n</code></pre> <code>to_markdown(page=None, selector='main, article, #content, .content, body', include_images=True)</code> <code>async</code> \u00b6 <p>Convert webpage content to markdown format</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <p>The page to parse (uses current page if None)</p> <code>None</code> <code>selector</code> <p>CSS selector for the content to extract</p> <code>'main, article, #content, .content, body'</code> <code>include_images</code> <p>Whether to include image references</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <p>Markdown content</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def to_markdown(self, page=None, selector=\"main, article, #content, .content, body\",\n                      include_images=True):\n    \"\"\"\n    Convert webpage content to markdown format\n\n    Args:\n        page: The page to parse (uses current page if None)\n        selector: CSS selector for the content to extract\n        include_images: Whether to include image references\n\n    Returns:\n        str: Markdown content\n    \"\"\"\n    return await self.browser.extract_markdown(page, selector, include_images)\n</code></pre> <code>to_screenshot(page=None, full_page=True, path=None, initial_delay=1000, scroll_delay=500, format='png')</code> <code>async</code> \u00b6 <p>Take a screenshot with scrolling functionality</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <p>The page to screenshot</p> <code>None</code> <code>full_page</code> <p>Whether to capture the full page</p> <code>True</code> <code>path</code> <p>Path to save the screenshot</p> <code>None</code> <code>initial_delay</code> <p>Delay in ms before starting screenshot</p> <code>1000</code> <code>scroll_delay</code> <p>Delay in ms between scrolls</p> <code>500</code> <code>format</code> <p>Image format ('png' or 'jpeg')</p> <code>'png'</code> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def to_screenshot(self, page=None, full_page=True, path=None,\n                        initial_delay=1000, scroll_delay=500, format='png'):\n    \"\"\"\n    Take a screenshot with scrolling functionality\n\n    Args:\n        page: The page to screenshot\n        full_page: Whether to capture the full page\n        path: Path to save the screenshot\n        initial_delay: Delay in ms before starting screenshot\n        scroll_delay: Delay in ms between scrolls\n        format: Image format ('png' or 'jpeg')\n    \"\"\"\n    return await self.browser.take_scrolling_screenshot(\n        page, full_page, path, initial_delay, scroll_delay, format\n    )\n</code></pre> <code>to_structured(page=None, config=None)</code> <code>async</code> \u00b6 <p>Extract structured data from webpage using selector configuration</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def to_structured(self, page=None, config=None):\n    \"\"\"Extract structured data from webpage using selector configuration\"\"\"\n    return await self.browser.extract_structured_content(page, config)\n</code></pre> <code>to_text(page=None, selector='body')</code> <code>async</code> \u00b6 <p>Extract plain text from webpage</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def to_text(self, page=None, selector=\"body\"):\n    \"\"\"Extract plain text from webpage\"\"\"\n    return await self.browser.extract_text(page, selector)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.auto_install","title":"<code>auto_install(package_name, install_method='pip', upgrade=False, quiet=False, version=None, extra_args=None)</code>","text":"<p>Enhanced auto-save import with version and extra arguments support</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def auto_install(package_name, install_method='pip', upgrade=False, quiet=False, version=None, extra_args=None):\n    '''\n    Enhanced auto-save import with version and extra arguments support\n    '''\n    try:\n        # Attempt to import the package\n        return importlib.import_module(package_name)\n    except ImportError:\n        # Package not found, prepare for installation\n        print(f\"Package '{package_name}' not found. Attempting to install...\")\n        try:\n            # Determine Python executable based on virtual environment\n            venv_path = os.environ.get('VIRTUAL_ENV')\n            if venv_path:\n                venv_path = Path(venv_path)\n                if sys.platform == 'win32':\n                    python_exec = str(venv_path / 'Scripts' / 'python.exe')\n                else:\n                    python_exec = str(venv_path / 'bin' / 'python')\n                # Check if the Python executable exists\n                if not Path(python_exec).exists():\n                    python_exec = sys.executable\n            else:\n                python_exec = sys.executable\n\n            # Construct installation command with more flexibility\n            install_cmd = [python_exec, \"-m\", install_method, \"install\"]\n            if upgrade:\n                install_cmd.append(\"--upgrade\")\n            # Support specific version installation\n            if version:\n                install_cmd.append(f\"{package_name}=={version}\")\n            else:\n                install_cmd.append(package_name)\n            # Add extra arguments if provided\n            if extra_args:\n                install_cmd.extend(extra_args)\n            # Run installation with appropriate verbosity\n            installation_output = subprocess.run(\n                install_cmd,\n                capture_output=quiet,\n                text=True\n            )\n            # Check installation status\n            if installation_output.returncode == 0:\n                print(f\"Successfully installed {package_name}\")\n                return importlib.import_module(package_name)\n            else:\n                raise Exception(f\"Installation failed: {installation_output.stderr}\")\n        except Exception as install_error:\n            print(f\"Error installing {package_name}: {install_error}\")\n            return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.sync_globals_to_vars","title":"<code>sync_globals_to_vars(pipeline, namespace=None, prefix=None, include_types=None, exclude_patterns=None, exclude_private=True, deep_copy=False, only_serializable=False)</code>","text":"<pre><code>Sync global variables or a specific namespace to pipeline variables.\n\nArgs:\n    pipeline: Pipeline instance to sync variables to\n    namespace: Optional dictionary of variables (defaults to globals())\n    prefix: Optional prefix for variable names (e.g., 'global_')\n    include_types: Only include variables of these types\n    exclude_patterns: List of regex patterns to exclude\n    exclude_private: Exclude variables starting with underscore\n    deep_copy: Create deep copies of variables instead of references\n    only_serializable: Only include variables that can be serialized\n\nReturns:\n    SyncReport with details about added, skipped and error variables\n\nUsage example:\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.sync_globals_to_vars--basic-usage-sync-all-globals","title":"Basic usage - sync all globals","text":"<p>report = sync_globals_to_vars(pipeline)</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.sync_globals_to_vars--sync-only-numeric-types-with-prefix","title":"Sync only numeric types with prefix","text":"<p>report = sync_globals_to_vars(     pipeline,     include_types=[int, float],     prefix=\"global_\" )</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.sync_globals_to_vars--sync-from-specific-namespace","title":"Sync from specific namespace","text":"<p>import numpy as np namespace = {\"arr\": np.array([1,2,3])} report = sync_globals_to_vars(pipeline, namespace=namespace)</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.sync_globals_to_vars--sync-with-deep-copy-and-serialization-check","title":"Sync with deep copy and serialization check","text":"<p>report = sync_globals_to_vars(     pipeline,     deep_copy=True,     only_serializable=True )</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def sync_globals_to_vars(\n    pipeline: Any,\n    namespace: dict[str, Any] | None = None,\n    prefix: str | None = None,\n    include_types: type | list[type] | None = None,\n    exclude_patterns: list[str] | None = None,\n    exclude_private: bool = True,\n    deep_copy: bool = False,\n    only_serializable: bool = False\n) -&gt; SyncReport:\n    \"\"\"\n    Sync global variables or a specific namespace to pipeline variables.\n\n    Args:\n        pipeline: Pipeline instance to sync variables to\n        namespace: Optional dictionary of variables (defaults to globals())\n        prefix: Optional prefix for variable names (e.g., 'global_')\n        include_types: Only include variables of these types\n        exclude_patterns: List of regex patterns to exclude\n        exclude_private: Exclude variables starting with underscore\n        deep_copy: Create deep copies of variables instead of references\n        only_serializable: Only include variables that can be serialized\n\n    Returns:\n        SyncReport with details about added, skipped and error variables\n\n    Usage example:\n# Basic usage - sync all globals\nreport = sync_globals_to_vars(pipeline)\n\n# Sync only numeric types with prefix\nreport = sync_globals_to_vars(\n    pipeline,\n    include_types=[int, float],\n    prefix=\"global_\"\n)\n\n# Sync from specific namespace\nimport numpy as np\nnamespace = {\"arr\": np.array([1,2,3])}\nreport = sync_globals_to_vars(pipeline, namespace=namespace)\n\n# Sync with deep copy and serialization check\nreport = sync_globals_to_vars(\n    pipeline,\n    deep_copy=True,\n    only_serializable=True\n)\n    \"\"\"\n    # Initialize report\n    report = SyncReport(\n        added={},\n        skipped={},\n        errors={}\n    )\n\n    # Get namespace\n    if namespace is None:\n        # Get caller's globals\n        namespace = currentframe().f_back.f_globals\n\n    # Compile exclude patterns\n    if exclude_patterns:\n        patterns = [re.compile(pattern) for pattern in exclude_patterns]\n    else:\n        patterns = []\n\n    # Normalize include_types\n    if include_types and not isinstance(include_types, list | tuple | set):\n        include_types = [include_types]\n    def get_type_info(var: Any) -&gt; str:\n        \"\"\"Helper to get detailed type information\"\"\"\n        if isinstance(var, type):\n            return f\"class '{var.__name__}'\"\n        elif isinstance(var, BaseModel):\n            return f\"Pydantic model '{var.__class__.__name__}'\"\n        elif hasattr(var, '__class__'):\n            type_name = var.__class__.__name__\n            module_name = var.__class__.__module__\n            if module_name != 'builtins':\n                return f\"{module_name}.{type_name}\"\n            return type_name\n        return type(var).__name__\n    # Process each variable\n    for name, value in namespace.items():\n        try:\n            # Skip if matches exclude criteria\n            if exclude_private and name.startswith('_'):\n                report.skipped[name] = \"private variable\"\n                continue\n\n            if any(pattern.match(name) for pattern in patterns):\n                report.skipped[name] = \"matched exclude pattern\"\n                continue\n\n            if include_types and not isinstance(value, tuple(include_types)):\n                report.skipped[name] = f\"type {type(value).__name__} not in include_types\"\n                continue\n\n            # Test serialization if required\n            if only_serializable:\n                try:\n                    import pickle\n                    pickle.dumps(value)\n                except Exception as e:\n                    report.skipped[name] = f\"not serializable: {str(e)}\"\n                    continue\n\n            # Prepare variable\n            var_value = deepcopy(value) if deep_copy else value\n            var_name = f\"{prefix}{name}\" if prefix else name\n\n            # Add to pipeline variables\n            pipeline.variables[var_name] = var_value\n            report.added[var_name] = get_type_info(value)\n\n        except Exception as e:\n            report.errors[name] = str(e)\n\n    return report\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.parser","title":"<code>parser</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.parser.CodeProcessor","title":"<code>CodeProcessor</code>","text":"Source code in <code>toolboxv2/mods/isaa/CodingAgent/parser.py</code> <pre><code>class CodeProcessor:\n    def __init__(self, code_base='./'):\n        self.language_patterns = [\n            r'```([\\w-]+)\\n((?:#|//|&lt;!--)\\s*(\\S+))?\\n([\\s\\S]*?)```',  # Standard pattern\n            r'```([\\w-]+)\\s*\\n([\\s\\S]*?)```'  # Pattern without filename comment\n        ]\n        self.code_base = code_base\n\n    def extract_code(self, text):\n        code_blocks = {}\n        seen = set()\n        for pattern in self.language_patterns:\n            matches = re.finditer(pattern, text, re.DOTALL)\n            for match in matches:\n\n                print(match.groups())\n\n                if len(match.groups()) &lt; 3:\n                    continue\n\n                code = match.groups()[3]\n                filename = match.groups()[2]\n\n                if code == code_blocks.get(filename):\n                    continue\n\n                if code_blocks.get(filename) is not None and code != code_blocks.get(filename):\n                    comment_prfix = match.groups()[1].replace(filename, '')\n                    filename = code.split('\\n')[0].replace(comment_prfix, '')\n                    code = code.replace(comment_prfix + filename + '\\n', '')\n\n                    print(\"new code\", code)\n\n                seen.add(filename)\n\n                code_blocks[filename] = code\n        return code_blocks\n\n    def write_code(self, code_dict):\n        for filename, code in code_dict.items():\n            filepath = os.path.join(self.code_base, filename)\n            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n            print(\"Writing\", filepath)\n            with open(filepath, \"w\") as f:\n                f.write(code)\n\n    def extract_and_write_code(self, text):\n        code_blocks = self.extract_code(text)\n        files = []\n        for filename, new_code in code_blocks.items():\n            filepath = os.path.join(self.code_base, filename)\n            files.append(filepath)\n            if os.path.exists(filepath):\n                self.update_existing_file(filepath, new_code)\n            else:\n                self.write_code({filename: new_code})\n        return files\n\n    def update_existing_file(self, filepath, new_code):\n        \"\"\"\n            Update an existing Python file with new code while preserving existing implementations.\n\n            Args:\n                filepath (str): Path to the file to be updated\n                new_code (str): New code to merge with existing code\n            \"\"\"\n        try:\n            # Read existing code\n            with open(filepath) as f:\n                existing_code = f.read()\n\n            # Parse existing and new code\n            existing_ast_tree = ast.parse(existing_code)\n            new_ast_tree = ast.parse(new_code)\n\n            # Create updater and transform the AST\n            updater = CodeUpdater(existing_ast_tree)\n            updated_ast = updater.visit(new_ast_tree)\n\n            # Convert AST back to source code\n            updated_code = astor.to_source(updated_ast)\n\n            # Write updated code back to file\n            with open(filepath, 'w') as f:\n                f.write(updated_code)\n\n            print(f\"Successfully updated {filepath}\")\n            return True\n\n        except Exception as e:\n            print(f\"Error updating {filepath}: {e}\")\n            return False\n</code></pre> <code>update_existing_file(filepath, new_code)</code> \u00b6 <p>Update an existing Python file with new code while preserving existing implementations.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the file to be updated</p> required <code>new_code</code> <code>str</code> <p>New code to merge with existing code</p> required Source code in <code>toolboxv2/mods/isaa/CodingAgent/parser.py</code> <pre><code>def update_existing_file(self, filepath, new_code):\n    \"\"\"\n        Update an existing Python file with new code while preserving existing implementations.\n\n        Args:\n            filepath (str): Path to the file to be updated\n            new_code (str): New code to merge with existing code\n        \"\"\"\n    try:\n        # Read existing code\n        with open(filepath) as f:\n            existing_code = f.read()\n\n        # Parse existing and new code\n        existing_ast_tree = ast.parse(existing_code)\n        new_ast_tree = ast.parse(new_code)\n\n        # Create updater and transform the AST\n        updater = CodeUpdater(existing_ast_tree)\n        updated_ast = updater.visit(new_ast_tree)\n\n        # Convert AST back to source code\n        updated_code = astor.to_source(updated_ast)\n\n        # Write updated code back to file\n        with open(filepath, 'w') as f:\n            f.write(updated_code)\n\n        print(f\"Successfully updated {filepath}\")\n        return True\n\n    except Exception as e:\n        print(f\"Error updating {filepath}: {e}\")\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.runner","title":"<code>runner</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.runner.analyze_usage_from_markdown","title":"<code>analyze_usage_from_markdown(documentation, isaa, space_name)</code>","text":"<p>Analyzes Markdown documentation to extract information about how to use the described codebase, enabling a bot to implement and understand concrete aspects of the documentation and codebase.</p> <p>Parameters: - documentation (str): The Markdown documentation to be analyzed. - isaa (Tools): A toolset for performing operations like context memory management and task completions. - space_name (str): The name of the space where the analysis results will be stored.</p> <p>Returns: - None</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/runner.py</code> <pre><code>def analyze_usage_from_markdown(documentation: str, isaa: Tools, space_name: str) -&gt; str:\n    \"\"\"\n    Analyzes Markdown documentation to extract information about how to use the described codebase,\n    enabling a bot to implement and understand concrete aspects of the documentation and codebase.\n\n    Parameters:\n    - documentation (str): The Markdown documentation to be analyzed.\n    - isaa (Tools): A toolset for performing operations like context memory management and task completions.\n    - space_name (str): The name of the space where the analysis results will be stored.\n\n    Returns:\n    - None\n    \"\"\"\n    mem = isaa.get_memory()\n    mem_name = space_name + 'Docs'\n    # Split the documentation into sections for detailed analysis\n    documentation_sections = mem.split_text(mem_name, documentation, separators='md')\n\n    # Prepare to store the results of the documentation analysis\n    usage_information_results = []\n\n    # Initialize a progress bar for tracking the documentation analysis progress\n    with tqdm(total=len(documentation_sections), unit='sections', desc='Extracting Usage Information') as progress_bar:\n        for section in documentation_sections:\n            # Define the task for extracting usage information\n            analysis_task = (\n                \"Your task is to analyze the given documentation section to extract detailed information on:\\n\"\n                \"- How to set up the environment or dependencies,\\n\"\n                \"- Step-by-step usage instructions,\\n\"\n                \"- Code examples and their explanations,\\n\"\n                \"- API descriptions and how to interact with them,\\n\"\n                \"- Any configuration or customization options.\\n\\n\"\n                \"Summarize the extracted information in a structured format that a bot can later use to \"\n                \"understand and implement aspects of the codebase.\\n\\n\"\n                f\"Documentation Section:\\n{section}\\n\"\n            )\n\n            # Perform the analysis using the provided tools\n            analysis_result = isaa.mini_task_completion_format(\n                mini_task=analysis_task,\n                format_=\"Expected format markdown format\"\n            )\n\n            # Update the progress bar\n            progress_bar.update()\n\n            # Append the analysis result to the list of results\n            usage_information_results.append(analysis_result)\n\n    # Store the analysis results in the context memory\n    mem.add_data(mem_name, usage_information_results)\n    return mem_name\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.runner.code_writer_agent_loop","title":"<code>code_writer_agent_loop(isaa, task, memspaces, max_iterations=6, v_code_base='')</code>","text":"<p>needed functions mini agent ide, interactive umgebung</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/runner.py</code> <pre><code>def code_writer_agent_loop(isaa: Tools, task: str, memspaces: list[str], max_iterations: int = 6, v_code_base=r\"\"):\n    \"\"\"\n    needed functions mini agent ide, interactive umgebung\n\n    \"\"\"\n\n    don = False\n    iteration = 0\n    functions = []\n\n    coder_agent_builder = isaa.get_default_agent_builder(\"code\")\n    coder_agent_builder.set_verbose(True).set_functions(functions=functions)  # .set_tasklist(tasks).set_task_index(0)\n    coder_agent = isaa.register_agent(coder_agent_builder)\n\n    while iteration &lt; max_iterations and not don:\n        isaa.run_agent(coder_agent, task, max_iterations, running_mode=\"oncex\", persist=True)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.runner.parse_code","title":"<code>parse_code(source, isaa, space_name)</code>","text":"<p>start analyse, write docs and save to vector store mit meat daten analysed-code</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/runner.py</code> <pre><code>def parse_code(source, isaa: Tools, space_name) -&gt; str:\n    \"\"\" start analyse, write docs and save to vector store mit meat daten analysed-code \"\"\"\n\n    mem = isaa.get_memory()\n    mem_name = space_name + 'Code'\n    docs = mem.split_text(mem_name, source, separators='py')\n    results = []\n    with tqdm(total=len(docs), unit='steps', desc='parsing input') as pbar:\n        for code_prat in docs:\n            result = isaa.mini_task_completion_format(\"Your Task is to analyse the given code in terms of:\"\n                                                      \"\\n- Exportable and reusable components,\"\n                                                      \"\\n- Internal structure,\"\n                                                      \"\\n- External sources.\"\n                                                      \"Provide your findings in an structured markdown format,\"\n                                                      \" highlighting \"\n                                                      \"key observations and any recommendations for\"\n                                                      \"improvement.\"\n                                                      f\" source-code : {code_prat}\"\n                                                      ,\n                                                      \"Expected format markdown format\")\n            pbar.update()\n            results.append(result)\n\n    mem.add_data(mem_name, results)\n    return mem_name\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.runner.runner","title":"<code>runner(request, existing_code_base=None, local_sources=None, remote_sources=None, save_changes=False, crate_new_files=False, working_directory=None)</code>","text":"<pre><code>Fuction to use isaa and agents to generate production redy code\n\nAbaluf paln:\n    - aufberitung der input parameter\n        - parsing von request\n            - herausfinden von ben\u00f6tigten informationen -&gt; list [docs:bool, examples:bool]\n        - parsing von existing_code_base\n            - wenn None -&gt; do nothing\n            - wenn code,docs -&gt; start analyse, write docs and save to vector store mit meat daten analysed-code or analysed-docs\n            - wenn path -&gt;\n                - finde docs folder + code and save to vector store mit meat daten row-docs or row-code\n                - wenn kein docs folder gefunden wurde save code and to vector store mit meat daten row-code\n            - wenn url download to local_sources.path and -&gt;\n                - gist = start analyse, write docs and save to vector store mit meat daten analysed-code\n                - git = *- wenn path\n                - jupiter-notebook = *- gist\n        - parsing von local_sources -&gt;\n            - wenn code = *- wenn path -&gt;\n            - wenn text = use text_parser *- wenn code,docs\n            - wenn md = use md_parser *- wenn code,docs\n            - wenn pdf = use pdf_parser *- wenn code,docs\n        - parsing von remote_sources download to local_sources.path and *- parsing von local_sources -&gt;\n    - entgegenehemn der input im format :\n        request:dict = {\"request\":str, \"docs\":bool, \"examples\":bool}\n        sources:list = str&lt;VectorStorName&gt;,  str&lt;VectorStorName&gt;\n    - divide | request in subsequenzen aufbereiten\n        - use isaa and llmMode divideMode to dived request[\"request\"] into a json dict\n        - pars json dict wit anything_from_str_to_dict(data: str, expected_keys: dict = None)\n    - serialise | request dict into\n        - dict format keys form 0 to n\n        - items from keys is a list of task that can be execute in parallel.\n        - a item is just a string withs representatives a part of the divided request.\n    - entgegenehemn der input im format :\n        request_dict:dict = {\"0\":[\"change thing one\", \"refactor the man page\"], 1\":[\"change thing tow\"]\"...}\n    - information collection phase\n        - if request[\"docs\"] or request[\"examples\"]\n            - for request_dict.phases[i] test if docs needed or examples\n                - yes -&gt; search in vector base docs_base or code_base and use gather information + request_dict.phases[i] withe\n                 llm to provide problem specific documentation (explain + exact informations)\n                 build concrete prompt seam request_dict[f'{i}{k}_prompt'] = {request_dict.phases[i][k]}{'additional informations'}\n                - no -&gt; request_dict[f'{i}{k}_prompt'] ={request_dict.phases[i][k]}\n\n    - main phase\n        - start len(request_dict[i]) instances of parallel worker instances\n            -&gt; worker instances\n             - write code snippet according to request_dict[f'{i}{k}_prompt']\n              - test if compiles\n                - re write in format ````[language]\n</code></pre> <p>[comment-prefix][file-path] [content]```                   - save code snippet to divide_code</p> <pre><code>            --&gt; final worker re wirte code snippet and run integration test combine the snippets up to the final version\n             - test version run cond bas test and code test format : ````[language]\n</code></pre> <p>[comment-prefix][file-path]:[lines start-end, obtonal][content]```         - if crate_new_files             - write new content to files         - if save_changes             - write new content to lines         return new code or cahnsed and eddetet file pathes</p> <pre><code>Args:\n    request (str or list): in str for nl changes or rove implementation plans, or list for multibyte actions\n    existing_code_base (str, optional): the code in an ````[language]\n</code></pre> <p>[comment-prefix][file-path]``` format                                             or the path or url to the repository         local_sources (list[str], optional): local sources code-, text-, md-, and pdf-files         remote_sources (list[str], optional): remote sources gist-,GitHub-, pdf-url, and documentation urls         save_changes (bool, optional): default False         crate_new_files (bool, optional): default False         working_directory (str, None): path     Returns:         new_code (str): new code or path withe changes</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/runner.py</code> <pre><code>def runner(request: str or list,\n           existing_code_base: str = None,\n           local_sources: list = None,\n           remote_sources: list = None,\n           save_changes: bool = False,\n           crate_new_files: bool = False,\n           working_directory: str = None):\n    \"\"\"\n    Fuction to use isaa and agents to generate production redy code\n\n    Abaluf paln:\n        - aufberitung der input parameter\n            - parsing von request\n                - herausfinden von ben\u00f6tigten informationen -&gt; list [docs:bool, examples:bool]\n            - parsing von existing_code_base\n                - wenn None -&gt; do nothing\n                - wenn code,docs -&gt; start analyse, write docs and save to vector store mit meat daten analysed-code or analysed-docs\n                - wenn path -&gt;\n                    - finde docs folder + code and save to vector store mit meat daten row-docs or row-code\n                    - wenn kein docs folder gefunden wurde save code and to vector store mit meat daten row-code\n                - wenn url download to local_sources.path and -&gt;\n                    - gist = start analyse, write docs and save to vector store mit meat daten analysed-code\n                    - git = *- wenn path\n                    - jupiter-notebook = *- gist\n            - parsing von local_sources -&gt;\n                - wenn code = *- wenn path -&gt;\n                - wenn text = use text_parser *- wenn code,docs\n                - wenn md = use md_parser *- wenn code,docs\n                - wenn pdf = use pdf_parser *- wenn code,docs\n            - parsing von remote_sources download to local_sources.path and *- parsing von local_sources -&gt;\n        - entgegenehemn der input im format :\n            request:dict = {\"request\":str, \"docs\":bool, \"examples\":bool}\n            sources:list = str&lt;VectorStorName&gt;,  str&lt;VectorStorName&gt;\n        - divide | request in subsequenzen aufbereiten\n            - use isaa and llmMode divideMode to dived request[\"request\"] into a json dict\n            - pars json dict wit anything_from_str_to_dict(data: str, expected_keys: dict = None)\n        - serialise | request dict into\n            - dict format keys form 0 to n\n            - items from keys is a list of task that can be execute in parallel.\n            - a item is just a string withs representatives a part of the divided request.\n        - entgegenehemn der input im format :\n            request_dict:dict = {\"0\":[\"change thing one\", \"refactor the man page\"], 1\":[\"change thing tow\"]\"...}\n        - information collection phase\n            - if request[\"docs\"] or request[\"examples\"]\n                - for request_dict.phases[i] test if docs needed or examples\n                    - yes -&gt; search in vector base docs_base or code_base and use gather information + request_dict.phases[i] withe\n                     llm to provide problem specific documentation (explain + exact informations)\n                     build concrete prompt seam request_dict[f'{i}{k}_prompt'] = {request_dict.phases[i][k]}{'additional informations'}\n                    - no -&gt; request_dict[f'{i}{k}_prompt'] ={request_dict.phases[i][k]}\n\n        - main phase\n            - start len(request_dict[i]) instances of parallel worker instances\n                -&gt; worker instances\n                 - write code snippet according to request_dict[f'{i}{k}_prompt']\n                  - test if compiles\n                    - re write in format ````[language]\\n[comment-prefix] [file-path] [content]```\n                  - save code snippet to divide_code\n\n                --&gt; final worker re wirte code snippet and run integration test combine the snippets up to the final version\n                 - test version run cond bas test and code test format : ````[language]\\n[comment-prefix] [file-path]:[lines start-end, obtonal] [content]```\n        - if crate_new_files\n            - write new content to files\n        - if save_changes\n            - write new content to lines\n        return new code or cahnsed and eddetet file pathes\n\n\n\n    Args:\n        request (str or list): in str for nl changes or rove implementation plans, or list for multibyte actions\n        existing_code_base (str, optional): the code in an ````[language]\\n[comment-prefix] [file-path]``` format\n                                            or the path or url to the repository\n        local_sources (list[str], optional): local sources code-, text-, md-, and pdf-files\n        remote_sources (list[str], optional): remote sources gist-,GitHub-, pdf-url, and documentation urls\n        save_changes (bool, optional): default False\n        crate_new_files (bool, optional): default False\n        working_directory (str, None): path\n    Returns:\n        new_code (str): new code or path withe changes\n    \"\"\"\n    app = get_app(from_=f\"{Name}.runner\")\n    isaa: Tools = app.get_mod('isaa')\n    isaa.register_agents_setter(lambda x: x.set_logging_callback(print_prompt))\n    # isaa.global_stream_override = True\n    logger: Logger = get_logger()\n\n    space_name = \"testSp\"\n    if working_directory is None:\n        working_directory = '.\\\\'\n\n    logger.info(\"Start parsing input Parameters\")\n    request_dict, sources_list = parsing_inputs(isaa=isaa,\n                                                request=request,\n                                                existing_code_base=existing_code_base,\n                                                local_sources=local_sources,\n                                                remote_sources=remote_sources,\n                                                space_name=space_name,\n                                                working_directory=working_directory)\n\n    isaa.print(request_dict[\"result_list\"])  #\n\n    if request_dict[\"result_list\"][2] &lt; 0.3:\n        pass\n\n    mem = isaa.get_memory()\n    with tqdm(total=1, unit='step', desc='divide request') as pbar:\n        imple_plan = isaa.mini_task_completion(request_dict['request'], isaa.controller.rget(DivideMode))\n        pbar.update()\n    imple_request_list = imple_plan.split('\\n\\n')\n    llm_prompts = []\n    with tqdm(total=len(imple_request_list), unit='prompt', desc='generating prompts') as pbar:\n        def helper(task):\n            if not task or len(task) &lt; 10:\n                return\n            print(\"Task:\", task)\n            prompt_request = \"Task: \" + str(task)\n            # prompt_request += \"\\n full pan: \" + imple_plan\n            infos = []\n            if request_dict[\"result_list\"][0]:\n                infos += mem.search(sources_list[1], task)\n            if request_dict[\"result_list\"][1]:\n                infos += mem.search(sources_list[0], task)\n            if len(infos) &gt; 0:\n                prompt_request += \"Additional informations :\" + isaa.mini_task_completion(\n                    f\"Collect informations for {task} information: \" + '\\n'.join(\n                        [i[0].page_content if isinstance(i, tuple) else i.page_content for i in infos]),\n                    mode=TextExtractor,\n                    fetch_memory=True, all_mem=False)\n\n            return isaa.mini_task_completion(prompt_request, isaa.controller.rget(CreatePrompt))\n\n        with ThreadPoolExecutor(max_workers=6) as executor:\n            futures: set[Future] = {executor.submit(helper, _task) for _task in imple_request_list if _task}\n            for futures_ in futures:\n                llm_prompts.append(futures_.result())\n                pbar.update(1)\n    print(llm_prompts)\n    with Spinner(symbols='t', message='writing code snippets'):\n        sup_sulotns = isaa.mini_task_completion_format(llm_prompts,\n                                                       '```[language]\\n[comment-prefix] [file-path]:[lines start-end, '\n                                                       'obtonal] [content]```',\n                                                       None,\n                                                       \"code\")\n    print(sup_sulotns)\n\n    mem.add_data(\"Coding\" + space_name, sup_sulotns)\n\n    \"\"\"\n        - main phase\n            - start len(request_dict[i]) instances of parallel worker instances\n                -&gt; worker instances\n                 - write code snippet according to request_dict[f'{i}{k}_prompt']\n                  - test if compiles\n                    - re write in format ```[language]\\n[comment-prefix] [file-path] [content]```\n                  - save code snippet to divide_code\n\n                --&gt; final worker re wirte code snippet and run integration test combine the snippets up to the final version\n                 - test version run cond bas test and code test format : ````[language]\\n[comment-prefix] [file-path]:[lines start-end, obtonal] [content]```\n        - if crate_new_files\n            - write new content to files\n        - if save_changes\n            - write new content to lines\n        return new code or cahnsed and eddetet file pathes\n    \"\"\"\n    # dsd\n    return sup_sulotns\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.rust","title":"<code>rust</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.rust.CargoRustInterface","title":"<code>CargoRustInterface</code>","text":"Source code in <code>toolboxv2/mods/isaa/CodingAgent/rust.py</code> <pre><code>class CargoRustInterface:\n    def __init__(self, session_dir=None, auto_remove=True):\n        \"\"\"Initialize the Rust/Cargo interface\"\"\"\n        self.auto_remove = auto_remove\n        self._session_dir = session_dir or Path.home() / '.cargo_sessions'\n        self._session_dir.mkdir(exist_ok=True)\n        self.vfs = VirtualFileSystem(self._session_dir / 'virtual_fs')\n        self.output_history = {}\n        self._execution_count = 0\n        self.current_project = None\n\n    def reset(self):\n        \"\"\"Reset the interface state\"\"\"\n        if self.auto_remove and self.current_project:\n            shutil.rmtree(self.current_project, ignore_errors=True)\n        self.output_history.clear()\n        self._execution_count = 0\n        self.current_project = None\n\n    async def setup_project(self, name: str) -&gt; str:\n        \"\"\"Set up a new Cargo project\"\"\"\n        try:\n            project_path = self.vfs.base_dir / name\n            if project_path.exists():\n                shutil.rmtree(project_path)\n\n            result = subprocess.run(\n                ['cargo', 'new', str(project_path)],\n                capture_output=True,\n                text=True\n            )\n\n            if result.returncode != 0:\n                return f\"Error creating project: {result.stderr}\"\n\n            self.current_project = project_path\n            return f\"Created new project at {project_path}\"\n\n        except Exception as e:\n            return f\"Failed to create project: {str(e)}\"\n\n    async def add_dependency(self, name: str, version: str | None = None) -&gt; str:\n        \"\"\"Add a dependency to Cargo.toml\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            cargo_toml = self.current_project / \"Cargo.toml\"\n            if not cargo_toml.exists():\n                return \"Cargo.toml not found\"\n\n            cmd = ['cargo', 'add', name]\n            if version:\n                cmd.extend(['--vers', version])\n\n            result = subprocess.run(\n                cmd,\n                cwd=self.current_project,\n                capture_output=True,\n                text=True\n            )\n\n            return result.stdout if result.returncode == 0 else f\"Error: {result.stderr}\"\n\n        except Exception as e:\n            return f\"Failed to add dependency: {str(e)}\"\n\n    async def build(self, release: bool = False) -&gt; str:\n        \"\"\"Build the project\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            cmd = ['cargo', 'build']\n            if release:\n                cmd.append('--release')\n\n            result = subprocess.run(\n                cmd,\n                cwd=self.current_project,\n                capture_output=True,\n                text=True\n            )\n\n            return result.stdout if result.returncode == 0 else f\"Build error: {result.stderr}\"\n\n        except Exception as e:\n            return f\"Build failed: {str(e)}\"\n\n    async def test(self) -&gt; str:\n        \"\"\"Run project tests\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            result = subprocess.run(\n                ['cargo', 'test'],\n                cwd=self.current_project,\n                capture_output=True,\n                text=True\n            )\n\n            return result.stdout if result.returncode == 0 else f\"Test error: {result.stderr}\"\n\n        except Exception as e:\n            return f\"Tests failed: {str(e)}\"\n\n    async def run_code(self, code: str) -&gt; str:\n        \"\"\"Run Rust code\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            # Write code to main.rs\n            main_rs = self.current_project / \"src\" / \"main.rs\"\n            with open(main_rs, 'w') as f:\n                f.write(code)\n\n            # Build and run\n            build_result = subprocess.run(\n                ['cargo', 'build'],\n                cwd=self.current_project,\n                capture_output=True,\n                text=True\n            )\n\n            if build_result.returncode != 0:\n                return f\"Compilation error: {build_result.stderr}\"\n\n            run_result = subprocess.run(\n                ['cargo', 'run'],\n                cwd=self.current_project,\n                capture_output=True,\n                text=True\n            )\n\n            self._execution_count += 1\n            output = {\n                'code': code,\n                'stdout': run_result.stdout,\n                'stderr': run_result.stderr,\n                'result': run_result.returncode == 0\n            }\n            self.output_history[self._execution_count] = output\n\n            return run_result.stdout if run_result.returncode == 0 else f\"Runtime error: {run_result.stderr}\"\n\n        except Exception as e:\n            return f\"Execution failed: {str(e)}\"\n\n    async def modify_code(self, code: str, object_name: str, file: str = \"src/main.rs\") -&gt; str:\n        \"\"\"Modify existing Rust code\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            file_path = self.current_project / file\n            if not file_path.exists():\n                return f\"File {file} not found\"\n\n            with open(file_path) as f:\n                content = f.read()\n\n            # Handle function modification\n            if object_name.endswith(\"()\"):\n                func_name = object_name[:-2]\n                # Find and replace function definition\n                pattern = f\"fn {func_name}.*?}}(?=\\n|$)\"\n                updated_content = re.sub(pattern, code.strip(), content, flags=re.DOTALL)\n            else:\n                # Handle other modifications (structs, constants, etc.)\n                pattern = f\"{object_name}.*?(?=\\n|$)\"\n                updated_content = re.sub(pattern, code.strip(), content)\n\n            with open(file_path, 'w') as f:\n                f.write(updated_content)\n\n            return f\"Modified {object_name} in {file}\"\n\n        except Exception as e:\n            return f\"Modification failed: {str(e)}\"\n\n    def save_session(self, name: str):\n        \"\"\"Save current session state\"\"\"\n        session_file = self._session_dir / f\"{name}.json\"\n        state = {\n            'output_history': self.output_history,\n            'current_project': str(self.current_project) if self.current_project else None\n        }\n\n        with open(session_file, 'w') as f:\n            json.dump(state, f)\n\n    def load_session(self, name: str):\n        \"\"\"Load saved session state\"\"\"\n        session_file = self._session_dir / f\"{name}.json\"\n        if session_file.exists():\n            with open(session_file) as f:\n                state = json.load(f)\n                self.output_history = state['output_history']\n                self.current_project = Path(state['current_project']) if state['current_project'] else None\n</code></pre> <code>__init__(session_dir=None, auto_remove=True)</code> \u00b6 <p>Initialize the Rust/Cargo interface</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/rust.py</code> <pre><code>def __init__(self, session_dir=None, auto_remove=True):\n    \"\"\"Initialize the Rust/Cargo interface\"\"\"\n    self.auto_remove = auto_remove\n    self._session_dir = session_dir or Path.home() / '.cargo_sessions'\n    self._session_dir.mkdir(exist_ok=True)\n    self.vfs = VirtualFileSystem(self._session_dir / 'virtual_fs')\n    self.output_history = {}\n    self._execution_count = 0\n    self.current_project = None\n</code></pre> <code>add_dependency(name, version=None)</code> <code>async</code> \u00b6 <p>Add a dependency to Cargo.toml</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/rust.py</code> <pre><code>async def add_dependency(self, name: str, version: str | None = None) -&gt; str:\n    \"\"\"Add a dependency to Cargo.toml\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        cargo_toml = self.current_project / \"Cargo.toml\"\n        if not cargo_toml.exists():\n            return \"Cargo.toml not found\"\n\n        cmd = ['cargo', 'add', name]\n        if version:\n            cmd.extend(['--vers', version])\n\n        result = subprocess.run(\n            cmd,\n            cwd=self.current_project,\n            capture_output=True,\n            text=True\n        )\n\n        return result.stdout if result.returncode == 0 else f\"Error: {result.stderr}\"\n\n    except Exception as e:\n        return f\"Failed to add dependency: {str(e)}\"\n</code></pre> <code>build(release=False)</code> <code>async</code> \u00b6 <p>Build the project</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/rust.py</code> <pre><code>async def build(self, release: bool = False) -&gt; str:\n    \"\"\"Build the project\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        cmd = ['cargo', 'build']\n        if release:\n            cmd.append('--release')\n\n        result = subprocess.run(\n            cmd,\n            cwd=self.current_project,\n            capture_output=True,\n            text=True\n        )\n\n        return result.stdout if result.returncode == 0 else f\"Build error: {result.stderr}\"\n\n    except Exception as e:\n        return f\"Build failed: {str(e)}\"\n</code></pre> <code>load_session(name)</code> \u00b6 <p>Load saved session state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/rust.py</code> <pre><code>def load_session(self, name: str):\n    \"\"\"Load saved session state\"\"\"\n    session_file = self._session_dir / f\"{name}.json\"\n    if session_file.exists():\n        with open(session_file) as f:\n            state = json.load(f)\n            self.output_history = state['output_history']\n            self.current_project = Path(state['current_project']) if state['current_project'] else None\n</code></pre> <code>modify_code(code, object_name, file='src/main.rs')</code> <code>async</code> \u00b6 <p>Modify existing Rust code</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/rust.py</code> <pre><code>async def modify_code(self, code: str, object_name: str, file: str = \"src/main.rs\") -&gt; str:\n    \"\"\"Modify existing Rust code\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        file_path = self.current_project / file\n        if not file_path.exists():\n            return f\"File {file} not found\"\n\n        with open(file_path) as f:\n            content = f.read()\n\n        # Handle function modification\n        if object_name.endswith(\"()\"):\n            func_name = object_name[:-2]\n            # Find and replace function definition\n            pattern = f\"fn {func_name}.*?}}(?=\\n|$)\"\n            updated_content = re.sub(pattern, code.strip(), content, flags=re.DOTALL)\n        else:\n            # Handle other modifications (structs, constants, etc.)\n            pattern = f\"{object_name}.*?(?=\\n|$)\"\n            updated_content = re.sub(pattern, code.strip(), content)\n\n        with open(file_path, 'w') as f:\n            f.write(updated_content)\n\n        return f\"Modified {object_name} in {file}\"\n\n    except Exception as e:\n        return f\"Modification failed: {str(e)}\"\n</code></pre> <code>reset()</code> \u00b6 <p>Reset the interface state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/rust.py</code> <pre><code>def reset(self):\n    \"\"\"Reset the interface state\"\"\"\n    if self.auto_remove and self.current_project:\n        shutil.rmtree(self.current_project, ignore_errors=True)\n    self.output_history.clear()\n    self._execution_count = 0\n    self.current_project = None\n</code></pre> <code>run_code(code)</code> <code>async</code> \u00b6 <p>Run Rust code</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/rust.py</code> <pre><code>async def run_code(self, code: str) -&gt; str:\n    \"\"\"Run Rust code\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        # Write code to main.rs\n        main_rs = self.current_project / \"src\" / \"main.rs\"\n        with open(main_rs, 'w') as f:\n            f.write(code)\n\n        # Build and run\n        build_result = subprocess.run(\n            ['cargo', 'build'],\n            cwd=self.current_project,\n            capture_output=True,\n            text=True\n        )\n\n        if build_result.returncode != 0:\n            return f\"Compilation error: {build_result.stderr}\"\n\n        run_result = subprocess.run(\n            ['cargo', 'run'],\n            cwd=self.current_project,\n            capture_output=True,\n            text=True\n        )\n\n        self._execution_count += 1\n        output = {\n            'code': code,\n            'stdout': run_result.stdout,\n            'stderr': run_result.stderr,\n            'result': run_result.returncode == 0\n        }\n        self.output_history[self._execution_count] = output\n\n        return run_result.stdout if run_result.returncode == 0 else f\"Runtime error: {run_result.stderr}\"\n\n    except Exception as e:\n        return f\"Execution failed: {str(e)}\"\n</code></pre> <code>save_session(name)</code> \u00b6 <p>Save current session state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/rust.py</code> <pre><code>def save_session(self, name: str):\n    \"\"\"Save current session state\"\"\"\n    session_file = self._session_dir / f\"{name}.json\"\n    state = {\n        'output_history': self.output_history,\n        'current_project': str(self.current_project) if self.current_project else None\n    }\n\n    with open(session_file, 'w') as f:\n        json.dump(state, f)\n</code></pre> <code>setup_project(name)</code> <code>async</code> \u00b6 <p>Set up a new Cargo project</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/rust.py</code> <pre><code>async def setup_project(self, name: str) -&gt; str:\n    \"\"\"Set up a new Cargo project\"\"\"\n    try:\n        project_path = self.vfs.base_dir / name\n        if project_path.exists():\n            shutil.rmtree(project_path)\n\n        result = subprocess.run(\n            ['cargo', 'new', str(project_path)],\n            capture_output=True,\n            text=True\n        )\n\n        if result.returncode != 0:\n            return f\"Error creating project: {result.stderr}\"\n\n        self.current_project = project_path\n        return f\"Created new project at {project_path}\"\n\n    except Exception as e:\n        return f\"Failed to create project: {str(e)}\"\n</code></pre> <code>test()</code> <code>async</code> \u00b6 <p>Run project tests</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/rust.py</code> <pre><code>async def test(self) -&gt; str:\n    \"\"\"Run project tests\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        result = subprocess.run(\n            ['cargo', 'test'],\n            cwd=self.current_project,\n            capture_output=True,\n            text=True\n        )\n\n        return result.stdout if result.returncode == 0 else f\"Test error: {result.stderr}\"\n\n    except Exception as e:\n        return f\"Tests failed: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.rust.RustPipeline","title":"<code>RustPipeline</code>","text":"<p>A pipeline specialized for Rust development with crates.io integration and build feedback.</p> <p>Features: - Crates.io documentation crawling and caching - Rust compiler feedback parsing - Project structure management - Auto-dependency resolution - Test execution and coverage analysis</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/rust.py</code> <pre><code>class RustPipeline:\n    \"\"\"\n    A pipeline specialized for Rust development with crates.io integration and build feedback.\n\n    Features:\n    - Crates.io documentation crawling and caching\n    - Rust compiler feedback parsing\n    - Project structure management\n    - Auto-dependency resolution\n    - Test execution and coverage analysis\n    \"\"\"\n\n    def __init__(\n        self,\n        agent: Any,\n        project_path: Path,\n        verbose: bool = False,\n        max_iter: int = 12,\n        cache_dir: Path | None = None\n    ):\n        self.agent = agent\n        self.project_path = Path(project_path)\n        self.verbose = verbose\n        self.max_iter = max_iter\n        self.cache_dir = cache_dir or Path.home() / \".rust_pipeline_cache\"\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n\n        # Initialize crawler for docs.rs\n        self.crawler = AsyncWebCrawler(\n            config=BrowserConfig(\n                headless=True,\n                extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"]\n            )\n        )\n\n        # Track project state\n        self.cargo_toml: dict | None = None\n        self.current_crates: dict[str, CrateInfo] = {}\n        self.execution_history: list[ExecutionRecord] = []\n\n        # Initialize memory systems\n        self.docs_memory = ChatSession(agent.memory, \"RustDocs\")\n        self.build_memory = ChatSession(agent.memory, \"BuildResults\")\n\n    async def __aenter__(self):\n        await self.crawler.start()\n        await self.load_project_state()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.crawler.close()\n\n    async def load_project_state(self):\n        \"\"\"Load and parse current project state including Cargo.toml\"\"\"\n        cargo_path = self.project_path / \"Cargo.toml\"\n        if cargo_path.exists():\n            with open(cargo_path) as f:\n                self.cargo_toml = toml.load(f)\n\n        # Load cached crate documentation\n        for crate_file in self.cache_dir.glob(\"crate_*.json\"):\n            with open(crate_file) as f:\n                crate_data = json.load(f)\n                self.current_crates[crate_data[\"name\"]] = CrateInfo(**crate_data)\n\n    async def fetch_crate_docs(self, crate_name: str, version: str | None = None) -&gt; str:\n        \"\"\"Fetch and cache documentation from docs.rs\"\"\"\n        cache_file = self.cache_dir / f\"crate_{crate_name}.json\"\n\n        if cache_file.exists():\n            with open(cache_file) as f:\n                cached = json.load(f)\n                if version is None or cached[\"version\"] == version:\n                    return cached[\"documentation\"]\n\n        url = f\"https://docs.rs/{crate_name}\"\n        if version:\n            url += f\"/{version}\"\n\n        result = await self.crawler.arun(\n            url=url,\n            config=CrawlerRunConfig(markdown_generator=DefaultMarkdownGenerator()),\n            session_id=\"docs_session\"\n        )\n\n        if result.success:\n            crate_info = {\n                \"name\": crate_name,\n                \"version\": version or \"latest\",\n                \"documentation\": result.markdown_v2.raw_markdown,\n                \"timestamp\": datetime.now().isoformat()\n            }\n            with open(cache_file, \"w\") as f:\n                json.dump(crate_info, f)\n            return result.markdown_v2.raw_markdown\n        else:\n            return f\"Error fetching docs: {result.error_message}\"\n\n    async def execute_rust(self, code: str, file_path: str | None = None) -&gt; ExecutionRecord:\n        \"\"\"Execute Rust code, handling both file updates and compilation\"\"\"\n        try:\n            if file_path:\n                # Update existing or create new file\n                full_path = self.project_path / file_path\n                full_path.parent.mkdir(parents=True, exist_ok=True)\n                with open(full_path, \"w\") as f:\n                    f.write(code)\n\n                # Run cargo check for immediate feedback\n                result = await self._run_cargo_command(\"check\")\n                return ExecutionRecord(code=code, result=result, error=None)\n            else:\n                # For single expression evaluation, use cargo eval\n                result = await self._run_cargo_command(\"eval\", input_code=code)\n                return ExecutionRecord(code=code, result=result, error=None)\n        except Exception as e:\n            return ExecutionRecord(code=code, result=None, error=str(e))\n\n    async def _run_cargo_command(self, cmd: str, input_code: str | None = None) -&gt; str:\n        \"\"\"Execute cargo commands and capture output\"\"\"\n        if cmd == \"eval\" and input_code:\n            # Create temporary file for evaluation\n            eval_path = self.project_path / \"src\" / \"eval.rs\"\n            with open(eval_path, \"w\") as f:\n                f.write(f\"\"\"\nfn main() {{\n    println!(\"{{:?}}\", {{\n        {input_code}\n    }});\n}}\n\"\"\")\n\n        process = await asyncio.create_subprocess_exec(\n            \"cargo\", cmd,\n            cwd=self.project_path,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n\n        stdout, stderr = await process.communicate()\n        return f\"stdout:\\n{stdout.decode()}\\nstderr:\\n{stderr.decode()}\"\n\n    async def run(self, task: str) -&gt; dict[str, Any]:\n        \"\"\"Execute a Rust development task with documentation and build feedback\"\"\"\n        state = ThinkState.ACTION\n        result = None\n        iteration = 0\n\n        # Initialize task context\n        context = {\n            \"task\": task,\n            \"project_state\": self.cargo_toml,\n            \"available_crates\": list(self.current_crates.keys())\n        }\n\n        while state != ThinkState.DONE and iteration &lt; self.max_iter:\n            iteration += 1\n\n            if state == ThinkState.ACTION:\n                # Get agent's next action\n                think_result = await self.agent.think(context)\n\n                if think_result.action == \"code\":\n                    # Execute Rust code\n                    execution = await self.execute_rust(\n                        think_result.content,\n                        think_result.context.get(\"file_path\")\n                    )\n                    self.execution_history.append(execution)\n                    state = ThinkState.PROCESSING\n\n                elif think_result.action == \"docs\":\n                    # Fetch crate documentation\n                    crate = think_result.context[\"crate\"]\n                    docs = await self.fetch_crate_docs(\n                        crate,\n                        think_result.context.get(\"version\")\n                    )\n                    await self.docs_memory.add_message({\n                        \"role\": \"system\",\n                        \"content\": f\"Documentation for {crate}:\\n{docs}\"\n                    })\n                    state = ThinkState.ACTION\n\n                elif think_result.action == \"done\":\n                    state = ThinkState.DONE\n                    result = think_result.content\n\n            elif state == ThinkState.PROCESSING:\n                # Analyze execution results\n                last_execution = self.execution_history[-1]\n                if last_execution.error:\n                    # Parse compiler errors\n                    await self.build_memory.add_message({\n                        \"role\": \"system\",\n                        \"content\": f\"Build error:\\n{last_execution.error}\"\n                    })\n                    state = ThinkState.ACTION\n                else:\n                    state = ThinkState.ACTION\n\n            await asyncio.sleep(1)  # Prevent rate limiting\n\n        return {\n            \"result\": result,\n            \"execution_history\": self.execution_history,\n            \"docs_memory\": await self.docs_memory.get_messages(),\n            \"build_memory\": await self.build_memory.get_messages()\n        }\n</code></pre> <code>execute_rust(code, file_path=None)</code> <code>async</code> \u00b6 <p>Execute Rust code, handling both file updates and compilation</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/rust.py</code> <pre><code>async def execute_rust(self, code: str, file_path: str | None = None) -&gt; ExecutionRecord:\n    \"\"\"Execute Rust code, handling both file updates and compilation\"\"\"\n    try:\n        if file_path:\n            # Update existing or create new file\n            full_path = self.project_path / file_path\n            full_path.parent.mkdir(parents=True, exist_ok=True)\n            with open(full_path, \"w\") as f:\n                f.write(code)\n\n            # Run cargo check for immediate feedback\n            result = await self._run_cargo_command(\"check\")\n            return ExecutionRecord(code=code, result=result, error=None)\n        else:\n            # For single expression evaluation, use cargo eval\n            result = await self._run_cargo_command(\"eval\", input_code=code)\n            return ExecutionRecord(code=code, result=result, error=None)\n    except Exception as e:\n        return ExecutionRecord(code=code, result=None, error=str(e))\n</code></pre> <code>fetch_crate_docs(crate_name, version=None)</code> <code>async</code> \u00b6 <p>Fetch and cache documentation from docs.rs</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/rust.py</code> <pre><code>async def fetch_crate_docs(self, crate_name: str, version: str | None = None) -&gt; str:\n    \"\"\"Fetch and cache documentation from docs.rs\"\"\"\n    cache_file = self.cache_dir / f\"crate_{crate_name}.json\"\n\n    if cache_file.exists():\n        with open(cache_file) as f:\n            cached = json.load(f)\n            if version is None or cached[\"version\"] == version:\n                return cached[\"documentation\"]\n\n    url = f\"https://docs.rs/{crate_name}\"\n    if version:\n        url += f\"/{version}\"\n\n    result = await self.crawler.arun(\n        url=url,\n        config=CrawlerRunConfig(markdown_generator=DefaultMarkdownGenerator()),\n        session_id=\"docs_session\"\n    )\n\n    if result.success:\n        crate_info = {\n            \"name\": crate_name,\n            \"version\": version or \"latest\",\n            \"documentation\": result.markdown_v2.raw_markdown,\n            \"timestamp\": datetime.now().isoformat()\n        }\n        with open(cache_file, \"w\") as f:\n            json.dump(crate_info, f)\n        return result.markdown_v2.raw_markdown\n    else:\n        return f\"Error fetching docs: {result.error_message}\"\n</code></pre> <code>load_project_state()</code> <code>async</code> \u00b6 <p>Load and parse current project state including Cargo.toml</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/rust.py</code> <pre><code>async def load_project_state(self):\n    \"\"\"Load and parse current project state including Cargo.toml\"\"\"\n    cargo_path = self.project_path / \"Cargo.toml\"\n    if cargo_path.exists():\n        with open(cargo_path) as f:\n            self.cargo_toml = toml.load(f)\n\n    # Load cached crate documentation\n    for crate_file in self.cache_dir.glob(\"crate_*.json\"):\n        with open(crate_file) as f:\n            crate_data = json.load(f)\n            self.current_crates[crate_data[\"name\"]] = CrateInfo(**crate_data)\n</code></pre> <code>run(task)</code> <code>async</code> \u00b6 <p>Execute a Rust development task with documentation and build feedback</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/rust.py</code> <pre><code>async def run(self, task: str) -&gt; dict[str, Any]:\n    \"\"\"Execute a Rust development task with documentation and build feedback\"\"\"\n    state = ThinkState.ACTION\n    result = None\n    iteration = 0\n\n    # Initialize task context\n    context = {\n        \"task\": task,\n        \"project_state\": self.cargo_toml,\n        \"available_crates\": list(self.current_crates.keys())\n    }\n\n    while state != ThinkState.DONE and iteration &lt; self.max_iter:\n        iteration += 1\n\n        if state == ThinkState.ACTION:\n            # Get agent's next action\n            think_result = await self.agent.think(context)\n\n            if think_result.action == \"code\":\n                # Execute Rust code\n                execution = await self.execute_rust(\n                    think_result.content,\n                    think_result.context.get(\"file_path\")\n                )\n                self.execution_history.append(execution)\n                state = ThinkState.PROCESSING\n\n            elif think_result.action == \"docs\":\n                # Fetch crate documentation\n                crate = think_result.context[\"crate\"]\n                docs = await self.fetch_crate_docs(\n                    crate,\n                    think_result.context.get(\"version\")\n                )\n                await self.docs_memory.add_message({\n                    \"role\": \"system\",\n                    \"content\": f\"Documentation for {crate}:\\n{docs}\"\n                })\n                state = ThinkState.ACTION\n\n            elif think_result.action == \"done\":\n                state = ThinkState.DONE\n                result = think_result.content\n\n        elif state == ThinkState.PROCESSING:\n            # Analyze execution results\n            last_execution = self.execution_history[-1]\n            if last_execution.error:\n                # Parse compiler errors\n                await self.build_memory.add_message({\n                    \"role\": \"system\",\n                    \"content\": f\"Build error:\\n{last_execution.error}\"\n                })\n                state = ThinkState.ACTION\n            else:\n                state = ThinkState.ACTION\n\n        await asyncio.sleep(1)  # Prevent rate limiting\n\n    return {\n        \"result\": result,\n        \"execution_history\": self.execution_history,\n        \"docs_memory\": await self.docs_memory.get_messages(),\n        \"build_memory\": await self.build_memory.get_messages()\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster","title":"<code>SearchAgentCluster</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool","title":"<code>search_tool</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool.WebContentParser","title":"<code>WebContentParser</code>","text":"<p>Utility class for parsing web content using BrowserAnt</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>class WebContentParser:\n    \"\"\"Utility class for parsing web content using BrowserAnt\"\"\"\n\n    def __init__(self, browser_wrapper: BrowserWrapper):\n        \"\"\"Initialize with a browser wrapper\"\"\"\n        self.browser_wrapper = browser_wrapper\n\n    async def extract_article(self, url: str) -&gt; dict[str, Any]:\n        \"\"\"Extract article content with title, text, and metadata\"\"\"\n        await self.browser_wrapper.initialize()\n        page = await self.browser_wrapper.navigate(url)\n\n        # Execute readability.js to extract article content\n        readability_js = \"\"\"\n        function extractArticle() {\n            // Simple article extraction logic\n            const article = {\n                title: document.title,\n                byline: '',\n                content: '',\n                textContent: '',\n                excerpt: '',\n                siteName: '',\n                publishedTime: ''\n            };\n\n            // Try to find article elements\n            const articleElement = document.querySelector('article') ||\n                                   document.querySelector('main') ||\n                                   document.querySelector('.post-content') ||\n                                   document.querySelector('.entry-content');\n\n            if (articleElement) {\n                article.content = articleElement.innerHTML;\n                article.textContent = articleElement.textContent;\n            } else {\n                // Fallback to body content\n                article.content = document.body.innerHTML;\n                article.textContent = document.body.textContent;\n            }\n\n            // Try to extract metadata\n            const metaTags = document.querySelectorAll('meta');\n            metaTags.forEach(tag =&gt; {\n                const property = tag.getAttribute('property') || tag.getAttribute('name');\n                const content = tag.getAttribute('content');\n\n                if (property &amp;&amp; content) {\n                    if (property === 'og:site_name') article.siteName = content;\n                    if (property === 'og:title' &amp;&amp; !article.title) article.title = content;\n                    if (property === 'og:description' &amp;&amp; !article.excerpt) article.excerpt = content;\n                    if (property === 'article:published_time') article.publishedTime = content;\n                    if (property === 'author' || property === 'article:author') article.byline = content;\n                }\n            });\n\n            // Extract first paragraph as excerpt if not found\n            if (!article.excerpt) {\n                const paragraphs = document.querySelectorAll('p');\n                if (paragraphs.length &gt; 0) {\n                    for (let i = 0; i &lt; paragraphs.length; i++) {\n                        const text = paragraphs[i].textContent.trim();\n                        if (text.length &gt; 50) {\n                            article.excerpt = text;\n                            break;\n                        }\n                    }\n                }\n            }\n\n            return article;\n        }\n\n        return extractArticle();\n        \"\"\"\n\n        # Extract article content\n        article = await page.evaluate(readability_js)\n\n        # Add markdown version\n        article['markdown'] = await self.browser_wrapper.extract_markdown(page)\n\n        # Take a screenshot\n        screenshot_data = await self.browser_wrapper.take_scrolling_screenshot(page)\n        article['screenshot'] = base64.b64encode(screenshot_data).decode('utf-8')\n\n        return article\n\n    async def extract_table_data(self, url: str, table_selector: str = 'table') -&gt; list[dict[str, Any]]:\n        \"\"\"Extract tabular data from a webpage\"\"\"\n        await self.browser_wrapper.initialize()\n        page = await self.browser_wrapper.navigate(url)\n\n        # Script to extract table data\n        extract_table_js = \"\"\"\n        (tableSelector) =&gt; {\n            const tables = document.querySelectorAll(tableSelector);\n            if (tables.length === 0) return [];\n\n            // Use the first table found\n            const table = tables[0];\n            const headers = Array.from(table.querySelectorAll('th')).map(th =&gt; th.textContent.trim());\n\n            // If no headers found, try using the first row\n            const headerRow = headers.length &gt; 0 ? headers :\n                            Array.from(table.querySelectorAll('tr:first-child td')).map(td =&gt; td.textContent.trim());\n\n            const rows = Array.from(table.querySelectorAll('tr'));\n            const result = [];\n\n            // Start from 1 if we have headers, otherwise from 0\n            const startIdx = headers.length &gt; 0 ? 1 : 0;\n\n            for (let i = startIdx; i &lt; rows.length; i++) {\n                const row = rows[i];\n                const cells = Array.from(row.querySelectorAll('td')).map(td =&gt; td.textContent.trim());\n\n                if (cells.length &gt; 0) {\n                    const rowData = {};\n                    for (let j = 0; j &lt; Math.min(headerRow.length, cells.length); j++) {\n                        // Create a valid object key from header\n                        const key = headerRow[j].replace(/[^a-zA-Z0-9]/g, '_').toLowerCase();\n                        rowData[key] = cells[j];\n                    }\n                    result.push(rowData);\n                }\n            }\n\n            return result;\n        }\n        \"\"\"\n\n        # Extract data\n        table_data = await page.evaluate(extract_table_js, table_selector)\n        return table_data\n\n    async def extract_links(self, url: str, link_selector: str = 'a') -&gt; list[dict[str, str]]:\n        \"\"\"Extract all links from a webpage\"\"\"\n        await self.browser_wrapper.initialize()\n        page = await self.browser_wrapper.navigate(url)\n\n        # Script to extract links\n        extract_links_js = \"\"\"\n        (linkSelector) =&gt; {\n            const links = Array.from(document.querySelectorAll(linkSelector));\n            return links.map(link =&gt; {\n                return {\n                    text: link.textContent.trim(),\n                    href: link.href,\n                    title: link.getAttribute('title') || '',\n                    isExternal: link.hostname !== window.location.hostname\n                };\n            }).filter(link =&gt; link.href &amp;&amp; link.href.startsWith('http'));\n        }\n        \"\"\"\n\n        # Extract links\n        links = await page.evaluate(extract_links_js, link_selector)\n        return links\n</code></pre> <code>__init__(browser_wrapper)</code> \u00b6 <p>Initialize with a browser wrapper</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>def __init__(self, browser_wrapper: BrowserWrapper):\n    \"\"\"Initialize with a browser wrapper\"\"\"\n    self.browser_wrapper = browser_wrapper\n</code></pre> <code>extract_article(url)</code> <code>async</code> \u00b6 <p>Extract article content with title, text, and metadata</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def extract_article(self, url: str) -&gt; dict[str, Any]:\n    \"\"\"Extract article content with title, text, and metadata\"\"\"\n    await self.browser_wrapper.initialize()\n    page = await self.browser_wrapper.navigate(url)\n\n    # Execute readability.js to extract article content\n    readability_js = \"\"\"\n    function extractArticle() {\n        // Simple article extraction logic\n        const article = {\n            title: document.title,\n            byline: '',\n            content: '',\n            textContent: '',\n            excerpt: '',\n            siteName: '',\n            publishedTime: ''\n        };\n\n        // Try to find article elements\n        const articleElement = document.querySelector('article') ||\n                               document.querySelector('main') ||\n                               document.querySelector('.post-content') ||\n                               document.querySelector('.entry-content');\n\n        if (articleElement) {\n            article.content = articleElement.innerHTML;\n            article.textContent = articleElement.textContent;\n        } else {\n            // Fallback to body content\n            article.content = document.body.innerHTML;\n            article.textContent = document.body.textContent;\n        }\n\n        // Try to extract metadata\n        const metaTags = document.querySelectorAll('meta');\n        metaTags.forEach(tag =&gt; {\n            const property = tag.getAttribute('property') || tag.getAttribute('name');\n            const content = tag.getAttribute('content');\n\n            if (property &amp;&amp; content) {\n                if (property === 'og:site_name') article.siteName = content;\n                if (property === 'og:title' &amp;&amp; !article.title) article.title = content;\n                if (property === 'og:description' &amp;&amp; !article.excerpt) article.excerpt = content;\n                if (property === 'article:published_time') article.publishedTime = content;\n                if (property === 'author' || property === 'article:author') article.byline = content;\n            }\n        });\n\n        // Extract first paragraph as excerpt if not found\n        if (!article.excerpt) {\n            const paragraphs = document.querySelectorAll('p');\n            if (paragraphs.length &gt; 0) {\n                for (let i = 0; i &lt; paragraphs.length; i++) {\n                    const text = paragraphs[i].textContent.trim();\n                    if (text.length &gt; 50) {\n                        article.excerpt = text;\n                        break;\n                    }\n                }\n            }\n        }\n\n        return article;\n    }\n\n    return extractArticle();\n    \"\"\"\n\n    # Extract article content\n    article = await page.evaluate(readability_js)\n\n    # Add markdown version\n    article['markdown'] = await self.browser_wrapper.extract_markdown(page)\n\n    # Take a screenshot\n    screenshot_data = await self.browser_wrapper.take_scrolling_screenshot(page)\n    article['screenshot'] = base64.b64encode(screenshot_data).decode('utf-8')\n\n    return article\n</code></pre> <code>extract_links(url, link_selector='a')</code> <code>async</code> \u00b6 <p>Extract all links from a webpage</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def extract_links(self, url: str, link_selector: str = 'a') -&gt; list[dict[str, str]]:\n    \"\"\"Extract all links from a webpage\"\"\"\n    await self.browser_wrapper.initialize()\n    page = await self.browser_wrapper.navigate(url)\n\n    # Script to extract links\n    extract_links_js = \"\"\"\n    (linkSelector) =&gt; {\n        const links = Array.from(document.querySelectorAll(linkSelector));\n        return links.map(link =&gt; {\n            return {\n                text: link.textContent.trim(),\n                href: link.href,\n                title: link.getAttribute('title') || '',\n                isExternal: link.hostname !== window.location.hostname\n            };\n        }).filter(link =&gt; link.href &amp;&amp; link.href.startsWith('http'));\n    }\n    \"\"\"\n\n    # Extract links\n    links = await page.evaluate(extract_links_js, link_selector)\n    return links\n</code></pre> <code>extract_table_data(url, table_selector='table')</code> <code>async</code> \u00b6 <p>Extract tabular data from a webpage</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def extract_table_data(self, url: str, table_selector: str = 'table') -&gt; list[dict[str, Any]]:\n    \"\"\"Extract tabular data from a webpage\"\"\"\n    await self.browser_wrapper.initialize()\n    page = await self.browser_wrapper.navigate(url)\n\n    # Script to extract table data\n    extract_table_js = \"\"\"\n    (tableSelector) =&gt; {\n        const tables = document.querySelectorAll(tableSelector);\n        if (tables.length === 0) return [];\n\n        // Use the first table found\n        const table = tables[0];\n        const headers = Array.from(table.querySelectorAll('th')).map(th =&gt; th.textContent.trim());\n\n        // If no headers found, try using the first row\n        const headerRow = headers.length &gt; 0 ? headers :\n                        Array.from(table.querySelectorAll('tr:first-child td')).map(td =&gt; td.textContent.trim());\n\n        const rows = Array.from(table.querySelectorAll('tr'));\n        const result = [];\n\n        // Start from 1 if we have headers, otherwise from 0\n        const startIdx = headers.length &gt; 0 ? 1 : 0;\n\n        for (let i = startIdx; i &lt; rows.length; i++) {\n            const row = rows[i];\n            const cells = Array.from(row.querySelectorAll('td')).map(td =&gt; td.textContent.trim());\n\n            if (cells.length &gt; 0) {\n                const rowData = {};\n                for (let j = 0; j &lt; Math.min(headerRow.length, cells.length); j++) {\n                    // Create a valid object key from header\n                    const key = headerRow[j].replace(/[^a-zA-Z0-9]/g, '_').toLowerCase();\n                    rowData[key] = cells[j];\n                }\n                result.push(rowData);\n            }\n        }\n\n        return result;\n    }\n    \"\"\"\n\n    # Extract data\n    table_data = await page.evaluate(extract_table_js, table_selector)\n    return table_data\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool.WebScraper","title":"<code>WebScraper</code>","text":"<pre><code>A high-performance web scraper using BrowserAnt with multi-tab parallel processing.\nHandles both structured and unstructured data collection efficiently.\n</code></pre> <p>import asyncio from pydantic import BaseModel, Field from typing import List, Optional</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool.WebScraper--define-a-structured-data-model","title":"Define a structured data model","text":"<p>class ProductInfo(BaseModel):     title: str     price: str     description: Optional[str] = None     rating: Optional[str] = None     availability: Optional[str] = None</p> <p>async def main():     # Initialize the scraper     scraper = WebScraper()</p> <pre><code># Example 1: Simple scraping of a single URL\nresult = await scraper.scrape_url(\"https://example.com\")\nprint(f\"Title: {result['title']}\")\nprint(f\"Content: {result['markdown'][:200]}...\")\n\n# Example 2: Parallel scraping of multiple URLs\nurls = [\n    \"https://example.com/page1\",\n    \"https://example.com/page2\",\n    \"https://example.com/page3\"\n]\nresults = await scraper.scrape_urls(urls)\n\n# Example 3: Structured data extraction\nproducts = await scraper.scrape_structured_data(\n    urls=[\"https://example.com/product1\", \"https://example.com/product2\"],\n    model=ProductInfo,\n    extraction_task=\"Extract product information including title, price, and availability status\"\n)\n\nfor product in products:\n    if product:\n        print(f\"Product: {product.title}, Price: {product.price}\")\n\n# Clean up\nawait scraper.close()\n</code></pre> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>class WebScraper:\n    \"\"\"\n    A high-performance web scraper using BrowserAnt with multi-tab parallel processing.\n    Handles both structured and unstructured data collection efficiently.\nimport asyncio\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\n\n# Define a structured data model\nclass ProductInfo(BaseModel):\n    title: str\n    price: str\n    description: Optional[str] = None\n    rating: Optional[str] = None\n    availability: Optional[str] = None\n\nasync def main():\n    # Initialize the scraper\n    scraper = WebScraper()\n\n    # Example 1: Simple scraping of a single URL\n    result = await scraper.scrape_url(\"https://example.com\")\n    print(f\"Title: {result['title']}\")\n    print(f\"Content: {result['markdown'][:200]}...\")\n\n    # Example 2: Parallel scraping of multiple URLs\n    urls = [\n        \"https://example.com/page1\",\n        \"https://example.com/page2\",\n        \"https://example.com/page3\"\n    ]\n    results = await scraper.scrape_urls(urls)\n\n    # Example 3: Structured data extraction\n    products = await scraper.scrape_structured_data(\n        urls=[\"https://example.com/product1\", \"https://example.com/product2\"],\n        model=ProductInfo,\n        extraction_task=\"Extract product information including title, price, and availability status\"\n    )\n\n    for product in products:\n        if product:\n            print(f\"Product: {product.title}, Price: {product.price}\")\n\n    # Clean up\n    await scraper.close()\n    \"\"\"\n\n    def __init__(\n        self,\n        config: WebScraperConfig = WebScraperConfig(),\n        llm: str | BaseChatModel | None = None,\n        chrome_path: str | None = None,\n        remote_url: str | None = None,\n        browser_config: dict[str, Any] | None = None\n    ):\n        \"\"\"\n        Initialize the web scraper with configuration.\n\n        Args:\n            config: Configuration for scraper behavior\n            llm: Language model for intelligent data extraction\n            chrome_path: Path to Chrome executable\n            remote_url: URL for remote browser connection\n            browser_config: Additional browser configuration\n        \"\"\"\n        self.config = config\n        self.browser_wrapper = BrowserWrapper(\n            llm=llm,\n            headless=config.headless,\n            chrome_path=chrome_path,\n            remote_url=remote_url,\n            config=browser_config\n        )\n        self.active_tasks = set()\n        self._semaphore = asyncio.Semaphore(config.max_concurrent_tabs)\n        self._results = {}\n\n        # Create screenshot directory if needed\n        if config.save_screenshots and not os.path.exists(config.screenshot_dir):\n            os.makedirs(config.screenshot_dir)\n\n    async def initialize(self):\n        \"\"\"Initialize the browser if not already initialized\"\"\"\n        await self.browser_wrapper.initialize()\n\n    async def close(self):\n        \"\"\"Close the browser and clean up resources\"\"\"\n        # Wait for all active tasks to complete\n        if self.active_tasks:\n            await asyncio.gather(*self.active_tasks)\n        await self.browser_wrapper.close()\n\n\n    # Add this method to your WebScraper class\n    async def search_web(\n        self,\n        query: str,\n        max_results: int = 5,\n        include_content: bool = True,\n        extract_images: bool = False,\n        extract_tables: bool = False,\n        extract_links: bool = False,\n        save_to_file: str | None = None\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Perform a comprehensive web search and return high-quality data for the given query.\n\n        Args:\n            query: Search query string\n            max_results: Maximum number of results to process (default: 5)\n            include_content: Whether to include full content from result pages (default: True)\n            extract_images: Whether to extract images from result pages (default: False)\n            extract_tables: Whether to extract tables from result pages (default: False)\n            extract_links: Whether to extract links from result pages (default: False)\n            save_to_file: Path to save results as JSON (optional)\n\n        Returns:\n            Dictionary containing search results and extracted information\n        \"\"\"\n        await self.initialize()\n        try:\n            start_time = datetime.now()\n\n            # Try different search engines in order\n            search_engines = [\n                {\n                    \"url\": f\"https://www.google.com/search?q={urllib.parse.quote_plus(query)}\",\n                    \"result_selector\": \".g\",\n                    \"title_selector\": \"h3\",\n                    \"link_selector\": \"a\",\n                    \"snippet_selector\": \".VwiC3b\",\n                    \"name\": \"google\"\n                },\n                {\n                    \"url\": f\"https://www.bing.com/search?q={urllib.parse.quote_plus(query)}\",\n                    \"result_selector\": \".b_algo\",\n                    \"title_selector\": \"h2\",\n                    \"link_selector\": \"a\",\n                    \"snippet_selector\": \".b_caption p\",\n                    \"name\": \"bing\"\n                },\n                {\n                    \"url\": f\"https://duckduckgo.com/?q={urllib.parse.quote_plus(query)}\",\n                    \"result_selector\": \".result\",\n                    \"title_selector\": \"h2\",\n                    \"link_selector\": \"a.result__a\",\n                    \"snippet_selector\": \".result__snippet\",\n                    \"name\": \"duckduckgo\"\n                }\n            ]\n\n            results = []\n\n            for engine in search_engines:\n                try:\n                    # Navigate to search engine\n                    page = await self.browser_wrapper.navigate(engine[\"url\"])\n                    await page.wait_for_load_state(\"networkidle\")\n                    await page.wait_for_timeout(2000)  # Wait for results to load\n\n                    # Extract search results\n                    search_results = await page.evaluate(\n                        \"\"\"\n                        (selectors) =&gt; {\n                            const results = [];\n                            const elements = document.querySelectorAll(selectors.result_selector);\n\n                            for (const element of elements) {\n                                const titleElement = element.querySelector(selectors.title_selector);\n                                const linkElement = element.querySelector(selectors.link_selector);\n                                const snippetElement = element.querySelector(selectors.snippet_selector);\n\n                                if (titleElement &amp;&amp; linkElement) {\n                                    const url = linkElement.href;\n                                    // Skip non-http links and same-domain results\n                                    if (url &amp;&amp; url.startsWith('http') &amp;&amp;\n                                        !url.includes('google.com/search') &amp;&amp;\n                                        !url.includes('bing.com/search') &amp;&amp;\n                                        !url.includes('duckduckgo.com')) {\n                                        results.push({\n                                            title: titleElement.textContent.trim(),\n                                            url: url,\n                                            snippet: snippetElement ? snippetElement.textContent.trim() : '',\n                                            source: selectors.name\n                                        });\n                                    }\n                                }\n                            }\n                            return results;\n                        }\n                        \"\"\",\n                        engine\n                    )\n\n                    if search_results and len(search_results) &gt; 0:\n                        # We got results, add them and break\n                        results = search_results\n                        break\n\n                except Exception as e:\n                    print(f\"Error searching with {engine['name']}: {str(e)}\")\n                    continue  # Try next engine\n\n            # Filter and limit results\n            unique_urls = set()\n            filtered_results = []\n\n            for result in results:\n                if result['url'] not in unique_urls and len(filtered_results) &lt; max_results:\n                    unique_urls.add(result['url'])\n                    filtered_results.append(result)\n\n            results = filtered_results\n\n            # Get detailed content if requested\n            if include_content and results:\n                # Extract content from each result page\n                urls_to_scrape = [result['url'] for result in results]\n\n                # Configure what to extract\n                extract_config = {}\n                if extract_tables:\n                    extract_config['tables'] = 'table'\n                if extract_images:\n                    extract_config['images'] = 'img'\n                if extract_links:\n                    extract_config['links'] = 'a'\n\n                # Scrape all pages in parallel using our efficient multi-tab approach\n                scraped_data = await self.scrape_urls(\n                    urls_to_scrape,\n                    extract_config=extract_config if extract_config else None\n                )\n\n                # Add content to results\n                for i, result in enumerate(results):\n                    if i &lt; len(scraped_data) and 'error' not in scraped_data[i]:\n                        result['content'] = {\n                            'title': scraped_data[i].get('title', result['title']),\n                            'markdown': scraped_data[i].get('markdown', ''),\n                            'text': scraped_data[i].get('text', ''),\n                        }\n\n                        # Add structured data if available\n                        if extract_config and 'structured_data' in scraped_data[i]:\n                            structured_data = scraped_data[i]['structured_data']\n                            for key, value in structured_data.items():\n                                if value:  # Only add non-empty data\n                                    result['content'][key] = value\n\n            # Prepare final response\n            response = {\n                'query': query,\n                'timestamp': datetime.now().isoformat(),\n                'num_results': len(results),\n                'results': results,\n                'execution_time': (datetime.now() - start_time).total_seconds()\n            }\n\n            # Save to file if requested\n            if save_to_file:\n                os.makedirs(os.path.dirname(os.path.abspath(save_to_file)), exist_ok=True)\n                with open(save_to_file, 'w', encoding='utf-8') as f:\n                    json.dump(response, f, ensure_ascii=False, indent=2)\n\n            return response\n\n        finally:\n            # Make sure we clean up browser resources\n            await self.close()\n\n    async def _scrape_url(self, url: str, task_id: str, extract_config: dict[str, Any] = None):\n        \"\"\"\n        Internal method to scrape a single URL\n\n        Args:\n            url: URL to scrape\n            task_id: Unique identifier for this scraping task\n            extract_config: Configuration for what/how to extract\n        \"\"\"\n        try:\n            async with self._semaphore:\n                # Navigate to the URL\n                page = await self.browser_wrapper.navigate(url)\n\n                # Wait for network to become idle\n                await page.wait_for_load_state(\"networkidle\")\n\n                # Perform initial delay\n                if self.config.initial_delay &gt; 0:\n                    await page.wait_for_timeout(self.config.initial_delay)\n\n                # Auto-scroll if configured\n                if self.config.auto_scroll:\n                    await self._auto_scroll(page)\n\n                # Initialize result dictionary\n                result = {\n                    \"url\": url,\n                    \"title\": await page.title(),\n                    \"timestamp\": datetime.now().isoformat(),\n                }\n\n                # Take screenshot if needed\n                if self.config.save_screenshots:\n                    file_name = f\"{urlparse(url).netloc}_{task_id}.png\"\n                    screenshot_path = os.path.join(self.config.screenshot_dir, file_name)\n                    result[\"screenshot\"] = screenshot_path\n                    await self.browser_wrapper.take_scrolling_screenshot(\n                        page=page,\n                        path=screenshot_path,\n                        initial_delay=0,  # We've already waited\n                        scroll_delay=self.config.scroll_delay\n                    )\n\n                # Extract content based on configuration\n                if extract_config:\n                    result[\"structured_data\"] = await self.browser_wrapper.extract_structured_content(\n                        page=page,\n                        config=extract_config\n                    )\n\n                # Extract markdown if configured\n                if self.config.extract_markdown:\n                    result[\"markdown\"] = await self.browser_wrapper.extract_markdown(page=page)\n\n                # Extract text if configured\n                if self.config.extract_text:\n                    result[\"text\"] = await self.browser_wrapper.extract_text(page=page)\n\n                # Extract HTML if configured\n                if self.config.extract_html:\n                    result[\"html\"] = await page.content()\n\n                self._results[task_id] = result\n                return result\n\n        except Exception as e:\n            self._results[task_id] = {\"error\": str(e), \"url\": url}\n            return {\"error\": str(e), \"url\": url}\n\n    async def _auto_scroll(self, page):\n        \"\"\"Automatically scroll down the page to load lazy content\"\"\"\n        try:\n            # Get page dimensions\n            dimensions = await page.evaluate(\"\"\"\n                () =&gt; {\n                    return {\n                        width: document.documentElement.scrollWidth,\n                        height: document.documentElement.scrollHeight,\n                        windowHeight: window.innerHeight\n                    }\n                }\n            \"\"\")\n\n            # Scroll down the page gradually\n            current_position = 0\n            while current_position &lt; dimensions['height']:\n                await page.evaluate(f\"window.scrollTo(0, {current_position})\")\n                await page.wait_for_timeout(self.config.scroll_delay)\n                current_position += dimensions['windowHeight'] // 2\n\n            # Scroll back to top\n            await page.evaluate(\"window.scrollTo(0, 0)\")\n        except Exception as e:\n            print(f\"Error during auto-scroll: {e}\")\n\n    async def scrape_url(self, url: str, extract_config: dict[str, Any] = None) -&gt; dict[str, Any]:\n        \"\"\"\n        Scrape a single URL and return the results\n\n        Args:\n            url: URL to scrape\n            extract_config: Configuration for structured data extraction\n\n        Returns:\n            Dictionary containing scraped data\n        \"\"\"\n        await self.initialize()\n        task_id = f\"{len(self._results)}_{datetime.now().timestamp()}\"\n        result = await self._scrape_url(url, task_id, extract_config)\n        return result\n\n    async def scrape_urls(\n        self,\n        urls: list[str],\n        extract_config: dict[str, Any] | None = None\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Scrape multiple URLs in parallel and return all results\n\n        Args:\n            urls: List of URLs to scrape\n            extract_config: Configuration for structured data extraction\n\n        Returns:\n            List of dictionaries containing scraped data\n        \"\"\"\n        await self.initialize()\n        tasks = []\n\n        for i, url in enumerate(urls):\n            task_id = f\"{i}_{datetime.now().timestamp()}\"\n            task = asyncio.create_task(self._scrape_url(url, task_id, extract_config))\n            self.active_tasks.add(task)\n            task.add_done_callback(self.active_tasks.discard)\n            tasks.append(task)\n\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return [r if not isinstance(r, Exception) else {\"error\": str(r)} for r in results]\n\n    async def scrape_structured_data(\n        self,\n        urls: list[str],\n        model: type[T],\n        extraction_task: str = None\n    ) -&gt; list[T]:\n        \"\"\"\n        Scrape and parse structured data into pydantic models\n\n        Args:\n            urls: List of URLs to scrape\n            model: Pydantic model class for structured data\n            extraction_task: Natural language description of what to extract\n\n        Returns:\n            List of parsed data objects\n        \"\"\"\n        await self.initialize()\n\n        # Create intelligent extraction task if provided\n        if extraction_task:\n            # Create a custom system prompt for extraction\n            class ExtractionPrompt(SystemPrompt):\n                def important_rules(self) -&gt; str:\n                    existing_rules = super().important_rules()\n                    new_rules = f\"\"\"\n                    9. EXTRACTION GOAL:\n                    - Your primary goal is to extract data according to this specific task: {extraction_task}\n                    - You should carefully identify and extract the information as accurately as possible.\n                    - Focus only on relevant information that matches the specified data structure.\n                    \"\"\"\n                    return f'{existing_rules}\\n{new_rules}'\n\n            # Define the extraction task for each URL\n            tasks = []\n            for url in urls:\n                # Setup intelligent extraction for each URL\n                task = asyncio.create_task(self._run_extraction_agent(\n                    url=url,\n                    model=model,\n                    extraction_task=extraction_task,\n                    system_prompt_class=ExtractionPrompt\n                ))\n                self.active_tasks.add(task)\n                task.add_done_callback(self.active_tasks.discard)\n                tasks.append(task)\n\n            # Wait for all extractions to complete\n            results = await asyncio.gather(*tasks, return_exceptions=True)\n            return [r if not isinstance(r, Exception) else None for r in results]\n        else:\n            # Manual extraction based on model fields\n            field_selectors = {}\n            for field_name in model.__annotations__:\n                # Convert field name to likely CSS selector\n                snake_case = field_name\n                selector = f\".{snake_case.replace('_', '-')}, #{snake_case.replace('_', '-')}\"\n                field_selectors[field_name] = selector\n\n            # Scrape with these selectors\n            raw_results = await self.scrape_urls(urls, extract_config=field_selectors)\n\n            # Convert to pydantic models\n            parsed_results = []\n            for result in raw_results:\n                try:\n                    if \"structured_data\" in result and \"error\" not in result:\n                        # Map the extracted data to model fields\n                        model_data = {}\n                        for field_name in model.__annotations__:\n                            if field_name in result[\"structured_data\"]:\n                                field_value = result[\"structured_data\"][field_name]\n                                if isinstance(field_value, list) and len(field_value) &gt; 0:\n                                    model_data[field_name] = field_value[0]  # Take first match\n                                else:\n                                    model_data[field_name] = field_value\n\n                        # Create the model instance\n                        parsed_results.append(model(**model_data))\n                    else:\n                        parsed_results.append(None)\n                except Exception as e:\n                    print(f\"Error parsing result: {e}\")\n                    parsed_results.append(None)\n\n            return parsed_results\n\n    async def _run_extraction_agent(\n        self,\n        url: str,\n        model: type[T],\n        extraction_task: str,\n        system_prompt_class: type[SystemPrompt]\n    ) -&gt; T:\n        \"\"\"Run an intelligent agent to extract structured data\"\"\"\n        # Define output model for the agent\n        controller = Controller(output_model=model)\n\n        # Create the task description\n        fields_info = \"\\n\".join([f\"- {field}: {model.__annotations__[field].__name__}\"\n                                 for field in model.__annotations__])\n\n        task = f\"\"\"\n        Go to {url} and extract the following information:\n        {fields_info}\n\n        Specific extraction instructions: {extraction_task}\n        \"\"\"\n\n        # Create and run the agent\n        agent = await self.browser_wrapper.create_agent(task=task)\n        agent._controller = controller\n        agent._system_prompt_class = system_prompt_class\n\n        history = await agent.run()\n\n        # Parse the result\n        result = history.final_result()\n        if result:\n            try:\n                return model.model_validate_json(result)\n            except Exception as e:\n                print(f\"Error parsing agent result: {e}\")\n                return None\n        return None\n</code></pre> <code>__init__(config=WebScraperConfig(), llm=None, chrome_path=None, remote_url=None, browser_config=None)</code> \u00b6 <p>Initialize the web scraper with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>WebScraperConfig</code> <p>Configuration for scraper behavior</p> <code>WebScraperConfig()</code> <code>llm</code> <code>str | BaseChatModel | None</code> <p>Language model for intelligent data extraction</p> <code>None</code> <code>chrome_path</code> <code>str | None</code> <p>Path to Chrome executable</p> <code>None</code> <code>remote_url</code> <code>str | None</code> <p>URL for remote browser connection</p> <code>None</code> <code>browser_config</code> <code>dict[str, Any] | None</code> <p>Additional browser configuration</p> <code>None</code> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>def __init__(\n    self,\n    config: WebScraperConfig = WebScraperConfig(),\n    llm: str | BaseChatModel | None = None,\n    chrome_path: str | None = None,\n    remote_url: str | None = None,\n    browser_config: dict[str, Any] | None = None\n):\n    \"\"\"\n    Initialize the web scraper with configuration.\n\n    Args:\n        config: Configuration for scraper behavior\n        llm: Language model for intelligent data extraction\n        chrome_path: Path to Chrome executable\n        remote_url: URL for remote browser connection\n        browser_config: Additional browser configuration\n    \"\"\"\n    self.config = config\n    self.browser_wrapper = BrowserWrapper(\n        llm=llm,\n        headless=config.headless,\n        chrome_path=chrome_path,\n        remote_url=remote_url,\n        config=browser_config\n    )\n    self.active_tasks = set()\n    self._semaphore = asyncio.Semaphore(config.max_concurrent_tabs)\n    self._results = {}\n\n    # Create screenshot directory if needed\n    if config.save_screenshots and not os.path.exists(config.screenshot_dir):\n        os.makedirs(config.screenshot_dir)\n</code></pre> <code>close()</code> <code>async</code> \u00b6 <p>Close the browser and clean up resources</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def close(self):\n    \"\"\"Close the browser and clean up resources\"\"\"\n    # Wait for all active tasks to complete\n    if self.active_tasks:\n        await asyncio.gather(*self.active_tasks)\n    await self.browser_wrapper.close()\n</code></pre> <code>initialize()</code> <code>async</code> \u00b6 <p>Initialize the browser if not already initialized</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def initialize(self):\n    \"\"\"Initialize the browser if not already initialized\"\"\"\n    await self.browser_wrapper.initialize()\n</code></pre> <code>scrape_structured_data(urls, model, extraction_task=None)</code> <code>async</code> \u00b6 <p>Scrape and parse structured data into pydantic models</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list[str]</code> <p>List of URLs to scrape</p> required <code>model</code> <code>type[T]</code> <p>Pydantic model class for structured data</p> required <code>extraction_task</code> <code>str</code> <p>Natural language description of what to extract</p> <code>None</code> <p>Returns:</p> Type Description <code>list[T]</code> <p>List of parsed data objects</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def scrape_structured_data(\n    self,\n    urls: list[str],\n    model: type[T],\n    extraction_task: str = None\n) -&gt; list[T]:\n    \"\"\"\n    Scrape and parse structured data into pydantic models\n\n    Args:\n        urls: List of URLs to scrape\n        model: Pydantic model class for structured data\n        extraction_task: Natural language description of what to extract\n\n    Returns:\n        List of parsed data objects\n    \"\"\"\n    await self.initialize()\n\n    # Create intelligent extraction task if provided\n    if extraction_task:\n        # Create a custom system prompt for extraction\n        class ExtractionPrompt(SystemPrompt):\n            def important_rules(self) -&gt; str:\n                existing_rules = super().important_rules()\n                new_rules = f\"\"\"\n                9. EXTRACTION GOAL:\n                - Your primary goal is to extract data according to this specific task: {extraction_task}\n                - You should carefully identify and extract the information as accurately as possible.\n                - Focus only on relevant information that matches the specified data structure.\n                \"\"\"\n                return f'{existing_rules}\\n{new_rules}'\n\n        # Define the extraction task for each URL\n        tasks = []\n        for url in urls:\n            # Setup intelligent extraction for each URL\n            task = asyncio.create_task(self._run_extraction_agent(\n                url=url,\n                model=model,\n                extraction_task=extraction_task,\n                system_prompt_class=ExtractionPrompt\n            ))\n            self.active_tasks.add(task)\n            task.add_done_callback(self.active_tasks.discard)\n            tasks.append(task)\n\n        # Wait for all extractions to complete\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return [r if not isinstance(r, Exception) else None for r in results]\n    else:\n        # Manual extraction based on model fields\n        field_selectors = {}\n        for field_name in model.__annotations__:\n            # Convert field name to likely CSS selector\n            snake_case = field_name\n            selector = f\".{snake_case.replace('_', '-')}, #{snake_case.replace('_', '-')}\"\n            field_selectors[field_name] = selector\n\n        # Scrape with these selectors\n        raw_results = await self.scrape_urls(urls, extract_config=field_selectors)\n\n        # Convert to pydantic models\n        parsed_results = []\n        for result in raw_results:\n            try:\n                if \"structured_data\" in result and \"error\" not in result:\n                    # Map the extracted data to model fields\n                    model_data = {}\n                    for field_name in model.__annotations__:\n                        if field_name in result[\"structured_data\"]:\n                            field_value = result[\"structured_data\"][field_name]\n                            if isinstance(field_value, list) and len(field_value) &gt; 0:\n                                model_data[field_name] = field_value[0]  # Take first match\n                            else:\n                                model_data[field_name] = field_value\n\n                    # Create the model instance\n                    parsed_results.append(model(**model_data))\n                else:\n                    parsed_results.append(None)\n            except Exception as e:\n                print(f\"Error parsing result: {e}\")\n                parsed_results.append(None)\n\n        return parsed_results\n</code></pre> <code>scrape_url(url, extract_config=None)</code> <code>async</code> \u00b6 <p>Scrape a single URL and return the results</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to scrape</p> required <code>extract_config</code> <code>dict[str, Any]</code> <p>Configuration for structured data extraction</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing scraped data</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def scrape_url(self, url: str, extract_config: dict[str, Any] = None) -&gt; dict[str, Any]:\n    \"\"\"\n    Scrape a single URL and return the results\n\n    Args:\n        url: URL to scrape\n        extract_config: Configuration for structured data extraction\n\n    Returns:\n        Dictionary containing scraped data\n    \"\"\"\n    await self.initialize()\n    task_id = f\"{len(self._results)}_{datetime.now().timestamp()}\"\n    result = await self._scrape_url(url, task_id, extract_config)\n    return result\n</code></pre> <code>scrape_urls(urls, extract_config=None)</code> <code>async</code> \u00b6 <p>Scrape multiple URLs in parallel and return all results</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list[str]</code> <p>List of URLs to scrape</p> required <code>extract_config</code> <code>dict[str, Any] | None</code> <p>Configuration for structured data extraction</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of dictionaries containing scraped data</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def scrape_urls(\n    self,\n    urls: list[str],\n    extract_config: dict[str, Any] | None = None\n) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Scrape multiple URLs in parallel and return all results\n\n    Args:\n        urls: List of URLs to scrape\n        extract_config: Configuration for structured data extraction\n\n    Returns:\n        List of dictionaries containing scraped data\n    \"\"\"\n    await self.initialize()\n    tasks = []\n\n    for i, url in enumerate(urls):\n        task_id = f\"{i}_{datetime.now().timestamp()}\"\n        task = asyncio.create_task(self._scrape_url(url, task_id, extract_config))\n        self.active_tasks.add(task)\n        task.add_done_callback(self.active_tasks.discard)\n        tasks.append(task)\n\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    return [r if not isinstance(r, Exception) else {\"error\": str(r)} for r in results]\n</code></pre> <code>search_web(query, max_results=5, include_content=True, extract_images=False, extract_tables=False, extract_links=False, save_to_file=None)</code> <code>async</code> \u00b6 <p>Perform a comprehensive web search and return high-quality data for the given query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query string</p> required <code>max_results</code> <code>int</code> <p>Maximum number of results to process (default: 5)</p> <code>5</code> <code>include_content</code> <code>bool</code> <p>Whether to include full content from result pages (default: True)</p> <code>True</code> <code>extract_images</code> <code>bool</code> <p>Whether to extract images from result pages (default: False)</p> <code>False</code> <code>extract_tables</code> <code>bool</code> <p>Whether to extract tables from result pages (default: False)</p> <code>False</code> <code>extract_links</code> <code>bool</code> <p>Whether to extract links from result pages (default: False)</p> <code>False</code> <code>save_to_file</code> <code>str | None</code> <p>Path to save results as JSON (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing search results and extracted information</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def search_web(\n    self,\n    query: str,\n    max_results: int = 5,\n    include_content: bool = True,\n    extract_images: bool = False,\n    extract_tables: bool = False,\n    extract_links: bool = False,\n    save_to_file: str | None = None\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Perform a comprehensive web search and return high-quality data for the given query.\n\n    Args:\n        query: Search query string\n        max_results: Maximum number of results to process (default: 5)\n        include_content: Whether to include full content from result pages (default: True)\n        extract_images: Whether to extract images from result pages (default: False)\n        extract_tables: Whether to extract tables from result pages (default: False)\n        extract_links: Whether to extract links from result pages (default: False)\n        save_to_file: Path to save results as JSON (optional)\n\n    Returns:\n        Dictionary containing search results and extracted information\n    \"\"\"\n    await self.initialize()\n    try:\n        start_time = datetime.now()\n\n        # Try different search engines in order\n        search_engines = [\n            {\n                \"url\": f\"https://www.google.com/search?q={urllib.parse.quote_plus(query)}\",\n                \"result_selector\": \".g\",\n                \"title_selector\": \"h3\",\n                \"link_selector\": \"a\",\n                \"snippet_selector\": \".VwiC3b\",\n                \"name\": \"google\"\n            },\n            {\n                \"url\": f\"https://www.bing.com/search?q={urllib.parse.quote_plus(query)}\",\n                \"result_selector\": \".b_algo\",\n                \"title_selector\": \"h2\",\n                \"link_selector\": \"a\",\n                \"snippet_selector\": \".b_caption p\",\n                \"name\": \"bing\"\n            },\n            {\n                \"url\": f\"https://duckduckgo.com/?q={urllib.parse.quote_plus(query)}\",\n                \"result_selector\": \".result\",\n                \"title_selector\": \"h2\",\n                \"link_selector\": \"a.result__a\",\n                \"snippet_selector\": \".result__snippet\",\n                \"name\": \"duckduckgo\"\n            }\n        ]\n\n        results = []\n\n        for engine in search_engines:\n            try:\n                # Navigate to search engine\n                page = await self.browser_wrapper.navigate(engine[\"url\"])\n                await page.wait_for_load_state(\"networkidle\")\n                await page.wait_for_timeout(2000)  # Wait for results to load\n\n                # Extract search results\n                search_results = await page.evaluate(\n                    \"\"\"\n                    (selectors) =&gt; {\n                        const results = [];\n                        const elements = document.querySelectorAll(selectors.result_selector);\n\n                        for (const element of elements) {\n                            const titleElement = element.querySelector(selectors.title_selector);\n                            const linkElement = element.querySelector(selectors.link_selector);\n                            const snippetElement = element.querySelector(selectors.snippet_selector);\n\n                            if (titleElement &amp;&amp; linkElement) {\n                                const url = linkElement.href;\n                                // Skip non-http links and same-domain results\n                                if (url &amp;&amp; url.startsWith('http') &amp;&amp;\n                                    !url.includes('google.com/search') &amp;&amp;\n                                    !url.includes('bing.com/search') &amp;&amp;\n                                    !url.includes('duckduckgo.com')) {\n                                    results.push({\n                                        title: titleElement.textContent.trim(),\n                                        url: url,\n                                        snippet: snippetElement ? snippetElement.textContent.trim() : '',\n                                        source: selectors.name\n                                    });\n                                }\n                            }\n                        }\n                        return results;\n                    }\n                    \"\"\",\n                    engine\n                )\n\n                if search_results and len(search_results) &gt; 0:\n                    # We got results, add them and break\n                    results = search_results\n                    break\n\n            except Exception as e:\n                print(f\"Error searching with {engine['name']}: {str(e)}\")\n                continue  # Try next engine\n\n        # Filter and limit results\n        unique_urls = set()\n        filtered_results = []\n\n        for result in results:\n            if result['url'] not in unique_urls and len(filtered_results) &lt; max_results:\n                unique_urls.add(result['url'])\n                filtered_results.append(result)\n\n        results = filtered_results\n\n        # Get detailed content if requested\n        if include_content and results:\n            # Extract content from each result page\n            urls_to_scrape = [result['url'] for result in results]\n\n            # Configure what to extract\n            extract_config = {}\n            if extract_tables:\n                extract_config['tables'] = 'table'\n            if extract_images:\n                extract_config['images'] = 'img'\n            if extract_links:\n                extract_config['links'] = 'a'\n\n            # Scrape all pages in parallel using our efficient multi-tab approach\n            scraped_data = await self.scrape_urls(\n                urls_to_scrape,\n                extract_config=extract_config if extract_config else None\n            )\n\n            # Add content to results\n            for i, result in enumerate(results):\n                if i &lt; len(scraped_data) and 'error' not in scraped_data[i]:\n                    result['content'] = {\n                        'title': scraped_data[i].get('title', result['title']),\n                        'markdown': scraped_data[i].get('markdown', ''),\n                        'text': scraped_data[i].get('text', ''),\n                    }\n\n                    # Add structured data if available\n                    if extract_config and 'structured_data' in scraped_data[i]:\n                        structured_data = scraped_data[i]['structured_data']\n                        for key, value in structured_data.items():\n                            if value:  # Only add non-empty data\n                                result['content'][key] = value\n\n        # Prepare final response\n        response = {\n            'query': query,\n            'timestamp': datetime.now().isoformat(),\n            'num_results': len(results),\n            'results': results,\n            'execution_time': (datetime.now() - start_time).total_seconds()\n        }\n\n        # Save to file if requested\n        if save_to_file:\n            os.makedirs(os.path.dirname(os.path.abspath(save_to_file)), exist_ok=True)\n            with open(save_to_file, 'w', encoding='utf-8') as f:\n                json.dump(response, f, ensure_ascii=False, indent=2)\n\n        return response\n\n    finally:\n        # Make sure we clean up browser resources\n        await self.close()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool.WebScraperConfig","title":"<code>WebScraperConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for web scraper operations</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>class WebScraperConfig(BaseModel):\n    \"\"\"Configuration for web scraper operations\"\"\"\n    max_concurrent_tabs: int = 5\n    default_timeout: float = 30000\n    scroll_delay: int = 500\n    initial_delay: int = 1000\n    viewport_height: int = 900\n    viewport_width: int = 1600\n    wait_for_selectors: bool = True\n    auto_scroll: bool = True\n    save_screenshots: bool = False\n    screenshot_dir: str = \"./screenshots\"\n    extract_markdown: bool = True\n    extract_text: bool = True\n    extract_html: bool = False\n    headless: bool = False\n    disable_images: bool = False\n    user_agent: str | None = None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool.sanitize_filename","title":"<code>sanitize_filename(filename)</code>","text":"<p>Convert a string to a valid filename</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>def sanitize_filename(filename: str) -&gt; str:\n    \"\"\"Convert a string to a valid filename\"\"\"\n    # Replace spaces with underscores and remove invalid characters\n    sanitized = re.sub(r'[^\\w\\s-]', '', filename).strip().lower()\n    return re.sub(r'[-\\s]+', '_', sanitized)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool.scrape_documentation_to_markdown","title":"<code>scrape_documentation_to_markdown(start_url, topic=None, max_pages=30, max_depth=3, output_dir=None, toc_filename='table_of_contents.md')</code>  <code>async</code>","text":"<p>Recursively scrape documentation pages starting from a URL, focused on a specific topic, and convert to Markdown.</p> <p>Parameters:</p> Name Type Description Default <code>start_url</code> <code>str</code> <p>The documentation homepage or entry point</p> required <code>topic</code> <code>str | None</code> <p>The topic to focus on (e.g., \"streaming\", \"authentication\")</p> <code>None</code> <code>max_pages</code> <code>int</code> <p>Maximum number of pages to scrape</p> <code>30</code> <code>max_depth</code> <code>int</code> <p>Maximum depth of link traversal</p> <code>3</code> <code>output_dir</code> <code>str | None</code> <p>Directory to save the MD files (if None, just returns them)</p> <code>None</code> <code>toc_filename</code> <code>str</code> <p>Filename for the table of contents</p> <code>'table_of_contents.md'</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary mapping page titles to markdown content</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def scrape_documentation_to_markdown(\n    start_url: str,\n    topic: str | None = None,\n    max_pages: int = 30,\n    max_depth: int = 3,\n    output_dir: str | None = None,\n    toc_filename: str = \"table_of_contents.md\"\n) -&gt; dict[str, str]:\n    \"\"\"\n    Recursively scrape documentation pages starting from a URL,\n    focused on a specific topic, and convert to Markdown.\n\n    Args:\n        start_url: The documentation homepage or entry point\n        topic: The topic to focus on (e.g., \"streaming\", \"authentication\")\n        max_pages: Maximum number of pages to scrape\n        max_depth: Maximum depth of link traversal\n        output_dir: Directory to save the MD files (if None, just returns them)\n        toc_filename: Filename for the table of contents\n\n    Returns:\n        Dictionary mapping page titles to markdown content\n    \"\"\"\n    # Initialize scraper with efficient settings for docs\n    scraper_config = WebScraperConfig(\n        max_concurrent_tabs=5,\n        headless=global_headless,\n        disable_images=True,\n        extract_html=False,\n        auto_scroll=True,\n        scroll_delay=300,\n        initial_delay=500,\n        save_screenshots=False\n    )\n\n    scraper = WebScraper(config=scraper_config)\n    await scraper.initialize()\n\n    # Track visited and pending URLs\n    visited_urls: set[str] = set()\n    pending_urls: list[dict] = [{\"url\": start_url, \"depth\": 0, \"parent\": None}]\n    results: dict[str, dict] = {}\n    domain = urlparse(start_url).netloc\n\n    logging.info(f\"Starting documentation scrape from {start_url}\")\n    if topic:\n        logging.info(f\"Focusing on topic: {topic}\")\n\n    # Create a regular expression pattern for topic if provided\n    topic_pattern = re.compile(rf'\\b{re.escape(topic)}\\b', re.IGNORECASE) if topic else None\n\n    try:\n        # Process URLs breadth-first until we hit max pages or have no more URLs\n        while pending_urls and len(results) &lt; max_pages:\n            # Get the next URL to process\n            current = pending_urls.pop(0)\n            current_url = current[\"url\"]\n            current_depth = current[\"depth\"]\n\n            # Skip if we've already visited this URL\n            if current_url in visited_urls:\n                continue\n\n            logging.info(f\"Scraping: {current_url} (depth: {current_depth})\")\n            visited_urls.add(current_url)\n\n            # Scrape the current page\n            page_result = await scraper.scrape_url(current_url)\n\n            # Skip pages with errors\n            if \"error\" in page_result:\n                logging.warning(f\"Error scraping {current_url}: {page_result['error']}\")\n                continue\n\n            # Check if page is relevant to the topic\n            is_relevant = True\n            if topic_pattern:\n                markdown_content = page_result.get(\"markdown\", \"\")\n                text_content = page_result.get(\"text\", \"\")\n\n                # Check if topic appears in title, URL, or content\n                has_topic_in_title = topic_pattern.search(page_result.get(\"title\", \"\"))\n                has_topic_in_url = topic_pattern.search(current_url)\n                has_topic_in_content = (\n                    topic_pattern.search(markdown_content) or\n                    topic_pattern.search(text_content)\n                )\n\n                is_relevant = has_topic_in_title or has_topic_in_url or has_topic_in_content\n\n            # Process this page if it's relevant\n            if is_relevant:\n                # Extract title and content\n                title = page_result.get(\"title\", f\"Page {len(results) + 1}\")\n\n                # Store the result\n                results[current_url] = {\n                    \"title\": title,\n                    \"markdown\": page_result.get(\"markdown\", \"\"),\n                    \"depth\": current_depth,\n                    \"parent\": current[\"parent\"]\n                }\n\n                # Only proceed deeper if we haven't hit max depth\n                if current_depth &lt; max_depth:\n                    # Extract links to follow\n                    parser = scraper.browser_wrapper.get_parser()\n                    links = await parser.extract_links(current_url)\n\n                    # Filter links for internal documentation pages\n                    doc_links = []\n                    for link in links:\n                        link_url = link[\"href\"]\n                        parsed_url = urlparse(link_url)\n\n                        # Only include links to the same domain\n                        if parsed_url.netloc == domain or not parsed_url.netloc:\n                            # Normalize URL\n                            if not parsed_url.netloc:\n                                link_url = urljoin(current_url, link_url)\n\n                            # Skip anchor links to same page\n                            if link_url.split('#')[0] == current_url.split('#')[0]:\n                                continue\n\n                            # Skip non-documentation links (common patterns)\n                            skip_patterns = [\n                                r'(\\.pdf|\\.zip|\\.tar|\\.gz)$',  # Downloads\n                                r'/search/',  # Search pages\n                                r'/login/',  # Auth pages\n                                r'/logout/',  # Auth pages\n                                r'/tag/',  # Tag pages\n                                r'/version/',  # Version switching\n                                r'/latest/',  # Version switching\n                                r'/download/',  # Download pages\n                                r'/contact/',  # Contact pages\n                                r'/blog/',  # Blog posts (unless that's what we want)\n                            ]\n\n                            should_skip = any(re.search(pattern, link_url) for pattern in skip_patterns)\n                            if should_skip:\n                                continue\n\n                            # Check if it's potentially relevant to the topic\n                            is_potentially_relevant = True\n                            if topic_pattern:\n                                has_topic_in_link_text = topic_pattern.search(link[\"text\"])\n                                has_topic_in_link_url = topic_pattern.search(link_url)\n                                is_potentially_relevant = has_topic_in_link_text or has_topic_in_link_url\n\n                            # Add to pending if it's potentially relevant and not already visited\n                            if is_potentially_relevant and link_url not in visited_urls:\n                                doc_links.append({\n                                    \"url\": link_url,\n                                    \"depth\": current_depth + 1,\n                                    \"parent\": current_url\n                                })\n\n                    # Add the filtered links to our pending list\n                    pending_urls.extend(doc_links)\n\n        # Generate markdown output\n        markdown_results = {}\n\n        # Create a hierarchy for building a table of contents\n        pages_hierarchy = {}\n        for url, page_data in results.items():\n            title = page_data[\"title\"]\n            markdown = page_data[\"markdown\"]\n\n            # Add page URL reference at the bottom\n            markdown += f\"\\n\\n---\\n*Source: [{url}]({url})*\"\n\n            # Add to outputs\n            markdown_results[url] = markdown\n\n            # Track in hierarchy for TOC\n            depth = page_data[\"depth\"]\n            parent = page_data[\"parent\"]\n\n            if depth not in pages_hierarchy:\n                pages_hierarchy[depth] = []\n\n            pages_hierarchy[depth].append({\n                \"url\": url,\n                \"title\": title,\n                \"parent\": parent\n            })\n\n        # Generate table of contents\n        toc = f\"# Documentation: {topic if topic else 'All Topics'}\\n\\n\"\n        toc += f\"*Generated from: [{start_url}]({start_url})*\\n\\n\"\n        toc += \"## Table of Contents\\n\\n\"\n\n        # Sort by depth to build hierarchy\n        for depth in sorted(pages_hierarchy.keys()):\n            pages = pages_hierarchy[depth]\n\n            for page in pages:\n                # Calculate indentation based on depth\n                indent = \"  \" * depth\n                page_filename = sanitize_filename(page[\"title\"]) + \".md\"\n                toc += f\"{indent}- [{page['title']}]({page_filename})\\n\"\n\n        # Save the results if output directory specified\n        if output_dir:\n            os.makedirs(output_dir, exist_ok=True)\n\n            # Write the TOC file\n            with open(os.path.join(output_dir, toc_filename), \"w\", encoding=\"utf-8\") as f:\n                f.write(toc)\n\n            # Write each page file\n            for url, content in markdown_results.items():\n                page_title = results[url][\"title\"]\n                filename = sanitize_filename(page_title) + \".md\"\n                filepath = os.path.join(output_dir, filename)\n\n                with open(filepath, \"w\", encoding=\"utf-8\") as f:\n                    f.write(content)\n\n            logging.info(f\"Saved {len(markdown_results)} documentation pages to {output_dir}\")\n\n        # Include the TOC in the results\n        markdown_results[\"table_of_contents\"] = toc\n\n        return markdown_results\n\n    finally:\n        # Make sure we clean up browser resources\n        await scraper.close()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.auto_docs","title":"<code>auto_docs</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.auto_docs.analyze_missing_information","title":"<code>analyze_missing_information(data_chunk, current_context)</code>","text":"<p>Analyzes what information is missing from the current chunk and suggests improvements or additions.</p> Source code in <code>toolboxv2/mods/isaa/auto_docs.py</code> <pre><code>def analyze_missing_information(data_chunk, current_context):\n    \"\"\"\n    Analyzes what information is missing from the current chunk and suggests improvements or additions.\n    \"\"\"\n    # For simplicity, let's assume missing info is any reference to a non-existing concept in the context\n    keywords = extract_keywords_and_entities(data_chunk)\n    missing_information = [keyword for keyword in keywords if keyword not in current_context]\n    return missing_information\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.auto_docs.bottom_up_traversal","title":"<code>bottom_up_traversal(tree, current_context=None, depth=0, collected_docs=None, mini_task_completion=None)</code>","text":"<p>Traverse the tree from bottom-up, filling in missing information and collecting documentation.</p> Source code in <code>toolboxv2/mods/isaa/auto_docs.py</code> <pre><code>def bottom_up_traversal(tree, current_context=None, depth=0, collected_docs=None, mini_task_completion=None):\n    \"\"\"\n    Traverse the tree from bottom-up, filling in missing information and collecting documentation.\n    \"\"\"\n    if current_context is None:\n        current_context = set()\n    if mini_task_completion is None:\n        mini_task_completion = mini_task_completion_moc\n    if collected_docs is None:\n        collected_docs = []\n\n    for node in tree:\n        element = node['element']\n        children = node['children']\n\n        # Step 1: Recursively process the children (bottom-up)\n        child_context = set()\n        for child in children:\n            child_docs = bottom_up_traversal([child], current_context | child_context, depth + 1,\n                                             mini_task_completion=mini_task_completion)\n            collected_docs.extend(child_docs)\n            # Merge children's information into the current context\n            for doc in child_docs:\n                child_context.update(doc.get('missing_data', []))  # Get missing data from children\n\n        # Step 2: Process current node after all children are processed\n        context_window = current_context | child_context  # Combine parent and child contexts\n\n        # Analyze the current element to detect missing information\n        data_chunk = str(element) + node.get('response', '')\n        missing_data = analyze_missing_information(data_chunk, context_window)\n\n        # If missing data exists, format the prompt and use mini_task_completion for enrichment\n        prompt = generate_llm_prompt(data_chunk, list(context_window))\n        llm_response = mini_task_completion(prompt)\n        # Append the result for the current element to the documentation\n        collected_docs.append({\n            'element': element,\n            'response': llm_response,\n            'missing_data': missing_data,\n            'references': list(context_window),  # List of references to other elements\n            'context_window': list(context_window)\n        })\n        if missing_data:\n            # Update current context with missing data (enrich it)\n            current_context.update(missing_data)\n\n        # Optionally store source code or other node-related data\n        if 'source_code' not in node:\n            node['source_code'] = \"\"\n        node['source_code'] += '\\n' + element.get('source_code', '')\n\n    return collected_docs\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.auto_docs.extract_keywords_and_entities","title":"<code>extract_keywords_and_entities(text)</code>","text":"<p>Extracts important keywords, named entities, and concepts from a text.</p> Source code in <code>toolboxv2/mods/isaa/auto_docs.py</code> <pre><code>def extract_keywords_and_entities(text):\n    \"\"\"\n    Extracts important keywords, named entities, and concepts from a text.\n    \"\"\"\n    words = word_tokenize(text.lower())  # Tokenizing text\n    stop_words = set(stopwords.words('english'))\n    filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n    return filtered_words\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.auto_docs.generate_llm_prompt","title":"<code>generate_llm_prompt(data_chunk, context)</code>","text":"<p>Formats a prompt for an LLM based on the provided data and context.</p> Source code in <code>toolboxv2/mods/isaa/auto_docs.py</code> <pre><code>def generate_llm_prompt(data_chunk, context):\n    \"\"\"\n    Formats a prompt for an LLM based on the provided data and context.\n    \"\"\"\n    extract_keywords_and_entities(data_chunk)\n    context_str = \" \".join(context)\n    prompt = f\"Analyze the following codebase context: {context_str}. Then analyze this chunk of data: {data_chunk}. Identify key concepts, relations in the knowledge base.\"\n    return prompt\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.auto_docs.mini_task_completion_moc","title":"<code>mini_task_completion_moc(prompt)</code>","text":"<p>This is a mock function to simulate the behavior of an LLM completion process. In reality, this would interact with an actual LLM API (like OpenAI, etc.).</p> Source code in <code>toolboxv2/mods/isaa/auto_docs.py</code> <pre><code>def mini_task_completion_moc(prompt):\n    \"\"\"\n    This is a mock function to simulate the behavior of an LLM completion process.\n    In reality, this would interact with an actual LLM API (like OpenAI, etc.).\n    \"\"\"\n    return prompt\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base","title":"<code>base</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent","title":"<code>Agent</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent.agent","title":"<code>agent</code>","text":"<code>AgentModelData</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Configuration for the LLM model and API settings via LiteLLM.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>class AgentModelData(BaseModel):\n    \"\"\"Configuration for the LLM model and API settings via LiteLLM.\"\"\"\n    name: str | None = Field(default=None, description=\"Agent's internal name, often derived from builder.\")\n    model: str = Field(..., description=\"Primary LiteLLM model identifier (e.g., 'gemini/gemini-1.5-flash-latest', 'ollama/mistral').\")\n    provider: str | None = Field(default=None, description=\"LiteLLM provider override if needed.\")\n    system_message: str = Field(default=\"You are a helpful AI assistant.\", description=\"Base system prompt.\")\n\n    temperature: float | None = Field(default=None, ge=0.0, le=2.0) # Use LiteLLM defaults if None\n    top_k: int | None = Field(default=None, ge=1)\n    top_p: float | None = Field(default=None, ge=0.0, le=1.0)\n    max_tokens: int | None = Field(default=None, ge=1, description=\"Max tokens for LLM generation.\")\n    max_input_tokens: int | None = Field(default=None, ge=1, description=\"Max context window size (for trimming).\")\n\n    api_key: str | None = Field(default=None, description=\"API key (use env vars in production).\")\n    api_base: str | None = Field(default=None, description=\"API base URL (for local models/proxies).\")\n    api_version: str | None = Field(default=None, description=\"API version (e.g., Azure).\")\n\n    stop_sequence: list[str] | None = Field(default=None, alias=\"stop\") # Alias for LiteLLM\n    presence_penalty: float | None = Field(default=None)\n    frequency_penalty: float | None = Field(default=None)\n\n    user_id: str | None = Field(default=None, description=\"User identifier for LLM calls ('user' param).\")\n    budget_manager: BudgetManager | None = Field(default=None, description=\"LiteLLM BudgetManager instance.\")\n    caching: bool | None = Field(default=True, description=\"Enable/disable LiteLLM caching.\")\n\n    # Model config for Pydantic v2\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        extra='ignore', # Ignore extra fields from builder/ADK init\n        populate_by_name=True # Allow using 'stop' alias\n    )\n</code></pre> <code>EnhancedAgent</code> \u00b6 <p>               Bases: <code>*_AgentBaseClass</code></p> <p>Enhanced, production-oriented Unified Agent integrating LiteLLM, ADK, A2A, and MCP (via ADK).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>class EnhancedAgent(*_AgentBaseClass):\n    \"\"\"\n    Enhanced, production-oriented Unified Agent integrating LiteLLM, ADK, A2A, and MCP (via ADK).\n    \"\"\"\n    # --- Core Configuration ---\n    amd: AgentModelData # Primary model config\n    format_model: str | None = Field(default=None, description=\"Optional separate model for JSON formatting (a_format_class).\")\n    format_model_: str | None = Field(default=None, description=\"helper var for format_model\", exclude=True)\n    world_model: WorldModel = Field(default_factory=WorldModel)\n    verbose: bool = Field(default=False)\n    internal_state: InternalAgentState = Field(default=InternalAgentState.IDLE)\n\n    # --- LiteLLM Specific ---\n    stream: bool = Field(default=False, description=\"Whether LLM calls should stream chunks.\")\n    # Use a simple dict for history for now, can be replaced with persistent store interface\n    # Keyed by session_id\n    message_history: dict[str, list[dict[str, Any]]] = Field(default_factory=dict)\n    max_history_tokens: int | None = Field(default=None, description=\"Alternative to max_turns for history trimming based on token count.\")\n    max_history_turns: int = Field(default=20, description=\"Max conversation turns (user+assistant) for history.\") # Used if max_history_tokens is None\n    trim_strategy: Literal[\"litellm\", \"basic\"] = Field(default=\"litellm\")\n    total_cost: float = Field(default=0.0, description=\"Accumulated cost tracked via LiteLLM.\")\n\n    # --- Framework Components (Initialized via Builder/Setup) ---\n    # ADK\n    adk_runner: Runner | None = Field(default=None, description=\"ADK Runner instance if enabled.\")\n    adk_session_service: BaseSessionService | None = Field(default=None, description=\"ADK Session Service (often from runner).\")\n    sync_adk_state: bool = Field(default=True, description=\"Sync WorldModel with ADK Session.state.\")\n    # Exit stack to manage lifecycles of components like MCPToolset connections\n    # CRITICAL FIX: Use contextlib.AsyncExitStack type hint\n    adk_exit_stack: contextlib.AsyncExitStack | None = Field(default=None, description=\"AsyncExitStack for managing ADK toolset lifecycles.\")\n\n    # MCP Server (Agent acts AS an MCP Server)\n    mcp_server: FastMCP | None = Field(default=None, description=\"MCP server instance if agent exposes MCP capabilities.\")\n    # A2A Server (Agent acts AS an A2A Server)\n    a2a_server: A2AServer | None = Field(default=None, description=\"A2A server instance if agent exposes A2A capabilities.\")\n    # A2A Client (Agent acts AS an A2A Client)\n    a2a_clients: dict[str, A2AClient] = Field(default_factory=dict, description=\"Cached A2A clients for target agents.\")\n    a2a_client_lock: asyncio.Lock = Field(default_factory=asyncio.Lock, description=\"Lock for A2A client cache access.\")\n    a2a_poll_interval: float = Field(default=2.0, description=\"Polling interval for A2A task results (seconds).\")\n    a2a_poll_timeout: float = Field(default=60.0, description=\"Max time to wait for A2A task completion.\")\n\n    # --- Callbacks ---\n    stream_callback: Callable[[str], None | Awaitable[None]] | None = Field(default=None, description=\"Callback for each LLM stream chunk.\")\n    post_run_callback: Callable[[str, str, float], None | Awaitable[None]] | None = Field(default=None, description=\"Callback after a_run completes (session_id, final_response, turn_cost).\")\n    progress_callback: Callable[[Any], None | Awaitable[None]] | None = Field(default=None, description=\"Callback for progress updates (e.g., tool execution, A2A polling).\")\n    human_in_loop_callback: Callable[[dict], str | Awaitable[str]] | None = Field(default=None, description=\"Callback for HIL intervention points.\")\n\n    # --- Observability ---\n    tracer: Any | None = Field(default=None, description=\"OpenTelemetry Tracer instance.\") # Type hint depends on OTel setup\n\n    # --- Internal State ---\n    last_llm_result: Any | None = Field(default=None, description=\"Raw result from the last LiteLLM call.\")\n\n    # Model config\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        extra='ignore' # Critical for compatibility with ADK LlmAgent init\n    )\n\n    @model_validator(mode='after')\n    def _enhanced_agent_post_init(self) -&gt; 'EnhancedAgent':\n        \"\"\"\n        Performs initialization steps after Pydantic has validated fields.\n        \"\"\"\n        # --- (Existing post_init logic remains the same) ---\n        logger.setLevel(logging.DEBUG if self.verbose else logging.INFO)\n        os.environ['LITELLM_LOG'] = 'DEBUG' if self.verbose else 'NONE'\n        logger.debug(f\"Verbose logging {'enabled' if self.verbose else 'disabled'} for agent {self.amd.name}\")\n        self._setup_telemetry()\n        if ADK_AVAILABLE and isinstance(self, LlmAgent):\n            logger.debug(f\"Running post-init logic for ADK agent '{self.amd.name}'\")\n            self._ensure_internal_adk_tools() # Ensure tools are added *after* Pydantic init\n            if self.adk_runner and hasattr(self.adk_runner, 'session_service'):\n                self.adk_session_service = self.adk_runner.session_service\n                logger.debug(\"Associated ADK session service from runner.\")\n        if 'default' not in self.message_history:\n            self.message_history['default'] = []\n        logger.info(\n            f\"EnhancedAgent '{self.amd.name}' initialized. Model: {self.amd.model}. \"\n            f\"Capabilities: ADK({ADK_AVAILABLE}), A2A({A2A_AVAILABLE}), MCP({MCP_AVAILABLE})\"\n        )\n        self.model =  LiteLlm(model=self.amd.model)\n        return self\n\n    # --- ADK Post Init (Called automatically by Pydantic if method exists in base) ---\n    # This method name is expected by ADK's BaseModel integration.\n    # Pydantic v2 runs validators based on MRO, so if LlmAgent has this, it runs.\n    # We don't strictly need to define it here unless overriding LlmAgent's version.\n    # def model_post_init(self, __context: Any) -&gt; None:\n    #     \"\"\"ADK post-initialization (if inheriting from ADK BaseModel).\"\"\"\n    #     # Call super() if overriding LlmAgent's method\n    #     # super().model_post_init(__context) # If LlmAgent has this method\n    #     logger.debug(f\"ADK model_post_init for Agent '{self.amd.name}' (EnhancedAgent)\")\n    #     # Add post-init logic specific to ADK features here, AFTER ADK's own init\n    #     self._ensure_internal_adk_tools()\n    #     if self.adk_runner:\n    #         self.adk_session_service = self.adk_runner.session_service\n\n\n    # --- Telemetry Setup ---\n    def _setup_telemetry(self):\n        \"\"\"Initializes the OpenTelemetry tracer.\"\"\"\n        if OTEL_AVAILABLE and not self.tracer:\n            # Get tracer from global provider (needs to be configured elsewhere)\n            # In a real app, you'd configure the TracerProvider with exporters\n            # provider = TracerProvider() # Example: basic provider\n            # provider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter())) # Example: console output\n            # trace.set_tracer_provider(provider)\n            self.tracer = trace.get_tracer(\"enhanced_agent\", \"0.1.0\")\n            logger.info(\"OpenTelemetry tracer initialized.\")\n        elif not OTEL_AVAILABLE:\n            self.tracer = DummyTracer() # Use NoOp tracer if OTel not installed\n            logger.debug(\"OpenTelemetry not available, using NoOp tracer.\")\n\n\n    # --- Setup Methods (Called by Builder) ---\n\n    def setup_mcp_server(self, host=\"0.0.0.0\", port=8000, **mcp_kwargs):\n        \"\"\"Initialize and configure the MCP server capabilities *for this agent*.\n           This agent will ACT AS an MCP Server.\n        \"\"\"\n        if not MCP_AVAILABLE:\n            logger.warning(\"MCP library not installed. Cannot setup MCP server.\")\n            return None\n        if self.mcp_server:\n            logger.warning(\"MCP server already initialized.\")\n            return self.mcp_server\n        name = mcp_kwargs.get(\"name\")\n        del mcp_kwargs[\"name\"]\n        self.mcp_server = FastMCP(name=name or f\"{self.amd.name}-mcp-server\",\n                                  description=f\"MCP interface for EnhancedAgent {self.amd.name}\",\n                                  **mcp_kwargs)\n        logger.info(f\"Setting up MCP server for agent '{self.amd.name}' on {host}:{port}\")\n\n        # --- Register Agent's core functionalities as MCP services ---\n        # Example: Expose World Model (Read-only for safety)\n        @self.mcp_server.resource(f\"agent://{self.amd.name}/world_model\")\n        def mcp_get_world_model_resource() -&gt; dict[str, Any]:\n            \"\"\"Gets the agent's world model.\"\"\"\n            logger.debug(f\"[MCP Resource] agent://{self.amd.name}/world_model accessed\")\n            return self.world_model.to_dict()\n\n        # Example: Expose a simple query tool via MCP\n        @self.mcp_server.tool(name=\"simple_llm_query\")\n        async def mcp_simple_query(prompt: str) -&gt; str:\n            \"\"\"Sends a simple prompt to the agent's LLM (non-persistent run).\"\"\"\n            logger.debug(f\"[MCP Tool] simple_llm_query called: {prompt[:50]}...\")\n            # Use a minimal, non-persistent run, disable recursive calls\n            response = await self.a_run(\n                prompt, session_id=f\"mcp_query_{uuid.uuid4()}\",\n                persist_history=False, strategy_override=ProcessingStrategy.DIRECT_LLM\n            )\n            return response\n\n        # If ADK tools exist, potentially expose them via MCP automatically?\n        if ADK_AVAILABLE and isinstance(self, LlmAgent) and self.tools:\n             logger.info(\"Attempting to expose ADK tools via MCP server...\")\n             for adk_tool in self.tools:\n                 if adk_tool.name in [\"code_execution\", \"adk_tool_a2a_send_and_wait\", \"adk_tool_a2a_send_no_wait\", \"adk_tool_a2a_get_task_status\", \"adk_tool_a2a_cancel_task\"]:\n                     continue\n                 if not isinstance(adk_tool, BaseTool): continue\n                 try:\n                     mcp_schema = adk_to_mcp_tool_type(adk_tool)\n\n                     # Define the MCP tool handler dynamically\n                     async def mcp_tool_handler(tool_name=adk_tool.name, **kwargs):\n                         logger.info(f\"[MCP Tool via ADK] Calling {tool_name} with {kwargs}\")\n                         # ADK tools expect ToolContext, which we don't have here.\n                         # We might need to simulate it or adapt the tool execution.\n                         # This simple version calls the tool's underlying function if possible.\n                         # WARNING: This bypasses ADK's standard tool execution flow.\n                         if hasattr(adk_tool, 'func') and callable(adk_tool.func):\n                             # This assumes the function doesn't need ToolContext\n                             result = await adk_tool.func(**kwargs)\n                             # Convert result to MCP content (e.g., TextContent)\n                             if isinstance(result, str):\n                                 return [mcp_types.TextContent(type=\"text\", text=result)]\n                             else:\n                                 try:\n                                     return [mcp_types.TextContent(type=\"text\", text=json.dumps(result))]\n                                 except:\n                                     return [mcp_types.TextContent(type=\"text\", text=str(result))]\n                         else:\n                             logger.warning(f\"Cannot directly call ADK tool {tool_name} via MCP.\")\n                             return [mcp_types.TextContent(type=\"text\", text=f\"Error: Cannot execute ADK tool {tool_name} directly.\")]\n\n                     # Register the dynamic handler with the MCP server\n                     self.mcp_server.tool(name=mcp_schema.name)(mcp_tool_handler)\n                     logger.info(f\"Exposed ADK tool '{adk_tool.name}' as MCP tool '{mcp_schema.name}'.\")\n\n                 except Exception as e:\n                     logger.warning(f\"Failed to expose ADK tool '{adk_tool.name}' via MCP: {e}\")\n\n\n        logger.info(f\"MCP server setup complete for agent '{self.amd.name}'. Run `agent.run_mcp_server()` to start.\")\n        return self.mcp_server\n\n    def run_mcp_server(self, transport='sse', **kwargs):\n        \"\"\"Starts the MCP server (blocking).\"\"\"\n        if not self.mcp_server:\n            logger.error(\"MCP server not initialized. Call setup_mcp_server first.\")\n            return\n        if not MCP_AVAILABLE:\n             logger.error(\"MCP library not available. Cannot run MCP server.\")\n             return\n        logger.info(f\"Starting MCP server for agent '{self.amd.name}' using {transport} transport...\")\n        # This is blocking, run in a separate process/thread for a long-running agent\n        try:\n            self.mcp_server.run(transport=transport, **kwargs)\n        except Exception as e:\n            logger.error(f\"MCP server failed to run: {e}\", exc_info=True)\n\n    # MCP Client Setup is now handled by ADK's MCPToolset via the Builder\n\n\n    def setup_a2a_server(self, host=\"0.0.0.0\", port=5000, **a2a_server_options):\n        \"\"\"\n        Initialize and configure the A2A server capabilities using python-a2a.\n        This dynamically creates a server class with the agent's capabilities.\n        \"\"\"\n        if not A2A_AVAILABLE:\n            logger.warning(\"python-a2a library not installed. Cannot setup A2A server.\")\n            return None\n        if self.a2a_server:\n            logger.warning(\"A2A server already initialized.\")\n            return self.a2a_server\n\n        logger.info(f\"Setting up A2A server for agent '{self.amd.name}' on {host}:{port}\")\n\n        agent_instance = self # Reference to the current EnhancedAgent instance\n\n        # Define the A2A Server class dynamically using the decorator\n        @a2a_agent_decorator(\n            name=self.amd.name or \"EnhancedAgent\",\n            description=f\"Enhanced Agent '{self.amd.name}' - Capabilities: ADK({ADK_AVAILABLE}), MCP({MCP_AVAILABLE}), A2A({A2A_AVAILABLE})\",\n            version=\"1.0.0\",\n            # Other AgentCard fields...\n        )\n        class DynamicA2AServer(A2AServer):\n            bound_agent: EnhancedAgent = agent_instance\n\n            def handle_task(self, task: Task) -&gt; Task:\n                \"\"\" Handles incoming A2A tasks by calling the EnhancedAgent's async logic. \"\"\"\n                # --- (handle_task implementation remains the same as before) ---\n                logger.info(f\"[A2A Server {self.bound_agent.amd.name}] Received task: {task.id}\")\n                async def run_agent_async():\n                    # ... (logic to extract prompt, call a_run, update task) ...\n                    try:\n                        user_prompt = \"\"\n                        # ... (extract user_prompt from task.message) ...\n                        if task.message and task.message.get(\"content\"):\n                            content = task.message[\"content\"]\n                            if isinstance(content, dict) and content.get(\"type\") == \"text\":\n                                user_prompt = content.get(\"text\", \"\").strip()\n                            elif isinstance(content, str):\n                                user_prompt = content.strip()\n\n                        if not user_prompt:\n                            raise ValueError(\"Task message has no text content.\")\n\n                        session_id = task.message.get(\"session_id\", task.id)\n                        agent_response = await self.bound_agent.a_run(\n                            user_prompt,\n                            session_id=session_id,\n                            persist_history=False,\n                            a2a_task_id=task.id\n                        )\n                        task.artifacts = [{\"parts\": [{\"type\": \"text\", \"text\": str(agent_response)}]}]\n                        task.status = TaskStatus(state=TaskState.COMPLETED)\n                    except Exception as e:\n                        # ... (error handling) ...\n                        logger.error(f\"[A2A Task {task.id}] Error during processing: {e}\", exc_info=True)\n                        error_msg = f\"Internal agent error: {str(e)}\"\n                        task.artifacts = [{\"parts\": [{\"type\": \"text\", \"text\": error_msg}]}]\n                        task.status = TaskStatus(state=TaskState.FAILED, message={\"role\": \"agent\", \"content\": {\"type\": \"text\", \"text\": error_msg}})\n                    return task\n                try:\n                    updated_task = asyncio.run(run_agent_async())\n                    return updated_task\n                except RuntimeError as e:\n                    # ... (handle RuntimeError) ...\n                    logger.error(f\"RuntimeError calling asyncio.run in handle_task: {e}.\")\n                    task.status = TaskStatus(state=TaskState.FAILED, message={\"role\": \"agent\", \"content\": {\"type\": \"text\", \"text\": \"Internal Server Error processing task asynchronously.\"}})\n                    return task\n                # --- (end of handle_task logic) ---\n\n\n            # --- Expose Skills ---\n            @a2a_skill_decorator(\n                name=\"General Query\",\n                description=\"Process general natural language queries using the agent's primary LLM.\",\n                examples=[\"What is the capital of France?\", \"Summarize the plot of Hamlet.\"]\n            )\n            def general_query_skill(self, query: str) -&gt; str:\n                \"\"\"Handles general queries via the skill mechanism by calling a_run.\"\"\"\n                logger.info(f\"[A2A Skill] Received general_query: {query[:50]}...\")\n                async def run_skill_async():\n                    # Call a_run, forcing direct LLM strategy for simple queries\n                    response = await self.bound_agent.a_run(\n                        query,\n                        a2a_task_id=f\"skill_query_{uuid.uuid4()}\",\n                        strategy_override=ProcessingStrategy.DIRECT_LLM,\n                        persist_history=False\n                        )\n                    return response\n                try:\n                    # Bridge sync skill call to async agent logic\n                    return asyncio.run(run_skill_async())\n                except RuntimeError:\n                     logger.error(\"RuntimeError calling asyncio.run in general_query_skill.\")\n                     return \"Error: Could not process skill asynchronously.\"\n\n            # --- FIXED: Generic Skill for ADK Tools ---\n            if ADK_AVAILABLE and isinstance(agent_instance, LlmAgent) and agent_instance.tools:\n                # Check if there are any ADK tools to expose\n                adk_tool_list = [t for t in agent_instance.tools if isinstance(t, BaseTool)]\n                if adk_tool_list:\n                    logger.info(f\"Exposing {len(adk_tool_list)} ADK tools via 'execute_adk_tool' A2A skill.\")\n\n                    @a2a_skill_decorator(\n                        name=\"execute_adk_tool\",\n                        description=f\"Executes a registered ADK tool. Available tools: {', '.join([t.name for t in adk_tool_list])}\",\n                        examples=[\"Execute tool 'some_tool_name' with argument 'arg1'='value1'\"] # Generic example\n                    )\n                    def execute_adk_tool_skill(self, tool_name: str, arguments: dict[str, Any]) -&gt; str:\n                        \"\"\"Generic skill to execute an ADK tool by name with arguments.\"\"\"\n                        logger.info(f\"[A2A Skill] Request to execute ADK tool: {tool_name} with args: {arguments}\")\n\n                        # Find the ADK tool instance on the bound agent\n                        tool_to_call: BaseTool | None = None\n                        for tool in self.bound_agent.tools:\n                            if isinstance(tool, BaseTool) and tool.name == tool_name:\n                                tool_to_call = tool\n                                break\n\n                        if not tool_to_call:\n                            logger.warning(f\"[A2A Skill] ADK tool '{tool_name}' not found.\")\n                            return f\"Error: ADK tool '{tool_name}' not found on this agent.\"\n\n                        # --- Bridge sync skill call to async ADK tool execution ---\n                        async def run_adk_tool_async():\n                            try:\n                                # ADK tools require ToolContext. We can provide a minimal one or None.\n                                # Providing None might limit tool functionality.\n                                # Let's try providing None for simplicity first.\n                                adk_tool_context = None\n\n                                # Check if the tool has an async run method (most ADK tools should)\n                                if hasattr(tool_to_call, 'run_async') and iscoroutinefunction(tool_to_call.run_async):\n                                    # Pass arguments directly to run_async\n                                    result = await tool_to_call.run_async(args=arguments, tool_context=adk_tool_context)\n                                    # Convert result to string for A2A response\n                                    if isinstance(result, str): return result\n                                    try: return json.dumps(result)\n                                    except: return str(result)\n                                elif hasattr(tool_to_call, 'run') and callable(tool_to_call.run):\n                                    # Fallback to synchronous run in thread pool\n                                    logger.warning(f\"ADK tool '{tool_name}' has no run_async, using synchronous run in thread.\")\n                                    result = await asyncio.to_thread(tool_to_call.run, args=arguments, tool_context=adk_tool_context)\n                                    if isinstance(result, str): return result\n                                    try: return json.dumps(result)\n                                    except: return str(result)\n                                else:\n                                     return f\"Error: ADK tool '{tool_name}' has no callable run or run_async method.\"\n\n                            except Exception as e:\n                                logger.error(f\"[A2A Skill] Error executing ADK tool '{tool_name}': {e}\", exc_info=True)\n                                return f\"Error executing ADK tool {tool_name}: {e}\"\n\n                        # Execute the async tool runner\n                        try:\n                            return asyncio.run(run_adk_tool_async())\n                        except RuntimeError:\n                            logger.error(f\"RuntimeError calling asyncio.run in execute_adk_tool_skill for tool {tool_name}.\")\n                            return \"Error: Could not execute ADK tool asynchronously.\"\n\n            # --- End of Skill Definitions ---\n\n        # Instantiate the dynamic server class\n        try:\n             self.a2a_server = DynamicA2AServer(**a2a_server_options)\n             logger.info(f\"A2A server instance created for agent '{self.amd.name}'.\")\n             return self.a2a_server\n        except Exception as e:\n             logger.error(f\"Failed to instantiate dynamic A2A Server: {e}\", exc_info=True)\n             return None\n\n\n    def run_a2a_server(self, host=\"0.0.0.0\", port=5000, **kwargs):\n        \"\"\"Starts the A2A server (blocking) using the python-a2a run_server function.\"\"\"\n        if not self.a2a_server:\n            logger.error(\"A2A server not initialized. Call setup_a2a_server first.\")\n            return\n        if not A2A_AVAILABLE:\n            logger.error(\"python-a2a library not available. Cannot run A2A server.\")\n            return\n\n        # Get effective host/port from server instance if set, otherwise use args\n        effective_host = getattr(self.a2a_server, 'host', host)\n        effective_port = getattr(self.a2a_server, 'port', port)\n\n        logger.info(f\"Starting A2A server for agent '{self.amd.name}' via run_server_func on {effective_host}:{effective_port}...\")\n        try:\n            # Call the imported run_server function, passing the agent instance\n            run_a2a_server_func(self.a2a_server, host=effective_host, port=effective_port, **kwargs) # This blocks\n        except Exception as e:\n            logger.error(f\"A2A server failed to run: {e}\", exc_info=True)\n\n    async def setup_a2a_client(self, target_agent_url: str) -&gt; A2AClient | None:\n        \"\"\"Gets or creates an A2A client for a specific target agent URL using python-a2a.\"\"\"\n        if not A2A_AVAILABLE:\n            logger.warning(\"python-a2a library not installed. Cannot setup A2A client.\")\n            return None\n\n        async with self.a2a_client_lock:\n            if target_agent_url in self.a2a_clients:\n                logger.debug(f\"Reusing cached A2A client for {target_agent_url}\")\n                return self.a2a_clients[target_agent_url]\n\n            logger.info(f\"Setting up A2A client for target: {target_agent_url}\")\n            try:\n                # python-a2a client likely fetches card on init or first call\n                client = A2AClient(base_url=target_agent_url) # Pass the URL directly\n                # Verify connection implicitly by getting card (optional, client might do lazy loading)\n                # agent_card = await client.get_agent_card() # If method exists\n                # logger.info(f\"Successfully connected A2A client to agent: {agent_card.name}\")\n                self.a2a_clients[target_agent_url] = client\n                logger.info(f\"A2A client created for target: {target_agent_url}\")\n                return client\n            except Exception as e:\n                logger.error(f\"Failed to setup A2A client for {target_agent_url}: {e}\", exc_info=True)\n                return None\n\n    async def close_a2a_clients(self):\n        \"\"\"Closes all cached A2A client connections.\"\"\"\n        async with self.a2a_client_lock:\n            logger.info(f\"Closing {len(self.a2a_clients)} A2A clients.\")\n            # A2AClient may manage underlying httpx clients automatically.\n            # If explicit close needed in future versions, add here.\n            # for client in self.a2a_clients.values():\n            #     await client.close() # If available\n            self.a2a_clients.clear()\n\n    def setup_adk_runner(self, runner_options: dict[str, Any] | None = None):\n        \"\"\"Initializes an ADK runner for this agent (if ADK enabled).\"\"\"\n        if not ADK_AVAILABLE:\n            logger.warning(\"ADK not available. Cannot setup ADK runner.\")\n            return None\n        if not isinstance(self, LlmAgent):\n            logger.error(\"Agent must inherit from LlmAgent to use ADK runner directly.\")\n            return None\n        if self.adk_runner:\n            logger.warning(\"ADK runner already initialized.\")\n            return self.adk_runner\n\n        runner_opts = runner_options or {}\n        runner_class = runner_opts.pop(\"runner_class\", InMemoryRunner) # Default to InMemory\n        app_name = runner_opts.pop(\"app_name\", f\"{self.amd.name}_ADKApp\")\n\n        if runner_class == InMemoryRunner:\n            runner_opts = {}\n\n        logger.info(f\"Setting up ADK Runner ({runner_class.__name__}) for app '{app_name}'...\")\n\n        try:\n             # Pass the agent instance and other options to the runner constructor\n            self.adk_runner = runner_class(agent=self, app_name=app_name, **runner_opts)\n            self.adk_session_service = self.adk_runner.session_service # Store session service\n            logger.info(f\"ADK {runner_class.__name__} setup complete for agent '{self.amd.name}'.\")\n            return self.adk_runner\n        except Exception as e:\n            logger.error(f\"Failed to setup ADK runner: {e}\", exc_info=True)\n            self.adk_runner = None\n            self.adk_session_service = None\n            return None\n\n\n    # --- Core Agent Logic (`a_run`) ---\n\n    async def a_run(self,\n                    user_input: str,\n                    session_id: str | None = None,\n                    persist_history: bool = True,\n                    strategy_override: ProcessingStrategy | None = None,\n                    kwargs_override: dict[str, Any] | None = None, # For fine-grained control\n                    a2a_task_id: str | None = None # Context if called from A2A task\n                    ) -&gt; str:\n        \"\"\"\n        Main asynchronous execution logic for the agent turn.\n\n        Orchestrates world model updates, state sync, strategy selection,\n        execution, cost tracking, and callbacks.\n        \"\"\"\n        self.internal_state = InternalAgentState.PROCESSING\n        start_time = time.monotonic()\n        session_id = session_id or \"default\" # Use 'default' if none provided\n        response = \"Error: Processing failed.\" # Default error\n        turn_cost = 0.0\n        span = None # OTel span\n\n        if not self.tracer: self._setup_telemetry() # Ensure tracer exists\n\n        try:\n            with self.tracer.start_as_current_span(f\"Agent Run: {self.amd.name}\", attributes={\"session_id\": session_id}) as span:\n\n                # Ensure session history list exists\n                if session_id not in self.message_history:\n                    logger.debug(f\"Initializing history for session: {session_id}\")\n                    self.message_history[session_id] = []\n\n                logger.info(f\"--- Agent Run Start (Session: {session_id}) ---\")\n                span.add_event(\"Agent run started\")\n                logger.info(f\"User Input: {user_input[:100]}...\")\n                span.set_attribute(\"user_input\", user_input[:500]) # Log truncated input\n\n                # 0. Get ADK Session State (if ADK enabled and syncing)\n                adk_session_state = None\n                if self.sync_adk_state and self.adk_session_service:\n                    try:\n                        # ADK SessionService methods are typically synchronous\n                        # Run in threadpool to avoid blocking\n                        adk_session = await asyncio.to_thread(\n                             self.adk_session_service.get_session,\n                             app_name=self.adk_runner.app_name, # Assuming runner is set if syncing\n                             user_id=self.amd.user_id or \"adk_user\", # Needs consistent user ID\n                             session_id=session_id\n                        )\n                        if adk_session:\n                            adk_session_state = adk_session.state\n                        else:\n                            logger.warning(f\"ADK Session '{session_id}' not found for state sync.\")\n                            # Optionally create session here? Be careful about race conditions.\n                    except Exception as sync_e:\n                        logger.error(f\"Error getting ADK session state for sync: {sync_e}\")\n\n                # 1. Update World Model &amp; Sync State (Run *before* strategy selection)\n                # flow_world_model is now responsible for syncing *from* ADK state initially\n                await self.flow_world_model(user_input, session_id, adk_session_state)\n                span.add_event(\"World model updated\")\n\n                # 2. Prepare message history for this turn\n                current_turn_messages = self._prepare_llm_messages(user_input, session_id)\n                span.set_attribute(\"history_length\", len(current_turn_messages) -1) # Exclude current input\n\n                # 3. Determine Processing Strategy\n                if strategy_override:\n                    strategy = strategy_override\n                    strategy_reasoning = \"Strategy overridden by caller.\"\n                    logger.info(f\"Strategy forced by override: {strategy.value}\")\n                else:\n                    strategy, strategy_reasoning = self._determine_strategy_heuristic(user_input, current_turn_messages)\n                    logger.info(f\"Strategy Selected: {strategy.value} (Reason: {strategy_reasoning})\")\n                span.set_attribute(\"selected_strategy\", strategy.value)\n                span.set_attribute(\"strategy_reasoning\", strategy_reasoning)\n\n\n                # --- Prepare kwargs for execution based on strategy ---\n                exec_kwargs = kwargs_override or {}\n                exec_kwargs['session_id'] = session_id\n                exec_kwargs['user_input'] = user_input\n                exec_kwargs['current_turn_messages'] = current_turn_messages\n                exec_kwargs['adk_session_state'] = adk_session_state # Pass state for potential use/update\n\n\n                # 4. Execute Selected Strategy\n                logger.info(f\"Executing strategy: {strategy.value}\")\n                if strategy == ProcessingStrategy.ADK_RUN:\n                    if ADK_AVAILABLE and self.adk_runner:\n                        response = await self._execute_adk_run(**exec_kwargs)\n                    else:\n                        logger.warning(\"ADK_RUN strategy selected, but ADK runner not available/configured. Falling back.\")\n                        # Fallback strategy? Maybe DIRECT_LLM?\n                        strategy = ProcessingStrategy.DIRECT_LLM\n                        response = await self._execute_direct_llm(**exec_kwargs)\n\n                elif strategy == ProcessingStrategy.A2A_CALL:\n                    if A2A_AVAILABLE:\n                        response = await self._execute_a2a_call(**exec_kwargs)\n                    else:\n                        logger.warning(\"A2A_CALL strategy selected, but A2A not available. Falling back.\")\n                        strategy = ProcessingStrategy.DIRECT_LLM\n                        response = await self._execute_direct_llm(**exec_kwargs)\n\n                else: # Default: DIRECT_LLM\n                    response = await self._execute_direct_llm(**exec_kwargs)\n\n                span.set_attribute(\"raw_response_length\", len(response))\n                span.add_event(\"Strategy execution complete\")\n\n                # 5. Persist History (if successful and enabled)\n                # Add assistant response to history\n                if persist_history and not response.startswith(\"Error:\"):\n                     self._add_to_history(session_id, LLMMessage(role=\"assistant\", content=response).to_dict())\n\n                # 6. Sync World Model *back* to ADK State (if changed and enabled)\n                if self.sync_adk_state and adk_session_state is not None:\n                    try:\n                        self.world_model.sync_to_adk_state(adk_session_state)\n                        span.add_event(\"ADK state synchronized and updated\")\n                    except Exception as sync_e:\n                         logger.error(f\"Error syncing/updating ADK session state: {sync_e}\")\n                         span.record_exception(sync_e)\n\n                # 7. Track Cost (using last_llm_result if available)\n                if self.last_llm_result:\n                    try:\n                        cost = completion_cost(completion_response=self.last_llm_result, model=self.amd.model)\n                        if cost:\n                            turn_cost = cost\n                            self.total_cost += turn_cost\n                            logger.info(f\"Turn Cost: ${turn_cost:.6f}, Total Cost: ${self.total_cost:.6f}\")\n                            span.set_attribute(\"llm_cost\", turn_cost)\n                            span.set_attribute(\"total_agent_cost\", self.total_cost)\n                        self.last_llm_result = None # Clear after use\n                    except Exception as cost_e:\n                        logger.warning(f\"Failed to calculate cost: {cost_e}\")\n                        span.add_event(\"Cost calculation failed\", attributes={\"error\": str(cost_e)})\n\n\n                # 8. Run Post Callback\n                if self.post_run_callback and not response.startswith(\"Error:\"):\n                    try:\n                        if iscoroutinefunction(self.post_run_callback):\n                            await self.post_run_callback(session_id, response, turn_cost)\n                        else:\n                            self.post_run_callback(session_id, response, turn_cost)\n                        span.add_event(\"Post-run callback executed\")\n                    except Exception as cb_e:\n                        logger.error(f\"Post-run callback failed: {cb_e}\", exc_info=True)\n                        span.record_exception(cb_e)\n\n\n                logger.info(f\"Agent Run finished in {time.monotonic() - start_time:.2f}s. Response: {response[:100]}...\")\n\n        except Exception as e:\n            logger.error(f\"Error during agent run (Session: {session_id}): {e}\", exc_info=True)\n            self.internal_state = InternalAgentState.ERROR\n            response = f\"Error: An internal error occurred during processing: {str(e)}\"\n            if span:\n                 span.set_status(trace.Status(trace.StatusCode.ERROR, f\"Agent run failed: {e}\"))\n                 span.record_exception(e)\n        finally:\n            self.internal_state = InternalAgentState.IDLE\n            if span: span.end() # Ensure span is closed\n            logger.info(f\"--- Agent Run End (Session: {session_id}) ---\")\n\n        return str(response) # Ensure string output\n\n    def run(self, user_input: str, session_id: str | None = None, **kwargs) -&gt; str:\n        \"\"\"Synchronous wrapper for a_run.\"\"\"\n        try:\n            # get_event_loop() is deprecated in 3.10+, use get_running_loop() or new_event_loop()\n            try:\n                asyncio.get_running_loop()\n                # If loop is running, cannot use asyncio.run. Need to schedule and wait.\n                # This is complex to get right universally (e.g., in notebooks vs servers).\n                # Simplest approach for sync call from sync context is asyncio.run()\n                # If called from async context, user should await a_run() directly.\n                logger.warning(\"Synchronous 'run' called from a running event loop. \"\n                               \"This might block the loop. Consider using 'await a_run'.\")\n                # Fallback to basic run, may error if loop is running\n                return asyncio.run(self.a_run(user_input, session_id=session_id, **kwargs))\n            except RuntimeError: # No running event loop\n                 return asyncio.run(self.a_run(user_input, session_id=session_id, **kwargs))\n        except Exception as e:\n            logger.error(f\"Error in synchronous run wrapper: {e}\", exc_info=True)\n            return f\"Error: Failed to execute synchronous run: {e}\"\n\n    # --- Strategy Determination ---\n\n    def _determine_strategy_heuristic(self, user_input: str, messages: list[dict]) -&gt; tuple[ProcessingStrategy, str]:\n        \"\"\"Determines the processing strategy using heuristics (faster than LLM).\"\"\"\n        # 1. Check for keywords indicating specific needs\n        input_lower = user_input.lower()\n        # Example Keywords:\n        code_keywords = {\"execute\", \"run code\", \"python\", \"calculate\", \"script\"}\n        search_keywords = {\"search\", \"google\", \"find information\", \"what is\", \"who is\"}\n        agent_keywords = {\"ask agent\", \"tell agent\", \"delegate to\"} # Keywords for A2A/MCP delegation\n        tool_keywords = {\"use tool\", \"run tool\"} # Keywords for specific tool use\n\n        # 2. Check Agent Capabilities (Tools, Servers, Clients)\n        has_adk_tools = ADK_AVAILABLE and isinstance(self, LlmAgent) and bool(self.tools)\n        has_adk_code_executor = ADK_AVAILABLE and isinstance(self, LlmAgent) and self.code_executor is not None\n        can_do_adk_search = any(isinstance(t, type(adk_google_search) | AdkVertexAiSearchTool) for t in getattr(self, 'tools', []))\n        can_do_a2a = A2A_AVAILABLE and bool(self.a2a_clients) # Check if clients configured\n        # MCP check relies on tools being added via MCPToolset in ADK\n        has_adk_tools and any(isinstance(t, BaseTool) and getattr(t, '_is_mcp_tool', False) for t in self.tools) # Heuristic\n\n\n        # --- Strategy Logic ---\n        # Priority: ADK (if tools/code/search needed) &gt; A2A (if delegation requested) &gt; Direct LLM\n\n        # ADK: If code execution or search is explicitly requested or implied, or specific ADK tools mentioned\n        if ADK_AVAILABLE and self.adk_runner:\n            if has_adk_code_executor and any(kw in input_lower for kw in code_keywords):\n                return ProcessingStrategy.ADK_RUN, \"Input suggests code execution, using ADK.\"\n            if can_do_adk_search and any(kw in input_lower for kw in search_keywords):\n                 return ProcessingStrategy.ADK_RUN, \"Input suggests web/data search, using ADK.\"\n            # Check if input mentions names of specific ADK tools\n            if has_adk_tools:\n                tool_names = {t.name.lower() for t in self.tools}\n                if any(f\" {name} \" in input_lower for name in tool_names) or any(kw in input_lower for kw in tool_keywords):\n                     return ProcessingStrategy.ADK_RUN, \"Input mentions specific ADK tool or requests tool use.\"\n            # General ADK case: If ADK is primary mode and input isn't trivial\n            if len(user_input.split()) &gt; 5: # Simple heuristic for non-trivial input\n                # If ADK tools exist, assume ADK might be needed for planning\n                if has_adk_tools:\n                    return ProcessingStrategy.ADK_RUN, \"Complex input and ADK tools available, using ADK planning.\"\n                # If only basic LLM agent, still might use ADK runner for session mgmt? Check config.\n                # Defaulting to DIRECT_LLM if no specific ADK features seem required.\n\n        # A2A: If delegation is requested and A2A clients are available\n        if can_do_a2a and any(kw in input_lower for kw in agent_keywords):\n             # : Could use LLM here to extract target agent if multiple clients exist\n            return ProcessingStrategy.A2A_CALL, \"Input suggests delegating to another agent.\"\n\n        # Fallback: Direct LLM\n        return ProcessingStrategy.DIRECT_LLM, \"Input seems suitable for direct LLM processing.\"\n\n\n    # --- Strategy Execution Helpers ---\n\n    def _prepare_llm_messages(self, user_input: str, session_id: str) -&gt; list[dict]:\n        \"\"\"Prepares the list of messages for the LLM call, including history and system prompts.\"\"\"\n        session_history = self.message_history.get(session_id, [])\n\n        # Construct message list\n        messages: list[dict] = []\n        messages.extend(self.construct_initial_prompts()) # System/world model/tool prompts\n        # Add history (ensure alternating roles if possible, handle potential issues)\n        messages.extend(session_history)\n        # Add current user input\n        messages.append(LLMMessage(role=\"user\", content=user_input).to_dict())\n\n        # Trim messages based on token count or turn limit\n        trimmed_messages = self._trim_messages(messages)\n\n        # Add user input to persistent history *before* LLM call\n        # Note: assistant response added *after* successful call in a_run\n        self._add_to_history(session_id, LLMMessage(role=\"user\", content=user_input).to_dict())\n\n        return trimmed_messages\n\n    async def _execute_direct_llm(self, current_turn_messages: list[dict], session_id: str, **kwargs) -&gt; str:\n        \"\"\"Executes a direct call to the LLM using LiteLLM.\"\"\"\n        logger.debug(\"Executing direct LLM call...\")\n        if not current_turn_messages: return \"Error: No messages prepared for LLM.\"\n        try:\n            response_content = await self.a_run_llm_completion(current_turn_messages)\n            return response_content\n        except Exception as e:\n            logger.error(f\"Direct LLM execution failed: {e}\", exc_info=True)\n            return f\"Error during LLM generation: {e}\"\n\n    async def _execute_adk_run(self, user_input: str, session_id: str, adk_session_state: State | None, **kwargs) -&gt; str:\n        \"\"\"Executes the agent's logic using the configured ADK runner.\"\"\"\n        if not self.adk_runner or not self.adk_session_service:\n            return \"Error: ADK Runner or Session Service is not configured for this agent.\"\n\n        logger.debug(f\"Executing ADK run for session {session_id}...\")\n        final_response_text = \"Error: ADK processing did not yield a final textual response.\"\n        # Use user_id from AMD if available, default otherwise\n        user_id = self.amd.user_id or \"adk_user\"\n        app_name = self.adk_runner.app_name\n\n        try:\n            # 1. Ensure ADK session exists\n            try:\n                # Check and potentially create session (synchronous, run in thread)\n                session_exists = await asyncio.to_thread(\n                    self.adk_session_service.get_session, app_name=app_name, user_id=user_id, session_id=session_id\n                )\n                if not session_exists:\n                     logger.info(f\"Creating ADK session {session_id} for user {user_id} in app {app_name}\")\n                     # Pass initial state from World Model if syncing\n                     initial_state = self.world_model.to_dict() if self.sync_adk_state else {}\n                     await asyncio.to_thread(\n                         self.adk_session_service.create_session,\n                         app_name=app_name, user_id=user_id, session_id=session_id,\n                         state=initial_state\n                     )\n                elif adk_session_state is None and self.sync_adk_state:\n                    # If session existed but we couldn't get state earlier, try again\n                     session = await asyncio.to_thread(self.adk_session_service.get_session, app_name=app_name, user_id=user_id, session_id=session_id)\n                     if session: adk_session_state = session.state\n\n            except Exception as session_e:\n                logger.error(f\"Failed to ensure ADK session {session_id}: {session_e}\", exc_info=True)\n                return f\"Error setting up ADK session: {session_e}\"\n\n            # 2. Prepare ADK input (handle multi-modal later)\n            # Assuming user_input is text for now\n            adk_input_content = Content(role='user', parts=[Part(text=user_input)])\n\n            # 3. Execute ADK run_async\n            all_events_str = [] # For logging/debugging\n            async for event in self.adk_runner.run_async(\n                user_id=user_id, session_id=session_id, new_message=adk_input_content):\n\n                # Log event details (optional, can be verbose)\n                try:\n                    event_dict = event.model_dump(exclude_none=True)\n                    all_events_str.append(json.dumps(event_dict, default=str)) # Serialize complex types\n                    logger.debug(f\"ADK Event ({event.author}): {all_events_str[-1]}\")\n                except Exception as log_e:\n                    logger.debug(f\"ADK Event ({event.author}): [Error logging event details: {log_e}]\")\n\n                # Call progress callback\n                if self.progress_callback:\n                     try:\n                         progress_data = {\"type\": \"adk_event\", \"event\": event.model_dump(exclude_none=True)}\n                         if iscoroutinefunction(self.progress_callback): await self.progress_callback(progress_data)\n                         else: self.progress_callback(progress_data)\n                     except Exception as cb_e: logger.warning(f\"Progress callback failed for ADK event: {cb_e}\")\n\n                # Check for Human-in-Loop triggers (example)\n                #if event.actions and event.actions.request_human_input:\n                #     if self.human_in_loop_callback:\n                #         logger.info(f\"ADK requesting human input: {event.actions.request_human_input.reason}\")\n                         # This needs a mechanism to pause and resume the run_async loop\n                         # HIL is complex with async generators. Placeholder for now.\n                         # human_response = await self.human_in_loop_callback(...)\n                         # Need to inject response back into ADK runner - not straightforward\n               #          logger.warning(\"Human-in-Loop requested by ADK, but interaction is not implemented.\")\n                         # Could potentially send an error response back?\n               #      else:\n               #         logger.warning(\"ADK requested human input, but no HIL callback is configured.\")\n\n\n                # Extract final textual response\n                if event.is_final_response():\n                    # Prioritize text part\n                    if event.content and event.content.parts:\n                        text_parts = [p.text for p in event.content.parts if hasattr(p, 'text')]\n                        if text_parts:\n                            final_response_text = \"\\n\".join(text_parts).strip()\n                        else: # Handle other content types if needed (e.g., function call results as final)\n                            # For now, just serialize the first part if no text found\n                            final_response_text = str(event.content.parts[0]) if event.content.parts else \"ADK finished with non-text content.\"\n                    elif event.actions and event.actions.escalate:\n                        final_response_text = f\"Error: Agent escalated: {event.error_message or 'No specific message.'}\"\n                    elif event.error_message:\n                         final_response_text = f\"Error: ADK processing failed: {event.error_message}\"\n                    else:\n                         final_response_text = \"ADK processing finished without a clear textual response.\"\n                    break # Stop processing events\n\n            # 4. Update World Model from final ADK state (if syncing)\n            # This happens *after* the run completes, the sync in a_run updates the persisted state.\n            if self.sync_adk_state and adk_session_state is not None:\n                 # Fetch potentially updated state after run completion\n                 try:\n                     final_session = await asyncio.to_thread(self.adk_session_service.get_session, app_name=app_name, user_id=user_id, session_id=session_id)\n                     if final_session:\n                         self.world_model.sync_from_adk_state(final_session.state)\n                     else:\n                         logger.warning(f\"Could not fetch final ADK state for session {session_id} after run.\")\n                 except Exception as sync_e:\n                     logger.error(f\"Error fetching final ADK state: {sync_e}\")\n\n\n            logger.debug(\"ADK run finished.\")\n            return final_response_text\n\n        except Exception as e:\n            logger.error(f\"ADK execution failed: {e}\", exc_info=True)\n            # Return partial events log on error for debugging\n            events_preview = \"\\n\".join(all_events_str[:5])\n            return f\"Error during ADK processing: {e}\\nFirst Events:\\n{events_preview}\"\n\n    async def _execute_a2a_call(self, user_input: str, session_id: str, **kwargs) -&gt; str:\n        \"\"\"Executes a call to another agent via A2A using python-a2a and waits for the result.\"\"\"\n\n        client = None\n        task_id = None\n\n        if not A2A_AVAILABLE: return \"Error: python-a2a library not available.\"\n\n        logger.debug(\"Executing A2A call...\")\n\n        target_agent_url = kwargs.get('target_a2a_agent_url')\n        task_prompt = kwargs.get('a2a_task_prompt', user_input)\n\n        if not target_agent_url:\n            if len(self.a2a_clients) == 1:\n                target_agent_url = list(self.a2a_clients.keys())[0]\n                logger.info(f\"Using only available A2A client target: {target_agent_url}\")\n            else:\n                 return \"Error: Target A2A agent URL not specified and multiple clients configured.\"\n        try:\n            client = await self.setup_a2a_client(target_agent_url)\n            if not client:\n                return f\"Error: Could not connect to A2A agent at {target_agent_url}\"\n\n            task_id = str(uuid.uuid4())\n            a2a_session_id = f\"a2a_{session_id}_{task_id[:8]}\"\n\n            logger.info(f\"Sending A2A task '{task_id}' to {target_agent_url}...\")\n\n            # --- Call python-a2a client's task sending method ---\n            # The library might have a high-level `create_task` or similar.\n            # Let's assume a `send_task` method exists that takes message content.\n            # We construct the message payload expected by the library.\n            # This structure might need adjustment based on python-a2a's specifics.\n            message_payload = {\n                \"role\": \"user\", # Assuming MessageRole.USER maps to \"user\"\n                \"content\": {\n                    \"type\": \"text\", # Assuming TextContent maps to this\n                    \"text\": task_prompt\n                 }\n            }\n            # The client method might take id/sessionId separately or as part of a task object\n            # Assuming a method signature like: send_task(message: Dict, task_id: str, session_id: str)\n            # This is an *assumption* based on typical A2A needs.\n            if hasattr(client, 'send_task'):\n                initial_task_info = await client.send_task(\n                    message=message_payload,\n                    task_id=task_id,\n                    session_id=a2a_session_id\n                ) # Adjust call based on actual method signature\n            elif hasattr(client, 'create_task'): # Alternative common pattern\n                 initial_task_info = await client.create_task(\n                     message=message_payload,\n                     task_id=task_id,\n                     session_id=a2a_session_id\n                 )\n            else:\n                 # Fallback to 'ask' if specific task methods are unavailable (less control)\n                 logger.warning(\"A2A client lacks specific send_task/create_task method, using high-level 'ask'. Polling might not work.\")\n                 # 'ask' likely blocks and returns the final result directly\n                 response_text = await client.ask(task_prompt, session_id=a2a_session_id)\n                 return response_text\n\n\n            # --- Process initial response and Poll ---\n            # Check the structure of initial_task_info (might be a Task object, dict, etc.)\n            # Extract initial state if possible\n            initial_state = TaskState.SUBMITTED # Default if state not returned immediately\n            if isinstance(initial_task_info, dict) and initial_task_info.get('status'):\n                initial_state_val = initial_task_info['status'].get('state')\n                if initial_state_val: initial_state = TaskState(initial_state_val) # Convert string to Enum\n            elif hasattr(initial_task_info, 'status') and hasattr(initial_task_info.status, 'state'):\n                 initial_state = initial_task_info.status.state\n\n            logger.info(f\"A2A task submitted (ID: {task_id}). Initial State: {initial_state}\")\n\n            # Don't poll if initial state is already final (unlikely but possible)\n            if initial_state in (TaskState.COMPLETED, TaskState.FAILED, TaskState.CANCELLED):\n                 logger.warning(f\"A2A task {task_id} already in final state {initial_state} after submission.\")\n                 # Need to extract result from initial_task_info here\n                 # ... logic to extract result based on initial_task_info structure ...\n                 return f\"Task finished immediately with state {initial_state}.\" # Placeholder\n\n            self.internal_state = InternalAgentState.WAITING_FOR_TOOL\n            final_result = await self._poll_a2a_task(client, task_id, target_agent_url)\n            self.internal_state = InternalAgentState.PROCESSING\n            return final_result\n\n        except TimeoutError:\n             logger.error(f\"A2A task {task_id} timed out after {self.a2a_poll_timeout}s.\")\n             # Attempt cancellation?\n             cancel_response = \"No clinet\"\n             if client:\n                cancel_response = await client.cancel_task(task_id=task_id)\n             return f\"Error: A2A task timed out waiting for result from {target_agent_url} {cancel_response}.\"\n        except Exception as e:\n            logger.error(f\"A2A execution failed: {e}\", exc_info=True)\n            return f\"Error during A2A call: {e}\"\n\n    async def _poll_a2a_task(self, client: A2AClient, task_id: str, target_url: str) -&gt; str:\n        \"\"\"Polls the GetTask endpoint using python-a2a client until a final state.\"\"\"\n        if not hasattr(client, 'get_task'):\n             raise NotImplementedError(f\"A2A client for {target_url} does not support 'get_task' for polling.\")\n\n        logger.debug(f\"Polling A2A task {task_id} on {target_url}...\")\n        start_time = time.monotonic()\n\n        while time.monotonic() - start_time &lt; self.a2a_poll_timeout:\n            try:\n                # Assume get_task takes task_id (and potentially historyLength)\n                task_details = await client.get_task(task_id=task_id, history_length=1)\n\n                # --- Parse the response (structure depends on python-a2a implementation) ---\n                current_state = TaskState.UNKNOWN\n                final_text = f\"A2A Task {task_id} finished.\"\n                error_message = None\n\n                # Example parsing assuming task_details is dict-like or object-like\n                status_info = None\n                if isinstance(task_details, dict):\n                    status_info = task_details.get('status')\n                elif hasattr(task_details, 'status'):\n                    status_info = task_details.status\n\n                if status_info:\n                    state_val = status_info.get('state') if isinstance(status_info, dict) else getattr(status_info, 'state', None)\n                    if state_val:\n                        try:\n                            current_state = TaskState(state_val) # Convert string to Enum\n                        except ValueError:\n                             logger.warning(f\"Received unknown task state '{state_val}' for task {task_id}\")\n\n                    logger.debug(f\"A2A task {task_id} current state: {current_state}\")\n\n                    # Call progress callback\n                    if self.progress_callback:\n                         # ... (progress callback logic remains the same) ...\n                        pass\n\n                    # Check for final state\n                    if current_state in (TaskState.COMPLETED, TaskState.FAILED, TaskState.CANCELLED):\n                        logger.info(f\"A2A task {task_id} reached final state: {current_state}\")\n\n                        # Extract final result from artifacts\n                        artifacts = task_details.get('artifacts') if isinstance(task_details, dict) else getattr(task_details, 'artifacts', None)\n                        if artifacts and isinstance(artifacts, list) and artifacts:\n                            # Simple extraction: assume first artifact, first part is text\n                            try:\n                                parts = artifacts[0].get('parts') if isinstance(artifacts[0], dict) else getattr(artifacts[0], 'parts', [])\n                                if parts and isinstance(parts, list) and parts:\n                                    text_part = parts[0].get('text') if isinstance(parts[0], dict) else getattr(parts[0], 'text', None)\n                                    if text_part:\n                                        final_text = str(text_part).strip()\n                            except Exception as parse_e:\n                                logger.warning(f\"Could not parse artifacts for task {task_id}: {parse_e}\")\n                                final_text = \"[Could not parse final artifact]\"\n\n                        # Handle failed/cancelled states\n                        if current_state == TaskState.FAILED:\n                            # Try to extract error message from status\n                            status_message_info = status_info.get('message') if isinstance(status_info, dict) else getattr(status_info, 'message', None)\n                            if status_message_info:\n                                # Assuming message content is similar structure to artifacts\n                                try:\n                                     err_content = status_message_info.get('content') if isinstance(status_message_info, dict) else getattr(status_message_info, 'content', None)\n                                     if err_content:\n                                         error_message = err_content.get('text') if isinstance(err_content, dict) else getattr(err_content, 'text', 'Unknown error')\n                                except: pass # Ignore parsing errors here\n                            return f\"Error: A2A task failed on {target_url}: {error_message or final_text}\"\n                        elif current_state == TaskState.CANCELLED:\n                            return f\"Info: A2A task was cancelled on {target_url}.\"\n                        else: # Completed\n                            return final_text\n\n                else:\n                    logger.warning(f\"A2A get_task for {task_id} returned no status info: {task_details}\")\n\n            except APIConnectionError as conn_e:\n                 logger.warning(f\"Connection error polling A2A task {task_id}: {conn_e}. Retrying...\")\n            except Exception as e:\n                logger.error(f\"Error polling A2A task {task_id}: {e}\", exc_info=True)\n                return f\"Error polling A2A task status: {e}\"\n\n            await asyncio.sleep(self.a2a_poll_interval)\n\n        raise TimeoutError(f\"Polling A2A task {task_id} timed out.\")\n\n    # --- Internal Helper Methods ---\n\n    def construct_initial_prompts(self) -&gt; list[dict]:\n        \"\"\"Constructs the initial system/context messages for the LLM prompt.\"\"\"\n        messages = []\n        # Base System Prompt\n        if self.amd.system_message:\n            messages.append(LLMMessage(\"system\", self.amd.system_message).to_dict())\n\n        # World Model Context\n        wm_repr = self.world_model.show()\n        if wm_repr != \"[empty]\":\n            messages.append(LLMMessage(\"system\", f\"Current World State:\\n{wm_repr}\").to_dict())\n\n        # Capabilities Overview (ADK specific parts depend on LlmAgent inheritance)\n        caps = [\"LiteLLM (Core LLM access)\"]\n        if ADK_AVAILABLE and isinstance(self, LlmAgent):\n            if self.tools: caps.append(\"ADK Tools (including potential MCP/A2A wrappers)\")\n            if self.code_executor: caps.append(\"ADK Code Execution\")\n            if any(isinstance(t, type(adk_google_search) | AdkVertexAiSearchTool) for t in getattr(self, 'tools', [])):\n                 caps.append(\"ADK Search\")\n        if A2A_AVAILABLE and self.a2a_clients: caps.append(\"A2A Client (delegate to other agents)\")\n        if self.mcp_server: caps.append(\"MCP Server (exposes capabilities)\")\n        if self.a2a_server: caps.append(\"A2A Server (receives tasks)\")\n\n        messages.append(LLMMessage(\"system\", f\"Your Capabilities: {', '.join(caps)}.\").to_dict())\n\n        # ADK Tool Instructions (if ADK enabled and tools exist)\n        if ADK_AVAILABLE and isinstance(self, LlmAgent) and self.tools:\n            try:\n                # Use ADK's internal method to get schema if possible, otherwise basic list\n                tool_schemas = getattr(self, 'tool_schemas', None) # ADK might populate this\n                if tool_schemas:\n                     tool_list_str = json.dumps(tool_schemas, indent=2)\n                     messages.append(LLMMessage(\"system\", f\"You have access to the following tools (use FunctionCall format):\\n{tool_list_str}\").to_dict())\n                else: # Fallback to basic list\n                    tool_list = \"\\n\".join([f\"- {tool.name}: {tool.description or 'No description'}\" for tool in self.tools])\n                    messages.append(LLMMessage(\"system\", f\"You can use the following tools:\\n{tool_list}\\nRespond with a FunctionCall to use a tool.\").to_dict())\n            except Exception as e:\n                 logger.warning(f\"Could not generate detailed ADK tool instructions: {e}\")\n\n\n        # Add specific instructions for A2A delegation if needed\n        if A2A_AVAILABLE and self.a2a_clients:\n             client_names = list(self.a2a_clients.keys()) # Target URLs act as names here\n             messages.append(LLMMessage(\"system\", f\"You can delegate tasks to other agents via A2A using their URLs (e.g., {client_names[0]} if available). Indicate clearly if you want to delegate.\").to_dict())\n\n        return messages\n\n    def _add_to_history(self, session_id: str, message: dict[str, Any]):\n         \"\"\"Adds a message to the session history, respecting limits.\"\"\"\n         if session_id not in self.message_history:\n              self.message_history[session_id] = []\n         self.message_history[session_id].append(message)\n\n         # Apply trimming immediately after adding (simpler than doing it before call)\n         self.message_history[session_id] = self._trim_messages(self.message_history[session_id])\n\n\n    def _trim_messages(self, messages: list[dict]) -&gt; list[dict]:\n        \"\"\"Trims message list based on configured strategy (tokens or turns).\"\"\"\n        if self.max_history_tokens and self.amd.model:\n            # Token-based trimming\n            max_tokens = self.max_history_tokens\n            if self.trim_strategy == \"litellm\":\n                try:\n                    trimmed = trim_messages(messages, model=self.amd.model, max_tokens=max_tokens)\n                    if len(trimmed) &lt; len(messages):\n                        logger.debug(f\"Trimmed history from {len(messages)} to {len(trimmed)} messages using LiteLLM token strategy ({max_tokens} tokens).\")\n                    return trimmed\n                except Exception as e:\n                    logger.warning(f\"LiteLLM trimming failed ({e}), falling back to basic token trim.\")\n                    # Fallthrough to basic token trim\n            # Basic token trim (keep system, remove oldest convo pairs)\n            system_msgs = [m for m in messages if m.get('role') == 'system']\n            convo_msgs = [m for m in messages if m.get('role') != 'system']\n            current_tokens = token_counter(messages=messages, model=self.amd.model)\n            while current_tokens &gt; max_tokens and len(convo_msgs) &gt;= 2:\n                 convo_msgs = convo_msgs[2:] # Remove oldest pair\n                 current_tokens = token_counter(messages=system_msgs + convo_msgs, model=self.amd.model)\n            final_messages = system_msgs + convo_msgs\n            if len(final_messages) &lt; len(messages):\n                 logger.debug(f\"Trimmed history from {len(messages)} to {len(final_messages)} messages using basic token strategy ({max_tokens} tokens).\")\n            return final_messages\n\n        elif self.max_history_turns &gt; 0:\n            # Turn-based trimming\n            system_msgs = [m for m in messages if m.get('role') == 'system']\n            convo_msgs = [m for m in messages if m.get('role') != 'system']\n            # Keep last N turns (each turn = user + assistant = 2 messages)\n            max_convo_messages = self.max_history_turns * 2\n            if len(convo_msgs) &gt; max_convo_messages:\n                trimmed_convo = convo_msgs[-max_convo_messages:]\n                logger.debug(f\"Trimmed history from {len(convo_msgs)//2} to {len(trimmed_convo)//2} turns.\")\n                return system_msgs + trimmed_convo\n            else:\n                return messages # No trimming needed\n        else:\n            # No trimming configured or possible\n            logger.warning(\"History trimming not configured or possible (missing max_tokens/model or max_turns).\")\n            return messages\n\n\n    async def a_run_llm_completion(self, llm_messages: list[dict], **kwargs) -&gt; str:\n        \"\"\"Core wrapper around LiteLLM acompletion with error handling, streaming, and cost tracking.\"\"\"\n        if not llm_messages:\n            logger.warning(\"a_run_llm_completion called with empty message list.\")\n            return \"Error: No message provided to the model.\"\n\n        self.print_verbose(f\"Running model '{self.amd.model}' with {len(llm_messages)} messages.\")\n        # self.print_verbose(\"Messages:\", json.dumps(llm_messages, indent=2)) # Very verbose\n\n        # Prepare LiteLLM parameters from AgentModelData and kwargs overrides\n        params = {\n            'model': self.format_model or self.amd.model,\n            'messages': llm_messages,\n            'temperature': self.amd.temperature,\n            'top_p': self.amd.top_p,\n            'top_k': self.amd.top_k,\n            'max_tokens': self.amd.max_tokens,\n            'stream': self.stream,\n            'stop': self.amd.stop_sequence,\n            'user': self.amd.user_id,\n            'api_base': self.amd.api_base,\n            'api_version': self.amd.api_version,\n            'api_key': self.amd.api_key,\n            'presence_penalty': self.amd.presence_penalty,\n            'frequency_penalty': self.amd.frequency_penalty,\n            'caching': self.amd.caching,\n            'response_format': kwargs.get('response_format'), # For a_format_class\n            'tools': kwargs.get('tools'), # For LiteLLM function calling (less common now with ADK)\n        }\n        # Filter out None values as LiteLLM prefers absence over None for some params\n        params = {k: v for k, v in params.items() if v is not None}\n\n        # Add budget manager if present\n        if self.amd.budget_manager: params['budget_manager'] = self.amd.budget_manager\n\n        full_response_content = \"\"\n        tool_calls_requested = None # Store tool calls if generated\n\n        try:\n            response_object = await acompletion(**params)\n\n            if self.stream:\n                collected_chunks = []\n                async for chunk in response_object:\n                    # Store raw chunk for potential analysis or replay\n                    collected_chunks.append(chunk)\n                    # Extract text delta\n                    chunk_delta = chunk.choices[0].delta.content or \"\"\n                    if chunk_delta:\n                        full_response_content += chunk_delta\n                        if self.stream_callback:\n                             try:\n                                 # Provide only the new text chunk\n                                 if iscoroutinefunction(self.stream_callback): await self.stream_callback(chunk_delta)\n                                 else: self.stream_callback(chunk_delta)\n                             except Exception as cb_e:\n                                 logger.warning(f\"Stream callback failed: {cb_e}\")\n                    # Check for tool call deltas (less common in streaming)\n                    tool_deltas = chunk.choices[0].delta.tool_calls\n                    if tool_deltas:\n                         logger.warning(\"Received tool call delta during streaming - handling may be incomplete.\")\n                         # : Implement robust handling of streaming tool calls if needed\n\n                # After stream, construct a final response object mimicking non-streaming one for cost tracking\n                # This is an approximation, LiteLLM might offer better ways.\n                final_choice = {\"message\": {\"role\": \"assistant\", \"content\": full_response_content}}\n                # If tool calls were detected during streaming, add them (complex to reconstruct accurately)\n                # if reconstructed_tool_calls: final_choice[\"message\"][\"tool_calls\"] = reconstructed_tool_calls\n                self.last_llm_result = {\n                    \"choices\": [{\"message\": final_choice[\"message\"]}],\n                    \"model\": self.amd.model, # Needed for cost tracking\n                    # Usage stats are often missing or zero in streaming chunks, need final value if available\n                    \"usage\": getattr(collected_chunks[-1], 'usage', {\"prompt_tokens\": 0, \"completion_tokens\": 0})\n                }\n\n            else: # Non-streaming\n                self.last_llm_result = response_object # Store the full response\n                # Extract content and potential tool calls\n                message = response_object.choices[0].message\n                full_response_content = message.content or \"\"\n                tool_calls_requested = message.tool_calls # List of ToolCall objects\n\n                # Check if LiteLLM did function/tool calling (different from ADK tools)\n                # This path is less likely if using ADK, but supported by LiteLLM\n                if tool_calls_requested:\n                    logger.info(f\"LiteLLM requested {len(tool_calls_requested)} tool calls.\")\n                    # This requires a separate mechanism to execute these LiteLLM-requested tools\n                    # and send back 'tool' role messages in the next turn.\n                    # Not implemented here as focus is on ADK/A2A tools.\n                    # For now, return a message indicating tool call request.\n                    calls_repr = \", \".join([f\"{tc.function.name}\" for tc in tool_calls_requested])\n                    return f\"Info: LLM requested tool calls ({calls_repr}). Direct execution not implemented.\"\n\n\n            self.print_verbose(f\"Model Response: {full_response_content[:100]}...\")\n            return full_response_content\n\n        except RateLimitError as e:\n            logger.error(f\"Rate limit error from {self.amd.model}: {e}\")\n            # Implement backoff/retry? For now, re-raise.\n            raise\n        except (BadRequestError, APIConnectionError, InternalServerError) as e:\n            logger.error(f\"API/Server error during LiteLLM call for {self.amd.model}: {e}\", exc_info=True)\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error during LiteLLM completion: {e}\", exc_info=True)\n            raise\n\n    async def a_format_class(self,\n                             pydantic_model: type[BaseModel],\n                             prompt: str,\n                             message_context: list[dict] | None = None,\n                             max_retries: int = 2) -&gt; dict[str, Any]:\n        \"\"\"Uses LiteLLM's response_format feature to get structured JSON output, with retries.\"\"\"\n        logger.debug(f\"Formatting prompt for Pydantic model: {pydantic_model.__name__}\")\n        model_schema = pydantic_model.model_json_schema()\n\n        messages = message_context or []\n        # System prompt explaining the task and schema\n        messages.append({\n            \"role\": \"system\",\n            \"content\": f\"Your task is to analyze the user's request and extract information into a JSON object.\\n\"\n                       f\"Strictly adhere to the following Pydantic schema:\\n\"\n                       f\"```json\\n{json.dumps(model_schema, indent=2)}\\n```\\n\"\n                       f\"Guidelines:\\n\"\n                       f\"- Analyze the request carefully.\\n\"\n                       f\"- Output *only* the JSON object, nothing else (no explanations, apologies, or markdown).\\n\"\n                       f\"- Ensure the JSON is valid and conforms exactly to the schema.\\n\"\n                       f\"- Omit optional fields if the information is not present in the request.\"\n        })\n        messages.append({\"role\": \"user\", \"content\": prompt})\n\n        # Use LiteLLM's JSON mode (requires compatible model/provider)\n        response_format_config = {\"type\": \"json_object\"}\n        # Some providers might need the schema explicitly even in json_object mode\n        # response_format_config = {\"type\": \"json_object\", \"schema\": model_schema}\n\n        original_stream_state = self.stream\n        self.stream = False # Ensure streaming is off for structured output\n        try:\n            last_exception = None\n            for attempt in range(max_retries + 1):\n                try:\n                    logger.debug(f\"Attempt {attempt + 1}/{max_retries + 1} to get structured JSON.\")\n                    # Use a potentially faster/cheaper model optimized for JSON tasks if configured?\n                    self.format_model = self.format_model_\n                    response_text = await self.a_run_llm_completion(messages, response_format=response_format_config)\n                    self.format_model = None\n                    # Clean and parse the JSON response\n                    try:\n                         # Basic cleaning: remove potential markdown fences\n                        cleaned_response = re.sub(r'^```json\\s*|\\s*```$', '', response_text.strip(), flags=re.MULTILINE)\n\n                         # Try parsing using Pydantic's TypeAdapter for direct validation\n                        adapter = TypeAdapter(pydantic_model)\n                        validated_obj = adapter.validate_json(cleaned_response)\n                        result_dict = validated_obj.model_dump(mode='json') # Get dict representation\n\n                        logger.debug(f\"Successfully formatted and validated JSON: {result_dict}\")\n                        return result_dict\n\n                    except (json.JSONDecodeError, ValidationError) as e:\n                        logger.warning(f\"Attempt {attempt + 1} failed: Invalid JSON or schema mismatch. Error: {e}. Response: {response_text[:500]}\")\n                        last_exception = ValueError(f\"LLM response did not match schema after cleaning. Error: {e}. Response: '{response_text[:200]}...'\")\n                        # Add feedback to the model for retry\n                        messages.append({\"role\": \"assistant\", \"content\": response_text}) # Show previous attempt\n                        messages.append({\"role\": \"system\", \"content\": f\"Your previous response was invalid ({e}). Please try again, ensuring you output *only* valid JSON matching the schema.\"})\n\n                except Exception as e:\n                    logger.error(f\"Error during a_format_class (attempt {attempt + 1}): {e}\", exc_info=True)\n                    last_exception = e\n                    # Don't retry on non-parsing errors immediately, could be API issue\n                    break\n\n                # Wait before retrying\n                if attempt &lt; max_retries:\n                     await asyncio.sleep(1.5 ** attempt) # Exponential backoff\n\n            # If all retries fail\n            logger.error(f\"Failed to get valid structured JSON after {max_retries + 1} attempts.\")\n            raise last_exception or ValueError(\"Failed to get structured JSON response from LLM.\")\n\n        finally:\n            self.stream = original_stream_state # Restore stream setting\n\n\n    async def flow_world_model(self, text_input: str, session_id: str, adk_session_state: State | None):\n        \"\"\"\n        Analyzes input, updates internal WorldModel, and syncs with ADK state if enabled.\n        Sync Priority: If ADK state exists, sync *from* it first. Then update based on text.\n                     The sync *to* ADK happens after the agent run completes.\n        \"\"\"\n        logger.debug(f\"Flowing world model based on text: {text_input[:100]}...\")\n\n        # 1. Sync FROM ADK State (if enabled and state available)\n        if self.sync_adk_state and adk_session_state is not None:\n             logger.debug(\"Syncing World Model FROM ADK session state...\")\n             self.world_model.sync_from_adk_state(adk_session_state)\n\n        # 2. Update World Model based on Text Input (using LLM)\n        # This adds/modifies based on the current turn's input\n        # Define Pydantic model for structured update extraction\n        current_keys = list(self.world_model.to_dict().keys())\n        class WorldModelAdaption(BaseModel):\n            action: Literal['add', 'update', 'remove', 'none'] = Field(..., description=\"Action on the world model.\")\n            key: str | None = Field(None, description=f\"Key to modify/add/remove (e.g., 'user_location', 'task_status'). Existing keys: {current_keys}\")\n            value: Any | None = Field(None, description=\"New value (for 'add'/'update'). Should be JSON serializable.\")\n            reasoning: str = Field(..., description=\"Why this change (or no change) is needed based on the input.\")\n\n        prompt = (f\"Analyze the following text and current world state to determine if the agent's world model needs changes.\\n\"\n                  f\"Current World State Keys: {current_keys}\\n\"\n                  f\"Text Input: ```\\n{text_input}\\n```\\n\"\n                  f\"Decide action, key, value, and reasoning. Focus on factual updates derived *from the text*. Do not hallucinate.\")\n\n        try:\n            # Use a potentially faster/cheaper model for this classification task\n            # Could eventually use a separate AMD config for this call\n            adaption_dict = await self.a_format_class(WorldModelAdaption, prompt)\n            adaption = WorldModelAdaption(**adaption_dict)\n\n            logger.info(f\"World Model Adaption proposed: {adaption.action} on key '{adaption.key}'. Reason: {adaption.reasoning}\")\n\n            if adaption.action == 'add' or adaption.action == 'update':\n                if adaption.key and adaption.value is not None:\n                    self.world_model.set(adaption.key, adaption.value)\n                else:\n                    logger.warning(\"World model 'add'/'update' ignored: missing key or value.\")\n            elif adaption.action == 'remove':\n                if adaption.key:\n                    self.world_model.remove(adaption.key)\n                else:\n                    logger.warning(\"World model 'remove' ignored: missing key.\")\n            # Else ('none'): do nothing\n\n        except (ValidationError, Exception) as e:\n            logger.warning(f\"Failed to determine world model adaption via LLM: {e}. World model may be based only on ADK sync or previous state.\")\n\n        # NOTE: Sync TO ADK happens *after* the full agent run in a_run()\n\n\n    # --- ADK Tool Implementations (Internal Wrappers) ---\n    def _ensure_internal_adk_tools(self):\n        \"\"\"Adds essential internal ADK tools if not already present.\"\"\"\n        if not ADK_AVAILABLE or not isinstance(self, LlmAgent):\n            return\n        if self.tools is None: self.tools = []\n\n        existing_tool_names = {tool.name for tool in self.tools if isinstance(tool, BaseTool)}\n\n        internal_adk_tools = {\n            \"get_world_model_key\": self.adk_tool_world_model_get,\n            \"show_world_model\": self.adk_tool_world_model_show,\n        }\n        if A2A_AVAILABLE:\n            internal_adk_tools[\"a2a_send_and_wait\"] = self.adk_tool_a2a_send_and_wait\n            # Add NEW tools\n            internal_adk_tools[\"a2a_send_no_wait\"] = self.adk_tool_a2a_send_no_wait\n            internal_adk_tools[\"a2a_get_task_status\"] = self.adk_tool_a2a_get_task_status\n            internal_adk_tools[\"a2a_cancel_task\"] = self.adk_tool_a2a_cancel_task\n\n        for name, func in internal_adk_tools.items():\n            if name not in existing_tool_names:\n                try:\n                    tool_instance = FunctionTool(func=func) # ADK infers from func signature/docstring\n                    self.tools.append(tool_instance)\n                    logger.debug(f\"Registered internal ADK tool: {name}\")\n                except Exception as e:\n                    logger.warning(f\"Failed to register internal ADK tool '{name}': {e}.\")\n\n    # --- Existing ADK Tools ---\n    async def adk_tool_world_model_get(self, tool_context: ToolContext | None, key: str) -&gt; Any | None:\n        \"\"\"ADK Tool: Retrieves a specific value from the agent's world model.\"\"\"\n        # ... (implementation remains the same) ...\n        logger.info(f\"[ADK Tool] get_world_model_key called for key: {key}\")\n        return self.world_model.get(key)\n\n    async def adk_tool_world_model_show(self, tool_context: ToolContext | None) -&gt; str:\n        \"\"\"ADK Tool: Returns a string representation of the agent's entire world model.\"\"\"\n        # ... (implementation remains the same) ...\n        logger.info(\"[ADK Tool] show_world_model called\")\n        return self.world_model.show()\n\n    async def adk_tool_a2a_send_and_wait(self,\n                                         tool_context: ToolContext | None,\n                                         target_agent_url: str,\n                                         task_prompt: str,\n                                         session_id: str | None = None\n                                         ) -&gt; str:\n        \"\"\"ADK Tool: Sends a task to another agent via A2A and waits for the final text result.\"\"\"\n        # ... (implementation remains the same, calls _execute_a2a_call) ...\n        if not A2A_AVAILABLE: return \"Error: python-a2a library not available.\"\n        logger.info(f\"[ADK Tool] a2a_send_and_wait called for target: {target_agent_url}\")\n        tool_session_id = session_id or f\"adk_tool_a2a_{uuid.uuid4()}\"\n        try:\n            return await self._execute_a2a_call(\n                 user_input=task_prompt,\n                 session_id=tool_session_id,\n                 target_a2a_agent_url=target_agent_url,\n                 a2a_task_prompt=task_prompt\n            )\n        except Exception as e:\n             logger.error(f\"[ADK Tool] a2a_send_and_wait failed: {e}\", exc_info=True)\n             return f\"Error executing A2A task via ADK tool: {e}\"\n\n        # --- NEW ADK Tools for A2A ---\n\n    async def adk_tool_a2a_send_no_wait(self,\n                                        tool_context: ToolContext | None,\n                                        target_agent_url: str,\n                                        task_prompt: str,\n                                        session_id: str | None = None\n                                        ) -&gt; str:\n        \"\"\"ADK Tool: Sends a task to another agent via A2A and returns the task ID immediately.\n\n        Args:\n            target_agent_url: The full URL of the target A2A agent.\n            task_prompt: The natural language prompt or task for the target agent.\n            session_id: Optional session ID to use for the A2A interaction.\n\n        Returns:\n            The unique ID of the submitted A2A task, or an error message.\n        \"\"\"\n        if not A2A_AVAILABLE: return \"Error: python-a2a library not available.\"\n        logger.info(f\"[ADK Tool] a2a_send_no_wait called for target: {target_agent_url}\")\n\n        try:\n            client = await self.setup_a2a_client(target_agent_url)\n            if not client:\n                return f\"Error: Could not connect to A2A agent at {target_agent_url}\"\n\n            task_id = str(uuid.uuid4())\n            a2a_session_id = session_id or f\"a2a_tool_nowait_{task_id[:8]}\"\n\n            message_payload = {\"role\": \"user\", \"content\": {\"type\": \"text\", \"text\": task_prompt}}\n\n            initial_task_info = None\n            if hasattr(client, 'send_task'):\n                initial_task_info = await client.send_task(message=message_payload, task_id=task_id,\n                                                           session_id=a2a_session_id)\n            elif hasattr(client, 'create_task'):\n                initial_task_info = await client.create_task(message=message_payload, task_id=task_id,\n                                                             session_id=a2a_session_id)\n            else:\n                return \"Error: A2A client does not support send_task or create_task.\"\n\n            # Check for immediate errors from the submission call\n            # Structure depends on python-a2a's return value\n            error_info = None\n            if isinstance(initial_task_info, dict):\n                error_info = initial_task_info.get('error')\n            elif hasattr(initial_task_info, 'error'):\n                error_info = initial_task_info.error\n\n            if error_info:\n                err_msg = error_info.get('message', str(error_info)) if isinstance(error_info, dict) else str(\n                    error_info)\n                logger.error(f\"A2A send_task (no wait) failed immediately: {err_msg}\")\n                return f\"Error submitting A2A task: {err_msg}\"\n            else:\n                logger.info(f\"A2A task '{task_id}' submitted successfully (no wait) to {target_agent_url}.\")\n                return task_id  # Return the ID for later polling/checking\n\n        except Exception as e:\n            logger.error(f\"[ADK Tool] a2a_send_no_wait failed: {e}\", exc_info=True)\n            return f\"Error sending A2A task (no wait): {e}\"\n\n    async def adk_tool_a2a_get_task_status(self,\n                                           tool_context: ToolContext | None,\n                                           target_agent_url: str,\n                                           task_id: str\n                                           ) -&gt; dict[str, Any]:\n        \"\"\"ADK Tool: Gets the current status and details of an A2A task.\n\n        Args:\n            target_agent_url: The URL of the agent hosting the task.\n            task_id: The ID of the task to check.\n\n        Returns:\n            A dictionary containing task status details (state, message, artifacts) or an error.\n        \"\"\"\n        if not A2A_AVAILABLE: return {\"error\": \"python-a2a library not available.\"}\n        logger.info(f\"[ADK Tool] a2a_get_task_status called for task {task_id} on {target_agent_url}\")\n\n        try:\n            client = await self.setup_a2a_client(target_agent_url)\n            if not client:\n                return {\"error\": f\"Could not connect to A2A agent at {target_agent_url}\"}\n\n            if not hasattr(client, 'get_task'):\n                return {\"error\": f\"A2A client for {target_agent_url} does not support 'get_task'.\"}\n\n            # Get task details from the client\n            task_details = await client.get_task(task_id=task_id, history_length=1)  # History=1 gets latest status\n\n            # Parse and return relevant info\n            if isinstance(task_details, dict):\n                # Basic parsing, adjust based on actual python-a2a structure\n                status_info = task_details.get('status', {})\n                artifacts = task_details.get('artifacts')\n                return {\n                    \"task_id\": task_id,\n                    \"state\": status_info.get('state', 'UNKNOWN'),\n                    \"status_message\": status_info.get('message'),  # Might be complex object\n                    \"artifacts\": artifacts,  # Might be complex list\n                    \"raw_response\": task_details  # Include raw for debugging\n                }\n            elif hasattr(task_details, 'status'):  # Object-like response\n                status_obj = task_details.status\n                artifacts_obj = getattr(task_details, 'artifacts', None)\n                return {\n                    \"task_id\": task_id,\n                    \"state\": getattr(status_obj, 'state', TaskState.UNKNOWN).value,  # Get enum value\n                    \"status_message\": getattr(status_obj, 'message', None),\n                    \"artifacts\": artifacts_obj,\n                    \"raw_response\": vars(task_details)  # Example conversion\n                }\n            else:\n                return {\"error\": \"Received unexpected response structure from get_task.\", \"raw_response\": task_details}\n\n        except Exception as e:\n            # Catch specific errors from python-a2a if they exist (e.g., TaskNotFoundError)\n            # if isinstance(e, TaskNotFoundError):\n            #    logger.warning(f\"[ADK Tool] A2A Task {task_id} not found on {target_agent_url}.\")\n            #    return {\"error\": f\"Task {task_id} not found.\"}\n            logger.error(f\"[ADK Tool] a2a_get_task_status failed: {e}\", exc_info=True)\n            return {\"error\": f\"Error getting A2A task status: {e}\"}\n\n    async def adk_tool_a2a_cancel_task(self,\n                                       tool_context: ToolContext | None,\n                                       target_agent_url: str,\n                                       task_id: str\n                                       ) -&gt; dict[str, Any]:\n        \"\"\"ADK Tool: Attempts to cancel an ongoing A2A task.\n\n        Args:\n            target_agent_url: The URL of the agent hosting the task.\n            task_id: The ID of the task to cancel.\n\n        Returns:\n            A dictionary indicating success or failure, possibly with the task's state after cancellation attempt.\n        \"\"\"\n        if not A2A_AVAILABLE: return {\"error\": \"python-a2a library not available.\"}\n        logger.info(f\"[ADK Tool] a2a_cancel_task called for task {task_id} on {target_agent_url}\")\n\n        try:\n            client = await self.setup_a2a_client(target_agent_url)\n            if not client:\n                return {\"error\": f\"Could not connect to A2A agent at {target_agent_url}\"}\n\n            if not hasattr(client, 'cancel_task'):\n                return {\"error\": f\"A2A client for {target_agent_url} does not support 'cancel_task'.\"}\n\n            # Call the client's cancel method\n            # The response structure depends heavily on the library implementation\n            cancel_response = await client.cancel_task(task_id=task_id)\n\n            # Parse response - could be simple success/fail, or updated task state\n            if isinstance(cancel_response, dict):\n                if 'error' in cancel_response:\n                    error_info = cancel_response['error']\n                    err_msg = error_info.get('message', str(error_info)) if isinstance(error_info, dict) else str(\n                        error_info)\n                    logger.warning(f\"A2A cancel_task failed for {task_id}: {err_msg}\")\n                    return {\"success\": False, \"error\": err_msg, \"raw_response\": cancel_response}\n                else:\n                    # Assume success, response might contain updated task state\n                    logger.info(f\"A2A task {task_id} cancellation requested successfully.\")\n                    # Try to extract state if returned\n                    state = cancel_response.get('result', {}).get('status', {}).get('state', 'UNKNOWN')\n                    return {\"success\": True, \"state_after_request\": state, \"raw_response\": cancel_response}\n            elif cancel_response is True:  # Simple boolean success\n                return {\"success\": True, \"state_after_request\": \"UNKNOWN\"}\n            else:  # Assume object-like or other structure\n                # Add parsing based on observed python-a2a behavior\n                logger.info(f\"A2A task {task_id} cancellation request sent, parsing result.\")\n                # Example: Check for specific attributes if object is returned\n                state = getattr(getattr(getattr(cancel_response, 'result', None), 'status', None), 'state',\n                                TaskState.UNKNOWN).value\n                return {\"success\": True, \"state_after_request\": state,\n                        \"raw_response\": vars(cancel_response) if hasattr(cancel_response, '__dict__') else str(\n                            cancel_response)}\n\n\n        except Exception as e:\n            # Catch specific errors like TaskNotFound, TaskNotCancelable if defined by python-a2a\n            # if isinstance(e, TaskNotFoundError):\n            #    return {\"success\": False, \"error\": f\"Task {task_id} not found.\"}\n            # if isinstance(e, TaskNotCancelableError):\n            #    return {\"success\": False, \"error\": f\"Task {task_id} is not in a cancelable state.\"}\n            logger.error(f\"[ADK Tool] a2a_cancel_task failed: {e}\", exc_info=True)\n            return {\"success\": False, \"error\": f\"Error cancelling A2A task: {e}\"}\n\n    # async def adk_tool_a2a_get_task(self, tool_context: Optional[ToolContext], target_agent_url: str, task_id: str) -&gt; Dict:\n    #     \"\"\"ADK Tool: Gets the current status and details of an A2A task.\"\"\"\n    #     # Implementation would be similar to _poll_a2a_task but return the status dict directly\n    #     pass\n\n\n    # --- Cost Tracking ---\n    def _track_cost(self, response_obj: Any):\n        \"\"\"Updates cost using LiteLLM.\"\"\"\n        if not response_obj: return\n        try:\n            cost = completion_cost(completion_response=response_obj, model=self.amd.model)\n            if cost is not None:\n                self.total_cost += cost\n                logger.info(f\"Turn Cost: ${cost:.6f}, Total Accumulated Cost: ${self.total_cost:.6f}\")\n            else:\n                 logger.debug(\"Cost calculation returned None (possibly streaming or non-standard response).\")\n        except Exception as e:\n            logger.warning(f\"Failed to calculate/track cost: {e}\")\n\n\n    # --- Cleanup ---\n    async def close(self):\n        \"\"\"Gracefully close connections and resources.\"\"\"\n        logger.info(f\"Closing resources for agent '{self.amd.name}'...\")\n        # Close A2A resources\n        if self.a2a_server and hasattr(self.a2a_server, 'stop'): # Check if server has stop method\n             logger.info(\"Stopping A2A server...\")\n             try:\n                 await self.a2a_server.stop() # Assuming stop is async\n             except Exception as e: logger.warning(f\"Error stopping A2A server: {e}\")\n        if hasattr(self, '_a2a_task_manager_instance') and hasattr(self._a2a_task_manager_instance, 'close'):\n             logger.info(\"Closing A2A task manager...\")\n             await self._a2a_task_manager_instance.close()\n        await self.close_a2a_clients()\n\n        # Close MCP server if running\n        if self.mcp_server and hasattr(self.mcp_server, 'stop'): # Check for stop method\n             logger.info(\"Stopping MCP server...\")\n             try:\n                 # MCP server run is blocking, stop might need separate mechanism\n                 # or be handled by process termination. If stop method exists:\n                 # await self.mcp_server.stop() # Assuming async stop\n                 logger.warning(\"MCP server 'stop' might need manual implementation or process signal.\")\n             except Exception as e: logger.warning(f\"Error stopping MCP server: {e}\")\n\n\n        # Close ADK resources (MCPToolset connections managed by exit stack)\n        if self.adk_exit_stack:\n            logger.info(\"Closing ADK AsyncExitStack (manages MCPToolset connections)...\")\n            try:\n                await self.adk_exit_stack.aclose()\n            except Exception as e:\n                logger.warning(f\"Error closing ADK exit stack: {e}\")\n\n        # Close ADK runner if it has a close method\n        if self.adk_runner and hasattr(self.adk_runner, 'close'):\n             logger.info(\"Closing ADK runner...\")\n             try:\n                  # Check if close is async\n                 if iscoroutinefunction(self.adk_runner.close):\n                     await self.adk_runner.close()\n                 else:\n                     self.adk_runner.close()\n             except Exception as e: logger.warning(f\"Error closing ADK runner: {e}\")\n\n\n        logger.info(f\"Agent '{self.amd.name}' resource cleanup finished.\")\n\n    def print_verbose(self, *args):\n        \"\"\"Conditional logging helper.\"\"\"\n        if self.verbose:\n            logger.debug(' '.join(map(str, args)))\n</code></pre> <code>a_format_class(pydantic_model, prompt, message_context=None, max_retries=2)</code> <code>async</code> \u00b6 <p>Uses LiteLLM's response_format feature to get structured JSON output, with retries.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def a_format_class(self,\n                         pydantic_model: type[BaseModel],\n                         prompt: str,\n                         message_context: list[dict] | None = None,\n                         max_retries: int = 2) -&gt; dict[str, Any]:\n    \"\"\"Uses LiteLLM's response_format feature to get structured JSON output, with retries.\"\"\"\n    logger.debug(f\"Formatting prompt for Pydantic model: {pydantic_model.__name__}\")\n    model_schema = pydantic_model.model_json_schema()\n\n    messages = message_context or []\n    # System prompt explaining the task and schema\n    messages.append({\n        \"role\": \"system\",\n        \"content\": f\"Your task is to analyze the user's request and extract information into a JSON object.\\n\"\n                   f\"Strictly adhere to the following Pydantic schema:\\n\"\n                   f\"```json\\n{json.dumps(model_schema, indent=2)}\\n```\\n\"\n                   f\"Guidelines:\\n\"\n                   f\"- Analyze the request carefully.\\n\"\n                   f\"- Output *only* the JSON object, nothing else (no explanations, apologies, or markdown).\\n\"\n                   f\"- Ensure the JSON is valid and conforms exactly to the schema.\\n\"\n                   f\"- Omit optional fields if the information is not present in the request.\"\n    })\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    # Use LiteLLM's JSON mode (requires compatible model/provider)\n    response_format_config = {\"type\": \"json_object\"}\n    # Some providers might need the schema explicitly even in json_object mode\n    # response_format_config = {\"type\": \"json_object\", \"schema\": model_schema}\n\n    original_stream_state = self.stream\n    self.stream = False # Ensure streaming is off for structured output\n    try:\n        last_exception = None\n        for attempt in range(max_retries + 1):\n            try:\n                logger.debug(f\"Attempt {attempt + 1}/{max_retries + 1} to get structured JSON.\")\n                # Use a potentially faster/cheaper model optimized for JSON tasks if configured?\n                self.format_model = self.format_model_\n                response_text = await self.a_run_llm_completion(messages, response_format=response_format_config)\n                self.format_model = None\n                # Clean and parse the JSON response\n                try:\n                     # Basic cleaning: remove potential markdown fences\n                    cleaned_response = re.sub(r'^```json\\s*|\\s*```$', '', response_text.strip(), flags=re.MULTILINE)\n\n                     # Try parsing using Pydantic's TypeAdapter for direct validation\n                    adapter = TypeAdapter(pydantic_model)\n                    validated_obj = adapter.validate_json(cleaned_response)\n                    result_dict = validated_obj.model_dump(mode='json') # Get dict representation\n\n                    logger.debug(f\"Successfully formatted and validated JSON: {result_dict}\")\n                    return result_dict\n\n                except (json.JSONDecodeError, ValidationError) as e:\n                    logger.warning(f\"Attempt {attempt + 1} failed: Invalid JSON or schema mismatch. Error: {e}. Response: {response_text[:500]}\")\n                    last_exception = ValueError(f\"LLM response did not match schema after cleaning. Error: {e}. Response: '{response_text[:200]}...'\")\n                    # Add feedback to the model for retry\n                    messages.append({\"role\": \"assistant\", \"content\": response_text}) # Show previous attempt\n                    messages.append({\"role\": \"system\", \"content\": f\"Your previous response was invalid ({e}). Please try again, ensuring you output *only* valid JSON matching the schema.\"})\n\n            except Exception as e:\n                logger.error(f\"Error during a_format_class (attempt {attempt + 1}): {e}\", exc_info=True)\n                last_exception = e\n                # Don't retry on non-parsing errors immediately, could be API issue\n                break\n\n            # Wait before retrying\n            if attempt &lt; max_retries:\n                 await asyncio.sleep(1.5 ** attempt) # Exponential backoff\n\n        # If all retries fail\n        logger.error(f\"Failed to get valid structured JSON after {max_retries + 1} attempts.\")\n        raise last_exception or ValueError(\"Failed to get structured JSON response from LLM.\")\n\n    finally:\n        self.stream = original_stream_state # Restore stream setting\n</code></pre> <code>a_run(user_input, session_id=None, persist_history=True, strategy_override=None, kwargs_override=None, a2a_task_id=None)</code> <code>async</code> \u00b6 <p>Main asynchronous execution logic for the agent turn.</p> <p>Orchestrates world model updates, state sync, strategy selection, execution, cost tracking, and callbacks.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def a_run(self,\n                user_input: str,\n                session_id: str | None = None,\n                persist_history: bool = True,\n                strategy_override: ProcessingStrategy | None = None,\n                kwargs_override: dict[str, Any] | None = None, # For fine-grained control\n                a2a_task_id: str | None = None # Context if called from A2A task\n                ) -&gt; str:\n    \"\"\"\n    Main asynchronous execution logic for the agent turn.\n\n    Orchestrates world model updates, state sync, strategy selection,\n    execution, cost tracking, and callbacks.\n    \"\"\"\n    self.internal_state = InternalAgentState.PROCESSING\n    start_time = time.monotonic()\n    session_id = session_id or \"default\" # Use 'default' if none provided\n    response = \"Error: Processing failed.\" # Default error\n    turn_cost = 0.0\n    span = None # OTel span\n\n    if not self.tracer: self._setup_telemetry() # Ensure tracer exists\n\n    try:\n        with self.tracer.start_as_current_span(f\"Agent Run: {self.amd.name}\", attributes={\"session_id\": session_id}) as span:\n\n            # Ensure session history list exists\n            if session_id not in self.message_history:\n                logger.debug(f\"Initializing history for session: {session_id}\")\n                self.message_history[session_id] = []\n\n            logger.info(f\"--- Agent Run Start (Session: {session_id}) ---\")\n            span.add_event(\"Agent run started\")\n            logger.info(f\"User Input: {user_input[:100]}...\")\n            span.set_attribute(\"user_input\", user_input[:500]) # Log truncated input\n\n            # 0. Get ADK Session State (if ADK enabled and syncing)\n            adk_session_state = None\n            if self.sync_adk_state and self.adk_session_service:\n                try:\n                    # ADK SessionService methods are typically synchronous\n                    # Run in threadpool to avoid blocking\n                    adk_session = await asyncio.to_thread(\n                         self.adk_session_service.get_session,\n                         app_name=self.adk_runner.app_name, # Assuming runner is set if syncing\n                         user_id=self.amd.user_id or \"adk_user\", # Needs consistent user ID\n                         session_id=session_id\n                    )\n                    if adk_session:\n                        adk_session_state = adk_session.state\n                    else:\n                        logger.warning(f\"ADK Session '{session_id}' not found for state sync.\")\n                        # Optionally create session here? Be careful about race conditions.\n                except Exception as sync_e:\n                    logger.error(f\"Error getting ADK session state for sync: {sync_e}\")\n\n            # 1. Update World Model &amp; Sync State (Run *before* strategy selection)\n            # flow_world_model is now responsible for syncing *from* ADK state initially\n            await self.flow_world_model(user_input, session_id, adk_session_state)\n            span.add_event(\"World model updated\")\n\n            # 2. Prepare message history for this turn\n            current_turn_messages = self._prepare_llm_messages(user_input, session_id)\n            span.set_attribute(\"history_length\", len(current_turn_messages) -1) # Exclude current input\n\n            # 3. Determine Processing Strategy\n            if strategy_override:\n                strategy = strategy_override\n                strategy_reasoning = \"Strategy overridden by caller.\"\n                logger.info(f\"Strategy forced by override: {strategy.value}\")\n            else:\n                strategy, strategy_reasoning = self._determine_strategy_heuristic(user_input, current_turn_messages)\n                logger.info(f\"Strategy Selected: {strategy.value} (Reason: {strategy_reasoning})\")\n            span.set_attribute(\"selected_strategy\", strategy.value)\n            span.set_attribute(\"strategy_reasoning\", strategy_reasoning)\n\n\n            # --- Prepare kwargs for execution based on strategy ---\n            exec_kwargs = kwargs_override or {}\n            exec_kwargs['session_id'] = session_id\n            exec_kwargs['user_input'] = user_input\n            exec_kwargs['current_turn_messages'] = current_turn_messages\n            exec_kwargs['adk_session_state'] = adk_session_state # Pass state for potential use/update\n\n\n            # 4. Execute Selected Strategy\n            logger.info(f\"Executing strategy: {strategy.value}\")\n            if strategy == ProcessingStrategy.ADK_RUN:\n                if ADK_AVAILABLE and self.adk_runner:\n                    response = await self._execute_adk_run(**exec_kwargs)\n                else:\n                    logger.warning(\"ADK_RUN strategy selected, but ADK runner not available/configured. Falling back.\")\n                    # Fallback strategy? Maybe DIRECT_LLM?\n                    strategy = ProcessingStrategy.DIRECT_LLM\n                    response = await self._execute_direct_llm(**exec_kwargs)\n\n            elif strategy == ProcessingStrategy.A2A_CALL:\n                if A2A_AVAILABLE:\n                    response = await self._execute_a2a_call(**exec_kwargs)\n                else:\n                    logger.warning(\"A2A_CALL strategy selected, but A2A not available. Falling back.\")\n                    strategy = ProcessingStrategy.DIRECT_LLM\n                    response = await self._execute_direct_llm(**exec_kwargs)\n\n            else: # Default: DIRECT_LLM\n                response = await self._execute_direct_llm(**exec_kwargs)\n\n            span.set_attribute(\"raw_response_length\", len(response))\n            span.add_event(\"Strategy execution complete\")\n\n            # 5. Persist History (if successful and enabled)\n            # Add assistant response to history\n            if persist_history and not response.startswith(\"Error:\"):\n                 self._add_to_history(session_id, LLMMessage(role=\"assistant\", content=response).to_dict())\n\n            # 6. Sync World Model *back* to ADK State (if changed and enabled)\n            if self.sync_adk_state and adk_session_state is not None:\n                try:\n                    self.world_model.sync_to_adk_state(adk_session_state)\n                    span.add_event(\"ADK state synchronized and updated\")\n                except Exception as sync_e:\n                     logger.error(f\"Error syncing/updating ADK session state: {sync_e}\")\n                     span.record_exception(sync_e)\n\n            # 7. Track Cost (using last_llm_result if available)\n            if self.last_llm_result:\n                try:\n                    cost = completion_cost(completion_response=self.last_llm_result, model=self.amd.model)\n                    if cost:\n                        turn_cost = cost\n                        self.total_cost += turn_cost\n                        logger.info(f\"Turn Cost: ${turn_cost:.6f}, Total Cost: ${self.total_cost:.6f}\")\n                        span.set_attribute(\"llm_cost\", turn_cost)\n                        span.set_attribute(\"total_agent_cost\", self.total_cost)\n                    self.last_llm_result = None # Clear after use\n                except Exception as cost_e:\n                    logger.warning(f\"Failed to calculate cost: {cost_e}\")\n                    span.add_event(\"Cost calculation failed\", attributes={\"error\": str(cost_e)})\n\n\n            # 8. Run Post Callback\n            if self.post_run_callback and not response.startswith(\"Error:\"):\n                try:\n                    if iscoroutinefunction(self.post_run_callback):\n                        await self.post_run_callback(session_id, response, turn_cost)\n                    else:\n                        self.post_run_callback(session_id, response, turn_cost)\n                    span.add_event(\"Post-run callback executed\")\n                except Exception as cb_e:\n                    logger.error(f\"Post-run callback failed: {cb_e}\", exc_info=True)\n                    span.record_exception(cb_e)\n\n\n            logger.info(f\"Agent Run finished in {time.monotonic() - start_time:.2f}s. Response: {response[:100]}...\")\n\n    except Exception as e:\n        logger.error(f\"Error during agent run (Session: {session_id}): {e}\", exc_info=True)\n        self.internal_state = InternalAgentState.ERROR\n        response = f\"Error: An internal error occurred during processing: {str(e)}\"\n        if span:\n             span.set_status(trace.Status(trace.StatusCode.ERROR, f\"Agent run failed: {e}\"))\n             span.record_exception(e)\n    finally:\n        self.internal_state = InternalAgentState.IDLE\n        if span: span.end() # Ensure span is closed\n        logger.info(f\"--- Agent Run End (Session: {session_id}) ---\")\n\n    return str(response) # Ensure string output\n</code></pre> <code>a_run_llm_completion(llm_messages, **kwargs)</code> <code>async</code> \u00b6 <p>Core wrapper around LiteLLM acompletion with error handling, streaming, and cost tracking.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def a_run_llm_completion(self, llm_messages: list[dict], **kwargs) -&gt; str:\n    \"\"\"Core wrapper around LiteLLM acompletion with error handling, streaming, and cost tracking.\"\"\"\n    if not llm_messages:\n        logger.warning(\"a_run_llm_completion called with empty message list.\")\n        return \"Error: No message provided to the model.\"\n\n    self.print_verbose(f\"Running model '{self.amd.model}' with {len(llm_messages)} messages.\")\n    # self.print_verbose(\"Messages:\", json.dumps(llm_messages, indent=2)) # Very verbose\n\n    # Prepare LiteLLM parameters from AgentModelData and kwargs overrides\n    params = {\n        'model': self.format_model or self.amd.model,\n        'messages': llm_messages,\n        'temperature': self.amd.temperature,\n        'top_p': self.amd.top_p,\n        'top_k': self.amd.top_k,\n        'max_tokens': self.amd.max_tokens,\n        'stream': self.stream,\n        'stop': self.amd.stop_sequence,\n        'user': self.amd.user_id,\n        'api_base': self.amd.api_base,\n        'api_version': self.amd.api_version,\n        'api_key': self.amd.api_key,\n        'presence_penalty': self.amd.presence_penalty,\n        'frequency_penalty': self.amd.frequency_penalty,\n        'caching': self.amd.caching,\n        'response_format': kwargs.get('response_format'), # For a_format_class\n        'tools': kwargs.get('tools'), # For LiteLLM function calling (less common now with ADK)\n    }\n    # Filter out None values as LiteLLM prefers absence over None for some params\n    params = {k: v for k, v in params.items() if v is not None}\n\n    # Add budget manager if present\n    if self.amd.budget_manager: params['budget_manager'] = self.amd.budget_manager\n\n    full_response_content = \"\"\n    tool_calls_requested = None # Store tool calls if generated\n\n    try:\n        response_object = await acompletion(**params)\n\n        if self.stream:\n            collected_chunks = []\n            async for chunk in response_object:\n                # Store raw chunk for potential analysis or replay\n                collected_chunks.append(chunk)\n                # Extract text delta\n                chunk_delta = chunk.choices[0].delta.content or \"\"\n                if chunk_delta:\n                    full_response_content += chunk_delta\n                    if self.stream_callback:\n                         try:\n                             # Provide only the new text chunk\n                             if iscoroutinefunction(self.stream_callback): await self.stream_callback(chunk_delta)\n                             else: self.stream_callback(chunk_delta)\n                         except Exception as cb_e:\n                             logger.warning(f\"Stream callback failed: {cb_e}\")\n                # Check for tool call deltas (less common in streaming)\n                tool_deltas = chunk.choices[0].delta.tool_calls\n                if tool_deltas:\n                     logger.warning(\"Received tool call delta during streaming - handling may be incomplete.\")\n                     # : Implement robust handling of streaming tool calls if needed\n\n            # After stream, construct a final response object mimicking non-streaming one for cost tracking\n            # This is an approximation, LiteLLM might offer better ways.\n            final_choice = {\"message\": {\"role\": \"assistant\", \"content\": full_response_content}}\n            # If tool calls were detected during streaming, add them (complex to reconstruct accurately)\n            # if reconstructed_tool_calls: final_choice[\"message\"][\"tool_calls\"] = reconstructed_tool_calls\n            self.last_llm_result = {\n                \"choices\": [{\"message\": final_choice[\"message\"]}],\n                \"model\": self.amd.model, # Needed for cost tracking\n                # Usage stats are often missing or zero in streaming chunks, need final value if available\n                \"usage\": getattr(collected_chunks[-1], 'usage', {\"prompt_tokens\": 0, \"completion_tokens\": 0})\n            }\n\n        else: # Non-streaming\n            self.last_llm_result = response_object # Store the full response\n            # Extract content and potential tool calls\n            message = response_object.choices[0].message\n            full_response_content = message.content or \"\"\n            tool_calls_requested = message.tool_calls # List of ToolCall objects\n\n            # Check if LiteLLM did function/tool calling (different from ADK tools)\n            # This path is less likely if using ADK, but supported by LiteLLM\n            if tool_calls_requested:\n                logger.info(f\"LiteLLM requested {len(tool_calls_requested)} tool calls.\")\n                # This requires a separate mechanism to execute these LiteLLM-requested tools\n                # and send back 'tool' role messages in the next turn.\n                # Not implemented here as focus is on ADK/A2A tools.\n                # For now, return a message indicating tool call request.\n                calls_repr = \", \".join([f\"{tc.function.name}\" for tc in tool_calls_requested])\n                return f\"Info: LLM requested tool calls ({calls_repr}). Direct execution not implemented.\"\n\n\n        self.print_verbose(f\"Model Response: {full_response_content[:100]}...\")\n        return full_response_content\n\n    except RateLimitError as e:\n        logger.error(f\"Rate limit error from {self.amd.model}: {e}\")\n        # Implement backoff/retry? For now, re-raise.\n        raise\n    except (BadRequestError, APIConnectionError, InternalServerError) as e:\n        logger.error(f\"API/Server error during LiteLLM call for {self.amd.model}: {e}\", exc_info=True)\n        raise\n    except Exception as e:\n        logger.error(f\"Unexpected error during LiteLLM completion: {e}\", exc_info=True)\n        raise\n</code></pre> <code>adk_tool_a2a_cancel_task(tool_context, target_agent_url, task_id)</code> <code>async</code> \u00b6 <p>ADK Tool: Attempts to cancel an ongoing A2A task.</p> <p>Parameters:</p> Name Type Description Default <code>target_agent_url</code> <code>str</code> <p>The URL of the agent hosting the task.</p> required <code>task_id</code> <code>str</code> <p>The ID of the task to cancel.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary indicating success or failure, possibly with the task's state after cancellation attempt.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def adk_tool_a2a_cancel_task(self,\n                                   tool_context: ToolContext | None,\n                                   target_agent_url: str,\n                                   task_id: str\n                                   ) -&gt; dict[str, Any]:\n    \"\"\"ADK Tool: Attempts to cancel an ongoing A2A task.\n\n    Args:\n        target_agent_url: The URL of the agent hosting the task.\n        task_id: The ID of the task to cancel.\n\n    Returns:\n        A dictionary indicating success or failure, possibly with the task's state after cancellation attempt.\n    \"\"\"\n    if not A2A_AVAILABLE: return {\"error\": \"python-a2a library not available.\"}\n    logger.info(f\"[ADK Tool] a2a_cancel_task called for task {task_id} on {target_agent_url}\")\n\n    try:\n        client = await self.setup_a2a_client(target_agent_url)\n        if not client:\n            return {\"error\": f\"Could not connect to A2A agent at {target_agent_url}\"}\n\n        if not hasattr(client, 'cancel_task'):\n            return {\"error\": f\"A2A client for {target_agent_url} does not support 'cancel_task'.\"}\n\n        # Call the client's cancel method\n        # The response structure depends heavily on the library implementation\n        cancel_response = await client.cancel_task(task_id=task_id)\n\n        # Parse response - could be simple success/fail, or updated task state\n        if isinstance(cancel_response, dict):\n            if 'error' in cancel_response:\n                error_info = cancel_response['error']\n                err_msg = error_info.get('message', str(error_info)) if isinstance(error_info, dict) else str(\n                    error_info)\n                logger.warning(f\"A2A cancel_task failed for {task_id}: {err_msg}\")\n                return {\"success\": False, \"error\": err_msg, \"raw_response\": cancel_response}\n            else:\n                # Assume success, response might contain updated task state\n                logger.info(f\"A2A task {task_id} cancellation requested successfully.\")\n                # Try to extract state if returned\n                state = cancel_response.get('result', {}).get('status', {}).get('state', 'UNKNOWN')\n                return {\"success\": True, \"state_after_request\": state, \"raw_response\": cancel_response}\n        elif cancel_response is True:  # Simple boolean success\n            return {\"success\": True, \"state_after_request\": \"UNKNOWN\"}\n        else:  # Assume object-like or other structure\n            # Add parsing based on observed python-a2a behavior\n            logger.info(f\"A2A task {task_id} cancellation request sent, parsing result.\")\n            # Example: Check for specific attributes if object is returned\n            state = getattr(getattr(getattr(cancel_response, 'result', None), 'status', None), 'state',\n                            TaskState.UNKNOWN).value\n            return {\"success\": True, \"state_after_request\": state,\n                    \"raw_response\": vars(cancel_response) if hasattr(cancel_response, '__dict__') else str(\n                        cancel_response)}\n\n\n    except Exception as e:\n        # Catch specific errors like TaskNotFound, TaskNotCancelable if defined by python-a2a\n        # if isinstance(e, TaskNotFoundError):\n        #    return {\"success\": False, \"error\": f\"Task {task_id} not found.\"}\n        # if isinstance(e, TaskNotCancelableError):\n        #    return {\"success\": False, \"error\": f\"Task {task_id} is not in a cancelable state.\"}\n        logger.error(f\"[ADK Tool] a2a_cancel_task failed: {e}\", exc_info=True)\n        return {\"success\": False, \"error\": f\"Error cancelling A2A task: {e}\"}\n</code></pre> <code>adk_tool_a2a_get_task_status(tool_context, target_agent_url, task_id)</code> <code>async</code> \u00b6 <p>ADK Tool: Gets the current status and details of an A2A task.</p> <p>Parameters:</p> Name Type Description Default <code>target_agent_url</code> <code>str</code> <p>The URL of the agent hosting the task.</p> required <code>task_id</code> <code>str</code> <p>The ID of the task to check.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing task status details (state, message, artifacts) or an error.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def adk_tool_a2a_get_task_status(self,\n                                       tool_context: ToolContext | None,\n                                       target_agent_url: str,\n                                       task_id: str\n                                       ) -&gt; dict[str, Any]:\n    \"\"\"ADK Tool: Gets the current status and details of an A2A task.\n\n    Args:\n        target_agent_url: The URL of the agent hosting the task.\n        task_id: The ID of the task to check.\n\n    Returns:\n        A dictionary containing task status details (state, message, artifacts) or an error.\n    \"\"\"\n    if not A2A_AVAILABLE: return {\"error\": \"python-a2a library not available.\"}\n    logger.info(f\"[ADK Tool] a2a_get_task_status called for task {task_id} on {target_agent_url}\")\n\n    try:\n        client = await self.setup_a2a_client(target_agent_url)\n        if not client:\n            return {\"error\": f\"Could not connect to A2A agent at {target_agent_url}\"}\n\n        if not hasattr(client, 'get_task'):\n            return {\"error\": f\"A2A client for {target_agent_url} does not support 'get_task'.\"}\n\n        # Get task details from the client\n        task_details = await client.get_task(task_id=task_id, history_length=1)  # History=1 gets latest status\n\n        # Parse and return relevant info\n        if isinstance(task_details, dict):\n            # Basic parsing, adjust based on actual python-a2a structure\n            status_info = task_details.get('status', {})\n            artifacts = task_details.get('artifacts')\n            return {\n                \"task_id\": task_id,\n                \"state\": status_info.get('state', 'UNKNOWN'),\n                \"status_message\": status_info.get('message'),  # Might be complex object\n                \"artifacts\": artifacts,  # Might be complex list\n                \"raw_response\": task_details  # Include raw for debugging\n            }\n        elif hasattr(task_details, 'status'):  # Object-like response\n            status_obj = task_details.status\n            artifacts_obj = getattr(task_details, 'artifacts', None)\n            return {\n                \"task_id\": task_id,\n                \"state\": getattr(status_obj, 'state', TaskState.UNKNOWN).value,  # Get enum value\n                \"status_message\": getattr(status_obj, 'message', None),\n                \"artifacts\": artifacts_obj,\n                \"raw_response\": vars(task_details)  # Example conversion\n            }\n        else:\n            return {\"error\": \"Received unexpected response structure from get_task.\", \"raw_response\": task_details}\n\n    except Exception as e:\n        # Catch specific errors from python-a2a if they exist (e.g., TaskNotFoundError)\n        # if isinstance(e, TaskNotFoundError):\n        #    logger.warning(f\"[ADK Tool] A2A Task {task_id} not found on {target_agent_url}.\")\n        #    return {\"error\": f\"Task {task_id} not found.\"}\n        logger.error(f\"[ADK Tool] a2a_get_task_status failed: {e}\", exc_info=True)\n        return {\"error\": f\"Error getting A2A task status: {e}\"}\n</code></pre> <code>adk_tool_a2a_send_and_wait(tool_context, target_agent_url, task_prompt, session_id=None)</code> <code>async</code> \u00b6 <p>ADK Tool: Sends a task to another agent via A2A and waits for the final text result.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def adk_tool_a2a_send_and_wait(self,\n                                     tool_context: ToolContext | None,\n                                     target_agent_url: str,\n                                     task_prompt: str,\n                                     session_id: str | None = None\n                                     ) -&gt; str:\n    \"\"\"ADK Tool: Sends a task to another agent via A2A and waits for the final text result.\"\"\"\n    # ... (implementation remains the same, calls _execute_a2a_call) ...\n    if not A2A_AVAILABLE: return \"Error: python-a2a library not available.\"\n    logger.info(f\"[ADK Tool] a2a_send_and_wait called for target: {target_agent_url}\")\n    tool_session_id = session_id or f\"adk_tool_a2a_{uuid.uuid4()}\"\n    try:\n        return await self._execute_a2a_call(\n             user_input=task_prompt,\n             session_id=tool_session_id,\n             target_a2a_agent_url=target_agent_url,\n             a2a_task_prompt=task_prompt\n        )\n    except Exception as e:\n         logger.error(f\"[ADK Tool] a2a_send_and_wait failed: {e}\", exc_info=True)\n         return f\"Error executing A2A task via ADK tool: {e}\"\n</code></pre> <code>adk_tool_a2a_send_no_wait(tool_context, target_agent_url, task_prompt, session_id=None)</code> <code>async</code> \u00b6 <p>ADK Tool: Sends a task to another agent via A2A and returns the task ID immediately.</p> <p>Parameters:</p> Name Type Description Default <code>target_agent_url</code> <code>str</code> <p>The full URL of the target A2A agent.</p> required <code>task_prompt</code> <code>str</code> <p>The natural language prompt or task for the target agent.</p> required <code>session_id</code> <code>str | None</code> <p>Optional session ID to use for the A2A interaction.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The unique ID of the submitted A2A task, or an error message.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def adk_tool_a2a_send_no_wait(self,\n                                    tool_context: ToolContext | None,\n                                    target_agent_url: str,\n                                    task_prompt: str,\n                                    session_id: str | None = None\n                                    ) -&gt; str:\n    \"\"\"ADK Tool: Sends a task to another agent via A2A and returns the task ID immediately.\n\n    Args:\n        target_agent_url: The full URL of the target A2A agent.\n        task_prompt: The natural language prompt or task for the target agent.\n        session_id: Optional session ID to use for the A2A interaction.\n\n    Returns:\n        The unique ID of the submitted A2A task, or an error message.\n    \"\"\"\n    if not A2A_AVAILABLE: return \"Error: python-a2a library not available.\"\n    logger.info(f\"[ADK Tool] a2a_send_no_wait called for target: {target_agent_url}\")\n\n    try:\n        client = await self.setup_a2a_client(target_agent_url)\n        if not client:\n            return f\"Error: Could not connect to A2A agent at {target_agent_url}\"\n\n        task_id = str(uuid.uuid4())\n        a2a_session_id = session_id or f\"a2a_tool_nowait_{task_id[:8]}\"\n\n        message_payload = {\"role\": \"user\", \"content\": {\"type\": \"text\", \"text\": task_prompt}}\n\n        initial_task_info = None\n        if hasattr(client, 'send_task'):\n            initial_task_info = await client.send_task(message=message_payload, task_id=task_id,\n                                                       session_id=a2a_session_id)\n        elif hasattr(client, 'create_task'):\n            initial_task_info = await client.create_task(message=message_payload, task_id=task_id,\n                                                         session_id=a2a_session_id)\n        else:\n            return \"Error: A2A client does not support send_task or create_task.\"\n\n        # Check for immediate errors from the submission call\n        # Structure depends on python-a2a's return value\n        error_info = None\n        if isinstance(initial_task_info, dict):\n            error_info = initial_task_info.get('error')\n        elif hasattr(initial_task_info, 'error'):\n            error_info = initial_task_info.error\n\n        if error_info:\n            err_msg = error_info.get('message', str(error_info)) if isinstance(error_info, dict) else str(\n                error_info)\n            logger.error(f\"A2A send_task (no wait) failed immediately: {err_msg}\")\n            return f\"Error submitting A2A task: {err_msg}\"\n        else:\n            logger.info(f\"A2A task '{task_id}' submitted successfully (no wait) to {target_agent_url}.\")\n            return task_id  # Return the ID for later polling/checking\n\n    except Exception as e:\n        logger.error(f\"[ADK Tool] a2a_send_no_wait failed: {e}\", exc_info=True)\n        return f\"Error sending A2A task (no wait): {e}\"\n</code></pre> <code>adk_tool_world_model_get(tool_context, key)</code> <code>async</code> \u00b6 <p>ADK Tool: Retrieves a specific value from the agent's world model.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def adk_tool_world_model_get(self, tool_context: ToolContext | None, key: str) -&gt; Any | None:\n    \"\"\"ADK Tool: Retrieves a specific value from the agent's world model.\"\"\"\n    # ... (implementation remains the same) ...\n    logger.info(f\"[ADK Tool] get_world_model_key called for key: {key}\")\n    return self.world_model.get(key)\n</code></pre> <code>adk_tool_world_model_show(tool_context)</code> <code>async</code> \u00b6 <p>ADK Tool: Returns a string representation of the agent's entire world model.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def adk_tool_world_model_show(self, tool_context: ToolContext | None) -&gt; str:\n    \"\"\"ADK Tool: Returns a string representation of the agent's entire world model.\"\"\"\n    # ... (implementation remains the same) ...\n    logger.info(\"[ADK Tool] show_world_model called\")\n    return self.world_model.show()\n</code></pre> <code>close()</code> <code>async</code> \u00b6 <p>Gracefully close connections and resources.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def close(self):\n    \"\"\"Gracefully close connections and resources.\"\"\"\n    logger.info(f\"Closing resources for agent '{self.amd.name}'...\")\n    # Close A2A resources\n    if self.a2a_server and hasattr(self.a2a_server, 'stop'): # Check if server has stop method\n         logger.info(\"Stopping A2A server...\")\n         try:\n             await self.a2a_server.stop() # Assuming stop is async\n         except Exception as e: logger.warning(f\"Error stopping A2A server: {e}\")\n    if hasattr(self, '_a2a_task_manager_instance') and hasattr(self._a2a_task_manager_instance, 'close'):\n         logger.info(\"Closing A2A task manager...\")\n         await self._a2a_task_manager_instance.close()\n    await self.close_a2a_clients()\n\n    # Close MCP server if running\n    if self.mcp_server and hasattr(self.mcp_server, 'stop'): # Check for stop method\n         logger.info(\"Stopping MCP server...\")\n         try:\n             # MCP server run is blocking, stop might need separate mechanism\n             # or be handled by process termination. If stop method exists:\n             # await self.mcp_server.stop() # Assuming async stop\n             logger.warning(\"MCP server 'stop' might need manual implementation or process signal.\")\n         except Exception as e: logger.warning(f\"Error stopping MCP server: {e}\")\n\n\n    # Close ADK resources (MCPToolset connections managed by exit stack)\n    if self.adk_exit_stack:\n        logger.info(\"Closing ADK AsyncExitStack (manages MCPToolset connections)...\")\n        try:\n            await self.adk_exit_stack.aclose()\n        except Exception as e:\n            logger.warning(f\"Error closing ADK exit stack: {e}\")\n\n    # Close ADK runner if it has a close method\n    if self.adk_runner and hasattr(self.adk_runner, 'close'):\n         logger.info(\"Closing ADK runner...\")\n         try:\n              # Check if close is async\n             if iscoroutinefunction(self.adk_runner.close):\n                 await self.adk_runner.close()\n             else:\n                 self.adk_runner.close()\n         except Exception as e: logger.warning(f\"Error closing ADK runner: {e}\")\n\n\n    logger.info(f\"Agent '{self.amd.name}' resource cleanup finished.\")\n</code></pre> <code>close_a2a_clients()</code> <code>async</code> \u00b6 <p>Closes all cached A2A client connections.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def close_a2a_clients(self):\n    \"\"\"Closes all cached A2A client connections.\"\"\"\n    async with self.a2a_client_lock:\n        logger.info(f\"Closing {len(self.a2a_clients)} A2A clients.\")\n        # A2AClient may manage underlying httpx clients automatically.\n        # If explicit close needed in future versions, add here.\n        # for client in self.a2a_clients.values():\n        #     await client.close() # If available\n        self.a2a_clients.clear()\n</code></pre> <code>construct_initial_prompts()</code> \u00b6 <p>Constructs the initial system/context messages for the LLM prompt.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def construct_initial_prompts(self) -&gt; list[dict]:\n    \"\"\"Constructs the initial system/context messages for the LLM prompt.\"\"\"\n    messages = []\n    # Base System Prompt\n    if self.amd.system_message:\n        messages.append(LLMMessage(\"system\", self.amd.system_message).to_dict())\n\n    # World Model Context\n    wm_repr = self.world_model.show()\n    if wm_repr != \"[empty]\":\n        messages.append(LLMMessage(\"system\", f\"Current World State:\\n{wm_repr}\").to_dict())\n\n    # Capabilities Overview (ADK specific parts depend on LlmAgent inheritance)\n    caps = [\"LiteLLM (Core LLM access)\"]\n    if ADK_AVAILABLE and isinstance(self, LlmAgent):\n        if self.tools: caps.append(\"ADK Tools (including potential MCP/A2A wrappers)\")\n        if self.code_executor: caps.append(\"ADK Code Execution\")\n        if any(isinstance(t, type(adk_google_search) | AdkVertexAiSearchTool) for t in getattr(self, 'tools', [])):\n             caps.append(\"ADK Search\")\n    if A2A_AVAILABLE and self.a2a_clients: caps.append(\"A2A Client (delegate to other agents)\")\n    if self.mcp_server: caps.append(\"MCP Server (exposes capabilities)\")\n    if self.a2a_server: caps.append(\"A2A Server (receives tasks)\")\n\n    messages.append(LLMMessage(\"system\", f\"Your Capabilities: {', '.join(caps)}.\").to_dict())\n\n    # ADK Tool Instructions (if ADK enabled and tools exist)\n    if ADK_AVAILABLE and isinstance(self, LlmAgent) and self.tools:\n        try:\n            # Use ADK's internal method to get schema if possible, otherwise basic list\n            tool_schemas = getattr(self, 'tool_schemas', None) # ADK might populate this\n            if tool_schemas:\n                 tool_list_str = json.dumps(tool_schemas, indent=2)\n                 messages.append(LLMMessage(\"system\", f\"You have access to the following tools (use FunctionCall format):\\n{tool_list_str}\").to_dict())\n            else: # Fallback to basic list\n                tool_list = \"\\n\".join([f\"- {tool.name}: {tool.description or 'No description'}\" for tool in self.tools])\n                messages.append(LLMMessage(\"system\", f\"You can use the following tools:\\n{tool_list}\\nRespond with a FunctionCall to use a tool.\").to_dict())\n        except Exception as e:\n             logger.warning(f\"Could not generate detailed ADK tool instructions: {e}\")\n\n\n    # Add specific instructions for A2A delegation if needed\n    if A2A_AVAILABLE and self.a2a_clients:\n         client_names = list(self.a2a_clients.keys()) # Target URLs act as names here\n         messages.append(LLMMessage(\"system\", f\"You can delegate tasks to other agents via A2A using their URLs (e.g., {client_names[0]} if available). Indicate clearly if you want to delegate.\").to_dict())\n\n    return messages\n</code></pre> <code>flow_world_model(text_input, session_id, adk_session_state)</code> <code>async</code> \u00b6 <p>Analyzes input, updates internal WorldModel, and syncs with ADK state if enabled. Sync Priority: If ADK state exists, sync from it first. Then update based on text.              The sync to ADK happens after the agent run completes.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def flow_world_model(self, text_input: str, session_id: str, adk_session_state: State | None):\n    \"\"\"\n    Analyzes input, updates internal WorldModel, and syncs with ADK state if enabled.\n    Sync Priority: If ADK state exists, sync *from* it first. Then update based on text.\n                 The sync *to* ADK happens after the agent run completes.\n    \"\"\"\n    logger.debug(f\"Flowing world model based on text: {text_input[:100]}...\")\n\n    # 1. Sync FROM ADK State (if enabled and state available)\n    if self.sync_adk_state and adk_session_state is not None:\n         logger.debug(\"Syncing World Model FROM ADK session state...\")\n         self.world_model.sync_from_adk_state(adk_session_state)\n\n    # 2. Update World Model based on Text Input (using LLM)\n    # This adds/modifies based on the current turn's input\n    # Define Pydantic model for structured update extraction\n    current_keys = list(self.world_model.to_dict().keys())\n    class WorldModelAdaption(BaseModel):\n        action: Literal['add', 'update', 'remove', 'none'] = Field(..., description=\"Action on the world model.\")\n        key: str | None = Field(None, description=f\"Key to modify/add/remove (e.g., 'user_location', 'task_status'). Existing keys: {current_keys}\")\n        value: Any | None = Field(None, description=\"New value (for 'add'/'update'). Should be JSON serializable.\")\n        reasoning: str = Field(..., description=\"Why this change (or no change) is needed based on the input.\")\n\n    prompt = (f\"Analyze the following text and current world state to determine if the agent's world model needs changes.\\n\"\n              f\"Current World State Keys: {current_keys}\\n\"\n              f\"Text Input: ```\\n{text_input}\\n```\\n\"\n              f\"Decide action, key, value, and reasoning. Focus on factual updates derived *from the text*. Do not hallucinate.\")\n\n    try:\n        # Use a potentially faster/cheaper model for this classification task\n        # Could eventually use a separate AMD config for this call\n        adaption_dict = await self.a_format_class(WorldModelAdaption, prompt)\n        adaption = WorldModelAdaption(**adaption_dict)\n\n        logger.info(f\"World Model Adaption proposed: {adaption.action} on key '{adaption.key}'. Reason: {adaption.reasoning}\")\n\n        if adaption.action == 'add' or adaption.action == 'update':\n            if adaption.key and adaption.value is not None:\n                self.world_model.set(adaption.key, adaption.value)\n            else:\n                logger.warning(\"World model 'add'/'update' ignored: missing key or value.\")\n        elif adaption.action == 'remove':\n            if adaption.key:\n                self.world_model.remove(adaption.key)\n            else:\n                logger.warning(\"World model 'remove' ignored: missing key.\")\n        # Else ('none'): do nothing\n\n    except (ValidationError, Exception) as e:\n        logger.warning(f\"Failed to determine world model adaption via LLM: {e}. World model may be based only on ADK sync or previous state.\")\n</code></pre> <code>print_verbose(*args)</code> \u00b6 <p>Conditional logging helper.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def print_verbose(self, *args):\n    \"\"\"Conditional logging helper.\"\"\"\n    if self.verbose:\n        logger.debug(' '.join(map(str, args)))\n</code></pre> <code>run(user_input, session_id=None, **kwargs)</code> \u00b6 <p>Synchronous wrapper for a_run.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def run(self, user_input: str, session_id: str | None = None, **kwargs) -&gt; str:\n    \"\"\"Synchronous wrapper for a_run.\"\"\"\n    try:\n        # get_event_loop() is deprecated in 3.10+, use get_running_loop() or new_event_loop()\n        try:\n            asyncio.get_running_loop()\n            # If loop is running, cannot use asyncio.run. Need to schedule and wait.\n            # This is complex to get right universally (e.g., in notebooks vs servers).\n            # Simplest approach for sync call from sync context is asyncio.run()\n            # If called from async context, user should await a_run() directly.\n            logger.warning(\"Synchronous 'run' called from a running event loop. \"\n                           \"This might block the loop. Consider using 'await a_run'.\")\n            # Fallback to basic run, may error if loop is running\n            return asyncio.run(self.a_run(user_input, session_id=session_id, **kwargs))\n        except RuntimeError: # No running event loop\n             return asyncio.run(self.a_run(user_input, session_id=session_id, **kwargs))\n    except Exception as e:\n        logger.error(f\"Error in synchronous run wrapper: {e}\", exc_info=True)\n        return f\"Error: Failed to execute synchronous run: {e}\"\n</code></pre> <code>run_a2a_server(host='0.0.0.0', port=5000, **kwargs)</code> \u00b6 <p>Starts the A2A server (blocking) using the python-a2a run_server function.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def run_a2a_server(self, host=\"0.0.0.0\", port=5000, **kwargs):\n    \"\"\"Starts the A2A server (blocking) using the python-a2a run_server function.\"\"\"\n    if not self.a2a_server:\n        logger.error(\"A2A server not initialized. Call setup_a2a_server first.\")\n        return\n    if not A2A_AVAILABLE:\n        logger.error(\"python-a2a library not available. Cannot run A2A server.\")\n        return\n\n    # Get effective host/port from server instance if set, otherwise use args\n    effective_host = getattr(self.a2a_server, 'host', host)\n    effective_port = getattr(self.a2a_server, 'port', port)\n\n    logger.info(f\"Starting A2A server for agent '{self.amd.name}' via run_server_func on {effective_host}:{effective_port}...\")\n    try:\n        # Call the imported run_server function, passing the agent instance\n        run_a2a_server_func(self.a2a_server, host=effective_host, port=effective_port, **kwargs) # This blocks\n    except Exception as e:\n        logger.error(f\"A2A server failed to run: {e}\", exc_info=True)\n</code></pre> <code>run_mcp_server(transport='sse', **kwargs)</code> \u00b6 <p>Starts the MCP server (blocking).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def run_mcp_server(self, transport='sse', **kwargs):\n    \"\"\"Starts the MCP server (blocking).\"\"\"\n    if not self.mcp_server:\n        logger.error(\"MCP server not initialized. Call setup_mcp_server first.\")\n        return\n    if not MCP_AVAILABLE:\n         logger.error(\"MCP library not available. Cannot run MCP server.\")\n         return\n    logger.info(f\"Starting MCP server for agent '{self.amd.name}' using {transport} transport...\")\n    # This is blocking, run in a separate process/thread for a long-running agent\n    try:\n        self.mcp_server.run(transport=transport, **kwargs)\n    except Exception as e:\n        logger.error(f\"MCP server failed to run: {e}\", exc_info=True)\n</code></pre> <code>setup_a2a_client(target_agent_url)</code> <code>async</code> \u00b6 <p>Gets or creates an A2A client for a specific target agent URL using python-a2a.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def setup_a2a_client(self, target_agent_url: str) -&gt; A2AClient | None:\n    \"\"\"Gets or creates an A2A client for a specific target agent URL using python-a2a.\"\"\"\n    if not A2A_AVAILABLE:\n        logger.warning(\"python-a2a library not installed. Cannot setup A2A client.\")\n        return None\n\n    async with self.a2a_client_lock:\n        if target_agent_url in self.a2a_clients:\n            logger.debug(f\"Reusing cached A2A client for {target_agent_url}\")\n            return self.a2a_clients[target_agent_url]\n\n        logger.info(f\"Setting up A2A client for target: {target_agent_url}\")\n        try:\n            # python-a2a client likely fetches card on init or first call\n            client = A2AClient(base_url=target_agent_url) # Pass the URL directly\n            # Verify connection implicitly by getting card (optional, client might do lazy loading)\n            # agent_card = await client.get_agent_card() # If method exists\n            # logger.info(f\"Successfully connected A2A client to agent: {agent_card.name}\")\n            self.a2a_clients[target_agent_url] = client\n            logger.info(f\"A2A client created for target: {target_agent_url}\")\n            return client\n        except Exception as e:\n            logger.error(f\"Failed to setup A2A client for {target_agent_url}: {e}\", exc_info=True)\n            return None\n</code></pre> <code>setup_a2a_server(host='0.0.0.0', port=5000, **a2a_server_options)</code> \u00b6 <p>Initialize and configure the A2A server capabilities using python-a2a. This dynamically creates a server class with the agent's capabilities.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def setup_a2a_server(self, host=\"0.0.0.0\", port=5000, **a2a_server_options):\n    \"\"\"\n    Initialize and configure the A2A server capabilities using python-a2a.\n    This dynamically creates a server class with the agent's capabilities.\n    \"\"\"\n    if not A2A_AVAILABLE:\n        logger.warning(\"python-a2a library not installed. Cannot setup A2A server.\")\n        return None\n    if self.a2a_server:\n        logger.warning(\"A2A server already initialized.\")\n        return self.a2a_server\n\n    logger.info(f\"Setting up A2A server for agent '{self.amd.name}' on {host}:{port}\")\n\n    agent_instance = self # Reference to the current EnhancedAgent instance\n\n    # Define the A2A Server class dynamically using the decorator\n    @a2a_agent_decorator(\n        name=self.amd.name or \"EnhancedAgent\",\n        description=f\"Enhanced Agent '{self.amd.name}' - Capabilities: ADK({ADK_AVAILABLE}), MCP({MCP_AVAILABLE}), A2A({A2A_AVAILABLE})\",\n        version=\"1.0.0\",\n        # Other AgentCard fields...\n    )\n    class DynamicA2AServer(A2AServer):\n        bound_agent: EnhancedAgent = agent_instance\n\n        def handle_task(self, task: Task) -&gt; Task:\n            \"\"\" Handles incoming A2A tasks by calling the EnhancedAgent's async logic. \"\"\"\n            # --- (handle_task implementation remains the same as before) ---\n            logger.info(f\"[A2A Server {self.bound_agent.amd.name}] Received task: {task.id}\")\n            async def run_agent_async():\n                # ... (logic to extract prompt, call a_run, update task) ...\n                try:\n                    user_prompt = \"\"\n                    # ... (extract user_prompt from task.message) ...\n                    if task.message and task.message.get(\"content\"):\n                        content = task.message[\"content\"]\n                        if isinstance(content, dict) and content.get(\"type\") == \"text\":\n                            user_prompt = content.get(\"text\", \"\").strip()\n                        elif isinstance(content, str):\n                            user_prompt = content.strip()\n\n                    if not user_prompt:\n                        raise ValueError(\"Task message has no text content.\")\n\n                    session_id = task.message.get(\"session_id\", task.id)\n                    agent_response = await self.bound_agent.a_run(\n                        user_prompt,\n                        session_id=session_id,\n                        persist_history=False,\n                        a2a_task_id=task.id\n                    )\n                    task.artifacts = [{\"parts\": [{\"type\": \"text\", \"text\": str(agent_response)}]}]\n                    task.status = TaskStatus(state=TaskState.COMPLETED)\n                except Exception as e:\n                    # ... (error handling) ...\n                    logger.error(f\"[A2A Task {task.id}] Error during processing: {e}\", exc_info=True)\n                    error_msg = f\"Internal agent error: {str(e)}\"\n                    task.artifacts = [{\"parts\": [{\"type\": \"text\", \"text\": error_msg}]}]\n                    task.status = TaskStatus(state=TaskState.FAILED, message={\"role\": \"agent\", \"content\": {\"type\": \"text\", \"text\": error_msg}})\n                return task\n            try:\n                updated_task = asyncio.run(run_agent_async())\n                return updated_task\n            except RuntimeError as e:\n                # ... (handle RuntimeError) ...\n                logger.error(f\"RuntimeError calling asyncio.run in handle_task: {e}.\")\n                task.status = TaskStatus(state=TaskState.FAILED, message={\"role\": \"agent\", \"content\": {\"type\": \"text\", \"text\": \"Internal Server Error processing task asynchronously.\"}})\n                return task\n            # --- (end of handle_task logic) ---\n\n\n        # --- Expose Skills ---\n        @a2a_skill_decorator(\n            name=\"General Query\",\n            description=\"Process general natural language queries using the agent's primary LLM.\",\n            examples=[\"What is the capital of France?\", \"Summarize the plot of Hamlet.\"]\n        )\n        def general_query_skill(self, query: str) -&gt; str:\n            \"\"\"Handles general queries via the skill mechanism by calling a_run.\"\"\"\n            logger.info(f\"[A2A Skill] Received general_query: {query[:50]}...\")\n            async def run_skill_async():\n                # Call a_run, forcing direct LLM strategy for simple queries\n                response = await self.bound_agent.a_run(\n                    query,\n                    a2a_task_id=f\"skill_query_{uuid.uuid4()}\",\n                    strategy_override=ProcessingStrategy.DIRECT_LLM,\n                    persist_history=False\n                    )\n                return response\n            try:\n                # Bridge sync skill call to async agent logic\n                return asyncio.run(run_skill_async())\n            except RuntimeError:\n                 logger.error(\"RuntimeError calling asyncio.run in general_query_skill.\")\n                 return \"Error: Could not process skill asynchronously.\"\n\n        # --- FIXED: Generic Skill for ADK Tools ---\n        if ADK_AVAILABLE and isinstance(agent_instance, LlmAgent) and agent_instance.tools:\n            # Check if there are any ADK tools to expose\n            adk_tool_list = [t for t in agent_instance.tools if isinstance(t, BaseTool)]\n            if adk_tool_list:\n                logger.info(f\"Exposing {len(adk_tool_list)} ADK tools via 'execute_adk_tool' A2A skill.\")\n\n                @a2a_skill_decorator(\n                    name=\"execute_adk_tool\",\n                    description=f\"Executes a registered ADK tool. Available tools: {', '.join([t.name for t in adk_tool_list])}\",\n                    examples=[\"Execute tool 'some_tool_name' with argument 'arg1'='value1'\"] # Generic example\n                )\n                def execute_adk_tool_skill(self, tool_name: str, arguments: dict[str, Any]) -&gt; str:\n                    \"\"\"Generic skill to execute an ADK tool by name with arguments.\"\"\"\n                    logger.info(f\"[A2A Skill] Request to execute ADK tool: {tool_name} with args: {arguments}\")\n\n                    # Find the ADK tool instance on the bound agent\n                    tool_to_call: BaseTool | None = None\n                    for tool in self.bound_agent.tools:\n                        if isinstance(tool, BaseTool) and tool.name == tool_name:\n                            tool_to_call = tool\n                            break\n\n                    if not tool_to_call:\n                        logger.warning(f\"[A2A Skill] ADK tool '{tool_name}' not found.\")\n                        return f\"Error: ADK tool '{tool_name}' not found on this agent.\"\n\n                    # --- Bridge sync skill call to async ADK tool execution ---\n                    async def run_adk_tool_async():\n                        try:\n                            # ADK tools require ToolContext. We can provide a minimal one or None.\n                            # Providing None might limit tool functionality.\n                            # Let's try providing None for simplicity first.\n                            adk_tool_context = None\n\n                            # Check if the tool has an async run method (most ADK tools should)\n                            if hasattr(tool_to_call, 'run_async') and iscoroutinefunction(tool_to_call.run_async):\n                                # Pass arguments directly to run_async\n                                result = await tool_to_call.run_async(args=arguments, tool_context=adk_tool_context)\n                                # Convert result to string for A2A response\n                                if isinstance(result, str): return result\n                                try: return json.dumps(result)\n                                except: return str(result)\n                            elif hasattr(tool_to_call, 'run') and callable(tool_to_call.run):\n                                # Fallback to synchronous run in thread pool\n                                logger.warning(f\"ADK tool '{tool_name}' has no run_async, using synchronous run in thread.\")\n                                result = await asyncio.to_thread(tool_to_call.run, args=arguments, tool_context=adk_tool_context)\n                                if isinstance(result, str): return result\n                                try: return json.dumps(result)\n                                except: return str(result)\n                            else:\n                                 return f\"Error: ADK tool '{tool_name}' has no callable run or run_async method.\"\n\n                        except Exception as e:\n                            logger.error(f\"[A2A Skill] Error executing ADK tool '{tool_name}': {e}\", exc_info=True)\n                            return f\"Error executing ADK tool {tool_name}: {e}\"\n\n                    # Execute the async tool runner\n                    try:\n                        return asyncio.run(run_adk_tool_async())\n                    except RuntimeError:\n                        logger.error(f\"RuntimeError calling asyncio.run in execute_adk_tool_skill for tool {tool_name}.\")\n                        return \"Error: Could not execute ADK tool asynchronously.\"\n\n        # --- End of Skill Definitions ---\n\n    # Instantiate the dynamic server class\n    try:\n         self.a2a_server = DynamicA2AServer(**a2a_server_options)\n         logger.info(f\"A2A server instance created for agent '{self.amd.name}'.\")\n         return self.a2a_server\n    except Exception as e:\n         logger.error(f\"Failed to instantiate dynamic A2A Server: {e}\", exc_info=True)\n         return None\n</code></pre> <code>setup_adk_runner(runner_options=None)</code> \u00b6 <p>Initializes an ADK runner for this agent (if ADK enabled).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def setup_adk_runner(self, runner_options: dict[str, Any] | None = None):\n    \"\"\"Initializes an ADK runner for this agent (if ADK enabled).\"\"\"\n    if not ADK_AVAILABLE:\n        logger.warning(\"ADK not available. Cannot setup ADK runner.\")\n        return None\n    if not isinstance(self, LlmAgent):\n        logger.error(\"Agent must inherit from LlmAgent to use ADK runner directly.\")\n        return None\n    if self.adk_runner:\n        logger.warning(\"ADK runner already initialized.\")\n        return self.adk_runner\n\n    runner_opts = runner_options or {}\n    runner_class = runner_opts.pop(\"runner_class\", InMemoryRunner) # Default to InMemory\n    app_name = runner_opts.pop(\"app_name\", f\"{self.amd.name}_ADKApp\")\n\n    if runner_class == InMemoryRunner:\n        runner_opts = {}\n\n    logger.info(f\"Setting up ADK Runner ({runner_class.__name__}) for app '{app_name}'...\")\n\n    try:\n         # Pass the agent instance and other options to the runner constructor\n        self.adk_runner = runner_class(agent=self, app_name=app_name, **runner_opts)\n        self.adk_session_service = self.adk_runner.session_service # Store session service\n        logger.info(f\"ADK {runner_class.__name__} setup complete for agent '{self.amd.name}'.\")\n        return self.adk_runner\n    except Exception as e:\n        logger.error(f\"Failed to setup ADK runner: {e}\", exc_info=True)\n        self.adk_runner = None\n        self.adk_session_service = None\n        return None\n</code></pre> <code>setup_mcp_server(host='0.0.0.0', port=8000, **mcp_kwargs)</code> \u00b6 <p>Initialize and configure the MCP server capabilities for this agent. This agent will ACT AS an MCP Server.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def setup_mcp_server(self, host=\"0.0.0.0\", port=8000, **mcp_kwargs):\n    \"\"\"Initialize and configure the MCP server capabilities *for this agent*.\n       This agent will ACT AS an MCP Server.\n    \"\"\"\n    if not MCP_AVAILABLE:\n        logger.warning(\"MCP library not installed. Cannot setup MCP server.\")\n        return None\n    if self.mcp_server:\n        logger.warning(\"MCP server already initialized.\")\n        return self.mcp_server\n    name = mcp_kwargs.get(\"name\")\n    del mcp_kwargs[\"name\"]\n    self.mcp_server = FastMCP(name=name or f\"{self.amd.name}-mcp-server\",\n                              description=f\"MCP interface for EnhancedAgent {self.amd.name}\",\n                              **mcp_kwargs)\n    logger.info(f\"Setting up MCP server for agent '{self.amd.name}' on {host}:{port}\")\n\n    # --- Register Agent's core functionalities as MCP services ---\n    # Example: Expose World Model (Read-only for safety)\n    @self.mcp_server.resource(f\"agent://{self.amd.name}/world_model\")\n    def mcp_get_world_model_resource() -&gt; dict[str, Any]:\n        \"\"\"Gets the agent's world model.\"\"\"\n        logger.debug(f\"[MCP Resource] agent://{self.amd.name}/world_model accessed\")\n        return self.world_model.to_dict()\n\n    # Example: Expose a simple query tool via MCP\n    @self.mcp_server.tool(name=\"simple_llm_query\")\n    async def mcp_simple_query(prompt: str) -&gt; str:\n        \"\"\"Sends a simple prompt to the agent's LLM (non-persistent run).\"\"\"\n        logger.debug(f\"[MCP Tool] simple_llm_query called: {prompt[:50]}...\")\n        # Use a minimal, non-persistent run, disable recursive calls\n        response = await self.a_run(\n            prompt, session_id=f\"mcp_query_{uuid.uuid4()}\",\n            persist_history=False, strategy_override=ProcessingStrategy.DIRECT_LLM\n        )\n        return response\n\n    # If ADK tools exist, potentially expose them via MCP automatically?\n    if ADK_AVAILABLE and isinstance(self, LlmAgent) and self.tools:\n         logger.info(\"Attempting to expose ADK tools via MCP server...\")\n         for adk_tool in self.tools:\n             if adk_tool.name in [\"code_execution\", \"adk_tool_a2a_send_and_wait\", \"adk_tool_a2a_send_no_wait\", \"adk_tool_a2a_get_task_status\", \"adk_tool_a2a_cancel_task\"]:\n                 continue\n             if not isinstance(adk_tool, BaseTool): continue\n             try:\n                 mcp_schema = adk_to_mcp_tool_type(adk_tool)\n\n                 # Define the MCP tool handler dynamically\n                 async def mcp_tool_handler(tool_name=adk_tool.name, **kwargs):\n                     logger.info(f\"[MCP Tool via ADK] Calling {tool_name} with {kwargs}\")\n                     # ADK tools expect ToolContext, which we don't have here.\n                     # We might need to simulate it or adapt the tool execution.\n                     # This simple version calls the tool's underlying function if possible.\n                     # WARNING: This bypasses ADK's standard tool execution flow.\n                     if hasattr(adk_tool, 'func') and callable(adk_tool.func):\n                         # This assumes the function doesn't need ToolContext\n                         result = await adk_tool.func(**kwargs)\n                         # Convert result to MCP content (e.g., TextContent)\n                         if isinstance(result, str):\n                             return [mcp_types.TextContent(type=\"text\", text=result)]\n                         else:\n                             try:\n                                 return [mcp_types.TextContent(type=\"text\", text=json.dumps(result))]\n                             except:\n                                 return [mcp_types.TextContent(type=\"text\", text=str(result))]\n                     else:\n                         logger.warning(f\"Cannot directly call ADK tool {tool_name} via MCP.\")\n                         return [mcp_types.TextContent(type=\"text\", text=f\"Error: Cannot execute ADK tool {tool_name} directly.\")]\n\n                 # Register the dynamic handler with the MCP server\n                 self.mcp_server.tool(name=mcp_schema.name)(mcp_tool_handler)\n                 logger.info(f\"Exposed ADK tool '{adk_tool.name}' as MCP tool '{mcp_schema.name}'.\")\n\n             except Exception as e:\n                 logger.warning(f\"Failed to expose ADK tool '{adk_tool.name}' via MCP: {e}\")\n\n\n    logger.info(f\"MCP server setup complete for agent '{self.amd.name}'. Run `agent.run_mcp_server()` to start.\")\n    return self.mcp_server\n</code></pre> <code>LLMMessage</code> <code>dataclass</code> \u00b6 <p>Represents a message in a conversation, compatible with LiteLLM.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@dataclass\nclass LLMMessage:\n    \"\"\"Represents a message in a conversation, compatible with LiteLLM.\"\"\"\n    role: Literal[\"user\", \"assistant\", \"system\", \"tool\"]\n    content: str | list[dict[str, Any]] # String or multimodal content (LiteLLM format)\n    tool_call_id: str | None = None # For tool responses\n    name: str | None = None # For tool calls/responses (function name)\n\n    # Add tool_calls for assistant messages requesting tool use (LiteLLM format)\n    tool_calls: list[dict[str, Any]] | None = None # e.g., [{\"id\": \"call_123\", \"function\": {\"name\": \"...\", \"arguments\": \"{...}\"}}]\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Converts to dict suitable for LiteLLM.\"\"\"\n        d = {\n            \"role\": self.role,\n            \"content\": self.content,\n        }\n        if self.tool_call_id: d[\"tool_call_id\"] = self.tool_call_id\n        if self.name: d[\"name\"] = self.name\n        if self.tool_calls: d[\"tool_calls\"] = self.tool_calls\n        return d\n</code></pre> <code>to_dict()</code> \u00b6 <p>Converts to dict suitable for LiteLLM.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Converts to dict suitable for LiteLLM.\"\"\"\n    d = {\n        \"role\": self.role,\n        \"content\": self.content,\n    }\n    if self.tool_call_id: d[\"tool_call_id\"] = self.tool_call_id\n    if self.name: d[\"name\"] = self.name\n    if self.tool_calls: d[\"tool_calls\"] = self.tool_calls\n    return d\n</code></pre> <code>WorldModel</code> <code>dataclass</code> \u00b6 <p>Thread-safe persistent understanding of the world for the agent.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@dataclass\nclass WorldModel:\n    \"\"\"Thread-safe persistent understanding of the world for the agent.\"\"\"\n    data: dict[str, Any] = dataclass_field(default_factory=dict)\n    _lock: SkipValidation[threading.Lock] = dataclass_field(default_factory=threading.Lock, init=False, repr=False)\n\n    def get(self, key: str, default: Any = None) -&gt; Any:\n        with self._lock:\n            return self.data.get(key, default)\n\n    def set(self, key: str, value: Any):\n        with self._lock:\n            logger.debug(f\"WorldModel SET: {key} = {value}\")\n            self.data[key] = value\n\n    def remove(self, key: str):\n        with self._lock:\n            if key in self.data:\n                logger.debug(f\"WorldModel REMOVE: {key}\")\n                del self.data[key]\n\n    def show(self) -&gt; str:\n        with self._lock:\n            if not self.data:\n                return \"[empty]\"\n            try:\n                items = [f\"- {k}: {json.dumps(v, indent=None, ensure_ascii=False, default=str)}\"\n                         for k, v in self.data.items()]\n                return \"\\n\".join(items)\n            except Exception:\n                items = [f\"- {k}: {str(v)}\" for k, v in self.data.items()]\n                return \"\\n\".join(items)\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        with self._lock:\n            return self.data.copy()\n\n    def update_from_dict(self, data_dict: dict[str, Any]):\n        with self._lock:\n            self.data.update(data_dict)\n            logger.debug(f\"WorldModel updated from dict: {list(data_dict.keys())}\")\n\n    def sync_from_adk_state(self, adk_state: State):\n        \"\"\"Updates the WorldModel from an ADK Session State.\"\"\"\n        if not ADK_AVAILABLE or not isinstance(adk_state, State):\n            return\n        with self._lock:\n            # Simple overwrite strategy, could be more sophisticated (merge, etc.)\n            self.data = adk_state.to_dict() # ADK State is dict-like\n            logger.debug(f\"WorldModel synced FROM ADK state. Keys: {list(self.data.keys())}\")\n\n    def sync_to_adk_state(self, adk_state: State):\n        \"\"\"Updates an ADK Session State from the WorldModel.\"\"\"\n        if not ADK_AVAILABLE or not isinstance(adk_state, State):\n            return\n        with self._lock:\n            # Update the ADK state dictionary directly\n            adk_state.update(self.data)\n            logger.debug(f\"WorldModel synced TO ADK state. Keys: {list(adk_state.keys())}\")\n</code></pre> <code>sync_from_adk_state(adk_state)</code> \u00b6 <p>Updates the WorldModel from an ADK Session State.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def sync_from_adk_state(self, adk_state: State):\n    \"\"\"Updates the WorldModel from an ADK Session State.\"\"\"\n    if not ADK_AVAILABLE or not isinstance(adk_state, State):\n        return\n    with self._lock:\n        # Simple overwrite strategy, could be more sophisticated (merge, etc.)\n        self.data = adk_state.to_dict() # ADK State is dict-like\n        logger.debug(f\"WorldModel synced FROM ADK state. Keys: {list(self.data.keys())}\")\n</code></pre> <code>sync_to_adk_state(adk_state)</code> \u00b6 <p>Updates an ADK Session State from the WorldModel.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def sync_to_adk_state(self, adk_state: State):\n    \"\"\"Updates an ADK Session State from the WorldModel.\"\"\"\n    if not ADK_AVAILABLE or not isinstance(adk_state, State):\n        return\n    with self._lock:\n        # Update the ADK state dictionary directly\n        adk_state.update(self.data)\n        logger.debug(f\"WorldModel synced TO ADK state. Keys: {list(adk_state.keys())}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent.builder","title":"<code>builder</code>","text":"<code>BuilderConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Serializable configuration state for the EnhancedAgentBuilder.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>class BuilderConfig(BaseModel):\n    \"\"\"Serializable configuration state for the EnhancedAgentBuilder.\"\"\"\n    agent_name: str = \"UnnamedEnhancedAgent\"\n    agent_version: str = \"0.1.0\"\n\n    # Core Model Config (Subset of AgentModelData, as some are instance-specific like BudgetManager)\n    model_identifier: str | None = None\n    formatter_llm_model: str | None = None\n    system_message: str = \"You are a helpful AI assistant.\"\n    temperature: float | None = None\n    top_k: int | None = None\n    top_p: float | None = None\n    max_tokens_output: int | None = None # Max tokens for LLM *generation*\n    max_tokens_input: int | None = None # Max context window (for trimming)\n    api_key_env_var: str | None = None # Store env var name, not the key itself\n    api_base: str | None = None\n    api_version: str | None = None\n    stop_sequence: list[str] | None = None\n    llm_user_id: str | None = None # 'user' param for LLM calls\n    enable_litellm_caching: bool = True\n\n    # Agent Behavior\n    enable_streaming: bool = False\n    verbose_logging: bool = False\n    world_model_initial_data: dict[str, Any] | None = None\n    history: BuilderHistoryConfig = Field(default_factory=BuilderHistoryConfig)\n\n    # Framework Integrations\n    adk: BuilderADKConfig = Field(default_factory=BuilderADKConfig)\n    a2a: BuilderA2AConfig = Field(default_factory=BuilderA2AConfig)\n    mcp: BuilderMCPConfig = Field(default_factory=BuilderMCPConfig)\n\n    # Cost Tracking (Configuration for persistence)\n    cost_tracker_config: dict[str, Any] | None = Field(default={'type': 'json', 'filepath': './user_costs.json'}, description=\"Config for UserCostTracker (e.g., type, path)\")\n\n    # Observability (Configuration)\n    telemetry_config: dict[str, Any] | None = Field(default={'enabled': False, 'service_name': None, 'endpoint': None}, description=\"Basic OTel config hints\")\n\n    model_config = ConfigDict(validate_assignment=True)\n\n    @model_validator(mode='after')\n    def _resolve_names(self) -&gt; 'BuilderConfig':\n        # Ensure service name defaults to agent name if not set\n        if self.telemetry_config and self.telemetry_config.get('enabled') and not self.telemetry_config.get('service_name'):\n            self.telemetry_config['service_name'] = self.agent_name\n        # Ensure MCP server name defaults if not set\n        if self.mcp.enabled and not self.mcp.server_name:\n             self.mcp.server_name = f\"{self.agent_name}_MCPServer\"\n        return self\n</code></pre> <code>EnhancedAgentBuilder</code> \u00b6 <p>Fluent builder for configuring and constructing production-ready EnhancedAgent instances. Supports loading configuration from files and provides methods for detailed setup.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>class EnhancedAgentBuilder:\n    \"\"\"\n    Fluent builder for configuring and constructing production-ready EnhancedAgent instances.\n    Supports loading configuration from files and provides methods for detailed setup.\n    \"\"\"\n\n    def __init__(self,agent_name: str = \"DefaultAgent\", config: BuilderConfig | None = None, config_path: str | Path | None = None):\n        \"\"\"\n        Initialize the builder. Can start with a config object, path, or blank.\n\n        Args:\n            config: An existing BuilderConfig object.\n            config_path: Path to a YAML/JSON configuration file for the builder.\n        \"\"\"\n        if config and config_path:\n            raise ValueError(\"Provide either config object or config_path, not both.\")\n\n        if config_path:\n            self.load_config(config_path) # Sets self._config\n        elif config:\n            self._config = config.copy(deep=True)\n        else:\n            self._config = BuilderConfig() # Start with defaults\n\n        # --- Transient fields (not saved/loaded directly via BuilderConfig JSON) ---\n        # Instances or non-serializable objects provided programmatically.\n        self._adk_tools_transient: list[ADKBaseTool | Callable] = []\n        self._adk_code_executor_instance: ADKBaseCodeExecutor | None = None\n        self._adk_runner_instance: ADKRunner | None = None\n        self._adk_session_service_instance: ADKSessionService | None = None\n        self._adk_planner_instance: ADKPlanner | None = None\n        self._litellm_budget_manager_instance: BudgetManager | None = None\n        self._user_cost_tracker_instance: UserCostTracker | None = None\n        self._otel_trace_provider_instance: TracerProvider | None = None\n        self._callbacks_transient: dict[str, Callable] = {}\n        # Pre-initialized server instances (less common, but possible)\n        self._a2a_server_instance: A2AServer | None = None\n        self._mcp_server_instance: FastMCP | None = None\n\n        # Set initial log level based on loaded config\n        logger.setLevel(logging.DEBUG if self._config.verbose_logging else logging.INFO)\n        self.with_agent_name(agent_name)\n\n    # --- Configuration Save/Load ---\n\n    def save_config(self, path: str | Path, indent: int = 2):\n        \"\"\"Saves the current builder configuration to a JSON file.\"\"\"\n        filepath = Path(path)\n        try:\n            filepath.parent.mkdir(parents=True, exist_ok=True)\n            config_json = self._config.model_dump_json(indent=indent)\n            with open(filepath, 'w') as f:\n                f.write(config_json)\n            logger.info(f\"Builder configuration saved to {filepath}\")\n        except OSError as e:\n            logger.error(f\"Failed to save builder configuration to {filepath}: {e}\")\n        except ValidationError as e:\n             logger.error(f\"Configuration is invalid, cannot save: {e}\")\n        except Exception as e:\n             logger.error(f\"An unexpected error occurred during config save: {e}\")\n\n\n    def load_config(self, path: str | Path) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Loads builder configuration from a JSON file, overwriting current settings.\"\"\"\n        filepath = Path(path)\n        if not filepath.exists():\n            raise FileNotFoundError(f\"Builder configuration file not found: {filepath}\")\n        try:\n            with open(filepath) as f:\n                config_data = json.load(f)\n            self._config = BuilderConfig.model_validate(config_data)\n            logger.info(f\"Builder configuration loaded from {filepath}\")\n            # Reset transient fields, as they are not saved\n            self._reset_transient_fields()\n            logger.warning(\"Transient fields (callbacks, tool instances, tracker instance, etc.) reset. Re-add them if needed.\")\n            # Update logger level based on loaded config\n            logger.setLevel(logging.DEBUG if self._config.verbose_logging else logging.INFO)\n        except (OSError, json.JSONDecodeError) as e:\n            logger.error(f\"Failed to load or parse builder configuration from {filepath}: {e}\")\n            raise\n        except ValidationError as e:\n             logger.error(f\"Loaded configuration data is invalid: {e}\")\n             raise\n        return self\n\n    def _reset_transient_fields(self):\n        \"\"\"Clears fields that are not part of the saved BuilderConfig.\"\"\"\n        self._adk_tools_transient = []\n        self._adk_code_executor_instance = None\n        self._adk_runner_instance = None\n        self._adk_session_service_instance = None\n        self._adk_planner_instance = None\n        self._litellm_budget_manager_instance = None\n        self._user_cost_tracker_instance = None\n        self._otel_trace_provider_instance = None\n        self._callbacks_transient = {}\n        self._a2a_server_instance = None\n        self._mcp_server_instance = None\n\n    # --- Fluent Configuration Methods (Modify self._config) ---\n\n    def with_agent_name(self, name: str) -&gt; 'EnhancedAgentBuilder':\n        self._config.agent_name = name\n        # Update dependent defaults\n        self._config = BuilderConfig.model_validate(self._config.model_dump())\n        return self\n\n    def with_agent_version(self, version: str) -&gt; 'EnhancedAgentBuilder':\n        self._config.agent_version = version\n        return self\n\n    def with_model(self, model_identifier: str) -&gt; 'EnhancedAgentBuilder':\n        self._config.model_identifier = model_identifier\n        # Auto-detect context window if not set\n        if not self._config.max_tokens_input:\n            try:\n                max_input = get_max_tokens(model_identifier)\n                if max_input:\n                    self._config.max_tokens_input = max_input\n                    logger.info(f\"Auto-detected max_input_tokens for {model_identifier}: {max_input}\")\n                else:\n                     # Default fallback if detection fails\n                    self._config.max_tokens_input = 4096\n                    logger.warning(f\"Could not auto-detect max_input_tokens for {model_identifier}, defaulting to 4096.\")\n            except Exception as e:\n                 self._config.max_tokens_input = 4096\n                 logger.warning(f\"Error auto-detecting max_input_tokens ({e}), defaulting to 4096.\")\n        # Auto-configure Ollama base URL\n        if 'ollama/' in model_identifier and not self._config.api_base:\n            self.with_api_base(\"http://localhost:11434\") # Uses the method to log\n        return self\n\n    def with_system_message(self, message: str) -&gt; 'EnhancedAgentBuilder':\n        self._config.system_message = message\n        return self\n\n    def with_temperature(self, temp: float) -&gt; 'EnhancedAgentBuilder':\n        self._config.temperature = temp\n        return self\n\n    def with_max_output_tokens(self, tokens: int) -&gt; 'EnhancedAgentBuilder':\n        self._config.max_tokens_output = tokens\n        return self\n\n    def with_max_input_tokens(self, tokens: int) -&gt; 'EnhancedAgentBuilder':\n        self._config.max_tokens_input = tokens\n        return self\n\n    def with_stop_sequence(self, stop: list[str]) -&gt; 'EnhancedAgentBuilder':\n        self._config.stop_sequence = stop\n        return self\n\n    def with_api_key_from_env(self, env_var_name: str) -&gt; 'EnhancedAgentBuilder':\n        self._config.api_key_env_var = env_var_name\n        # Quick check if env var exists\n        if not os.getenv(env_var_name):\n            logger.warning(f\"API key environment variable '{env_var_name}' is not set.\")\n        return self\n\n    def with_api_base(self, base_url: str | None) -&gt; 'EnhancedAgentBuilder':\n        self._config.api_base = base_url\n        logger.info(f\"API base set to: {base_url}\")\n        return self\n\n    def with_api_version(self, version: str | None) -&gt; 'EnhancedAgentBuilder':\n        self._config.api_version = version\n        return self\n\n    def with_llm_user_id(self, user_id: str) -&gt; 'EnhancedAgentBuilder':\n        self._config.llm_user_id = user_id\n        return self\n\n    def enable_litellm_caching(self, enable: bool = True) -&gt; 'EnhancedAgentBuilder':\n        self._config.enable_litellm_caching = enable\n        return self\n\n    def enable_streaming(self, enable: bool = True) -&gt; 'EnhancedAgentBuilder':\n        self._config.enable_streaming = enable\n        return self\n\n    def verbose(self, enable: bool = True) -&gt; 'EnhancedAgentBuilder':\n        self._config.verbose_logging = enable\n        logger.setLevel(logging.DEBUG if enable else logging.INFO)\n        os.environ['LITELLM_LOG'] = 'DEBUG' if enable else 'NONE' # Control LiteLLM verbosity too\n        return self\n    def formatter_llm_model(self, model: str) -&gt; 'EnhancedAgentBuilder':\n        self._config.formatter_llm_model = model\n        return self\n\n    def with_initial_world_data(self, data: dict[str, Any]) -&gt; 'EnhancedAgentBuilder':\n        self._config.world_model_initial_data = data\n        return self\n\n    def with_history_options(self, max_turns: int | None = 20, max_tokens: int | None = None, trim_strategy: Literal[\"litellm\", \"basic\"] = \"litellm\") -&gt; 'EnhancedAgentBuilder':\n        self._config.history = BuilderHistoryConfig(max_turns=max_turns, max_tokens=max_tokens, trim_strategy=trim_strategy)\n        return self\n\n    # --- ADK Configuration Methods ---\n    def _ensure_adk(self, feature: str):\n        if not ADK_AVAILABLE:\n            logger.warning(f\"ADK not available. Cannot configure ADK feature: {feature}.\")\n            return False\n        self._config.adk.enabled = True # Mark ADK as enabled if any ADK feature is used\n        return True\n\n    def enable_adk(self, runner_class: type[ADKRunner] = InMemoryRunner, runner_options: dict[str, Any] | None = None) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Enables ADK integration with a specified runner.\"\"\"\n        if not self._ensure_adk(\"Runner\"): return self\n        self._config.adk.runner_class_name = runner_class.__name__\n        self._config.adk.runner_options = runner_options or {}\n        logger.info(f\"ADK integration enabled with runner: {self._config.adk.runner_class_name}\")\n        return self\n\n    def with_adk_description(self, description: str) -&gt; 'EnhancedAgentBuilder':\n        if not self._ensure_adk(\"Description\"): return self\n        self._config.adk.description = description\n        return self\n\n    def with_adk_tool_instance(self, tool: ADKBaseTool) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Adds a pre-initialized ADK Tool instance (transient).\"\"\"\n        if not self._ensure_adk(\"Tool Instance\"): return self\n        if not isinstance(tool, ADKBaseTool):\n            raise TypeError(f\"Expected ADK BaseTool instance, got {type(tool)}\")\n        self._adk_tools_transient.append(tool)\n        return self\n\n    def with_adk_tool_function(self, func: Callable) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Adds a callable function as an ADK tool (transient).\"\"\"\n        if not self._ensure_adk(\"Tool Function\"): return self\n        if not callable(func):\n            raise TypeError(f\"Expected callable function for ADK tool, got {type(func)}\")\n        self._adk_tools_transient.append(func)\n        return self\n\n    def with_adk_mcp_toolset(self, connection_type: Literal[\"stdio\", \"sse\"], **kwargs) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Configures an ADK MCP Toolset connection (saved in config).\"\"\"\n        if not self._ensure_adk(\"MCP Toolset\"): return self\n        if connection_type == \"stdio\":\n            if \"command\" not in kwargs: raise ValueError(\"Stdio MCP toolset requires 'command' argument.\")\n            config = {\"type\": \"stdio\", \"command\": kwargs[\"command\"], \"args\": kwargs.get(\"args\", [])}\n        elif connection_type == \"sse\":\n            if \"url\" not in kwargs: raise ValueError(\"SSE MCP toolset requires 'url' argument.\")\n            config = {\"type\": \"sse\", \"url\": kwargs[\"url\"]}\n        else:\n            raise ValueError(f\"Unknown MCP toolset connection type: {connection_type}\")\n        self._config.adk.mcp_toolset_configs.append(config)\n        logger.info(f\"Configured ADK MCP Toolset: {config}\")\n        return self\n\n    def with_adk_code_executor(self, executor_type: Literal[\"adk_builtin\", \"unsafe_simple\", \"secure_placeholder\", \"none\"]) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Configures the type of ADK code executor to use (saved in config).\"\"\"\n        if not self._ensure_adk(\"Code Executor Type\"): return self\n        if executor_type == \"unsafe_simple\":\n            logger.critical(\"***********************************************************\")\n            logger.critical(\"*** WARNING: Configuring UNSAFE SimplePythonExecutor!   ***\")\n            logger.critical(\"***********************************************************\")\n        elif executor_type == \"secure_placeholder\":\n            logger.warning(\"Configuring SecureCodeExecutorPlaceholder. Implement actual sandboxing!\")\n        elif executor_type == \"adk_builtin\":\n            if self._config.model_identifier and (\"gemini-1.5\" not in self._config.model_identifier and \"gemini-2\" not in self._config.model_identifier) :\n                logger.warning(f\"ADK built-in code execution selected, but model '{self._config.model_identifier}' might not support it. Ensure model compatibility.\")\n            logger.info(\"Configuring ADK built-in code execution (tool-based, requires compatible model).\")\n\n        self._config.adk.code_executor_config = executor_type\n        self._adk_code_executor_instance = None # Clear any previously set instance\n        return self\n\n    def with_adk_code_executor_instance(self, executor: ADKBaseCodeExecutor) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Provides a pre-initialized ADK code executor instance (transient).\"\"\"\n        if not self._ensure_adk(\"Code Executor Instance\"): return self\n        if not isinstance(executor, ADKBaseCodeExecutor):\n            raise TypeError(f\"Expected ADKBaseCodeExecutor instance, got {type(executor)}\")\n        self._adk_code_executor_instance = executor\n        self._config.adk.code_executor_config = \"custom_instance\" # Mark config\n        logger.info(f\"Using custom ADK code executor instance: {type(executor).__name__}\")\n        return self\n\n    def enable_adk_state_sync(self, enable: bool = True) -&gt; 'EnhancedAgentBuilder':\n        if not self._ensure_adk(\"State Sync\"): return self\n        self._config.adk.sync_state = enable\n        return self\n\n    # --- Server Configuration Methods ---\n    def enable_a2a_server(self, host: str = \"0.0.0.0\", port: int = 5000, **extra_options) -&gt; 'EnhancedAgentBuilder':\n        if not A2A_AVAILABLE:\n            logger.warning(\"python-a2a library not available. Cannot enable A2A server.\")\n            self._config.a2a.enabled = False\n            return self\n        self._config.a2a.enabled = True\n        self._config.a2a.host = host\n        self._config.a2a.port = port\n        self._config.a2a.extra_options = extra_options\n        return self\n\n    def add_a2a_known_client(self, name: str, url: str) -&gt; 'EnhancedAgentBuilder':\n        if not A2A_AVAILABLE:\n            logger.warning(\"python-a2a library not available. Cannot add known A2A client.\")\n            return self\n        # A2A client setup is handled by the agent itself, we just store the config\n        self._config.a2a.known_clients[name] = url\n        logger.info(f\"Added known A2A client config: '{name}' -&gt; {url}\")\n        return self\n\n    def enable_mcp_server(self, host: str = \"0.0.0.0\", port: int = 8000, server_name: str | None = None, **extra_options) -&gt; 'EnhancedAgentBuilder':\n         if not MCP_AVAILABLE:\n             logger.warning(\"MCP library (FastMCP) not available. Cannot enable MCP server.\")\n             self._config.mcp.enabled = False\n             return self\n         self._config.mcp.enabled = True\n         self._config.mcp.host = host\n         self._config.mcp.port = port\n         self._config.mcp.server_name = server_name # Will default later if None\n         self._config.mcp.extra_options = extra_options\n         # Re-validate to update default name if needed\n         self._config = BuilderConfig.model_validate(self._config.model_dump())\n         return self\n\n    # --- Cost Tracking &amp; Budgeting Methods ---\n    def with_cost_tracker(self, tracker: UserCostTracker) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Provides a pre-initialized UserCostTracker instance (transient).\"\"\"\n        if not hasattr(tracker, \"get_all_costs\"): # Check protocol using isinstance\n             raise TypeError(\"Cost tracker must implement the UserCostTracker protocol.\")\n        self._user_cost_tracker_instance = tracker\n        # Clear file config if instance is provided\n        self._config.cost_tracker_config = {'type': 'custom_instance'}\n        logger.info(f\"Using custom UserCostTracker instance: {type(tracker).__name__}\")\n        return self\n\n    def with_json_cost_tracker(self, filepath: str | Path) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Configures the builder to use the JsonFileUserCostTracker (saved in config).\"\"\"\n        self._config.cost_tracker_config = {'type': 'json', 'filepath': str(filepath)}\n        self._user_cost_tracker_instance = None # Clear any instance\n        logger.info(f\"Configured JsonFileUserCostTracker: {filepath}\")\n        return self\n\n    def with_litellm_budget_manager(self, manager: BudgetManager) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Provides a pre-initialized LiteLLM BudgetManager instance (transient).\"\"\"\n        if not LITELLM_AVAILABLE:\n             logger.warning(\"LiteLLM not available, cannot set BudgetManager.\")\n             return self\n        if not isinstance(manager, BudgetManager):\n            raise TypeError(\"Expected litellm.BudgetManager instance.\")\n        self._litellm_budget_manager_instance = manager\n        return self\n\n    # --- Observability Methods ---\n    def enable_telemetry(self, service_name: str | None = None, endpoint: str | None = None) -&gt; 'EnhancedAgentBuilder':\n         if not OTEL_AVAILABLE:\n              logger.warning(\"OpenTelemetry SDK not available. Cannot enable telemetry.\")\n              self._config.telemetry_config = {'enabled': False}\n              return self\n         self._config.telemetry_config = {\n             'enabled': True,\n             'service_name': service_name, # Defaults to agent name later\n             'endpoint': endpoint # For OTLP exporter, e.g. \"http://localhost:4317\"\n         }\n         # Re-validate to update default name if needed\n         self._config = BuilderConfig.model_validate(self._config.model_dump())\n         return self\n\n    def with_telemetry_provider_instance(self, provider: TracerProvider) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Provides a pre-initialized OpenTelemetry TracerProvider instance (transient).\"\"\"\n        if not OTEL_AVAILABLE:\n            logger.warning(\"OpenTelemetry SDK not available. Cannot set TracerProvider.\")\n            return self\n        if not isinstance(provider, TracerProvider):\n             raise TypeError(\"Expected opentelemetry.sdk.trace.TracerProvider instance.\")\n        self._otel_trace_provider_instance = provider\n        # Mark telemetry as enabled, but using custom instance\n        self._config.telemetry_config = {'enabled': True, 'type': 'custom_instance'}\n        logger.info(\"Using custom OpenTelemetry TracerProvider instance.\")\n        return self\n\n    # --- Callback Methods (Transient) ---\n    def with_stream_callback(self, func: Callable[[str], None | Awaitable[None]]) -&gt; 'EnhancedAgentBuilder':\n        self._callbacks_transient['stream_callback'] = func; return self\n    def with_post_run_callback(self, func: Callable[[str, str, float, str | None], None | Awaitable[None]]) -&gt; 'EnhancedAgentBuilder':\n        self._callbacks_transient['post_run_callback'] = func; return self # Added user_id\n    def with_progress_callback(self, func: Callable[[Any], None | Awaitable[None]]) -&gt; 'EnhancedAgentBuilder':\n        self._callbacks_transient['progress_callback'] = func; return self\n    def with_human_in_loop_callback(self, func: Callable[[dict], str | Awaitable[str]]) -&gt; 'EnhancedAgentBuilder':\n        self._callbacks_transient['human_in_loop_callback'] = func; return self\n\n    # --- Build Method ---\n    async def build(self) -&gt; EnhancedAgent:\n        \"\"\"\n        Constructs and returns the configured EnhancedAgent instance.\n        Handles asynchronous setup like fetching ADK MCP tools.\n        \"\"\"\n        logger.info(f\"--- Building EnhancedAgent: {self._config.agent_name} v{self._config.agent_version} ---\")\n\n        # 1. Final Config Validation (Pydantic model handles most)\n        if not self._config.model_identifier:\n            raise ValueError(\"LLM model identifier is required. Use .with_model()\")\n\n        # 2. Resolve API Key\n        api_key = None\n        if self._config.api_key_env_var:\n            api_key = os.getenv(self._config.api_key_env_var)\n            if not api_key:\n                logger.warning(f\"API key environment variable '{self._config.api_key_env_var}' is set in config but not found in environment.\")\n            # else: logger.debug(\"API key loaded from environment variable.\") # Avoid logging key presence\n\n        # 3. Setup Telemetry Provider (if instance provided)\n        if self._otel_trace_provider_instance and OTEL_AVAILABLE:\n            trace.set_tracer_provider(self._otel_trace_provider_instance)\n            logger.info(\"Global OpenTelemetry TracerProvider set from provided instance.\")\n        elif self._config.telemetry_config.get('enabled') and self._config.telemetry_config.get('type') != 'custom_instance' and OTEL_AVAILABLE:\n             # Basic provider setup from config (can be expanded)\n             logger.info(\"Setting up basic OpenTelemetry based on config (ConsoleExporter example).\")\n             from opentelemetry.sdk.trace.export import (\n                 BatchSpanProcessor,\n                 ConsoleSpanExporter,\n             )\n             provider = TracerProvider()\n             provider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))\n             #: Add OTLP exporter based on self._config.telemetry_config['endpoint']\n             trace.set_tracer_provider(provider)\n             self._otel_trace_provider_instance = provider # Store for potential access?\n\n        # 4. Prepare Core Components\n        # Agent Model Data\n        try:\n            amd = AgentModelData(\n                name=self._config.agent_name,\n                model=self._config.model_identifier,\n                system_message=self._config.system_message,\n                temperature=self._config.temperature,\n                top_k=self._config.top_k,\n                top_p=self._config.top_p,\n                max_tokens=self._config.max_tokens_output,\n                max_input_tokens=self._config.max_tokens_input,\n                api_key=api_key,\n                api_base=self._config.api_base,\n                api_version=self._config.api_version,\n                stop_sequence=self._config.stop_sequence,\n                user_id=self._config.llm_user_id,\n                budget_manager=self._litellm_budget_manager_instance,\n                caching=self._config.enable_litellm_caching\n            )\n        except ValidationError as e:\n            logger.error(f\"Validation error creating AgentModelData: {e}\")\n            raise\n\n        # World Model\n        world_model = self._config.world_model_initial_data or {}\n\n        # User Cost Tracker\n        cost_tracker = self._user_cost_tracker_instance # Use provided instance if available\n        if not cost_tracker and self._config.cost_tracker_config:\n            tracker_type = self._config.cost_tracker_config.get('type')\n            if tracker_type == 'json':\n                filepath = self._config.cost_tracker_config.get('filepath')\n                if filepath:\n                    cost_tracker = JsonFileUserCostTracker(filepath)\n                    logger.info(f\"Initialized JsonFileUserCostTracker ({filepath})\")\n                else:\n                    logger.warning(\"JSON cost tracker configured but filepath missing.\")\n            elif tracker_type == 'custom_instance':\n                 logger.warning(\"Cost tracker configured as 'custom_instance' but no instance was provided via .with_cost_tracker().\")\n            # Add other tracker types (DB, InMemory) here\n\n        # 5. Prepare ADK Components\n        adk_runner_instance = self._adk_runner_instance\n        adk_session_service = self._adk_session_service_instance\n        adk_planner_instance = self._adk_planner_instance\n        adk_code_executor = self._adk_code_executor_instance # Use provided instance first\n        adk_exit_stack = None\n        processed_adk_tools = list(self._adk_tools_transient) # Start with transient tools\n\n        if ADK_AVAILABLE and self._config.adk.enabled:\n            logger.info(\"Configuring ADK components...\")\n            adk_exit_stack = contextlib.AsyncExitStack()\n\n            # --- ADK Runner &amp; Session Service ---\n            if not adk_runner_instance:\n                runner_cls_name = self._config.adk.runner_class_name\n                runner_opts = self._config.adk.runner_options\n                try:\n                    # Dynamically import/get runner class\n                    if runner_cls_name == \"InMemoryRunner\": runner_class = InMemoryRunner\n                    elif runner_cls_name == \"Runner\": runner_class = Runner\n                    elif runner_cls_name == \"AsyncWebRunner\": runner_class = AsyncWebRunner # If available\n                    else: raise ValueError(f\"Unsupported ADK Runner class name: {runner_cls_name}\")\n\n                    # Special handling: InMemoryRunner needs agent instance *later*\n                    if runner_class is InMemoryRunner or runner_class is Runner:\n                         logger.debug(\"Deferring InMemoryRunner creation until after agent instantiation.\")\n                         # Store config to create it later\n                         adk_runner_config_for_later = {\n                             \"runner_class\": runner_class,\n                             \"app_name\": runner_opts.get(\"app_name\", f\"{self._config.agent_name}_ADKApp\"),\n                             \"session_service\": adk_session_service, # Pass service if already created\n                             **runner_opts # Pass other options\n                         }\n                         adk_runner_instance = None # Ensure it's None for now\n                    else: # Other runners might be creatable now\n                         # Need to ensure session service is handled correctly if runner needs it\n                         if not adk_session_service:\n                             # Create default session service if needed by runner\n                             # This part is complex as runners might create their own\n                             logger.info(\"Using default ADK InMemorySessionService for runner.\")\n                             adk_session_service = InMemorySessionService()\n\n                         adk_runner_instance = runner_class(\n                             session_service=adk_session_service,\n                             app_name=runner_opts.get(\"app_name\", f\"{self._config.agent_name}_ADKApp\"),\n                             **runner_opts # Pass other options\n                         )\n                         logger.info(f\"Created ADK Runner instance: {runner_cls_name}\")\n\n                except (ImportError, ValueError, TypeError) as e:\n                    logger.error(f\"Failed to configure ADK Runner '{runner_cls_name}': {e}\", exc_info=True)\n                    raise ValueError(f\"Failed to setup ADK Runner: {e}\") from e\n\n            # Ensure session service exists if runner created one\n            if adk_runner_instance and hasattr(adk_runner_instance, 'session_service'):\n                 if not adk_session_service:\n                     adk_session_service = adk_runner_instance.session_service\n                 elif adk_session_service is not adk_runner_instance.session_service:\n                     logger.warning(\"Provided ADK SessionService differs from the one in the provided ADK Runner. Using the runner's service.\")\n                     adk_session_service = adk_runner_instance.session_service\n\n            # Fallback: create default session service if none exists by now\n            if not adk_session_service:\n                  logger.info(\"Using default ADK InMemorySessionService.\")\n                  adk_session_service = InMemorySessionService()\n\n\n            # --- ADK Code Executor ---\n            if not adk_code_executor: # If instance wasn't provided directly\n                executor_config = self._config.adk.code_executor_config\n                if executor_config == \"unsafe_simple\":\n                    adk_code_executor = UnsafeSimplePythonExecutor()\n                    logger.critical(\"UNSAFE code executor instance created!\")\n                elif executor_config == \"secure_placeholder\":\n                    adk_code_executor = SecureCodeExecutorPlaceholder()\n                    logger.warning(\"SecureCodeExecutorPlaceholder instance created.\")\n                elif executor_config == \"adk_builtin\":\n                    # This type uses the TOOL, not an executor instance passed to LlmAgent init\n                    adk_code_executor = adk_built_in_code_execution\n                    #if not any(getattr(t, 'func', None) == tool_func for t in processed_adk_tools if isinstance(t, FunctionTool)):\n                    #     tool_func.__name__ = \"code_execution\"\n                    # processed_adk_tools.append(tool_func)\n                    #     logger.info(\"Added ADK built-in code execution tool.\")\n                    adk_code_executor = None # Ensure no executor instance is passed for this case\n                elif executor_config == \"none\":\n                    adk_code_executor = None\n                elif executor_config == \"custom_instance\":\n                    # Should have been provided via .with_adk_code_executor_instance()\n                    logger.error(\"ADK code executor configured as 'custom_instance' but no instance was provided.\")\n                    adk_code_executor = None\n                # Add handling for dict config if needed in the future\n\n            # --- ADK Tools (Wrap callables) ---\n            temp_tools = []\n            for tool_input in processed_adk_tools:\n                 if isinstance(tool_input, ADKBaseTool):\n                     temp_tools.append(tool_input)\n                 elif callable(tool_input):\n                     try:\n                         wrapped = ADKFunctionTool(func=tool_input)\n                         temp_tools.append(wrapped)\n                     except Exception as e: logger.warning(f\"Could not wrap callable '{getattr(tool_input, '__name__', 'unknown')}' as ADK tool: {e}\")\n                 else: logger.warning(f\"Skipping invalid ADK tool input: {type(tool_input)}\")\n            processed_adk_tools = temp_tools\n\n            # --- ADK MCP Toolsets ---\n            for mcp_conf in self._config.adk.mcp_toolset_configs:\n                 logger.info(f\"Fetching tools from configured MCP Server: {mcp_conf}...\")\n                 try:\n                      params = None\n                      if mcp_conf.get(\"type\") == \"stdio\":\n                          params = StdioServerParameters(command=mcp_conf[\"command\"], args=mcp_conf.get(\"args\", []))\n                      elif mcp_conf.get(\"type\") == \"sse\":\n                           params = SseServerParams(url=mcp_conf[\"url\"])\n\n                      if params:\n                          mcp_tools, _ = await MCPToolset.from_server(\n                              connection_params=params,\n                              async_exit_stack=adk_exit_stack\n                          )\n                          for tool in mcp_tools: tool._is_mcp_tool = True\n                          processed_adk_tools.extend(mcp_tools)\n                          logger.info(f\"Fetched {len(mcp_tools)} tools via ADK MCPToolset ({mcp_conf.get('type')}).\")\n                      else:\n                           logger.warning(f\"Unsupported MCP config type: {mcp_conf.get('type')}\")\n\n                 except Exception as e:\n                      logger.error(f\"Failed to fetch tools from MCP server {mcp_conf}: {e}\", exc_info=True)\n                      # Decide whether to raise or continue\n\n            # --- ADK Planner, Examples, Output Schema ---\n\n\n\n        # 6. Instantiate EnhancedAgent\n        try:\n            # Base arguments for EnhancedAgent\n            agent_init_kwargs = {\n                'amd': amd,\n                'world_model': world_model,\n                'format_model': self._config.formatter_llm_model if self._config.formatter_llm_model else None, # Example passing extra config\n                'verbose': self._config.verbose_logging,\n                'stream': self._config.enable_streaming,\n                'max_history_turns': self._config.history.max_turns,\n                'max_history_tokens': self._config.history.max_tokens,\n                'trim_strategy': self._config.history.trim_strategy,\n                'sync_adk_state': self._config.adk.sync_state if ADK_AVAILABLE else False,\n                'adk_exit_stack': adk_exit_stack, # Pass stack for cleanup\n                'user_cost_tracker': cost_tracker, # Pass the tracker instance\n                **self._callbacks_transient, # Pass configured callbacks\n                # Pass server instances if provided (less common)\n                'a2a_server': self._a2a_server_instance,\n                'mcp_server': self._mcp_server_instance,\n            }\n\n            # Add ADK-specific arguments if inheriting from LlmAgent\n            agent_class = EnhancedAgent\n            if ADK_AVAILABLE and issubclass(EnhancedAgent, ADKLlmAgent):\n                 logger.debug(\"Adding ADK LlmAgent specific arguments to init.\")\n                 adk_specific_kwargs = {\n                     'name': self._config.agent_name, # Required by LlmAgent\n                     'model': LiteLlm(model=self._config.model_identifier), # LlmAgent needs BaseLlm instance\n                     'description': self._config.adk.description or self._config.system_message,\n                     'instruction': self._config.system_message, # Or dedicated instruction field?\n                     'tools': processed_adk_tools,\n                     'code_executor': adk_code_executor, # Pass the *instance*\n                     'planner': adk_planner_instance,\n                     # Process examples/schema if needed\n                     'examples': [ADKExample(**ex) for ex in self._config.adk.examples] if self._config.adk.examples else None,\n                     'output_schema': self._config.adk.output_schema,\n                     # Pass runner/session service if NOT using InMemoryRunner deferred creation\n                     # If runner is created later, it's assigned post-init\n                     'runner': adk_runner_instance if adk_runner_instance else None, # Pass runner if created now\n                     'session_service': adk_session_service, # Pass session service\n                 }\n                 # Merge, ensuring agent_init_kwargs takes precedence for overlapping basic fields if necessary\n                 # but allow ADK specifics to be added. Be careful with overlaps like 'name'.\n                 # EnhancedAgent init should handle reconciling these if needed.\n                 # A safer merge:\n                 final_kwargs = agent_init_kwargs.copy()\n                 for k, v in adk_specific_kwargs.items():\n                      if k not in final_kwargs: # Only add ADK specifics not already handled\n                          final_kwargs[k] = v\n                      # Handle specific overrides/merges needed for LlmAgent base\n                      elif k == 'tools' and v: # Merge tools\n                          final_kwargs['tools'] = (final_kwargs.get('tools') or []) + v\n                      # Overwrite description/instruction from ADK config if set\n                      elif k in ['description', 'instruction'] and v or k == 'code_executor' or k == 'model':\n                           final_kwargs[k] = v\n\n                 agent_init_kwargs = final_kwargs\n\n\n            logger.debug(f\"Final keys for EnhancedAgent init: {list(agent_init_kwargs.keys())}\")\n\n            # --- Instantiate the Agent ---\n            agent = agent_class(**agent_init_kwargs)\n            # --- Agent Instantiated ---\n\n            # If ADK InMemoryRunner creation was deferred, create and assign now\n            if ADK_AVAILABLE and 'adk_runner_config_for_later' in locals():\n                 cfg = locals()['adk_runner_config_for_later']\n                 if not isinstance(cfg['runner_class'], InMemoryRunner) and cfg.get('session_service') is None: cfg['session_service'] = agent.adk_session_service # Ensure service is passed\n                 agent.setup_adk_runner(cfg)\n                 logger.info(f\"Created and assigned deferred ADK Runner instance: {agent.adk_runner.__class__.__name__}\")\n                 # Ensure agent has runner's session service if it differs\n                 if agent.adk_runner and agent.adk_session_service is not agent.adk_runner.session_service:\n                      logger.warning(\"Agent session service differs from deferred runner's service. Updating agent's reference.\")\n                      agent.adk_session_service = agent.adk_runner.session_service\n            elif ADK_AVAILABLE and adk_runner_instance and not agent.adk_runner:\n                # If runner was created earlier but not passed via LlmAgent init (e.g. non-LlmAgent base)\n                # Or if we want to explicitly assign it\n                 agent.adk_runner = adk_runner_instance\n                 # Ensure session service consistency\n                 if agent.adk_session_service is not agent.adk_runner.session_service:\n                      agent.adk_session_service = agent.adk_runner.session_service\n\n\n        except ValidationError as e:\n            logger.error(f\"Pydantic validation error Instantiating EnhancedAgent: {e}\", exc_info=True)\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error Instantiating EnhancedAgent: {e}\", exc_info=True)\n            raise\n\n        # 7. Setup Agent's Internal Server Capabilities (if enabled and not pre-initialized)\n        if self._config.a2a.enabled and not agent.a2a_server:\n            if AGENT_A2A_AVAILABLE:\n                logger.info(\"Setting up A2A server on agent instance...\")\n                agent.setup_a2a_server(\n                    host=self._config.a2a.host,\n                    port=self._config.a2a.port,\n                    **self._config.a2a.extra_options\n                )\n            else: logger.warning(\"A2A server configured in builder, but A2A not available in agent environment.\")\n\n        if self._config.mcp.enabled and not agent.mcp_server:\n            if AGENT_MCP_AVAILABLE:\n                logger.info(\"Setting up MCP server on agent instance...\")\n                agent.setup_mcp_server(\n                    host=self._config.mcp.host,\n                    port=self._config.mcp.port,\n                    name=self._config.mcp.server_name, # Already defaulted\n                    **self._config.mcp.extra_options\n                )\n            else: logger.warning(\"MCP server configured in builder, but MCP not available in agent environment.\")\n\n        # 8. Setup A2A known clients configuration on the agent\n        if self._config.a2a.known_clients:\n             if AGENT_A2A_AVAILABLE:\n                 # The agent likely handles client creation on demand,\n                 # but we can pass the config for it to use.\n                 # Assuming agent has a way to receive this, e.g., during init or a setter\n                 if hasattr(agent, 'set_known_a2a_clients'):\n                     agent.set_known_a2a_clients(self._config.a2a.known_clients)\n                 else:\n                      # Fallback: store on a generic config dict? Less ideal.\n                      # agent.config.a2a_known_clients = self._config.a2a.known_clients\n                      logger.warning(\"Agent does not have 'set_known_a2a_clients' method. Known client config stored raw.\")\n             else:\n                  logger.warning(\"A2A known clients configured, but A2A not available in agent env.\")\n\n\n        logger.info(f\"--- EnhancedAgent Build Complete: {agent.amd.name} ---\")\n        return agent\n</code></pre> <code>__init__(agent_name='DefaultAgent', config=None, config_path=None)</code> \u00b6 <p>Initialize the builder. Can start with a config object, path, or blank.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BuilderConfig | None</code> <p>An existing BuilderConfig object.</p> <code>None</code> <code>config_path</code> <code>str | Path | None</code> <p>Path to a YAML/JSON configuration file for the builder.</p> <code>None</code> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def __init__(self,agent_name: str = \"DefaultAgent\", config: BuilderConfig | None = None, config_path: str | Path | None = None):\n    \"\"\"\n    Initialize the builder. Can start with a config object, path, or blank.\n\n    Args:\n        config: An existing BuilderConfig object.\n        config_path: Path to a YAML/JSON configuration file for the builder.\n    \"\"\"\n    if config and config_path:\n        raise ValueError(\"Provide either config object or config_path, not both.\")\n\n    if config_path:\n        self.load_config(config_path) # Sets self._config\n    elif config:\n        self._config = config.copy(deep=True)\n    else:\n        self._config = BuilderConfig() # Start with defaults\n\n    # --- Transient fields (not saved/loaded directly via BuilderConfig JSON) ---\n    # Instances or non-serializable objects provided programmatically.\n    self._adk_tools_transient: list[ADKBaseTool | Callable] = []\n    self._adk_code_executor_instance: ADKBaseCodeExecutor | None = None\n    self._adk_runner_instance: ADKRunner | None = None\n    self._adk_session_service_instance: ADKSessionService | None = None\n    self._adk_planner_instance: ADKPlanner | None = None\n    self._litellm_budget_manager_instance: BudgetManager | None = None\n    self._user_cost_tracker_instance: UserCostTracker | None = None\n    self._otel_trace_provider_instance: TracerProvider | None = None\n    self._callbacks_transient: dict[str, Callable] = {}\n    # Pre-initialized server instances (less common, but possible)\n    self._a2a_server_instance: A2AServer | None = None\n    self._mcp_server_instance: FastMCP | None = None\n\n    # Set initial log level based on loaded config\n    logger.setLevel(logging.DEBUG if self._config.verbose_logging else logging.INFO)\n    self.with_agent_name(agent_name)\n</code></pre> <code>build()</code> <code>async</code> \u00b6 <p>Constructs and returns the configured EnhancedAgent instance. Handles asynchronous setup like fetching ADK MCP tools.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>async def build(self) -&gt; EnhancedAgent:\n    \"\"\"\n    Constructs and returns the configured EnhancedAgent instance.\n    Handles asynchronous setup like fetching ADK MCP tools.\n    \"\"\"\n    logger.info(f\"--- Building EnhancedAgent: {self._config.agent_name} v{self._config.agent_version} ---\")\n\n    # 1. Final Config Validation (Pydantic model handles most)\n    if not self._config.model_identifier:\n        raise ValueError(\"LLM model identifier is required. Use .with_model()\")\n\n    # 2. Resolve API Key\n    api_key = None\n    if self._config.api_key_env_var:\n        api_key = os.getenv(self._config.api_key_env_var)\n        if not api_key:\n            logger.warning(f\"API key environment variable '{self._config.api_key_env_var}' is set in config but not found in environment.\")\n        # else: logger.debug(\"API key loaded from environment variable.\") # Avoid logging key presence\n\n    # 3. Setup Telemetry Provider (if instance provided)\n    if self._otel_trace_provider_instance and OTEL_AVAILABLE:\n        trace.set_tracer_provider(self._otel_trace_provider_instance)\n        logger.info(\"Global OpenTelemetry TracerProvider set from provided instance.\")\n    elif self._config.telemetry_config.get('enabled') and self._config.telemetry_config.get('type') != 'custom_instance' and OTEL_AVAILABLE:\n         # Basic provider setup from config (can be expanded)\n         logger.info(\"Setting up basic OpenTelemetry based on config (ConsoleExporter example).\")\n         from opentelemetry.sdk.trace.export import (\n             BatchSpanProcessor,\n             ConsoleSpanExporter,\n         )\n         provider = TracerProvider()\n         provider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))\n         #: Add OTLP exporter based on self._config.telemetry_config['endpoint']\n         trace.set_tracer_provider(provider)\n         self._otel_trace_provider_instance = provider # Store for potential access?\n\n    # 4. Prepare Core Components\n    # Agent Model Data\n    try:\n        amd = AgentModelData(\n            name=self._config.agent_name,\n            model=self._config.model_identifier,\n            system_message=self._config.system_message,\n            temperature=self._config.temperature,\n            top_k=self._config.top_k,\n            top_p=self._config.top_p,\n            max_tokens=self._config.max_tokens_output,\n            max_input_tokens=self._config.max_tokens_input,\n            api_key=api_key,\n            api_base=self._config.api_base,\n            api_version=self._config.api_version,\n            stop_sequence=self._config.stop_sequence,\n            user_id=self._config.llm_user_id,\n            budget_manager=self._litellm_budget_manager_instance,\n            caching=self._config.enable_litellm_caching\n        )\n    except ValidationError as e:\n        logger.error(f\"Validation error creating AgentModelData: {e}\")\n        raise\n\n    # World Model\n    world_model = self._config.world_model_initial_data or {}\n\n    # User Cost Tracker\n    cost_tracker = self._user_cost_tracker_instance # Use provided instance if available\n    if not cost_tracker and self._config.cost_tracker_config:\n        tracker_type = self._config.cost_tracker_config.get('type')\n        if tracker_type == 'json':\n            filepath = self._config.cost_tracker_config.get('filepath')\n            if filepath:\n                cost_tracker = JsonFileUserCostTracker(filepath)\n                logger.info(f\"Initialized JsonFileUserCostTracker ({filepath})\")\n            else:\n                logger.warning(\"JSON cost tracker configured but filepath missing.\")\n        elif tracker_type == 'custom_instance':\n             logger.warning(\"Cost tracker configured as 'custom_instance' but no instance was provided via .with_cost_tracker().\")\n        # Add other tracker types (DB, InMemory) here\n\n    # 5. Prepare ADK Components\n    adk_runner_instance = self._adk_runner_instance\n    adk_session_service = self._adk_session_service_instance\n    adk_planner_instance = self._adk_planner_instance\n    adk_code_executor = self._adk_code_executor_instance # Use provided instance first\n    adk_exit_stack = None\n    processed_adk_tools = list(self._adk_tools_transient) # Start with transient tools\n\n    if ADK_AVAILABLE and self._config.adk.enabled:\n        logger.info(\"Configuring ADK components...\")\n        adk_exit_stack = contextlib.AsyncExitStack()\n\n        # --- ADK Runner &amp; Session Service ---\n        if not adk_runner_instance:\n            runner_cls_name = self._config.adk.runner_class_name\n            runner_opts = self._config.adk.runner_options\n            try:\n                # Dynamically import/get runner class\n                if runner_cls_name == \"InMemoryRunner\": runner_class = InMemoryRunner\n                elif runner_cls_name == \"Runner\": runner_class = Runner\n                elif runner_cls_name == \"AsyncWebRunner\": runner_class = AsyncWebRunner # If available\n                else: raise ValueError(f\"Unsupported ADK Runner class name: {runner_cls_name}\")\n\n                # Special handling: InMemoryRunner needs agent instance *later*\n                if runner_class is InMemoryRunner or runner_class is Runner:\n                     logger.debug(\"Deferring InMemoryRunner creation until after agent instantiation.\")\n                     # Store config to create it later\n                     adk_runner_config_for_later = {\n                         \"runner_class\": runner_class,\n                         \"app_name\": runner_opts.get(\"app_name\", f\"{self._config.agent_name}_ADKApp\"),\n                         \"session_service\": adk_session_service, # Pass service if already created\n                         **runner_opts # Pass other options\n                     }\n                     adk_runner_instance = None # Ensure it's None for now\n                else: # Other runners might be creatable now\n                     # Need to ensure session service is handled correctly if runner needs it\n                     if not adk_session_service:\n                         # Create default session service if needed by runner\n                         # This part is complex as runners might create their own\n                         logger.info(\"Using default ADK InMemorySessionService for runner.\")\n                         adk_session_service = InMemorySessionService()\n\n                     adk_runner_instance = runner_class(\n                         session_service=adk_session_service,\n                         app_name=runner_opts.get(\"app_name\", f\"{self._config.agent_name}_ADKApp\"),\n                         **runner_opts # Pass other options\n                     )\n                     logger.info(f\"Created ADK Runner instance: {runner_cls_name}\")\n\n            except (ImportError, ValueError, TypeError) as e:\n                logger.error(f\"Failed to configure ADK Runner '{runner_cls_name}': {e}\", exc_info=True)\n                raise ValueError(f\"Failed to setup ADK Runner: {e}\") from e\n\n        # Ensure session service exists if runner created one\n        if adk_runner_instance and hasattr(adk_runner_instance, 'session_service'):\n             if not adk_session_service:\n                 adk_session_service = adk_runner_instance.session_service\n             elif adk_session_service is not adk_runner_instance.session_service:\n                 logger.warning(\"Provided ADK SessionService differs from the one in the provided ADK Runner. Using the runner's service.\")\n                 adk_session_service = adk_runner_instance.session_service\n\n        # Fallback: create default session service if none exists by now\n        if not adk_session_service:\n              logger.info(\"Using default ADK InMemorySessionService.\")\n              adk_session_service = InMemorySessionService()\n\n\n        # --- ADK Code Executor ---\n        if not adk_code_executor: # If instance wasn't provided directly\n            executor_config = self._config.adk.code_executor_config\n            if executor_config == \"unsafe_simple\":\n                adk_code_executor = UnsafeSimplePythonExecutor()\n                logger.critical(\"UNSAFE code executor instance created!\")\n            elif executor_config == \"secure_placeholder\":\n                adk_code_executor = SecureCodeExecutorPlaceholder()\n                logger.warning(\"SecureCodeExecutorPlaceholder instance created.\")\n            elif executor_config == \"adk_builtin\":\n                # This type uses the TOOL, not an executor instance passed to LlmAgent init\n                adk_code_executor = adk_built_in_code_execution\n                #if not any(getattr(t, 'func', None) == tool_func for t in processed_adk_tools if isinstance(t, FunctionTool)):\n                #     tool_func.__name__ = \"code_execution\"\n                # processed_adk_tools.append(tool_func)\n                #     logger.info(\"Added ADK built-in code execution tool.\")\n                adk_code_executor = None # Ensure no executor instance is passed for this case\n            elif executor_config == \"none\":\n                adk_code_executor = None\n            elif executor_config == \"custom_instance\":\n                # Should have been provided via .with_adk_code_executor_instance()\n                logger.error(\"ADK code executor configured as 'custom_instance' but no instance was provided.\")\n                adk_code_executor = None\n            # Add handling for dict config if needed in the future\n\n        # --- ADK Tools (Wrap callables) ---\n        temp_tools = []\n        for tool_input in processed_adk_tools:\n             if isinstance(tool_input, ADKBaseTool):\n                 temp_tools.append(tool_input)\n             elif callable(tool_input):\n                 try:\n                     wrapped = ADKFunctionTool(func=tool_input)\n                     temp_tools.append(wrapped)\n                 except Exception as e: logger.warning(f\"Could not wrap callable '{getattr(tool_input, '__name__', 'unknown')}' as ADK tool: {e}\")\n             else: logger.warning(f\"Skipping invalid ADK tool input: {type(tool_input)}\")\n        processed_adk_tools = temp_tools\n\n        # --- ADK MCP Toolsets ---\n        for mcp_conf in self._config.adk.mcp_toolset_configs:\n             logger.info(f\"Fetching tools from configured MCP Server: {mcp_conf}...\")\n             try:\n                  params = None\n                  if mcp_conf.get(\"type\") == \"stdio\":\n                      params = StdioServerParameters(command=mcp_conf[\"command\"], args=mcp_conf.get(\"args\", []))\n                  elif mcp_conf.get(\"type\") == \"sse\":\n                       params = SseServerParams(url=mcp_conf[\"url\"])\n\n                  if params:\n                      mcp_tools, _ = await MCPToolset.from_server(\n                          connection_params=params,\n                          async_exit_stack=adk_exit_stack\n                      )\n                      for tool in mcp_tools: tool._is_mcp_tool = True\n                      processed_adk_tools.extend(mcp_tools)\n                      logger.info(f\"Fetched {len(mcp_tools)} tools via ADK MCPToolset ({mcp_conf.get('type')}).\")\n                  else:\n                       logger.warning(f\"Unsupported MCP config type: {mcp_conf.get('type')}\")\n\n             except Exception as e:\n                  logger.error(f\"Failed to fetch tools from MCP server {mcp_conf}: {e}\", exc_info=True)\n                  # Decide whether to raise or continue\n\n        # --- ADK Planner, Examples, Output Schema ---\n\n\n\n    # 6. Instantiate EnhancedAgent\n    try:\n        # Base arguments for EnhancedAgent\n        agent_init_kwargs = {\n            'amd': amd,\n            'world_model': world_model,\n            'format_model': self._config.formatter_llm_model if self._config.formatter_llm_model else None, # Example passing extra config\n            'verbose': self._config.verbose_logging,\n            'stream': self._config.enable_streaming,\n            'max_history_turns': self._config.history.max_turns,\n            'max_history_tokens': self._config.history.max_tokens,\n            'trim_strategy': self._config.history.trim_strategy,\n            'sync_adk_state': self._config.adk.sync_state if ADK_AVAILABLE else False,\n            'adk_exit_stack': adk_exit_stack, # Pass stack for cleanup\n            'user_cost_tracker': cost_tracker, # Pass the tracker instance\n            **self._callbacks_transient, # Pass configured callbacks\n            # Pass server instances if provided (less common)\n            'a2a_server': self._a2a_server_instance,\n            'mcp_server': self._mcp_server_instance,\n        }\n\n        # Add ADK-specific arguments if inheriting from LlmAgent\n        agent_class = EnhancedAgent\n        if ADK_AVAILABLE and issubclass(EnhancedAgent, ADKLlmAgent):\n             logger.debug(\"Adding ADK LlmAgent specific arguments to init.\")\n             adk_specific_kwargs = {\n                 'name': self._config.agent_name, # Required by LlmAgent\n                 'model': LiteLlm(model=self._config.model_identifier), # LlmAgent needs BaseLlm instance\n                 'description': self._config.adk.description or self._config.system_message,\n                 'instruction': self._config.system_message, # Or dedicated instruction field?\n                 'tools': processed_adk_tools,\n                 'code_executor': adk_code_executor, # Pass the *instance*\n                 'planner': adk_planner_instance,\n                 # Process examples/schema if needed\n                 'examples': [ADKExample(**ex) for ex in self._config.adk.examples] if self._config.adk.examples else None,\n                 'output_schema': self._config.adk.output_schema,\n                 # Pass runner/session service if NOT using InMemoryRunner deferred creation\n                 # If runner is created later, it's assigned post-init\n                 'runner': adk_runner_instance if adk_runner_instance else None, # Pass runner if created now\n                 'session_service': adk_session_service, # Pass session service\n             }\n             # Merge, ensuring agent_init_kwargs takes precedence for overlapping basic fields if necessary\n             # but allow ADK specifics to be added. Be careful with overlaps like 'name'.\n             # EnhancedAgent init should handle reconciling these if needed.\n             # A safer merge:\n             final_kwargs = agent_init_kwargs.copy()\n             for k, v in adk_specific_kwargs.items():\n                  if k not in final_kwargs: # Only add ADK specifics not already handled\n                      final_kwargs[k] = v\n                  # Handle specific overrides/merges needed for LlmAgent base\n                  elif k == 'tools' and v: # Merge tools\n                      final_kwargs['tools'] = (final_kwargs.get('tools') or []) + v\n                  # Overwrite description/instruction from ADK config if set\n                  elif k in ['description', 'instruction'] and v or k == 'code_executor' or k == 'model':\n                       final_kwargs[k] = v\n\n             agent_init_kwargs = final_kwargs\n\n\n        logger.debug(f\"Final keys for EnhancedAgent init: {list(agent_init_kwargs.keys())}\")\n\n        # --- Instantiate the Agent ---\n        agent = agent_class(**agent_init_kwargs)\n        # --- Agent Instantiated ---\n\n        # If ADK InMemoryRunner creation was deferred, create and assign now\n        if ADK_AVAILABLE and 'adk_runner_config_for_later' in locals():\n             cfg = locals()['adk_runner_config_for_later']\n             if not isinstance(cfg['runner_class'], InMemoryRunner) and cfg.get('session_service') is None: cfg['session_service'] = agent.adk_session_service # Ensure service is passed\n             agent.setup_adk_runner(cfg)\n             logger.info(f\"Created and assigned deferred ADK Runner instance: {agent.adk_runner.__class__.__name__}\")\n             # Ensure agent has runner's session service if it differs\n             if agent.adk_runner and agent.adk_session_service is not agent.adk_runner.session_service:\n                  logger.warning(\"Agent session service differs from deferred runner's service. Updating agent's reference.\")\n                  agent.adk_session_service = agent.adk_runner.session_service\n        elif ADK_AVAILABLE and adk_runner_instance and not agent.adk_runner:\n            # If runner was created earlier but not passed via LlmAgent init (e.g. non-LlmAgent base)\n            # Or if we want to explicitly assign it\n             agent.adk_runner = adk_runner_instance\n             # Ensure session service consistency\n             if agent.adk_session_service is not agent.adk_runner.session_service:\n                  agent.adk_session_service = agent.adk_runner.session_service\n\n\n    except ValidationError as e:\n        logger.error(f\"Pydantic validation error Instantiating EnhancedAgent: {e}\", exc_info=True)\n        raise\n    except Exception as e:\n        logger.error(f\"Unexpected error Instantiating EnhancedAgent: {e}\", exc_info=True)\n        raise\n\n    # 7. Setup Agent's Internal Server Capabilities (if enabled and not pre-initialized)\n    if self._config.a2a.enabled and not agent.a2a_server:\n        if AGENT_A2A_AVAILABLE:\n            logger.info(\"Setting up A2A server on agent instance...\")\n            agent.setup_a2a_server(\n                host=self._config.a2a.host,\n                port=self._config.a2a.port,\n                **self._config.a2a.extra_options\n            )\n        else: logger.warning(\"A2A server configured in builder, but A2A not available in agent environment.\")\n\n    if self._config.mcp.enabled and not agent.mcp_server:\n        if AGENT_MCP_AVAILABLE:\n            logger.info(\"Setting up MCP server on agent instance...\")\n            agent.setup_mcp_server(\n                host=self._config.mcp.host,\n                port=self._config.mcp.port,\n                name=self._config.mcp.server_name, # Already defaulted\n                **self._config.mcp.extra_options\n            )\n        else: logger.warning(\"MCP server configured in builder, but MCP not available in agent environment.\")\n\n    # 8. Setup A2A known clients configuration on the agent\n    if self._config.a2a.known_clients:\n         if AGENT_A2A_AVAILABLE:\n             # The agent likely handles client creation on demand,\n             # but we can pass the config for it to use.\n             # Assuming agent has a way to receive this, e.g., during init or a setter\n             if hasattr(agent, 'set_known_a2a_clients'):\n                 agent.set_known_a2a_clients(self._config.a2a.known_clients)\n             else:\n                  # Fallback: store on a generic config dict? Less ideal.\n                  # agent.config.a2a_known_clients = self._config.a2a.known_clients\n                  logger.warning(\"Agent does not have 'set_known_a2a_clients' method. Known client config stored raw.\")\n         else:\n              logger.warning(\"A2A known clients configured, but A2A not available in agent env.\")\n\n\n    logger.info(f\"--- EnhancedAgent Build Complete: {agent.amd.name} ---\")\n    return agent\n</code></pre> <code>enable_adk(runner_class=InMemoryRunner, runner_options=None)</code> \u00b6 <p>Enables ADK integration with a specified runner.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def enable_adk(self, runner_class: type[ADKRunner] = InMemoryRunner, runner_options: dict[str, Any] | None = None) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Enables ADK integration with a specified runner.\"\"\"\n    if not self._ensure_adk(\"Runner\"): return self\n    self._config.adk.runner_class_name = runner_class.__name__\n    self._config.adk.runner_options = runner_options or {}\n    logger.info(f\"ADK integration enabled with runner: {self._config.adk.runner_class_name}\")\n    return self\n</code></pre> <code>load_config(path)</code> \u00b6 <p>Loads builder configuration from a JSON file, overwriting current settings.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def load_config(self, path: str | Path) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Loads builder configuration from a JSON file, overwriting current settings.\"\"\"\n    filepath = Path(path)\n    if not filepath.exists():\n        raise FileNotFoundError(f\"Builder configuration file not found: {filepath}\")\n    try:\n        with open(filepath) as f:\n            config_data = json.load(f)\n        self._config = BuilderConfig.model_validate(config_data)\n        logger.info(f\"Builder configuration loaded from {filepath}\")\n        # Reset transient fields, as they are not saved\n        self._reset_transient_fields()\n        logger.warning(\"Transient fields (callbacks, tool instances, tracker instance, etc.) reset. Re-add them if needed.\")\n        # Update logger level based on loaded config\n        logger.setLevel(logging.DEBUG if self._config.verbose_logging else logging.INFO)\n    except (OSError, json.JSONDecodeError) as e:\n        logger.error(f\"Failed to load or parse builder configuration from {filepath}: {e}\")\n        raise\n    except ValidationError as e:\n         logger.error(f\"Loaded configuration data is invalid: {e}\")\n         raise\n    return self\n</code></pre> <code>save_config(path, indent=2)</code> \u00b6 <p>Saves the current builder configuration to a JSON file.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def save_config(self, path: str | Path, indent: int = 2):\n    \"\"\"Saves the current builder configuration to a JSON file.\"\"\"\n    filepath = Path(path)\n    try:\n        filepath.parent.mkdir(parents=True, exist_ok=True)\n        config_json = self._config.model_dump_json(indent=indent)\n        with open(filepath, 'w') as f:\n            f.write(config_json)\n        logger.info(f\"Builder configuration saved to {filepath}\")\n    except OSError as e:\n        logger.error(f\"Failed to save builder configuration to {filepath}: {e}\")\n    except ValidationError as e:\n         logger.error(f\"Configuration is invalid, cannot save: {e}\")\n    except Exception as e:\n         logger.error(f\"An unexpected error occurred during config save: {e}\")\n</code></pre> <code>with_adk_code_executor(executor_type)</code> \u00b6 <p>Configures the type of ADK code executor to use (saved in config).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_adk_code_executor(self, executor_type: Literal[\"adk_builtin\", \"unsafe_simple\", \"secure_placeholder\", \"none\"]) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Configures the type of ADK code executor to use (saved in config).\"\"\"\n    if not self._ensure_adk(\"Code Executor Type\"): return self\n    if executor_type == \"unsafe_simple\":\n        logger.critical(\"***********************************************************\")\n        logger.critical(\"*** WARNING: Configuring UNSAFE SimplePythonExecutor!   ***\")\n        logger.critical(\"***********************************************************\")\n    elif executor_type == \"secure_placeholder\":\n        logger.warning(\"Configuring SecureCodeExecutorPlaceholder. Implement actual sandboxing!\")\n    elif executor_type == \"adk_builtin\":\n        if self._config.model_identifier and (\"gemini-1.5\" not in self._config.model_identifier and \"gemini-2\" not in self._config.model_identifier) :\n            logger.warning(f\"ADK built-in code execution selected, but model '{self._config.model_identifier}' might not support it. Ensure model compatibility.\")\n        logger.info(\"Configuring ADK built-in code execution (tool-based, requires compatible model).\")\n\n    self._config.adk.code_executor_config = executor_type\n    self._adk_code_executor_instance = None # Clear any previously set instance\n    return self\n</code></pre> <code>with_adk_code_executor_instance(executor)</code> \u00b6 <p>Provides a pre-initialized ADK code executor instance (transient).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_adk_code_executor_instance(self, executor: ADKBaseCodeExecutor) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Provides a pre-initialized ADK code executor instance (transient).\"\"\"\n    if not self._ensure_adk(\"Code Executor Instance\"): return self\n    if not isinstance(executor, ADKBaseCodeExecutor):\n        raise TypeError(f\"Expected ADKBaseCodeExecutor instance, got {type(executor)}\")\n    self._adk_code_executor_instance = executor\n    self._config.adk.code_executor_config = \"custom_instance\" # Mark config\n    logger.info(f\"Using custom ADK code executor instance: {type(executor).__name__}\")\n    return self\n</code></pre> <code>with_adk_mcp_toolset(connection_type, **kwargs)</code> \u00b6 <p>Configures an ADK MCP Toolset connection (saved in config).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_adk_mcp_toolset(self, connection_type: Literal[\"stdio\", \"sse\"], **kwargs) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Configures an ADK MCP Toolset connection (saved in config).\"\"\"\n    if not self._ensure_adk(\"MCP Toolset\"): return self\n    if connection_type == \"stdio\":\n        if \"command\" not in kwargs: raise ValueError(\"Stdio MCP toolset requires 'command' argument.\")\n        config = {\"type\": \"stdio\", \"command\": kwargs[\"command\"], \"args\": kwargs.get(\"args\", [])}\n    elif connection_type == \"sse\":\n        if \"url\" not in kwargs: raise ValueError(\"SSE MCP toolset requires 'url' argument.\")\n        config = {\"type\": \"sse\", \"url\": kwargs[\"url\"]}\n    else:\n        raise ValueError(f\"Unknown MCP toolset connection type: {connection_type}\")\n    self._config.adk.mcp_toolset_configs.append(config)\n    logger.info(f\"Configured ADK MCP Toolset: {config}\")\n    return self\n</code></pre> <code>with_adk_tool_function(func)</code> \u00b6 <p>Adds a callable function as an ADK tool (transient).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_adk_tool_function(self, func: Callable) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Adds a callable function as an ADK tool (transient).\"\"\"\n    if not self._ensure_adk(\"Tool Function\"): return self\n    if not callable(func):\n        raise TypeError(f\"Expected callable function for ADK tool, got {type(func)}\")\n    self._adk_tools_transient.append(func)\n    return self\n</code></pre> <code>with_adk_tool_instance(tool)</code> \u00b6 <p>Adds a pre-initialized ADK Tool instance (transient).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_adk_tool_instance(self, tool: ADKBaseTool) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Adds a pre-initialized ADK Tool instance (transient).\"\"\"\n    if not self._ensure_adk(\"Tool Instance\"): return self\n    if not isinstance(tool, ADKBaseTool):\n        raise TypeError(f\"Expected ADK BaseTool instance, got {type(tool)}\")\n    self._adk_tools_transient.append(tool)\n    return self\n</code></pre> <code>with_cost_tracker(tracker)</code> \u00b6 <p>Provides a pre-initialized UserCostTracker instance (transient).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_cost_tracker(self, tracker: UserCostTracker) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Provides a pre-initialized UserCostTracker instance (transient).\"\"\"\n    if not hasattr(tracker, \"get_all_costs\"): # Check protocol using isinstance\n         raise TypeError(\"Cost tracker must implement the UserCostTracker protocol.\")\n    self._user_cost_tracker_instance = tracker\n    # Clear file config if instance is provided\n    self._config.cost_tracker_config = {'type': 'custom_instance'}\n    logger.info(f\"Using custom UserCostTracker instance: {type(tracker).__name__}\")\n    return self\n</code></pre> <code>with_json_cost_tracker(filepath)</code> \u00b6 <p>Configures the builder to use the JsonFileUserCostTracker (saved in config).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_json_cost_tracker(self, filepath: str | Path) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Configures the builder to use the JsonFileUserCostTracker (saved in config).\"\"\"\n    self._config.cost_tracker_config = {'type': 'json', 'filepath': str(filepath)}\n    self._user_cost_tracker_instance = None # Clear any instance\n    logger.info(f\"Configured JsonFileUserCostTracker: {filepath}\")\n    return self\n</code></pre> <code>with_litellm_budget_manager(manager)</code> \u00b6 <p>Provides a pre-initialized LiteLLM BudgetManager instance (transient).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_litellm_budget_manager(self, manager: BudgetManager) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Provides a pre-initialized LiteLLM BudgetManager instance (transient).\"\"\"\n    if not LITELLM_AVAILABLE:\n         logger.warning(\"LiteLLM not available, cannot set BudgetManager.\")\n         return self\n    if not isinstance(manager, BudgetManager):\n        raise TypeError(\"Expected litellm.BudgetManager instance.\")\n    self._litellm_budget_manager_instance = manager\n    return self\n</code></pre> <code>with_telemetry_provider_instance(provider)</code> \u00b6 <p>Provides a pre-initialized OpenTelemetry TracerProvider instance (transient).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_telemetry_provider_instance(self, provider: TracerProvider) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Provides a pre-initialized OpenTelemetry TracerProvider instance (transient).\"\"\"\n    if not OTEL_AVAILABLE:\n        logger.warning(\"OpenTelemetry SDK not available. Cannot set TracerProvider.\")\n        return self\n    if not isinstance(provider, TracerProvider):\n         raise TypeError(\"Expected opentelemetry.sdk.trace.TracerProvider instance.\")\n    self._otel_trace_provider_instance = provider\n    # Mark telemetry as enabled, but using custom instance\n    self._config.telemetry_config = {'enabled': True, 'type': 'custom_instance'}\n    logger.info(\"Using custom OpenTelemetry TracerProvider instance.\")\n    return self\n</code></pre> <code>JsonFileUserCostTracker</code> \u00b6 <p>Stores user costs persistently in a JSON file.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>class JsonFileUserCostTracker:\n    \"\"\"Stores user costs persistently in a JSON file.\"\"\"\n    def __init__(self, filepath: str | Path):\n        self.filepath = Path(filepath)\n        self._costs: dict[str, float] = {}\n        self._lock = threading.Lock()\n        self.load() # Load costs on initialization\n\n    def get_cost(self, user_id: str) -&gt; float:\n        with self._lock:\n            return self._costs.get(user_id, 0.0)\n\n    def add_cost(self, user_id: str, cost: float) -&gt; None:\n        if not user_id:\n            logger.warning(\"Cost tracking skipped: user_id is missing.\")\n            return\n        if cost &gt; 0:\n            with self._lock:\n                self._costs[user_id] = self._costs.get(user_id, 0.0) + cost\n                logger.debug(f\"Cost added for user '{user_id}': +{cost:.6f}. New total: {self._costs[user_id]:.6f}\")\n            # Optional: Auto-save periodically or based on number of updates\n            # For simplicity, we rely on explicit save() or agent close\n\n    def get_all_costs(self) -&gt; dict[str, float]:\n        with self._lock:\n            return self._costs.copy()\n\n    def save(self) -&gt; None:\n        with self._lock:\n            try:\n                self.filepath.parent.mkdir(parents=True, exist_ok=True)\n                with open(self.filepath, 'w') as f:\n                    json.dump(self._costs, f, indent=2)\n                logger.info(f\"User costs saved to {self.filepath}\")\n            except OSError as e:\n                logger.error(f\"Failed to save user costs to {self.filepath}: {e}\")\n\n    def load(self) -&gt; None:\n        with self._lock:\n            if self.filepath.exists():\n                try:\n                    with open(self.filepath) as f:\n                        self._costs = json.load(f)\n                    logger.info(f\"User costs loaded from {self.filepath}\")\n                except (OSError, json.JSONDecodeError) as e:\n                    logger.error(f\"Failed to load user costs from {self.filepath}: {e}. Starting fresh.\")\n                    self._costs = {}\n            else:\n                logger.info(f\"User cost file not found ({self.filepath}). Starting fresh.\")\n                self._costs = {}\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.save()\n</code></pre> <code>UserCostTracker</code> \u00b6 <p>               Bases: <code>Protocol</code></p> <p>Protocol for tracking costs per user.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>class UserCostTracker(Protocol):\n    \"\"\"Protocol for tracking costs per user.\"\"\"\n    def get_cost(self, user_id: str) -&gt; float: ...\n    def add_cost(self, user_id: str, cost: float) -&gt; None: ...\n    def get_all_costs(self) -&gt; dict[str, float]: ...\n    def save(self) -&gt; None: ...\n    def load(self) -&gt; None: ...\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent.config","title":"<code>config</code>","text":"<code>A2AConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Configuration for A2A integration.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>class A2AConfig(BaseModel):\n    \"\"\"Configuration for A2A integration.\"\"\"\n    server: dict[str, Any] | None = Field(default=None, description=\"Configuration to run an A2A server (host, port, etc.).\")\n    known_agents: dict[str, str] = Field(default_factory=dict, description=\"Named A2A agent URLs to interact with (e.g., {'weather_agent': 'http://weather:5000'}).\")\n    default_task_timeout: int = Field(default=120, description=\"Default timeout in seconds for waiting on A2A task results.\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre> <code>ADKConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Configuration for ADK integration.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>class ADKConfig(BaseModel):\n    \"\"\"Configuration for ADK integration.\"\"\"\n    enabled: bool = Field(default=True, description=\"Enable ADK features if ADK is installed.\")\n    description: str | None = Field(default=None, description=\"ADK LlmAgent description.\")\n    instruction_override: str | None = Field(default=None, description=\"Override agent's system message for ADK.\")\n    # Tools added via builder or auto-discovery\n    code_executor: str | BaseCodeExecutor | None = Field(default=None, description=\"Reference name or instance of ADK code executor.\")\n    planner: str | BasePlanner | None = Field(default=None, description=\"Reference name or instance of ADK planner.\")\n    examples: list[Example] | None = Field(default=None, description=\"Few-shot examples for ADK.\")\n    output_schema: type[BaseModel] | None = Field(default=None, description=\"Pydantic model for structured output.\")\n    # MCP Toolset config handled separately if ADK is enabled\n    use_mcp_toolset: bool = Field(default=True, description=\"Use ADK's MCPToolset for MCP client connections if ADK is enabled.\")\n    # Runner config handled separately\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre> <code>AgentConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Main configuration schema for an EnhancedAgent.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>class AgentConfig(BaseModel):\n    \"\"\"Main configuration schema for an EnhancedAgent.\"\"\"\n    agent_name: str = Field(..., description=\"Unique name for this agent instance.\")\n    version: str = Field(default=\"0.1.0\")\n\n    agent_instruction: str = Field(default=\"You are a helpful AI assistant. Answer user questions to the best of your knowledge. Respond concisely. use tools when needed\")\n    agent_description: str = Field(default=\"An configurable, production-ready agent with integrated capabilities.\")\n\n    # Model Selection\n    models: list[ModelConfig] = Field(..., description=\"List of available LLM configurations.\")\n    default_llm_model: str = Field(..., description=\"Name of the ModelConfig to use for general LLM calls.\")\n    formatter_llm_model: str | None = Field(default=None, description=\"Optional: Name of a faster/cheaper ModelConfig for a_format_class calls.\")\n\n    # Core Agent Settings\n    world_model_initial_data: dict[str, Any] | None = Field(default=None)\n    enable_streaming: bool = Field(default=False)\n    verbose: bool = Field(default=False)\n    log_level: str = Field(default=\"INFO\", description=\"Logging level (DEBUG, INFO, WARNING, ERROR).\")\n    max_history_length: int = Field(default=20, description=\"Max conversation turns for LiteLLM history.\")\n    trim_strategy: Literal[\"litellm\", \"basic\"] = Field(default=\"litellm\")\n    persist_history: bool = Field(default=True, description=\"Persist conversation history (requires persistent ChatSession).\")\n    user_id_default: str | None = Field(default=None, description=\"Default user ID for interactions.\")\n\n    # Secure Code Execution\n    code_executor_type: Literal[\"restricted\", \"docker\", \"none\"] = Field(default=\"restricted\", description=\"Type of code executor to use.\")\n    code_executor_config: dict[str, Any] = Field(default_factory=dict, description=\"Configuration specific to the chosen code executor.\")\n    enable_adk_code_execution_tool: bool = Field(default=True, description=\"Expose code execution as an ADK tool if ADK is enabled.\")\n\n    # Framework Integrations\n    adk: ADKConfig | None = Field(default_factory=ADKConfig if ADK_AVAILABLE_CONF else lambda: None)\n    mcp: MCPConfig | None = Field(default_factory=MCPConfig if MCP_AVAILABLE_CONF else lambda: None)\n    a2a: A2AConfig | None = Field(default_factory=A2AConfig if A2A_AVAILABLE_CONF else lambda: None)\n\n    # Observability &amp; Cost\n    observability: ObservabilityConfig | None = Field(default_factory=ObservabilityConfig)\n    budget_manager: BudgetManager | None = Field(default=None, description=\"Global LiteLLM budget manager instance.\") # Needs to be passed in\n\n    # Human-in-the-Loop\n    enable_hitl: bool = Field(default=False, description=\"Enable basic Human-in-the-Loop hooks.\")\n\n    # Add other global settings as needed\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode='after')\n    def validate_model_references(self) -&gt; 'AgentConfig':\n        model_names = {m.name for m in self.models}\n        if self.default_llm_model not in model_names:\n            raise ValueError(f\"default_llm_model '{self.default_llm_model}' not found in defined models.\")\n        if self.formatter_llm_model and self.formatter_llm_model not in model_names:\n            raise ValueError(f\"formatter_llm_model '{self.formatter_llm_model}' not found in defined models.\")\n        return self\n\n    @model_validator(mode='after')\n    def validate_framework_availability(self) -&gt; 'AgentConfig':\n        if self.adk and self.adk.enabled and not ADK_AVAILABLE_CONF:\n            logger.warning(\"ADK configuration provided but ADK library not installed. Disabling ADK features.\")\n            self.adk.enabled = False\n        if self.mcp and (self.mcp.server or self.mcp.client_connections) and not MCP_AVAILABLE_CONF:\n             logger.warning(\"MCP configuration provided but MCP library not installed. Disabling MCP features.\")\n             self.mcp = None # Or disable specific parts\n        if self.a2a and (self.a2a.server or self.a2a.known_agents) and not A2A_AVAILABLE_CONF:\n             logger.warning(\"A2A configuration provided but A2A library not installed. Disabling A2A features.\")\n             self.a2a = None # Or disable specific parts\n        return self\n\n    @classmethod\n    def load_from_yaml(cls, path: str | Path) -&gt; 'AgentConfig':\n        \"\"\"Loads configuration from a YAML file.\"\"\"\n        file_path = Path(path)\n        if not file_path.is_file():\n            raise FileNotFoundError(f\"Configuration file not found: {path}\")\n        with open(file_path) as f:\n            config_data = yaml.safe_load(f)\n        logger.info(f\"Loaded agent configuration from {path}\")\n        return cls(**config_data)\n\n    def save_to_yaml(self, path: str | Path):\n        \"\"\"Saves the current configuration to a YAML file.\"\"\"\n        file_path = Path(path)\n        file_path.parent.mkdir(parents=True, exist_ok=True)\n        with open(file_path, 'w') as f:\n            # Use Pydantic's model_dump for clean serialization\n            yaml.dump(self.model_dump(mode='python'), f, sort_keys=False)\n        logger.info(f\"Saved agent configuration to {path}\")\n</code></pre> <code>load_from_yaml(path)</code> <code>classmethod</code> \u00b6 <p>Loads configuration from a YAML file.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>@classmethod\ndef load_from_yaml(cls, path: str | Path) -&gt; 'AgentConfig':\n    \"\"\"Loads configuration from a YAML file.\"\"\"\n    file_path = Path(path)\n    if not file_path.is_file():\n        raise FileNotFoundError(f\"Configuration file not found: {path}\")\n    with open(file_path) as f:\n        config_data = yaml.safe_load(f)\n    logger.info(f\"Loaded agent configuration from {path}\")\n    return cls(**config_data)\n</code></pre> <code>save_to_yaml(path)</code> \u00b6 <p>Saves the current configuration to a YAML file.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>def save_to_yaml(self, path: str | Path):\n    \"\"\"Saves the current configuration to a YAML file.\"\"\"\n    file_path = Path(path)\n    file_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(file_path, 'w') as f:\n        # Use Pydantic's model_dump for clean serialization\n        yaml.dump(self.model_dump(mode='python'), f, sort_keys=False)\n    logger.info(f\"Saved agent configuration to {path}\")\n</code></pre> <code>MCPConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Configuration for MCP integration.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>class MCPConfig(BaseModel):\n    \"\"\"Configuration for MCP integration.\"\"\"\n    server: dict[str, Any] | None = Field(default=None, description=\"Configuration to run an MCP server (host, port, etc.).\")\n    client_connections: dict[str, str] = Field(default_factory=dict, description=\"Named MCP server URLs to connect to as a client (e.g., {'files': 'stdio:npx @mcp/server-filesystem /data'}).\")\n    # ADK's MCPToolset handles client connections if ADKConfig.use_mcp_toolset is True\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre> <code>ModelConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Configuration specific to an LLM model via LiteLLM.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>class ModelConfig(BaseModel):\n    \"\"\"Configuration specific to an LLM model via LiteLLM.\"\"\"\n    # Used as key for model selection\n    name: str = Field(..., description=\"Unique identifier/alias for this model configuration (e.g., 'fast_formatter', 'main_reasoner').\")\n    model: str = Field(..., description=\"LiteLLM model string (e.g., 'gemini/gemini-1.5-pro-latest', 'ollama/mistral').\")\n    provider: str | None = Field(default=None, description=\"LiteLLM provider override if needed.\")\n    api_key: str | None = Field(default=None, description=\"API Key (consider using environment variables).\")\n    api_base: str | None = Field(default=None, description=\"API Base URL (for local models, proxies).\")\n    api_version: str | None = Field(default=None, description=\"API Version (e.g., for Azure).\")\n\n    # Common LLM Parameters\n    temperature: float | None = Field(default=0.7)\n    top_p: float | None = Field(default=None)\n    top_k: int | None = Field(default=None)\n    max_tokens: int | None = Field(default=2048, description=\"Max tokens for generation.\")\n    max_input_tokens: int | None = Field(default=None, description=\"Max input context window (autodetected if None).\")\n    stop_sequence: list[str] | None = Field(default=None)\n    presence_penalty: float | None = Field(default=None)\n    frequency_penalty: float | None = Field(default=None)\n    system_message: str | None = Field(default=None, description=\"Default system message for this model.\")\n\n    # LiteLLM Specific\n    caching: bool = Field(default=True, description=\"Enable LiteLLM caching for this model.\")\n    # budget_manager: Optional[BudgetManager] = Field(default=None) # Budget manager applied globally or per-agent\n\n    model_config = ConfigDict(arbitrary_types_allowed=True, extra='allow') # Allow extra LiteLLM params\n</code></pre> <code>ObservabilityConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Configuration for observability (OpenTelemetry).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>class ObservabilityConfig(BaseModel):\n    \"\"\"Configuration for observability (OpenTelemetry).\"\"\"\n    enabled: bool = Field(default=True)\n    endpoint: str | None = Field(default=None, description=\"OTLP endpoint URL (e.g., http://jaeger:4317).\")\n    service_name: str | None = Field(default=None, description=\"Service name for traces/metrics (defaults to agent name).\")\n    # Add more OTel config options as needed (headers, certs, resource attributes)\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent.executors","title":"<code>executors</code>","text":"<code>DockerCodeExecutor</code> \u00b6 <p>               Bases: <code>_BaseExecutorClass</code></p> <p>Executes Python code in a sandboxed Docker container.</p> <p>Requires Docker to be installed and running, and the 'docker' Python SDK.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/executors.py</code> <pre><code>class DockerCodeExecutor(_BaseExecutorClass):\n    \"\"\"\n    Executes Python code in a sandboxed Docker container.\n\n    Requires Docker to be installed and running, and the 'docker' Python SDK.\n    \"\"\"\n    DEFAULT_DOCKER_IMAGE = \"python:3.10-slim\" # Use a minimal image\n    DEFAULT_TIMEOUT = 10 # Seconds\n    DEFAULT_MEM_LIMIT = \"128m\"\n    DEFAULT_CPUS = 0.5\n\n    def __init__(self,\n                 docker_image: str = DEFAULT_DOCKER_IMAGE,\n                 timeout: int = DEFAULT_TIMEOUT,\n                 mem_limit: str = DEFAULT_MEM_LIMIT,\n                 cpus: float = DEFAULT_CPUS,\n                 network_mode: str = \"none\", # Disable networking by default for security\n                 docker_client_config: dict | None = None):\n        if not DOCKER_AVAILABLE:\n            raise ImportError(\"Docker SDK not installed ('pip install docker'). Cannot use DockerCodeExecutor.\")\n\n        self.docker_image = docker_image\n        self.timeout = timeout\n        self.mem_limit = mem_limit\n        self.cpus = cpus\n        self.network_mode = network_mode\n        try:\n            self.client = docker.from_env(**(docker_client_config or {}))\n            self.client.ping() # Check connection\n            # Ensure image exists locally or pull it\n            try:\n                self.client.images.get(self.docker_image)\n                logger.info(f\"Docker image '{self.docker_image}' found locally.\")\n            except ImageNotFound:\n                logger.warning(f\"Docker image '{self.docker_image}' not found locally. Attempting to pull...\")\n                try:\n                    self.client.images.pull(self.docker_image)\n                    logger.info(f\"Successfully pulled Docker image '{self.docker_image}'.\")\n                except APIError as pull_err:\n                    raise RuntimeError(f\"Failed to pull Docker image '{self.docker_image}': {pull_err}\") from pull_err\n        except Exception as e:\n            raise RuntimeError(f\"Failed to connect to Docker daemon: {e}. Is Docker running?\") from e\n        logger.info(f\"DockerCodeExecutor initialized (Image: {docker_image}, Timeout: {timeout}s, Network: {network_mode})\")\n\n    def _execute(self, code: str) -&gt; dict[str, Any]:\n        \"\"\"Internal execution logic.\"\"\"\n        result = {\"stdout\": \"\", \"stderr\": \"\", \"error\": None, \"exit_code\": None}\n        container = None\n\n        try:\n            logger.debug(f\"Creating Docker container from image '{self.docker_image}'...\")\n            container = self.client.containers.run(\n                image=self.docker_image,\n                command=[\"python\", \"-c\", code],\n                detach=True,\n                mem_limit=self.mem_limit,\n                nano_cpus=int(self.cpus * 1e9),\n                network_mode=self.network_mode,\n                # Security considerations: Consider read-only filesystem, dropping capabilities\n                read_only=True,\n                # working_dir=\"/app\", # Define a working dir if needed\n                # volumes={...} # Mount volumes carefully if required\n            )\n            logger.debug(f\"Container '{container.short_id}' started.\")\n\n            # Wait for container completion with timeout\n            container_result = container.wait(timeout=self.timeout)\n            result[\"exit_code\"] = container_result.get(\"StatusCode\", None)\n\n            # Retrieve logs\n            result[\"stdout\"] = container.logs(stdout=True, stderr=False).decode('utf-8', errors='replace').strip()\n            result[\"stderr\"] = container.logs(stdout=False, stderr=True).decode('utf-8', errors='replace').strip()\n\n            logger.debug(f\"Container '{container.short_id}' finished with exit code {result['exit_code']}.\")\n            if result[\"exit_code\"] != 0:\n                 logger.warning(f\"Container stderr: {result['stderr'][:500]}...\") # Log stderr on failure\n\n        except ContainerError as e:\n            result[\"error\"] = f\"ContainerError: {e}\"\n            result[\"stderr\"] = e.stderr.decode('utf-8', errors='replace').strip() if e.stderr else str(e)\n            result[\"exit_code\"] = e.exit_status\n            logger.error(f\"Container '{container.short_id if container else 'N/A'}' failed: {result['error']}\\nStderr: {result['stderr']}\")\n        except APIError as e:\n            result[\"error\"] = f\"Docker APIError: {e}\"\n            result[\"exit_code\"] = -1\n            logger.error(f\"Docker API error during execution: {e}\")\n        except Exception as e:\n            # Catch potential timeout errors from container.wait or other unexpected issues\n            result[\"error\"] = f\"Unexpected execution error: {type(e).__name__}: {e}\"\n            result[\"exit_code\"] = -1\n            # Check if it looks like a timeout\n            if isinstance(e, TimeoutError) or \"Timeout\" in str(e): # docker SDK might raise requests.exceptions.ReadTimeout\n                result[\"stderr\"] = f\"Execution timed out after {self.timeout} seconds.\"\n                logger.warning(f\"Container execution timed out ({self.timeout}s).\")\n            else:\n                logger.error(f\"Unexpected error during Docker execution: {e}\", exc_info=True)\n        finally:\n            if container:\n                try:\n                    logger.debug(f\"Removing container '{container.short_id}'...\")\n                    container.remove(force=True)\n                except APIError as rm_err:\n                    logger.warning(f\"Failed to remove container {container.short_id}: {rm_err}\")\n\n        return result\n\n     # --- ADK Compatibility Method ---\n    if ADK_EXEC_AVAILABLE:\n        def execute_code(self, invocation_context: InvocationContext, code_input: CodeExecutionInput) -&gt; CodeExecutionResult:\n            logger.debug(f\"DockerCodeExecutor executing ADK request (lang: {code_input.language}). Code: {code_input.code[:100]}...\")\n            if code_input.language.lower() != 'python':\n                 return CodeExecutionResult(output=f\"Error: Unsupported language '{code_input.language}'. Only Python is supported.\", outcome=\"OUTCOME_FAILURE\")\n\n            exec_result = self._execute(code_input.code)\n\n            output_str = \"\"\n            if exec_result[\"stdout\"]:\n                output_str += f\"Stdout:\\n{exec_result['stdout']}\\n\"\n            if exec_result[\"stderr\"]:\n                 output_str += f\"Stderr:\\n{exec_result['stderr']}\\n\"\n            if not output_str and exec_result[\"exit_code\"] == 0:\n                 output_str = \"Execution successful with no output.\"\n            elif not output_str and exec_result[\"exit_code\"] != 0:\n                 output_str = f\"Execution failed with no output (Exit code: {exec_result['exit_code']}). Error: {exec_result['error']}\"\n\n            outcome = \"OUTCOME_SUCCESS\" if exec_result[\"exit_code\"] == 0 else \"OUTCOME_FAILURE\"\n\n            return CodeExecutionResult(output=output_str.strip(), outcome=outcome)\n    # --- End ADK Compatibility ---\n\n    # --- Direct Call Method ---\n    def execute(self, code: str) -&gt; dict[str, Any]:\n        \"\"\"Directly execute code, returning detailed dictionary.\"\"\"\n        logger.debug(f\"DockerCodeExecutor executing direct call. Code: {code[:100]}...\")\n        return self._execute(code)\n</code></pre> <code>execute(code)</code> \u00b6 <p>Directly execute code, returning detailed dictionary.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/executors.py</code> <pre><code>def execute(self, code: str) -&gt; dict[str, Any]:\n    \"\"\"Directly execute code, returning detailed dictionary.\"\"\"\n    logger.debug(f\"DockerCodeExecutor executing direct call. Code: {code[:100]}...\")\n    return self._execute(code)\n</code></pre> <code>RestrictedPythonExecutor</code> \u00b6 <p>               Bases: <code>_BaseExecutorClass</code></p> <p>Executes Python code using restrictedpython.</p> <p>Safer than exec() but NOT a full sandbox. Known vulnerabilities exist. Use with extreme caution and only with trusted code sources or for low-risk operations. Docker is strongly recommended for untrusted code.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/executors.py</code> <pre><code>class RestrictedPythonExecutor(_BaseExecutorClass):\n    \"\"\"\n    Executes Python code using restrictedpython.\n\n    Safer than exec() but NOT a full sandbox. Known vulnerabilities exist.\n    Use with extreme caution and only with trusted code sources or for\n    low-risk operations. Docker is strongly recommended for untrusted code.\n    \"\"\"\n    DEFAULT_ALLOWED_GLOBALS = {\n        **safe_globals,\n        '_print_': restrictedpython.PrintCollector,\n        '_getattr_': restrictedpython.safe_getattr,\n        '_getitem_': restrictedpython.safe_getitem,\n        '_write_': restrictedpython.guarded_setattr, # Allows modifying specific safe objects if needed\n        # Add other safe builtins or modules carefully\n        'math': __import__('math'),\n        'random': __import__('random'),\n        'datetime': __import__('datetime'),\n        'time': __import__('time'),\n        # 'requests': None, # Example: Explicitly disallow\n    }\n\n    def __init__(self, allowed_globals: dict | None = None, max_execution_time: int = 5):\n        if not RESTRICTEDPYTHON_AVAILABLE:\n            raise ImportError(\"restrictedpython is not installed. Cannot use RestrictedPythonExecutor.\")\n        self.allowed_globals = allowed_globals or self.DEFAULT_ALLOWED_GLOBALS\n        self.max_execution_time = max_execution_time # Basic timeout (not perfectly enforced by restrictedpython)\n        logger.warning(\"Initialized RestrictedPythonExecutor. This provides LIMITED sandboxing. Use Docker for untrusted code.\")\n\n    def _execute(self, code: str) -&gt; dict[str, Any]:\n        \"\"\"Internal execution logic.\"\"\"\n        start_time = time.monotonic()\n        result = {\"stdout\": \"\", \"stderr\": \"\", \"error\": None, \"exit_code\": None}\n        local_vars = {}\n        stdout_capture = io.StringIO()\n        stderr_capture = io.StringIO()\n\n        try:\n            # Basic timeout check (not preemptive)\n            if time.monotonic() - start_time &gt; self.max_execution_time:\n                 raise TimeoutError(f\"Execution exceeded max time of {self.max_execution_time}s (pre-check).\")\n\n            # Compile the code in restricted mode\n            byte_code = compile_restricted(code, filename='&lt;inline code&gt;', mode='exec')\n\n            # Add a print collector to capture output\n            self.allowed_globals['_print_'] = restrictedpython.PrintCollector\n            print_collector = self.allowed_globals['_print_']()\n            exec_globals = {**self.allowed_globals, '_print': print_collector}\n\n            # Execute the compiled code\n            # Note: restrictedpython does not inherently support robust timeouts during exec\n            exec(byte_code, exec_globals, local_vars)\n\n            # Check execution time again\n            duration = time.monotonic() - start_time\n            if duration &gt; self.max_execution_time:\n                logger.warning(f\"Execution finished but exceeded max time ({duration:.2f}s &gt; {self.max_execution_time}s).\")\n                # Potentially treat as an error or partial success\n\n            result[\"stdout\"] = print_collector.printed_text # Access collected prints\n            result[\"exit_code\"] = 0 # Assume success if no exception\n\n        except TimeoutError as e:\n            result[\"stderr\"] = f\"TimeoutError: {e}\"\n            result[\"error\"] = str(e)\n            result[\"exit_code\"] = -1 # Indicate timeout\n        except SyntaxError as e:\n            result[\"stderr\"] = f\"SyntaxError: {e}\"\n            result[\"error\"] = str(e)\n            result[\"exit_code\"] = 1\n        except Exception as e:\n            # Capture other potential execution errors allowed by restrictedpython\n            error_type = type(e).__name__\n            error_msg = f\"{error_type}: {e}\"\n            result[\"stderr\"] = error_msg\n            result[\"error\"] = str(e)\n            result[\"exit_code\"] = 1\n            logger.warning(f\"RestrictedPython execution caught exception: {error_msg}\", exc_info=False) # Avoid logging potentially sensitive details from code\n        finally:\n            stdout_capture.close() # Not used directly with PrintCollector\n            stderr_capture.close()\n\n        return result\n\n    # --- ADK Compatibility Method ---\n    if ADK_EXEC_AVAILABLE:\n        def execute_code(self, invocation_context: InvocationContext, code_input: CodeExecutionInput) -&gt; CodeExecutionResult:\n            logger.debug(f\"RestrictedPythonExecutor executing ADK request (lang: {code_input.language}). Code: {code_input.code[:100]}...\")\n            if code_input.language.lower() != 'python':\n                 return CodeExecutionResult(output=f\"Error: Unsupported language '{code_input.language}'. Only Python is supported.\", outcome=\"OUTCOME_FAILURE\")\n\n            exec_result = self._execute(code_input.code)\n\n            output_str = \"\"\n            if exec_result[\"stdout\"]:\n                output_str += f\"Stdout:\\n{exec_result['stdout']}\\n\"\n            if exec_result[\"stderr\"]:\n                 output_str += f\"Stderr:\\n{exec_result['stderr']}\\n\"\n            if not output_str and exec_result[\"exit_code\"] == 0:\n                 output_str = \"Execution successful with no output.\"\n            elif not output_str and exec_result[\"exit_code\"] != 0:\n                 output_str = f\"Execution failed with no output (Exit code: {exec_result['exit_code']}). Error: {exec_result['error']}\"\n\n\n            outcome = \"OUTCOME_SUCCESS\" if exec_result[\"exit_code\"] == 0 else \"OUTCOME_FAILURE\"\n\n            return CodeExecutionResult(output=output_str.strip(), outcome=outcome)\n    # --- End ADK Compatibility ---\n\n    # --- Direct Call Method ---\n    def execute(self, code: str) -&gt; dict[str, Any]:\n        \"\"\"Directly execute code, returning detailed dictionary.\"\"\"\n        logger.debug(f\"RestrictedPythonExecutor executing direct call. Code: {code[:100]}...\")\n        return self._execute(code)\n</code></pre> <code>execute(code)</code> \u00b6 <p>Directly execute code, returning detailed dictionary.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/executors.py</code> <pre><code>def execute(self, code: str) -&gt; dict[str, Any]:\n    \"\"\"Directly execute code, returning detailed dictionary.\"\"\"\n    logger.debug(f\"RestrictedPythonExecutor executing direct call. Code: {code[:100]}...\")\n    return self._execute(code)\n</code></pre> <code>get_code_executor(config)</code> \u00b6 <p>Creates a code executor instance based on configuration.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/executors.py</code> <pre><code>def get_code_executor(config: 'AgentConfig') -&gt; RestrictedPythonExecutor | DockerCodeExecutor | BaseCodeExecutor | None:\n    \"\"\"Creates a code executor instance based on configuration.\"\"\"\n    executor_type = config.code_executor_type\n    executor_config = config.code_executor_config or {}\n\n    if executor_type == \"restricted\":\n        if not RESTRICTEDPYTHON_AVAILABLE:\n            logger.error(\"RestrictedPython executor configured but library not installed. Code execution disabled.\")\n            return None\n        return RestrictedPythonExecutor(**executor_config)\n    elif executor_type == \"docker\":\n        if not DOCKER_AVAILABLE:\n            logger.error(\"Docker executor configured but library not installed or Docker not running. Code execution disabled.\")\n            return None\n        try:\n            return DockerCodeExecutor(**executor_config)\n        except Exception as e:\n            logger.error(f\"Failed to initialize DockerCodeExecutor: {e}. Code execution disabled.\")\n            return None\n    elif executor_type == \"none\":\n        logger.info(\"Code execution explicitly disabled in configuration.\")\n        return None\n    elif executor_type and ADK_EXEC_AVAILABLE and isinstance(executor_type, BaseCodeExecutor):\n        # Allow passing a pre-configured ADK executor instance\n        logger.info(f\"Using pre-configured ADK code executor: {type(executor_type).__name__}\")\n        return executor_type\n    else:\n        logger.warning(f\"Unknown or unsupported code_executor_type: '{executor_type}'. Code execution disabled.\")\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent.utils","title":"<code>utils</code>","text":"<code>LLMMessage</code> <code>dataclass</code> \u00b6 <p>Represents a message in a conversation with the LLM.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/utils.py</code> <pre><code>@dataclass\nclass LLMMessage:\n    \"\"\"Represents a message in a conversation with the LLM.\"\"\"\n    role: str  # \"user\", \"assistant\", \"system\", \"tool\"\n    # Content can be string or list (e.g., multimodal with text/image dicts)\n    # Conforms to LiteLLM/OpenAI structure\n    content: str | list[dict[str, Any]]\n    tool_call_id: str | None = None  # For tool responses\n    name: str | None = None  # For tool calls/responses (function name)\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert to dictionary, handling potential dataclass nuances.\"\"\"\n        d = {\"role\": self.role, \"content\": self.content}\n        if self.tool_call_id:\n            d[\"tool_call_id\"] = self.tool_call_id\n        if self.name:\n            d[\"name\"] = self.name\n        return d\n</code></pre> <code>to_dict()</code> \u00b6 <p>Convert to dictionary, handling potential dataclass nuances.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/utils.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert to dictionary, handling potential dataclass nuances.\"\"\"\n    d = {\"role\": self.role, \"content\": self.content}\n    if self.tool_call_id:\n        d[\"tool_call_id\"] = self.tool_call_id\n    if self.name:\n        d[\"name\"] = self.name\n    return d\n</code></pre> <code>WorldModel</code> <code>dataclass</code> \u00b6 <p>Thread-safe representation of the agent's persistent understanding of the world.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/utils.py</code> <pre><code>@dataclass\nclass WorldModel:\n    \"\"\"Thread-safe representation of the agent's persistent understanding of the world.\"\"\"\n    data: dict[str, Any] = dataclass_field(default_factory=dict)\n    _lock: threading.Lock = dataclass_field(default_factory=threading.Lock)\n\n    def get(self, key: str, default: Any = None) -&gt; Any:\n        with self._lock:\n            return self.data.get(key, default)\n\n    def set(self, key: str, value: Any):\n        with self._lock:\n            logger_wm.debug(f\"WorldModel SET: {key} = {value}\")\n            self.data[key] = value\n\n    def remove(self, key: str):\n        with self._lock:\n            if key in self.data:\n                logger_wm.debug(f\"WorldModel REMOVE: {key}\")\n                del self.data[key]\n\n    def show(self) -&gt; str:\n        with self._lock:\n            if not self.data:\n                return \"[empty]\"\n            try:\n                items = [f\"- {k}: {json.dumps(v, indent=None, ensure_ascii=False, default=str)}\"\n                         for k, v in self.data.items()]\n                return \"\\n\".join(items)\n            except Exception:\n                items = [f\"- {k}: {str(v)}\" for k, v in self.data.items()]\n                return \"\\n\".join(items)\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        with self._lock:\n            # Deep copy might be needed if values are mutable and modified externally\n            # For simplicity, shallow copy is used here.\n            return self.data.copy()\n\n    def update_from_dict(self, data_dict: dict[str, Any]):\n        with self._lock:\n            self.data.update(data_dict)\n            logger_wm.debug(f\"WorldModel updated from dict: {list(data_dict.keys())}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils","title":"<code>AgentUtils</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.AISemanticMemory","title":"<code>AISemanticMemory</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>class AISemanticMemory(metaclass=Singleton):\n    def __init__(self,\n                 base_path: str = \"/semantic_memory\",\n                 default_model: str = os.getenv(\"DEFAULTMODELSUMMERY\"),\n                 default_embedding_model: str = os.getenv(\"DEFAULTMODELEMBEDDING\"),\n                 default_similarity_threshold: float = 0.61,\n                 default_batch_size: int = 64,\n                 default_n_clusters: int = 2,\n                 default_deduplication_threshold: float = 0.85):\n        \"\"\"\n        Initialize AISemanticMemory with KnowledgeBase integration\n\n        Args:\n            base_path: Root directory for memory storage\n            default_model: Default model for text generation\n            default_embedding_model: Default embedding model\n            default_similarity_threshold: Default similarity threshold for retrieval\n            default_batch_size: Default batch size for processing\n            default_n_clusters: Default number of clusters for FAISS\n            default_deduplication_threshold: Default threshold for deduplication\n        \"\"\"\n        self.base_path = os.path.join(os.getcwd(), \".data\", base_path)\n        self.memories: dict[str, KnowledgeBase] = {}\n\n        # Map of embedding models to their dimensions\n        self.embedding_dims = {\n            \"text-embedding-3-small\": 1536,\n            \"text-embedding-3-large\": 3072,\n            \"nomic-embed-text\": 768,\n            \"default\": 768\n        }\n\n        self.default_config = {\n            \"embedding_model\": default_embedding_model,\n            \"embedding_dim\": self._get_embedding_dim(default_embedding_model),\n            \"similarity_threshold\": default_similarity_threshold,\n            \"batch_size\": default_batch_size,\n            \"n_clusters\": default_n_clusters,\n            \"deduplication_threshold\": default_deduplication_threshold,\n            \"model_name\": default_model\n        }\n\n    def _get_embedding_dim(self, model_name: str) -&gt; int:\n        \"\"\"Get embedding dimension for a model\"\"\"\n        return self.embedding_dims.get(model_name, 768)\n\n    @staticmethod\n    def _sanitize_name(name: str) -&gt; str:\n        \"\"\"Sanitize memory name for filesystem safety\"\"\"\n        name = re.sub(r'[^a-zA-Z0-9_-]', '-', name)[:63].strip('-')\n        if not name:\n            raise ValueError(\"Invalid memory name\")\n        if len(name) &lt; 3:\n            name += \"Z\" * (3 - len(name))\n        return name\n\n    def create_memory(self,\n                      name: str,\n                      model_config: dict | None = None,\n                      storage_config: dict | None = None) -&gt; KnowledgeBase:\n        \"\"\"\n        Create new memory store with KnowledgeBase\n\n        Args:\n            name: Unique name for the memory store\n            model_config: Configuration for embedding model\n            storage_config: Configuration for KnowledgeBase parameters\n        \"\"\"\n        sanitized = self._sanitize_name(name)\n        if sanitized in self.memories:\n            raise ValueError(f\"Memory '{name}' already exists\")\n\n        # Determine embedding model and dimension\n        embedding_model = self.default_config[\"embedding_model\"]\n        model_name = self.default_config[\"model_name\"]\n        if model_config:\n            embedding_model = model_config.get(\"embedding_model\", embedding_model)\n            model_name = model_config.get(\"model_name\", model_name)\n        embedding_dim = self._get_embedding_dim(embedding_model)\n\n        # Get KnowledgeBase parameters\n        kb_params = {\n            \"embedding_dim\": embedding_dim,\n            \"embedding_model\": embedding_model,\n            \"similarity_threshold\": self.default_config[\"similarity_threshold\"],\n            \"batch_size\": self.default_config[\"batch_size\"],\n            \"n_clusters\": self.default_config[\"n_clusters\"],\n            \"deduplication_threshold\": self.default_config[\"deduplication_threshold\"],\n            \"model_name\": model_name,\n        }\n\n        if storage_config:\n            kb_params.update({\n                \"similarity_threshold\": storage_config.get(\"similarity_threshold\", kb_params[\"similarity_threshold\"]),\n                \"batch_size\": storage_config.get(\"batch_size\", kb_params[\"batch_size\"]),\n                \"n_clusters\": storage_config.get(\"n_clusters\", kb_params[\"n_clusters\"]),\n                \"model_name\": storage_config.get(\"model_name\", kb_params[\"model_name\"]),\n                \"embedding_model\": storage_config.get(\"embedding_model\", kb_params[\"embedding_model\"]),\n                \"deduplication_threshold\": storage_config.get(\"deduplication_threshold\",\n                                                              kb_params[\"deduplication_threshold\"]),\n            })\n\n        # Create KnowledgeBase instance\n        self.memories[sanitized] = KnowledgeBase(**kb_params)\n        return self.memories[sanitized]\n\n    async def add_data(self,\n                       memory_name: str,\n                       data: str | list[str] | bytes | dict,\n                       metadata: dict | None = None) -&gt; bool:\n        \"\"\"\n        Add data to memory store\n\n        Args:\n            memory_name: Target memory store\n            data: Text, list of texts, binary file, or structured data\n            metadata: Optional metadata\n        \"\"\"\n        name = self._sanitize_name(memory_name)\n        kb = self.memories.get(name)\n        if not kb:\n            kb = self.create_memory(name)\n\n        # Process input data\n        texts = []\n        if isinstance(data, bytes):\n            try:\n                import textract\n                text = textract.process(data).decode('utf-8')\n                texts = [text.replace('\\\\t', '').replace('\\t', '')]\n            except Exception as e:\n                raise ValueError(f\"File processing failed: {str(e)}\")\n        elif isinstance(data, str):\n            texts = [data.replace('\\\\t', '').replace('\\t', '')]\n        elif isinstance(data, list):\n            texts = [d.replace('\\\\t', '').replace('\\t', '') for d in data]\n        elif isinstance(data, dict):\n            # Custom KG not supported in current KnowledgeBase\n            raise NotImplementedError(\"Custom knowledge graph insertion not supported\")\n        else:\n            raise ValueError(\"Unsupported data type\")\n\n        # Add data to KnowledgeBase\n        try:\n            added, duplicates = await kb.add_data(texts, metadata)\n            return added &gt; 0\n        except Exception as e:\n            import traceback\n            print(traceback.format_exc())\n            raise RuntimeError(f\"Data addition failed: {str(e)}\")\n\n    def get(self, names):\n        return [m for n,m in self._get_target_memories(names)]\n\n    async def query(self,\n                    query: str,\n                    memory_names: str | list[str] | None = None,\n                    query_params: dict | None = None,\n                    to_str: bool = False,\n                    unified_retrieve: bool =False) -&gt; str | list[dict]:\n        \"\"\"\n        Query memories using KnowledgeBase retrieval\n\n        Args:\n            query: Search query\n            memory_names: Target memory names\n            query_params: Query parameters\n            to_str: Return string format\n            unified_retrieve: Unified retrieve\n        \"\"\"\n        targets = self._get_target_memories(memory_names)\n        if not targets:\n            return []\n\n        results = []\n        for name, kb in targets:\n            #try:\n                # Use KnowledgeBase's retrieve_with_overview for comprehensive results\n                result = await kb.retrieve_with_overview(\n                    query=query,\n                    k=query_params.get(\"k\", 3) if query_params else 3,\n                    min_similarity=query_params.get(\"min_similarity\", 0.2) if query_params else 0.2,\n                    cross_ref_depth=query_params.get(\"cross_ref_depth\", 2) if query_params else 2,\n                    max_cross_refs=query_params.get(\"max_cross_refs\", 2) if query_params else 2,\n                    max_sentences=query_params.get(\"max_sentences\", 5) if query_params else 5\n                ) if not unified_retrieve else await kb.unified_retrieve(\n                    query=query,\n                    k=query_params.get(\"k\", 2) if query_params else 2,\n                    min_similarity=query_params.get(\"min_similarity\", 0.2) if query_params else 0.2,\n                    cross_ref_depth=query_params.get(\"cross_ref_depth\", 2) if query_params else 2,\n                    max_cross_refs=query_params.get(\"max_cross_refs\", 6) if query_params else 6,\n                    max_sentences=query_params.get(\"max_sentences\", 12) if query_params else 12\n                )\n                results.append({\n                    \"memory\": name,\n                    \"result\": result\n                })\n            #except Exception as e:\n            #    print(f\"Query failed on {name}: {str(e)}\")\n        if to_str:\n            if not unified_retrieve:\n                str_res = [\n                    f\"{x['memory']} - {json.dumps(x['result'].overview)}\\n - {[c.text for c in x['result'].details]}\\n - {[(k, [c.text for c in v]) for k, v in x['result'].cross_references.items()]}\"\n                    for x in results]\n                # str_res =\n            else:\n                str_res = json.dumps(results)\n            return str_res\n        return results\n\n    def _get_target_memories(self, memory_names: str | list[str] | None) -&gt; list[tuple[str, KnowledgeBase]]:\n        \"\"\"Get target memories for query\"\"\"\n        if not memory_names:\n            return list(self.memories.items())\n\n        names = [memory_names] if isinstance(memory_names, str) else memory_names\n\n        targets = []\n        for name in names:\n            sanitized = self._sanitize_name(name)\n            if kb := self.memories.get(sanitized):\n                targets.append((sanitized, kb))\n        return targets\n\n    def list_memories(self) -&gt; list[str]:\n        \"\"\"List all available memories\"\"\"\n        return list(self.memories.keys())\n\n    async def delete_memory(self, name: str) -&gt; bool:\n        \"\"\"Delete a memory store\"\"\"\n        sanitized = self._sanitize_name(name)\n        if sanitized in self.memories:\n            del self.memories[sanitized]\n            return True\n        return False\n\n    def save_memory(self, name: str, path: str) -&gt; bool | bytes:\n        \"\"\"Save a memory store to disk\"\"\"\n        sanitized = self._sanitize_name(name)\n        if kb := self.memories.get(sanitized):\n            try:\n                return kb.save(path)\n            except Exception as e:\n                print(f\"Error saving memory: {str(e)}\")\n                return False\n        return False\n\n    def load_memory(self, name: str, path: str | bytes) -&gt; bool:\n        \"\"\"Load a memory store from disk\"\"\"\n        sanitized = self._sanitize_name(name)\n        if sanitized in self.memories:\n            return False\n        try:\n            self.memories[sanitized] = KnowledgeBase.load(path)\n            return True\n        except Exception:\n            # print(f\"Error loading memory: {str(e)}\")\n            return False\n</code></pre> <code>__init__(base_path='/semantic_memory', default_model=os.getenv('DEFAULTMODELSUMMERY'), default_embedding_model=os.getenv('DEFAULTMODELEMBEDDING'), default_similarity_threshold=0.61, default_batch_size=64, default_n_clusters=2, default_deduplication_threshold=0.85)</code> \u00b6 <p>Initialize AISemanticMemory with KnowledgeBase integration</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str</code> <p>Root directory for memory storage</p> <code>'/semantic_memory'</code> <code>default_model</code> <code>str</code> <p>Default model for text generation</p> <code>getenv('DEFAULTMODELSUMMERY')</code> <code>default_embedding_model</code> <code>str</code> <p>Default embedding model</p> <code>getenv('DEFAULTMODELEMBEDDING')</code> <code>default_similarity_threshold</code> <code>float</code> <p>Default similarity threshold for retrieval</p> <code>0.61</code> <code>default_batch_size</code> <code>int</code> <p>Default batch size for processing</p> <code>64</code> <code>default_n_clusters</code> <code>int</code> <p>Default number of clusters for FAISS</p> <code>2</code> <code>default_deduplication_threshold</code> <code>float</code> <p>Default threshold for deduplication</p> <code>0.85</code> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def __init__(self,\n             base_path: str = \"/semantic_memory\",\n             default_model: str = os.getenv(\"DEFAULTMODELSUMMERY\"),\n             default_embedding_model: str = os.getenv(\"DEFAULTMODELEMBEDDING\"),\n             default_similarity_threshold: float = 0.61,\n             default_batch_size: int = 64,\n             default_n_clusters: int = 2,\n             default_deduplication_threshold: float = 0.85):\n    \"\"\"\n    Initialize AISemanticMemory with KnowledgeBase integration\n\n    Args:\n        base_path: Root directory for memory storage\n        default_model: Default model for text generation\n        default_embedding_model: Default embedding model\n        default_similarity_threshold: Default similarity threshold for retrieval\n        default_batch_size: Default batch size for processing\n        default_n_clusters: Default number of clusters for FAISS\n        default_deduplication_threshold: Default threshold for deduplication\n    \"\"\"\n    self.base_path = os.path.join(os.getcwd(), \".data\", base_path)\n    self.memories: dict[str, KnowledgeBase] = {}\n\n    # Map of embedding models to their dimensions\n    self.embedding_dims = {\n        \"text-embedding-3-small\": 1536,\n        \"text-embedding-3-large\": 3072,\n        \"nomic-embed-text\": 768,\n        \"default\": 768\n    }\n\n    self.default_config = {\n        \"embedding_model\": default_embedding_model,\n        \"embedding_dim\": self._get_embedding_dim(default_embedding_model),\n        \"similarity_threshold\": default_similarity_threshold,\n        \"batch_size\": default_batch_size,\n        \"n_clusters\": default_n_clusters,\n        \"deduplication_threshold\": default_deduplication_threshold,\n        \"model_name\": default_model\n    }\n</code></pre> <code>add_data(memory_name, data, metadata=None)</code> <code>async</code> \u00b6 <p>Add data to memory store</p> <p>Parameters:</p> Name Type Description Default <code>memory_name</code> <code>str</code> <p>Target memory store</p> required <code>data</code> <code>str | list[str] | bytes | dict</code> <p>Text, list of texts, binary file, or structured data</p> required <code>metadata</code> <code>dict | None</code> <p>Optional metadata</p> <code>None</code> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>async def add_data(self,\n                   memory_name: str,\n                   data: str | list[str] | bytes | dict,\n                   metadata: dict | None = None) -&gt; bool:\n    \"\"\"\n    Add data to memory store\n\n    Args:\n        memory_name: Target memory store\n        data: Text, list of texts, binary file, or structured data\n        metadata: Optional metadata\n    \"\"\"\n    name = self._sanitize_name(memory_name)\n    kb = self.memories.get(name)\n    if not kb:\n        kb = self.create_memory(name)\n\n    # Process input data\n    texts = []\n    if isinstance(data, bytes):\n        try:\n            import textract\n            text = textract.process(data).decode('utf-8')\n            texts = [text.replace('\\\\t', '').replace('\\t', '')]\n        except Exception as e:\n            raise ValueError(f\"File processing failed: {str(e)}\")\n    elif isinstance(data, str):\n        texts = [data.replace('\\\\t', '').replace('\\t', '')]\n    elif isinstance(data, list):\n        texts = [d.replace('\\\\t', '').replace('\\t', '') for d in data]\n    elif isinstance(data, dict):\n        # Custom KG not supported in current KnowledgeBase\n        raise NotImplementedError(\"Custom knowledge graph insertion not supported\")\n    else:\n        raise ValueError(\"Unsupported data type\")\n\n    # Add data to KnowledgeBase\n    try:\n        added, duplicates = await kb.add_data(texts, metadata)\n        return added &gt; 0\n    except Exception as e:\n        import traceback\n        print(traceback.format_exc())\n        raise RuntimeError(f\"Data addition failed: {str(e)}\")\n</code></pre> <code>create_memory(name, model_config=None, storage_config=None)</code> \u00b6 <p>Create new memory store with KnowledgeBase</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique name for the memory store</p> required <code>model_config</code> <code>dict | None</code> <p>Configuration for embedding model</p> <code>None</code> <code>storage_config</code> <code>dict | None</code> <p>Configuration for KnowledgeBase parameters</p> <code>None</code> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def create_memory(self,\n                  name: str,\n                  model_config: dict | None = None,\n                  storage_config: dict | None = None) -&gt; KnowledgeBase:\n    \"\"\"\n    Create new memory store with KnowledgeBase\n\n    Args:\n        name: Unique name for the memory store\n        model_config: Configuration for embedding model\n        storage_config: Configuration for KnowledgeBase parameters\n    \"\"\"\n    sanitized = self._sanitize_name(name)\n    if sanitized in self.memories:\n        raise ValueError(f\"Memory '{name}' already exists\")\n\n    # Determine embedding model and dimension\n    embedding_model = self.default_config[\"embedding_model\"]\n    model_name = self.default_config[\"model_name\"]\n    if model_config:\n        embedding_model = model_config.get(\"embedding_model\", embedding_model)\n        model_name = model_config.get(\"model_name\", model_name)\n    embedding_dim = self._get_embedding_dim(embedding_model)\n\n    # Get KnowledgeBase parameters\n    kb_params = {\n        \"embedding_dim\": embedding_dim,\n        \"embedding_model\": embedding_model,\n        \"similarity_threshold\": self.default_config[\"similarity_threshold\"],\n        \"batch_size\": self.default_config[\"batch_size\"],\n        \"n_clusters\": self.default_config[\"n_clusters\"],\n        \"deduplication_threshold\": self.default_config[\"deduplication_threshold\"],\n        \"model_name\": model_name,\n    }\n\n    if storage_config:\n        kb_params.update({\n            \"similarity_threshold\": storage_config.get(\"similarity_threshold\", kb_params[\"similarity_threshold\"]),\n            \"batch_size\": storage_config.get(\"batch_size\", kb_params[\"batch_size\"]),\n            \"n_clusters\": storage_config.get(\"n_clusters\", kb_params[\"n_clusters\"]),\n            \"model_name\": storage_config.get(\"model_name\", kb_params[\"model_name\"]),\n            \"embedding_model\": storage_config.get(\"embedding_model\", kb_params[\"embedding_model\"]),\n            \"deduplication_threshold\": storage_config.get(\"deduplication_threshold\",\n                                                          kb_params[\"deduplication_threshold\"]),\n        })\n\n    # Create KnowledgeBase instance\n    self.memories[sanitized] = KnowledgeBase(**kb_params)\n    return self.memories[sanitized]\n</code></pre> <code>delete_memory(name)</code> <code>async</code> \u00b6 <p>Delete a memory store</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>async def delete_memory(self, name: str) -&gt; bool:\n    \"\"\"Delete a memory store\"\"\"\n    sanitized = self._sanitize_name(name)\n    if sanitized in self.memories:\n        del self.memories[sanitized]\n        return True\n    return False\n</code></pre> <code>list_memories()</code> \u00b6 <p>List all available memories</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def list_memories(self) -&gt; list[str]:\n    \"\"\"List all available memories\"\"\"\n    return list(self.memories.keys())\n</code></pre> <code>load_memory(name, path)</code> \u00b6 <p>Load a memory store from disk</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def load_memory(self, name: str, path: str | bytes) -&gt; bool:\n    \"\"\"Load a memory store from disk\"\"\"\n    sanitized = self._sanitize_name(name)\n    if sanitized in self.memories:\n        return False\n    try:\n        self.memories[sanitized] = KnowledgeBase.load(path)\n        return True\n    except Exception:\n        # print(f\"Error loading memory: {str(e)}\")\n        return False\n</code></pre> <code>query(query, memory_names=None, query_params=None, to_str=False, unified_retrieve=False)</code> <code>async</code> \u00b6 <p>Query memories using KnowledgeBase retrieval</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query</p> required <code>memory_names</code> <code>str | list[str] | None</code> <p>Target memory names</p> <code>None</code> <code>query_params</code> <code>dict | None</code> <p>Query parameters</p> <code>None</code> <code>to_str</code> <code>bool</code> <p>Return string format</p> <code>False</code> <code>unified_retrieve</code> <code>bool</code> <p>Unified retrieve</p> <code>False</code> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>async def query(self,\n                query: str,\n                memory_names: str | list[str] | None = None,\n                query_params: dict | None = None,\n                to_str: bool = False,\n                unified_retrieve: bool =False) -&gt; str | list[dict]:\n    \"\"\"\n    Query memories using KnowledgeBase retrieval\n\n    Args:\n        query: Search query\n        memory_names: Target memory names\n        query_params: Query parameters\n        to_str: Return string format\n        unified_retrieve: Unified retrieve\n    \"\"\"\n    targets = self._get_target_memories(memory_names)\n    if not targets:\n        return []\n\n    results = []\n    for name, kb in targets:\n        #try:\n            # Use KnowledgeBase's retrieve_with_overview for comprehensive results\n            result = await kb.retrieve_with_overview(\n                query=query,\n                k=query_params.get(\"k\", 3) if query_params else 3,\n                min_similarity=query_params.get(\"min_similarity\", 0.2) if query_params else 0.2,\n                cross_ref_depth=query_params.get(\"cross_ref_depth\", 2) if query_params else 2,\n                max_cross_refs=query_params.get(\"max_cross_refs\", 2) if query_params else 2,\n                max_sentences=query_params.get(\"max_sentences\", 5) if query_params else 5\n            ) if not unified_retrieve else await kb.unified_retrieve(\n                query=query,\n                k=query_params.get(\"k\", 2) if query_params else 2,\n                min_similarity=query_params.get(\"min_similarity\", 0.2) if query_params else 0.2,\n                cross_ref_depth=query_params.get(\"cross_ref_depth\", 2) if query_params else 2,\n                max_cross_refs=query_params.get(\"max_cross_refs\", 6) if query_params else 6,\n                max_sentences=query_params.get(\"max_sentences\", 12) if query_params else 12\n            )\n            results.append({\n                \"memory\": name,\n                \"result\": result\n            })\n        #except Exception as e:\n        #    print(f\"Query failed on {name}: {str(e)}\")\n    if to_str:\n        if not unified_retrieve:\n            str_res = [\n                f\"{x['memory']} - {json.dumps(x['result'].overview)}\\n - {[c.text for c in x['result'].details]}\\n - {[(k, [c.text for c in v]) for k, v in x['result'].cross_references.items()]}\"\n                for x in results]\n            # str_res =\n        else:\n            str_res = json.dumps(results)\n        return str_res\n    return results\n</code></pre> <code>save_memory(name, path)</code> \u00b6 <p>Save a memory store to disk</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def save_memory(self, name: str, path: str) -&gt; bool | bytes:\n    \"\"\"Save a memory store to disk\"\"\"\n    sanitized = self._sanitize_name(name)\n    if kb := self.memories.get(sanitized):\n        try:\n            return kb.save(path)\n        except Exception as e:\n            print(f\"Error saving memory: {str(e)}\")\n            return False\n    return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.PyEnvEval","title":"<code>PyEnvEval</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>class PyEnvEval:\n    def __init__(self):\n        self.local_env = locals().copy()\n        self.global_env = {'local_env': self.local_env}  # globals().copy()\n\n    def eval_code(self, code):\n        try:\n            exec(code, self.global_env, self.local_env)\n            result = eval(code, self.global_env, self.local_env)\n            return self.format_output(result)\n        except Exception as e:\n            return self.format_output(str(e))\n\n    def get_env(self):\n        local_env_str = self.format_env(self.local_env)\n        return f'Locals:\\n{local_env_str}'\n\n    @staticmethod\n    def format_output(output):\n        return f'Ergebnis: {output}'\n\n    @staticmethod\n    def format_env(env):\n        return '\\n'.join(f'{key}: {value}' for key, value in env.items())\n\n    def run_and_display(self, python_code):\n        \"\"\"function to eval python code\"\"\"\n        start = f'Start-state:\\n{self.get_env()}'\n        result = self.eval_code(python_code)\n        end = f'End-state:\\n{self.get_env()}'\n        return f'{start}\\nResult:\\n{result}\\n{end}'\n\n    def tool(self):\n        return {\"PythonEval\": {\"func\": self.run_and_display, \"description\": \"Use Python Code to Get to an Persis Answer! input must be valid python code all non code parts must be comments!\"}}\n</code></pre> <code>run_and_display(python_code)</code> \u00b6 <p>function to eval python code</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def run_and_display(self, python_code):\n    \"\"\"function to eval python code\"\"\"\n    start = f'Start-state:\\n{self.get_env()}'\n    result = self.eval_code(python_code)\n    end = f'End-state:\\n{self.get_env()}'\n    return f'{start}\\nResult:\\n{result}\\n{end}'\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.anything_from_str_to_dict","title":"<code>anything_from_str_to_dict(data, expected_keys=None, mini_task=lambda x: '')</code>","text":"<p>Versucht, einen String in ein oder mehrere Dictionaries umzuwandeln. Ber\u00fccksichtigt dabei die erwarteten Schl\u00fcssel und ihre Standardwerte.</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def anything_from_str_to_dict(data: str, expected_keys: dict = None, mini_task=lambda x: ''):\n    \"\"\"\n    Versucht, einen String in ein oder mehrere Dictionaries umzuwandeln.\n    Ber\u00fccksichtigt dabei die erwarteten Schl\u00fcssel und ihre Standardwerte.\n    \"\"\"\n    if len(data) &lt; 4:\n        return []\n\n    if expected_keys is None:\n        expected_keys = {}\n\n    result = []\n    json_objects = find_json_objects_in_str(data)\n    if not json_objects and data.startswith('[') and data.endswith(']'):\n        json_objects = eval(data)\n    if json_objects and len(json_objects) &gt; 0 and isinstance(json_objects[0], dict):\n        result.extend([{**expected_keys, **ob} for ob in json_objects])\n    if not result:\n        completed_object = complete_json_object(data, mini_task)\n        if completed_object is not None:\n            result.append(completed_object)\n    if len(result) == 0 and expected_keys:\n        result = [{list(expected_keys.keys())[0]: data}]\n    for res in result:\n        if isinstance(res, list) and len(res) &gt; 0:\n            res = res[0]\n        for key, value in expected_keys.items():\n            if key not in res:\n                res[key] = value\n\n    if len(result) == 0:\n        fixed = fix_json(data)\n        if fixed:\n            result.append(fixed)\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.complete_json_object","title":"<code>complete_json_object(data, mini_task)</code>","text":"<p>Ruft eine Funktion auf, um einen String in das richtige Format zu bringen. Gibt das resultierende JSON-Objekt zur\u00fcck, wenn die Funktion erfolgreich ist, sonst None.</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def complete_json_object(data: str, mini_task):\n    \"\"\"\n    Ruft eine Funktion auf, um einen String in das richtige Format zu bringen.\n    Gibt das resultierende JSON-Objekt zur\u00fcck, wenn die Funktion erfolgreich ist, sonst None.\n    \"\"\"\n    ret = mini_task(\n        f\"Vervollst\u00e4ndige das Json Object. Und bringe den string in das Richtige format. data={data}\\nJson=\")\n    if ret:\n        return anything_from_str_to_dict(ret)\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.find_json_objects_in_str","title":"<code>find_json_objects_in_str(data)</code>","text":"<p>Sucht nach JSON-Objekten innerhalb eines Strings. Gibt eine Liste von JSON-Objekten zur\u00fcck, die im String gefunden wurden.</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def find_json_objects_in_str(data: str):\n    \"\"\"\n    Sucht nach JSON-Objekten innerhalb eines Strings.\n    Gibt eine Liste von JSON-Objekten zur\u00fcck, die im String gefunden wurden.\n    \"\"\"\n    json_objects = extract_json_objects(data)\n    if not isinstance(json_objects, list):\n        json_objects = [json_objects]\n    return [get_json_from_json_str(ob, 10) for ob in json_objects if get_json_from_json_str(ob, 10) is not None]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.get_json_from_json_str","title":"<code>get_json_from_json_str(json_str, repeat=1)</code>","text":"<p>Versucht, einen JSON-String in ein Python-Objekt umzuwandeln.</p> <p>Wenn beim Parsen ein Fehler auftritt, versucht die Funktion, das Problem zu beheben, indem sie das Zeichen an der Position des Fehlers durch ein Escape-Zeichen ersetzt. Dieser Vorgang wird bis zu <code>repeat</code>-mal wiederholt.</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str or list or dict</code> <p>Der JSON-String, der geparst werden soll.</p> required <code>repeat</code> <code>int</code> <p>Die Anzahl der Versuche, das Parsen durchzuf\u00fchren.</p> <code>1</code> <p>Returns:</p> Type Description <code>dict or None</code> <p>Das resultierende Python-Objekt.</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def get_json_from_json_str(json_str: str or list or dict, repeat: int = 1) -&gt; dict or None:\n    \"\"\"Versucht, einen JSON-String in ein Python-Objekt umzuwandeln.\n\n    Wenn beim Parsen ein Fehler auftritt, versucht die Funktion, das Problem zu beheben,\n    indem sie das Zeichen an der Position des Fehlers durch ein Escape-Zeichen ersetzt.\n    Dieser Vorgang wird bis zu `repeat`-mal wiederholt.\n\n    Args:\n        json_str: Der JSON-String, der geparst werden soll.\n        repeat: Die Anzahl der Versuche, das Parsen durchzuf\u00fchren.\n\n    Returns:\n        Das resultierende Python-Objekt.\n    \"\"\"\n    for _ in range(repeat):\n        try:\n            return parse_json_with_auto_detection(json_str)\n        except json.JSONDecodeError as e:\n            unexp = int(re.findall(r'\\(char (\\d+)\\)', str(e))[0])\n            unesc = json_str.rfind(r'\"', 0, unexp)\n            json_str = json_str[:unesc] + r'\\\"' + json_str[unesc + 1:]\n            closg = json_str.find(r'\"', unesc + 2)\n            json_str = json_str[:closg] + r'\\\"' + json_str[closg + 1:]\n        new = fix_json_object(json_str)\n        if new is not None:\n            json_str = new\n    get_logger().info(f\"Unable to parse JSON string after {json_str}\")\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.parse_json_with_auto_detection","title":"<code>parse_json_with_auto_detection(json_data)</code>","text":"<p>Parses JSON data, automatically detecting if a value is a JSON string and parsing it accordingly. If a value cannot be parsed as JSON, it is returned as is.</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def parse_json_with_auto_detection(json_data):\n    \"\"\"\n    Parses JSON data, automatically detecting if a value is a JSON string and parsing it accordingly.\n    If a value cannot be parsed as JSON, it is returned as is.\n    \"\"\"\n\n    def try_parse_json(value):\n        \"\"\"\n        Tries to parse a value as JSON. If the parsing fails, the original value is returned.\n        \"\"\"\n        try:\n            # print(\"parse_json_with_auto_detection:\", type(value), value)\n            parsed_value = json.loads(value)\n            # print(\"parsed_value:\", type(parsed_value), parsed_value)\n            # If the parsed value is a string, it might be a JSON string, so we try to parse it again\n            if isinstance(parsed_value, str):\n                return eval(parsed_value)\n            else:\n                return parsed_value\n        except Exception:\n            # logging.warning(f\"Failed to parse value as JSON: {value}. Exception: {e}\")\n            return value\n\n    get_logger()\n\n    if isinstance(json_data, dict):\n        return {key: parse_json_with_auto_detection(value) for key, value in json_data.items()}\n    elif isinstance(json_data, list):\n        return [parse_json_with_auto_detection(item) for item in json_data]\n    else:\n        return try_parse_json(json_data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agents","title":"<code>Agents</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agents.Agent","title":"<code>Agent</code>  <code>dataclass</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/Agents.py</code> <pre><code>@dataclass\nclass Agent:\n    amd: AgentModelData = field(default_factory=AgentModelData)\n\n    main_run_model : str | None = None\n\n    stream: bool = field(default=False)\n    messages: list[dict[str, str]] = field(default_factory=list)\n    trim: str = field(default=\"IsaaTrim\")\n    max_history_length: int = field(default=10)\n    similarity_threshold: int = field(default=75)\n    verbose: bool = field(default=logger.level == logging.DEBUG)\n    batch_completion: bool = field(default=False)\n    stream_function: Callable[[str], bool or None] = field(default_factory=print)\n\n    max_tokens: int | None = field(default=None)\n\n    taskstack: TaskStack | None = field(default_factory=TaskStack)\n    executor: ThreadPoolExecutor = field(default_factory=lambda: ThreadPoolExecutor(max_workers=1))\n\n    status_dict: dict[str, TaskStatus] = field(default_factory=dict)\n    _stop_event: threading.Event = field(default_factory=threading.Event)\n    state: AgentState = AgentState.IDLE\n    world_model: dict[str, str] = field(default_factory=dict)\n\n    post_callback: Callable | None = field(default=None)\n    progress_callback: Callable | None = field(default=None)\n\n    functions: list[LLMFunction] | None = field(default=None)\n    add_function_to_prompt: bool | None = field(default=None)\n\n    config: dict[str, Any] | None = field(default=None)\n\n    batch_completion_messages: list[list[LLMMessage]] | None = field(default=None)\n\n    memory: AISemanticMemory | None = field(default=None)\n    content_memory: ShortTermMemory | None = field(default=None)\n\n    capabilities: Capabilities | None = field(default=None)\n    mode: (LLMMode or ModeController) | None = field(default=None)\n    last_result: dict[str, Any] | None = field(default=None)\n\n    model: (gpt4all.GPT4All or HuggingFaceHub) | None = field(default=None)\n    hits: str | None = field(default=None)\n\n    next_fuction: str | None = field(default=None)\n    llm_function_runner: LLMFunctionRunner | None = field(default=None)\n\n    rformat: dict | None = field(default=None)\n\n    user_input: str | None = field(default=None)\n\n    vision: bool | None = field(default=None)\n    audio: bool | None = field(default=None)\n\n    if_for_fuction_use_overrides: bool = False\n\n\n    def show_world_model(self):\n        if not self.world_model:\n            return \"balnk\"\n        return \"Key &lt;&gt; Value\\n\" + \"\\n\".join([f\"{k} &lt;&gt; {v}\" for k, v in self.world_model.items()])\n\n    async def flow_world_model(self, query):\n\n        prompt = f\"Determine if to change the current world model ##{self.show_world_model()}## basd on the new information :\" + query\n\n        class WorldModelAdaption(BaseModel):\n            \"\"\"world model adaption action['remove' or 'add' or 'change' or None] ;\n            key from the existing word model or new one ; key format xyz.abc like Person.Tom\n            information changed or added. informations must be in str relation graph format like 'Person:Name, Works:at, Startup:Complot'\"\"\"\n            action: str | None = field(default=None)\n            key: str | None = field(default=None)\n            informations: str | None = field(default=None)\n\n        model_action = await self.a_format_class(WorldModelAdaption, prompt)\n        self.print_verbose(str(model_action))\n        if model_action.get(\"action\") is None or model_action.get(\"key\") is None:\n            return\n\n        if (\"remove\" in model_action[\"action\"] or \"del\" in model_action[\"action\"]) and model_action[\"key\"] in self.world_model:\n            del self.world_model[model_action[\"key\"]]\n\n        if model_action[\"informations\"] is None:\n            return\n\n        self.world_model[model_action[\"key\"]] = model_action[\"informations\"]\n\n    def run_in_background(self):\n        \"\"\"Start a task in background mode\"\"\"\n        self._stop_event.clear()\n\n        if self.state != AgentState.RUNNING:\n            self.state = AgentState.RUNNING\n            self.executor.submit(self._background_worker)\n\n        return self.state\n\n    def stop(self):\n        self._stop_event.set()\n\n    def _background_worker(self):\n        \"\"\"Background worker that processes queued tasks\"\"\"\n        while not self._stop_event.is_set():\n            try:\n                # Get task with timeout to allow checking stop_event\n                task = self.taskstack.get_next_task()\n                if task is None:\n                    self.state = AgentState.IDLE\n                    return\n                if task.id not in self.status_dict:\n                    task_status = TaskStatus(\n                        task_id=task.id,\n                        status=\"queued\",\n                        progress=0.0\n                    )\n                    self.status_dict[task.id] = task_status\n\n                status = self.status_dict[task.id]\n                status.status = \"starting\"\n                self.progress_callback(status)\n\n                try:\n                    # Run the main agent logic\n                    asyncio.run(self.run(task))\n\n                except Exception as e:\n                    status.status = \"error\"\n                    status.error = str(e)\n\n                self.progress_callback(status)\n\n            except queue.Empty:\n                continue\n\n        self.state = AgentState.STOPPED\n\n    async def run(self, user_input_or_task: str or Task, with_memory=None, with_functions=None, max_iterations=3, chat_session=None, with_split=False, **kwargs):\n\n        persist = False\n        task_from = \"user\"\n        persist_mem = False\n        out = None\n        # print(user_input_or_task)\n        if max_iterations &lt;= 0:\n            return \"overflow max iterations\"\n        if isinstance(user_input_or_task, str):\n            task = self._to_task(user_input_or_task)\n            user_input = user_input_or_task\n        elif isinstance(user_input_or_task, Task):\n            task = user_input_or_task\n            user_input = task.description\n        else:\n            raise ValueError(\"Invalid user input or task\")\n\n        # Update progress as we go through stages\n        stage = [1]\n\n        if self.progress_callback is None:\n            self.progress_callback = lambda x: None\n\n        async def update_progress(total_stages: int = 13):\n            status = self.status_dict.get(task.id, None)\n            if status is None:\n                return\n            status.progress = float(f\"{stage[0] / total_stages:.2f}\")\n            stage[0] += 1\n            status.status = \"running\" if stage[0] &lt; total_stages else \"completed\"\n            if asyncio.iscoroutinefunction(self.progress_callback):\n                await self.progress_callback(status)\n            else:\n                self.progress_callback(status)\n\n        await update_progress()\n        await self.flow_world_model(user_input)\n        message = [{\"role\": \"system\", \"content\": f\"World Model(read only): {self.show_world_model()}\"}]\n\n        if chat_session is not None:\n            history = \" ==== CHAT HISTORY ===\\n\"+ \"\\n\".join(f\"{x['role'].upper()}: {x['content']}\" for x in chat_session.get_past_x(self.max_history_length)) + \" === HISTORY END ===\"\n        else:\n            history = \" ==== CHAT HISTORY ===\\nASSISTANT: \"+ str(self.last_result)+ \" === HISTORY END ===\"\n\n        if len(history) &gt; 75:\n            message.append({\"role\": \"system\", \"content\": history})\n\n        if with_split is None:\n\n            class DoSplit(BaseModel):\n                \"\"\"Deside if u need to split ths Task in sub tasks, only ste try on complex tasks\"\"\"\n                split: bool = field(default=False)\n\n            split_task = self.format_class(DoSplit, user_input)[\"split\"]\n\n        else:\n            split_task = with_split\n\n        self.print_verbose(f'Split {split_task}')\n        await update_progress()\n        if split_task:\n\n            class TaskList(BaseModel):\n                \"\"\"sTask Breakdown Format:\n1. Each subtask should be represented as a structured object with:\n   - description: Clear, actionable description of the subtask\n   - priority: Integer ranking (1 being highest priority)\n   - estimated_complexity: Float value between 0.0 (simplest) and 1.0 (most complex)\n   - time_sensitivity: Float value indicating urgency (0.0 for least urgent, 1.0 for most urgent)\n\n2. Requirements for breakdown:\n   - Break the main task into logical, self-contained subtasks\n   - Order subtasks by dependencies and priority\n   - Ensure each description is specific and measurable\n   - Consider interdependencies when assigning priority\n   - Factor in both technical complexity and business impact when estimating complexity\n   - Account for deadlines and dependencies in time sensitivity ratings\n\n3. include all Informations in the description !\n\"\"\"\n                sub_tasks: list[sTask]\n\n            sub_tasks = self.format_class(TaskList, user_input)[\"sub_tasks\"]\n\n            self.print_verbose(f\"Subtasks {len(sub_tasks)}\")\n            st_d = '\\n'.join([f'{i}) '+(s[\"description\"] if isinstance(s, dict) else s)+ '\\n' for i, s in enumerate(sub_tasks)])\n            self.print_verbose(f\"Subtasks\\n {st_d}\")\n\n            if len(sub_tasks) &gt; 1:\n                for subt in sub_tasks[1:][::-1]:\n                    subt_ = Task(\n                        id=str(uuid.uuid4())[:16],\n                        description=subt[\"description\"] if isinstance(subt, dict) else subt,\n                        priority=subt[\"priority\"] if isinstance(subt, dict) else 1,\n                        estimated_complexity=subt[\"estimated_complexity\"] if isinstance(subt, dict) else 1,\n                        time_sensitivity=subt[\"time_sensitivity\"] if isinstance(subt, dict) else 1,\n                        created_at=datetime.now()\n                    )\n                    self.taskstack.add_task(subt_)\n            st = (sub_tasks[0][\"description\"] if isinstance(sub_tasks[0], dict) else sub_tasks[0])\n            if len(sub_tasks) &gt; 0 and st != \"\" and st != \"default description\":\n                _sub_tasks = sub_tasks[0]\n                task = Task(\n                    id=task.id,\n                    description=st+ task.description if len(sub_tasks) == 1 else '',\n                    priority=_sub_tasks[\"priority\"] if isinstance(sub_tasks[0], dict) else 1,\n                    estimated_complexity=_sub_tasks[\"estimated_complexity\"] if isinstance(sub_tasks[0], dict) else 1,\n                    time_sensitivity=_sub_tasks[\"time_sensitivity\"] if isinstance(sub_tasks[0], dict) else 1,\n                    created_at=datetime.now()\n                )\n\n        # Stage 2: Memory handling\n        # Stage 1: Initialize and validate inputs\n        await update_progress()\n        if with_functions is None:\n            class WithFunctions(BaseModel):\n                f\"\"\"Deside if u need to call one function to perform this task {[f.name for f in self.functions] if self.functions is not None else ''}\"\"\"\n                use_function: bool = field(default=False)\n\n            with_functions = self.format_class(WithFunctions, user_input).get(\"use_function\", True)\n            self.print_verbose(f'Auto {with_functions=}')\n\n        await update_progress()\n        if with_memory is None:\n            class WithMemory(BaseModel):\n                \"\"\"Deside if u need to get memory data to perform this task\"\"\"\n                use_memory: bool = field(default=False)\n\n            with_memory = self.format_class(WithMemory, user_input).get(\"use_memory\", True)\n            self.print_verbose(f'Auto {with_memory=}')\n        # Stage 4: Function execution\n        await update_progress()\n        if with_memory:\n            persist_mem = True\n            message = await self.get_memory(user_input, message, callback=update_progress)\n\n        # Stage 3: Function execution\n        await update_progress()\n\n        sto_model = None\n        if self.main_run_model is not None:\n            sto_model = self.amd.model\n            self.amd.model = self.main_run_model\n\n        if with_functions is False:\n            self.add_function_to_prompt = False\n\n            with Spinner(message=\"Fetching llm_message...\", symbols='+'):\n                llm_message = self.get_llm_message(user_input, persist=persist,\n                                                   task_from=task_from, message=message)\n            await update_progress()\n\n            iterations = max_iterations\n            while out is None and iterations &gt; 0:\n                iterations -= 1\n                out = await self.a_run_model(llm_message=llm_message, persist_local=persist, persist_mem=persist_mem, **kwargs)\n            await update_progress()\n\n            await update_progress()\n\n        elif with_functions is True:\n            self.add_function_to_prompt = True\n\n            with Spinner(message=\"Fetching llm_message...\", symbols='+'):\n                llm_message = self.get_llm_message(user_input, persist=persist,\n                                                   task_from=task_from, message=message)\n            await update_progress()\n\n            iterations = max_iterations\n            save_stream = self.stream\n            self.stream = False\n            while out is None and iterations &gt; 0:\n                iterations -= 1\n                out = await self.a_run_model(llm_message=llm_message, persist_local=persist, persist_mem=persist_mem, **kwargs)\n            await update_progress()\n            self.stream = save_stream\n\n            if self.if_for_fuction_use(out):\n                res = await self.execute_fuction(persist=persist, persist_mem=persist_mem)\n                out += f\"(system tool '{self.llm_function_runner.llm_function.name if self.llm_function_runner is not None else '-#-'}' inputs : {self.llm_function_runner.args if self.llm_function_runner is not None else '-#-'} output): {res}\"\n                #out += await self.a_mini_task(out_, \"system\", \"\"\"Return a Markdown-formatted output that includes both the function call and its context. Evaluate the result:\n                #If an error or failure occurs, simply indicate that an error occurred without attempting to fix it.\n                #Otherwise, format the function output neatly so it can be used directly.\"\"\", message=[x for x in llm_message if x.get('role') != \"system\"])\n            await update_progress()\n\n        else:\n            if sto_model is not None:\n                self.amd.model = sto_model\n            raise ValueError(f\"Could not {with_functions=} must be true or false\")\n\n        if sto_model is not None:\n            self.amd.model = sto_model\n        stage[0] = 1\n        await update_progress(1)\n\n        if task.id not in self.status_dict:\n            task_status = TaskStatus(\n                task_id=task.id,\n                status=\"queued\",\n                progress=0.0\n            )\n            self.status_dict[task.id] = task_status\n\n        self.status_dict[task.id].status = \"completed\"\n        self.status_dict[task.id].result = out\n        self.status_dict[task.id].progress = 1.0\n\n        return out\n\n    def _to_task(self, query):\n        class TaskStats(BaseModel):\n            \"\"\"Esitmate priority HIGH = 3,MEDIUM = 2,LOW = 1, complexity between 0 and 1\"\"\"\n            priority: int\n            complexity: float\n\n        ts = self.format_class(TaskStats, query)\n        task = Task(\n            id=str(uuid.uuid4())[:16],\n            description=query,\n            priority=ts[\"priority\"],\n            estimated_complexity=ts[\"complexity\"],\n            time_sensitivity=1 - ts[\"complexity\"],\n            created_at=datetime.now()\n        )\n        self.status_dict[task.id] = TaskStatus(\n            task_id=task.id,\n            status=\"queued\",\n            progress=0.0\n        )\n        return task\n\n    async def get_memory(self, ref, massages=None, callback=None, memory_names=None, **kwargs):\n\n        if massages is None:\n            massages = []\n\n        if callback is None:\n            async def callback(*a,**k):\n                pass\n        update_progress = callback\n\n        def add_unique_message(role: str, content: str):\n            new_message = asdict(LLMMessage(role, content.strip()))\n            if new_message not in massages:\n                massages.append(new_message)\n\n        class MemoryQuery(BaseModel):\n            \"\"\"ref question or  context for this task\"\"\"\n            query: str\n\n        class RefMemory(BaseModel):\n            queries: list[MemoryQuery]\n\n        queries = self.format_class(RefMemory, ref)\n\n        queries = queries[\"queries\"][:3]\n        await update_progress()\n        if self.memory:\n            for query in queries:\n                query = query[\"query\"]\n                memory_task = await self.memory.query(query, memory_names=[self.amd.name] if memory_names is None else memory_names, to_str=True, **kwargs)\n                if memory_task:\n                    mem_data = json.dumps(memory_task, indent=2)\n                    add_unique_message(\"system\", \"(system General-context) :\" + mem_data)\n        await update_progress()\n        if self.content_memory and len(self.content_memory.text) &gt; 260:\n            class Context(BaseModel):\n                \"\"\"Persise and relevant context only for {ref[:5000]}\"\"\"\n                context: str\n\n            context = self.format_class(Context, self.content_memory.text + f\"Persise and relevant context only for {ref[:5000]}\")[\"context\"]\n            if context:\n                add_unique_message(\"system\", \"(system memory-context) :\" + context)\n            await update_progress()\n        self.print_verbose(f\"Addet {len(massages)} entry(s)\")\n        return massages\n\n    def mini_task(self, user_task, task_from=\"user\", mini_task=None, message=None, persist=False):\n        if message is None:\n            message = []\n        if mini_task is not None:\n            message.append({'role': 'system', 'content': mini_task})\n        self.add_function_to_prompt = False\n        if isinstance(user_task, str):\n            llm_message = self.get_llm_message(user_task, persist=persist, task_from=task_from, message=message)\n        elif isinstance(user_task, list):\n            llm_message = self.get_batch_llm_messages(user_task, task_from=task_from, message=message.copy())\n        else:\n            raise ValueError(f\"Invalid mini_task type valid ar str or List[str] is {type(mini_task)} {mini_task}\")\n        return self.run_model(llm_message=llm_message, persist_local=persist, batch=isinstance(mini_task, list))\n\n    async def a_mini_task(self, user_task, task_from=\"user\", mini_task=None, message=None, persist=False):\n        if message is None:\n            message = []\n        if mini_task is not None:\n            message.append({'role': 'system', 'content': mini_task})\n        self.add_function_to_prompt = False\n        if isinstance(user_task, str):\n            llm_message = self.get_llm_message(user_task, persist=persist, task_from=task_from, message=message)\n        elif isinstance(user_task, list):\n            llm_message = await self.get_batch_llm_messages(user_task, task_from=task_from, message=message.copy())\n        else:\n            raise ValueError(f\"Invalid mini_task type valid ar str or List[str] is {type(mini_task)}\")\n        return await self.a_run_model(llm_message=llm_message, persist_local=persist, batch=isinstance(mini_task, list))\n\n    def format_class(self, format_class, task, **kwargs):\n        tstrem = self.stream\n        self.stream = False\n        llm_message = self.get_llm_message(task, persist=False, **kwargs)\n        if 'claude' in self.amd.model and llm_message[0]['role'] != 'user':\n            llm_message = [{'role':'user','content':'start :)'}] +llm_message\n\n        try:\n            resp = self.completion(\n                llm_message=llm_message,\n                response_format=format_class,\n            )\n\n            c = self.format_helper(resp)\n        except litellm.exceptions.BadRequestError as e:\n            if 'failed_generation' not in str(e):\n                raise e\n            c = str(e).split('\"failed_generation\":')[-1][:-3]\n        res = after_format(c)\n        self.stream = tstrem\n        print(res)\n        return res\n\n    async def a_format_class(self, format_class, task, **kwargs):\n        tstrem = self.stream\n        self.stream = False\n        llm_message = self.get_llm_message(task, persist=False, **kwargs)\n        if 'claude' in self.amd.model and llm_message[0]['role'] != 'user':\n            llm_message = [{'role':'user','content':'start :)'}] +llm_message\n        # print_prompt(llm_message)\n        try:\n            resp = await self.acompletion(\n                llm_message=llm_message,\n                response_format=format_class,\n            )\n\n            c = self.format_helper(resp)\n        except litellm.exceptions.BadRequestError as e:\n            if 'failed_generation' not in str(e):\n                raise e\n            c = str(e).split('\"failed_generation\":')[-1][:-3]\n        # print(resp)\n        self.last_result = c\n\n        try:\n            res = after_format(c)\n            self.stream = tstrem\n            return res\n        except Exception as e:\n            self.print_verbose(f\"Error formatting, Retrying... {e}\")\n            llm_message = [{'role': 'system', 'content': f'retry error : {e}'}] + llm_message\n            resp = await self.acompletion(\n                llm_message=llm_message,\n                response_format=format_class,\n            )\n            self.stream = tstrem\n            # print(resp)\n            c = self.format_helper(resp)\n            res = after_format(c)\n            self.stream = tstrem\n            return res\n\n    def format_helper(self, resp):\n        c = None\n        if not self.stream:\n            with contextlib.suppress(ValueError):\n                c = resp.choices[0].message.tool_calls[0].function.arguments\n        if c is None:\n            c = self.parse_completion(resp)\n        return c\n\n    def function_invoke(self, name, **kwargs):\n        if self.functions is None:\n            return \"no functions\"\n        fuction_list = [f.function for f in self.functions if f.name == name]\n        if len(fuction_list):\n            try:\n                return fuction_list[0](**kwargs)\n            except Exception as e:\n                return f\"Error in fuction {name} :\" + str(e)\n        return f\"function {name} not found\"\n\n    def reset_context(self):\n        self.messages = []\n        self.world_model = {}\n        self.content_memory.text = \"\"\n\n    def check_valid(self):\n\n        if self.amd.name is None:\n            print(self.amd)\n            return False\n\n        if self.amd.provider is not None and self.amd.provider.upper() in [\"LOCAL\"]:\n            return True\n\n        response = True  # check_valid_key(model=self.amd.model, api_key=self.amd.api_key)\n\n        if not response:\n            self.print_verbose(f\"Agent Failed {self.amd.name} {self.amd.model}\")\n\n        self.print_verbose(f\"Agent Parsed {self.amd.name} {self.amd.model}\")\n        return response\n\n    def construct_first_msg(self) -&gt; list[dict[str, str]]:\n        llm_prompt = self.amd.system_message\n        self.print_verbose(\"construct first msg\")\n        cfunctions_infos = []\n        message = []\n        if self.capabilities:\n            llm_prompt += '\\n' + self.capabilities.trait\n\n            if self.capabilities.functions:\n                cfunctions_infos = [functions for functions in self.capabilities.functions if\n                                    functions not in (self.functions if self.functions else [])]\n\n        if self.mode:\n            llm_prompt += '\\n' + self.mode.system_msg\n\n            if self.mode.examples:\n                llm_prompt += \"\\nExamples: \\n\" + '-----\\n' + \"\\n---\\n\".join(\n                    self.mode.examples) + '\\n END of Examples!\\n'\n\n        if self.add_function_to_prompt and (self.functions or len(cfunctions_infos)):\n            functions_infos = \"\\n\".join(\n                [str(functions) for functions in (self.functions if self.functions else []) + cfunctions_infos])\n            functions_infos = functions_infos.replace(\"_empty\", 'str')\n            message.append({'role': 'system', 'content': \"calling a function by using this exact syntax (json) : {\"\n                                                         \"'Action':str, 'Inputs':str or dict}\\nWhere Action is equal to \"\n                                                         \"the function name and Inputs to the function args. use str for \"\n                                                         \"single input function and a kwarg dict for multiple inputs!! (in one line do not use line brakes or special enclosing!)\"\n                                                         \"USE THIS FORMAT\\n\" + f\"Callable functions:\\n{functions_infos}\\n--+--\\nTemplate Call {{'Action':str, 'Inputs': {{function inputs args or kwargs}}}} all function calls must include 'Action' AND Inputs' as key!\\nAfter Calling a function type 3 '.' and new line ...\\n\"})\n\n        if llm_prompt:\n            message.append({'role': 'system', 'content': llm_prompt})\n        return message\n\n    async def get_batch_llm_messages(self, user_input: list[str], fetch_memory: bool | None = None,\n                               message=None, task_from: str = 'user'):\n        llm_messages = []\n        for task in user_input:\n            msg = self.get_llm_message(user_input=task, persist=False, message=message, task_from=task_from)\n            if fetch_memory:\n                msg = await self.get_memory(task, msg)\n            llm_messages.append(msg)\n        return llm_messages\n\n    def get_llm_message(self, user_input: str, persist: bool | None = None,\n                        message=None, task_from: str = 'user'):\n        llm_message = message\n        if llm_message is None:\n            llm_message = []\n\n        self.user_input = user_input\n\n        # Helper function to add a message to llm_message without duplicates\n        def add_unique_message(role: str, content: str):\n            new_message = asdict(LLMMessage(role, content.strip()))\n            if new_message not in llm_message:\n                llm_message.append(new_message)\n\n        # Add initial system message if it's the first call\n        if not persist or len(self.messages) == 0:\n            llm_message.extend(self.construct_first_msg())\n\n        if persist and len(self.messages) &gt; 1 and \"system\" not in [x.get(\"role\") for x in self.messages]:\n            [add_unique_message(m['role'], m['content']) for m in self.construct_first_msg() if\n             'content' in m and 'role' in m]\n\n        # Add the current task\n        llm_message.append(asdict(LLMMessage(task_from, user_input.strip())))\n        # Add mode-specific message if applicable\n        if self.mode and self.mode.post_msg:\n            add_unique_message(\"system\", self.mode.post_msg)\n\n        # Trim the message history\n        llm_message = self.trim_msg(llm_message)\n\n        # Handle persistence and update content memory\n        if persist:\n            self.messages.extend(llm_message)\n            if self.content_memory:\n                self.content_memory.text += f\"\\nUSER:{user_input}\\nRESPONSE:\"\n            llm_message = self.messages\n\n        # Organize messages: system messages first, then chat history\n        system_messages = [msg for msg in llm_message if msg['role'] == 'system']\n        chat_history = [msg for msg in llm_message if msg['role'] != 'system']\n\n        # Limit chat history length and offload excess\n        max_history_length = self.max_history_length  # Adjust as needed\n        if len(chat_history) &gt; max_history_length:\n            self.offloaded_history = chat_history[:-max_history_length]\n            chat_history = chat_history[-max_history_length:]\n\n        # Reduce system messages\n        reduced_system_messages = reduce_system_messages(system_messages,\n                                                         similarity_threshold=self.similarity_threshold)\n\n        # Combine organized messages\n        llm_message = reduced_system_messages + chat_history\n\n        self.print_verbose(f\"Returning llm message {len(llm_message)}\")\n        return llm_message\n\n    def trim_msg(self, llm_message=None, isaa=None):\n\n        if self.trim == 'IsaaTrim' and isaa:\n\n\n            # print(\"================================\\n\", self.prompt_str(llm_message),\n            # \"\\n================================\\n\")\n            def get_tokens_estimation(text, only_len=True):\n                if isinstance(text, list):\n                    text = '\\n'.join(msg['content'] for msg in text if isinstance(msg['content'], str))\n\n                tokens = get_token_mini(text, self.amd.model, isaa, only_len)\n                if only_len and tokens == 0:\n                    tokens = int(len(text) * (3 / 4))\n                return tokens\n\n            new_msg = isaa.short_prompt_messages(llm_message.copy(), get_tokens_estimation,\n                                                 get_max_token_fom_model_name(self.amd.model))\n            om = ''.join([c['content'] for c in llm_message])\n            nm = ''.join([c['content'] for c in new_msg])\n            nt = get_tokens_estimation(nm, True)\n            self.print_verbose(f\"Timing with IsaaTrim from {len(om)} to {len(nm)}\")\n            self.print_verbose(f\"Timing with IsaaTrim place {get_max_token_fom_model_name(self.amd.model)-nt}\")\n            self.print_verbose(f\" tokens {get_tokens_estimation(om, True)} to {nt} max {get_max_token_fom_model_name(self.amd.model)} \")\n            if new_msg:\n                llm_message = new_msg\n\n        else:  #         if self.trim == 'Trims':\n            self.print_verbose(\"Timing with Trims\")\n            with Spinner(message=\"Sorten prompt lit...\", symbols='d'):\n                new_msg = trim_messages(llm_message, self.amd.model)\n                if new_msg:\n                    llm_message = new_msg\n        return llm_message\n\n    def set_rformat(self, specification: dict):\n        if isinstance(specification, dict):\n            self.rformat = {\"type\": \"json_object\", \"json_schema\": specification, \"strict\": True}\n        elif isinstance(specification, str):\n            self.rformat = {\"type\": \"text\", \"schema\": specification, \"strict\": True}\n\n    def reset_rformat(self):\n        self.rformat = None\n\n    def prompt_str(self, llm_message):\n        llm_message = self.trim_msg(llm_message)\n        prompt = \"\\n\".join(f\"{d.get('role')}:{d.get('content')}\" for d in llm_message)\n\n        return prompt\n\n    def completion(self, llm_message, batch=False, **kwargs):\n        self.print_verbose(\"Starting completion\")\n\n        if self.vision:\n            llm_message = llm_message.copy()\n            for msg in llm_message:\n                if msg.get('role') != 'assistant':\n                    msg['content'] = self.content_add_immage(msg['content'])\n\n        if self.amd.provider is not None and self.amd.provider.upper() == \"GPT4All\" and self.model is None:\n            self.model = gpt4all.GPT4All(self.amd.model)\n\n        if self.amd.provider is not None and self.amd.provider.upper() == \"GPT4All\" and self.model is not None:\n            prompt = self.prompt_str(llm_message)\n\n            if not prompt:\n                print(\"No prompt\")\n                return\n\n            if kwargs.get('mock_response', False):\n                return kwargs.get('mock_response')\n\n            stop_callback = None\n\n            if self.amd.stop_sequence:\n\n                self.hits = \"\"  # TODO : IO string wirte\n\n                def stop_callback_func(token: int, response):\n                    self.hits += response\n                    if self.hits in self.amd.stop_sequence:\n                        return False\n                    if response == ' ':\n                        self.hits = \"\"\n\n                    return True\n\n                stop_callback = stop_callback_func\n\n            # Werte, die \u00fcberpr\u00fcft werden sollen\n            dynamic_values = {\n\n                'streaming': self.stream,\n                'temp': self.amd.temperature,\n                'top_k': self.amd.top_k,\n                'top_p': self.amd.top_p,\n                'repeat_penalty': self.amd.repeat_penalty,\n                'repeat_last_n': self.amd.repeat_last_n,\n                'n_batch': self.amd.n_batch,\n                'max_tokens': self.max_tokens,\n                'callback': stop_callback\n            }\n\n            # F\u00fcge Werte zu kwargs hinzu, wenn sie nicht None sind\n            kwargs.update(add_to_kwargs_if_not_none(**dynamic_values))\n\n            result = self.model.generate(\n                prompt=prompt,\n                **kwargs\n            )\n            self.print_verbose(\"Local Completion don\")\n            return result\n\n        # Werte, die \u00fcberpr\u00fcft werden sollen\n        dynamic_values = {\n            'response_format': self.rformat,\n            'temperature': self.amd.temperature,\n            'top_p': self.amd.top_p,\n            'top_k': self.amd.top_k,\n            'stream': self.stream,\n            'stop': self.amd.stop_sequence,\n            'max_tokens': self.max_tokens,\n            'user': self.amd.user_id,\n            'api_base': self.amd.api_base,\n            'api_version': self.amd.api_version,\n            'api_key': self.amd.api_key,\n            'verbose': self.verbose,\n            # 'fallbacks': self.amd.fallbacks,\n            'caching': self.amd.caching,\n            'functions': [{\"name\": f.name, \"description\": f.description, \"parameters\": f.parameters} for f in\n                                   self.functions] if self.add_function_to_prompt else None,\n            'custom_llm_provider': self.amd.provider if self.amd.provider is not None and self.amd.provider.upper() != \"DEFAULT\" else None\n        }\n\n        if 'claude' in self.amd.model:\n            dynamic_values['drop_params'] = True\n\n        if self.add_function_to_prompt:\n            litellm.add_function_to_prompt = True\n\n        # F\u00fcge Werte zu kwargs hinzu, wenn sie nicht None sind\n        kwargs.update(add_to_kwargs_if_not_none(**dynamic_values))\n\n        if batch:\n            result = batch_completion(\n                model=self.amd.model,\n                messages=llm_message,\n                # fallbacks=os.getenv(\"FALLBACKS_MODELS\").split(','),\n                **kwargs\n            )\n        else:\n            # print(\"Model completion\", self.amd.model, llm_message, kwargs)\n            result = completion(\n                model=self.amd.model,\n                messages=llm_message,\n                # fallbacks=os.getenv(\"FALLBACKS_MODELS\").split(','),\n                **kwargs\n            )\n\n        litellm.add_function_to_prompt = False\n        self.print_verbose(\"Completion\", \"Done\" if not self.stream else \"in progress..\")\n        return result\n\n    async def acompletion(self, llm_message, batch=False, **kwargs):\n        self.print_verbose(\"Starting acompletion\")\n\n        if self.vision:\n            llm_message = llm_message.copy()\n            for msg in llm_message:\n                if msg.get('role') != 'assistant':\n                    msg['content'] = self.content_add_immage(msg['content'])\n\n        if self.amd.provider is not None and self.amd.provider.upper() == \"GPT4All\" and self.model is None:\n            self.model = gpt4all.GPT4All(self.amd.model)\n\n        if self.amd.provider is not None and self.amd.provider.upper() == \"GPT4All\" and self.model is not None:\n            prompt = self.prompt_str(llm_message)\n\n            if not prompt:\n                print(\"No prompt\")\n                return\n\n            if kwargs.get('mock_response', False):\n                return kwargs.get('mock_response')\n\n            stop_callback = None\n\n            if self.amd.stop_sequence:\n\n                self.hits = \"\"  # TODO : IO string wirte\n\n                def stop_callback_func(token: int, response):\n                    self.hits += response\n                    if self.hits in self.amd.stop_sequence:\n                        return False\n                    if response == ' ':\n                        self.hits = \"\"\n\n                    return True\n\n                stop_callback = stop_callback_func\n\n            # Werte, die \u00fcberpr\u00fcft werden sollen\n            dynamic_values = {\n\n                'streaming': self.stream,\n                'temp': self.amd.temperature,\n                'top_k': self.amd.top_k,\n                'top_p': self.amd.top_p,\n                'repeat_penalty': self.amd.repeat_penalty,\n                'repeat_last_n': self.amd.repeat_last_n,\n                'n_batch': self.amd.n_batch,\n                'max_tokens': self.max_tokens,\n                'callback': stop_callback\n            }\n\n            # F\u00fcge Werte zu kwargs hinzu, wenn sie nicht None sind\n            kwargs.update(add_to_kwargs_if_not_none(**dynamic_values))\n\n            result = self.model.generate(\n                prompt=prompt,\n                **kwargs\n            )\n            self.print_verbose(\"Local Completion don\")\n            return result\n\n        # Werte, die \u00fcberpr\u00fcft werden sollen\n        dynamic_values = {\n            'response_format': self.rformat,\n            'temperature': self.amd.temperature,\n            'top_p': self.amd.top_p,\n            'top_k': self.amd.top_k,\n            'stream': self.stream,\n            'stop': self.amd.stop_sequence,\n            'max_tokens': self.max_tokens,\n            'user': self.amd.user_id,\n            'api_base': self.amd.api_base,\n            'api_version': self.amd.api_version,\n            'api_key': self.amd.api_key,\n            'verbose': self.verbose,\n            # 'fallbacks': self.amd.fallbacks,\n            'caching': self.amd.caching,\n            'functions': [{\"name\": f.name, \"description\": f.description, \"parameters\": f.parameters} for f in\n                                   self.functions] if self.add_function_to_prompt else None,\n            'custom_llm_provider': self.amd.provider if self.amd.provider is not None and self.amd.provider.upper() != \"DEFAULT\" else None\n        }\n\n        if 'claude' in self.amd.model:\n            dynamic_values['drop_params'] = True\n\n        if self.add_function_to_prompt:\n            litellm.add_function_to_prompt = True\n\n        # F\u00fcge Werte zu kwargs hinzu, wenn sie nicht None sind\n        kwargs.update(add_to_kwargs_if_not_none(**dynamic_values))\n\n        if batch:\n            result = batch_completion(\n                model=self.amd.model,\n                messages=llm_message,\n                # fallbacks=os.getenv(\"FALLBACKS_MODELS\").split(','),\n                **kwargs\n            )\n        else:\n            # print(\"Model completion\", self.amd.model, llm_message, kwargs)\n            result = await acompletion(\n                model=self.amd.model,\n                messages=llm_message,\n                # fallbacks=os.getenv(\"FALLBACKS_MODELS\").split(','),\n                **kwargs\n            )\n\n        litellm.add_function_to_prompt = False\n        self.print_verbose(\"Completion\", \"Done\" if not self.stream else \"in progress..\")\n        return result\n\n    def transcription(self, file_bytes, prompt=\"\", temperature=0,\n                      response_format: Literal[\"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\"] | None = None\n                      ):\n        return litellm.transcription(\n            model=self.amd.model,\n            file=file_bytes,\n            prompt=prompt,\n            temperature=temperature,\n            response_format=response_format,\n        )\n\n    def model_function_result_passer(self, result, default=None):\n        if default is None:\n            default = \"\"\n        else:\n            default += \"\\n\"\n\n        # Check if we are in streaming mode using the self.stram flag\n        if self.stream:\n            # Iterate over each streaming choice chunk\n            for choice in result.choices:\n                delta = getattr(choice, \"delta\", None)\n                if delta and getattr(delta, \"content\", None):\n                        default = delta.content\n        else:\n            # Non-streaming mode handling\n            print(result.choices[0])\n            if hasattr(result.choices[0].message, \"tool_calls\") and result.choices[\n                0].message.tool_calls and self.functions is not None:\n                if len(result.choices[0].message.tool_calls) != 1:\n                    default += f\"taskstack added {len(result.choices[0].message.tool_calls)}\"\n                    for fuc_call in result.choices[0].message.tool_calls:\n                        self.taskstack.add_task(\n                            self._to_task(\n                                f\"Call this function '{fuc_call.function.name}' with these arguments: {fuc_call.function.arguments}\"\n                            )\n                        )\n                else:\n                    callable_functions = [func.name.lower() for func in\n                                          self.functions] if self.functions is not None else []\n                    function_name = result.choices[0].message.tool_calls[0].function.name.lower()\n                    if function_name in callable_functions:\n                        llm_function = self.functions[callable_functions.index(function_name)]\n                        self.if_for_fuction_use_overrides = True\n                        d = json.loads(result.choices[0].message.tool_calls[0].function.arguments)\n                        if 'properties' in d and isinstance(d['properties'], dict):\n                            d = d['properties']\n                        self.llm_function_runner = LLMFunctionRunner(\n                            llm_function=llm_function,\n                            args=(),\n                            kwargs=d,\n                        )\n                        default += f\"Calling {result.choices[0].message.tool_calls[0].function.name} with arguments {result.choices[0].message.tool_calls[0].function.arguments}\"\n        return default\n\n    def parse_completion(self,result):\n        llm_response = \"\"\n        if not self.stream:\n            return get_str_response(chunk=result)\n\n        if self.stream:\n            self.print_verbose(\"Start streaming\")\n\n            if self.stream_function is None:\n                self.stream_function = stram_print\n\n            chunks = []\n            for chunk in result:\n                chunks.append(chunk)\n                message = get_str_response(chunk=chunk)\n                message = self.model_function_result_passer(chunk, message)\n                llm_response += message\n                if self.stream_function(message):\n                    break\n            self.print_verbose(\"Done streaming\")\n            result = litellm.stream_chunk_builder(chunks)\n        return self.model_function_result_passer(result, llm_response)\n\n    def run_model(self, llm_message, persist_local=True, persist_mem=True, batch=False, **kwargs):\n\n        if not llm_message:\n            return None\n\n        self.print_verbose(\"Running llm model\")\n\n        self.next_fuction = None\n\n        if len(llm_message) &gt; 2:\n            llm_message = [{'role': 'assistant',\n                            'content': f'Hello, I am an intelligent agent created to assist you. To provide the best possible response, I will first gather information about you and any relevant context. I will then analyze the requirements for a unified agent response and develop a multi-step reasoning process to address your needs. This process will involve distinct streams of thought and personality, culminating in a final, cohesive action. Please provide any additional details or instructions you may have, and I will do my best to deliver a helpful and personalized solution. To anabel a sees of time i must allways remember the [system time {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}]'}] + llm_message\n\n        if len(llm_message) == 1 and llm_message[0]['role'] != 'user':\n            llm_message = [{'role': 'user', 'content': 'Performe the system task!'}] + llm_message\n        if llm_message[0]['role'] != 'user':\n            llm_message = [{'role': 'user', 'content': '.'}] + llm_message\n        # print_prompt(llm_message)\n\n        result = None\n        max_r = 2\n        r_try = 0\n        last_error_ = None\n        llm_response = \"\"\n        tok_input = get_token_mini(llm_message, self.amd.model)\n        try:\n            tok_max = get_max_tokens(self.amd.model)\n        except Exception:\n            tok_max = 199000\n        print(f\"AGENT {self.amd.name} TOKENS {tok_input} {tok_max}\")\n        if tok_input &gt; tok_max:\n            llm_message = self.trim_msg(llm_message)\n        while result is None and r_try &lt; max_r:\n            try:\n                result = self.completion(llm_message=llm_message, batch=batch, **kwargs)\n                r_try = 9999\n                break\n            except litellm.RateLimitError as e:\n                print(f\"RateLimitError {e}\")\n                last_error_ = e\n                if '413' in str(e) and 'reduce' not in str(e):\n                    with Spinner(\"Reitlimit Waiting 1 minute\"):\n                        time.sleep(30)\n                r_try += 1\n                llm_message = self.trim_msg(llm_message)\n            except litellm.InternalServerError as e:\n                print(f\"InternalServerError {e}\")\n                last_error_ = e\n                r_try += 1\n                # print_prompt(llm_message)\n                lm = len(llm_message)\n                llm_message = self.trim_msg(llm_message)\n                print(f\"AFTER TRIM {lm}/{len(llm_message)}\")\n                # print_prompt(llm_message)\n                with Spinner(\"Waring... for api\", count_down=True, time_in_s=r_try * 10):\n                    time.sleep(r_try * 10)\n                continue\n\n        if result is None and last_error_ is not None:\n            raise last_error_\n\n        llm_response = self.parse_completion(result)\n\n        if not batch:\n            return self.compute_result(result, llm_message, llm_response, persist_local, persist_mem)\n        return [self.compute_result(_result, llm_message, llm_response, persist_local, persist_mem) for _result in\n                result]\n\n    async def a_run_model(self, llm_message, persist_local=True, persist_mem=True, batch=False, **kwargs):\n\n        if not llm_message:\n            return None\n\n        self.print_verbose(\"Running llm model\")\n\n        self.next_fuction = None\n\n        if len(llm_message) &gt; 2:\n            llm_message = [{'role': 'assistant',\n                            'content': f'Hello, I am an intelligent agent created to assist you. To provide the best possible response, I will first gather information about you and any relevant context. I will then analyze the requirements for a unified agent response and develop a multi-step reasoning process to address your needs. This process will involve distinct streams of thought and personality, culminating in a final, cohesive action. Please provide any additional details or instructions you may have, and I will do my best to deliver a helpful and personalized solution. [system time {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}]'}] + llm_message\n\n        if len(llm_message) == 1 and llm_message[0]['role'] != 'user':\n            llm_message = [{'role': 'user', 'content': 'Performe the system task!'}] + llm_message\n        if llm_message[0]['role'] != 'user':\n            llm_message = [{'role': 'user', 'content': '.'}] + llm_message\n        # print_prompt(llm_message)\n\n        result = None\n        max_r = 2\n        r_try = 0\n        last_error_ = None\n        llm_response = \"\"\n        tok_input = get_token_mini(llm_message, self.amd.model)\n        try:\n            tok_max = get_max_tokens(self.amd.model)\n        except Exception:\n            tok_max = 199000\n        print(f\"AGENT {self.amd.name} TOKENS {tok_input} {tok_max}\")\n        if tok_input &gt; tok_max:\n            llm_message = self.trim_msg(llm_message)\n        while result is None and r_try &lt; max_r:\n            try:\n                result = await self.acompletion(llm_message=llm_message, batch=batch, **kwargs)\n                r_try = 9999\n                break\n            except litellm.RateLimitError as e:\n                print(f\"RateLimitError {e}\")\n                last_error_ = e\n                if '413' in str(e) and 'reduce' not in str(e):\n                    with Spinner(\"Reitlimit Waiting 1 minute\"):\n                        time.sleep(30)\n                r_try += 1\n                llm_message = self.trim_msg(llm_message)\n            except litellm.InternalServerError as e:\n                print(f\"InternalServerError {e}\")\n                last_error_ = e\n                r_try += 1\n                # print_prompt(llm_message)\n                lm = len(llm_message)\n                llm_message = self.trim_msg(llm_message)\n                print(f\"AFTER TRIM {lm}/{len(llm_message)}\")\n                # print_prompt(llm_message)\n                with Spinner(\"Waring... for api\", count_down=True, time_in_s=r_try * 10):\n                    time.sleep(r_try * 10)\n                continue\n\n        if result is None and last_error_ is not None:\n            raise last_error_\n\n        llm_response = self.parse_completion(result)\n\n        if not batch:\n            return await self.acompute_result(result, llm_message, llm_response, persist_local, persist_mem)\n        return [await self.acompute_result(_result, llm_message, llm_response, persist_local, persist_mem) for _result in\n                result]\n\n    async def acompute_result(self, result, llm_message, llm_response, persist_local=False, persist_mem=False) -&gt; str:\n        print_prompt(llm_message + [{'content': llm_response, 'role': 'assistant'}])\n        self.last_result = llm_response\n        if self.amd.budget_manager:\n            self.amd.budget_manager.update_cost(user=self.amd.user_id, model=self.amd.model, completion_obj=result)\n\n        await self.save_to_memory(llm_response, persist_local, persist_mem)\n\n        if self.mode is not None:\n            if isinstance(llm_message[-1], dict):\n                _llm_message = [llm_message]\n            else:\n                _llm_message = llm_message\n            for llm_message in _llm_message:\n                # print(f\"{isinstance(self.mode, ModeController)=} and {hasattr(self.mode, 'add_shot')=} and {llm_message[-1].get('content', False)=}\")\n                if isinstance(self.mode, ModeController) and hasattr(self.mode, 'add_shot') and llm_message[-1].get(\n                    'content',\n                    False):\n                    self.mode.add_shot(llm_message[-1].get('content'), llm_response)\n\n        if self.post_callback:\n            await self.post_callback(llm_response)\n        return llm_response\n\n    def compute_result(self, result, llm_message, llm_response, persist_local=False, persist_mem=False) -&gt; str:\n        print_prompt(llm_message + [{'content': llm_response, 'role': 'assistant'}])\n        self.last_result = llm_response\n        if self.amd.budget_manager:\n            self.amd.budget_manager.update_cost(user=self.amd.user_id, model=self.amd.model, completion_obj=result)\n\n        # get_app().run_a_from_sync(self.save_to_memory,llm_response, persist_local, persist_mem)\n\n        if self.mode is not None:\n            if isinstance(llm_message[-1], dict):\n                _llm_message = [llm_message]\n            else:\n                _llm_message = llm_message\n            for llm_message in _llm_message:\n                # print(f\"{isinstance(self.mode, ModeController)=} and {hasattr(self.mode, 'add_shot')=} and {llm_message[-1].get('content', False)=}\")\n                if isinstance(self.mode, ModeController) and hasattr(self.mode, 'add_shot') and llm_message[-1].get(\n                    'content',\n                    False):\n                    self.mode.add_shot(llm_message[-1].get('content'), llm_response)\n\n        if self.post_callback:\n            self.post_callback(llm_response)\n        return llm_response\n\n    async def save_to_memory(self, llm_response: str, persist_local=False, persist_mem=False):\n\n        if isinstance(llm_response, list) and len(llm_response) &gt; 0 and isinstance(llm_response[0], str):\n            llm_response = '\\n'.join(llm_response)\n        elif isinstance(llm_response, list) and len(llm_response) == 0:\n            return\n\n        async def helper():\n            if persist_local:\n                self.messages.append({'content': llm_response, 'role': 'assistant'})\n\n            if self.amd.name.startswith(\"TaskC\"):\n                return\n            if persist_mem and self.memory is not None:\n                self.print_verbose(\"persist response to persistent_memory\")\n                await self.memory.add_data(self.amd.name,  'CHAT - HISTORY\\n'+self.user_input + '\\n:\\n' + str(llm_response))\n\n            if persist_mem and self.content_memory is not None:\n                self.print_verbose(\"persist response to content_memory\")\n                self.content_memory.text += llm_response\n\n        await helper()\n        # threading.Thread(target=helper, daemon=True).start()\n\n    def if_for_fuction_use(self, llm_response):\n        if self.if_for_fuction_use_overrides:\n            self.if_for_fuction_use_overrides = False\n            self.print_verbose(\"Function runner initialized\")\n            return True\n        llm_fuction = None\n        fuction_inputs = None\n        self.next_fuction = None\n        if self.capabilities is not None and self.capabilities.functions is not None and len(\n            self.capabilities.functions) &gt; 0:\n            callable_functions = [fuction_name.name.lower() for fuction_name in self.capabilities.functions]\n\n            self.next_fuction, fuction_inputs = self.test_use_function(llm_response, callable_functions)\n            if self.next_fuction is not None:\n                llm_fuction = self.capabilities.functions[callable_functions.index(self.next_fuction.lower())]\n\n        if self.functions is not None and len(self.functions) &gt; 0 and self.next_fuction is None:\n            callable_functions = [fuction_name.name.lower() for fuction_name in self.functions]\n            self.next_fuction, fuction_inputs = self.test_use_function(llm_response, callable_functions)\n            if self.next_fuction is not None:\n                llm_fuction = self.functions[callable_functions.index(self.next_fuction.lower())]\n\n        if self.next_fuction is None and llm_fuction is None:\n            self.llm_function_runner = LLMFunctionRunner(\n                llm_function=None,\n                args=None,\n                kwargs=None,\n            )\n            self.print_verbose(\"No fuction called\")\n\n            return False\n\n        args = []\n        kwargs = {}\n\n        if fuction_inputs is not None:\n            args, kwargs = self.parse_arguments(fuction_inputs, llm_fuction.parameters)\n\n        self.llm_function_runner = LLMFunctionRunner(\n            llm_function=llm_fuction,\n            args=args,\n            kwargs=kwargs,\n        )\n        self.print_verbose(\"Function runner initialized\")\n        return True\n\n    def print_verbose(self, *args, **kwargs):\n        if self.verbose and self.amd.name is not None:\n            print(Style.BLUE(f\"AGENT:{self.amd.name}: \"), end='')\n            print(' '.join(args)[:250], **kwargs)\n\n    async def execute_fuction(self, persist=True, persist_mem=True):\n        if self.next_fuction is None:\n            if self.verbose:\n                print(\"No fuction to execute\")\n            return \"No fuction to execute\"\n\n        if self.llm_function_runner is None:\n            if self.verbose:\n                print(\"No llm function runner to execute\")\n            return \"No llm function runner to execute\"\n\n        if not self.llm_function_runner.validate():\n            if self.verbose:\n                print(\"Invalid llm function runner\")\n            return \"Invalid llm function runner\"\n\n        result = self.llm_function_runner()\n        if asyncio.iscoroutine(result):\n            result = await result\n\n        self.print_verbose(f\"Fuction {self.llm_function_runner.llm_function.name} Result : {result}\")\n\n        if persist:\n            self.messages.append({'content': f\"(system tool {self.next_fuction}) result:{result}\", 'role': \"system\"})\n\n        if persist_mem and self.content_memory is not None:\n            self.content_memory.text += f\"F:{result}\"\n            self.print_verbose(\"Persist to content Memory\")\n\n        if persist_mem and self.memory is not None:\n            await self.memory.add_data(self.amd.name, f\"FUNKTION Result:{result}\")\n            self.print_verbose(f\"Persist to Memory sapce {self.amd.name}\")\n        if not isinstance(result, str):\n            result = str(result)\n        return result\n\n    def stram_registrator(self, func: Callable[[str], bool]):\n        self.print_verbose(\"StramRegistrator\")\n        self.stream_function = func\n\n    def init_memory(self, isaa, name: str = None):\n        if name is None or name == 'None':\n            name = self.amd.name\n        if name is None:\n            raise ValueError(\"Invalid Agent\")\n        if name == 'None':\n            return\n        self.print_verbose(\"Initializing Memory\")\n        self.memory = isaa.get_memory()\n        self.content_memory = ShortTermMemory(isaa, name + \"-ShortTermMemory\")\n\n    def save_memory(self):\n        self.print_verbose(\"Saving memory\")\n        if self.content_memory is not None:\n            self.print_verbose(\"Saved memory to collective\")\n            self.content_memory.clear_to_collective()\n\n    def token_counter(self, messages: list):\n        return token_counter(model=self.amd.model, messages=messages)\n\n    @staticmethod\n    def fuzzy_string_match(input_string: str, match_list: list, return_info=False):\n        input_string = input_string.lower()\n        matches = []\n        for i in list(range(len(input_string)))[::-1]:\n            for match in match_list:\n                if match.startswith(input_string[:i]):\n                    matches.append(match)\n        for i in list(range(len(input_string))):\n            for match in match_list:\n                if match.endswith(input_string[i:]):\n                    matches.append(match)\n        v_match = []\n        for match in match_list:\n            v_match.append(matches.count(match))\n        # print(v_match)\n        # print(match_list)\n        if return_info:\n            return match_list, v_match\n        return match_list[v_match.index(max(v_match))]\n\n    def test_use_function(self, agent_text: str, all_actions: list[str], language='en') -&gt; tuple[\n        str or None, str or None]:\n        if not agent_text:\n            return None, None\n        agent_text = agent_text.replace('`', '').replace('\\n', '')\n        # all_actions = [action_.lower() for action_ in all_actions]\n        self.print_verbose(\"Starting tests... tools...\")\n\n        action, inputs = _extract_from_json(agent_text.replace(\"'\", '\"'), all_actions)\n        # print(f\"1 {action=}| {inputs=} {agent_text}\")\n        if action is not None:\n            return action.lower(), inputs\n\n        if language == 'de':\n\n            # print(\"_extract_from_string\")\n            action, inputs = _extract_from_string_de(agent_text.replace(\"'\", '\"'), all_actions)\n            # print(f\"2 {action=}| {inputs=} {agent_text}\")\n            if action is not None:\n                return action.lower(), inputs\n\n        action, inputs = _extract_from_string(agent_text.replace(\"'\", '\"'), all_actions)\n        # print(f\"3 {action=}| {inputs=} {agent_text}\")\n        if action is not None:\n            return action.lower(), inputs\n\n        try:\n            agent_dict = anything_from_str_to_dict(agent_text)\n            # print(f\"4 {agent_dict=}| {agent_text}\")\n            if len(agent_dict) &gt; 0:\n                action = agent_dict.get(\"Action\", '')\n                inputs = agent_dict.get(\"Inputs\", {})\n            if action is not None:\n                return action, inputs\n        except ValueError:\n            pass\n\n        return None, None\n\n    @staticmethod\n    def parse_arguments(command: str, parameters: list or dict) -&gt; (list, dict):\n        # Initialisierung der Ausgabeliste und des W\u00f6rterbuchs\n        out_list = []\n        out_dict = {}\n        args = []\n        param_keys = parameters if isinstance(parameters, list) else (\n            list(parameters.keys()) if hasattr(parameters, 'keys') else list(parameters))\n\n        # \u00dcberpr\u00fcfung, ob der Befehl ein W\u00f6rterbuch enth\u00e4lt\n        if isinstance(command, dict):\n            command = json.dumps(command)\n        if isinstance(command, list):\n            args = command\n        if not isinstance(command, str):\n            command = str(command)\n\n        if \"{\" in command and \"}\" in command:\n            s = {}\n            for x in param_keys:\n                s[x] = None\n            arg_dict = anything_from_str_to_dict(command, expected_keys=s)\n\n            if isinstance(arg_dict, list) and len(arg_dict) &gt;= 1:\n                arg_dict = arg_dict[0]\n\n            # \u00dcberpr\u00fcfung, ob es nur einen falschen Schl\u00fcssel und einen fehlenden g\u00fcltigen Schl\u00fcssel gibt\n\n            missing_keys = [key for key in param_keys if key not in arg_dict]\n            extra_keys = [key for key in arg_dict if key not in param_keys]\n\n            if len(missing_keys) == 1 and len(extra_keys) == 1:\n                correct_key = missing_keys[0]\n                wrong_key = extra_keys[0]\n                arg_dict[correct_key] = arg_dict.pop(wrong_key)\n            out_dict = arg_dict\n        else:\n            # Aufteilung des Befehls durch Komma\n            if len(param_keys) == 0:\n                pass\n            elif len(param_keys) == 1:\n                out_list.append(command)\n            elif len(param_keys) &gt;= 2:\n\n                comma_cont = command.count(',')\n                saces_cont = command.count(' ')\n                newline_cont = command.count('\\n')\n                split_key = \"-\"\n                if comma_cont == len(param_keys) - 1:\n                    split_key = \",\"\n                elif newline_cont == len(param_keys) - 1:\n                    split_key = \"\\n\"\n                elif saces_cont == len(param_keys) - 1:\n                    split_key = \" \"\n\n                print(f\"{len(param_keys)=}\\n{comma_cont}\\n{saces_cont}\\n{newline_cont}\")\n\n                if len(param_keys) == 2:\n                    if split_key == \"-\":\n                        split_key = \",\"\n                        pos_space = command.find(\" \")\n                        pos_comma = command.find(\",\")\n                        if pos_space &lt; pos_comma:\n                            split_key = \" \"\n                    args = [arg.strip() for arg in command.split(split_key)]\n                    args = [args[0], split_key.join(args[1:])]\n                else:\n                    args = [arg.strip() for arg in command.split(split_key)]\n\n                # Bef\u00fcllen des W\u00f6rterbuchs und der Liste basierend auf der Signatur\n\n        for i, arg in enumerate(args):\n            if i &lt; len(param_keys) and i != \"callbacks\":\n                out_dict[param_keys[i]] = arg\n            else:\n                out_list.append(arg)\n\n        return out_list, out_dict\n\n    @staticmethod\n    def content_add_immage(content):\n        import base64\n        from urllib.parse import urlparse\n\n        import requests\n\n        def parse_image_references(text):\n            \"\"\"\n            Find image references in the format 'Image[path/url]' and analyze each match.\n\n            Args:\n                text (str): Text to search for image references\n\n            Returns:\n                list[tuple]: List of tuples (image_path_url, is_path, is_url, image_type)\n            \"\"\"\n            from urllib.parse import urlparse\n\n            # Pattern to match Image[...] format\n            pattern = r'Image\\[(.*?)\\]'\n\n            # Common image extensions\n            image_extensions = {\n                'jpg': 'JPEG',\n                'jpeg': 'JPEG',\n                'png': 'PNG',\n                'gif': 'GIF',\n                'pdf': 'PDF',\n                'bmp': 'BMP',\n                'webp': 'WEBP',\n                'svg': 'SVG',\n                'tiff': 'TIFF'\n            }\n\n            def analyze_match(match):\n                path_or_url = match.strip()\n\n                # Check if it's a URL\n                try:\n                    parsed = urlparse(path_or_url)\n                    is_url = bool(parsed.scheme and parsed.netloc)\n                except:\n                    is_url = False\n\n                # Check if it's a local path\n                is_path = not is_url and ('/' in path_or_url or '\\\\' in path_or_url)\n\n                # Get file extension and type\n                extension = path_or_url.split('.')[-1].lower().split('?')[0]\n                image_type = image_extensions.get(extension, 'Unknown')\n\n                return (path_or_url, is_path, is_url, image_type)\n\n            # Find all matches and analyze them\n            matches = re.finditer(pattern, text)\n            results = [analyze_match(m.group(1)) for m in matches]\n\n            return results\n\n        image_urls = parse_image_references(content)\n\n        if len(image_urls) == 0:\n            return content\n\n        def is_valid_url(url):\n            try:\n                result = urlparse(url)\n                return all([result.scheme, result.netloc])\n            except ValueError:\n                return False\n\n        def encode_image_local(image_url, img_type):\n            with open(image_url, \"rb\") as image_file:\n                return {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/{img_type};base64,{base64.b64encode(image_file.read()).decode('utf-8')}\"\n                    },\n                }\n\n        def encode_image_url(image_url, img_type):\n            response = requests.get(image_url)\n            file_data = response.content\n            return {\n                \"type\": \"image_url\",\n                \"image_url\": f\"data:image/{img_type};base64,{base64.b64encode(file_data).decode('utf-8')}\",\n            }\n\n        new_content = [{\"type\": \"text\", \"text\": content}]\n\n        for image_path_url, is_path, is_url, image_type in image_urls:\n\n            if is_url:\n                is_valid = is_valid_url(image_path_url)\n                new_content.append(encode_image_url(image_path_url, image_type)\n                                   if is_valid else\n                                   encode_image_local(image_path_url, image_type))\n                continue\n\n            if is_path:\n                new_content.append(encode_image_local(image_path_url, image_type))\n                continue\n\n        return new_content\n</code></pre> <code>run_in_background()</code> \u00b6 <p>Start a task in background mode</p> Source code in <code>toolboxv2/mods/isaa/base/Agents.py</code> <pre><code>def run_in_background(self):\n    \"\"\"Start a task in background mode\"\"\"\n    self._stop_event.clear()\n\n    if self.state != AgentState.RUNNING:\n        self.state = AgentState.RUNNING\n        self.executor.submit(self._background_worker)\n\n    return self.state\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agents.Trims","title":"<code>Trims</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Enum</code></p> <p>The <code>Trims</code> class represents the available text trim options for LLM.</p> Source code in <code>toolboxv2/mods/isaa/base/Agents.py</code> <pre><code>@dataclass\nclass Trims(Enum):\n    \"\"\"\n    The `Trims` class represents the available text trim options for LLM.\n    \"\"\"\n    LITELLM = \"Trims\"\n    ISAA = \"IsaaTrim\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agents.sTask","title":"<code>sTask</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>.2f</p> Source code in <code>toolboxv2/mods/isaa/base/Agents.py</code> <pre><code>@dataclass\nclass sTask(BaseModel):\n    \"\"\".2f\"\"\"\n    description: str\n    priority: int\n    estimated_complexity: float  # Range 0.0 to 1.0\n    time_sensitivity: float  # Range 0.0 to 1.0\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agents.reduce_system_messages","title":"<code>reduce_system_messages(messages, similarity_threshold=80)</code>","text":"<p>Reduce system messages by removing duplicates and highly similar content using fuzzy string matching.</p> <p>:param messages: List of message dictionaries with 'role' and 'content' keys :param similarity_threshold: Threshold for considering messages similar (0-100) :return: List of reduced system messages</p> Source code in <code>toolboxv2/mods/isaa/base/Agents.py</code> <pre><code>def reduce_system_messages(messages: list[dict[str, str]], similarity_threshold: int = 80) -&gt; list[dict[str, str]]:\n    \"\"\"\n    Reduce system messages by removing duplicates and highly similar content using fuzzy string matching.\n\n    :param messages: List of message dictionaries with 'role' and 'content' keys\n    :param similarity_threshold: Threshold for considering messages similar (0-100)\n    :return: List of reduced system messages\n    \"\"\"\n    system_messages = [msg for msg in messages if msg['role'] == 'system']\n    reduced_messages = []\n    reduced_messages_l = []\n\n    for message in system_messages:\n        is_unique = True\n\n        for existing_message in reduced_messages:\n            similarity = fuzz.ratio(message['content'], existing_message['content'])\n            if similarity &gt;= similarity_threshold:\n                is_unique = False\n                if len(message['content']) &gt; len(existing_message['content']):\n                    reduced_messages[reduced_messages_l.index(len(existing_message['content']))] = message\n                    reduced_messages_l[reduced_messages_l.index(len(existing_message['content']))] = len(\n                        message['content'])\n\n        if is_unique:\n            reduced_messages.append(message)\n            reduced_messages_l.append(len(message['content']))\n\n    return reduced_messages\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase","title":"<code>KnowledgeBase</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.Chunk","title":"<code>Chunk</code>  <code>dataclass</code>","text":"<p>Represents a chunk of text with its embedding and metadata</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>@dataclass(slots=True)\nclass Chunk:\n    \"\"\"Represents a chunk of text with its embedding and metadata\"\"\"\n    text: str\n    embedding: np.ndarray\n    metadata: dict[str, Any]\n    content_hash: str\n    cluster_id: int | None = None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.ConceptAnalysis","title":"<code>ConceptAnalysis</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the analysis of key concepts.</p> <p>Attributes:</p> Name Type Description <code>key_concepts</code> <code>list[str]</code> <p>A list of primary key concepts identified.</p> <code>relationships</code> <code>list[str]</code> <p>A list of relationships between the identified key concepts.</p> <code>importance_hierarchy</code> <code>list[str]</code> <p>A list that represents the hierarchical importance of the key concepts.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class ConceptAnalysis(BaseModel):\n    \"\"\"\n    Represents the analysis of key concepts.\n\n    Attributes:\n        key_concepts (list[str]): A list of primary key concepts identified.\n        relationships (list[str]): A list of relationships between the identified key concepts.\n        importance_hierarchy (list[str]): A list that represents the hierarchical importance of the key concepts.\n    \"\"\"\n    key_concepts: list[str]\n    relationships: list[str]\n    importance_hierarchy: list[str]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.ConceptExtractor","title":"<code>ConceptExtractor</code>","text":"<p>Handles extraction of concepts and relationships from text</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class ConceptExtractor:\n    \"\"\"Handles extraction of concepts and relationships from text\"\"\"\n\n    def __init__(self, knowledge_base, requests_per_second = 85.):\n        self.kb = knowledge_base\n        self.concept_graph = ConceptGraph()\n        self.requests_per_second = requests_per_second\n\n    async def extract_concepts(self, texts: list[str], metadatas: list[dict[str, Any]]) -&gt; list[list[Concept]]:\n        \"\"\"\n        Extract concepts from texts using concurrent processing with rate limiting.\n        Requests are made at the specified rate while responses are processed asynchronously.\n        \"\"\"\n        # Ensure metadatas list matches texts length\n        metadatas = metadatas + [{}] * (len(texts) - len(metadatas))\n\n        # Initialize rate limiter\n        rate_limiter = DynamicRateLimiter()\n\n        system_prompt = (\n            \"Analyze the given text and extract key concepts and their relationships. For each concept:\\n\"\n            \"1. Identify the concept name and category (technical, domain, method, property, ...)\\n\"\n            \"2. Determine relationships with other concepts (uses, part_of, similar_to, depends_on, ...)\\n\"\n            \"3. Assess importance (0-1 score) based on centrality to the text\\n\"\n            \"4. Extract relevant context snippets\\n\"\n            \"5. Max 5 Concepts!\\n\"\n            \"only return in json format!\\n\"\n            \"\"\"{\"concepts\": [{\n                \"name\": \"concept_name\",\n                \"category\": \"category_name\",\n                \"relationships\": {\n                    \"relationship_type\": [\"related_concept1\", \"related_concept2\"]\n                },\n                \"importance_score\": 0.0,\n                \"context_snippets\": [\"relevant text snippet\"]\n            }]}\\n\"\"\"\n        )\n\n        # Prepare all requests\n        requests = [\n            (idx, f\"Text to Convert in to JSON structure:\\n{text}\", system_prompt, metadata)\n            for idx, (text, metadata) in enumerate(zip(texts, metadatas, strict=False))\n        ]\n\n        async def process_single_request(idx: int, prompt: str, system_prompt: str, metadata: dict[str, Any]):\n            \"\"\"Process a single request with rate limiting\"\"\"\n            try:\n                # Wait for rate limit\n                await rate_limiter.acquire()\n                i__[1] += 1\n                # Make API call without awaiting the response\n                response_future = litellm_complete(\n                    prompt=prompt,\n                    system_prompt=system_prompt,\n                    response_format=Concepts,\n                    model_name=self.kb.model_name,\n                    fallbacks=[\"groq/gemma2-9b-it\"] +\n                              [m for m in os.getenv(\"FALLBACKS_MODELS_PREM\", '').split(',') if m]\n                )\n\n                return idx, response_future\n\n            except Exception as e:\n                print(f\"Error initiating request {idx}: {str(e)}\")\n                return idx, None\n\n        async def process_response(idx: int, response_future) -&gt; list[Concept]:\n            \"\"\"Process the response once it's ready\"\"\"\n            try:\n                if response_future is None:\n                    return []\n\n                response = await response_future\n                return await self._process_response(response, metadatas[idx])\n\n            except Exception as e:\n                print(f\"Error processing response {idx}: {str(e)}\")\n                return []\n\n        # Create tasks for all requests\n        request_tasks = []\n        batch_size = self.kb.batch_size\n\n        rate_limiter.update_rate(self.requests_per_second)\n\n        for batch_start in range(0, len(requests), batch_size):\n            batch = requests[batch_start:batch_start + batch_size]\n\n            # Create tasks for the batch\n            batch_tasks = [\n                process_single_request(idx, prompt, sys_prompt, meta)\n                for idx, prompt, sys_prompt, meta in batch\n            ]\n            request_tasks.extend(batch_tasks)\n\n        # Execute all requests with rate limiting\n        request_results = await asyncio.gather(*request_tasks)\n\n        # Process responses as they complete\n        response_tasks = [\n            process_response(idx, response_future)\n            for idx, response_future in request_results\n        ]\n\n        # Gather all results\n        all_results = await asyncio.gather(*response_tasks)\n\n        # Sort results by original index\n        sorted_results = [[] for _ in texts]\n        for idx, concepts in enumerate(all_results):\n            sorted_results[idx] = concepts\n\n        return sorted_results\n\n    async def _process_response(self, response: Any, metadata: dict[str, Any]) -&gt; list[Concept]:\n        \"\"\"Helper method to process a single response and convert it to Concepts\"\"\"\n        try:\n            # Extract content from response\n            if hasattr(response, 'choices'):\n                content = response.choices[0].message.content\n                if content is None:\n                    content = response.choices[0].message.tool_calls[0].function.arguments\n                if content is None:\n                    return []\n            elif isinstance(response, str):\n                content = response\n            else:\n                print(f\"Unexpected response type: {type(response)}\")\n                return []\n\n            # Parse JSON and create concepts\n            concept_data = after_format(content)\n            concepts = []\n\n            for concept_info in concept_data.get(\"concepts\", []):\n                concept = Concept(\n                    name=concept_info[\"name\"],\n                    category=concept_info.get(\"category\", \"N/A\"),\n                    relationships={k: set(v) for k, v in concept_info.get(\"relationships\", {}).items()},\n                    importance_score=concept_info.get(\"importance_score\", 0.1),\n                    context_snippets=concept_info.get(\"context_snippets\", \"N/A\"),\n                    metadata=metadata\n                )\n                concepts.append(concept)\n                self.concept_graph.add_concept(concept)\n\n            return concepts\n\n        except Exception:\n            i__[2] +=1\n            return []\n\n    async def process_chunks(self, chunks: list[Chunk]) -&gt; None:\n        \"\"\"\n        Process all chunks in batch to extract and store concepts.\n        Each chunk's metadata will be updated with the concept names and relationships.\n        \"\"\"\n        # Gather all texts from the chunks.\n        texts = [chunk.text for chunk in chunks]\n        # Call extract_concepts once with all texts.\n        all_concepts = await self.extract_concepts(texts, [chunk.metadata for chunk in chunks])\n\n        # Update each chunk's metadata with its corresponding concepts.\n        for chunk, concepts in zip(chunks, all_concepts, strict=False):\n            chunk.metadata[\"concepts\"] = [c.name for c in concepts]\n            chunk.metadata[\"concept_relationships\"] = {\n                c.name: {k: list(v) for k, v in c.relationships.items()}\n                for c in concepts\n            }\n\n    async def query_concepts(self, query: str) -&gt; dict[str, any]:\n        \"\"\"Query the concept graph based on natural language query\"\"\"\n\n        system_prompt = \"\"\"\n        Convert the natural language query about concepts into a structured format that specifies:\n        1. Main concepts of interest\n        2. Desired relationship types\n        3. Any category filters\n        4. Importance threshold\n\n        Format as JSON.\n        \"\"\"\n\n        prompt = f\"\"\"\n        Query: {query}\n\n        Convert to this JSON structure:\n        {{\n            \"target_concepts\": [\"concept1\", \"concept2\"],\n            \"relationship_types\": [\"type1\", \"type2\"],\n            \"categories\": [\"category1\", \"category2\"],\n            \"min_importance\": 0.0\n        }}\n        \"\"\"\n\n        try:\n            response = await litellm_complete(\n                model_name=self.kb.model_name,\n                prompt=prompt,\n                system_prompt=system_prompt,\n                response_format=TConcept\n            )\n\n            query_params = json.loads(response)\n\n            results = {\n                \"concepts\": {},\n                \"relationships\": [],\n                \"groups\": []\n            }\n\n            # Find matching concepts\n            for concept_name in query_params[\"target_concepts\"]:\n                if concept_name in self.concept_graph.concepts:\n                    concept = self.concept_graph.concepts[concept_name]\n                    if concept.importance_score &gt;= query_params[\"min_importance\"]:\n                        results[\"concepts\"][concept_name] = {\n                            \"category\": concept.category,\n                            \"importance\": concept.importance_score,\n                            \"context\": concept.context_snippets\n                        }\n\n                        # Get relationships\n                        for rel_type in query_params[\"relationship_types\"]:\n                            related = self.concept_graph.get_related_concepts(\n                                concept_name, rel_type\n                            )\n                            for related_concept in related:\n                                results[\"relationships\"].append({\n                                    \"from\": concept_name,\n                                    \"to\": related_concept,\n                                    \"type\": rel_type\n                                })\n\n            # Group concepts by category\n            category_groups = defaultdict(list)\n            for concept_name, concept_info in results[\"concepts\"].items():\n                category_groups[concept_info[\"category\"]].append(concept_name)\n            results[\"groups\"] = [\n                {\"category\": cat, \"concepts\": concepts}\n                for cat, concepts in category_groups.items()\n            ]\n\n            return results\n\n        except Exception as e:\n            print(f\"Error querying concepts: {str(e)}\")\n            return {\"concepts\": {}, \"relationships\": [], \"groups\": []}\n</code></pre> <code>extract_concepts(texts, metadatas)</code> <code>async</code> \u00b6 <p>Extract concepts from texts using concurrent processing with rate limiting. Requests are made at the specified rate while responses are processed asynchronously.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def extract_concepts(self, texts: list[str], metadatas: list[dict[str, Any]]) -&gt; list[list[Concept]]:\n    \"\"\"\n    Extract concepts from texts using concurrent processing with rate limiting.\n    Requests are made at the specified rate while responses are processed asynchronously.\n    \"\"\"\n    # Ensure metadatas list matches texts length\n    metadatas = metadatas + [{}] * (len(texts) - len(metadatas))\n\n    # Initialize rate limiter\n    rate_limiter = DynamicRateLimiter()\n\n    system_prompt = (\n        \"Analyze the given text and extract key concepts and their relationships. For each concept:\\n\"\n        \"1. Identify the concept name and category (technical, domain, method, property, ...)\\n\"\n        \"2. Determine relationships with other concepts (uses, part_of, similar_to, depends_on, ...)\\n\"\n        \"3. Assess importance (0-1 score) based on centrality to the text\\n\"\n        \"4. Extract relevant context snippets\\n\"\n        \"5. Max 5 Concepts!\\n\"\n        \"only return in json format!\\n\"\n        \"\"\"{\"concepts\": [{\n            \"name\": \"concept_name\",\n            \"category\": \"category_name\",\n            \"relationships\": {\n                \"relationship_type\": [\"related_concept1\", \"related_concept2\"]\n            },\n            \"importance_score\": 0.0,\n            \"context_snippets\": [\"relevant text snippet\"]\n        }]}\\n\"\"\"\n    )\n\n    # Prepare all requests\n    requests = [\n        (idx, f\"Text to Convert in to JSON structure:\\n{text}\", system_prompt, metadata)\n        for idx, (text, metadata) in enumerate(zip(texts, metadatas, strict=False))\n    ]\n\n    async def process_single_request(idx: int, prompt: str, system_prompt: str, metadata: dict[str, Any]):\n        \"\"\"Process a single request with rate limiting\"\"\"\n        try:\n            # Wait for rate limit\n            await rate_limiter.acquire()\n            i__[1] += 1\n            # Make API call without awaiting the response\n            response_future = litellm_complete(\n                prompt=prompt,\n                system_prompt=system_prompt,\n                response_format=Concepts,\n                model_name=self.kb.model_name,\n                fallbacks=[\"groq/gemma2-9b-it\"] +\n                          [m for m in os.getenv(\"FALLBACKS_MODELS_PREM\", '').split(',') if m]\n            )\n\n            return idx, response_future\n\n        except Exception as e:\n            print(f\"Error initiating request {idx}: {str(e)}\")\n            return idx, None\n\n    async def process_response(idx: int, response_future) -&gt; list[Concept]:\n        \"\"\"Process the response once it's ready\"\"\"\n        try:\n            if response_future is None:\n                return []\n\n            response = await response_future\n            return await self._process_response(response, metadatas[idx])\n\n        except Exception as e:\n            print(f\"Error processing response {idx}: {str(e)}\")\n            return []\n\n    # Create tasks for all requests\n    request_tasks = []\n    batch_size = self.kb.batch_size\n\n    rate_limiter.update_rate(self.requests_per_second)\n\n    for batch_start in range(0, len(requests), batch_size):\n        batch = requests[batch_start:batch_start + batch_size]\n\n        # Create tasks for the batch\n        batch_tasks = [\n            process_single_request(idx, prompt, sys_prompt, meta)\n            for idx, prompt, sys_prompt, meta in batch\n        ]\n        request_tasks.extend(batch_tasks)\n\n    # Execute all requests with rate limiting\n    request_results = await asyncio.gather(*request_tasks)\n\n    # Process responses as they complete\n    response_tasks = [\n        process_response(idx, response_future)\n        for idx, response_future in request_results\n    ]\n\n    # Gather all results\n    all_results = await asyncio.gather(*response_tasks)\n\n    # Sort results by original index\n    sorted_results = [[] for _ in texts]\n    for idx, concepts in enumerate(all_results):\n        sorted_results[idx] = concepts\n\n    return sorted_results\n</code></pre> <code>process_chunks(chunks)</code> <code>async</code> \u00b6 <p>Process all chunks in batch to extract and store concepts. Each chunk's metadata will be updated with the concept names and relationships.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def process_chunks(self, chunks: list[Chunk]) -&gt; None:\n    \"\"\"\n    Process all chunks in batch to extract and store concepts.\n    Each chunk's metadata will be updated with the concept names and relationships.\n    \"\"\"\n    # Gather all texts from the chunks.\n    texts = [chunk.text for chunk in chunks]\n    # Call extract_concepts once with all texts.\n    all_concepts = await self.extract_concepts(texts, [chunk.metadata for chunk in chunks])\n\n    # Update each chunk's metadata with its corresponding concepts.\n    for chunk, concepts in zip(chunks, all_concepts, strict=False):\n        chunk.metadata[\"concepts\"] = [c.name for c in concepts]\n        chunk.metadata[\"concept_relationships\"] = {\n            c.name: {k: list(v) for k, v in c.relationships.items()}\n            for c in concepts\n        }\n</code></pre> <code>query_concepts(query)</code> <code>async</code> \u00b6 <p>Query the concept graph based on natural language query</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def query_concepts(self, query: str) -&gt; dict[str, any]:\n    \"\"\"Query the concept graph based on natural language query\"\"\"\n\n    system_prompt = \"\"\"\n    Convert the natural language query about concepts into a structured format that specifies:\n    1. Main concepts of interest\n    2. Desired relationship types\n    3. Any category filters\n    4. Importance threshold\n\n    Format as JSON.\n    \"\"\"\n\n    prompt = f\"\"\"\n    Query: {query}\n\n    Convert to this JSON structure:\n    {{\n        \"target_concepts\": [\"concept1\", \"concept2\"],\n        \"relationship_types\": [\"type1\", \"type2\"],\n        \"categories\": [\"category1\", \"category2\"],\n        \"min_importance\": 0.0\n    }}\n    \"\"\"\n\n    try:\n        response = await litellm_complete(\n            model_name=self.kb.model_name,\n            prompt=prompt,\n            system_prompt=system_prompt,\n            response_format=TConcept\n        )\n\n        query_params = json.loads(response)\n\n        results = {\n            \"concepts\": {},\n            \"relationships\": [],\n            \"groups\": []\n        }\n\n        # Find matching concepts\n        for concept_name in query_params[\"target_concepts\"]:\n            if concept_name in self.concept_graph.concepts:\n                concept = self.concept_graph.concepts[concept_name]\n                if concept.importance_score &gt;= query_params[\"min_importance\"]:\n                    results[\"concepts\"][concept_name] = {\n                        \"category\": concept.category,\n                        \"importance\": concept.importance_score,\n                        \"context\": concept.context_snippets\n                    }\n\n                    # Get relationships\n                    for rel_type in query_params[\"relationship_types\"]:\n                        related = self.concept_graph.get_related_concepts(\n                            concept_name, rel_type\n                        )\n                        for related_concept in related:\n                            results[\"relationships\"].append({\n                                \"from\": concept_name,\n                                \"to\": related_concept,\n                                \"type\": rel_type\n                            })\n\n        # Group concepts by category\n        category_groups = defaultdict(list)\n        for concept_name, concept_info in results[\"concepts\"].items():\n            category_groups[concept_info[\"category\"]].append(concept_name)\n        results[\"groups\"] = [\n            {\"category\": cat, \"concepts\": concepts}\n            for cat, concepts in category_groups.items()\n        ]\n\n        return results\n\n    except Exception as e:\n        print(f\"Error querying concepts: {str(e)}\")\n        return {\"concepts\": {}, \"relationships\": [], \"groups\": []}\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.ConceptGraph","title":"<code>ConceptGraph</code>","text":"<p>Manages concept relationships and hierarchies</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class ConceptGraph:\n    \"\"\"Manages concept relationships and hierarchies\"\"\"\n\n    def __init__(self):\n        self.concepts: dict[str, Concept] = {}\n\n    def add_concept(self, concept: Concept):\n        \"\"\"Add or update a concept in the graph\"\"\"\n        if concept.name.lower() in self.concepts:\n            # Merge relationships and context\n            existing = self.concepts[concept.name.lower()]\n            for rel_type, related in concept.relationships.items():\n                if rel_type not in existing.relationships:\n                    existing.relationships[rel_type] = set()\n                existing.relationships[rel_type].update(related)\n            existing.context_snippets.extend(concept.context_snippets)\n            # Update importance score with rolling average\n            existing.importance_score = (existing.importance_score + concept.importance_score) / 2\n        else:\n            self.concepts[concept.name.lower()] = concept\n\n    def get_related_concepts(self, concept_name: str, relationship_type: str | None = None) -&gt; set[str]:\n        \"\"\"Get related concepts, optionally filtered by relationship type\"\"\"\n        if concept_name not in self.concepts:\n            return set()\n\n        concept = self.concepts[concept_name.lower()]\n        if relationship_type:\n            return concept.relationships.get(relationship_type, set())\n\n        related = set()\n        for relations in concept.relationships.values():\n            related.update(relations)\n        return related\n\n\n    def convert_to_networkx(self) -&gt; nx.DiGraph:\n        \"\"\"Convert ConceptGraph to NetworkX graph with layout\"\"\"\n        print(f\"Converting to NetworkX graph with {len(self.concepts.values())} concepts\")\n\n        G = nx.DiGraph()\n\n        if len(self.concepts.values()) == 0:\n            return G\n\n        for concept in self.concepts.values():\n            cks = '\\n - '.join(concept.context_snippets[:4])\n            G.add_node(\n                concept.name,\n                size=concept.importance_score * 10,\n                group=concept.category,\n                title=f\"\"\"\n                    {concept.name}\n                    Category: {concept.category}\n                    Importance: {concept.importance_score:.2f}\n                    Context: \\n - {cks}\n                    \"\"\"\n            )\n\n            for rel_type, targets in concept.relationships.items():\n                for target in targets:\n                    G.add_edge(concept.name, target, label=rel_type, title=rel_type)\n\n        return G\n</code></pre> <code>add_concept(concept)</code> \u00b6 <p>Add or update a concept in the graph</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def add_concept(self, concept: Concept):\n    \"\"\"Add or update a concept in the graph\"\"\"\n    if concept.name.lower() in self.concepts:\n        # Merge relationships and context\n        existing = self.concepts[concept.name.lower()]\n        for rel_type, related in concept.relationships.items():\n            if rel_type not in existing.relationships:\n                existing.relationships[rel_type] = set()\n            existing.relationships[rel_type].update(related)\n        existing.context_snippets.extend(concept.context_snippets)\n        # Update importance score with rolling average\n        existing.importance_score = (existing.importance_score + concept.importance_score) / 2\n    else:\n        self.concepts[concept.name.lower()] = concept\n</code></pre> <code>convert_to_networkx()</code> \u00b6 <p>Convert ConceptGraph to NetworkX graph with layout</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def convert_to_networkx(self) -&gt; nx.DiGraph:\n    \"\"\"Convert ConceptGraph to NetworkX graph with layout\"\"\"\n    print(f\"Converting to NetworkX graph with {len(self.concepts.values())} concepts\")\n\n    G = nx.DiGraph()\n\n    if len(self.concepts.values()) == 0:\n        return G\n\n    for concept in self.concepts.values():\n        cks = '\\n - '.join(concept.context_snippets[:4])\n        G.add_node(\n            concept.name,\n            size=concept.importance_score * 10,\n            group=concept.category,\n            title=f\"\"\"\n                {concept.name}\n                Category: {concept.category}\n                Importance: {concept.importance_score:.2f}\n                Context: \\n - {cks}\n                \"\"\"\n        )\n\n        for rel_type, targets in concept.relationships.items():\n            for target in targets:\n                G.add_edge(concept.name, target, label=rel_type, title=rel_type)\n\n    return G\n</code></pre> <code>get_related_concepts(concept_name, relationship_type=None)</code> \u00b6 <p>Get related concepts, optionally filtered by relationship type</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def get_related_concepts(self, concept_name: str, relationship_type: str | None = None) -&gt; set[str]:\n    \"\"\"Get related concepts, optionally filtered by relationship type\"\"\"\n    if concept_name not in self.concepts:\n        return set()\n\n    concept = self.concepts[concept_name.lower()]\n    if relationship_type:\n        return concept.relationships.get(relationship_type, set())\n\n    related = set()\n    for relations in concept.relationships.values():\n        related.update(relations)\n    return related\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.Concepts","title":"<code>Concepts</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a collection of key concepts.</p> <p>Attributes:</p> Name Type Description <code>concepts</code> <code>List[rConcept]</code> <p>A list of Concept instances, each representing an individual key concept.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class Concepts(BaseModel):\n    \"\"\"\n    Represents a collection of key concepts.\n\n    Attributes:\n        concepts (List[rConcept]): A list of Concept instances, each representing an individual key concept.\n    \"\"\"\n    concepts: list[rConcept]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.DataModel","title":"<code>DataModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The main data model that encapsulates the overall analysis.</p> <p>Attributes:</p> Name Type Description <code>main_summary</code> <code>str</code> <p>A Detailed overview summarizing the key findings and relations format MD string.</p> <code>concept_analysis</code> <code>ConceptAnalysis</code> <p>An instance containing the analysis of key concepts.</p> <code>topic_insights</code> <code>TopicInsights</code> <p>An instance containing insights regarding the topics.</p> <code>relevance_assessment</code> <code>RelevanceAssessment</code> <p>An instance assessing the relevance and alignment of the query.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class DataModel(BaseModel):\n    \"\"\"\n    The main data model that encapsulates the overall analysis.\n\n    Attributes:\n        main_summary (str): A Detailed overview summarizing the key findings and relations format MD string.\n        concept_analysis (ConceptAnalysis): An instance containing the analysis of key concepts.\n        topic_insights (TopicInsights): An instance containing insights regarding the topics.\n        relevance_assessment (RelevanceAssessment): An instance assessing the relevance and alignment of the query.\n    \"\"\"\n    main_summary: str\n    concept_analysis: ConceptAnalysis\n    topic_insights: TopicInsights\n    relevance_assessment: RelevanceAssessment\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.DynamicRateLimiter","title":"<code>DynamicRateLimiter</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class DynamicRateLimiter:\n    def __init__(self):\n        self.last_request_time = 0.0\n        self._lock = asyncio.Lock()\n\n    def update_rate(self, requests_per_second: float):\n        \"\"\"Update rate limit dynamically\"\"\"\n        self.min_interval = 1.0 / requests_per_second if requests_per_second &gt; 0 else float('inf')\n\n    async def acquire(self):\n        \"\"\"Acquire permission to make a request\"\"\"\n        async with self._lock:\n            current_time = time.time()\n            time_since_last = current_time - self.last_request_time\n            if time_since_last &lt; self.min_interval:\n                wait_time = self.min_interval - time_since_last\n                await asyncio.sleep(wait_time)\n            self.last_request_time = time.time()\n</code></pre> <code>acquire()</code> <code>async</code> \u00b6 <p>Acquire permission to make a request</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def acquire(self):\n    \"\"\"Acquire permission to make a request\"\"\"\n    async with self._lock:\n        current_time = time.time()\n        time_since_last = current_time - self.last_request_time\n        if time_since_last &lt; self.min_interval:\n            wait_time = self.min_interval - time_since_last\n            await asyncio.sleep(wait_time)\n        self.last_request_time = time.time()\n</code></pre> <code>update_rate(requests_per_second)</code> \u00b6 <p>Update rate limit dynamically</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def update_rate(self, requests_per_second: float):\n    \"\"\"Update rate limit dynamically\"\"\"\n    self.min_interval = 1.0 / requests_per_second if requests_per_second &gt; 0 else float('inf')\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.GraphVisualizer","title":"<code>GraphVisualizer</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class GraphVisualizer:\n    @staticmethod\n    def visualize(nx_graph: nx.DiGraph, output_file: str = \"concept_graph.html\", get_output=False):\n        \"\"\"Create interactive visualization using PyVis\"\"\"\n        from pyvis.network import Network\n        net = Network(\n            height=\"800px\",\n            width=\"100%\",\n            notebook=False,\n            directed=True,\n            bgcolor=\"#1a1a1a\",\n            font_color=\"white\"\n        )\n\n        net.from_nx(nx_graph)\n\n        net.save_graph(output_file)\n        print(f\"Graph saved to {output_file} Open in browser to view.\", len(nx_graph))\n        if get_output:\n            c = open(output_file, encoding=\"utf-8\").read()\n            os.remove(output_file)\n            return c\n</code></pre> <code>visualize(nx_graph, output_file='concept_graph.html', get_output=False)</code> <code>staticmethod</code> \u00b6 <p>Create interactive visualization using PyVis</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>@staticmethod\ndef visualize(nx_graph: nx.DiGraph, output_file: str = \"concept_graph.html\", get_output=False):\n    \"\"\"Create interactive visualization using PyVis\"\"\"\n    from pyvis.network import Network\n    net = Network(\n        height=\"800px\",\n        width=\"100%\",\n        notebook=False,\n        directed=True,\n        bgcolor=\"#1a1a1a\",\n        font_color=\"white\"\n    )\n\n    net.from_nx(nx_graph)\n\n    net.save_graph(output_file)\n    print(f\"Graph saved to {output_file} Open in browser to view.\", len(nx_graph))\n    if get_output:\n        c = open(output_file, encoding=\"utf-8\").read()\n        os.remove(output_file)\n        return c\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.KnowledgeBase","title":"<code>KnowledgeBase</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class KnowledgeBase:\n    def __init__(self, embedding_dim: int = 768, similarity_threshold: float = 0.61, batch_size: int = 64,\n                 n_clusters: int = 4, deduplication_threshold: float = 0.85, model_name=os.getenv(\"DEFAULTMODELSUMMERY\"),\n                 embedding_model=os.getenv(\"DEFAULTMODELEMBEDDING\"),\n                 vis_class:str | None = \"EnhancedVectorStore\",\n                 vis_kwargs:dict[str, Any] | None=None,\n                 requests_per_second=85.,\n                 chunk_size: int = 3600,\n                 chunk_overlap: int = 130,\n                 separator: str = \"\\n\"\n                 ):\n        \"\"\"Initialize the knowledge base with given parameters\"\"\"\n\n        self.existing_hashes: set[str] = set()\n        self.embedding_model = embedding_model\n        self.embedding_dim = embedding_dim\n        self.similarity_threshold = similarity_threshold\n        self.deduplication_threshold = deduplication_threshold\n        if model_name == \"openrouter/mistralai/mistral-nemo\":\n            batch_size = 9\n            requests_per_second = 1.5\n        self.batch_size = batch_size\n        self.n_clusters = n_clusters\n        self.model_name = model_name\n        self.sto: list = []\n\n        self.text_splitter = TextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap, separator=separator)\n        self.similarity_graph = {}\n        self.concept_extractor = ConceptExtractor(self, requests_per_second)\n\n        self.vis_class = None\n        self.vis_kwargs = None\n        self.vdb = None\n        self.init_vis(vis_class, vis_kwargs)\n\n    def init_vis(self, vis_class, vis_kwargs):\n        if vis_class is None:\n            vis_class = \"EnhancedVectorStore\"\n        if vis_class == \"FastVectorStoreO\":\n            if vis_kwargs is None:\n                vis_kwargs = {\n                    \"embedding_size\": self.embedding_dim\n                }\n            self.vdb = FastVectorStoreO(**vis_kwargs)\n        if vis_class == \"FaissVectorStore\":\n            if vis_kwargs is None:\n                vis_kwargs = {\n                    \"dimension\": self.embedding_dim\n                }\n            self.vdb = FaissVectorStore(**vis_kwargs)\n        if vis_class == \"EnhancedVectorStore\":\n            if vis_kwargs is None:\n                vis_kwargs = {\n                    \"dimension\": self.embedding_dim\n                }\n            vis_kwargs = VectorStoreConfig(**vis_kwargs)\n            self.vdb = EnhancedVectorStore(vis_kwargs)\n        if vis_class == \"FastVectorStore1\":\n            self.vdb = FastVectorStore1()\n        if vis_class == \"NumpyVectorStore\":\n            self.vdb = NumpyVectorStore()\n        if vis_class == \"QdrantVectorStore\":\n            if vis_kwargs is None:\n                vis_kwargs = {\n                    \"embedding_size\": self.embedding_dim,\n                    \"collection_name\" : 'default_' + str(uuid.uuid4())[:6]\n                }\n            if 'collection_name' not in vis_kwargs:\n                vis_kwargs['collection_name'] = 'default_'+str(uuid.uuid4())[:6]\n            self.vdb = QdrantVectorStore(**vis_kwargs)\n\n        self.vis_class = vis_class\n        self.vis_kwargs = vis_kwargs\n\n\n    @staticmethod\n    def compute_hash(text: str) -&gt; str:\n        \"\"\"Compute SHA-256 hash of text\"\"\"\n        return hashlib.sha256(text.encode('utf-8', errors='ignore')).hexdigest()\n\n    async def _get_embeddings(self, texts: list[str]) -&gt; np.ndarray:\n        \"\"\"Get normalized embeddings in batches\"\"\"\n        try:\n            async def process_batch(batch: list[str]) -&gt; np.ndarray:\n                from toolboxv2.mods.isaa.extras.adapter import litellm_embed\n                # print(\"Processing\", batch)\n                embeddings = await litellm_embed(texts=batch, model=self.embedding_model)\n                return normalize_vectors(embeddings)\n\n            tasks = []\n            for i in range(0, len(texts), self.batch_size):\n                batch = texts[i:i + self.batch_size]\n                tasks.append(process_batch(batch))\n\n            embeddings = await asyncio.gather(*tasks)\n            i__[0] += len(texts)\n            return np.vstack(embeddings)\n        except Exception as e:\n            get_logger().error(f\"Error generating embeddings: {str(e)}\")\n            raise\n\n\n\n    def _remove_similar_chunks(self, threshold: float = None) -&gt; int:\n        \"\"\"Remove chunks that are too similar to each other\"\"\"\n        if len(self.vdb.chunks) &lt; 2:\n            return 0\n\n        if threshold is None:\n            threshold = self.deduplication_threshold\n\n        try:\n            # Get all embeddings\n            embeddings = np.vstack([c.embedding for c in self.vdb.chunks])\n            n = len(embeddings)\n\n            # Compute similarity matrix\n            similarities = np.dot(embeddings, embeddings.T)\n\n            # Create mask for chunks to keep\n            keep_mask = np.ones(n, dtype=bool)\n\n            # Iterate through chunks\n            for i in range(n):\n                if not keep_mask[i]:\n                    continue\n\n                # Find chunks that are too similar to current chunk\n                similar_indices = similarities[i] &gt;= threshold\n                similar_indices[i] = False  # Don't count self-similarity\n\n                # Mark similar chunks for removal\n                keep_mask[similar_indices] = False\n\n            # Keep only unique chunks\n            unique_chunks = [chunk for chunk, keep in zip(self.vdb.chunks, keep_mask, strict=False) if keep]\n            removed_count = len(self.vdb.chunks) - len(unique_chunks)\n\n            # Update chunks and hashes\n            self.vdb.chunks = unique_chunks\n            self.existing_hashes = {chunk.content_hash for chunk in self.vdb.chunks}\n\n            # Rebuild index if chunks were removed\n            if removed_count &gt; 0:\n                self.vdb.rebuild_index()\n\n\n            return removed_count\n\n        except Exception as e:\n            get_logger().error(f\"Error removing similar chunks: {str(e)}\")\n            raise\n\n    async def _add_data(\n        self,\n        texts: list[str],\n        metadata: list[dict[str, Any]] | None= None,\n    ) -&gt; tuple[int, int]:\n        \"\"\"\n        Process and add new data to the knowledge base\n        Returns: Tuple of (added_count, duplicate_count)\n        \"\"\"\n        if len(texts) == 0:\n            return -1, -1\n        try:\n            # Compute hashes and filter exact duplicates\n            hashes = [self.compute_hash(text) for text in texts]\n            unique_data = []\n            for t, m, h in zip(texts, metadata, hashes, strict=False):\n                if h in self.existing_hashes:\n                    continue\n                # Update existing hashes\n                self.existing_hashes.add(h)\n                unique_data.append((t, m, h))\n\n            if not unique_data:\n                return 0, len(texts)\n\n            # Get embeddings\n            embeddings = await self._get_embeddings(texts)\n\n            texts = []\n            metadata = []\n            hashes = []\n            embeddings_final = []\n            if len(self.vdb.chunks):\n                for i, d in enumerate(unique_data):\n                    c = self.vdb.search(embeddings[i], 5, self.deduplication_threshold)\n                    if len(c) &gt; 2:\n                        continue\n                    t, m, h = d\n                    texts.append(t)\n                    metadata.append(m)\n                    hashes.append(h)\n                    embeddings_final.append(embeddings[i])\n\n            else:\n                texts , metadata, hashes = zip(*unique_data, strict=False)\n                embeddings_final = embeddings\n\n            if not texts:  # All were similar to existing chunks\n                return 0, len(unique_data)\n\n            # Create and add new chunks\n            new_chunks = [\n                Chunk(text=t, embedding=e, metadata=m, content_hash=h)\n                for t, e, m, h in zip(texts, embeddings_final, metadata, hashes, strict=False)\n            ]\n\n            # Add new chunks\n            # Update index\n            if new_chunks:\n                all_embeddings = np.vstack([c.embedding for c in new_chunks])\n                self.vdb.add_embeddings(all_embeddings, new_chunks)\n\n            # Remove similar chunks from the entire collection\n            removed = self._remove_similar_chunks()\n            get_logger().info(f\"Removed {removed} similar chunks during deduplication\")\n            # Invalidate visualization cache\n\n            if len(new_chunks) - removed &gt; 0:\n                # Process new chunks for concepts\n                await self.concept_extractor.process_chunks(new_chunks)\n            print(\"[total, calls, errors]\", i__)\n\n            return len(new_chunks) - removed, len(texts) - len(new_chunks) + removed\n\n        except Exception as e:\n            get_logger().error(f\"Error adding data: {str(e)}\")\n            raise\n\n\n    async def add_data(\n        self,\n        texts: list[str],\n        metadata: list[dict[str, Any]] | None = None,\n    ) -&gt; tuple[int, int]:\n        \"\"\"Enhanced version with smart splitting and clustering\"\"\"\n        if isinstance(texts, str):\n            texts = [texts]\n        if metadata is None:\n            metadata = [{}] * len(texts)\n        if isinstance(metadata, dict):\n            metadata = [metadata]\n        if len(texts) != len(metadata):\n            raise ValueError(\"Length of texts and metadata must match\")\n        if len(texts) == 1 and len(texts[0]) &lt; 10_000:\n            if len(self.sto) &lt; self.batch_size and len(texts) == 1:\n                self.sto.append((texts[0], metadata[0]))\n                return -1, -1\n            if len(self.sto) &gt;= self.batch_size:\n                _ = [texts.append(t) or metadata.append([m]) for (t, m) in self.sto]\n                self.sto = []\n\n        # Split large texts\n        split_texts = []\n        split_metadata = []\n\n        while Spinner(\"Saving Data to Memory\", symbols='t'):\n\n            for idx, text in enumerate(texts):\n                chunks = self.text_splitter.split_text(text)\n                split_texts.extend(chunks)\n\n                # Adjust metadata for splits\n                meta = metadata[idx] if metadata else {}\n                if isinstance(meta, list):\n                    meta = meta[0]\n                for i, _chunk in enumerate(chunks):\n                    chunk_meta = meta.copy()\n                    chunk_meta.update({\n                        'chunk_index': i,\n                        'total_chunks': len(chunks),\n                        'original_text_id': idx\n                    })\n                    split_metadata.append(chunk_meta)\n\n            return await self._add_data(split_texts, split_metadata)\n\n    def _update_similarity_graph(self, embeddings: np.ndarray, chunk_ids: list[int]):\n        \"\"\"Update similarity graph for connected information detection\"\"\"\n        similarities = np.dot(embeddings, embeddings.T)\n\n        for i in range(len(chunk_ids)):\n            for j in range(i + 1, len(chunk_ids)):\n                if similarities[i, j] &gt;= self.similarity_threshold:\n                    id1, id2 = chunk_ids[i], chunk_ids[j]\n                    if id1 not in self.similarity_graph:\n                        self.similarity_graph[id1] = set()\n                    if id2 not in self.similarity_graph:\n                        self.similarity_graph[id2] = set()\n                    self.similarity_graph[id1].add(id2)\n                    self.similarity_graph[id2].add(id1)\n\n    async def retrieve(\n        self,\n        query: str=\"\",\n        query_embedding: np.ndarray | None = None,\n        k: int = 5,\n        min_similarity: float = 0.2,\n        include_connected: bool = True\n    ) -&gt; list[Chunk]:\n        \"\"\"Enhanced retrieval with connected information\"\"\"\n        if query_embedding is None:\n            query_embedding = (await self._get_embeddings([query]))[0]\n        k = min(k, len(self.vdb.chunks)-1)\n        if k &lt;= 0:\n            return []\n        initial_results = self.vdb.search(query_embedding, k, min_similarity)\n\n        if not include_connected or not initial_results:\n            return initial_results\n\n        # Find connected chunks\n        connected_chunks = set()\n        for chunk in initial_results:\n            chunk_id = self.vdb.chunks.index(chunk)\n            if chunk_id in self.similarity_graph:\n                connected_chunks.update(self.similarity_graph[chunk_id])\n\n        # Add connected chunks to results\n        all_chunks = self.vdb.chunks\n        additional_results = [all_chunks[i] for i in connected_chunks\n                              if all_chunks[i] not in initial_results]\n\n        # Sort by similarity to query\n        all_results = initial_results + additional_results\n\n        return sorted(\n            all_results,\n            key=lambda x: np.dot(x.embedding, query_embedding),\n            reverse=True\n        )[:k * 2]  # Return more results when including connected information\n\n    async def forget_irrelevant(self, irrelevant_concepts: list[str], similarity_threshold: float | None=None) -&gt; int:\n        \"\"\"\n        Remove chunks similar to irrelevant concepts\n        Returns: Number of chunks removed\n        \"\"\"\n        if not irrelevant_concepts:\n            return 0\n\n        if similarity_threshold is None:\n            similarity_threshold = self.similarity_threshold\n\n        try:\n            irrelevant_embeddings = await self._get_embeddings(irrelevant_concepts)\n            initial_count = len(self.vdb.chunks)\n\n            def is_relevant(chunk: Chunk) -&gt; bool:\n                similarities = np.dot(chunk.embedding, irrelevant_embeddings.T)\n                do_keep = np.max(similarities) &lt; similarity_threshold\n                if do_keep:\n                    return True\n                for c in chunk.metadata.get(\"concepts\", []):\n                    if c in self.concept_extractor.concept_graph.concepts:\n                        del self.concept_extractor.concept_graph.concepts[c]\n                return False\n\n            relevant_chunks = [chunk for chunk in self.vdb.chunks if is_relevant(chunk)]\n            self.vdb.chunks = relevant_chunks\n            self.existing_hashes = {chunk.content_hash for chunk in self.vdb.chunks}\n            self.vdb.rebuild_index()\n\n\n            return initial_count - len(self.vdb.chunks)\n\n        except Exception as e:\n            get_logger().error(f\"Error forgetting irrelevant concepts: {str(e)}\")\n            raise\n\n    ## ----------------------------------------------------------------\n\n    def _cluster_chunks(\n        self,\n        chunks: list[Chunk],\n        query_embedding: np.ndarray | None = None,\n        min_cluster_size: int = 2,\n        min_samples: int = 1,\n        max_clusters: int = 10\n    ) -&gt; dict[int, list[Chunk]]:\n        \"\"\"\n        Enhanced clustering of chunks into topics with query awareness\n        and dynamic parameter adjustment\n        \"\"\"\n        if len(chunks) &lt; 2:\n            return {0: chunks}\n\n        embeddings = np.vstack([chunk.embedding for chunk in chunks])\n\n        # Normalize embeddings for cosine similarity\n        embeddings = normalize_vectors(embeddings)\n\n        # If query is provided, weight embeddings by query relevance\n        if query_embedding is not None:\n            query_similarities = np.dot(embeddings, query_embedding)\n            # Apply soft weighting to maintain structure while considering query relevance\n            embeddings = embeddings * query_similarities[:, np.newaxis]\n            embeddings = normalize_vectors(embeddings)\n\n        # Dynamic parameter adjustment based on dataset size\n        adjusted_min_cluster_size = max(\n            min_cluster_size,\n            min(len(chunks) // 10, 5)  # Scale with data size, max 5\n        )\n\n        adjusted_min_samples = max(\n            min_samples,\n            adjusted_min_cluster_size // 2\n        )\n\n        # Try different parameter combinations for optimal clustering\n        best_clusters = None\n        best_score = float('-inf')\n\n        epsilon_range = [0.2, 0.3, 0.4]\n\n        for epsilon in epsilon_range:\n            clusterer = HDBSCAN(\n                min_cluster_size=adjusted_min_cluster_size,\n                min_samples=adjusted_min_samples,\n                metric='cosine',\n                cluster_selection_epsilon=epsilon\n            )\n\n            cluster_labels = clusterer.fit_predict(embeddings)\n\n            # Skip if all points are noise\n            if len(set(cluster_labels)) &lt;= 1:\n                continue\n\n            # Calculate clustering quality metrics\n            score = self._evaluate_clustering(\n                embeddings,\n                cluster_labels,\n                query_embedding\n            )\n\n            if score &gt; best_score:\n                best_score = score\n                best_clusters = cluster_labels\n\n        # If no good clustering found, fall back to simpler approach\n        if best_clusters is None:\n            return self._fallback_clustering(chunks, query_embedding)\n\n        # Organize chunks by cluster\n        clusters: dict[int, list[Chunk]] = {}\n\n        # Sort clusters by size and relevance\n        cluster_scores = []\n\n        for label in set(best_clusters):\n            if label == -1:  # Handle noise points separately\n                continue\n\n            # Fixed: Use boolean mask to select chunks for current cluster\n            cluster_mask = best_clusters == label\n            cluster_chunks = [chunk for chunk, is_in_cluster in zip(chunks, cluster_mask, strict=False) if is_in_cluster]\n\n            # Skip empty clusters\n            if not cluster_chunks:\n                continue\n\n            # Calculate cluster score based on size and query relevance\n            score = len(cluster_chunks)\n            if query_embedding is not None:\n                cluster_embeddings = np.vstack([c.embedding for c in cluster_chunks])\n                query_relevance = np.mean(np.dot(cluster_embeddings, query_embedding))\n                score = score * (1 + query_relevance)  # Boost by relevance\n\n            cluster_scores.append((label, score, cluster_chunks))\n\n        # Sort clusters by score and limit to max_clusters\n        cluster_scores.sort(key=lambda x: x[1], reverse=True)\n\n        # Assign cleaned clusters\n        for i, (_, _, cluster_chunks) in enumerate(cluster_scores[:max_clusters]):\n            clusters[i] = cluster_chunks\n\n        # Handle noise points by assigning to nearest cluster\n        noise_chunks = [chunk for chunk, label in zip(chunks, best_clusters, strict=False) if label == -1]\n        if noise_chunks:\n            self._assign_noise_points(noise_chunks, clusters, query_embedding)\n\n        return clusters\n\n    @staticmethod\n    def _evaluate_clustering(\n        embeddings: np.ndarray,\n        labels: np.ndarray,\n        query_embedding: np.ndarray | None = None\n    ) -&gt; float:\n        \"\"\"\n        Evaluate clustering quality using multiple metrics\n        \"\"\"\n        if len(set(labels)) &lt;= 1:\n            return float('-inf')\n\n        # Calculate silhouette score for cluster cohesion\n        from sklearn.metrics import silhouette_score\n        try:\n            sil_score = silhouette_score(embeddings, labels, metric='cosine')\n        except:\n            sil_score = -1\n\n        # Calculate Davies-Bouldin score for cluster separation\n        from sklearn.metrics import davies_bouldin_score\n        try:\n            db_score = -davies_bouldin_score(embeddings, labels)  # Negated as lower is better\n        except:\n            db_score = -1\n\n        # Calculate query relevance if provided\n        query_score = 0\n        if query_embedding is not None:\n            unique_labels = set(labels) - {-1}\n            if unique_labels:\n                query_sims = []\n                for label in unique_labels:\n                    cluster_mask = labels == label\n                    cluster_embeddings = embeddings[cluster_mask]\n                    cluster_centroid = np.mean(cluster_embeddings, axis=0)\n                    query_sims.append(np.dot(cluster_centroid, query_embedding))\n                query_score = np.mean(query_sims)\n\n        # Combine scores with weights\n        combined_score = (\n            0.4 * sil_score +\n            0.3 * db_score +\n            0.3 * query_score\n        )\n\n        return combined_score\n\n    @staticmethod\n    def _fallback_clustering(\n        chunks: list[Chunk],\n        query_embedding: np.ndarray | None = None\n    ) -&gt; dict[int, list[Chunk]]:\n        \"\"\"\n        Simple fallback clustering when HDBSCAN fails\n        \"\"\"\n        if query_embedding is not None:\n            # Sort by query relevance\n            chunks_with_scores = [\n                (chunk, np.dot(chunk.embedding, query_embedding))\n                for chunk in chunks\n            ]\n            chunks_with_scores.sort(key=lambda x: x[1], reverse=True)\n            chunks = [c for c, _ in chunks_with_scores]\n\n        # Create fixed-size clusters\n        clusters = {}\n        cluster_size = max(2, len(chunks) // 5)\n\n        for i in range(0, len(chunks), cluster_size):\n            clusters[len(clusters)] = chunks[i:i + cluster_size]\n\n        return clusters\n\n    @staticmethod\n    def _assign_noise_points(\n        noise_chunks: list[Chunk],\n        clusters: dict[int, list[Chunk]],\n        query_embedding: np.ndarray | None = None\n    ) -&gt; None:\n        \"\"\"\n        Assign noise points to nearest clusters\n        \"\"\"\n        if not clusters:\n            clusters[0] = noise_chunks\n            return\n\n        for chunk in noise_chunks:\n            best_cluster = None\n            best_similarity = float('-inf')\n\n            for cluster_id, cluster_chunks in clusters.items():\n                cluster_embeddings = np.vstack([c.embedding for c in cluster_chunks])\n                cluster_centroid = np.mean(cluster_embeddings, axis=0)\n\n                similarity = np.dot(chunk.embedding, cluster_centroid)\n\n                # Consider query relevance in assignment if available\n                if query_embedding is not None:\n                    query_sim = np.dot(chunk.embedding, query_embedding)\n                    similarity = 0.7 * similarity + 0.3 * query_sim\n\n                if similarity &gt; best_similarity:\n                    best_similarity = similarity\n                    best_cluster = cluster_id\n\n            if best_cluster is not None:\n                clusters[best_cluster].append(chunk)\n\n    @staticmethod\n    def _generate_topic_summary(\n        chunks: list[Chunk],\n        query_embedding: np.ndarray,\n        max_sentences=3\n    ) -&gt; str:\n        \"\"\"Generate a summary for a topic using most representative chunks\"\"\"\n        if not chunks:\n            return \"\"\n\n        # Find chunks most similar to cluster centroid\n        embeddings = np.vstack([chunk.embedding for chunk in chunks])\n        centroid = embeddings.mean(axis=0)\n\n        # Calculate similarities to both centroid and query\n        centroid_sims = np.dot(embeddings, centroid)\n        query_sims = np.dot(embeddings, query_embedding)\n\n        # Combine both similarities\n        combined_sims = 0.7 * centroid_sims + 0.3 * query_sims\n\n        # Select top sentences from most representative chunks\n        top_indices = np.argsort(combined_sims)[-max_sentences:]\n        summary_chunks = [chunks[i] for i in top_indices]\n\n        # Extract key sentences\n        sentences = []\n        for chunk in summary_chunks:\n            sentences.extend(sent.strip() for sent in chunk.text.split('.') if sent.strip())\n\n        return '. '.join(sentences[:max_sentences]) + '.'\n\n    async def retrieve_with_overview(\n        self,\n        query: str,\n        query_embedding=None,\n        k: int = 5,\n        min_similarity: float = 0.2,\n        max_sentences: int = 5,\n        cross_ref_depth: int = 2,\n        max_cross_refs: int = 10  # New parameter to control cross-reference count\n    ) -&gt; RetrievalResult:\n        \"\"\"Enhanced retrieval with better cross-reference handling\"\"\"\n        # Get initial results with query embedding\n        if query_embedding is None:\n            query_embedding = (await self._get_embeddings([query]))[0]\n        initial_results = await self.retrieve(query_embedding=query_embedding, k=k, min_similarity=min_similarity)\n\n        if not initial_results:\n            return RetrievalResult([], [], {})\n\n        # Find cross-references with similarity scoring\n        initial_ids = {self.vdb.chunks.index(chunk) for chunk in initial_results}\n        related_ids = self._find_cross_references(\n            initial_ids,\n            depth=cross_ref_depth,\n            query_embedding=query_embedding  # Pass query embedding for relevance scoring\n        )\n\n        # Get all relevant chunks with smarter filtering\n        all_chunks = self.vdb.chunks\n        all_relevant_chunks = initial_results + [\n            chunk for i, chunk in enumerate(all_chunks)\n            if i in related_ids and self._is_relevant_cross_ref(\n                chunk,\n                query_embedding,\n                initial_results\n            )\n        ]\n\n        # Enhanced clustering with dynamic cluster size\n        clusters = self._cluster_chunks(\n            all_relevant_chunks,\n            query_embedding=query_embedding\n        )\n\n        # Fallback: If no clusters are found, treat all relevant chunks as a single cluster.\n        if not clusters:\n            print(\"No clusters found. Falling back to using all relevant chunks as a single cluster.\")\n            clusters = {0: all_relevant_chunks}\n\n        # Generate summaries and organize results\n        overview = []\n        cross_references = {}\n\n        for cluster_id, cluster_chunks in clusters.items():\n            summary = self._generate_topic_summary(\n                cluster_chunks,\n                query_embedding,\n                max_sentences=max_sentences  # Increased for more context\n            )\n\n            # Enhanced chunk sorting with combined scoring\n            sorted_chunks = self._sort_chunks_by_relevance(\n                cluster_chunks,\n                query_embedding,\n                initial_results\n            )\n\n            # Separate direct matches and cross-references\n            direct_matches_ = [{'text':c.text, 'metadata':c.metadata} for c in sorted_chunks if c in initial_results]\n            direct_matches = []\n            for match in direct_matches_:\n                if match in direct_matches:\n                    continue\n                direct_matches.append(match)\n            cross_refs_ = [c for c in sorted_chunks if c not in initial_results]\n            cross_refs = []\n            for match in cross_refs_:\n                if match in cross_refs:\n                    continue\n                cross_refs.append(match)\n            # Limit cross-references while maintaining diversity\n            selected_cross_refs = self._select_diverse_cross_refs(\n                cross_refs,\n                max_cross_refs,\n                query_embedding\n            )\n\n            topic_info = {\n                'topic_id': cluster_id,\n                'summary': summary,\n                'main_chunks': [x for x in direct_matches[:3]],\n                'chunk_count': len(cluster_chunks),\n                'relevance_score': self._calculate_topic_relevance(\n                    cluster_chunks,\n                    query_embedding\n                )\n            }\n            overview.append(topic_info)\n\n            if selected_cross_refs:\n                cross_references[f\"topic_{cluster_id}\"] = selected_cross_refs\n\n        # Sort overview by relevance score\n        overview.sort(key=lambda x: x['relevance_score'], reverse=True)\n\n        return RetrievalResult(\n            overview=overview,\n            details=initial_results,\n            cross_references=cross_references\n        )\n\n    def _find_cross_references(\n        self,\n        chunk_ids: set[int],\n        depth: int,\n        query_embedding: np.ndarray\n    ) -&gt; set[int]:\n        \"\"\"Enhanced cross-reference finding with relevance scoring\"\"\"\n        related_ids = set(chunk_ids)\n        current_depth = 0\n        frontier = set(chunk_ids)\n\n        while current_depth &lt; depth and frontier:\n            new_frontier = set()\n            for chunk_id in frontier:\n                if chunk_id in self.similarity_graph:\n                    # Score potential cross-references by relevance\n                    candidates = self.similarity_graph[chunk_id] - related_ids\n                    scored_candidates = [\n                        (cid, self._calculate_topic_relevance(\n                            [self.vdb.chunks[cid]],\n                            query_embedding\n                        ))\n                        for cid in candidates\n                    ]\n\n                    # Filter by relevance threshold\n                    relevant_candidates = {\n                        cid for cid, score in scored_candidates\n                        if score &gt; 0.5  # Adjustable threshold\n                    }\n                    new_frontier.update(relevant_candidates)\n\n            related_ids.update(new_frontier)\n            frontier = new_frontier\n            current_depth += 1\n\n        return related_ids\n\n    @staticmethod\n    def _is_relevant_cross_ref(\n        chunk: Chunk,\n        query_embedding: np.ndarray,\n        initial_results: list[Chunk]\n    ) -&gt; bool:\n        \"\"\"Determine if a cross-reference is relevant enough to include\"\"\"\n        # Calculate similarity to query\n        query_similarity = np.dot(chunk.embedding, query_embedding)\n\n        # Calculate similarity to initial results\n        initial_similarities = [\n            np.dot(chunk.embedding, r.embedding) for r in initial_results\n        ]\n        max_initial_similarity = max(initial_similarities)\n\n        # Combined relevance score\n        relevance_score = 0.7 * query_similarity + 0.3 * max_initial_similarity\n\n        return relevance_score &gt; 0.6  # Adjustable threshold\n\n    @staticmethod\n    def _select_diverse_cross_refs(\n        cross_refs: list[Chunk],\n        max_count: int,\n        query_embedding: np.ndarray\n    ) -&gt; list[Chunk]:\n        \"\"\"Select diverse and relevant cross-references\"\"\"\n        if not cross_refs or len(cross_refs) &lt;= max_count:\n            return cross_refs\n\n        # Calculate diversity scores\n        embeddings = np.vstack([c.embedding for c in cross_refs])\n        similarities = np.dot(embeddings, embeddings.T)\n\n        selected = []\n        remaining = list(enumerate(cross_refs))\n\n        while len(selected) &lt; max_count and remaining:\n            # Score remaining chunks by relevance and diversity\n            scores = []\n            for idx, chunk in remaining:\n                relevance = np.dot(chunk.embedding, query_embedding)\n                diversity = 1.0\n                if selected:\n                    # Calculate diversity penalty based on similarity to selected chunks\n                    selected_similarities = [\n                        similarities[idx][list(cross_refs).index(s)]\n                        for s in selected\n                    ]\n                    diversity = 1.0 - max(selected_similarities)\n\n                combined_score = 0.7 * relevance + 0.3 * diversity\n                scores.append((combined_score, idx, chunk))\n\n            # Select the highest scoring chunk\n            scores.sort(reverse=True)\n            _, idx, chunk = scores[0]\n            selected.append(chunk)\n            remaining = [(i, c) for i, c in remaining if i != idx]\n\n        return selected\n\n    @staticmethod\n    def _calculate_topic_relevance(\n        chunks: list[Chunk],\n        query_embedding: np.ndarray,\n    ) -&gt; float:\n        \"\"\"Calculate overall topic relevance score\"\"\"\n        if not chunks:\n            return 0.0\n\n        similarities = [\n            np.dot(chunk.embedding, query_embedding) for chunk in chunks\n        ]\n        return np.mean(similarities)\n\n    @staticmethod\n    def _sort_chunks_by_relevance(\n        chunks: list[Chunk],\n        query_embedding: np.ndarray,\n        initial_results: list[Chunk]\n    ) -&gt; list[Chunk]:\n        \"\"\"Sort chunks by combined relevance score\"\"\"\n        scored_chunks = []\n        for chunk in chunks:\n            query_similarity = np.dot(chunk.embedding, query_embedding)\n            initial_similarities = [\n                np.dot(chunk.embedding, r.embedding)\n                for r in initial_results\n            ]\n            max_initial_similarity = max(initial_similarities) if initial_similarities else 0\n\n            # Combined score favoring query relevance\n            combined_score = 0.7 * query_similarity + 0.3 * max_initial_similarity\n            scored_chunks.append((combined_score, chunk))\n\n        scored_chunks.sort(reverse=True)\n        return [chunk for _, chunk in scored_chunks]\n\n    async def query_concepts(self, query: str) -&gt; dict[str, any]:\n        \"\"\"Query concepts extracted from the knowledge base\"\"\"\n        return await self.concept_extractor.query_concepts(query)\n\n    async def unified_retrieve(\n        self,\n        query: str,\n        k: int = 5,\n        min_similarity: float = 0.2,\n        cross_ref_depth: int = 2,\n        max_cross_refs: int = 10,\n        max_sentences: int = 10\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Unified retrieval function that combines concept querying, retrieval with overview,\n        and basic retrieval, then generates a comprehensive summary using LLM.\n\n        Args:\n            query: Search query string\n            k: Number of primary results to retrieve\n            min_similarity: Minimum similarity threshold for retrieval\n            cross_ref_depth: Depth for cross-reference search\n            max_cross_refs: Maximum number of cross-references per topic\n            max_sentences: Maximum number Sentences in the main summary text\n\n        Returns:\n            Dictionary containing comprehensive results including summary and details\n        \"\"\"\n        # Get concept information\n        concept_results = await self.concept_extractor.query_concepts(query)\n\n        # Get retrieval overview\n\n        query_embedding = (await self._get_embeddings([query]))[0]\n        overview_results = await self.retrieve_with_overview(\n            query=query,\n            query_embedding=query_embedding,\n            k=k,\n            min_similarity=min_similarity,\n            cross_ref_depth=cross_ref_depth,\n            max_cross_refs=max_cross_refs,\n            max_sentences=max_sentences\n        )\n\n        # Get basic retrieval results\n        basic_results = await self.retrieve(\n            query_embedding=query_embedding,\n            k=k,\n            min_similarity=min_similarity\n        )\n        if len(basic_results) == 0:\n            return {}\n        if len(basic_results) == 1 and isinstance(basic_results[0], str) and basic_results[0].endswith('[]\\n - []\\n - []'):\n            return {}\n\n        # Prepare context for LLM summary\n        context = {\n            \"concepts\": {\n                \"main_concepts\": concept_results.get(\"concepts\", {}),\n                \"relationships\": concept_results.get(\"relationships\", []),\n                \"concept_groups\": concept_results.get(\"groups\", [])\n            },\n            \"topics\": [\n                {\n                    \"id\": topic[\"topic_id\"],\n                    \"summary\": topic[\"summary\"],\n                    \"relevance\": topic[\"relevance_score\"],\n                    \"chunk_count\": topic[\"chunk_count\"]\n                }\n                for topic in overview_results.overview\n            ],\n            \"key_chunks\": [\n                {\n                    \"text\": chunk.text,\n                    \"metadata\": chunk.metadata\n                }\n                for chunk in basic_results\n            ]\n        }\n\n        # Generate comprehensive summary using LLM\n        system_prompt = \"\"\"\n        Analyze the provided search results and generate a comprehensive summary\n        that includes:\n        1. Main concepts and their relationships\n        2. Key topics and their relevance\n        3. Most important findings and insights\n        4. Cross-references and connections between topics\n        5. Potential gaps or areas for further investigation\n\n        Format the response as a JSON object with these sections.\n        \"\"\"\n\n        prompt = f\"\"\"\n        Query: {query}\n\n        Context:\n        {json.dumps(context, indent=2)}\n\n        Generate a comprehensive analysis and summary following the structure:\n        \"\"\"\n\n        try:\n            await asyncio.sleep(0.25)\n            llm_response = await litellm_complete(\n                model_name=self.model_name,\n                prompt=prompt,\n                system_prompt=system_prompt,\n                response_format=DataModel,\n            )\n            summary_analysis = json.loads(llm_response)\n        except Exception as e:\n            get_logger().error(f\"Error generating summary: {str(e)}\")\n            summary_analysis = {\n                \"main_summary\": \"Error generating summary\",\n                \"error\": str(e)\n            }\n\n        # Compile final results\n        return {\n            \"summary\": summary_analysis,\n            \"raw_results\": {\n                \"concepts\": concept_results,\n                \"overview\": {\n                    \"topics\": overview_results.overview,\n                    \"cross_references\": overview_results.cross_references\n                },\n                \"relevant_chunks\": [\n                    {\n                        \"text\": chunk.text,\n                        \"metadata\": chunk.metadata,\n                        \"cluster_id\": chunk.cluster_id\n                    }\n                    for chunk in basic_results\n                ]\n            },\n            \"metadata\": {\n                \"query\": query,\n                \"timestamp\": time.time(),\n                \"retrieval_params\": {\n                    \"k\": k,\n                    \"min_similarity\": min_similarity,\n                    \"cross_ref_depth\": cross_ref_depth,\n                    \"max_cross_refs\": max_cross_refs\n                }\n            }\n        }\n\n    def save(self, path: str) -&gt; bytes | None:\n        \"\"\"\n        Save the complete knowledge base to disk, including all sub-components\n\n        Args:\n            path (str): Path where the knowledge base will be saved\n        \"\"\"\n        try:\n            data = {\n                # Core components\n                'vdb': self.vdb.save(),\n                'vis_kwargs': self.vis_kwargs,\n                'vis_class': self.vis_class,\n                'existing_hashes': self.existing_hashes,\n\n                # Configuration parameters\n                'embedding_dim': self.embedding_dim,\n                'similarity_threshold': self.similarity_threshold,\n                'batch_size': self.batch_size,\n                'n_clusters': self.n_clusters,\n                'deduplication_threshold': self.deduplication_threshold,\n                'model_name': self.model_name,\n                'embedding_model': self.embedding_model,\n\n                # Cache and graph data\n                'similarity_graph': self.similarity_graph,\n                'sto': self.sto,\n\n                # Text splitter configuration\n                'text_splitter_config': {\n                    'chunk_size': self.text_splitter.chunk_size,\n                    'chunk_overlap': self.text_splitter.chunk_overlap,\n                    'separator': self.text_splitter.separator\n                },\n\n                # Concept extractor data\n                'concept_graph': {\n                    'concepts': {\n                        name: {\n                            'name': concept.name,\n                            'category': concept.category,\n                            'relationships': {k: list(v) for k, v in concept.relationships.items()},\n                            'importance_score': concept.importance_score,\n                            'context_snippets': concept.context_snippets,\n                            'metadata': concept.metadata\n                        }\n                        for name, concept in self.concept_extractor.concept_graph.concepts.items()\n                    }\n                }\n            }\n            if path is None:\n                return pickle.dumps(data)\n            # Save to disk using pickle\n            with open(path, 'wb') as f:\n                pickle.dump(data, f)\n            print(f\"Knowledge base successfully saved to {path} with {len(self.concept_extractor.concept_graph.concepts.items())} concepts\")\n\n        except Exception as e:\n            print(f\"Error saving knowledge base: {str(e)}\")\n            raise\n    def init_vdb(self, db:AbstractVectorStore=AbstractVectorStore):\n        pass\n    @classmethod\n    def load(cls, path: str | bytes) -&gt; 'KnowledgeBase':\n        \"\"\"\n        Load a complete knowledge base from disk, including all sub-components\n\n        Args:\n            path (str): Path from where to load the knowledge base\n\n        Returns:\n            KnowledgeBase: A fully restored knowledge base instance\n        \"\"\"\n        try:\n            if isinstance(path, str):\n                # Load data from disk\n                with open(path, 'rb') as f:\n                    data = pickle.load(f)\n            elif isinstance(path, bytes):\n                data = pickle.loads(path)\n            else:\n                raise ValueError(\"Invalid path type\")\n\n            # Create new knowledge base instance with saved configuration\n            kb = cls(\n                embedding_dim=data['embedding_dim'],\n                similarity_threshold=data['similarity_threshold'],\n                batch_size=data['batch_size'],\n                n_clusters=data['n_clusters'],\n                deduplication_threshold=data['deduplication_threshold'],\n                model_name=data['model_name'],\n                embedding_model=data['embedding_model']\n            )\n\n            # Restore core components\n            kb.init_vis(data.get('vis_class'), data.get('vis_kwargs'))\n            kb.existing_hashes = data['existing_hashes']\n\n            # Restore cache and graph data\n            kb.similarity_graph = data.get('similarity_graph', {})\n            kb.sto = data.get('sto', [])\n\n            # Restore text splitter configuration\n            splitter_config = data.get('text_splitter_config', {})\n            kb.text_splitter = TextSplitter(\n                chunk_size=splitter_config.get('chunk_size', 12_000),\n                chunk_overlap=splitter_config.get('chunk_overlap', 200),\n                separator=splitter_config.get('separator', '\\n')\n            )\n\n            # Restore concept graph\n            concept_data = data.get('concept_graph', {}).get('concepts', {})\n            for concept_info in concept_data.values():\n                concept = Concept(\n                    name=concept_info['name'],\n                    category=concept_info['category'],\n                    relationships={k: set(v) for k, v in concept_info['relationships'].items()},\n                    importance_score=concept_info['importance_score'],\n                    context_snippets=concept_info['context_snippets'],\n                    metadata=concept_info['metadata']\n                )\n                kb.concept_extractor.concept_graph.add_concept(concept)\n\n            print(f\"Knowledge base successfully loaded from {path} with {len(concept_data)} concepts\")\n            return kb\n\n        except Exception as e:\n            print(f\"Error loading knowledge base: {str(e)}\")\n            raise\n\n    def vis(self,output_file: str = \"concept_graph.html\", get_output_html=False, get_output_net=False):\n        if not self.concept_extractor.concept_graph.concepts:\n            print(\"NO Concepts defined\")\n            return None\n        net = self.concept_extractor.concept_graph.convert_to_networkx()\n        if get_output_net:\n            return net\n        return GraphVisualizer.visualize(net, output_file=output_file, get_output=get_output_html)\n</code></pre> <code>__init__(embedding_dim=768, similarity_threshold=0.61, batch_size=64, n_clusters=4, deduplication_threshold=0.85, model_name=os.getenv('DEFAULTMODELSUMMERY'), embedding_model=os.getenv('DEFAULTMODELEMBEDDING'), vis_class='EnhancedVectorStore', vis_kwargs=None, requests_per_second=85.0, chunk_size=3600, chunk_overlap=130, separator='\\n')</code> \u00b6 <p>Initialize the knowledge base with given parameters</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def __init__(self, embedding_dim: int = 768, similarity_threshold: float = 0.61, batch_size: int = 64,\n             n_clusters: int = 4, deduplication_threshold: float = 0.85, model_name=os.getenv(\"DEFAULTMODELSUMMERY\"),\n             embedding_model=os.getenv(\"DEFAULTMODELEMBEDDING\"),\n             vis_class:str | None = \"EnhancedVectorStore\",\n             vis_kwargs:dict[str, Any] | None=None,\n             requests_per_second=85.,\n             chunk_size: int = 3600,\n             chunk_overlap: int = 130,\n             separator: str = \"\\n\"\n             ):\n    \"\"\"Initialize the knowledge base with given parameters\"\"\"\n\n    self.existing_hashes: set[str] = set()\n    self.embedding_model = embedding_model\n    self.embedding_dim = embedding_dim\n    self.similarity_threshold = similarity_threshold\n    self.deduplication_threshold = deduplication_threshold\n    if model_name == \"openrouter/mistralai/mistral-nemo\":\n        batch_size = 9\n        requests_per_second = 1.5\n    self.batch_size = batch_size\n    self.n_clusters = n_clusters\n    self.model_name = model_name\n    self.sto: list = []\n\n    self.text_splitter = TextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap, separator=separator)\n    self.similarity_graph = {}\n    self.concept_extractor = ConceptExtractor(self, requests_per_second)\n\n    self.vis_class = None\n    self.vis_kwargs = None\n    self.vdb = None\n    self.init_vis(vis_class, vis_kwargs)\n</code></pre> <code>add_data(texts, metadata=None)</code> <code>async</code> \u00b6 <p>Enhanced version with smart splitting and clustering</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def add_data(\n    self,\n    texts: list[str],\n    metadata: list[dict[str, Any]] | None = None,\n) -&gt; tuple[int, int]:\n    \"\"\"Enhanced version with smart splitting and clustering\"\"\"\n    if isinstance(texts, str):\n        texts = [texts]\n    if metadata is None:\n        metadata = [{}] * len(texts)\n    if isinstance(metadata, dict):\n        metadata = [metadata]\n    if len(texts) != len(metadata):\n        raise ValueError(\"Length of texts and metadata must match\")\n    if len(texts) == 1 and len(texts[0]) &lt; 10_000:\n        if len(self.sto) &lt; self.batch_size and len(texts) == 1:\n            self.sto.append((texts[0], metadata[0]))\n            return -1, -1\n        if len(self.sto) &gt;= self.batch_size:\n            _ = [texts.append(t) or metadata.append([m]) for (t, m) in self.sto]\n            self.sto = []\n\n    # Split large texts\n    split_texts = []\n    split_metadata = []\n\n    while Spinner(\"Saving Data to Memory\", symbols='t'):\n\n        for idx, text in enumerate(texts):\n            chunks = self.text_splitter.split_text(text)\n            split_texts.extend(chunks)\n\n            # Adjust metadata for splits\n            meta = metadata[idx] if metadata else {}\n            if isinstance(meta, list):\n                meta = meta[0]\n            for i, _chunk in enumerate(chunks):\n                chunk_meta = meta.copy()\n                chunk_meta.update({\n                    'chunk_index': i,\n                    'total_chunks': len(chunks),\n                    'original_text_id': idx\n                })\n                split_metadata.append(chunk_meta)\n\n        return await self._add_data(split_texts, split_metadata)\n</code></pre> <code>compute_hash(text)</code> <code>staticmethod</code> \u00b6 <p>Compute SHA-256 hash of text</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>@staticmethod\ndef compute_hash(text: str) -&gt; str:\n    \"\"\"Compute SHA-256 hash of text\"\"\"\n    return hashlib.sha256(text.encode('utf-8', errors='ignore')).hexdigest()\n</code></pre> <code>forget_irrelevant(irrelevant_concepts, similarity_threshold=None)</code> <code>async</code> \u00b6 <p>Remove chunks similar to irrelevant concepts Returns: Number of chunks removed</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def forget_irrelevant(self, irrelevant_concepts: list[str], similarity_threshold: float | None=None) -&gt; int:\n    \"\"\"\n    Remove chunks similar to irrelevant concepts\n    Returns: Number of chunks removed\n    \"\"\"\n    if not irrelevant_concepts:\n        return 0\n\n    if similarity_threshold is None:\n        similarity_threshold = self.similarity_threshold\n\n    try:\n        irrelevant_embeddings = await self._get_embeddings(irrelevant_concepts)\n        initial_count = len(self.vdb.chunks)\n\n        def is_relevant(chunk: Chunk) -&gt; bool:\n            similarities = np.dot(chunk.embedding, irrelevant_embeddings.T)\n            do_keep = np.max(similarities) &lt; similarity_threshold\n            if do_keep:\n                return True\n            for c in chunk.metadata.get(\"concepts\", []):\n                if c in self.concept_extractor.concept_graph.concepts:\n                    del self.concept_extractor.concept_graph.concepts[c]\n            return False\n\n        relevant_chunks = [chunk for chunk in self.vdb.chunks if is_relevant(chunk)]\n        self.vdb.chunks = relevant_chunks\n        self.existing_hashes = {chunk.content_hash for chunk in self.vdb.chunks}\n        self.vdb.rebuild_index()\n\n\n        return initial_count - len(self.vdb.chunks)\n\n    except Exception as e:\n        get_logger().error(f\"Error forgetting irrelevant concepts: {str(e)}\")\n        raise\n</code></pre> <code>load(path)</code> <code>classmethod</code> \u00b6 <p>Load a complete knowledge base from disk, including all sub-components</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path from where to load the knowledge base</p> required <p>Returns:</p> Name Type Description <code>KnowledgeBase</code> <code>KnowledgeBase</code> <p>A fully restored knowledge base instance</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>@classmethod\ndef load(cls, path: str | bytes) -&gt; 'KnowledgeBase':\n    \"\"\"\n    Load a complete knowledge base from disk, including all sub-components\n\n    Args:\n        path (str): Path from where to load the knowledge base\n\n    Returns:\n        KnowledgeBase: A fully restored knowledge base instance\n    \"\"\"\n    try:\n        if isinstance(path, str):\n            # Load data from disk\n            with open(path, 'rb') as f:\n                data = pickle.load(f)\n        elif isinstance(path, bytes):\n            data = pickle.loads(path)\n        else:\n            raise ValueError(\"Invalid path type\")\n\n        # Create new knowledge base instance with saved configuration\n        kb = cls(\n            embedding_dim=data['embedding_dim'],\n            similarity_threshold=data['similarity_threshold'],\n            batch_size=data['batch_size'],\n            n_clusters=data['n_clusters'],\n            deduplication_threshold=data['deduplication_threshold'],\n            model_name=data['model_name'],\n            embedding_model=data['embedding_model']\n        )\n\n        # Restore core components\n        kb.init_vis(data.get('vis_class'), data.get('vis_kwargs'))\n        kb.existing_hashes = data['existing_hashes']\n\n        # Restore cache and graph data\n        kb.similarity_graph = data.get('similarity_graph', {})\n        kb.sto = data.get('sto', [])\n\n        # Restore text splitter configuration\n        splitter_config = data.get('text_splitter_config', {})\n        kb.text_splitter = TextSplitter(\n            chunk_size=splitter_config.get('chunk_size', 12_000),\n            chunk_overlap=splitter_config.get('chunk_overlap', 200),\n            separator=splitter_config.get('separator', '\\n')\n        )\n\n        # Restore concept graph\n        concept_data = data.get('concept_graph', {}).get('concepts', {})\n        for concept_info in concept_data.values():\n            concept = Concept(\n                name=concept_info['name'],\n                category=concept_info['category'],\n                relationships={k: set(v) for k, v in concept_info['relationships'].items()},\n                importance_score=concept_info['importance_score'],\n                context_snippets=concept_info['context_snippets'],\n                metadata=concept_info['metadata']\n            )\n            kb.concept_extractor.concept_graph.add_concept(concept)\n\n        print(f\"Knowledge base successfully loaded from {path} with {len(concept_data)} concepts\")\n        return kb\n\n    except Exception as e:\n        print(f\"Error loading knowledge base: {str(e)}\")\n        raise\n</code></pre> <code>query_concepts(query)</code> <code>async</code> \u00b6 <p>Query concepts extracted from the knowledge base</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def query_concepts(self, query: str) -&gt; dict[str, any]:\n    \"\"\"Query concepts extracted from the knowledge base\"\"\"\n    return await self.concept_extractor.query_concepts(query)\n</code></pre> <code>retrieve(query='', query_embedding=None, k=5, min_similarity=0.2, include_connected=True)</code> <code>async</code> \u00b6 <p>Enhanced retrieval with connected information</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def retrieve(\n    self,\n    query: str=\"\",\n    query_embedding: np.ndarray | None = None,\n    k: int = 5,\n    min_similarity: float = 0.2,\n    include_connected: bool = True\n) -&gt; list[Chunk]:\n    \"\"\"Enhanced retrieval with connected information\"\"\"\n    if query_embedding is None:\n        query_embedding = (await self._get_embeddings([query]))[0]\n    k = min(k, len(self.vdb.chunks)-1)\n    if k &lt;= 0:\n        return []\n    initial_results = self.vdb.search(query_embedding, k, min_similarity)\n\n    if not include_connected or not initial_results:\n        return initial_results\n\n    # Find connected chunks\n    connected_chunks = set()\n    for chunk in initial_results:\n        chunk_id = self.vdb.chunks.index(chunk)\n        if chunk_id in self.similarity_graph:\n            connected_chunks.update(self.similarity_graph[chunk_id])\n\n    # Add connected chunks to results\n    all_chunks = self.vdb.chunks\n    additional_results = [all_chunks[i] for i in connected_chunks\n                          if all_chunks[i] not in initial_results]\n\n    # Sort by similarity to query\n    all_results = initial_results + additional_results\n\n    return sorted(\n        all_results,\n        key=lambda x: np.dot(x.embedding, query_embedding),\n        reverse=True\n    )[:k * 2]  # Return more results when including connected information\n</code></pre> <code>retrieve_with_overview(query, query_embedding=None, k=5, min_similarity=0.2, max_sentences=5, cross_ref_depth=2, max_cross_refs=10)</code> <code>async</code> \u00b6 <p>Enhanced retrieval with better cross-reference handling</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def retrieve_with_overview(\n    self,\n    query: str,\n    query_embedding=None,\n    k: int = 5,\n    min_similarity: float = 0.2,\n    max_sentences: int = 5,\n    cross_ref_depth: int = 2,\n    max_cross_refs: int = 10  # New parameter to control cross-reference count\n) -&gt; RetrievalResult:\n    \"\"\"Enhanced retrieval with better cross-reference handling\"\"\"\n    # Get initial results with query embedding\n    if query_embedding is None:\n        query_embedding = (await self._get_embeddings([query]))[0]\n    initial_results = await self.retrieve(query_embedding=query_embedding, k=k, min_similarity=min_similarity)\n\n    if not initial_results:\n        return RetrievalResult([], [], {})\n\n    # Find cross-references with similarity scoring\n    initial_ids = {self.vdb.chunks.index(chunk) for chunk in initial_results}\n    related_ids = self._find_cross_references(\n        initial_ids,\n        depth=cross_ref_depth,\n        query_embedding=query_embedding  # Pass query embedding for relevance scoring\n    )\n\n    # Get all relevant chunks with smarter filtering\n    all_chunks = self.vdb.chunks\n    all_relevant_chunks = initial_results + [\n        chunk for i, chunk in enumerate(all_chunks)\n        if i in related_ids and self._is_relevant_cross_ref(\n            chunk,\n            query_embedding,\n            initial_results\n        )\n    ]\n\n    # Enhanced clustering with dynamic cluster size\n    clusters = self._cluster_chunks(\n        all_relevant_chunks,\n        query_embedding=query_embedding\n    )\n\n    # Fallback: If no clusters are found, treat all relevant chunks as a single cluster.\n    if not clusters:\n        print(\"No clusters found. Falling back to using all relevant chunks as a single cluster.\")\n        clusters = {0: all_relevant_chunks}\n\n    # Generate summaries and organize results\n    overview = []\n    cross_references = {}\n\n    for cluster_id, cluster_chunks in clusters.items():\n        summary = self._generate_topic_summary(\n            cluster_chunks,\n            query_embedding,\n            max_sentences=max_sentences  # Increased for more context\n        )\n\n        # Enhanced chunk sorting with combined scoring\n        sorted_chunks = self._sort_chunks_by_relevance(\n            cluster_chunks,\n            query_embedding,\n            initial_results\n        )\n\n        # Separate direct matches and cross-references\n        direct_matches_ = [{'text':c.text, 'metadata':c.metadata} for c in sorted_chunks if c in initial_results]\n        direct_matches = []\n        for match in direct_matches_:\n            if match in direct_matches:\n                continue\n            direct_matches.append(match)\n        cross_refs_ = [c for c in sorted_chunks if c not in initial_results]\n        cross_refs = []\n        for match in cross_refs_:\n            if match in cross_refs:\n                continue\n            cross_refs.append(match)\n        # Limit cross-references while maintaining diversity\n        selected_cross_refs = self._select_diverse_cross_refs(\n            cross_refs,\n            max_cross_refs,\n            query_embedding\n        )\n\n        topic_info = {\n            'topic_id': cluster_id,\n            'summary': summary,\n            'main_chunks': [x for x in direct_matches[:3]],\n            'chunk_count': len(cluster_chunks),\n            'relevance_score': self._calculate_topic_relevance(\n                cluster_chunks,\n                query_embedding\n            )\n        }\n        overview.append(topic_info)\n\n        if selected_cross_refs:\n            cross_references[f\"topic_{cluster_id}\"] = selected_cross_refs\n\n    # Sort overview by relevance score\n    overview.sort(key=lambda x: x['relevance_score'], reverse=True)\n\n    return RetrievalResult(\n        overview=overview,\n        details=initial_results,\n        cross_references=cross_references\n    )\n</code></pre> <code>save(path)</code> \u00b6 <p>Save the complete knowledge base to disk, including all sub-components</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path where the knowledge base will be saved</p> required Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def save(self, path: str) -&gt; bytes | None:\n    \"\"\"\n    Save the complete knowledge base to disk, including all sub-components\n\n    Args:\n        path (str): Path where the knowledge base will be saved\n    \"\"\"\n    try:\n        data = {\n            # Core components\n            'vdb': self.vdb.save(),\n            'vis_kwargs': self.vis_kwargs,\n            'vis_class': self.vis_class,\n            'existing_hashes': self.existing_hashes,\n\n            # Configuration parameters\n            'embedding_dim': self.embedding_dim,\n            'similarity_threshold': self.similarity_threshold,\n            'batch_size': self.batch_size,\n            'n_clusters': self.n_clusters,\n            'deduplication_threshold': self.deduplication_threshold,\n            'model_name': self.model_name,\n            'embedding_model': self.embedding_model,\n\n            # Cache and graph data\n            'similarity_graph': self.similarity_graph,\n            'sto': self.sto,\n\n            # Text splitter configuration\n            'text_splitter_config': {\n                'chunk_size': self.text_splitter.chunk_size,\n                'chunk_overlap': self.text_splitter.chunk_overlap,\n                'separator': self.text_splitter.separator\n            },\n\n            # Concept extractor data\n            'concept_graph': {\n                'concepts': {\n                    name: {\n                        'name': concept.name,\n                        'category': concept.category,\n                        'relationships': {k: list(v) for k, v in concept.relationships.items()},\n                        'importance_score': concept.importance_score,\n                        'context_snippets': concept.context_snippets,\n                        'metadata': concept.metadata\n                    }\n                    for name, concept in self.concept_extractor.concept_graph.concepts.items()\n                }\n            }\n        }\n        if path is None:\n            return pickle.dumps(data)\n        # Save to disk using pickle\n        with open(path, 'wb') as f:\n            pickle.dump(data, f)\n        print(f\"Knowledge base successfully saved to {path} with {len(self.concept_extractor.concept_graph.concepts.items())} concepts\")\n\n    except Exception as e:\n        print(f\"Error saving knowledge base: {str(e)}\")\n        raise\n</code></pre> <code>unified_retrieve(query, k=5, min_similarity=0.2, cross_ref_depth=2, max_cross_refs=10, max_sentences=10)</code> <code>async</code> \u00b6 <p>Unified retrieval function that combines concept querying, retrieval with overview, and basic retrieval, then generates a comprehensive summary using LLM.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query string</p> required <code>k</code> <code>int</code> <p>Number of primary results to retrieve</p> <code>5</code> <code>min_similarity</code> <code>float</code> <p>Minimum similarity threshold for retrieval</p> <code>0.2</code> <code>cross_ref_depth</code> <code>int</code> <p>Depth for cross-reference search</p> <code>2</code> <code>max_cross_refs</code> <code>int</code> <p>Maximum number of cross-references per topic</p> <code>10</code> <code>max_sentences</code> <code>int</code> <p>Maximum number Sentences in the main summary text</p> <code>10</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing comprehensive results including summary and details</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def unified_retrieve(\n    self,\n    query: str,\n    k: int = 5,\n    min_similarity: float = 0.2,\n    cross_ref_depth: int = 2,\n    max_cross_refs: int = 10,\n    max_sentences: int = 10\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Unified retrieval function that combines concept querying, retrieval with overview,\n    and basic retrieval, then generates a comprehensive summary using LLM.\n\n    Args:\n        query: Search query string\n        k: Number of primary results to retrieve\n        min_similarity: Minimum similarity threshold for retrieval\n        cross_ref_depth: Depth for cross-reference search\n        max_cross_refs: Maximum number of cross-references per topic\n        max_sentences: Maximum number Sentences in the main summary text\n\n    Returns:\n        Dictionary containing comprehensive results including summary and details\n    \"\"\"\n    # Get concept information\n    concept_results = await self.concept_extractor.query_concepts(query)\n\n    # Get retrieval overview\n\n    query_embedding = (await self._get_embeddings([query]))[0]\n    overview_results = await self.retrieve_with_overview(\n        query=query,\n        query_embedding=query_embedding,\n        k=k,\n        min_similarity=min_similarity,\n        cross_ref_depth=cross_ref_depth,\n        max_cross_refs=max_cross_refs,\n        max_sentences=max_sentences\n    )\n\n    # Get basic retrieval results\n    basic_results = await self.retrieve(\n        query_embedding=query_embedding,\n        k=k,\n        min_similarity=min_similarity\n    )\n    if len(basic_results) == 0:\n        return {}\n    if len(basic_results) == 1 and isinstance(basic_results[0], str) and basic_results[0].endswith('[]\\n - []\\n - []'):\n        return {}\n\n    # Prepare context for LLM summary\n    context = {\n        \"concepts\": {\n            \"main_concepts\": concept_results.get(\"concepts\", {}),\n            \"relationships\": concept_results.get(\"relationships\", []),\n            \"concept_groups\": concept_results.get(\"groups\", [])\n        },\n        \"topics\": [\n            {\n                \"id\": topic[\"topic_id\"],\n                \"summary\": topic[\"summary\"],\n                \"relevance\": topic[\"relevance_score\"],\n                \"chunk_count\": topic[\"chunk_count\"]\n            }\n            for topic in overview_results.overview\n        ],\n        \"key_chunks\": [\n            {\n                \"text\": chunk.text,\n                \"metadata\": chunk.metadata\n            }\n            for chunk in basic_results\n        ]\n    }\n\n    # Generate comprehensive summary using LLM\n    system_prompt = \"\"\"\n    Analyze the provided search results and generate a comprehensive summary\n    that includes:\n    1. Main concepts and their relationships\n    2. Key topics and their relevance\n    3. Most important findings and insights\n    4. Cross-references and connections between topics\n    5. Potential gaps or areas for further investigation\n\n    Format the response as a JSON object with these sections.\n    \"\"\"\n\n    prompt = f\"\"\"\n    Query: {query}\n\n    Context:\n    {json.dumps(context, indent=2)}\n\n    Generate a comprehensive analysis and summary following the structure:\n    \"\"\"\n\n    try:\n        await asyncio.sleep(0.25)\n        llm_response = await litellm_complete(\n            model_name=self.model_name,\n            prompt=prompt,\n            system_prompt=system_prompt,\n            response_format=DataModel,\n        )\n        summary_analysis = json.loads(llm_response)\n    except Exception as e:\n        get_logger().error(f\"Error generating summary: {str(e)}\")\n        summary_analysis = {\n            \"main_summary\": \"Error generating summary\",\n            \"error\": str(e)\n        }\n\n    # Compile final results\n    return {\n        \"summary\": summary_analysis,\n        \"raw_results\": {\n            \"concepts\": concept_results,\n            \"overview\": {\n                \"topics\": overview_results.overview,\n                \"cross_references\": overview_results.cross_references\n            },\n            \"relevant_chunks\": [\n                {\n                    \"text\": chunk.text,\n                    \"metadata\": chunk.metadata,\n                    \"cluster_id\": chunk.cluster_id\n                }\n                for chunk in basic_results\n            ]\n        },\n        \"metadata\": {\n            \"query\": query,\n            \"timestamp\": time.time(),\n            \"retrieval_params\": {\n                \"k\": k,\n                \"min_similarity\": min_similarity,\n                \"cross_ref_depth\": cross_ref_depth,\n                \"max_cross_refs\": max_cross_refs\n            }\n        }\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.RelevanceAssessment","title":"<code>RelevanceAssessment</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an assessment of the relevance of the data in relation to a specific query.</p> <p>Attributes:</p> Name Type Description <code>query_alignment</code> <code>float</code> <p>A float representing the alignment between the query and the data.</p> <code>confidence_score</code> <code>float</code> <p>A float indicating the confidence level in the alignment.</p> <code>coverage_analysis</code> <code>str</code> <p>A textual description analyzing the data coverage.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class RelevanceAssessment(BaseModel):\n    \"\"\"\n    Represents an assessment of the relevance of the data in relation to a specific query.\n\n    Attributes:\n        query_alignment (float): A float representing the alignment between the query and the data.\n        confidence_score (float): A float indicating the confidence level in the alignment.\n        coverage_analysis (str): A textual description analyzing the data coverage.\n    \"\"\"\n    query_alignment: float\n    confidence_score: float\n    coverage_analysis: str\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.RetrievalResult","title":"<code>RetrievalResult</code>  <code>dataclass</code>","text":"<p>Structure for organizing retrieval results</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>@dataclass\nclass RetrievalResult:\n    \"\"\"Structure for organizing retrieval results\"\"\"\n    overview: list[dict[str, any]]  # List of topic summaries\n    details: list[Chunk]  # Detailed chunks\n    cross_references: dict[str, list[Chunk]]  # Related chunks by topic\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.TConcept","title":"<code>TConcept</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the criteria or target parameters for concept selection and filtering.</p> <p>Attributes:</p> Name Type Description <code>min_importance</code> <code>float</code> <p>The minimum importance score a concept must have to be considered.</p> <code>target_concepts</code> <code>List[str]</code> <p>A list of names of target concepts to focus on.</p> <code>relationship_types</code> <code>List[str]</code> <p>A list of relationship types to be considered in the analysis.</p> <code>categories</code> <code>List[str]</code> <p>A list of concept categories to filter or group the concepts.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class TConcept(BaseModel):\n    \"\"\"\n    Represents the criteria or target parameters for concept selection and filtering.\n\n    Attributes:\n        min_importance (float): The minimum importance score a concept must have to be considered.\n        target_concepts (List[str]): A list of names of target concepts to focus on.\n        relationship_types (List[str]): A list of relationship types to be considered in the analysis.\n        categories (List[str]): A list of concept categories to filter or group the concepts.\n    \"\"\"\n    min_importance: float\n    target_concepts: list[str]\n    relationship_types: list[str]\n    categories: list[str]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.TextSplitter","title":"<code>TextSplitter</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class TextSplitter:\n    def __init__(\n        self,\n        chunk_size: int = 3600,\n        chunk_overlap: int = 130,\n        separator: str = \"\\n\"\n    ):\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.separator = separator\n\n    def approximate(self, text_len: int) -&gt; float:\n        \"\"\"\n        Approximate the number of chunks and average chunk size for a given text length\n\n        Args:\n            text_len (int): Length of the text to be split\n\n        Returns:\n            Tuple[int, int]: (number_of_chunks, approximate_chunk_size)\n        \"\"\"\n        if text_len &lt;= self.chunk_size:\n            return 1, text_len\n\n        # Handle extreme overlap cases\n        if self.chunk_overlap &gt;= self.chunk_size:\n            estimated_chunks = text_len\n            return estimated_chunks, 1\n\n        # Calculate based on overlap ratio\n        overlap_ratio = self.chunk_overlap / self.chunk_size\n        base_chunks = text_len / self.chunk_size\n        estimated_chunks = base_chunks * 2 / (overlap_ratio if overlap_ratio &gt; 0 else 1)\n\n        # print('#',estimated_chunks, base_chunks, overlap_ratio)\n        # Calculate average chunk size\n        avg_chunk_size = max(1, text_len / estimated_chunks)\n\n        return estimated_chunks * avg_chunk_size\n\n    def split_text(self, text: str) -&gt; list[str]:\n        \"\"\"Split text into chunks with overlap\"\"\"\n        # Clean and normalize text\n        text = re.sub(r'\\s+', ' ', text).strip()\n\n        # If text is shorter than chunk_size, return as is\n        if len(text) &lt;= self.chunk_size:\n            return [text]\n\n        chunks = []\n        start = 0\n\n        while start &lt; len(text):\n            # Find end of chunk\n            end = start + self.chunk_size\n\n            if end &gt;= len(text):\n                chunks.append(text[start:])\n                break\n\n            # Try to find a natural break point\n            last_separator = text.rfind(self.separator, start, end)\n            if last_separator != -1:\n                end = last_separator\n\n            # Add chunk\n            chunks.append(text[start:end])\n\n            # Calculate allowed overlap for this chunk\n            chunk_length = end - start\n            allowed_overlap = min(self.chunk_overlap, chunk_length - 1)\n\n            # Move start position considering adjusted overlap\n            start = end - allowed_overlap\n\n        return chunks\n</code></pre> <code>approximate(text_len)</code> \u00b6 <p>Approximate the number of chunks and average chunk size for a given text length</p> <p>Parameters:</p> Name Type Description Default <code>text_len</code> <code>int</code> <p>Length of the text to be split</p> required <p>Returns:</p> Type Description <code>float</code> <p>Tuple[int, int]: (number_of_chunks, approximate_chunk_size)</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def approximate(self, text_len: int) -&gt; float:\n    \"\"\"\n    Approximate the number of chunks and average chunk size for a given text length\n\n    Args:\n        text_len (int): Length of the text to be split\n\n    Returns:\n        Tuple[int, int]: (number_of_chunks, approximate_chunk_size)\n    \"\"\"\n    if text_len &lt;= self.chunk_size:\n        return 1, text_len\n\n    # Handle extreme overlap cases\n    if self.chunk_overlap &gt;= self.chunk_size:\n        estimated_chunks = text_len\n        return estimated_chunks, 1\n\n    # Calculate based on overlap ratio\n    overlap_ratio = self.chunk_overlap / self.chunk_size\n    base_chunks = text_len / self.chunk_size\n    estimated_chunks = base_chunks * 2 / (overlap_ratio if overlap_ratio &gt; 0 else 1)\n\n    # print('#',estimated_chunks, base_chunks, overlap_ratio)\n    # Calculate average chunk size\n    avg_chunk_size = max(1, text_len / estimated_chunks)\n\n    return estimated_chunks * avg_chunk_size\n</code></pre> <code>split_text(text)</code> \u00b6 <p>Split text into chunks with overlap</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def split_text(self, text: str) -&gt; list[str]:\n    \"\"\"Split text into chunks with overlap\"\"\"\n    # Clean and normalize text\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    # If text is shorter than chunk_size, return as is\n    if len(text) &lt;= self.chunk_size:\n        return [text]\n\n    chunks = []\n    start = 0\n\n    while start &lt; len(text):\n        # Find end of chunk\n        end = start + self.chunk_size\n\n        if end &gt;= len(text):\n            chunks.append(text[start:])\n            break\n\n        # Try to find a natural break point\n        last_separator = text.rfind(self.separator, start, end)\n        if last_separator != -1:\n            end = last_separator\n\n        # Add chunk\n        chunks.append(text[start:end])\n\n        # Calculate allowed overlap for this chunk\n        chunk_length = end - start\n        allowed_overlap = min(self.chunk_overlap, chunk_length - 1)\n\n        # Move start position considering adjusted overlap\n        start = end - allowed_overlap\n\n    return chunks\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.TopicInsights","title":"<code>TopicInsights</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents insights related to various topics.</p> <p>Attributes:</p> Name Type Description <code>primary_topics</code> <code>list[str]</code> <p>A list of main topics addressed.</p> <code>cross_references</code> <code>list[str]</code> <p>A list of cross-references that connect different topics.</p> <code>knowledge_gaps</code> <code>list[str]</code> <p>A list of identified gaps in the current knowledge.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class TopicInsights(BaseModel):\n    \"\"\"\n    Represents insights related to various topics.\n\n    Attributes:\n        primary_topics (list[str]): A list of main topics addressed.\n        cross_references (list[str]): A list of cross-references that connect different topics.\n        knowledge_gaps (list[str]): A list of identified gaps in the current knowledge.\n    \"\"\"\n    primary_topics: list[str]\n    cross_references: list[str]\n    knowledge_gaps: list[str]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.rConcept","title":"<code>rConcept</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a key concept with its relationships and associated metadata.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the concept.</p> <code>category</code> <code>str</code> <p>The category of the concept (e.g., 'technical', 'domain', 'method', etc.).</p> <code>relationships</code> <code>Dict[str, List[str]]</code> <p>A mapping where each key is a type of relationship and the value is a list of related concept names.</p> <code>importance_score</code> <code>float</code> <p>A numerical score representing the importance or relevance of the concept.</p> <code>context_snippets</code> <code>List[str]</code> <p>A list of text snippets providing context where the concept appears.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class rConcept(BaseModel):\n    \"\"\"\n    Represents a key concept with its relationships and associated metadata.\n\n    Attributes:\n        name (str): The name of the concept.\n        category (str): The category of the concept (e.g., 'technical', 'domain', 'method', etc.).\n        relationships (Dict[str, List[str]]): A mapping where each key is a type of relationship and the\n            value is a list of related concept names.\n        importance_score (float): A numerical score representing the importance or relevance of the concept.\n        context_snippets (List[str]): A list of text snippets providing context where the concept appears.\n    \"\"\"\n    name: str\n    category: str\n    relationships: dict[str, list[str]]\n    importance_score: float\n    context_snippets: list[str]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.normalize_vectors","title":"<code>normalize_vectors(vectors)</code>","text":"<p>Normalize vectors to unit length</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def normalize_vectors(vectors: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Normalize vectors to unit length\"\"\"\n    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n    return np.divide(vectors, norms, where=norms != 0)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.VectorStores","title":"<code>VectorStores</code>","text":"<p>Vector store implementations for the toolboxv2 system.</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.VectorStores.AbstractVectorStore","title":"<code>AbstractVectorStore</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for vector stores</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/defaults.py</code> <pre><code>class AbstractVectorStore(ABC):\n    \"\"\"Abstract base class for vector stores\"\"\"\n\n    @abstractmethod\n    def add_embeddings(self, embeddings: np.ndarray, chunks: list[Chunk]) -&gt; None:\n        \"\"\"Add embeddings and their corresponding chunks to the store\"\"\"\n        pass\n\n    @abstractmethod\n    def search(self, query_embedding: np.ndarray, k: int = 5, min_similarity: float = 0.7) -&gt; list[Chunk]:\n        \"\"\"Search for similar vectors\"\"\"\n        pass\n\n    @abstractmethod\n    def save(self) -&gt; bytes:\n        \"\"\"Save the vector store to disk\"\"\"\n        pass\n\n    @abstractmethod\n    def load(self, data: bytes) -&gt; 'AbstractVectorStore':\n        \"\"\"Load the vector store from disk\"\"\"\n        pass\n\n    @abstractmethod\n    def clear(self) -&gt; None:\n        \"\"\"Clear all data from the store\"\"\"\n        pass\n\n    @abstractmethod\n    def rebuild_index(self) -&gt; None:\n        \"\"\"Optional for faster searches\"\"\"\n        pass\n</code></pre> <code>add_embeddings(embeddings, chunks)</code> <code>abstractmethod</code> \u00b6 <p>Add embeddings and their corresponding chunks to the store</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/defaults.py</code> <pre><code>@abstractmethod\ndef add_embeddings(self, embeddings: np.ndarray, chunks: list[Chunk]) -&gt; None:\n    \"\"\"Add embeddings and their corresponding chunks to the store\"\"\"\n    pass\n</code></pre> <code>clear()</code> <code>abstractmethod</code> \u00b6 <p>Clear all data from the store</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/defaults.py</code> <pre><code>@abstractmethod\ndef clear(self) -&gt; None:\n    \"\"\"Clear all data from the store\"\"\"\n    pass\n</code></pre> <code>load(data)</code> <code>abstractmethod</code> \u00b6 <p>Load the vector store from disk</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/defaults.py</code> <pre><code>@abstractmethod\ndef load(self, data: bytes) -&gt; 'AbstractVectorStore':\n    \"\"\"Load the vector store from disk\"\"\"\n    pass\n</code></pre> <code>rebuild_index()</code> <code>abstractmethod</code> \u00b6 <p>Optional for faster searches</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/defaults.py</code> <pre><code>@abstractmethod\ndef rebuild_index(self) -&gt; None:\n    \"\"\"Optional for faster searches\"\"\"\n    pass\n</code></pre> <code>save()</code> <code>abstractmethod</code> \u00b6 <p>Save the vector store to disk</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/defaults.py</code> <pre><code>@abstractmethod\ndef save(self) -&gt; bytes:\n    \"\"\"Save the vector store to disk\"\"\"\n    pass\n</code></pre> <code>search(query_embedding, k=5, min_similarity=0.7)</code> <code>abstractmethod</code> \u00b6 <p>Search for similar vectors</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/defaults.py</code> <pre><code>@abstractmethod\ndef search(self, query_embedding: np.ndarray, k: int = 5, min_similarity: float = 0.7) -&gt; list[Chunk]:\n    \"\"\"Search for similar vectors\"\"\"\n    pass\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.VectorStores.NumpyVectorStore","title":"<code>NumpyVectorStore</code>","text":"<p>               Bases: <code>AbstractVectorStore</code></p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/defaults.py</code> <pre><code>class NumpyVectorStore(AbstractVectorStore):\n    def __init__(self, use_gpu=False):\n        self.embeddings = np.empty((0, 0))\n        self.chunks = []\n        # Initialize Taich\n        import taichi as ti\n        ti.init(arch=ti.gpu if use_gpu else ti.cpu)\n        self.normalized_embeddings = None\n\n    def add_embeddings(self, embeddings: np.ndarray, chunks: list[Chunk]) -&gt; None:\n        if len(embeddings.shape) != 2:\n            raise ValueError(\"Embeddings must be 2D array\")\n        if len(chunks) != embeddings.shape[0]:\n            raise ValueError(\"Mismatch between embeddings and chunks count\")\n\n        if self.embeddings.size == 0:\n            self.embeddings = embeddings\n        else:\n            if embeddings.shape[1] != self.embeddings.shape[1]:\n                raise ValueError(\"Embedding dimensions must match\")\n            self.embeddings = np.vstack([self.embeddings, embeddings])\n        self.chunks.extend(chunks)\n        # Reset normalized embeddings cache\n        self.normalized_embeddings = None\n\n    def search(self, query_embedding: np.ndarray, k: int = 5, min_similarity: float = 0.7) -&gt; list[Chunk]:\n        if self.embeddings.size == 0:\n            return []\n\n        # Pre-compute normalized embeddings if not cached\n        if self.normalized_embeddings is None:\n            self._precompute_normalized_embeddings()\n\n        # Normalize query\n        query_norm = self._normalize_vector(query_embedding)\n\n        # Enhanced Taichi kernel for similarity computation\n        n = len(self.chunks)\n        similarities = np.zeros(n, dtype=np.float32)\n        import taichi as ti\n        @ti.kernel\n        def compute_similarities_optimized(\n            query: ti.types.ndarray(dtype=ti.f32),\n            embeddings: ti.types.ndarray(dtype=ti.f32),\n            similarities: ti.types.ndarray(dtype=ti.f32),\n            n: ti.i32,\n            dim: ti.i32\n        ):\n            ti.loop_config(block_dim=256)\n            for i in range(n):\n                dot_product = 0.0\n                # Vectorized dot product computation\n                for j in range(dim):\n                    dot_product += embeddings[i, j] * query[j]\n                similarities[i] = dot_product\n\n        # Alternative optimized kernel using tile-based computation\n        @ti.kernel\n        def compute_similarities_tiled(\n            query: ti.types.ndarray(dtype=ti.f32),\n            embeddings: ti.types.ndarray(dtype=ti.f32),\n            similarities: ti.types.ndarray(dtype=ti.f32),\n            n: ti.i32,\n            dim: ti.i32\n        ):\n            tile_size = 16  # Adjust based on hardware\n            for i in range(n):\n                dot_product = 0.0\n                # Process in tiles for better cache utilization\n                for jt in range(0, dim):\n                    if jt % tile_size != 0:\n                        continue\n                    tile_sum = 0.0\n                    for j in range(jt, ti.min(jt + tile_size, dim)):\n                        tile_sum += embeddings[i, j] * query[j]\n                    dot_product += tile_sum\n                similarities[i] = dot_product\n\n        # Choose the appropriate kernel based on dimension size\n        if query_embedding.shape[0] &gt;= 256:\n            compute_similarities_tiled(\n                query_norm.astype(np.float32),\n                self.normalized_embeddings,\n                similarities,\n                n,\n                query_embedding.shape[0]\n            )\n        else:\n            compute_similarities_optimized(\n                query_norm.astype(np.float32),\n                self.normalized_embeddings,\n                similarities,\n                n,\n                query_embedding.shape[0]\n            )\n\n        # Optimize top-k selection\n        if k &gt;= n:\n            indices = np.argsort(-similarities)\n        else:\n            # Use partial sort for better performance when k &lt; n\n            indices = np.argpartition(-similarities, k)[:k]\n            indices = indices[np.argsort(-similarities[indices])]\n\n        # Filter results efficiently using vectorized operations\n        mask = similarities[indices] &gt;= min_similarity\n        filtered_indices = indices[mask]\n        return [self.chunks[idx] for idx in filtered_indices[:k]]\n\n    def save(self) -&gt; bytes:\n        return pickle.dumps({\n            'embeddings': self.embeddings,\n            'chunks': self.chunks\n        })\n\n    def load(self, data: bytes) -&gt; 'NumpyVectorStore':\n        loaded = pickle.loads(data)\n        self.embeddings = loaded['embeddings']\n        self.chunks = loaded['chunks']\n        return self\n\n    def clear(self) -&gt; None:\n        self.embeddings = np.empty((0, 0))\n        self.chunks = []\n        self.normalized_embeddings = None\n\n    def rebuild_index(self) -&gt; None:\n        pass  # No index to rebuild for numpy implementation\n\n    def _normalize_vector(self, vector: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Normalize a single vector efficiently.\"\"\"\n        return vector / (np.linalg.norm(vector) + 1e-8)\n\n    def _precompute_normalized_embeddings(self) -&gt; None:\n        \"\"\"Pre-compute and cache normalized embeddings.\"\"\"\n        # Allocate output array\n        self.normalized_embeddings = np.empty_like(self.embeddings, dtype=np.float32)\n\n        # Normalize embeddings using Taichi\n        batch_normalize(\n            self.embeddings.astype(np.float32),\n            self.normalized_embeddings,\n            self.embeddings.shape[0],\n            self.embeddings.shape[1]\n        )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.VectorStores.defaults","title":"<code>defaults</code>","text":"<code>AbstractVectorStore</code> \u00b6 <p>               Bases: <code>ABC</code></p> <p>Abstract base class for vector stores</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/defaults.py</code> <pre><code>class AbstractVectorStore(ABC):\n    \"\"\"Abstract base class for vector stores\"\"\"\n\n    @abstractmethod\n    def add_embeddings(self, embeddings: np.ndarray, chunks: list[Chunk]) -&gt; None:\n        \"\"\"Add embeddings and their corresponding chunks to the store\"\"\"\n        pass\n\n    @abstractmethod\n    def search(self, query_embedding: np.ndarray, k: int = 5, min_similarity: float = 0.7) -&gt; list[Chunk]:\n        \"\"\"Search for similar vectors\"\"\"\n        pass\n\n    @abstractmethod\n    def save(self) -&gt; bytes:\n        \"\"\"Save the vector store to disk\"\"\"\n        pass\n\n    @abstractmethod\n    def load(self, data: bytes) -&gt; 'AbstractVectorStore':\n        \"\"\"Load the vector store from disk\"\"\"\n        pass\n\n    @abstractmethod\n    def clear(self) -&gt; None:\n        \"\"\"Clear all data from the store\"\"\"\n        pass\n\n    @abstractmethod\n    def rebuild_index(self) -&gt; None:\n        \"\"\"Optional for faster searches\"\"\"\n        pass\n</code></pre> <code>add_embeddings(embeddings, chunks)</code> <code>abstractmethod</code> \u00b6 <p>Add embeddings and their corresponding chunks to the store</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/defaults.py</code> <pre><code>@abstractmethod\ndef add_embeddings(self, embeddings: np.ndarray, chunks: list[Chunk]) -&gt; None:\n    \"\"\"Add embeddings and their corresponding chunks to the store\"\"\"\n    pass\n</code></pre> <code>clear()</code> <code>abstractmethod</code> \u00b6 <p>Clear all data from the store</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/defaults.py</code> <pre><code>@abstractmethod\ndef clear(self) -&gt; None:\n    \"\"\"Clear all data from the store\"\"\"\n    pass\n</code></pre> <code>load(data)</code> <code>abstractmethod</code> \u00b6 <p>Load the vector store from disk</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/defaults.py</code> <pre><code>@abstractmethod\ndef load(self, data: bytes) -&gt; 'AbstractVectorStore':\n    \"\"\"Load the vector store from disk\"\"\"\n    pass\n</code></pre> <code>rebuild_index()</code> <code>abstractmethod</code> \u00b6 <p>Optional for faster searches</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/defaults.py</code> <pre><code>@abstractmethod\ndef rebuild_index(self) -&gt; None:\n    \"\"\"Optional for faster searches\"\"\"\n    pass\n</code></pre> <code>save()</code> <code>abstractmethod</code> \u00b6 <p>Save the vector store to disk</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/defaults.py</code> <pre><code>@abstractmethod\ndef save(self) -&gt; bytes:\n    \"\"\"Save the vector store to disk\"\"\"\n    pass\n</code></pre> <code>search(query_embedding, k=5, min_similarity=0.7)</code> <code>abstractmethod</code> \u00b6 <p>Search for similar vectors</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/defaults.py</code> <pre><code>@abstractmethod\ndef search(self, query_embedding: np.ndarray, k: int = 5, min_similarity: float = 0.7) -&gt; list[Chunk]:\n    \"\"\"Search for similar vectors\"\"\"\n    pass\n</code></pre> <code>Chunk</code> <code>dataclass</code> \u00b6 <p>Represents a chunk of text with its embedding and metadata</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/defaults.py</code> <pre><code>@dataclass(slots=True)\nclass Chunk:\n    \"\"\"Represents a chunk of text with its embedding and metadata\"\"\"\n    text: str\n    embedding: np.ndarray\n    metadata: dict[str, Any]\n    content_hash: str\n    cluster_id: int | None = None\n</code></pre> <code>NumpyVectorStore</code> \u00b6 <p>               Bases: <code>AbstractVectorStore</code></p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/defaults.py</code> <pre><code>class NumpyVectorStore(AbstractVectorStore):\n    def __init__(self, use_gpu=False):\n        self.embeddings = np.empty((0, 0))\n        self.chunks = []\n        # Initialize Taich\n        import taichi as ti\n        ti.init(arch=ti.gpu if use_gpu else ti.cpu)\n        self.normalized_embeddings = None\n\n    def add_embeddings(self, embeddings: np.ndarray, chunks: list[Chunk]) -&gt; None:\n        if len(embeddings.shape) != 2:\n            raise ValueError(\"Embeddings must be 2D array\")\n        if len(chunks) != embeddings.shape[0]:\n            raise ValueError(\"Mismatch between embeddings and chunks count\")\n\n        if self.embeddings.size == 0:\n            self.embeddings = embeddings\n        else:\n            if embeddings.shape[1] != self.embeddings.shape[1]:\n                raise ValueError(\"Embedding dimensions must match\")\n            self.embeddings = np.vstack([self.embeddings, embeddings])\n        self.chunks.extend(chunks)\n        # Reset normalized embeddings cache\n        self.normalized_embeddings = None\n\n    def search(self, query_embedding: np.ndarray, k: int = 5, min_similarity: float = 0.7) -&gt; list[Chunk]:\n        if self.embeddings.size == 0:\n            return []\n\n        # Pre-compute normalized embeddings if not cached\n        if self.normalized_embeddings is None:\n            self._precompute_normalized_embeddings()\n\n        # Normalize query\n        query_norm = self._normalize_vector(query_embedding)\n\n        # Enhanced Taichi kernel for similarity computation\n        n = len(self.chunks)\n        similarities = np.zeros(n, dtype=np.float32)\n        import taichi as ti\n        @ti.kernel\n        def compute_similarities_optimized(\n            query: ti.types.ndarray(dtype=ti.f32),\n            embeddings: ti.types.ndarray(dtype=ti.f32),\n            similarities: ti.types.ndarray(dtype=ti.f32),\n            n: ti.i32,\n            dim: ti.i32\n        ):\n            ti.loop_config(block_dim=256)\n            for i in range(n):\n                dot_product = 0.0\n                # Vectorized dot product computation\n                for j in range(dim):\n                    dot_product += embeddings[i, j] * query[j]\n                similarities[i] = dot_product\n\n        # Alternative optimized kernel using tile-based computation\n        @ti.kernel\n        def compute_similarities_tiled(\n            query: ti.types.ndarray(dtype=ti.f32),\n            embeddings: ti.types.ndarray(dtype=ti.f32),\n            similarities: ti.types.ndarray(dtype=ti.f32),\n            n: ti.i32,\n            dim: ti.i32\n        ):\n            tile_size = 16  # Adjust based on hardware\n            for i in range(n):\n                dot_product = 0.0\n                # Process in tiles for better cache utilization\n                for jt in range(0, dim):\n                    if jt % tile_size != 0:\n                        continue\n                    tile_sum = 0.0\n                    for j in range(jt, ti.min(jt + tile_size, dim)):\n                        tile_sum += embeddings[i, j] * query[j]\n                    dot_product += tile_sum\n                similarities[i] = dot_product\n\n        # Choose the appropriate kernel based on dimension size\n        if query_embedding.shape[0] &gt;= 256:\n            compute_similarities_tiled(\n                query_norm.astype(np.float32),\n                self.normalized_embeddings,\n                similarities,\n                n,\n                query_embedding.shape[0]\n            )\n        else:\n            compute_similarities_optimized(\n                query_norm.astype(np.float32),\n                self.normalized_embeddings,\n                similarities,\n                n,\n                query_embedding.shape[0]\n            )\n\n        # Optimize top-k selection\n        if k &gt;= n:\n            indices = np.argsort(-similarities)\n        else:\n            # Use partial sort for better performance when k &lt; n\n            indices = np.argpartition(-similarities, k)[:k]\n            indices = indices[np.argsort(-similarities[indices])]\n\n        # Filter results efficiently using vectorized operations\n        mask = similarities[indices] &gt;= min_similarity\n        filtered_indices = indices[mask]\n        return [self.chunks[idx] for idx in filtered_indices[:k]]\n\n    def save(self) -&gt; bytes:\n        return pickle.dumps({\n            'embeddings': self.embeddings,\n            'chunks': self.chunks\n        })\n\n    def load(self, data: bytes) -&gt; 'NumpyVectorStore':\n        loaded = pickle.loads(data)\n        self.embeddings = loaded['embeddings']\n        self.chunks = loaded['chunks']\n        return self\n\n    def clear(self) -&gt; None:\n        self.embeddings = np.empty((0, 0))\n        self.chunks = []\n        self.normalized_embeddings = None\n\n    def rebuild_index(self) -&gt; None:\n        pass  # No index to rebuild for numpy implementation\n\n    def _normalize_vector(self, vector: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Normalize a single vector efficiently.\"\"\"\n        return vector / (np.linalg.norm(vector) + 1e-8)\n\n    def _precompute_normalized_embeddings(self) -&gt; None:\n        \"\"\"Pre-compute and cache normalized embeddings.\"\"\"\n        # Allocate output array\n        self.normalized_embeddings = np.empty_like(self.embeddings, dtype=np.float32)\n\n        # Normalize embeddings using Taichi\n        batch_normalize(\n            self.embeddings.astype(np.float32),\n            self.normalized_embeddings,\n            self.embeddings.shape[0],\n            self.embeddings.shape[1]\n        )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.VectorStores.qdrant_store","title":"<code>qdrant_store</code>","text":"<p>QdrantVectorStore implementation for the toolboxv2 system. This vector store uses the Qdrant vector database (https://github.com/qdrant/qdrant-client) for storing and searching embeddings.</p> <code>QdrantVectorStore</code> \u00b6 <p>               Bases: <code>AbstractVectorStore</code></p> <p>Vector store implementation using Qdrant vector database.</p> <p>This implementation uses the Qdrant client to store and search embeddings. It supports both local and remote Qdrant instances.</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/qdrant_store.py</code> <pre><code>class QdrantVectorStore(AbstractVectorStore):\n    \"\"\"Vector store implementation using Qdrant vector database.\n\n    This implementation uses the Qdrant client to store and search embeddings.\n    It supports both local and remote Qdrant instances.\n    \"\"\"\n\n    def __init__(\n        self,\n        collection_name: str = \"default_collection\",\n        location: str | None = \":memory:\",  # Use in-memory Qdrant by default\n        url: str | None = None,\n        port: int = 6333,\n        grpc_port: int = 6334,\n        prefer_grpc: bool = False,\n        https: bool | None = None,\n        api_key: str | None = None,\n        timeout: int | None = None,\n        host: str | None = None,\n        path: str | None = None,\n        embedding_size: int = 768,\n        distance: str = \"Cosine\",\n        **kwargs,\n    ):\n        \"\"\"Initialize the QdrantVectorStore.\n\n        Args:\n            collection_name: Name of the collection to use in Qdrant\n            location: If \":memory:\" - use in-memory Qdrant instance.\n                      If None - use parameters from url or host/port.\n            url: URL of the Qdrant server (e.g., \"http://localhost:6333\")\n            port: Port of the REST API interface\n            grpc_port: Port of the gRPC interface\n            prefer_grpc: If true - use gPRC interface whenever possible\n            https: If true - use HTTPS(SSL) protocol\n            api_key: API key for authentication in Qdrant Cloud\n            timeout: Timeout for REST and gRPC API requests\n            host: Host name of Qdrant service\n            path: Persistence path for local Qdrant\n            embedding_size: Size of the embedding vectors\n            distance: Distance function to use (\"Cosine\", \"Euclid\", or \"Dot\")\n        \"\"\"\n        if not QDRANT_AVAILABLE:\n            raise ImportError(\n                \"Qdrant client is not available. Please install it with: pip install qdrant-client\"\n            )\n\n        self.collection_name = collection_name\n        self.embedding_size = embedding_size\n\n        # Map distance string to Qdrant Distance enum\n        distance_map = {\n            \"Cosine\": Distance.COSINE,\n            \"Euclid\": Distance.EUCLID,\n            \"Dot\": Distance.DOT,\n        }\n        self.distance = distance_map.get(distance, Distance.COSINE)\n\n        # Initialize Qdrant client\n        self.client = QdrantClient(\n            location=location,\n            url=url,\n            port=port,\n            grpc_port=grpc_port,\n            prefer_grpc=prefer_grpc,\n            https=https,\n            api_key=api_key,\n            timeout=timeout,\n            host=host,\n            path=path,\n        )\n\n        # Create collection if it doesn't exist\n        self._ensure_collection_exists()\n\n        # Keep a local cache of chunks for faster access\n        self.chunks = []\n\n    def _ensure_collection_exists(self) -&gt; None:\n        \"\"\"Ensure that the collection exists in Qdrant.\"\"\"\n        if not self.client.collection_exists(self.collection_name):\n            self.client.create_collection(\n                collection_name=self.collection_name,\n                vectors_config=VectorParams(\n                    size=self.embedding_size,\n                    distance=self.distance,\n                ),\n            )\n\n            # Create payload index for content_hash for faster lookups\n            self.client.create_payload_index(\n                collection_name=self.collection_name,\n                field_name=\"content_hash\",\n                field_schema=models.KeywordIndexParams(\n                    type=models.KeywordIndexType.KEYWORD,\n                    # on_disk=True\n                ),\n            )\n\n            # Create payload index for cluster_id for faster filtering\n            self.client.create_payload_index(\n                collection_name=self.collection_name,\n                field_name=\"cluster_id\",\n                field_schema=models.IntegerIndexParams(\n                    type=models.IntegerIndexType.INTEGER,\n                ),\n            )\n\n    def add_embeddings(self, embeddings: np.ndarray, chunks: list[Chunk]) -&gt; None:\n        \"\"\"Add embeddings and their corresponding chunks to the store.\n\n        Args:\n            embeddings: Numpy array of embeddings\n            chunks: List of Chunk objects corresponding to the embeddings\n        \"\"\"\n        if len(embeddings) == 0 or len(chunks) == 0:\n            return\n\n        # Prepare points for Qdrant\n        points = []\n        for i, (embedding, chunk) in enumerate(zip(embeddings, chunks, strict=False)):\n            # Generate a UUID for the point if not already present\n            point_id = str(uuid.uuid4())\n\n            # Create payload from chunk\n            payload = {\n                \"text\": chunk.text,\n                \"metadata\": chunk.metadata,\n                \"content_hash\": chunk.content_hash,\n            }\n\n            # Add cluster_id if it exists\n            if chunk.cluster_id is not None:\n                payload[\"cluster_id\"] = chunk.cluster_id\n\n            # Create point\n            point = PointStruct(\n                id=point_id,\n                vector=embedding.tolist(),\n                payload=payload,\n            )\n\n            points.append(point)\n\n            # Add to local cache\n            self.chunks.append(chunk)\n\n        # Upsert points to Qdrant\n        if len(points) &lt; 1000:\n            self.client.upsert(\n                collection_name=self.collection_name,\n                points=points,\n                wait=True\n            )\n        else:\n            for i in range(0, len(points), 1000):\n                self.client.upsert(\n                    collection_name=self.collection_name,\n                    points=points[i:i+1000],\n                    wait=False,\n                )\n\n\n    def search(self, query_embedding: np.ndarray, k: int = 5, min_similarity: float = 0.7) -&gt; list[Chunk]:\n        \"\"\"Search for similar vectors.\n\n        Args:\n            query_embedding: Query embedding vector\n            k: Number of results to return\n            min_similarity: Minimum similarity threshold\n\n        Returns:\n            List of Chunk objects for the most similar vectors\n        \"\"\"\n        # Convert similarity threshold to distance threshold based on distance metric\n        if self.distance == Distance.COSINE:\n            # For cosine, similarity of 0.7 means distance of 0.3\n            score_threshold = 1 - min_similarity\n        elif self.distance == Distance.DOT:\n            # For dot product, higher is more similar\n            score_threshold = min_similarity\n        else:  # Euclidean\n            # For Euclidean, lower is more similar, but there's no direct conversion\n            # Using a heuristic: similarity of 0.7 means distance of ~0.5\n            score_threshold = (1 - min_similarity) * 2\n\n        # Search in Qdrant\n        search_result = self.client.search(\n            collection_name=self.collection_name,\n            query_vector=query_embedding.tolist(),\n            limit=k,\n            score_threshold=score_threshold,\n        )\n\n        # Convert results to Chunks\n        chunks = []\n        for result in search_result:\n            # Extract data from the result\n            payload = result.payload\n            text = payload.get(\"text\", \"\")\n            metadata = payload.get(\"metadata\", {})\n            content_hash = payload.get(\"content_hash\", \"\")\n            cluster_id = payload.get(\"cluster_id\")\n\n            # Create embedding from the stored vector\n            # Note: Qdrant doesn't return the vector by default, so we need to retrieve it separately\n            # This is a limitation of this implementation\n            embedding = np.array([])\n\n            # Create and add the chunk\n            chunk = Chunk(\n                text=text,\n                embedding=embedding,\n                metadata=metadata,\n                content_hash=content_hash,\n                cluster_id=cluster_id,\n            )\n            chunks.append(chunk)\n\n        return chunks\n\n    def save(self) -&gt; bytes:\n        \"\"\"Save the vector store to disk.\n\n        Returns:\n            Serialized state of the vector store\n        \"\"\"\n        # We only need to save the configuration, as the data is stored in Qdrant\n        state = {\n            \"collection_name\": self.collection_name,\n            \"embedding_size\": self.embedding_size,\n            \"distance\": self.distance.name,\n            \"chunks\": self.chunks,\n        }\n        return pickle.dumps(state)\n\n    def load(self, data: bytes) -&gt; 'QdrantVectorStore':\n        \"\"\"Load the vector store from disk.\n\n        Args:\n            data: Serialized state of the vector store\n\n        Returns:\n            Loaded vector store\n        \"\"\"\n        state = pickle.loads(data)\n\n        # Update configuration\n        self.collection_name = state.get(\"collection_name\", self.collection_name)\n        self.embedding_size = state.get(\"embedding_size\", self.embedding_size)\n\n        # Convert distance string back to enum if needed\n        distance_name = state.get(\"distance\")\n        if distance_name:\n            self.distance = getattr(Distance, distance_name, self.distance)\n\n        # Load chunks\n        self.chunks = state.get(\"chunks\", [])\n\n        # Ensure collection exists with correct settings\n        self._ensure_collection_exists()\n\n        return self\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear all data from the store.\"\"\"\n        # Delete the collection if it exists\n        if self.client.collection_exists(self.collection_name):\n            self.client.delete_collection(self.collection_name)\n\n        # Recreate the collection\n        self._ensure_collection_exists()\n\n        # Clear local cache\n        self.chunks = []\n\n    def rebuild_index(self) -&gt; None:\n        \"\"\"Rebuild the index if needed.\n\n        For Qdrant, this is a no-op as the index is maintained automatically.\n        \"\"\"\n        pass\n\n    def get_by_content_hash(self, content_hash: str) -&gt; list[Chunk]:\n        \"\"\"Retrieve chunks by content hash.\n\n        Args:\n            content_hash: Content hash to search for\n\n        Returns:\n            List of Chunk objects with matching content hash\n        \"\"\"\n        # Create filter for content hash\n        content_filter = Filter(\n            must=[\n                FieldCondition(\n                    key=\"content_hash\",\n                    match=MatchValue(value=content_hash),\n                ),\n            ],\n        )\n\n        # Search in Qdrant\n        search_result = self.client.scroll(\n            collection_name=self.collection_name,\n            filter=content_filter,\n            limit=100,  # Adjust as needed\n            with_vectors=True,  # Request vectors to be returned\n        )\n\n        # Convert results to Chunks\n        chunks = []\n        for point in search_result[0]:\n            # Extract data from the result\n            payload = point.payload\n            text = payload.get(\"text\", \"\")\n            metadata = payload.get(\"metadata\", {})\n            content_hash = payload.get(\"content_hash\", \"\")\n            cluster_id = payload.get(\"cluster_id\")\n\n            # Get the embedding if available\n            embedding = np.array(point.vector) if hasattr(point, \"vector\") and point.vector else np.array([])\n\n            # Create and add the chunk\n            chunk = Chunk(\n                text=text,\n                embedding=embedding,\n                metadata=metadata,\n                content_hash=content_hash,\n                cluster_id=cluster_id,\n            )\n            chunks.append(chunk)\n\n        return chunks\n\n    def get_by_cluster_id(self, cluster_id: int) -&gt; list[Chunk]:\n        \"\"\"Retrieve chunks by cluster ID.\n\n        Args:\n            cluster_id: Cluster ID to search for\n\n        Returns:\n            List of Chunk objects with matching cluster ID\n        \"\"\"\n        # Create filter for cluster ID\n        cluster_filter = Filter(\n            must=[\n                FieldCondition(\n                    key=\"cluster_id\",\n                    match=MatchValue(value=cluster_id),\n                ),\n            ],\n        )\n\n        # Search in Qdrant\n        search_result = self.client.scroll(\n            collection_name=self.collection_name,\n            filter=cluster_filter,\n            limit=100,  # Adjust as needed\n            with_vectors=True,  # Request vectors to be returned\n        )\n\n        # Convert results to Chunks\n        chunks = []\n        for point in search_result[0]:\n            # Extract data from the result\n            payload = point.payload\n            text = payload.get(\"text\", \"\")\n            metadata = payload.get(\"metadata\", {})\n            content_hash = payload.get(\"content_hash\", \"\")\n            cluster_id = payload.get(\"cluster_id\")\n\n            # Get the embedding if available\n            embedding = np.array(point.vector) if hasattr(point, \"vector\") and point.vector else np.array([])\n\n            # Create and add the chunk\n            chunk = Chunk(\n                text=text,\n                embedding=embedding,\n                metadata=metadata,\n                content_hash=content_hash,\n                cluster_id=cluster_id,\n            )\n            chunks.append(chunk)\n\n        return chunks\n\n    def count(self) -&gt; int:\n        \"\"\"Get the number of vectors in the store.\n\n        Returns:\n            Number of vectors in the store\n        \"\"\"\n        # Get collection info\n        collection_info = self.client.get_collection(self.collection_name)\n\n        # Return vector count\n        return collection_info.vectors_count\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the number of vectors in the store.\n\n        Returns:\n            Number of vectors in the store\n        \"\"\"\n        return self.count()\n</code></pre> <code>__init__(collection_name='default_collection', location=':memory:', url=None, port=6333, grpc_port=6334, prefer_grpc=False, https=None, api_key=None, timeout=None, host=None, path=None, embedding_size=768, distance='Cosine', **kwargs)</code> \u00b6 <p>Initialize the QdrantVectorStore.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Name of the collection to use in Qdrant</p> <code>'default_collection'</code> <code>location</code> <code>str | None</code> <p>If \":memory:\" - use in-memory Qdrant instance.       If None - use parameters from url or host/port.</p> <code>':memory:'</code> <code>url</code> <code>str | None</code> <p>URL of the Qdrant server (e.g., \"http://localhost:6333\")</p> <code>None</code> <code>port</code> <code>int</code> <p>Port of the REST API interface</p> <code>6333</code> <code>grpc_port</code> <code>int</code> <p>Port of the gRPC interface</p> <code>6334</code> <code>prefer_grpc</code> <code>bool</code> <p>If true - use gPRC interface whenever possible</p> <code>False</code> <code>https</code> <code>bool | None</code> <p>If true - use HTTPS(SSL) protocol</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for authentication in Qdrant Cloud</p> <code>None</code> <code>timeout</code> <code>int | None</code> <p>Timeout for REST and gRPC API requests</p> <code>None</code> <code>host</code> <code>str | None</code> <p>Host name of Qdrant service</p> <code>None</code> <code>path</code> <code>str | None</code> <p>Persistence path for local Qdrant</p> <code>None</code> <code>embedding_size</code> <code>int</code> <p>Size of the embedding vectors</p> <code>768</code> <code>distance</code> <code>str</code> <p>Distance function to use (\"Cosine\", \"Euclid\", or \"Dot\")</p> <code>'Cosine'</code> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/qdrant_store.py</code> <pre><code>def __init__(\n    self,\n    collection_name: str = \"default_collection\",\n    location: str | None = \":memory:\",  # Use in-memory Qdrant by default\n    url: str | None = None,\n    port: int = 6333,\n    grpc_port: int = 6334,\n    prefer_grpc: bool = False,\n    https: bool | None = None,\n    api_key: str | None = None,\n    timeout: int | None = None,\n    host: str | None = None,\n    path: str | None = None,\n    embedding_size: int = 768,\n    distance: str = \"Cosine\",\n    **kwargs,\n):\n    \"\"\"Initialize the QdrantVectorStore.\n\n    Args:\n        collection_name: Name of the collection to use in Qdrant\n        location: If \":memory:\" - use in-memory Qdrant instance.\n                  If None - use parameters from url or host/port.\n        url: URL of the Qdrant server (e.g., \"http://localhost:6333\")\n        port: Port of the REST API interface\n        grpc_port: Port of the gRPC interface\n        prefer_grpc: If true - use gPRC interface whenever possible\n        https: If true - use HTTPS(SSL) protocol\n        api_key: API key for authentication in Qdrant Cloud\n        timeout: Timeout for REST and gRPC API requests\n        host: Host name of Qdrant service\n        path: Persistence path for local Qdrant\n        embedding_size: Size of the embedding vectors\n        distance: Distance function to use (\"Cosine\", \"Euclid\", or \"Dot\")\n    \"\"\"\n    if not QDRANT_AVAILABLE:\n        raise ImportError(\n            \"Qdrant client is not available. Please install it with: pip install qdrant-client\"\n        )\n\n    self.collection_name = collection_name\n    self.embedding_size = embedding_size\n\n    # Map distance string to Qdrant Distance enum\n    distance_map = {\n        \"Cosine\": Distance.COSINE,\n        \"Euclid\": Distance.EUCLID,\n        \"Dot\": Distance.DOT,\n    }\n    self.distance = distance_map.get(distance, Distance.COSINE)\n\n    # Initialize Qdrant client\n    self.client = QdrantClient(\n        location=location,\n        url=url,\n        port=port,\n        grpc_port=grpc_port,\n        prefer_grpc=prefer_grpc,\n        https=https,\n        api_key=api_key,\n        timeout=timeout,\n        host=host,\n        path=path,\n    )\n\n    # Create collection if it doesn't exist\n    self._ensure_collection_exists()\n\n    # Keep a local cache of chunks for faster access\n    self.chunks = []\n</code></pre> <code>__len__()</code> \u00b6 <p>Get the number of vectors in the store.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of vectors in the store</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/qdrant_store.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the number of vectors in the store.\n\n    Returns:\n        Number of vectors in the store\n    \"\"\"\n    return self.count()\n</code></pre> <code>add_embeddings(embeddings, chunks)</code> \u00b6 <p>Add embeddings and their corresponding chunks to the store.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>ndarray</code> <p>Numpy array of embeddings</p> required <code>chunks</code> <code>list[Chunk]</code> <p>List of Chunk objects corresponding to the embeddings</p> required Source code in <code>toolboxv2/mods/isaa/base/VectorStores/qdrant_store.py</code> <pre><code>def add_embeddings(self, embeddings: np.ndarray, chunks: list[Chunk]) -&gt; None:\n    \"\"\"Add embeddings and their corresponding chunks to the store.\n\n    Args:\n        embeddings: Numpy array of embeddings\n        chunks: List of Chunk objects corresponding to the embeddings\n    \"\"\"\n    if len(embeddings) == 0 or len(chunks) == 0:\n        return\n\n    # Prepare points for Qdrant\n    points = []\n    for i, (embedding, chunk) in enumerate(zip(embeddings, chunks, strict=False)):\n        # Generate a UUID for the point if not already present\n        point_id = str(uuid.uuid4())\n\n        # Create payload from chunk\n        payload = {\n            \"text\": chunk.text,\n            \"metadata\": chunk.metadata,\n            \"content_hash\": chunk.content_hash,\n        }\n\n        # Add cluster_id if it exists\n        if chunk.cluster_id is not None:\n            payload[\"cluster_id\"] = chunk.cluster_id\n\n        # Create point\n        point = PointStruct(\n            id=point_id,\n            vector=embedding.tolist(),\n            payload=payload,\n        )\n\n        points.append(point)\n\n        # Add to local cache\n        self.chunks.append(chunk)\n\n    # Upsert points to Qdrant\n    if len(points) &lt; 1000:\n        self.client.upsert(\n            collection_name=self.collection_name,\n            points=points,\n            wait=True\n        )\n    else:\n        for i in range(0, len(points), 1000):\n            self.client.upsert(\n                collection_name=self.collection_name,\n                points=points[i:i+1000],\n                wait=False,\n            )\n</code></pre> <code>clear()</code> \u00b6 <p>Clear all data from the store.</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/qdrant_store.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear all data from the store.\"\"\"\n    # Delete the collection if it exists\n    if self.client.collection_exists(self.collection_name):\n        self.client.delete_collection(self.collection_name)\n\n    # Recreate the collection\n    self._ensure_collection_exists()\n\n    # Clear local cache\n    self.chunks = []\n</code></pre> <code>count()</code> \u00b6 <p>Get the number of vectors in the store.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of vectors in the store</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/qdrant_store.py</code> <pre><code>def count(self) -&gt; int:\n    \"\"\"Get the number of vectors in the store.\n\n    Returns:\n        Number of vectors in the store\n    \"\"\"\n    # Get collection info\n    collection_info = self.client.get_collection(self.collection_name)\n\n    # Return vector count\n    return collection_info.vectors_count\n</code></pre> <code>get_by_cluster_id(cluster_id)</code> \u00b6 <p>Retrieve chunks by cluster ID.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_id</code> <code>int</code> <p>Cluster ID to search for</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>List of Chunk objects with matching cluster ID</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/qdrant_store.py</code> <pre><code>def get_by_cluster_id(self, cluster_id: int) -&gt; list[Chunk]:\n    \"\"\"Retrieve chunks by cluster ID.\n\n    Args:\n        cluster_id: Cluster ID to search for\n\n    Returns:\n        List of Chunk objects with matching cluster ID\n    \"\"\"\n    # Create filter for cluster ID\n    cluster_filter = Filter(\n        must=[\n            FieldCondition(\n                key=\"cluster_id\",\n                match=MatchValue(value=cluster_id),\n            ),\n        ],\n    )\n\n    # Search in Qdrant\n    search_result = self.client.scroll(\n        collection_name=self.collection_name,\n        filter=cluster_filter,\n        limit=100,  # Adjust as needed\n        with_vectors=True,  # Request vectors to be returned\n    )\n\n    # Convert results to Chunks\n    chunks = []\n    for point in search_result[0]:\n        # Extract data from the result\n        payload = point.payload\n        text = payload.get(\"text\", \"\")\n        metadata = payload.get(\"metadata\", {})\n        content_hash = payload.get(\"content_hash\", \"\")\n        cluster_id = payload.get(\"cluster_id\")\n\n        # Get the embedding if available\n        embedding = np.array(point.vector) if hasattr(point, \"vector\") and point.vector else np.array([])\n\n        # Create and add the chunk\n        chunk = Chunk(\n            text=text,\n            embedding=embedding,\n            metadata=metadata,\n            content_hash=content_hash,\n            cluster_id=cluster_id,\n        )\n        chunks.append(chunk)\n\n    return chunks\n</code></pre> <code>get_by_content_hash(content_hash)</code> \u00b6 <p>Retrieve chunks by content hash.</p> <p>Parameters:</p> Name Type Description Default <code>content_hash</code> <code>str</code> <p>Content hash to search for</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>List of Chunk objects with matching content hash</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/qdrant_store.py</code> <pre><code>def get_by_content_hash(self, content_hash: str) -&gt; list[Chunk]:\n    \"\"\"Retrieve chunks by content hash.\n\n    Args:\n        content_hash: Content hash to search for\n\n    Returns:\n        List of Chunk objects with matching content hash\n    \"\"\"\n    # Create filter for content hash\n    content_filter = Filter(\n        must=[\n            FieldCondition(\n                key=\"content_hash\",\n                match=MatchValue(value=content_hash),\n            ),\n        ],\n    )\n\n    # Search in Qdrant\n    search_result = self.client.scroll(\n        collection_name=self.collection_name,\n        filter=content_filter,\n        limit=100,  # Adjust as needed\n        with_vectors=True,  # Request vectors to be returned\n    )\n\n    # Convert results to Chunks\n    chunks = []\n    for point in search_result[0]:\n        # Extract data from the result\n        payload = point.payload\n        text = payload.get(\"text\", \"\")\n        metadata = payload.get(\"metadata\", {})\n        content_hash = payload.get(\"content_hash\", \"\")\n        cluster_id = payload.get(\"cluster_id\")\n\n        # Get the embedding if available\n        embedding = np.array(point.vector) if hasattr(point, \"vector\") and point.vector else np.array([])\n\n        # Create and add the chunk\n        chunk = Chunk(\n            text=text,\n            embedding=embedding,\n            metadata=metadata,\n            content_hash=content_hash,\n            cluster_id=cluster_id,\n        )\n        chunks.append(chunk)\n\n    return chunks\n</code></pre> <code>load(data)</code> \u00b6 <p>Load the vector store from disk.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Serialized state of the vector store</p> required <p>Returns:</p> Type Description <code>QdrantVectorStore</code> <p>Loaded vector store</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/qdrant_store.py</code> <pre><code>def load(self, data: bytes) -&gt; 'QdrantVectorStore':\n    \"\"\"Load the vector store from disk.\n\n    Args:\n        data: Serialized state of the vector store\n\n    Returns:\n        Loaded vector store\n    \"\"\"\n    state = pickle.loads(data)\n\n    # Update configuration\n    self.collection_name = state.get(\"collection_name\", self.collection_name)\n    self.embedding_size = state.get(\"embedding_size\", self.embedding_size)\n\n    # Convert distance string back to enum if needed\n    distance_name = state.get(\"distance\")\n    if distance_name:\n        self.distance = getattr(Distance, distance_name, self.distance)\n\n    # Load chunks\n    self.chunks = state.get(\"chunks\", [])\n\n    # Ensure collection exists with correct settings\n    self._ensure_collection_exists()\n\n    return self\n</code></pre> <code>rebuild_index()</code> \u00b6 <p>Rebuild the index if needed.</p> <p>For Qdrant, this is a no-op as the index is maintained automatically.</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/qdrant_store.py</code> <pre><code>def rebuild_index(self) -&gt; None:\n    \"\"\"Rebuild the index if needed.\n\n    For Qdrant, this is a no-op as the index is maintained automatically.\n    \"\"\"\n    pass\n</code></pre> <code>save()</code> \u00b6 <p>Save the vector store to disk.</p> <p>Returns:</p> Type Description <code>bytes</code> <p>Serialized state of the vector store</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/qdrant_store.py</code> <pre><code>def save(self) -&gt; bytes:\n    \"\"\"Save the vector store to disk.\n\n    Returns:\n        Serialized state of the vector store\n    \"\"\"\n    # We only need to save the configuration, as the data is stored in Qdrant\n    state = {\n        \"collection_name\": self.collection_name,\n        \"embedding_size\": self.embedding_size,\n        \"distance\": self.distance.name,\n        \"chunks\": self.chunks,\n    }\n    return pickle.dumps(state)\n</code></pre> <code>search(query_embedding, k=5, min_similarity=0.7)</code> \u00b6 <p>Search for similar vectors.</p> <p>Parameters:</p> Name Type Description Default <code>query_embedding</code> <code>ndarray</code> <p>Query embedding vector</p> required <code>k</code> <code>int</code> <p>Number of results to return</p> <code>5</code> <code>min_similarity</code> <code>float</code> <p>Minimum similarity threshold</p> <code>0.7</code> <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>List of Chunk objects for the most similar vectors</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/qdrant_store.py</code> <pre><code>def search(self, query_embedding: np.ndarray, k: int = 5, min_similarity: float = 0.7) -&gt; list[Chunk]:\n    \"\"\"Search for similar vectors.\n\n    Args:\n        query_embedding: Query embedding vector\n        k: Number of results to return\n        min_similarity: Minimum similarity threshold\n\n    Returns:\n        List of Chunk objects for the most similar vectors\n    \"\"\"\n    # Convert similarity threshold to distance threshold based on distance metric\n    if self.distance == Distance.COSINE:\n        # For cosine, similarity of 0.7 means distance of 0.3\n        score_threshold = 1 - min_similarity\n    elif self.distance == Distance.DOT:\n        # For dot product, higher is more similar\n        score_threshold = min_similarity\n    else:  # Euclidean\n        # For Euclidean, lower is more similar, but there's no direct conversion\n        # Using a heuristic: similarity of 0.7 means distance of ~0.5\n        score_threshold = (1 - min_similarity) * 2\n\n    # Search in Qdrant\n    search_result = self.client.search(\n        collection_name=self.collection_name,\n        query_vector=query_embedding.tolist(),\n        limit=k,\n        score_threshold=score_threshold,\n    )\n\n    # Convert results to Chunks\n    chunks = []\n    for result in search_result:\n        # Extract data from the result\n        payload = result.payload\n        text = payload.get(\"text\", \"\")\n        metadata = payload.get(\"metadata\", {})\n        content_hash = payload.get(\"content_hash\", \"\")\n        cluster_id = payload.get(\"cluster_id\")\n\n        # Create embedding from the stored vector\n        # Note: Qdrant doesn't return the vector by default, so we need to retrieve it separately\n        # This is a limitation of this implementation\n        embedding = np.array([])\n\n        # Create and add the chunk\n        chunk = Chunk(\n            text=text,\n            embedding=embedding,\n            metadata=metadata,\n            content_hash=content_hash,\n            cluster_id=cluster_id,\n        )\n        chunks.append(chunk)\n\n    return chunks\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras","title":"<code>extras</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.adapter","title":"<code>adapter</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.adapter--litellm-llm-interface-module","title":"LiteLLM LLM Interface Module","text":"<p>This module provides interfaces for interacting with LiteLLM's language models, including text generation and embedding capabilities.</p> <p>Author: Lightrag Team Created: 2025-02-04 License: MIT License Version: 1.0.0</p> <p>Change Log: - 1.0.0 (2025-02-04): Initial LiteLLM release     * Ported OpenAI logic to use litellm async client     * Updated error types and environment variable names     * Preserved streaming and embedding support</p> Dependencies <ul> <li>litellm</li> <li>numpy</li> <li>pipmaster</li> <li>Python &gt;= 3.10</li> </ul> Usage <p>from llm_interfaces.litellm import litellm_complete, litellm_embed</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.adapter.litellm_complete","title":"<code>litellm_complete(prompt, system_prompt=None, history_messages=None, keyword_extraction=False, model_name='groq/gemma2-9b-it', **kwargs)</code>  <code>async</code>","text":"<p>Public completion interface using the model name specified in the global configuration. Optionally extracts keywords if requested.</p> Source code in <code>toolboxv2/mods/isaa/extras/adapter.py</code> <pre><code>async def litellm_complete(\n    prompt, system_prompt=None, history_messages=None, keyword_extraction=False, model_name = \"groq/gemma2-9b-it\", **kwargs\n) -&gt; str | AsyncIterator[str]:\n    \"\"\"\n    Public completion interface using the model name specified in the global configuration.\n    Optionally extracts keywords if requested.\n    \"\"\"\n    if history_messages is None:\n        history_messages = []\n    # Check and set response format for keyword extraction if needed\n    keyword_extraction_flag = kwargs.pop(\"keyword_extraction\", None)\n    if keyword_extraction_flag:\n        kwargs[\"response_format\"] = \"json\"\n     # kwargs[\"hashing_kv\"].global_config[\"llm_model_name\"]\n\n    return await litellm_complete_if_cache(\n        model_name,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        **kwargs,\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.adapter.litellm_complete_if_cache","title":"<code>litellm_complete_if_cache(model, prompt, system_prompt=None, history_messages=None, base_url=None, api_key=None, **kwargs)</code>  <code>async</code>","text":"<p>Core function to query the LiteLLM model. It builds the message context, invokes the completion API, and returns either a complete result string or an async iterator for streaming responses.</p> Source code in <code>toolboxv2/mods/isaa/extras/adapter.py</code> <pre><code>@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n    retry=retry_if_exception_type((RateLimitError, Timeout, APIConnectionError)),\n)\nasync def litellm_complete_if_cache(\n    model,\n    prompt,\n    system_prompt=None,\n    history_messages=None,\n    base_url=None,\n    api_key=None,\n    **kwargs,\n) -&gt; str | AsyncIterator[str]:\n    \"\"\"\n    Core function to query the LiteLLM model. It builds the message context,\n    invokes the completion API, and returns either a complete result string or\n    an async iterator for streaming responses.\n    \"\"\"\n    # Set the API key if provided\n    if api_key:\n        os.environ[\"LITELLM_API_KEY\"] = api_key\n\n    # Remove internal keys not needed for the client call\n    kwargs.pop(\"hashing_kv\", None)\n    kwargs.pop(\"keyword_extraction\", None)\n\n    fallbacks_ = kwargs.pop(\"fallbacks\", [])\n    # Build the messages list from system prompt, conversation history, and the new prompt\n    messages = []\n    if system_prompt:\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\n    if history_messages is not None:\n        messages.extend(history_messages)\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    # Log query details for debugging purposes\n    try:\n        # Depending on the response format, choose the appropriate API call\n        if \"response_format\" in kwargs:\n            response = await acompletion(\n                model=model, messages=messages,\n                fallbacks=fallbacks_+os.getenv(\"FALLBACKS_MODELS\", '').split(','),\n                **kwargs\n            )\n        else:\n            response = await acompletion(\n                model=model, messages=messages,\n                fallbacks=os.getenv(\"FALLBACKS_MODELS\", '').split(','),\n                **kwargs\n            )\n    except Exception as e:\n        print(e)\n        get_logger().error(f\"Failed to litellm memory work {e}\")\n        return \"\"\n\n    # Check if the response is a streaming response (i.e. an async iterator)\n    if hasattr(response, \"__aiter__\"):\n\n        async def inner():\n            async for chunk in response:\n                # Assume LiteLLM response structure is similar to OpenAI's\n                content = chunk.choices[0].delta.content\n                if content is None:\n                    continue\n                yield content\n\n        return inner()\n    else:\n        # Non-streaming: extract and return the full content string\n\n        content = response.choices[0].message.content\n        if content is None:\n            content = response.choices[0].message.tool_calls[0].function.arguments\n        return content\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.adapter.litellm_embed","title":"<code>litellm_embed(texts, model='gemini/text-embedding-004', base_url=None, api_key=None)</code>  <code>async</code>","text":"<p>Generates embeddings for the given list of texts using LiteLLM.</p> Source code in <code>toolboxv2/mods/isaa/extras/adapter.py</code> <pre><code>@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=60),\n    retry=retry_if_exception_type((RateLimitError, Timeout, APIConnectionError)),\n)\nasync def litellm_embed(\n    texts: list[str],\n    model: str = \"gemini/text-embedding-004\",\n    base_url: str = None,\n    api_key: str = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Generates embeddings for the given list of texts using LiteLLM.\n    \"\"\"\n    response = await litellm.aembedding(\n        model=model, input=texts,\n        # encoding_format=\"float\"\n    )\n    return np.array([dp.embedding for dp in response.data])\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.filter","title":"<code>filter</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.filter.filter_relevant_texts","title":"<code>filter_relevant_texts(query, texts, fuzzy_threshold=70, semantic_threshold=0.75, model=None)</code>","text":"<p>Filters a list of texts based on their relevance to the query. It first uses a fuzzy matching score and, if that score is below the threshold, it then checks the semantic similarity.</p> <p>:param query: The query string. :param texts: List of page texts. :param fuzzy_threshold: Fuzzy matching score threshold (0-100). :param semantic_threshold: Semantic similarity threshold (0.0-1.0). :param model: A preloaded SentenceTransformer model (if None, one will be loaded). :return: Filtered list of texts deemed relevant.</p> Source code in <code>toolboxv2/mods/isaa/extras/filter.py</code> <pre><code>def filter_relevant_texts(query: str,\n                          texts: list[str],\n                          fuzzy_threshold: int = 70,\n                          semantic_threshold: float = 0.75,\n                          model = None) -&gt; list[str]:\n    \"\"\"\n    Filters a list of texts based on their relevance to the query.\n    It first uses a fuzzy matching score and, if that score is below the threshold,\n    it then checks the semantic similarity.\n\n    :param query: The query string.\n    :param texts: List of page texts.\n    :param fuzzy_threshold: Fuzzy matching score threshold (0-100).\n    :param semantic_threshold: Semantic similarity threshold (0.0-1.0).\n    :param model: A preloaded SentenceTransformer model (if None, one will be loaded).\n    :return: Filtered list of texts deemed relevant.\n    \"\"\"\n    try:\n        from rapidfuzz import fuzz\n    except Exception:\n        os.system([sys.executable, '-m', 'pip', 'install', 'RapidFuzz'])\n        from rapidfuzz import fuzz\n    try:\n        from sentence_transformers import SentenceTransformer, util\n    except Exception:\n        os.system([sys.executable, '-m', 'pip', 'install', 'sentence-transformers'])\n        from sentence_transformers import SentenceTransformer, util\n\n    if model is None:\n        # For efficiency, consider pre-loading this model outside the function.\n        model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\n    # Pre-compute query embedding for the semantic check:\n    query_embedding = model.encode(query, convert_to_tensor=True)\n\n    relevant_texts = []\n    for text in texts:\n        # --- Fuzzy Keyword Filtering ---\n        fuzzy_score = fuzz.partial_ratio(query.lower(), text.lower())\n        if fuzzy_score &gt;= fuzzy_threshold:\n            relevant_texts.append(text)\n        else:\n            # --- Semantic Similarity Filtering ---\n            text_embedding = model.encode(text, convert_to_tensor=True)\n            similarity = util.pytorch_cos_sim(query_embedding, text_embedding).item()\n            if similarity &gt;= semantic_threshold:\n                relevant_texts.append(text)\n    return relevant_texts\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.modes","title":"<code>modes</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.modes.generate_prompt","title":"<code>generate_prompt(subject, context='', additional_requirements=None)</code>","text":"<p>Generates a prompt based on the given subject, with optional context and additional requirements.</p> <p>Parameters: - subject (str): The main subject for the prompt. - context (str): Optional additional context to tailor the prompt. - additional_requirements (Dict[str, Any]): Optional additional parameters or requirements for the prompt.</p> <p>Returns: - str: A crafted prompt.</p> Source code in <code>toolboxv2/mods/isaa/extras/modes.py</code> <pre><code>def generate_prompt(subject: str, context: str = \"\", additional_requirements: dict[str, Any] = None) -&gt; str:\n    \"\"\"\n    Generates a prompt based on the given subject, with optional context and additional requirements.\n\n    Parameters:\n    - subject (str): The main subject for the prompt.\n    - context (str): Optional additional context to tailor the prompt.\n    - additional_requirements (Dict[str, Any]): Optional additional parameters or requirements for the prompt.\n\n    Returns:\n    - str: A crafted prompt.\n    \"\"\"\n    prompt = f\"Based on the subject '{subject}', with the context '{context}', generate a clear and precise instruction.\"\n    if additional_requirements:\n        prompt += f\" Consider the following requirements: {additional_requirements}.\"\n    return prompt\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.lnn","title":"<code>lnn</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.lnn.test","title":"<code>test</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.lnn.test.SuperLoss","title":"<code>SuperLoss</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>toolboxv2/mods/isaa/lnn/test.py</code> <pre><code>class SuperLoss(nn.Module):\n    def __init__(self, name=\"mse\"):\n        super().__init__()\n        self.loss_functions = {\n            'mse': nn.MSELoss(),\n            'mae': nn.L1Loss(),\n            'cross_entropy': nn.CrossEntropyLoss(),\n            'bce': nn.BCELoss(),\n            'bce2': nn.BCEWithLogitsLoss(),\n            'huber': nn.HuberLoss()\n        }\n        self.current_loss = name\n\n    def forward(self, predictions, targets):\n\n        if predictions.shape != targets.shape:\n            # print(f\"Forward Loss: shapes mismatch: {predictions.shape} {targets.shape}\")\n            targets = targets.view(predictions.shape)\n        return self.loss_functions[self.current_loss](predictions, targets)\n\n    def set_loss(self, loss_name):\n        if loss_name in self.loss_functions:\n            self.current_loss = loss_name\n        else:\n            raise ValueError(f\"Unbekannte Loss-Funktion: {loss_name}\")\n\n    def add_custom_loss(self, name, loss_function):\n        self.loss_functions[name] = loss_function\n\n    @staticmethod\n    def select_loss(data, task):\n        # Hier implementieren wir eine einfache Heuristik zur Auswahl der Loss-Funktion\n        if task == 'regression':\n            return 'mse'\n        elif task == 'classification':\n            if len(data.shape) == 2 and data.shape[1] &gt; 1:\n                return 'cross_entropy'\n            else:\n                return 'bce'\n        elif task == 'ranking':\n            return 'mse'  # Hier k\u00f6nnten Sie eine spezielle Ranking-Loss hinzuf\u00fcgen\n        else:\n            return 'mse'  # Standardm\u00e4\u00dfig MSE verwenden\n\n    def super_function(self, data, task):\n        optimal_loss = self.select_loss(data, task)\n        self.set_loss(optimal_loss)\n        return optimal_loss\n\n    def batch_compute_loss(self, batch_predictions, batch_targets):\n        \"\"\"\n        Berechnet den Loss f\u00fcr einen Batch von Vorhersagen und Zielen.\n\n        :param batch_predictions: Ein Tensor der Form (batch_size, ...) mit den Vorhersagen\n        :param batch_targets: Ein Tensor der Form (batch_size, ...) mit den Zielen\n        :return: Ein Tensor der Form (batch_size,) mit den Loss-Werten f\u00fcr jeden Datenpunkt im Batch\n        \"\"\"\n        batch_loss = self.forward(batch_predictions, batch_targets)\n\n        # F\u00fcr einige Loss-Funktionen m\u00fcssen wir m\u00f6glicherweise die Dimensionen reduzieren\n        if self.current_loss in ['cross_entropy', 'bce']:\n            return batch_loss\n        else:\n            return torch.mean(batch_loss, dim=tuple(range(1, len(batch_loss.shape))))\n</code></pre> <code>batch_compute_loss(batch_predictions, batch_targets)</code> \u00b6 <p>Berechnet den Loss f\u00fcr einen Batch von Vorhersagen und Zielen.</p> <p>:param batch_predictions: Ein Tensor der Form (batch_size, ...) mit den Vorhersagen :param batch_targets: Ein Tensor der Form (batch_size, ...) mit den Zielen :return: Ein Tensor der Form (batch_size,) mit den Loss-Werten f\u00fcr jeden Datenpunkt im Batch</p> Source code in <code>toolboxv2/mods/isaa/lnn/test.py</code> <pre><code>def batch_compute_loss(self, batch_predictions, batch_targets):\n    \"\"\"\n    Berechnet den Loss f\u00fcr einen Batch von Vorhersagen und Zielen.\n\n    :param batch_predictions: Ein Tensor der Form (batch_size, ...) mit den Vorhersagen\n    :param batch_targets: Ein Tensor der Form (batch_size, ...) mit den Zielen\n    :return: Ein Tensor der Form (batch_size,) mit den Loss-Werten f\u00fcr jeden Datenpunkt im Batch\n    \"\"\"\n    batch_loss = self.forward(batch_predictions, batch_targets)\n\n    # F\u00fcr einige Loss-Funktionen m\u00fcssen wir m\u00f6glicherweise die Dimensionen reduzieren\n    if self.current_loss in ['cross_entropy', 'bce']:\n        return batch_loss\n    else:\n        return torch.mean(batch_loss, dim=tuple(range(1, len(batch_loss.shape))))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.lnn.tf","title":"<code>tf</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.lnn.tf.ForwardForwardModel","title":"<code>ForwardForwardModel</code>","text":"<p>               Bases: <code>Model</code></p> Source code in <code>toolboxv2/mods/isaa/lnn/tf.py</code> <pre><code>class ForwardForwardModel(keras.Model):\n    def __init__(self, input_dim, hidden_dims, output_dim, threshold=1.0):\n        \"\"\"\n        input_dim should include the extra label dimensions (e.g. 784+10 for MNIST)\n        \"\"\"\n        super().__init__()\n        self.threshold = threshold\n        self.ff_layers = []\n        # Build hidden layers\n        for h in hidden_dims:\n            self.ff_layers.append(layers.Dense(h, activation='relu'))\n            self.ff_layers.append(layers.LayerNormalization())\n        # Final layer (linear output; the network is trained with a local objective)\n        self.ff_layers.append(layers.Dense(output_dim))\n\n    def call(self, x, return_activations=False):\n        activations = []\n        for layer in self.ff_layers:\n            x = layer(x)\n            activations.append(x)\n        if return_activations:\n            return activations\n        else:\n            return x\n</code></pre> <code>__init__(input_dim, hidden_dims, output_dim, threshold=1.0)</code> \u00b6 <p>input_dim should include the extra label dimensions (e.g. 784+10 for MNIST)</p> Source code in <code>toolboxv2/mods/isaa/lnn/tf.py</code> <pre><code>def __init__(self, input_dim, hidden_dims, output_dim, threshold=1.0):\n    \"\"\"\n    input_dim should include the extra label dimensions (e.g. 784+10 for MNIST)\n    \"\"\"\n    super().__init__()\n    self.threshold = threshold\n    self.ff_layers = []\n    # Build hidden layers\n    for h in hidden_dims:\n        self.ff_layers.append(layers.Dense(h, activation='relu'))\n        self.ff_layers.append(layers.LayerNormalization())\n    # Final layer (linear output; the network is trained with a local objective)\n    self.ff_layers.append(layers.Dense(output_dim))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.lnn.tf.LiquidStateMachine","title":"<code>LiquidStateMachine</code>","text":"<p>               Bases: <code>Layer</code></p> Source code in <code>toolboxv2/mods/isaa/lnn/tf.py</code> <pre><code>class LiquidStateMachine(layers.Layer):\n    def __init__(self, input_dim, reservoir_size, time_steps,\n                 alpha=0.9, threshold=1.0, connectivity=0.1):\n        \"\"\"\n        input_dim: number of input features\n        reservoir_size: number of spiking neurons in the reservoir\n        time_steps: number of time steps for simulation\n        alpha: leak constant\n        threshold: spiking threshold\n        connectivity: probability of a connection in the recurrent weight matrix\n        \"\"\"\n        super().__init__()\n        self.input_dim = input_dim\n        self.reservoir_size = reservoir_size\n        self.time_steps = time_steps\n        self.alpha = alpha\n        self.threshold = threshold\n        self.connectivity = connectivity\n        # Fixed random input weights\n        self.W_in = self.add_weight(\n            \"W_in\", shape=(input_dim, reservoir_size),\n            initializer=tf.random_normal_initializer(), trainable=False)\n        # Fixed random recurrent weights (sparse)\n        rec_init = tf.random.normal((reservoir_size, reservoir_size))\n        mask = tf.cast(tf.random.uniform((reservoir_size, reservoir_size)) &lt; connectivity, tf.float32)\n        rec_init = rec_init * mask\n        self.W_rec = self.add_weight(\n            \"W_rec\", shape=(reservoir_size, reservoir_size),\n            initializer=lambda shape, dtype: rec_init, trainable=False)\n\n    def call(self, inputs):\n        \"\"\"\n        inputs: spike trains of shape [batch, time_steps, input_dim]\n        Returns: a feature vector per sample (e.g., average firing rate) of shape [batch, reservoir_size]\n        \"\"\"\n        batch_size = tf.shape(inputs)[0]\n        # Initialize reservoir state (membrane potential)\n        u = tf.zeros((batch_size, self.reservoir_size))\n        # Initialize spikes (for recurrent input)\n        spikes = tf.zeros((batch_size, self.reservoir_size))\n        # Record spikes for each time step\n        spike_record = []\n        for t in range(self.time_steps):\n            x_t = inputs[:, t, :]  # [batch, input_dim]\n            # Compute input current: from external input and recurrent spikes\n            I = tf.matmul(x_t, self.W_in) + tf.matmul(spikes, self.W_rec)\n            # Update membrane potential\n            u = self.alpha * u + I\n            # Generate spikes: simple thresholding (non-differentiable; for training the readout only)\n            new_spikes = tf.cast(u &gt;= self.threshold, tf.float32)\n            # Reset the membrane potential where spikes occurred\n            u = u * (1 - new_spikes)\n            spikes = new_spikes  # update recurrent input\n            spike_record.append(new_spikes)\n        # Stack spikes over time and compute the mean firing rate\n        spike_record = tf.stack(spike_record, axis=1)  # [batch, time_steps, reservoir_size]\n        firing_rate = tf.reduce_mean(spike_record, axis=1)  # [batch, reservoir_size]\n        return firing_rate\n</code></pre> <code>__init__(input_dim, reservoir_size, time_steps, alpha=0.9, threshold=1.0, connectivity=0.1)</code> \u00b6 <p>input_dim: number of input features reservoir_size: number of spiking neurons in the reservoir time_steps: number of time steps for simulation alpha: leak constant threshold: spiking threshold connectivity: probability of a connection in the recurrent weight matrix</p> Source code in <code>toolboxv2/mods/isaa/lnn/tf.py</code> <pre><code>def __init__(self, input_dim, reservoir_size, time_steps,\n             alpha=0.9, threshold=1.0, connectivity=0.1):\n    \"\"\"\n    input_dim: number of input features\n    reservoir_size: number of spiking neurons in the reservoir\n    time_steps: number of time steps for simulation\n    alpha: leak constant\n    threshold: spiking threshold\n    connectivity: probability of a connection in the recurrent weight matrix\n    \"\"\"\n    super().__init__()\n    self.input_dim = input_dim\n    self.reservoir_size = reservoir_size\n    self.time_steps = time_steps\n    self.alpha = alpha\n    self.threshold = threshold\n    self.connectivity = connectivity\n    # Fixed random input weights\n    self.W_in = self.add_weight(\n        \"W_in\", shape=(input_dim, reservoir_size),\n        initializer=tf.random_normal_initializer(), trainable=False)\n    # Fixed random recurrent weights (sparse)\n    rec_init = tf.random.normal((reservoir_size, reservoir_size))\n    mask = tf.cast(tf.random.uniform((reservoir_size, reservoir_size)) &lt; connectivity, tf.float32)\n    rec_init = rec_init * mask\n    self.W_rec = self.add_weight(\n        \"W_rec\", shape=(reservoir_size, reservoir_size),\n        initializer=lambda shape, dtype: rec_init, trainable=False)\n</code></pre> <code>call(inputs)</code> \u00b6 <p>inputs: spike trains of shape [batch, time_steps, input_dim] Returns: a feature vector per sample (e.g., average firing rate) of shape [batch, reservoir_size]</p> Source code in <code>toolboxv2/mods/isaa/lnn/tf.py</code> <pre><code>def call(self, inputs):\n    \"\"\"\n    inputs: spike trains of shape [batch, time_steps, input_dim]\n    Returns: a feature vector per sample (e.g., average firing rate) of shape [batch, reservoir_size]\n    \"\"\"\n    batch_size = tf.shape(inputs)[0]\n    # Initialize reservoir state (membrane potential)\n    u = tf.zeros((batch_size, self.reservoir_size))\n    # Initialize spikes (for recurrent input)\n    spikes = tf.zeros((batch_size, self.reservoir_size))\n    # Record spikes for each time step\n    spike_record = []\n    for t in range(self.time_steps):\n        x_t = inputs[:, t, :]  # [batch, input_dim]\n        # Compute input current: from external input and recurrent spikes\n        I = tf.matmul(x_t, self.W_in) + tf.matmul(spikes, self.W_rec)\n        # Update membrane potential\n        u = self.alpha * u + I\n        # Generate spikes: simple thresholding (non-differentiable; for training the readout only)\n        new_spikes = tf.cast(u &gt;= self.threshold, tf.float32)\n        # Reset the membrane potential where spikes occurred\n        u = u * (1 - new_spikes)\n        spikes = new_spikes  # update recurrent input\n        spike_record.append(new_spikes)\n    # Stack spikes over time and compute the mean firing rate\n    spike_record = tf.stack(spike_record, axis=1)  # [batch, time_steps, reservoir_size]\n    firing_rate = tf.reduce_mean(spike_record, axis=1)  # [batch, reservoir_size]\n    return firing_rate\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.lnn.tf.poisson_encoder","title":"<code>poisson_encoder(x, time_steps, max_rate=1.0)</code>","text":"<p>x: [batch, features] with values in [0,1] Returns: spikes of shape [batch, time_steps, features]</p> Source code in <code>toolboxv2/mods/isaa/lnn/tf.py</code> <pre><code>def poisson_encoder(x, time_steps, max_rate=1.0):\n    \"\"\"\n    x: [batch, features] with values in [0,1]\n    Returns: spikes of shape [batch, time_steps, features]\n    \"\"\"\n    x_expanded = tf.expand_dims(x, axis=1)  # [batch, 1, features]\n    x_tiled = tf.tile(x_expanded, [1, time_steps, 1])\n    random_tensor = tf.random.uniform(tf.shape(x_tiled))\n    spikes = tf.cast(random_tensor &lt; x_tiled * max_rate, tf.float32)\n    return spikes\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.module","title":"<code>module</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.module.Tools","title":"<code>Tools</code>","text":"<p>               Bases: <code>MainTool</code>, <code>FileHandler</code></p> Source code in <code>toolboxv2/mods/isaa/module.py</code> <pre><code>class Tools(MainTool, FileHandler):\n\n    def __init__(self, app=None):\n\n        self.run_callback = None\n        self.coding_projects: dict[str, ProjectManager] = {}\n        self.pipes: dict[str, Pipeline] = {}\n        if app is None:\n            app = get_app(\"isaa-mod\")\n        self.version = version\n        self.name = \"isaa\"\n        self.Name = \"isaa\"\n        self.color = \"VIOLET2\"\n        self.config = {'controller-init': False,\n                       'agents-name-list': [],\n\n                       \"DEFAULTMODEL0\": \"ollama/llama3.1\",\n                       \"DEFAULT_AUDIO_MODEL\": \"groq/whisper-large-v3-turbo\",\n                       \"DEFAULTMODEL1\": \"ollama/llama3.1\",\n                       \"DEFAULTMODELST\": \"ollama/llama3.1\",\n                       \"DEFAULTMODEL2\": \"ollama/llama3.1\",\n                       \"DEFAULTMODELCODE\": \"ollama/llama3.1\",\n                       \"DEFAULTMODELSUMMERY\": \"ollama/llama3.1\",\n                       \"DEFAULTMODEL_LF_TOOLS\": \"ollama/llama3.1\",\n                       }\n        self.per_data = {}\n        self.agent_data = {}\n        self.keys = {\n            \"KEY\": \"key~~~~~~~\",\n            \"Config\": \"config~~~~\"\n        }\n        self.initstate = {}\n\n        extra_path = \"\"\n        if self.toolID:\n            extra_path = f\"/{self.toolID}\"\n        self.observation_term_mem_file = f\".data/{app.id}/Memory{extra_path}/observationMemory/\"\n        self.config['controller_file'] = f\".data/{app.id}{extra_path}/controller.json\"\n        self.mas_text_summaries_dict = FileCache(folder=f\".data/{app.id}/Memory{extra_path}/summaries/\")\n        self.tools = {\n            \"name\": \"isaa\",\n            \"Version\": self.show_version,\n            \"add_task\": self.add_task,\n            \"save_task\": self.save_task,\n            \"load_task\": self.load_task,\n            \"get_task\": self.get_task,\n            \"list_task\": self.list_task,\n            \"save_to_mem\": self.save_to_mem,\n            # \"mini_task\": self.mini_task_completion,\n            \"get_agent\": self.get_agent,\n            # \"run_agent\": self.run_agent,\n            \"run_task\": self.run_task,\n            \"crate_task_chain\": self.crate_task_chain,\n            \"format_class\": self.format_class,\n            \"get_memory\": self.get_memory,\n            \"get_pipe\": self.get_pipe,\n            \"run_pipe\": self.run_pipe,\n            \"rget_mode\": lambda mode: self.controller.rget(mode),\n            \"set_local_files_tools\": self.set_local_files_tools,\n        }\n        self.working_directory = os.getenv('ISAA_WORKING_PATH')\n        self.print_stream = stram_print\n        self.agent_collective_senses = False\n        self.global_stream_override = False\n        self.pipes_device = 1\n        self.lang_chain_tools_dict: dict[str, str] = {}\n        self.agent_chain = AgentChain(directory=f\".data/{app.id}{extra_path}/chains\")\n        self.agent_chain_executor = ChainTreeExecutor()\n        self.agent_chain_executor.function_runner = lambda name, **b: self.get_agent(\"self\").function_invoke(name,\n                                                                                                                   **b)\n        self.agent_chain_executor.agent_runner = lambda name, task, **k: self.run_agent(name, task, **k)\n        self.agent_memory: AISemanticMemory = f\"{app.id}{extra_path}/Memory\"\n        self.controller = ControllerManager({})\n        self.summarization_mode = 1  # 0 to 3  0 huggingface 1 text 2 opnai 3 gpt\n        self.summarization_limiter = 102000\n        self.speak = lambda x, *args, **kwargs: x\n        self.scripts = Scripts(f\".data/{app.id}{extra_path}/ScriptFile\")\n        self.ac_task = None\n        self.default_setter = None\n        self.local_files_tools = True\n        self.initialized = False\n\n        self.personality_code = ISAA0CODE\n\n        FileHandler.__init__(self, f\"isaa{extra_path.replace('/', '-')}.config\", app.id if app else __name__)\n        MainTool.__init__(self, load=self.on_start, v=self.version, tool=self.tools,\n                          name=self.name, logs=None, color=self.color, on_exit=self.on_exit)\n\n        self.fc_generators = {}\n        self.toolID = \"\"\n        MainTool.toolID = \"\"\n        self.web_search = web_search\n        self.shell_tool_function = shell_tool_function\n\n        self.print(f\"Start {self.spec}.isaa\")\n        # IsaaWebSocketUI(self)\n        # init_isaaflow_ui(self.app)\n        with Spinner(message=\"Starting module\", symbols='c'):\n            self.load_file_handler()\n            config = self.get_file_handler(self.keys[\"Config\"])\n            if config is not None:\n                if isinstance(config, str):\n                    config = json.loads(config)\n                if isinstance(config, dict):\n                    self.config = {**config, **self.config}\n\n            if self.spec == 'app':\n                self.load_keys_from_env()\n\n            if not os.path.exists(f\".data/{get_app('isaa-initIsaa').id}/Agents/\"):\n                os.mkdir(f\".data/{get_app('isaa-initIsaa').id}/Agents/\")\n            if not os.path.exists(f\".data/{get_app().id}/Memory/\"):\n                os.mkdir(f\".data/{get_app('isaa-initIsaa').id}/Memory/\")\n\n    def add_task(self, name, task):\n        self.agent_chain.add_task(name, task)\n\n    def list_task(self):\n        return str(self.agent_chain)\n\n    def remove_task(self, name):\n        return self.agent_chain.remove(name)\n\n    def save_task(self, name=None):\n        self.agent_chain.save_to_file(name)\n\n    def load_task(self, name=None):\n        self.agent_chain.load_from_file(name)\n\n    def get_task(self, name=None):\n        return self.agent_chain.get(name)\n\n    def run_task(self, task, name, sum_up=True):\n        self.agent_chain_executor.reset()\n        return self.agent_chain_executor.execute(task, self.agent_chain.get(name), sum_up=sum_up)\n\n    def crun_task(self, prompt):\n        chain_name = self.crate_task_chain(prompt)\n\n        out = self.run_task(prompt, chain_name) if chain_name else \"No chain generated\"\n\n        return out, chain_name\n\n    def crate_task_chain(self, prompt):\n\n        prompt += f\"\\n\\nAvalabel Agents: {self.config.get('agents-name-list', ['self', 'isaa'])}\"\n        prompt += f\"\\n\\nAvalabel Tools: {[f.function for f in self.get_agent('self').functions]}\"\n        prompt += f\"\\n\\nAvalabel Chains: {self.list_task()}\"\n\n        if 'TaskChainAgent' not in self.config['agents-name-list']:\n            task_chain_agent = self.get_default_agent_builder(\"code\")\n            task_chain_agent.set_amd_name(\"TaskChainAgent\")\n            tcm = self.controller.rget(TaskChainMode)\n            task_chain_agent.set_mode(tcm)\n            self.register_agent(task_chain_agent)\n\n        task_chain: TaskChain = TaskChain(**self.format_class(TaskChain, prompt, agent_name=\"TaskChainAgent\"))\n\n        self.print(f\"New TaskChain {task_chain.name} len:{len(task_chain.tasks)}\")\n\n        if task_chain and len(task_chain.tasks):\n            self.print(f\"adding : {task_chain.name}\")\n            self.agent_chain.add(task_chain.name, task_chain.model_dump().get(\"tasks\"))\n            self.agent_chain.add_discr(task_chain.name, task_chain.dis)\n        return task_chain.name\n\n    def get_augment(self, task_name=None, exclude=None):\n        return {\n            \"tools\": {},\n            \"Agents\": self.serialize_all(exclude=exclude),\n            \"customFunctions\": json.dumps(self.scripts.scripts),\n            \"tasks\": self.agent_chain.save_to_dict(task_name)\n        }\n\n    def init_from_augment(self, augment, agent_name: str or AgentBuilder = 'self', exclude=None):\n        if isinstance(agent_name, str):\n            agent = self.get_agent(agent_name)\n        elif isinstance(agent_name, AgentBuilder):\n            agent = agent_name\n        else:\n            return ValueError(f\"Invalid Type {type(agent_name)} accept ar : str and AgentProvider\")\n        a_keys = augment.keys()\n\n        if \"tools\" in a_keys:\n            tools = augment['tools']\n            print(\"tools:\", tools)\n            self.init_tools(tools, tools.get(\"tools.model\", self.config['DEFAULTMODEL_LF_TOOLS'], agent))\n            self.print(\"tools initialized\")\n\n        if \"Agents\" in a_keys:\n            agents = augment['Agents']\n            self.deserialize_all(agents)\n            self.print(\"Agents crated\")\n\n        if \"customFunctions\" in a_keys:\n            custom_functions = augment['customFunctions']\n            if isinstance(custom_functions, str):\n                custom_functions = json.loads(custom_functions)\n            if custom_functions:\n                self.scripts.scripts = custom_functions\n                self.print(\"customFunctions saved\")\n\n        if \"tasks\" in a_keys:\n            tasks = augment['tasks']\n            if isinstance(tasks, str):\n                tasks = json.loads(tasks)\n            if tasks:\n                self.agent_chain.load_from_dict(tasks)\n                self.print(\"tasks chains restored\")\n\n    def init_tools(self, tools, model_name: str, agent: Agent | None = None):  # not  in unit test\n\n\n        # tools = {  # Todo save tools to file and loade from usaage data format : and isaa_extras\n        #    \"lagChinTools\": [\"ShellTool\", \"ReadFileTool\", \"CopyFileTool\",\n        #                     \"DeleteFileTool\", \"MoveFileTool\", \"ListDirectoryTool\"],\n        #    \"huggingTools\": [],\n        #    \"Plugins\": [\"https://nla.zapier.com/.well-known/ai-plugin.json\"],\n        #    \"Custom\": [],\n        # }\n\n        if agent is None:\n            agent = self.get_agent(\"self\")\n\n        if 'Plugins' not in tools:\n            tools['Plugins'] = []\n        if 'lagChinTools' not in tools:\n            tools['lagChinTools'] = []\n        if 'huggingTools' not in tools:\n            tools['huggingTools'] = []\n\n        llm_fuctions = []\n\n        for plugin_url in set(tools['Plugins']):\n            get_logger().info(Style.BLUE(f\"Try opening plugin from : {plugin_url}\"))\n            try:\n                plugin_tool = AIPluginTool.from_plugin_url(plugin_url)\n                get_logger().info(Style.GREEN(f\"Plugin : {plugin_tool.name} loaded successfully\"))\n                plugin_tool.description += \"API Tool use request; infos :\" + plugin_tool.api_spec + \".\" + str(\n                    plugin_tool.args_schema)\n                llm_fuctions += crate_llm_function_from_langchain_tools(plugin_tool)\n                self.lang_chain_tools_dict[plugin_tool.name + \"-usage-information\"] = plugin_tool\n            except Exception as e:\n                get_logger().error(Style.RED(f\"Could not load : {plugin_url}\"))\n                get_logger().error(Style.GREEN(f\"{e}\"))\n\n        for tool in load_tools(list(set(tools['lagChinTools'])),\n                               self.get_llm_models(model_name)):\n            llm_fuctions += crate_llm_function_from_langchain_tools(tool)\n        for tool in set(tools['huggingTools']):\n            llm_fuctions += crate_llm_function_from_langchain_tools(\n                load_huggingface_tool(tool, self.config['HUGGINGFACEHUB_API_TOKEN']))\n        agent.functions += llm_fuctions\n\n    def serialize_all(self, exclude=None):\n        if exclude is None:\n            exclude = []\n        data = copy.deepcopy(self.agent_data)\n        for agent_name, agent_data in data.items():\n            for e in exclude:\n                del agent_data[e]\n            if 'taskstack' in agent_data:\n                del agent_data['taskstack']\n            if 'amd' in agent_data and 'provider' in agent_data['amd']:\n                if isinstance(agent_data['amd'].get('provider'), Enum):\n                    agent_data['amd']['provider'] = str(agent_data['amd'].get('provider').name).upper()\n            data[agent_name] = agent_data\n        return data\n\n    def deserialize_all(self, data):\n        for key, _agent_data in data.items():\n            _ = self.get_agent(key)\n\n    def init_isaa(self, name='self', build=False, only_v=False, **kwargs):\n        if self.initialized:\n            self.print(f\"Already initialized returning agent / builder name : {name}\")\n            if build:\n                return self.get_default_agent_builder(name)\n            return self.get_agent(name)\n\n        self.initialized = True\n        sys.setrecursionlimit(1500)\n\n        self.load_keys_from_env()\n\n        def helper():\n            self.agent_chain.load_from_file()\n            self.scripts.load_scripts()\n            self.config[\"controller-init\"] = True\n            return True\n\n        threading.Thread(target=helper, daemon=True).start()\n\n        with Spinner(message=\"Building Controller\", symbols='c'):\n            self.controller.init(self.config['controller_file'])\n\n        if build:\n            return self.get_agent(name)\n\n        with Spinner(message=f\"Preparing default config for Agent {name}\", symbols='c'):\n            return self.get_default_agent_builder(name)\n\n    def show_version(self):\n        self.print(\"Version: \", self.version)\n        return self.version\n\n    async def on_start(self):\n        pass\n        # init_isaaflow_ui(self.app)\n\n    def load_secrit_keys_from_env(self):\n        self.config['WOLFRAM_ALPHA_APPID'] = os.getenv('WOLFRAM_ALPHA_APPID')\n        self.config['HUGGINGFACEHUB_API_TOKEN'] = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n        self.config['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n        self.config['REPLICATE_API_TOKEN'] = os.getenv('REPLICATE_API_TOKEN')\n        self.config['IFTTTKey'] = os.getenv('IFTTTKey')\n        self.config['SERP_API_KEY'] = os.getenv('SERP_API_KEY')\n        self.config['PINECONE_API_KEY'] = os.getenv('PINECONE_API_KEY')\n        self.config['PINECONE_API_ENV'] = os.getenv('PINECONE_API_ENV')\n\n    def load_keys_from_env(self):\n        self.config['DEFAULTMODELST'] = os.getenv(\"DEFAULTMODELST\", \"ollama/llama3.1\")\n        self.config['DEFAULTMODEL0'] = os.getenv(\"DEFAULTMODEL0\", \"ollama/llama3.1\")\n        self.config['DEFAULTMODEL1'] = os.getenv(\"DEFAULTMODEL1\", \"ollama/llama3.1\")\n        self.config['DEFAULTMODEL2'] = os.getenv(\"DEFAULTMODEL2\", \"ollama/llama3.1\")\n        self.config['DEFAULTMODELCODE'] = os.getenv(\"DEFAULTMODELCODE\", \"ollama/llama3.1\")\n        self.config['DEFAULTMODELSUMMERY'] = os.getenv(\"DEFAULTMODELSUMMERY\", \"ollama/llama3.1\")\n        self.config['DEFAULTMODEL_LF_TOOLS'] = os.getenv(\"DEFAULTMODEL_LF_TOOLS\", \"ollama/llama3.1\")\n        self.config['VAULTS'] = os.getenv(\"VAULTS\")\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        self.print('Installing')\n        return construct_render(content=\"./app/0/isaa_installer/ii.html\",\n                                element_id=\"Installation\",\n                                externals=[\"/app/0/isaa_installer/ii.js\"],\n                                from_file=True)\n\n    def on_exit(self):\n\n        threading.Thread(target=self.save_to_mem, daemon=True).start()\n\n        for v in self.pipes.values():\n            v.on_exit()\n\n        self.config['augment'] = self.get_augment(exclude=['amd'])\n        del self.config['augment']['tasks']\n\n        if self.config[\"controller-init\"]:\n            self.controller.save(self.config['controller_file'])\n            self.config[\"controller-init\"] = False\n\n        for key in list(self.config.keys()):\n            if key.startswith(\"LLM-model-\"):\n                del self.config[key]\n            if key.startswith(\"agent-config-\"):\n                del self.config[key]\n            if key.endswith(\"_pipeline\"):\n                del self.config[key]\n            if key.endswith(\"-init\"):\n                self.config[key] = False\n            if key == 'agents-name-list':\n                for agent_name in self.config[key]:\n                    self.config[f\"agent-world_model-{agent_name}\"] = self.config[f'agent-config-{agent_name}'].world_model\n                self.config[key] = []\n        # print(self.config)\n        self.add_to_save_file_handler(self.keys[\"Config\"], json.dumps(self.config))\n        self.save_file_handler()\n        self.agent_chain.save_to_file()\n        self.scripts.save_scripts()\n\n    def init_pipeline(self, p_type, model, **kwargs):\n        global PIPLINE\n        if PIPLINE is None:\n            from transformers import pipeline as PIPLINE\n        if p_type not in self.initstate:\n            self.initstate[p_type + model] = False\n\n        if not self.initstate[p_type + model]:\n            self.app.logger.info(f\"init {p_type} pipeline\")\n            if self.pipes_device &gt;= 1 and torch.cuda.is_available():\n                if torch.cuda.device_count() &lt; self.pipes_device:\n                    self.print(\"device count exceeded ava-label ar\")\n                    for i in range(1, torch.cuda.device_count()):\n                        self.print(torch.cuda.get_device_name(i - 1))\n\n                self.config[f\"{p_type + model}_pipeline\"] = PIPLINE(p_type, model=model, device=self.pipes_device - 1,\n                                                                    **kwargs)\n            else:\n                self.app.logger.warning(\"Cuda is not available\")\n                self.config[f\"{p_type + model}_pipeline\"] = PIPLINE(p_type, model=model, **kwargs)\n            self.app.logger.info(\"Done\")\n            self.initstate[p_type + model] = True\n\n    def free_llm_model(self, names: list[str]):\n        for model in names:\n            self.initstate[f'LLM-model-{model}-init'] = False\n            del self.config[f'LLM-model-{model}']\n\n    def load_llm_models(self, names: list[str]):\n        for model in names:\n            if f'LLM-model-{model}-init' not in self.initstate:\n                self.initstate[f'LLM-model-{model}-init'] = False\n\n            if not self.initstate[f'LLM-model-{model}-init']:\n                self.initstate[f'LLM-model-{model}-init'] = True\n                if '/' in model:\n                    self.config[f'LLM-model-{model}'] = HuggingFaceHub(repo_id=model,\n                                                                       huggingfacehub_api_token=self.config[\n                                                                           'HUGGINGFACEHUB_API_TOKEN'])\n                    self.print(f'Initialized HF model : {model}')\n                elif model.startswith('gpt4all#'):\n                    m = gpt4all.GPT4All(model.replace('gpt4all#', ''))\n                    self.config[f'LLM-model-{model}'] = m\n                    self.print(f'Initialized gpt4all model : {model}')\n                elif model.startswith('gpt'):\n                    self.config[f'LLM-model-{model}'] = ChatOpenAI(model_name=model,\n                                                                   openai_api_key=self.config['OPENAI_API_KEY'],\n                                                                   streaming=True)\n                    self.print(f'Initialized OpenAi model : {model}')\n                else:\n                    self.config[f'LLM-model-{model}'] = OpenAI(model_name=model,\n                                                               openai_api_key=self.config['OPENAI_API_KEY'])\n                    self.print(f'Initialized OpenAi : {model}')\n\n    def get_llm_models(self, name: str):\n        if f'LLM-model-{name}' not in self.config:\n            self.load_llm_models([name])\n        return self.config[f'LLM-model-{name}']\n\n    def add_lang_chain_tools_to_agent(self, agent: Agent, tools: list[str] | None = None):\n\n        if tools is None:\n            tools = []\n        for key in self.lang_chain_tools_dict:\n            self.print(f\"Adding tool for loading : {key}\")\n            tools += [key]\n\n        self.lang_chain_tools_dict = {}\n\n        ll_functions = crate_llm_function_from_langchain_tools(tools)\n\n        agent.functions += ll_functions\n\n    def tools_to_llm_functions(self, tools: dict):\n        llm_functions = []\n        for tool_name, tool in tools.items():\n            if isinstance(tool, dict):\n                func = tool.get('func', None)\n            if isinstance(tool, Callable):\n                func = tool\n                tool = {'func': func}\n            if func is None:\n                self.app.logger.warning(f'No function found for {tool_name}')\n                continue\n\n            parameters = tool.get('parameters')\n            if parameters is None:\n\n                try:\n                    from litellm.utils import function_to_dict\n                    parameters = function_to_dict(func)[\"parameters\"][\"properties\"]\n                except:\n                    parameters = {}\n                    for _1, _ in signature(func).parameters.items():\n                        if hasattr(_.annotation, '__name__'):\n                            parameters[_1] = _.annotation.__name__\n                        else:\n                            parameters[_1] = _.annotation\n            llm_functions.append(\n                LLMFunction(name=tool_name,\n                            description=tool.get('description'),\n                            parameters=parameters,\n                            function=func)\n            )\n        return llm_functions\n\n    def get_agent_builder(self, name=\"BP\") -&gt; AgentBuilder:\n        return AgentBuilder(Agent).set_isaa_reference(self).set_amd_name(name)\n\n    def register_agents_setter(self, setter):\n        self.default_setter = setter\n\n    def register_agent(self, agent_builder):\n        if f'agent-config-{agent_builder.agent.amd.name}' in self.config:\n            print(f\"{agent_builder.agent.amd.name} Agent already registered\")\n            return\n\n        agent_builder.save_to_json(f\".data/{get_app('isaa.register_agent').id}/Agents/{agent_builder.agent.amd.name}.agent\")\n        self.config[f'agent-config-{agent_builder.agent.amd.name}'] = agent_builder.build()\n        self.config[\"agents-name-list\"].append(agent_builder.agent.amd.name)\n        self.agent_data[agent_builder.amd_attributes['name']] = agent_builder.get_dict()\n        self.print(f\"Agent:{agent_builder.agent.amd.name} Registered\")\n        return self.config[f'agent-config-{agent_builder.agent.amd.name}']\n\n    def get_default_agent_builder(self, name=\"self\") -&gt; AgentBuilder:\n        if name == 'None':\n            return self.get_default_agent_builder()\n        self.print(f\"Default AgentBuilder::{name}\")\n        agent_builder: AgentBuilder = self.get_agent_builder(name)\n\n        if name != \"\":\n            if os.path.exists(f\".data/{get_app('isaa.get_default_agent_builder').id}/Memory/{name}.agent\"):\n                agent_builder = agent_builder.load_from_json_file(f\".data/{get_app('isaa.get_default_agent_builder').id}/Memory/{name}.agent\", Agent)\n                agent_builder.set_isaa_reference(self)\n\n        if self.global_stream_override:\n            agent_builder.set_stream(True)\n\n        mem = self.get_memory()\n        tools = {}\n\n        agent_builder.set_memory(mem).set_amd_stop_sequence([\"QUERY:\", \"...\\n\"])  # .set_trim(Trims.isaa)\n\n        if self.default_setter is not None:\n            agent_builder = self.default_setter(agent_builder)\n\n        if self.local_files_tools:\n            pass\n            # if name in ['liveInterpretation', 'tools']:\n            #    toolkit = FileManagementToolkit(\n            #        root_dir=str(self.working_directory)\n            #    )  # If you don't provide a root_dir, operations will default to the current working directory\n            #    for file_tool in toolkit.get_tools():\n            #        # print(\"adding file tool\", file_tool.name)\n            #        tools[file_tool.name] = file_tool\n            # if name in ['self', 'liveInterpretation'] or 'ide' in name:\n            #     isaa_ide_online = self.app.mod_online(\"isaa_ide\", installed=True)\n            #     if isaa_ide_online:\n            #         isaa_ide = self.app.get_mod(\"isaa_ide\")\n            #         isaa_ide.scope = self.working_directory\n            #         isaa_ide.add_tools(tools)\n\n        agent_builder.init_agent_memory(name)\n\n        def run_agent(agent_name: str, text: str, **kwargs):\n            text = text.replace(\"'\", \"\").replace('\"', '')\n            if agent_name:\n                return self.run_agent(agent_name, text, **kwargs)\n            return \"Provide Information in The Action Input: fild or function call\"\n\n        async def run_pipe(task: str, do_continue:bool=False):\n            task = task.replace(\"'\", \"\").replace('\"', '')\n            return await self.run_pipe(name, task, do_continue)\n\n        def memory_search(query: str):\n            ress = self.get_memory().query(query,to_str=True)\n\n            if not ress:\n                return \"no informations found for :\" + query\n\n            return ress\n\n        async def ad_data(data: str):\n            await mem.add_data(name, str(data))\n\n            return 'added to memory'\n\n        def get_agents(*a,**k):\n            agents_name_list = self.config['agents-name-list'].copy()\n            if 'TaskCompletion' in agents_name_list:\n                agents_name_list.remove('TaskCompletion')\n            if 'create_task' in agents_name_list:\n                agents_name_list.remove('create_task')\n            if 'summary' in agents_name_list:\n                agents_name_list.remove('summary')\n            return agents_name_list\n\n        if name == \"self\":\n            # config.mode = \"free\"\n            # config.model_name = self.config['DEFAULTMODEL0']  # \"gpt-4\"\n            # config.max_iterations = 6\n            agent_builder.set_amd_model(self.config['DEFAULTMODEL0'])\n\n            tools[\"runAgent\"] = {\n                \"func\": lambda agent_name, instructions: self.run_agent(agent_name, instructions),\n                \"description\": \"The run_agent function takes a 2 arguments agent_name, instructions\"\n                               + f\"\"\"The function parses the input string x and extracts the values associated with the following keys:\n\n                       agent_name: The name of the agent to be run. : {get_agents()}\n                       instructions: The task that the agent is to perform. (do not enter the name of a task_chain!) give clear Instructions\n\n                   The function then runs the Agent with the specified name and Instructions.\"\"\"}\n\n            tools[\"getAvailableAgents\"] = {\n                \"func\": get_agents,\n                \"description\": \"Use to get list of all agents avalabel\"}\n\n            tools[\"saveDataToMemory\"] = {\"func\": ad_data, \"description\": \"tool to save data to memory,\"\n                                                                         \" write the data as specific\"\n                                                                         \" and accurate as possible.\"}\n\n            tools = {**tools, **{\n                \"memorySearch\": {\"func\": memory_search,\n                                 \"description\": \"must input reference context to search\"},\n                \"searchWeb\": {\"func\": self.web_search,\n                              \"description\": \"search the web (online) for information's input a query\"\n                    , \"format\": \"search(&lt;task&gt;)\"},\n                \"think\": {\"func\": lambda x: run_agent('think', x),\n                          \"description\": \"Run agent to solve a text based problem\"\n                    , \"format\": \"think(&lt;task&gt;)\"},\n                \"shell\": {\"func\": shell_tool_function,\n                          \"description\": \"Run shell command\"\n                    , \"format\": \"shell(command: str)\"},\n                \"miniTask\": {\"func\": lambda x: self.mini_task_completion(x),\n                             \"description\": \"programmable pattern completion engin. use text args:str only\"\n                    , \"format\": \"miniTask(&lt;detaild_discription&gt;)\"},\n                \"Coder\": {\"func\": self.code,\n                          \"description\": \"to write code basd from description\"\n                    , \"format\": \"coding_step(task&lt;detaild_discription&gt;: str, project_name&lt;data/$examplename&gt;: str)\"},\n                \"run_pipe\": {\"func\": run_pipe,\n                          \"description\": \"to perform complex multi step task in an inactive coding env\"\n                    , \"format\": \"run_pipe(task&lt;detaild_discription&gt;: str, do_continue&lt;if continue on last run or start fresh&gt;: bool)\"},\n\n            }}\n\n        if \"STT\" in name:\n\n            agent_builder.set_amd_model(self.config['DEFAULT_AUDIO_MODEL'])\n\n        if \"tool\" in name:\n            tools = {}\n            for key, _tool in self.lang_chain_tools_dict.items():\n                tools[key] = {\"func\": _tool, \"description\": _tool.description, \"format\": f\"{key}({_tool.args})\"}\n            agent_builder.set_amd_model(self.config['DEFAULTMODEL0'])\n\n        if \"search\" in name:\n\n            # config.mode = \"tools\"\n            # config.model_name = self.config['DEFAULTMODEL1']\n            # config.completion_mode = \"chat\"\n            # config.set_agent_type(\"structured-chat-zero-shot-react-description\")\n            # config.max_iterations = 6\n            # config.verbose = True\n            agent_builder.set_amd_model(self.config['DEFAULTMODEL0'])\n            agent_builder.set_content_memory_max_length(3500)\n            tools.update({\"memorySearch\": {\"func\": lambda context: memory_search(context),\n                                           \"description\": \"Search for memory  &lt;context&gt;\"}})\n            tools.update({\"WebSearch\": {\"func\": self.web_search,\n                                            \"description\": \"Search the web\"}})\n\n            tools[\"saveDataToMemory\"] = {\"func\": ad_data, \"description\": \"tool to save data to memory,\"\n                                                                         \" write the data as specific\"\n                                                                         \" and accurate as possible.\"}\n\n        if name == \"think\":\n            agent_builder.set_amd_model(self.config['DEFAULTMODELST'])\n            # .stop_sequence = [\"\\n\\n\\n\"]\n\n        if \"shell\" in name:\n            (agent_builder.set_amd_model(self.config['DEFAULTMODEL1'])\n             .set_amd_system_message(\"Act as an Command Shell Agent. You can run shell commandants by writing \"\n                                     \"\\nFUCTION: {'Action','shell','Input':[shell_command]}\"))\n            tools[\"shell\"] = {\"func\": shell_tool_function,\n                              \"description\": \"Run shell command\"\n                , \"parameters\": {\"type\": \"string\"}}\n            pass\n            # .set_model_name(self.config['DEFAULTMODEL1'])\n            # .add_system_information = False\n            # .stop_sequence = [\"\\n\"]\n\n        if name == \"liveInterpretation\":\n            pass\n            # .set_model_name(self.config['DEFAULTMODEL0']).stream = True\n            # config.stop_sequence = [\"!X!\"]\n\n        if name == \"summary\":\n            agent_builder.set_amd_model(self.config['DEFAULTMODELSUMMERY'])\n\n        if name == \"thinkm\":\n            agent_builder.set_amd_model(self.config['DEFAULTMODEL1'])\n\n        if name == \"TaskCompletion\":\n            agent_builder.set_amd_model(self.config['DEFAULTMODEL1'])\n\n        if name == \"code\":\n            agent_builder.set_amd_model(self.config['DEFAULTMODELCODE'])\n\n        tools = {**tools, **{\n\n            \"saveDataToMemory\": {\"func\": ad_data, \"description\": \"tool to save data to memory,\"\n                                                                 \" write the data as specific\"\n                                                                 \" and accurate as possible.\"},\n            \"memorySearch\": {\"func\": lambda x: memory_search(x),\n                             \"description\": \"Search for similar memory input &lt;context&gt;\"}, }}\n\n        if agent_builder.amd_attributes.get('model') is None:\n            agent_builder.set_amd_model(self.config['DEFAULTMODEL2'])\n        llm_functions = self.tools_to_llm_functions(tools)\n        agent_builder.set_functions(llm_functions)\n        os.makedirs(f\".data/{get_app('isaa-get-agent').id}/Agents/\", exist_ok=True)\n        agent_builder_dict = agent_builder.save_to_json(f\".data/{get_app('isaa-get-agent').id}/Agents/{name}.agent\")\n        self.agent_data[agent_builder.amd_attributes['name']] = agent_builder_dict\n        # agent_builder.set_verbose(True)\n        return agent_builder\n\n    def remove_agent_config(self, name):\n        del self.config[f'agent-config-{name}']\n        self.config[\"agents-name-list\"].remove(name)\n\n    def get_agent(self, agent_name=\"Normal\", model=None) -&gt; Agent:\n\n        if \"agents-name-list\" not in self.config:\n            self.config[\"agents-name-list\"] = []\n\n        # self.config[\"agents-name-list\"] = [k.replace('agent-config-', '') for k in self.config.keys() if k.startswith('agent-config-')])\n        if f'agent-config-{agent_name}' in self.config:\n            agent = self.config[f'agent-config-{agent_name}']\n            if model:\n                agent.amd.model = model\n            self.print(f\"collecting AGENT: {agent_name} \"\n                       f\"{'Mode:' + str(agent.mode) if agent.mode is not None else ''} \"\n                       f\"{'Cape:' + agent.capabilities.name if agent.capabilities is not None else ''}\")\n        else:\n            with Spinner(message=f\"Building Agent {agent_name}\", symbols='c'):\n                agent_builder = self.get_default_agent_builder(agent_name)\n                if model:\n                    agent_builder.set_amd_model(model)\n                if agent_builder.amd_attributes.get('model', '').startswith('ollama'):\n                    try:\n                        agent = agent_builder.build()\n                    except Exception:\n                        subprocess.Popen(\"wsl -e ollama serve\", shell=True, stdout=subprocess.PIPE,\n                                                   stderr=subprocess.PIPE)\n                        time.sleep(5)\n                        agent = agent_builder.build()\n                else:\n                    agent = agent_builder.build()\n            del agent_builder\n            agent.world_model = self.config.get(f\"agent-world_model-{agent_name}\", {})\n            self.config[f'agent-config-{agent_name}'] = agent\n            self.print(f\"Init:Agent::{agent_name}{' -' + str(agent.mode) if agent.mode is not None else ''}\")\n        if agent_name not in self.config[\"agents-name-list\"]:\n            self.config[\"agents-name-list\"].append(agent_name)\n        return agent\n\n\n    def mini_task_completion(self, mini_task:str, user_task=None, mode=None,\n                             max_tokens=None, task_from=\"system\", stream_function=None, message=None):\n        if mini_task is None:\n            return None\n        self.print(f\"running mini task Volumen {len(mini_task)}\")\n        agent: Agent = self.get_agent(\"TaskCompletion\")\n        agent.mode = mode\n        _stream_function = agent.stream_function\n        if stream_function is not None:\n            agent.stream_function = stream_function\n        sto_add_function_to_prompt = agent.add_function_to_prompt\n        agent.add_function_to_prompt = False\n        m = agent.max_tokens\n        if max_tokens is not None:\n            agent.max_tokens = max_tokens\n\n        if user_task is not None:\n            user_task, mini_task = mini_task, user_task\n\n        res = agent.mini_task(mini_task, task_from, user_task)\n\n        agent.mode = None\n        agent.add_function_to_prompt = sto_add_function_to_prompt\n        agent.max_tokens = m\n        agent.stream_function = _stream_function\n        agent.verbose = True\n\n        return res\n\n    def mini_task_completion_format(self, mini_task, format_, max_tokens=None, agent_name=\"TaskCompletion\",\n                                    task_from=\"system\", mode_overload=None, user_task=None):\n        if mini_task is None:\n            return None\n        self.print(f\"running f mini task Volumen {len(mini_task)}, format Volumen {len(mini_task)}\")\n        agent: Agent = self.get_agent(agent_name)\n        if mode_overload is None:\n            mode_overload = self.controller.rget(StrictFormatResponder)\n        # if not isinstance(format_, dict):\n        #     format_ = {'text':format_}\n        agent.set_rformat(format_)\n        res: str or list = self.mini_task_completion(mini_task=mini_task,\n                                                     mode=mode_overload,\n                                                     max_tokens=max_tokens,\n                                                     task_from=task_from,\n                                                     user_task=user_task)\n        agent.reset_rformat()\n        if isinstance(res, str):\n            res = res.strip()\n\n        if format_ == bool:\n            return agent.fuzzy_string_match(res, ['true', 'Treue', 'false', 'False']).lower() == 'true'\n\n        # if '{' in res and '}' in res:\n        #     res_ = anything_from_str_to_dict(res)\n        #     if len(res_) &gt; 0:\n        #         return res_[0]\n        return res\n\n\n    def format_class(self, format_class, task, agent_name=\"TaskCompletion\"):\n        if format_class is None:\n            return None\n        if not task:\n            return None\n        if isinstance(agent_name, str):\n            agent: Agent = self.get_agent(agent_name)\n        elif isinstance(agent_name, Agent):\n            agent = agent_name\n\n        return agent.format_class(format_class, task)\n\n    def get_pipe(self, agent_name, *args, **kwargs) -&gt; Pipeline:\n        if isinstance(agent_name, str):\n            agent: Agent = self.get_agent(agent_name)\n        else:\n            agent = agent_name\n\n        if agent.amd.name in self.pipes:\n            return self.pipes[agent.amd.name]\n\n        else:\n            self.pipes[agent.amd.name] = Pipeline(agent, *args, **kwargs)\n        return self.pipes[agent.amd.name]\n\n    async def run_pipe(self, agent_name, task,do_continue=False):\n        return await self.get_pipe(agent_name).run(task, do_continue=do_continue)\n\n\n    def short_prompt_messages(self, messages, get_tokens, max_tokens, prompt_token_margin=20):\n        prompt_len = get_tokens(messages)\n        max_tokens *= 0.985\n        if prompt_len &lt;= max_tokens - prompt_token_margin:\n            return messages\n\n        self.print(f\"Context length: {prompt_len}, Max tokens: {max_tokens} \")\n\n        # Pre-process first and last messages if they're too long\n        first_message = messages[0]\n        if len(messages) == 1:\n            first_message['content'] = self.mas_text_summaries(first_message['content'])\n            return [first_message]\n\n        last_message = messages[-1]\n\n        first_message_tokens = get_tokens([first_message])\n        last_message_tokens = get_tokens([last_message])\n\n        if first_message_tokens &gt; max_tokens // 2:\n            first_message['content'] = self.mas_text_summaries(first_message['content'])\n\n        if last_message_tokens &gt; max_tokens // 2:\n            last_message['content'] = self.mas_text_summaries(last_message['content'],\n                                                              ref=first_message['content'][:260])\n        if len(messages) == 2:\n            return [first_message] + [last_message]\n\n        # Keep first and last messages intact\n        middle_messages = messages[1:-1]\n\n        all_content = \"\\n\".join([msg['content'] for msg in middle_messages])\n\n        dilated_content = self.mas_text_summaries(all_content, ref=first_message.get('content', '')+last_message.get('content', ''))\n        new_middle_messages = {'role': \"system\", 'content': \"History -&gt; \"+dilated_content}\n\n        # Check if we're within token limit\n        if get_tokens([first_message]+ [new_middle_messages] + [last_message]) &lt;= max_tokens - prompt_token_margin:\n            return [first_message]+ [new_middle_messages] + [last_message]\n\n        # Final attempt: Use summarization\n        new_middle_messages['content'] = dilate_string(new_middle_messages['content'], \"\\n\", 2, 1)\n\n        # Ensure we're within token limit\n        final_messages = [first_message] + [new_middle_messages] + [last_message]\n        if get_tokens(final_messages) &gt; max_tokens - prompt_token_margin:\n            # If still too long, truncate the summary\n            allowed_length = max_tokens - prompt_token_margin - get_tokens([first_message, last_message])\n            if 0 &lt; allowed_length &lt; max_tokens // 10:\n                final_messages[1]['content'] = final_messages[1]['content'][:allowed_length]\n            elif allowed_length &lt; 0:\n                allowed_length *= -.5\n                allowed_length = int(allowed_length)\n                final_messages[0]['content'] = final_messages[0]['content'][:allowed_length]\n                final_messages[-1]['content'] = final_messages[-1]['content'][allowed_length:]\n\n        return final_messages\n\n\n    async def run_agent_in_environment(self, task,\n                                 agent_or_name: (str or Agent) | None = None,\n                                 agent_env: (str or AgentVirtualEnv) | None = None,\n                                 persist=False,\n                                 persist_ref=False,\n                                 max_iterations=10,\n                                 verbose=False,\n                                 message=None,\n                                 task_from='user',\n                                 get_final_code=False):\n\n        if isinstance(agent_or_name, str):\n            agent = self.get_agent(agent_or_name)\n        elif isinstance(agent_or_name, Agent):\n            agent = agent_or_name\n            name = agent.amd.name\n            if name not in self.config[\"agents-name-list\"]:\n                self.config[f'agent-config-{name}'] = agent\n        else:\n            agent = self.get_agent(\"self\")\n\n        def default_env():\n            env = AgentVirtualEnv()\n\n            @env.register_prefix(\"THINK\",\n                                 \"This text remains hidden. The THINK prefix should be used regularly to reason.\")\n            def process_think(content: str):\n                return self.run_agent('think', content, verbose=verbose)\n\n            @env.register_prefix(\"PLAN\", \"To reflect a plan.\")\n            def process_plan(content: str):\n                return self.run_agent('self', content, running_mode='pegasus', verbose=verbose)\n\n            @env.register_prefix(\"RESPONSE\", \"THE Final output! must write a response. in the final Turn!\")\n            def process_response(content: str):\n                return content\n\n            env.set_brake_test(lambda r: \"RESPONSE\" in r or r.rstrip().endswith('?') or r.rstrip().endswith('.'))\n            return env\n\n        if agent_env is None:\n            agent_env = default_env()\n\n        # save_state\n        tso_c = agent.capabilities\n        sto_add_function_to_prompt = agent.add_function_to_prompt\n        sto_verbose = agent.verbose\n\n        agent.add_function_to_prompt = True\n        agent.verbose = verbose\n\n        agent.capabilities = agent_env.get_llm_mode()\n\n        async def fuction_exec_helper(x):\n            return await agent.execute_fuction(persist, persist_ref) if x else None\n\n        out = \"\"\n        main_task = task\n        if not persist:\n            agent.reset_context()\n        turns = 0\n        for turn in range(max_iterations):\n            turns += 1\n            print()\n            self.print(f\"=================== Enter Turn : {turn + 1} of {max_iterations} =================\\n\")\n            self.print(f\"Task : {task[:60]}\")\n\n            agent_env.results_reset()\n\n            with Spinner(message=\"Fetching llm_message...\", symbols='+'):\n                message = agent.get_llm_message(task, persist=True, task_from=task_from, message=message)\n\n            out_ = await agent.a_run_model(message, persist_local=True, persist_mem=persist_ref)\n            #print_prompt(message + [{'role': 'assistant', 'content': out_},\n            #                        {'role': 'system', 'content': agent_env.results()}])\n            out += out_\n\n            with Spinner(message=\"Processioning Env step\", symbols='+'):\n                [await fuction_exec_helper(agent.if_for_fuction_use(line)) for line in out_.split('\\n')]\n                out += agent_env.results()\n\n            if agent_env.break_test(out):\n                break\n\n            task = f\"MAIN TASK: {main_task}\\nMAIN TASK END\\n in Turn {turn + 1}from{max_iterations}\\nLast Turn Results: {out_}\\n\\n{agent_env.results()}\\n\"\n\n        if not persist:\n            agent.reset_context()\n        agent.add_function_to_prompt = sto_add_function_to_prompt\n        agent.capabilities = tso_c\n        agent.verbose = sto_verbose\n\n        self.print(f\"DONE RUNNING ENV FOR {agent.amd.name} in Turns {turns}\")\n\n        if get_final_code:\n            return out, \"\"\n        return out\n\n    # @get_app('isaa-run-agent').tb(name=Name, test=False)\n    async def run_agent(self, name: str or Agent,\n                  text: str,\n                  verbose: bool = False,\n                  **kwargs):\n        if text is None:\n            return \"\"\n        agent = None\n        if isinstance(name, str):\n            # self.load_keys_from_env()\n            agent = self.get_agent(name)\n\n        elif isinstance(name, Agent):\n            agent = name\n            name = agent.amd.name\n\n            if name not in self.config[\"agents-name-list\"]:\n                self.config[f'agent-config-{name}'] = agent\n                self.print(f\"Register:Agent::{name}:{agent.amd.name} {str(agent.mode)}\\n\")\n\n        else:\n            raise ValueError(f\"Invalid arguments agent is not str or Agent {type(agent)}\")\n\n        agent.verbose = verbose\n        agent.print_verbose(f\"Running task {text[:200]}\")\n        # self.print(f\"Running agent {name}\")\n\n        stream = agent.stream\n        self.app.logger.info(f\"stream: {stream}\")\n\n        if agent.mode is not None and not self.controller.registered(agent.mode.name):\n            self.controller.add(agent.mode.name, agent.mode)\n\n        if stream and agent.stream_function is None:\n            agent.stream_function = self.print_stream\n\n        return await agent.run(text, **kwargs)\n\n    def mas_text_summaries(self, text, min_length=3600, ref=None):\n        \"\"\"text to summarises and ref is wit focus to summarise for, example text abut Plains, ref = Plains and Engines -&gt; to gent a summary basd of text about Plain Engines\"\"\"\n        len_text = len(text)\n        if len_text &lt; min_length:\n            return text\n        key = self.one_way_hash(text, 'summaries', 'isaa')\n        value = self.mas_text_summaries_dict.get(key)\n        self.print(f\"len input : {len_text}\")\n        if value is not None:\n            self.print(\"summ return vom chash\")\n            return value\n\n        if ref is None:\n            ref = text\n        with Spinner(\"Processioning Summarization\"):\n\n            texts = TextSplitter(chunk_size=min_length*10, chunk_overlap=min_length // 10).split_text(text)\n            relevant_texts = filter_relevant_texts(ref, texts=texts, fuzzy_threshold=51, semantic_threshold=0.52)\n            self.print(f\"Summary Volume from {len(text)} to {len(relevant_texts)}\")\n            if len(relevant_texts) == 0:\n                relevant_texts = texts\n            relevant_text_len = len(''.join(relevant_texts))\n            self.print(f\"Relevant texts Volume {len(relevant_texts)} \"\n                       f\"average cuk len {sum([len(r) for r in relevant_texts]) / len(relevant_texts)}\"\n                       f\" in mode \"\n                       f\": {self.summarization_mode} ratio {len(text)}/{relevant_text_len}\")\n\n            if min_length &gt; relevant_text_len:\n\n                class Segments(BaseModel):\n                    \"\"\"Important and relevant information segment in itself complete\"\"\"\n                    information: str\n\n                class SummarizationSegments(BaseModel):\n                    f\"\"\"importance for: {ref if ref != text else 'key details and concrete informations'}\"\"\"\n                    segments: list[Segments] = field(default_factory=list)\n\n                if len(relevant_texts) &gt; 26:\n                    bf = self.mas_text_summaries(' '.join(relevant_texts[20:]), min_length=min_length + 100, ref=ref)\n                    relevant_texts = relevant_texts[:20] + [bf]\n                segments = self.format_class(SummarizationSegments,\n                                  '\\n'.join(relevant_texts))[\"segments\"]\n                if sum([len(segment['information']) for segment in segments ]) &gt; min_length*2:\n                    summary = self.mini_task_completion(mini_task=\"Create a Summary\" +\n                                                                  (\n                                                                      \"\" if ref == text else \" for this reference: \" + ref),\n                                                        user_task='Segments:\\n'.join(\n                                                            [x['information'] for x in segments\n                                                             ]),\n                                                        mode=self.controller.rget(SummarizationMode))\n                else:\n                    summary = '\\n'.join([x['information']for x in segments])\n                if summary is None:\n                    return relevant_texts[:18] + [f\"Information chunks lost : {len(relevant_texts) - 18}\"]\n            else:\n                summary = '\\n'.join(relevant_texts)\n\n            if not isinstance(summary, str):\n                bf = self.mas_text_summaries(' '.join(relevant_texts[10:]), min_length=min_length + 100, ref=ref)\n                relevant_texts = relevant_texts[:10] + [bf]\n                summary = '\\n'.join(relevant_texts)\n\n        self.mas_text_summaries_dict.set(key, summary)\n\n        return summary\n\n    async def mass_text_summaries(self, text: str, min_length: int = 1600, ref: str | None = None) -&gt; str:\n        \"\"\"\n        Efficient large-text summarization using semantic memory retrieval\n        Features:\n        - Chunk-based parallel processing\n        - LightRAG-powered relevance scoring\n        - LiteLLM-optimized summarization\n        - Multi-level caching\n        \"\"\"\n\n        # 1. Text Length Check and Caching\n        len_text = len(text)\n        if len_text &lt; min_length:\n            return text\n\n        cache_key = self.one_way_hash(text, 'summaries', 'isaa')\n        if cached := self.mas_text_summaries_dict.get(cache_key):\n            self.print(\"Returning cached summary\")\n            return cached\n\n        # 2. Memory Initialization\n        semantic_memory = self.get_memory()\n        ref_query = ref or text\n\n        def _chunk_text(text: str, chunk_size: int = 4000) -&gt; list[str]:\n            \"\"\"Optimized text chunking with overlap\"\"\"\n            return [text[i:i + chunk_size]\n                    for i in range(0, len(text), chunk_size - 200)]\n\n        chunks = _chunk_text(text, chunk_size=4000)\n        await semantic_memory.add_data(\n                                   \"summary_cache\",\n                                   chunks,\n                                   {\"source\": \"mass_summary\"})\n\n\n        # 4. LightRAG-Powered Relevance Extraction\n        query_params = {}\n\n        # Hybrid search for key concepts\n        results = await semantic_memory.query(\n            query=ref_query,\n            memory_names=\"summary_cache\",\n            query_params=query_params\n        )\n\n        summary = self._generate_llm_summary(results, ref_query, min_length)\n\n        # 6. Cache Management\n        self.mas_text_summaries_dict.set(cache_key, summary)\n        return summary\n\n\n    def _generate_llm_summary(self, chunks: list[dict[str, str]], query: str, min_length: int) -&gt; str:\n        \"\"\"LiteLLM-optimized summarization pipeline\"\"\"\n        summary_prompt = f\"\"\"Generate a concise summary focusing on {query} from these key excerpts:\n        {chunks}\n\n        Requirements:\n        - Length between {min_length} and {min_length * 1.2} characters\n        - Maintain technical details and numerical data\n        - Use clear section headings\n        - Highlight relationships between concepts\"\"\"\n\n        return self.get_agent(\"self\").mini_task(summary_prompt)\n\n\n    def get_memory(self, name: str | None=None) -&gt; AISemanticMemory:\n        logger = get_logger()\n        if isinstance(self.agent_memory, str):\n            logger.info(Style.GREYBG(\"AISemanticMemory Initialized\"))\n            self.agent_memory = AISemanticMemory(base_path=self.agent_memory)\n        logger.info(Style.GREYBG(\"AIContextMemory requested\"))\n        cm = self.agent_memory\n        if name is not None:\n            r = cm.get(name)\n            if len(r) == 1:\n                return r[0]\n            return r\n        logger.info(Style.Bold(\"AISemanticMemory instance, returned\"))\n        return cm\n\n    def save_to_mem(self):\n        for name in self.config['agents-name-list']:\n            self.get_agent(agent_name=name).save_memory()\n\n    def set_local_files_tools(self, local_files_tools):\n        try:\n            self.local_files_tools = bool(local_files_tools)\n        except ValueError as e:\n            return f\"Invalid boolean value True or False not {local_files_tools} \\n{str(e)}\"\n        return f\"set to {self.local_files_tools=}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.module.Tools.mas_text_summaries","title":"<code>mas_text_summaries(text, min_length=3600, ref=None)</code>","text":"<p>text to summarises and ref is wit focus to summarise for, example text abut Plains, ref = Plains and Engines -&gt; to gent a summary basd of text about Plain Engines</p> Source code in <code>toolboxv2/mods/isaa/module.py</code> <pre><code>def mas_text_summaries(self, text, min_length=3600, ref=None):\n    \"\"\"text to summarises and ref is wit focus to summarise for, example text abut Plains, ref = Plains and Engines -&gt; to gent a summary basd of text about Plain Engines\"\"\"\n    len_text = len(text)\n    if len_text &lt; min_length:\n        return text\n    key = self.one_way_hash(text, 'summaries', 'isaa')\n    value = self.mas_text_summaries_dict.get(key)\n    self.print(f\"len input : {len_text}\")\n    if value is not None:\n        self.print(\"summ return vom chash\")\n        return value\n\n    if ref is None:\n        ref = text\n    with Spinner(\"Processioning Summarization\"):\n\n        texts = TextSplitter(chunk_size=min_length*10, chunk_overlap=min_length // 10).split_text(text)\n        relevant_texts = filter_relevant_texts(ref, texts=texts, fuzzy_threshold=51, semantic_threshold=0.52)\n        self.print(f\"Summary Volume from {len(text)} to {len(relevant_texts)}\")\n        if len(relevant_texts) == 0:\n            relevant_texts = texts\n        relevant_text_len = len(''.join(relevant_texts))\n        self.print(f\"Relevant texts Volume {len(relevant_texts)} \"\n                   f\"average cuk len {sum([len(r) for r in relevant_texts]) / len(relevant_texts)}\"\n                   f\" in mode \"\n                   f\": {self.summarization_mode} ratio {len(text)}/{relevant_text_len}\")\n\n        if min_length &gt; relevant_text_len:\n\n            class Segments(BaseModel):\n                \"\"\"Important and relevant information segment in itself complete\"\"\"\n                information: str\n\n            class SummarizationSegments(BaseModel):\n                f\"\"\"importance for: {ref if ref != text else 'key details and concrete informations'}\"\"\"\n                segments: list[Segments] = field(default_factory=list)\n\n            if len(relevant_texts) &gt; 26:\n                bf = self.mas_text_summaries(' '.join(relevant_texts[20:]), min_length=min_length + 100, ref=ref)\n                relevant_texts = relevant_texts[:20] + [bf]\n            segments = self.format_class(SummarizationSegments,\n                              '\\n'.join(relevant_texts))[\"segments\"]\n            if sum([len(segment['information']) for segment in segments ]) &gt; min_length*2:\n                summary = self.mini_task_completion(mini_task=\"Create a Summary\" +\n                                                              (\n                                                                  \"\" if ref == text else \" for this reference: \" + ref),\n                                                    user_task='Segments:\\n'.join(\n                                                        [x['information'] for x in segments\n                                                         ]),\n                                                    mode=self.controller.rget(SummarizationMode))\n            else:\n                summary = '\\n'.join([x['information']for x in segments])\n            if summary is None:\n                return relevant_texts[:18] + [f\"Information chunks lost : {len(relevant_texts) - 18}\"]\n        else:\n            summary = '\\n'.join(relevant_texts)\n\n        if not isinstance(summary, str):\n            bf = self.mas_text_summaries(' '.join(relevant_texts[10:]), min_length=min_length + 100, ref=ref)\n            relevant_texts = relevant_texts[:10] + [bf]\n            summary = '\\n'.join(relevant_texts)\n\n    self.mas_text_summaries_dict.set(key, summary)\n\n    return summary\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.module.Tools.mass_text_summaries","title":"<code>mass_text_summaries(text, min_length=1600, ref=None)</code>  <code>async</code>","text":"<p>Efficient large-text summarization using semantic memory retrieval Features: - Chunk-based parallel processing - LightRAG-powered relevance scoring - LiteLLM-optimized summarization - Multi-level caching</p> Source code in <code>toolboxv2/mods/isaa/module.py</code> <pre><code>async def mass_text_summaries(self, text: str, min_length: int = 1600, ref: str | None = None) -&gt; str:\n    \"\"\"\n    Efficient large-text summarization using semantic memory retrieval\n    Features:\n    - Chunk-based parallel processing\n    - LightRAG-powered relevance scoring\n    - LiteLLM-optimized summarization\n    - Multi-level caching\n    \"\"\"\n\n    # 1. Text Length Check and Caching\n    len_text = len(text)\n    if len_text &lt; min_length:\n        return text\n\n    cache_key = self.one_way_hash(text, 'summaries', 'isaa')\n    if cached := self.mas_text_summaries_dict.get(cache_key):\n        self.print(\"Returning cached summary\")\n        return cached\n\n    # 2. Memory Initialization\n    semantic_memory = self.get_memory()\n    ref_query = ref or text\n\n    def _chunk_text(text: str, chunk_size: int = 4000) -&gt; list[str]:\n        \"\"\"Optimized text chunking with overlap\"\"\"\n        return [text[i:i + chunk_size]\n                for i in range(0, len(text), chunk_size - 200)]\n\n    chunks = _chunk_text(text, chunk_size=4000)\n    await semantic_memory.add_data(\n                               \"summary_cache\",\n                               chunks,\n                               {\"source\": \"mass_summary\"})\n\n\n    # 4. LightRAG-Powered Relevance Extraction\n    query_params = {}\n\n    # Hybrid search for key concepts\n    results = await semantic_memory.query(\n        query=ref_query,\n        memory_names=\"summary_cache\",\n        query_params=query_params\n    )\n\n    summary = self._generate_llm_summary(results, ref_query, min_length)\n\n    # 6. Cache Management\n    self.mas_text_summaries_dict.set(cache_key, summary)\n    return summary\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.module.detect_shell","title":"<code>detect_shell()</code>","text":"<p>Detect system-appropriate shell with fallbacks</p> Source code in <code>toolboxv2/mods/isaa/module.py</code> <pre><code>def detect_shell() -&gt; str:\n    \"\"\"Detect system-appropriate shell with fallbacks\"\"\"\n    if platform.system() == \"Windows\":\n        return \"cmd.exe\"\n\n    # For Unix-like systems\n    return os.environ.get(\"SHELL\", \"/bin/sh\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.module.safe_decode","title":"<code>safe_decode(data)</code>","text":"<p>Handle encoding detection with multiple fallbacks</p> Source code in <code>toolboxv2/mods/isaa/module.py</code> <pre><code>def safe_decode(data: bytes) -&gt; str:\n    \"\"\"Handle encoding detection with multiple fallbacks\"\"\"\n    encodings = [\n        sys.stdout.encoding,\n        locale.getpreferredencoding(),\n        'utf-8',\n        'latin-1',\n        'iso-8859-1'\n    ]\n\n    for enc in encodings:\n        try:\n            return data.decode(enc)\n        except UnicodeDecodeError:\n            continue\n    return data.decode('utf-8', errors='replace')\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.module.shell_tool_function","title":"<code>shell_tool_function(command)</code>","text":"<p>Robust system-agnostic command execution Handles encoding issues and shell detection automatically</p> Source code in <code>toolboxv2/mods/isaa/module.py</code> <pre><code>def shell_tool_function(command: str) -&gt; str:\n    \"\"\"\n    Robust system-agnostic command execution\n    Handles encoding issues and shell detection automatically\n    \"\"\"\n    result: dict[str, Any] = {\"success\": False, \"output\": \"\", \"error\": \"\"}\n    shell = detect_shell()\n\n    try:\n        # Windows command formatting\n        if platform.system() == \"Windows\":\n            if \"powershell\" in shell.lower():\n                full_cmd = f\"{shell} -Command {shlex.quote(command)}\"\n            else:\n                full_cmd = f'{shell} /c \"{command}\"'\n        else:\n            # Unix-style command formatting\n            full_cmd = f\"{shell} -c {shlex.quote(command)}\"\n\n        # Execute command\n        process = subprocess.run(\n            full_cmd,\n            shell=True,\n            check=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            timeout=120,\n            text=False  # Handle decoding ourselves\n        )\n\n        result.update({\n            \"success\": True,\n            \"output\": safe_decode(process.stdout).split(\"EndOfString\")[-1],\n            \"error\": \"\"\n        })\n\n    except subprocess.CalledProcessError as e:\n        result.update({\n            \"error\": f\"Process error [{e.returncode}]\",\n            \"output\": safe_decode(e.output)\n        })\n    except subprocess.TimeoutExpired:\n        result.update({\n            \"error\": \"Timeout\",\n            \"output\": f\"Command timed out: {command}\"\n        })\n    except Exception as e:\n        result.update({\n            \"error\": f\"Unexpected error: {type(e).__name__}\",\n            \"output\": str(e)\n        })\n\n    return json.dumps(result, ensure_ascii=False)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.types","title":"<code>types</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.types.IsToolTask","title":"<code>IsToolTask</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>test if is a tool task</p> Source code in <code>toolboxv2/mods/isaa/types.py</code> <pre><code>class IsToolTask(BaseModel):\n    \"\"\"test if is a tool task \"\"\"\n    tools: bool = Field(..., description=\"If the task needs tool usage\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.types.PlanEval","title":"<code>PlanEval</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>evaluate if is possible to carry out the task effectively</p> Source code in <code>toolboxv2/mods/isaa/types.py</code> <pre><code>class PlanEval(BaseModel):\n    \"\"\"evaluate if is possible to carry out the task effectively\"\"\"\n    possible: bool = Field(..., description=\"Dos the sub tasks led to the final task completion?\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.types.SupTask","title":"<code>SupTask</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>SupTask</p> Source code in <code>toolboxv2/mods/isaa/types.py</code> <pre><code>class SupTask(BaseModel):\n    \"\"\"SupTask\"\"\"\n    task: str = Field(..., description=\"The Sub Task it self withe all information and instructions.\"\n                                       \" From the main task specific for this sub-task.\")\n    tools: bool | None = Field(..., description=\"If the subtask needs tool usage\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.types.TaskComplexity","title":"<code>TaskComplexity</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>evaluate the complexity of the task between 0 and 10</p> Source code in <code>toolboxv2/mods/isaa/types.py</code> <pre><code>class TaskComplexity(BaseModel):\n    \"\"\"evaluate the complexity of the task between 0 and 10\"\"\"\n    complexity: int = Field(..., description=\"complexity of the task\")\n    context: int = Field(...,\n                         description=\"complexity of the needed context to solve the task 0 to 3 (HISTORY=1) (WEB=1) (ALL=3)\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.types.TaskDone","title":"<code>TaskDone</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>evaluate if the task accomplished?</p> Source code in <code>toolboxv2/mods/isaa/types.py</code> <pre><code>class TaskDone(BaseModel):\n    \"\"\"evaluate if the task accomplished?\"\"\"\n    done: bool = Field(..., description=\"is the task accomplished?\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.types.TaskPlan","title":"<code>TaskPlan</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>TaskPlan</p> Source code in <code>toolboxv2/mods/isaa/types.py</code> <pre><code>class TaskPlan(BaseModel):\n    \"\"\"TaskPlan\"\"\"\n    sub_tasks: list[SupTask] = Field(..., description=\"A list of sub tasks to accomplish the main tasks\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.ui","title":"<code>ui</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.ui.nice","title":"<code>nice</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.ui.nice.IsaaWebSocketUI","title":"<code>IsaaWebSocketUI</code>","text":"Source code in <code>toolboxv2/mods/isaa/ui/nice.py</code> <pre><code>class IsaaWebSocketUI(metaclass=Singleton):\n    def __init__(self, isaa_tool, name=\"IsaaWebSocket\"):\n        self.isaa = isaa_tool\n        self.active_connections: dict[str, WebSocket] = {}\n        self.message_history: list[dict] = []\n        self.agent_states: dict[str, AgentState] = {}\n        self.ping_interval = 60\n        self.max_reconnect_attempts = 5\n\n    def _setup_verbose_override(self, agent, f=None):\n        original_print_verbose = agent.print_verbose\n\n        def new_print_verbose(msg, *args, **kwargs):\n            client_id = kwargs.get('client_id')\n            if client_id and client_id in self.agent_states:\n                if self.agent_states[client_id].verbose_output is None:\n                    self.agent_states[client_id].verbose_output = []\n                self.agent_states[client_id].verbose_output.append(msg)\n            original_print_verbose(msg, *args, **kwargs)\n            if f:\n                f(msg)\n\n        agent.print_verbose = new_print_verbose\n\n    async def connect_websocket(self, websocket: WebSocket, client_id: str):\n        await websocket.accept()\n        self.active_connections[client_id] = websocket\n        self.agent_states[client_id] = AgentState()\n        asyncio.create_task(self._keep_alive(client_id))\n\n    async def _keep_alive(self, client_id: str):\n        \"\"\"Send periodic ping to keep connection alive\"\"\"\n        while client_id in self.active_connections:\n            try:\n                await self.active_connections[client_id].send_json({\"type\": \"ping\"})\n                await asyncio.sleep(self.ping_interval)\n            except:\n                await self.disconnect_websocket(client_id)\n                break\n\n    async def disconnect_websocket(self, client_id: str):\n        if client_id in self.active_connections:\n            with contextlib.suppress(Exception):\n                await self.active_connections[client_id].close()\n            del self.active_connections[client_id]\n            if client_id in self.agent_states:\n                del self.agent_states[client_id]\n\n    async def _monitor_agent_task(self, task: asyncio.Task, client_id: str):\n        \"\"\"Monitor agent task and update state\"\"\"\n        try:\n            await task\n        except Exception as e:\n            await self._send_error(client_id, str(e))\n        finally:\n            if client_id in self.agent_states:\n                self.agent_states[client_id].is_running = False\n                await self._send_agent_state(client_id)\n\n    async def stream_agent_response(self, message: str, client_id: str, agent_name: str | None = None):\n        \"\"\"Stream agent responses to the client with async task management\"\"\"\n        if client_id not in self.agent_states:\n            return\n\n        state = self.agent_states[client_id]\n        if state.is_running:\n            await self._send_error(client_id, \"An agent is already running\")\n            return\n\n        async def run_agent():\n            #try:\n            state.is_running = True\n            state.verbose_output = []\n            await self._send_agent_state(client_id)\n\n            def get_callback(agent_name_):\n                def helper(response_chunk, *a, **k):\n                    try:\n                        return get_app('nice.get_callback.helper').run_a_from_sync(self._send_stream_update, *[client_id,\n                                                                                     response_chunk,\n                                                                                     agent_name_])\n                    except Exception as e:\n                        print(f\"Agent {agent_name_} faint to report : {response_chunk}\", e)\n                        pass\n\n                return helper\n\n            self.isaa.default_setter = lambda x: x.set_verbose(True).set_post_callback(\n                get_callback(x.amd_attributes['name'])).set_print_verbose(\n                get_callback(x.amd_attributes['name'] + \"-internal\"))\n\n            # self.isaa.run_callback = get_callback(agent_name)\n\n            for agent, name in zip([self.isaa.get_agent(name_) for name_ in self.isaa.config['agents-name-list']],\n                                   self.isaa.config['agents-name-list'], strict=False):\n                agent.post_callback = get_callback(name)\n                self._setup_verbose_override(agent, get_callback(name + \"-internal\"))\n\n            response = await asyncio.to_thread(\n                self.isaa.run_agent,\n                agent_name,\n                message,\n                persist=True,\n                verbose=True\n            )\n            # Save to network branch\n            network = self.isaa.get_memory().cognitive_network.network\n            branch_id = f\"chat-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n            network.data_holder.create_branch(network, branch_id)\n\n            # Save chat history\n            self.message_history.append({\n                'timestamp': datetime.now().isoformat(),\n                'role': 'agent' if agent_name else 'system',\n                'content': response,\n                'agent': agent_name,\n                'branch_id': branch_id,\n                'verbose_output': state.verbose_output\n            })\n\n            state.last_response = response\n            await self._send_agent_state(client_id)\n\n            #except Exception as e:\n            #    await self._send_error(client_id, str(e))\n            #finally:\n            # state.is_running = False\n            await self._send_agent_state(client_id)\n\n        task = asyncio.create_task(run_agent())\n        state.current_task = task\n        await self._monitor_agent_task(task, client_id)\n\n    async def _send_stream_update(self, client_id: str, content: str, agent_name: str | None = None):\n        \"\"\"Send streaming updates to connected client\"\"\"\n        if client_id in self.active_connections:\n            await self.active_connections[client_id].send_json({\n                'type': 'stream',\n                'content': content,\n                'agent': agent_name\n            })\n\n    async def _send_agent_state(self, client_id: str):\n        \"\"\"Send agent state update to client\"\"\"\n        if client_id in self.active_connections and client_id in self.agent_states:\n            await self.active_connections[client_id].send_json({\n                'type': 'agent_state',\n                'state': self.agent_states[client_id].to_dict()\n            })\n\n    async def _send_error(self, client_id: str, error: str):\n        \"\"\"Send error message to client\"\"\"\n        if client_id in self.active_connections:\n            await self.active_connections[client_id].send_json({\n                'type': 'error',\n                'content': error\n            })\n\n    async def handle_websocket_chat(self, websocket: WebSocket):\n        \"\"\"Handle main chat WebSocket connection\"\"\"\n        client_id = str(uuid.uuid4())\n        await self.connect_websocket(websocket, client_id)\n\n        try:\n            while True:\n                message = await websocket.receive_json()\n                if message['type'] == 'message':\n                    await self.stream_agent_response(\n                        message['content'],\n                        client_id,\n                        message.get('agent')\n                    )\n                elif message['type'] == 'get_branches':\n                    network = self.isaa.get_memory().cognitive_network.network\n                    branches = network.data_holder.get_visualization_data().get('branches', [])\n                    await websocket.send_json({\n                        'type': 'branches',\n                        'branches': branches\n                    })\n                elif message['type'] == 'switch_branch':\n                    network = self.isaa.get_memory().cognitive_network.network\n                    success = network.data_holder.switch_branch(\n                        network,\n                        message['branch_id']\n                    )\n                    await websocket.send_json({\n                        'type': 'branch_switch',\n                        'success': success\n                    })\n\n                elif message['type'] == 't2s':\n                    audio_data: bytes = self.isaa.app.run_any(TBEF.AUDIO.SPEECH, text=message['content'], voice_index=0,\n                                                              use_cache=False,\n                                                              provider='piper',\n                                                              config={'play_local': False,\n                                                                      'model_name': message.get('model_name', 'ryan')},\n                                                              local=False,\n                                                              save=False)\n\n                    if not audio_data:\n                        return\n                    await websocket.send_bytes(audio_data)\n        except Exception as e:\n            print(f\"WebSocket error: {e}\")\n        finally:\n            await self.disconnect_websocket(client_id)\n\n    def get_widget(self, **kwargs):\n        \"\"\"Generate the HTML widget\"\"\"\n        template = \"\"\"\n        &lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Enhanced Chat Interface&lt;/title&gt;\n&lt;/head&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"&gt;&lt;/script&gt;\n&lt;style&gt;\n:root {\n    --primary: #4f46e5;\n    --primary-light: #6366f1;\n    --bg-dark: #0f172a;\n    --panel-bg: #1e293b;\n    --text: #e2e8f0;\n    --text-muted: #94a3b8;\n    --border: #334155;\n    --danger: #ef4444;\n    --success: #22c55e;\n    --transition: all 0.3s ease;\n    --sidebar-width: 45vw;\n}\n\n/* Base Styles */\nbody {\n    margin: 0;\n    font-family: system-ui, -apple-system, sans-serif;\n    background: var(--bg-dark);\n    color: var(--text);\n    line-height: 1.5;\n    overflow-x: hidden;\n}\n/* Layout */\n.main-layout {\n    display: grid;\n    grid-template-columns: 0.25fr 1fr 0.25fr;\n    height: 100vh;\n    transition: var(--transition);\n    position: relative;\n}\n\n/* Panel Styles */\n.sidebar, .right-sidebar {\n    background: var(--panel-bg);\n    border-right: 1px solid var(--border);\n    padding: 1.5rem;\n    overflow-y: auto;\n    position: relative;\n    transition: var(--transition);\n}\n\n.right-sidebar {\n    border-left: 1px solid var(--border);\n    border-right: none;\n    width: var(--sidebar-width);\n}\n\n/* Panel Toggle States */\n.main-layout.left-hidden .sidebar {\ngrid-template-columns: 0 1fr var(--sidebar-width);\n    transform: translateX(-var(--sidebar-width));\n    width: 0;\n    padding: 0;\n    opacity: 0;\n}\n\n.main-layout.right-hidden .right-sidebar {\ngrid-template-columns: var(--sidebar-width) 1fr 0;\n    transform: translateX(var(--sidebar-width));\n    width: 0;\n    padding: 0;\n    opacity: 0;\n}\n\n.main-layout.both-hidden .sidebar,\n.main-layout.both-hidden .right-sidebar {\ngrid-template-columns: 0 1fr 0;\n    width: 0;\n    padding: 0;\n    opacity: 0;\n}\n\n.main-layout.both-hidden .sidebar {\n    transform: translateX(-var(--sidebar-width));\n}\n\n.main-layout.both-hidden .right-sidebar {\n    transform: translateX(var(--sidebar-width));\n}\n\n/* Toggle Buttons */\n.panel-toggle {\n    position: fixed;\n    background: var(--panel-bg);\n    border: 1px solid var(--border);\n    color: var(--text);\n    width: 24px;\n    height: 60px;\n    cursor: pointer;\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    transition: var(--transition);\n    z-index: 100;\n}\n\n.panel-toggle:hover {\n    background: var(--primary);\n    color: white;\n}\n\n.left-panel-toggle {\n    left: 0;\n    top: 50%;\n    transform: translateY(-50%);\n    border-radius: 0 4px 4px 0;\n}\n\n.right-panel-toggle {\n    right: 0;\n    top: 50%;\n    transform: translateY(-50%);\n    border-radius: 4px 0 0 4px;\n}\n\n/* Hide all sidebar content when collapsed except toggle button */\n.main-layout.left-hidden .sidebar &gt; *:not(.panel-toggle),\n.main-layout.right-hidden .right-sidebar &gt; *:not(.panel-toggle) {\n    display: none;\n}\n\n/* Media Queries */\n@media (max-width: 1024px) {\n    .main-layout {\n        --sidebar-width: 65vw;\n    }\n}\n\n@media (max-width: 768px) {\n    .main-layout {\n        grid-template-columns: 1fr;\n    }\n\n    .sidebar, .right-sidebar {\n        position: fixed;\n        top: 0;\n        height: 100vh;\n        z-index: 50;\n    }\n\n    .sidebar {\n        left: 0;\n        transform: translateX(-100%);\n    }\n\n    .right-sidebar {\n        right: 0;\n        transform: translateX(100%);\n    }\n\n    .main-layout:not(.left-hidden) .sidebar {\n        transform: translateX(0);\n    }\n\n    .main-layout:not(.right-hidden) .right-sidebar {\n        transform: translateX(0);\n    }\n\n    .chat-header {\n        padding: 0.75rem;\n    }\n\n    #agentSelect {\n        max-width: 150px;\n    }\n}\n\n@media (max-width: 480px) {\n    .message {\n        flex-direction: column;\n    }\n\n    .chat-input {\n        padding: 0.75rem;\n    }\n\n    #messageInput {\n        font-size: 16px; /* Prevent zoom on mobile */\n    }\n\n    .chat-header {\n        flex-direction: column;\n        gap: 0.5rem;\n    }\n\n    #agentSelect {\n        width: 100%;\n        max-width: none;\n    }\n\n    .controls {\n        display: flex;\n        gap: 0.5rem;\n        width: 100%;\n    }\n\n    .controls button {\n        flex: 1;\n        padding: 0.5rem;\n    }\n}\n\n/* Main Container */\n.main-container {\n    display: flex;\n    flex-direction: column;\n    height: 100vh;\n    overflow: hidden;\n}\n\n/* Chat Header */\n.chat-header {\n    padding: 1rem;\n    border-bottom: 1px solid var(--border);\n    display: flex;\n    justify-content: space-between;\n    align-items: center;\n    background: var(--panel-bg);\n}\n\n/* Chat Container */\n.chat-container {\n    flex: 1;\n    overflow-y: auto;\n    padding: 1rem;\n}\n\n.message {\n    display: flex;\n    gap: 1rem;\n    margin-bottom: 1rem;\n    padding: 1rem;\n    border-radius: 8px;\n    /*background: var(--panel-bg); */\n\n    backdrop-filter: blur(5px);\n    border: 1px solid rgba(255, 255, 255, 0.1);\n    transition: all 0.3s ease;\n    animation: messageAppear 0.5s ease-out;\n}\n\n\n@keyframes messageAppear {\n    from {\n        opacity: 0;\n        transform: translateY(20px);\n    }\n    to {\n        opacity: 1;\n        transform: translateY(0);\n    }\n}\n\n.user-message {\n    margin-left: auto;\n    background: var(--primary);\n}\n\n.message-avatar {\n    width: 32px;\n    height: 32px;\n    border-radius: 50%;\n    background: var(--primary-light);\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    font-weight: bold;\n}\n\n.message-content {\n    flex: 1;\n}\n\n/* Input Area */\n.chat-input {\n    padding: 1rem;\n    border-top: 1px solid var(--border);\n    display: flex;\n    gap: 1rem;\n    background: var(--panel-bg);\n}\n\n#messageInput {\n    flex: 1;\n    padding: 0.75rem;\n    border: 1px solid var(--border);\n    border-radius: 4px;\n    background: var(--bg-dark);\n    color: var(--text);\n}\n\n/* Buttons */\nbutton {\n    padding: 0.75rem 1.5rem;\n    border: none;\n    border-radius: 4px;\n    background: var(--primary);\n    color: white;\n    cursor: pointer;\n    transition: var(--transition);\n}\n\nbutton:hover {\n    background: var(--primary-light);\n}\n\n/* Status Indicators */\n.status-container {\n    display: flex;\n    align-items: center;\n    gap: 0.5rem;\n    margin-bottom: 1rem;\n    padding: 0.75rem;\n    background: var(--bg-dark);\n    border-radius: 4px;\n}\n\n.status-indicator {\n    width: 10px;\n    height: 10px;\n    border-radius: 50%;\n    transition: var(--transition);\n}\n\n.status-idle { background: var(--primary); }\n.status-active { background: var(--success); }\n.status-error { background: var(--danger); }\n\n/* Log Panel */\n.log-panel {\n    background: var(--bg-dark);\n    border: 1px solid var(--border);\n    border-radius: 4px;\n    padding: 0.75rem;\n    height: 80vh;\n    overflow-y: auto;\n    font-family: monospace;\n    font-size: 0.875rem;\n}\n\n.log-entry {\n    padding: 0.5rem;\n    border-bottom: 1px solid var(--border);\n}\n\n.log-internal {\n    color: var(--primary-light);\n    font-style: italic;\n}\n\n.log-error {\n    color: var(--danger);\n}\n\n/* File Upload */\n.file-upload-zone {\n    padding: 1rem;\n    border: 2px dashed var(--border);\n    border-radius: 4px;\n    text-align: center;\n    cursor: pointer;\n    margin-bottom: 1rem;\n}\n\n.file-upload-zone.drag-over {\n    border-color: var(--primary);\n    background: var(--bg-dark);\n}\n\n.file-item {\n    display: flex;\n    justify-content: space-between;\n    align-items: center;\n    padding: 0.5rem;\n    background: var(--bg-dark);\n    border-radius: 4px;\n    margin-bottom: 0.5rem;\n}\n\n/* Branch List */\n.branch-item {\n    display: flex;\n    justify-content: space-between;\n    align-items: center;\n    padding: 0.75rem;\n    background: var(--bg-dark);\n    border-radius: 4px;\n    margin-bottom: 0.5rem;\n}\n\n.branch-item.active {\n    border: 1px solid var(--primary);\n}\n\n\n\n@keyframes pulse {\n    0% { box-shadow: 0 0 20px var(--primary-glow); }\n    50% { box-shadow: 0 0 40px var(--primary-glow), 0 0 60px var(--accent-glow); }\n    100% { box-shadow: 0 0 20px var(--primary-glow); }\n}\n\n\n/* Agent Select Styling */\n#agentSelect {\n    padding: 0.75rem 2rem 0.75rem 1rem;\n    font-size: 0.875rem;\n    color: var(--text);\n    background-color: var(--bg-dark);\n    border: 1px solid var(--border);\n    border-radius: 4px;\n    appearance: none;\n    cursor: pointer;\n    transition: var(--transition);\n    background-image: url(\"data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='16' height='16' viewBox='0 0 24 24' fill='none' stroke='%2394a3b8' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpath d='M6 9l6 6 6-6'%3E%3C/path%3E%3C/svg%3E\");\n    background-repeat: no-repeat;\n    background-position: right 0.75rem center;\n    background-size: 16px;\n}\n\n#agentSelect:hover {\n    border-color: var(--primary);\n}\n\n#agentSelect:focus {\n    outline: none;\n    border-color: var(--primary);\n    box-shadow: 0 0 0 2px var(--primary-light);\n}\n\n#agentSelect option {\n    background-color: var(--panel-bg);\n    color: var(--text);\n    padding: 0.5rem;\n}\n&lt;/style&gt;\n&lt;body&gt;\n    &lt;div class=\"main-layout both-hidden\"&gt;\n        &lt;div class=\"sidebar\"&gt;\n            &lt;h3&gt;Branches&lt;/h3&gt;\n\n            &lt;div class=\"file-upload-zone\" id=\"fileUploadZone\"&gt;\n                &lt;input type=\"file\" id=\"fileInput\" multiple&gt;\n                &lt;p&gt;Drag &amp; drop files here or click to select&lt;/p&gt;\n            &lt;/div&gt;\n            &lt;div class=\"file-list\" id=\"fileList\"&gt;&lt;/div&gt;\n            &lt;div class=\"section-header\"&gt;\n                Branches\n                &lt;button id=\"new-branch-button\"&gt;New Branch&lt;/button&gt;\n            &lt;/div&gt;\n            &lt;div id=\"branchList\" class=\"branch-list\"&gt;&lt;/div&gt;\n            &lt;div id=\"verboseOutput\" class=\"verbose-output\"&gt;&lt;/div&gt;\n          &lt;/div&gt;\n           &lt;button class=\"panel-toggle left-panel-toggle\" onclick=\"toggleLeftPanel()\"&gt;\u25c0&lt;/button&gt;\n        &lt;div class=\"main-container\"&gt;\n            &lt;div class=\"chat-header\"&gt;\n            &lt;label for=\"agentSelect\"&gt;Select Agent or System&lt;/label&gt;\n                &lt;select id=\"agentSelect\"&gt;\n                    &lt;option value=\"\"&gt;ISAA system&lt;/option&gt;\n                    $agent_options\n                &lt;/select&gt;\n            &lt;/div&gt;\n            &lt;div id=\"chat\" class=\"chat-container\"&gt;\n                &lt;div class=\"messages-container\"&gt;&lt;/div&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"chat-input\"&gt;\n                    &lt;input type=\"text\" id=\"messageInput\" placeholder=\"Type your message...\"&gt;\n                    &lt;button id=\"sendButton\"&gt;Send&lt;/button&gt;\n                &lt;/div&gt;\n        &lt;/div&gt;\n        &lt;div class=\"right-sidebar\"&gt;\n            &lt;div class=\"section-header\"&gt;Agent Status&lt;/div&gt;\n            &lt;div class=\"agent-status\"&gt;\n                &lt;div class=\"status-indicator\"&gt;&lt;/div&gt;\n                &lt;span id=\"agentStatus\"&gt;Idle&lt;/span&gt;\n            &lt;/div&gt;\n\n            &lt;div id=\"logPanel\" class=\"log-panel\"&gt;\n            &lt;div class=\"section-header\"&gt;Internal&lt;/div&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n        &lt;button class=\"panel-toggle right-panel-toggle\" onclick=\"toggleRightPanel()\"&gt;\u25b6&lt;/button&gt;\n    &lt;/div&gt;\n\n    &lt;script&gt;\n\n    function toggleLeftPanel() {\n    const layout = document.querySelector('.main-layout');\n    if (layout.classList.contains('right-hidden')) {\n        layout.classList.replace('right-hidden', 'both-hidden');\n    } else if (layout.classList.contains('both-hidden')) {\n        layout.classList.remove('both-hidden');\n        layout.classList.add('right-hidden');\n    } else if (layout.classList.contains('left-hidden')) {\n        layout.classList.remove('left-hidden');\n    } else {\n        layout.classList.add('left-hidden');\n    }\n}\n\nfunction toggleRightPanel() {\n    const layout = document.querySelector('.main-layout');\n    if (layout.classList.contains('left-hidden')) {\n        layout.classList.replace('left-hidden', 'both-hidden');\n    } else if (layout.classList.contains('both-hidden')) {\n        layout.classList.remove('both-hidden');\n        layout.classList.add('left-hidden');\n    } else if (layout.classList.contains('right-hidden')) {\n        layout.classList.remove('right-hidden');\n    } else {\n        layout.classList.add('right-hidden');\n    }\n}\nclass StatusManager {\n    constructor() {\n        this.logPanel = document.getElementById('logPanel');\n        this.statusIndicator = document.querySelector('.status-indicator');\n    }\n\n    updateStatus(state) {\n        this.statusIndicator.className = 'status-indicator';\n        this.statusIndicator.classList.add(`status-${state}`);\n        this.addLog(`Status changed to: ${state}`);\n    }\n\n    addLog(message, type = 'info') {\n        const entry = document.createElement('div');\n        const time_ = document.createElement('div');\n        entry.className = `log-entry log-${type}`;\n        time_.textContent = `[${new Date().toLocaleTimeString()}]`;\n        entry.innerHTML = marked.parse(message)\n        entry.appendChild(time_);\n        this.logPanel.appendChild(entry);\n        this.logPanel.scrollTop = this.logPanel.scrollHeight;\n    }\n}\n\n    class EnhancedChat {\n    constructor() {\n        this.ws = null;\n        this.particles = [];\n\n        this.setupWebSocket();\n        this.setupUI();\n\n        this.reconnectAttempts = 0;\n        this.maxReconnectAttempts = 5;\n        this.setupFileUpload();\n        this.files = new Map();\n\n        this.currentBranch = null;\n    }\n\n    setupFileUpload() {\n        const zone = document.getElementById('fileUploadZone');\n        const input = document.getElementById('fileInput');\n        const fileList = document.getElementById('fileList');\n\n        zone.addEventListener('click', () =&gt; input.click());\n\n        zone.addEventListener('dragover', (e) =&gt; {\n            e.preventDefault();\n            zone.classList.add('drag-over');\n        });\n\n        zone.addEventListener('dragleave', () =&gt; {\n            zone.classList.remove('drag-over');\n        });\n\n        zone.addEventListener('drop', (e) =&gt; {\n            e.preventDefault();\n            zone.classList.remove('drag-over');\n            this.handleFiles(e.dataTransfer.files);\n        });\n\n        input.addEventListener('change', (e) =&gt; {\n            this.handleFiles(e.target.files);\n        });\n    }\n\n    handleFiles(fileList) {\n        Array.from(fileList).forEach(file =&gt; {\n            const fileId = `file-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;\n            this.files.set(fileId, file);\n\n            const fileItem = document.createElement('div');\n            fileItem.className = 'file-item';\n            fileItem.innerHTML = `\n                &lt;span&gt;${file.name}&lt;/span&gt;\n                &lt;div&gt;\n                    &lt;button onclick=\"chat.uploadFile('${fileId}')\"&gt;Upload&lt;/button&gt;\n                    &lt;button onclick=\"chat.removeFile('${fileId}')\"&gt;Remove&lt;/button&gt;\n                &lt;/div&gt;\n                &lt;div class=\"file-progress\"&gt;&lt;/div&gt;\n            `;\n\n            document.getElementById('fileList').appendChild(fileItem);\n        });\n    }\n\n    async uploadFile(fileId) {\n        const file = this.files.get(fileId);\n        if (!file) return;\n\n        const fileItem = document.querySelector(`[data-file-id=\"${fileId}\"]`);\n        const progress = fileItem.querySelector('.file-progress');\n\n        try {\n            const formData = new FormData();\n            formData.append('file', file);\n\n            const xhr = new XMLHttpRequest();\n            xhr.upload.onprogress = (e) =&gt; {\n                if (e.lengthComputable) {\n                    const percentComplete = (e.loaded / e.total) * 100;\n                    progress.style.width = percentComplete + '%';\n                }\n            };\n\n            const response = await fetch('/api/isaa/upload', {\n                method: 'POST',\n                body: formData\n            });\n\n            if (response.ok) {\n                const result = await response.json();\n                this.addMessage('system', `File uploaded: ${file.name}`);\n                this.removeFile(fileId);\n            } else {\n                throw new Error('Upload failed');\n            }\n        } catch (error) {\n            this.addMessage('error', `Failed to upload ${file.name}: ${error.message}`);\n            progress.style.backgroundColor = '#ef4444';\n        }\n    }\n\n    removeFile(fileId) {\n        this.files.delete(fileId);\n        const fileItem = document.querySelector(`[data-file-id=\"${fileId}\"]`);\n        if (fileItem) {\n            fileItem.remove();\n        }\n    }\n\n\n    setupWebSocket() {\n        if (this.ws) {\n            this.ws.close();\n        }\n\n        this.ws = new WebSocket(`wss://${window.location.host}/api/isaa/chat_websocket`);\n        this.ws.onmessage = (event) =&gt; this.handleMessage(JSON.parse(event.data));\n        this.ws.onclose = () =&gt; this.handleDisconnect();\n        this.ws.onerror = (error) =&gt; this.handleError(error);\n\n        this.ws.onopen = () =&gt; {\n            this.reconnectAttempts = 0;\n        };\n    }\n\n    handleDisconnect() {\n        if (this.reconnectAttempts &lt; this.maxReconnectAttempts) {\n            this.reconnectAttempts++;\n            setTimeout(() =&gt; this.setupWebSocket(), 5000);\n        } else {\n            alert('Failed to reconnect. Please refresh the page.');\n        }\n    }\n\n    handleError(error) {\n        console.error('WebSocket error:', error);\n    }\n\n\n    setupUI() {\n        document.getElementById('messageInput').addEventListener('keypress', (e) =&gt; {\n            if (e.key === 'Enter') this.sendMessage();\n        });\n\n        document.getElementById('sendButton').onclick = () =&gt; this.sendMessage();\n        document.getElementById('new-branch-button').onclick = () =&gt; this.createNewBranch();\n    }\n\n    handleMessage(data) {\n        const statusManager = new StatusManager();\n\n        console.log(data, data.type);\n        switch (data.type) {\n            case 'stream':\n                if (data.agent.endsWith('-internal')) {\n                    statusManager.addLog(data.agent+'\\\\n'+data.content, 'internal');\n                } else {\n                    this.addMessage('agent', data.content, data.agent);\n                    statusManager.updateStatus('responding');\n                }\n                break;\n            case 'error':\n                this.addMessage('error', data.content);\n                statusManager.updateStatus('error');\n                statusManager.addLog(data.content, 'error');\n                break;\n            case 'agent_state':\n                this.updateAgentState(data.state);\n                break;\n            case 'branches':\n                this.updateBranchList(data.branches);\n                break;\n            case 'ping':\n                this.ws.send(JSON.stringify({ type: 'pong' }));\n                break;\n        }\n    }\n\n    updateAgentState(state) {\n        const statusIndicator = document.querySelector('.status-indicator');\n        const statusText = document.getElementById('agentStatus');\n        const verboseOutput = document.getElementById('verboseOutput');\n\n        if (state.is_running) {\n            statusIndicator.classList.add('running');\n            statusIndicator.classList.remove('idle');\n            statusText.textContent = 'Running';\n        } else {\n            statusIndicator.classList.add('idle');\n            statusIndicator.classList.remove('running');\n            statusText.textContent = 'Idle';\n        }\n\n        if (state.verbose_output) {\n            verboseOutput.textContent = state.verbose_output.join('\\\\n');\n        }\n\n    }\n\n    updateBranchList(branches) {\n        const branchList = document.getElementById('branchList');\n        branchList.innerHTML = '';\n\n        branches.forEach(branch =&gt; {\n            const branchDiv = document.createElement('div');\n            branchDiv.className = `branch-item ${branch.id === this.currentBranch ? 'active' : ''}`;\n\n            branchDiv.innerHTML = `\n                &lt;span&gt;${branch.id}&lt;/span&gt;\n                &lt;div class=\"branch-actions\"&gt;\n                    &lt;button onclick=\"chat.switchBranch('${branch.id}')\"&gt;Switch&lt;/button&gt;\n                    &lt;button onclick=\"chat.restoreChat('${branch.id}')\"&gt;Restore&lt;/button&gt;\n                &lt;/div&gt;\n            `;\n\n            branchList.appendChild(branchDiv);\n        });\n\n        document.getElementById('currentBranch').textContent = `Current Branch: ${this.currentBranch || 'None'}`;\n    }\n\n    switchBranch(branchId) {\n        this.currentBranch = branchId;\n        this.ws.send(JSON.stringify({\n            type: 'switch_branch',\n            branch_id: branchId\n        }));\n        this.updateBranchList([{ id: branchId }]);\n    }\n\n    restoreChat(branchId) {\n        this.ws.send(JSON.stringify({\n            type: 'restore_chat',\n            branch_id: branchId\n        }));\n    }\n\n    createNewBranch() {\n        const branchId = `branch-${Date.now()}`;\n        this.ws.send(JSON.stringify({\n            type: 'create_branch',\n            branch_id: branchId\n        }));\n    }\n\n    addToLogs(message) {\n        const logPanel = document.getElementById('logPanel');\n        const logEntry = document.createElement('div');\n        logEntry.textContent = `${new Date().toISOString()} - ${message}`;\n        logPanel.appendChild(logEntry);\n        logPanel.scrollTop = logPanel.scrollHeight;\n    }\n\n    sendMessage() {\n        const input = document.getElementById('messageInput');\n        const agentSelect = document.getElementById('agentSelect');\n\n        if (!input.value) return;\n\n        this.ws.send(JSON.stringify({\n            type: 'message',\n            content: input.value,\n            agent: agentSelect.value\n        }));\n\n        this.addMessage('user', input.value);\n        input.value = '';\n    }\n\n    addMessage(role, content, agent = null) {\n        if (agent &amp;&amp; agent.endsWith('-internal')) {\n            return;\n        }\n        const chat = document.getElementById('chat');\n        const messageDiv = document.createElement('div');\n        messageDiv.className = `message ${role}-message`;\n\n        const avatar = document.createElement('div');\n        avatar.className = 'message-avatar';\n        avatar.textContent = role === 'user' ? 'U' : 'A';\n\n        const contentDiv = document.createElement('div');\n        contentDiv.className = 'message-content';\n\n        if (agent) {\n            contentDiv.innerHTML = `&lt;strong&gt;${agent}:&lt;/strong&gt; ${marked.parse(content)}`;\n        } else {\n            contentDiv.innerHTML =  marked.parse(content);\n        }\n\n        messageDiv.appendChild(avatar);\n        messageDiv.appendChild(contentDiv);\n        chat.appendChild(messageDiv);\n        chat.scrollTop = chat.scrollHeight;\n    }\n}\n\nwindow.onload = () =&gt; new EnhancedChat();\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n        \"\"\"\n\n        # Generate agent options HTML\n        agent_options = ''.join([\n            f'&lt;option value=\"{agent}\"&gt;{agent}&lt;/option&gt;'\n            for agent in self.isaa.config.get('agents-name-list', [])\n        ])\n\n        return template.replace(\"$agent_options\", agent_options)\n</code></pre> <code>get_widget(**kwargs)</code> \u00b6 <p>Generate the HTML widget</p> Source code in <code>toolboxv2/mods/isaa/ui/nice.py</code> <pre><code>    def get_widget(self, **kwargs):\n        \"\"\"Generate the HTML widget\"\"\"\n        template = \"\"\"\n        &lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Enhanced Chat Interface&lt;/title&gt;\n&lt;/head&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"&gt;&lt;/script&gt;\n&lt;style&gt;\n:root {\n    --primary: #4f46e5;\n    --primary-light: #6366f1;\n    --bg-dark: #0f172a;\n    --panel-bg: #1e293b;\n    --text: #e2e8f0;\n    --text-muted: #94a3b8;\n    --border: #334155;\n    --danger: #ef4444;\n    --success: #22c55e;\n    --transition: all 0.3s ease;\n    --sidebar-width: 45vw;\n}\n\n/* Base Styles */\nbody {\n    margin: 0;\n    font-family: system-ui, -apple-system, sans-serif;\n    background: var(--bg-dark);\n    color: var(--text);\n    line-height: 1.5;\n    overflow-x: hidden;\n}\n/* Layout */\n.main-layout {\n    display: grid;\n    grid-template-columns: 0.25fr 1fr 0.25fr;\n    height: 100vh;\n    transition: var(--transition);\n    position: relative;\n}\n\n/* Panel Styles */\n.sidebar, .right-sidebar {\n    background: var(--panel-bg);\n    border-right: 1px solid var(--border);\n    padding: 1.5rem;\n    overflow-y: auto;\n    position: relative;\n    transition: var(--transition);\n}\n\n.right-sidebar {\n    border-left: 1px solid var(--border);\n    border-right: none;\n    width: var(--sidebar-width);\n}\n\n/* Panel Toggle States */\n.main-layout.left-hidden .sidebar {\ngrid-template-columns: 0 1fr var(--sidebar-width);\n    transform: translateX(-var(--sidebar-width));\n    width: 0;\n    padding: 0;\n    opacity: 0;\n}\n\n.main-layout.right-hidden .right-sidebar {\ngrid-template-columns: var(--sidebar-width) 1fr 0;\n    transform: translateX(var(--sidebar-width));\n    width: 0;\n    padding: 0;\n    opacity: 0;\n}\n\n.main-layout.both-hidden .sidebar,\n.main-layout.both-hidden .right-sidebar {\ngrid-template-columns: 0 1fr 0;\n    width: 0;\n    padding: 0;\n    opacity: 0;\n}\n\n.main-layout.both-hidden .sidebar {\n    transform: translateX(-var(--sidebar-width));\n}\n\n.main-layout.both-hidden .right-sidebar {\n    transform: translateX(var(--sidebar-width));\n}\n\n/* Toggle Buttons */\n.panel-toggle {\n    position: fixed;\n    background: var(--panel-bg);\n    border: 1px solid var(--border);\n    color: var(--text);\n    width: 24px;\n    height: 60px;\n    cursor: pointer;\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    transition: var(--transition);\n    z-index: 100;\n}\n\n.panel-toggle:hover {\n    background: var(--primary);\n    color: white;\n}\n\n.left-panel-toggle {\n    left: 0;\n    top: 50%;\n    transform: translateY(-50%);\n    border-radius: 0 4px 4px 0;\n}\n\n.right-panel-toggle {\n    right: 0;\n    top: 50%;\n    transform: translateY(-50%);\n    border-radius: 4px 0 0 4px;\n}\n\n/* Hide all sidebar content when collapsed except toggle button */\n.main-layout.left-hidden .sidebar &gt; *:not(.panel-toggle),\n.main-layout.right-hidden .right-sidebar &gt; *:not(.panel-toggle) {\n    display: none;\n}\n\n/* Media Queries */\n@media (max-width: 1024px) {\n    .main-layout {\n        --sidebar-width: 65vw;\n    }\n}\n\n@media (max-width: 768px) {\n    .main-layout {\n        grid-template-columns: 1fr;\n    }\n\n    .sidebar, .right-sidebar {\n        position: fixed;\n        top: 0;\n        height: 100vh;\n        z-index: 50;\n    }\n\n    .sidebar {\n        left: 0;\n        transform: translateX(-100%);\n    }\n\n    .right-sidebar {\n        right: 0;\n        transform: translateX(100%);\n    }\n\n    .main-layout:not(.left-hidden) .sidebar {\n        transform: translateX(0);\n    }\n\n    .main-layout:not(.right-hidden) .right-sidebar {\n        transform: translateX(0);\n    }\n\n    .chat-header {\n        padding: 0.75rem;\n    }\n\n    #agentSelect {\n        max-width: 150px;\n    }\n}\n\n@media (max-width: 480px) {\n    .message {\n        flex-direction: column;\n    }\n\n    .chat-input {\n        padding: 0.75rem;\n    }\n\n    #messageInput {\n        font-size: 16px; /* Prevent zoom on mobile */\n    }\n\n    .chat-header {\n        flex-direction: column;\n        gap: 0.5rem;\n    }\n\n    #agentSelect {\n        width: 100%;\n        max-width: none;\n    }\n\n    .controls {\n        display: flex;\n        gap: 0.5rem;\n        width: 100%;\n    }\n\n    .controls button {\n        flex: 1;\n        padding: 0.5rem;\n    }\n}\n\n/* Main Container */\n.main-container {\n    display: flex;\n    flex-direction: column;\n    height: 100vh;\n    overflow: hidden;\n}\n\n/* Chat Header */\n.chat-header {\n    padding: 1rem;\n    border-bottom: 1px solid var(--border);\n    display: flex;\n    justify-content: space-between;\n    align-items: center;\n    background: var(--panel-bg);\n}\n\n/* Chat Container */\n.chat-container {\n    flex: 1;\n    overflow-y: auto;\n    padding: 1rem;\n}\n\n.message {\n    display: flex;\n    gap: 1rem;\n    margin-bottom: 1rem;\n    padding: 1rem;\n    border-radius: 8px;\n    /*background: var(--panel-bg); */\n\n    backdrop-filter: blur(5px);\n    border: 1px solid rgba(255, 255, 255, 0.1);\n    transition: all 0.3s ease;\n    animation: messageAppear 0.5s ease-out;\n}\n\n\n@keyframes messageAppear {\n    from {\n        opacity: 0;\n        transform: translateY(20px);\n    }\n    to {\n        opacity: 1;\n        transform: translateY(0);\n    }\n}\n\n.user-message {\n    margin-left: auto;\n    background: var(--primary);\n}\n\n.message-avatar {\n    width: 32px;\n    height: 32px;\n    border-radius: 50%;\n    background: var(--primary-light);\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    font-weight: bold;\n}\n\n.message-content {\n    flex: 1;\n}\n\n/* Input Area */\n.chat-input {\n    padding: 1rem;\n    border-top: 1px solid var(--border);\n    display: flex;\n    gap: 1rem;\n    background: var(--panel-bg);\n}\n\n#messageInput {\n    flex: 1;\n    padding: 0.75rem;\n    border: 1px solid var(--border);\n    border-radius: 4px;\n    background: var(--bg-dark);\n    color: var(--text);\n}\n\n/* Buttons */\nbutton {\n    padding: 0.75rem 1.5rem;\n    border: none;\n    border-radius: 4px;\n    background: var(--primary);\n    color: white;\n    cursor: pointer;\n    transition: var(--transition);\n}\n\nbutton:hover {\n    background: var(--primary-light);\n}\n\n/* Status Indicators */\n.status-container {\n    display: flex;\n    align-items: center;\n    gap: 0.5rem;\n    margin-bottom: 1rem;\n    padding: 0.75rem;\n    background: var(--bg-dark);\n    border-radius: 4px;\n}\n\n.status-indicator {\n    width: 10px;\n    height: 10px;\n    border-radius: 50%;\n    transition: var(--transition);\n}\n\n.status-idle { background: var(--primary); }\n.status-active { background: var(--success); }\n.status-error { background: var(--danger); }\n\n/* Log Panel */\n.log-panel {\n    background: var(--bg-dark);\n    border: 1px solid var(--border);\n    border-radius: 4px;\n    padding: 0.75rem;\n    height: 80vh;\n    overflow-y: auto;\n    font-family: monospace;\n    font-size: 0.875rem;\n}\n\n.log-entry {\n    padding: 0.5rem;\n    border-bottom: 1px solid var(--border);\n}\n\n.log-internal {\n    color: var(--primary-light);\n    font-style: italic;\n}\n\n.log-error {\n    color: var(--danger);\n}\n\n/* File Upload */\n.file-upload-zone {\n    padding: 1rem;\n    border: 2px dashed var(--border);\n    border-radius: 4px;\n    text-align: center;\n    cursor: pointer;\n    margin-bottom: 1rem;\n}\n\n.file-upload-zone.drag-over {\n    border-color: var(--primary);\n    background: var(--bg-dark);\n}\n\n.file-item {\n    display: flex;\n    justify-content: space-between;\n    align-items: center;\n    padding: 0.5rem;\n    background: var(--bg-dark);\n    border-radius: 4px;\n    margin-bottom: 0.5rem;\n}\n\n/* Branch List */\n.branch-item {\n    display: flex;\n    justify-content: space-between;\n    align-items: center;\n    padding: 0.75rem;\n    background: var(--bg-dark);\n    border-radius: 4px;\n    margin-bottom: 0.5rem;\n}\n\n.branch-item.active {\n    border: 1px solid var(--primary);\n}\n\n\n\n@keyframes pulse {\n    0% { box-shadow: 0 0 20px var(--primary-glow); }\n    50% { box-shadow: 0 0 40px var(--primary-glow), 0 0 60px var(--accent-glow); }\n    100% { box-shadow: 0 0 20px var(--primary-glow); }\n}\n\n\n/* Agent Select Styling */\n#agentSelect {\n    padding: 0.75rem 2rem 0.75rem 1rem;\n    font-size: 0.875rem;\n    color: var(--text);\n    background-color: var(--bg-dark);\n    border: 1px solid var(--border);\n    border-radius: 4px;\n    appearance: none;\n    cursor: pointer;\n    transition: var(--transition);\n    background-image: url(\"data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='16' height='16' viewBox='0 0 24 24' fill='none' stroke='%2394a3b8' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpath d='M6 9l6 6 6-6'%3E%3C/path%3E%3C/svg%3E\");\n    background-repeat: no-repeat;\n    background-position: right 0.75rem center;\n    background-size: 16px;\n}\n\n#agentSelect:hover {\n    border-color: var(--primary);\n}\n\n#agentSelect:focus {\n    outline: none;\n    border-color: var(--primary);\n    box-shadow: 0 0 0 2px var(--primary-light);\n}\n\n#agentSelect option {\n    background-color: var(--panel-bg);\n    color: var(--text);\n    padding: 0.5rem;\n}\n&lt;/style&gt;\n&lt;body&gt;\n    &lt;div class=\"main-layout both-hidden\"&gt;\n        &lt;div class=\"sidebar\"&gt;\n            &lt;h3&gt;Branches&lt;/h3&gt;\n\n            &lt;div class=\"file-upload-zone\" id=\"fileUploadZone\"&gt;\n                &lt;input type=\"file\" id=\"fileInput\" multiple&gt;\n                &lt;p&gt;Drag &amp; drop files here or click to select&lt;/p&gt;\n            &lt;/div&gt;\n            &lt;div class=\"file-list\" id=\"fileList\"&gt;&lt;/div&gt;\n            &lt;div class=\"section-header\"&gt;\n                Branches\n                &lt;button id=\"new-branch-button\"&gt;New Branch&lt;/button&gt;\n            &lt;/div&gt;\n            &lt;div id=\"branchList\" class=\"branch-list\"&gt;&lt;/div&gt;\n            &lt;div id=\"verboseOutput\" class=\"verbose-output\"&gt;&lt;/div&gt;\n          &lt;/div&gt;\n           &lt;button class=\"panel-toggle left-panel-toggle\" onclick=\"toggleLeftPanel()\"&gt;\u25c0&lt;/button&gt;\n        &lt;div class=\"main-container\"&gt;\n            &lt;div class=\"chat-header\"&gt;\n            &lt;label for=\"agentSelect\"&gt;Select Agent or System&lt;/label&gt;\n                &lt;select id=\"agentSelect\"&gt;\n                    &lt;option value=\"\"&gt;ISAA system&lt;/option&gt;\n                    $agent_options\n                &lt;/select&gt;\n            &lt;/div&gt;\n            &lt;div id=\"chat\" class=\"chat-container\"&gt;\n                &lt;div class=\"messages-container\"&gt;&lt;/div&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"chat-input\"&gt;\n                    &lt;input type=\"text\" id=\"messageInput\" placeholder=\"Type your message...\"&gt;\n                    &lt;button id=\"sendButton\"&gt;Send&lt;/button&gt;\n                &lt;/div&gt;\n        &lt;/div&gt;\n        &lt;div class=\"right-sidebar\"&gt;\n            &lt;div class=\"section-header\"&gt;Agent Status&lt;/div&gt;\n            &lt;div class=\"agent-status\"&gt;\n                &lt;div class=\"status-indicator\"&gt;&lt;/div&gt;\n                &lt;span id=\"agentStatus\"&gt;Idle&lt;/span&gt;\n            &lt;/div&gt;\n\n            &lt;div id=\"logPanel\" class=\"log-panel\"&gt;\n            &lt;div class=\"section-header\"&gt;Internal&lt;/div&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n        &lt;button class=\"panel-toggle right-panel-toggle\" onclick=\"toggleRightPanel()\"&gt;\u25b6&lt;/button&gt;\n    &lt;/div&gt;\n\n    &lt;script&gt;\n\n    function toggleLeftPanel() {\n    const layout = document.querySelector('.main-layout');\n    if (layout.classList.contains('right-hidden')) {\n        layout.classList.replace('right-hidden', 'both-hidden');\n    } else if (layout.classList.contains('both-hidden')) {\n        layout.classList.remove('both-hidden');\n        layout.classList.add('right-hidden');\n    } else if (layout.classList.contains('left-hidden')) {\n        layout.classList.remove('left-hidden');\n    } else {\n        layout.classList.add('left-hidden');\n    }\n}\n\nfunction toggleRightPanel() {\n    const layout = document.querySelector('.main-layout');\n    if (layout.classList.contains('left-hidden')) {\n        layout.classList.replace('left-hidden', 'both-hidden');\n    } else if (layout.classList.contains('both-hidden')) {\n        layout.classList.remove('both-hidden');\n        layout.classList.add('left-hidden');\n    } else if (layout.classList.contains('right-hidden')) {\n        layout.classList.remove('right-hidden');\n    } else {\n        layout.classList.add('right-hidden');\n    }\n}\nclass StatusManager {\n    constructor() {\n        this.logPanel = document.getElementById('logPanel');\n        this.statusIndicator = document.querySelector('.status-indicator');\n    }\n\n    updateStatus(state) {\n        this.statusIndicator.className = 'status-indicator';\n        this.statusIndicator.classList.add(`status-${state}`);\n        this.addLog(`Status changed to: ${state}`);\n    }\n\n    addLog(message, type = 'info') {\n        const entry = document.createElement('div');\n        const time_ = document.createElement('div');\n        entry.className = `log-entry log-${type}`;\n        time_.textContent = `[${new Date().toLocaleTimeString()}]`;\n        entry.innerHTML = marked.parse(message)\n        entry.appendChild(time_);\n        this.logPanel.appendChild(entry);\n        this.logPanel.scrollTop = this.logPanel.scrollHeight;\n    }\n}\n\n    class EnhancedChat {\n    constructor() {\n        this.ws = null;\n        this.particles = [];\n\n        this.setupWebSocket();\n        this.setupUI();\n\n        this.reconnectAttempts = 0;\n        this.maxReconnectAttempts = 5;\n        this.setupFileUpload();\n        this.files = new Map();\n\n        this.currentBranch = null;\n    }\n\n    setupFileUpload() {\n        const zone = document.getElementById('fileUploadZone');\n        const input = document.getElementById('fileInput');\n        const fileList = document.getElementById('fileList');\n\n        zone.addEventListener('click', () =&gt; input.click());\n\n        zone.addEventListener('dragover', (e) =&gt; {\n            e.preventDefault();\n            zone.classList.add('drag-over');\n        });\n\n        zone.addEventListener('dragleave', () =&gt; {\n            zone.classList.remove('drag-over');\n        });\n\n        zone.addEventListener('drop', (e) =&gt; {\n            e.preventDefault();\n            zone.classList.remove('drag-over');\n            this.handleFiles(e.dataTransfer.files);\n        });\n\n        input.addEventListener('change', (e) =&gt; {\n            this.handleFiles(e.target.files);\n        });\n    }\n\n    handleFiles(fileList) {\n        Array.from(fileList).forEach(file =&gt; {\n            const fileId = `file-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;\n            this.files.set(fileId, file);\n\n            const fileItem = document.createElement('div');\n            fileItem.className = 'file-item';\n            fileItem.innerHTML = `\n                &lt;span&gt;${file.name}&lt;/span&gt;\n                &lt;div&gt;\n                    &lt;button onclick=\"chat.uploadFile('${fileId}')\"&gt;Upload&lt;/button&gt;\n                    &lt;button onclick=\"chat.removeFile('${fileId}')\"&gt;Remove&lt;/button&gt;\n                &lt;/div&gt;\n                &lt;div class=\"file-progress\"&gt;&lt;/div&gt;\n            `;\n\n            document.getElementById('fileList').appendChild(fileItem);\n        });\n    }\n\n    async uploadFile(fileId) {\n        const file = this.files.get(fileId);\n        if (!file) return;\n\n        const fileItem = document.querySelector(`[data-file-id=\"${fileId}\"]`);\n        const progress = fileItem.querySelector('.file-progress');\n\n        try {\n            const formData = new FormData();\n            formData.append('file', file);\n\n            const xhr = new XMLHttpRequest();\n            xhr.upload.onprogress = (e) =&gt; {\n                if (e.lengthComputable) {\n                    const percentComplete = (e.loaded / e.total) * 100;\n                    progress.style.width = percentComplete + '%';\n                }\n            };\n\n            const response = await fetch('/api/isaa/upload', {\n                method: 'POST',\n                body: formData\n            });\n\n            if (response.ok) {\n                const result = await response.json();\n                this.addMessage('system', `File uploaded: ${file.name}`);\n                this.removeFile(fileId);\n            } else {\n                throw new Error('Upload failed');\n            }\n        } catch (error) {\n            this.addMessage('error', `Failed to upload ${file.name}: ${error.message}`);\n            progress.style.backgroundColor = '#ef4444';\n        }\n    }\n\n    removeFile(fileId) {\n        this.files.delete(fileId);\n        const fileItem = document.querySelector(`[data-file-id=\"${fileId}\"]`);\n        if (fileItem) {\n            fileItem.remove();\n        }\n    }\n\n\n    setupWebSocket() {\n        if (this.ws) {\n            this.ws.close();\n        }\n\n        this.ws = new WebSocket(`wss://${window.location.host}/api/isaa/chat_websocket`);\n        this.ws.onmessage = (event) =&gt; this.handleMessage(JSON.parse(event.data));\n        this.ws.onclose = () =&gt; this.handleDisconnect();\n        this.ws.onerror = (error) =&gt; this.handleError(error);\n\n        this.ws.onopen = () =&gt; {\n            this.reconnectAttempts = 0;\n        };\n    }\n\n    handleDisconnect() {\n        if (this.reconnectAttempts &lt; this.maxReconnectAttempts) {\n            this.reconnectAttempts++;\n            setTimeout(() =&gt; this.setupWebSocket(), 5000);\n        } else {\n            alert('Failed to reconnect. Please refresh the page.');\n        }\n    }\n\n    handleError(error) {\n        console.error('WebSocket error:', error);\n    }\n\n\n    setupUI() {\n        document.getElementById('messageInput').addEventListener('keypress', (e) =&gt; {\n            if (e.key === 'Enter') this.sendMessage();\n        });\n\n        document.getElementById('sendButton').onclick = () =&gt; this.sendMessage();\n        document.getElementById('new-branch-button').onclick = () =&gt; this.createNewBranch();\n    }\n\n    handleMessage(data) {\n        const statusManager = new StatusManager();\n\n        console.log(data, data.type);\n        switch (data.type) {\n            case 'stream':\n                if (data.agent.endsWith('-internal')) {\n                    statusManager.addLog(data.agent+'\\\\n'+data.content, 'internal');\n                } else {\n                    this.addMessage('agent', data.content, data.agent);\n                    statusManager.updateStatus('responding');\n                }\n                break;\n            case 'error':\n                this.addMessage('error', data.content);\n                statusManager.updateStatus('error');\n                statusManager.addLog(data.content, 'error');\n                break;\n            case 'agent_state':\n                this.updateAgentState(data.state);\n                break;\n            case 'branches':\n                this.updateBranchList(data.branches);\n                break;\n            case 'ping':\n                this.ws.send(JSON.stringify({ type: 'pong' }));\n                break;\n        }\n    }\n\n    updateAgentState(state) {\n        const statusIndicator = document.querySelector('.status-indicator');\n        const statusText = document.getElementById('agentStatus');\n        const verboseOutput = document.getElementById('verboseOutput');\n\n        if (state.is_running) {\n            statusIndicator.classList.add('running');\n            statusIndicator.classList.remove('idle');\n            statusText.textContent = 'Running';\n        } else {\n            statusIndicator.classList.add('idle');\n            statusIndicator.classList.remove('running');\n            statusText.textContent = 'Idle';\n        }\n\n        if (state.verbose_output) {\n            verboseOutput.textContent = state.verbose_output.join('\\\\n');\n        }\n\n    }\n\n    updateBranchList(branches) {\n        const branchList = document.getElementById('branchList');\n        branchList.innerHTML = '';\n\n        branches.forEach(branch =&gt; {\n            const branchDiv = document.createElement('div');\n            branchDiv.className = `branch-item ${branch.id === this.currentBranch ? 'active' : ''}`;\n\n            branchDiv.innerHTML = `\n                &lt;span&gt;${branch.id}&lt;/span&gt;\n                &lt;div class=\"branch-actions\"&gt;\n                    &lt;button onclick=\"chat.switchBranch('${branch.id}')\"&gt;Switch&lt;/button&gt;\n                    &lt;button onclick=\"chat.restoreChat('${branch.id}')\"&gt;Restore&lt;/button&gt;\n                &lt;/div&gt;\n            `;\n\n            branchList.appendChild(branchDiv);\n        });\n\n        document.getElementById('currentBranch').textContent = `Current Branch: ${this.currentBranch || 'None'}`;\n    }\n\n    switchBranch(branchId) {\n        this.currentBranch = branchId;\n        this.ws.send(JSON.stringify({\n            type: 'switch_branch',\n            branch_id: branchId\n        }));\n        this.updateBranchList([{ id: branchId }]);\n    }\n\n    restoreChat(branchId) {\n        this.ws.send(JSON.stringify({\n            type: 'restore_chat',\n            branch_id: branchId\n        }));\n    }\n\n    createNewBranch() {\n        const branchId = `branch-${Date.now()}`;\n        this.ws.send(JSON.stringify({\n            type: 'create_branch',\n            branch_id: branchId\n        }));\n    }\n\n    addToLogs(message) {\n        const logPanel = document.getElementById('logPanel');\n        const logEntry = document.createElement('div');\n        logEntry.textContent = `${new Date().toISOString()} - ${message}`;\n        logPanel.appendChild(logEntry);\n        logPanel.scrollTop = logPanel.scrollHeight;\n    }\n\n    sendMessage() {\n        const input = document.getElementById('messageInput');\n        const agentSelect = document.getElementById('agentSelect');\n\n        if (!input.value) return;\n\n        this.ws.send(JSON.stringify({\n            type: 'message',\n            content: input.value,\n            agent: agentSelect.value\n        }));\n\n        this.addMessage('user', input.value);\n        input.value = '';\n    }\n\n    addMessage(role, content, agent = null) {\n        if (agent &amp;&amp; agent.endsWith('-internal')) {\n            return;\n        }\n        const chat = document.getElementById('chat');\n        const messageDiv = document.createElement('div');\n        messageDiv.className = `message ${role}-message`;\n\n        const avatar = document.createElement('div');\n        avatar.className = 'message-avatar';\n        avatar.textContent = role === 'user' ? 'U' : 'A';\n\n        const contentDiv = document.createElement('div');\n        contentDiv.className = 'message-content';\n\n        if (agent) {\n            contentDiv.innerHTML = `&lt;strong&gt;${agent}:&lt;/strong&gt; ${marked.parse(content)}`;\n        } else {\n            contentDiv.innerHTML =  marked.parse(content);\n        }\n\n        messageDiv.appendChild(avatar);\n        messageDiv.appendChild(contentDiv);\n        chat.appendChild(messageDiv);\n        chat.scrollTop = chat.scrollHeight;\n    }\n}\n\nwindow.onload = () =&gt; new EnhancedChat();\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n        \"\"\"\n\n        # Generate agent options HTML\n        agent_options = ''.join([\n            f'&lt;option value=\"{agent}\"&gt;{agent}&lt;/option&gt;'\n            for agent in self.isaa.config.get('agents-name-list', [])\n        ])\n\n        return template.replace(\"$agent_options\", agent_options)\n</code></pre> <code>handle_websocket_chat(websocket)</code> <code>async</code> \u00b6 <p>Handle main chat WebSocket connection</p> Source code in <code>toolboxv2/mods/isaa/ui/nice.py</code> <pre><code>async def handle_websocket_chat(self, websocket: WebSocket):\n    \"\"\"Handle main chat WebSocket connection\"\"\"\n    client_id = str(uuid.uuid4())\n    await self.connect_websocket(websocket, client_id)\n\n    try:\n        while True:\n            message = await websocket.receive_json()\n            if message['type'] == 'message':\n                await self.stream_agent_response(\n                    message['content'],\n                    client_id,\n                    message.get('agent')\n                )\n            elif message['type'] == 'get_branches':\n                network = self.isaa.get_memory().cognitive_network.network\n                branches = network.data_holder.get_visualization_data().get('branches', [])\n                await websocket.send_json({\n                    'type': 'branches',\n                    'branches': branches\n                })\n            elif message['type'] == 'switch_branch':\n                network = self.isaa.get_memory().cognitive_network.network\n                success = network.data_holder.switch_branch(\n                    network,\n                    message['branch_id']\n                )\n                await websocket.send_json({\n                    'type': 'branch_switch',\n                    'success': success\n                })\n\n            elif message['type'] == 't2s':\n                audio_data: bytes = self.isaa.app.run_any(TBEF.AUDIO.SPEECH, text=message['content'], voice_index=0,\n                                                          use_cache=False,\n                                                          provider='piper',\n                                                          config={'play_local': False,\n                                                                  'model_name': message.get('model_name', 'ryan')},\n                                                          local=False,\n                                                          save=False)\n\n                if not audio_data:\n                    return\n                await websocket.send_bytes(audio_data)\n    except Exception as e:\n        print(f\"WebSocket error: {e}\")\n    finally:\n        await self.disconnect_websocket(client_id)\n</code></pre> <code>stream_agent_response(message, client_id, agent_name=None)</code> <code>async</code> \u00b6 <p>Stream agent responses to the client with async task management</p> Source code in <code>toolboxv2/mods/isaa/ui/nice.py</code> <pre><code>async def stream_agent_response(self, message: str, client_id: str, agent_name: str | None = None):\n    \"\"\"Stream agent responses to the client with async task management\"\"\"\n    if client_id not in self.agent_states:\n        return\n\n    state = self.agent_states[client_id]\n    if state.is_running:\n        await self._send_error(client_id, \"An agent is already running\")\n        return\n\n    async def run_agent():\n        #try:\n        state.is_running = True\n        state.verbose_output = []\n        await self._send_agent_state(client_id)\n\n        def get_callback(agent_name_):\n            def helper(response_chunk, *a, **k):\n                try:\n                    return get_app('nice.get_callback.helper').run_a_from_sync(self._send_stream_update, *[client_id,\n                                                                                 response_chunk,\n                                                                                 agent_name_])\n                except Exception as e:\n                    print(f\"Agent {agent_name_} faint to report : {response_chunk}\", e)\n                    pass\n\n            return helper\n\n        self.isaa.default_setter = lambda x: x.set_verbose(True).set_post_callback(\n            get_callback(x.amd_attributes['name'])).set_print_verbose(\n            get_callback(x.amd_attributes['name'] + \"-internal\"))\n\n        # self.isaa.run_callback = get_callback(agent_name)\n\n        for agent, name in zip([self.isaa.get_agent(name_) for name_ in self.isaa.config['agents-name-list']],\n                               self.isaa.config['agents-name-list'], strict=False):\n            agent.post_callback = get_callback(name)\n            self._setup_verbose_override(agent, get_callback(name + \"-internal\"))\n\n        response = await asyncio.to_thread(\n            self.isaa.run_agent,\n            agent_name,\n            message,\n            persist=True,\n            verbose=True\n        )\n        # Save to network branch\n        network = self.isaa.get_memory().cognitive_network.network\n        branch_id = f\"chat-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n        network.data_holder.create_branch(network, branch_id)\n\n        # Save chat history\n        self.message_history.append({\n            'timestamp': datetime.now().isoformat(),\n            'role': 'agent' if agent_name else 'system',\n            'content': response,\n            'agent': agent_name,\n            'branch_id': branch_id,\n            'verbose_output': state.verbose_output\n        })\n\n        state.last_response = response\n        await self._send_agent_state(client_id)\n\n        #except Exception as e:\n        #    await self._send_error(client_id, str(e))\n        #finally:\n        # state.is_running = False\n        await self._send_agent_state(client_id)\n\n    task = asyncio.create_task(run_agent())\n    state.current_task = task\n    await self._monitor_agent_task(task, client_id)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.setup","title":"<code>setup</code>","text":"<p>Main task of this Mod : 0) from 0 device to base TB (user) 1) from 0 device to devEnv (dev) 2) from devEnv0 to core0 HMR 0 downtime updates (admin) steps :     0) :      * shell - setup python and env      * shell    # on linux and mac tet install python3-venv      * shell    # using a mini forward script as base installation calld tb taht runs the toolboxv2 in the venv      * shell - install ex dependency (node, docker, ...)      * shell    # using winget or apt or co ...      * shell - install ToolBoxV2 from git or pip installation dependent      * shell - init auth connection / login to core0      * shell    # in mini forward script save and crate session + pars to venv       #0 installation from mod_sto         - core from pip or git 0 exras (isaa qn dyt ttt diff alert cyr bkt shm auto)       #0 test local     1) :      - installation devModStorage from core0      - setup files (dev mods)      - test local Core      -&gt; install (MOD-NAME)         install from .yaml file          specs: name                 version                 dependencies                 extras    2) :      -# development (?ISAA?)         -~ uploade to devModStorage        -&gt; test local        -&gt; test remote      -&gt; move to mod_sto      -&gt; install on to remote</p>"},{"location":"toolboxv2/#toolboxv2.flows_dict","title":"<code>toolboxv2.flows_dict(s='.py', remote=False, dir_path=None, flows_dict_=None)</code>","text":"Source code in <code>toolboxv2/flows/__init__.py</code> <pre><code>def flows_dict(s='.py', remote=False, dir_path=None, flows_dict_=None):\n\n    if flows_dict_ is None:\n        flows_dict_ = {}\n    with Spinner(\"Loading flows\"):\n        # Erhalte den Pfad zum aktuellen Verzeichnis\n        if dir_path is None:\n            for ex_path in os.getenv(\"EXTERNAL_PATH_RUNNABELS\", '').split(','):\n                if not ex_path or len(ex_path) == 0:\n                    continue\n                flows_dict(s,remote,ex_path,flows_dict_)\n            dir_path = os.path.dirname(os.path.realpath(__file__))\n        to = time.perf_counter()\n        # Iteriere \u00fcber alle Dateien im Verzeichnis\n        files = os.listdir(dir_path)\n        l_files = len(files)\n        for i, file_name in enumerate(files):\n            with Spinner(f\"{file_name} {i}/{l_files}\"):\n                # \u00dcberpr\u00fcfe, ob die Datei eine Python-Datei ist\n                if file_name == \"__init__.py\":\n                    pass\n\n                elif remote and s in file_name and file_name.endswith('.gist'):\n                    # print(\"Loading from Gist :\", file_name)\n                    name_f = os.path.splitext(file_name)[0]\n                    name = name_f.split('.')[0]\n                    # publisher = name_f.split('.')[1]\n                    url = name_f.split('.')[-1]\n                    # print(\"Ent\", name)\n                    # Lade das Modul\n                    print(f\"Gist Name: {name}, URL: {url}\")\n                    try:\n                        module = GistLoader(f\"{name}/{url}\").load_module(name)\n                    #try:\n                    #    module = GistLoader(f\"{name}/{url}\")\n                    except Exception as e:\n                        print(f\"Error loading module {name} from github {url}\")\n                        print(e)\n                        continue\n\n                    # F\u00fcge das Modul der Dictionary hinzu\n                    print(f\"{hasattr(module, 'run')} and {callable(module.run)} and {hasattr(module, 'NAME')}\")\n                    if hasattr(module, 'run') and callable(module.run) and hasattr(module, 'NAME'):\n                        # print(\"Collecing :\", module.NAME)\n                        flows_dict_[module.NAME] = module.run\n                elif file_name.endswith('.py') and s in file_name:\n                    name = os.path.splitext(file_name)[0]\n                    # print(\"Loading :\", name)\n                    # Lade das Modul\n                    spec = importlib.util.spec_from_file_location(name, os.path.join(dir_path, file_name))\n                    module = importlib.util.module_from_spec(spec)\n                    try:\n                        spec.loader.exec_module(module)\n                    except Exception as e:\n                        print(\"Error loading module \", name)\n                        print(e)\n                        continue\n\n                    # F\u00fcge das Modul der Dictionary hinzu\n                    if hasattr(module, 'run') and callable(module.run) and hasattr(module, 'NAME'):\n                        # print(\"Collecing :\", module.NAME)\n                        flows_dict_[module.NAME] = module.run\n\n        print(f\"Getting all flows took {time.perf_counter() - to:.2f} for {len(flows_dict_.keys())} elements\")\n        return flows_dict_\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.TBEF","title":"<code>toolboxv2.TBEF</code>","text":"<p>Automatic generated by ToolBox v = 0.1.21</p>"},{"location":"toolboxv2/#other-exposed-items","title":"Other Exposed Items","text":""},{"location":"toolboxv2/#toolboxv2.ToolBox_over","title":"<code>toolboxv2.ToolBox_over = 'root'</code>  <code>module-attribute</code>","text":""},{"location":"usage/","title":"ToolBoxV2 Developer Guide","text":"<p>Based on the provided documentation, here's a comprehensive guide on how to use the ToolBoxV2 framework for building applications.</p>"},{"location":"usage/#introduction","title":"Introduction","text":"<p>ToolBoxV2 is a Python framework that provides a structured approach to building applications with standardized request handling and response formatting. It consists of two main components:</p> <ol> <li>RequestData Classes - For handling HTTP requests with strong typing</li> <li>Result Class - For standardized response handling and error management</li> </ol>"},{"location":"usage/#setting-up-your-application","title":"Setting Up Your Application","text":""},{"location":"usage/#creating-a-module","title":"Creating a Module","text":"<p>Start by initializing your application module:</p> <pre><code>from toolboxv2 import get_app, App, RequestData, Result\nfrom typing import Dict, Optional\n\n# Define your module\nMOD_NAME = \"YOUR_MODULE_NAME\"\nversion = \"1.0\"\nexport = get_app(\"{MODULE-NAME.SUB-MODULE}\").tb\n</code></pre>"},{"location":"usage/#registering-functions","title":"Registering Functions","text":"<p>Use the <code>@export</code> decorator to register functions within your module:</p> <pre><code>@export(mod_name=MOD_NAME, version=version)\ndef your_function():\n    # Function logic here\n    return Result.ok(data=\"Success\")\n</code></pre>"},{"location":"usage/#function-types","title":"Function Types","text":""},{"location":"usage/#standard-system-functions","title":"Standard System Functions","text":"<pre><code># Basic function with App parameter\n@export(mod_name=MOD_NAME, version=version, row=True)\ndef system_function(app: App):\n    # Implementation\n    return \"Raw return value\"  # Will be returned as-is because row=True\n\n# Function without App parameter\n@export(mod_name=MOD_NAME, version=version)\ndef function_without_app():\n    # Implementation\n    return Result.ok(data=\"Success\")\n\n# Function with arguments\n@export(mod_name=MOD_NAME, version=version)\ndef function_with_args(name: str) -&gt; Result:\n    # Implementation\n    return Result.ok(data=name)\n\n# Function returning raw data\n@export(mod_name=MOD_NAME, version=version, row=True)\ndef function_with_args_kwargs(name: str, nickname: Optional[str]=None) -&gt; str:\n    if nickname is None:\n        nickname = \"\"\n    return name + nickname  # Returned as raw string\n</code></pre>"},{"location":"usage/#async-functions","title":"Async Functions","text":"<pre><code>@export(mod_name=MOD_NAME, version=version, row=True)\nasync def async_function(app: App):\n    # Async implementation\n    result = await some_async_operation()\n    return result\n</code></pre>"},{"location":"usage/#api-endpoints","title":"API Endpoints","text":"<pre><code># API endpoint with request parameter\n@export(mod_name=MOD_NAME, api=True, version=\"1.0\", request_as_kwarg=True)\nasync def get_data(request: Optional[RequestData]=None):\n    if request:\n        query_params = request.query_params\n        # Process query parameters\n    return Result.json(data={\"status\": \"success\"})\n\n# API endpoint with App and Request parameters\n@export(mod_name=MOD_NAME, api=True, version=\"1.0\", request_as_kwarg=True)\nasync def get_user_data(app, request: Optional[RequestData]=None):\n    # Implementation using app and request\n    return Result.ok(data={\"user\": \"data\"})\n\n# API endpoint with specific HTTP methods\n@export(mod_name=MOD_NAME, api=True, version=\"1.0\", api_methods=['PUT', 'POST'])\nasync def update_data(app, data: Dict):\n    # Process the JSON data received in the request body\n    return Result.ok(data=data)\n\n# API endpoint handling form data\n@export(mod_name=MOD_NAME, api=True, version=\"1.0\", api_methods=['PUT', 'POST'])\nasync def submit_form(app, form_data: Dict):\n    # Process form data\n    return Result.ok(data=form_data)\n</code></pre>"},{"location":"usage/#working-with-request-data","title":"Working with Request Data","text":""},{"location":"usage/#accessing-request-information","title":"Accessing Request Information","text":"<pre><code>@export(mod_name=MOD_NAME, api=True, version=\"1.0\", request_as_kwarg=True)\nasync def process_request(request: Optional[RequestData]=None):\n    if request:\n        # Access method and path\n        method = request.method\n        path = request.path\n\n        # Access headers\n        user_agent = request.headers.user_agent\n        content_type = request.headers.content_type\n        custom_header = request.headers.extra_headers.get('x-custom-header')\n\n        # Access query parameters\n        query_params = request.query_params\n        search_term = query_params.get('search')\n\n        # Access form data or JSON body\n        if request.form_data:\n            form_values = request.form_data\n\n        if request.body and request.content_type == 'application/json':\n            json_data = request.body\n\n    return Result.ok(data=\"Request processed\")\n</code></pre>"},{"location":"usage/#accessing-session-information","title":"Accessing Session Information","text":"<pre><code>@export(mod_name=MOD_NAME, api=True, version=\"1.0\", request_as_kwarg=True)\nasync def get_user_session(request: Optional[RequestData]=None):\n    if request and hasattr(request, 'session'):\n        # Access session data\n        session_id = request.session.SiID\n        user_name = request.session.user_name\n        session_level = request.session.level\n\n        # Access custom session data\n        custom_data = request.session.extra_data.get('custom_key')\n\n    return Result.ok(data={\"user\": user_name})\n</code></pre>"},{"location":"usage/#working-with-results","title":"Working with Results","text":""},{"location":"usage/#creating-different-types-of-responses","title":"Creating Different Types of Responses","text":"<pre><code>@export(mod_name=MOD_NAME, api=True, version=\"1.0\")\nasync def response_examples(app):\n    # Choose the appropriate response type based on your needs\n\n    # 1. Standard success response\n    return Result.ok(\n        data={\"key\": \"value\"},\n        info=\"Operation completed successfully\"\n    )\n\n    # 2. JSON response\n    return Result.json(\n        data={\"status\": \"online\", \"version\": \"1.0\"},\n        info=\"API status retrieved\"\n    )\n\n    # 3. HTML response\n    return Result.html(\n        data=\"&lt;html&gt;&lt;body&gt;&lt;h1&gt;Welcome&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\",\n        info=\"Page rendered\"\n    )\n\n    # 4. Text response\n    return Result.text(\n        text_data=\"Plain text content\",\n        content_type=\"text/plain\"\n    )\n\n    # 5. Binary file response\n    return Result.binary(\n        data=binary_data,\n        content_type=\"application/pdf\",\n        download_name=\"report.pdf\"\n    )\n\n    # 6. Redirect response\n    return Result.redirect(\n        url=\"/dashboard\",\n        status_code=302\n    )\n</code></pre>"},{"location":"usage/#error-handling","title":"Error Handling","text":"<pre><code>@export(mod_name=MOD_NAME, version=version)\ndef process_with_validation(user_input):\n    # Validate input\n    if not user_input:\n        return Result.default_user_error(\n            info=\"Empty input is not allowed\",\n            exec_code=400\n        )\n\n    # Process valid input\n    try:\n        processed_data = process_data(user_input)\n        return Result.ok(data=processed_data)\n    except Exception as e:\n        return Result.default_sys_error(\n            info=f\"Processing error: {str(e)}\",\n            exec_code=500\n        )\n</code></pre>"},{"location":"usage/#using-lazy_return-for-simplified-error-handling","title":"Using lazy_return for Simplified Error Handling","text":"<pre><code>@export(mod_name=MOD_NAME, version=version)\ndef validate_and_process(data):\n    # Validate data\n    validation_result = validate_data(data)\n\n    # If validation fails, return the error\n    # If validation succeeds, return the processed data\n    return validation_result.lazy_return(\n        'user',  # Use user error if validation fails\n        data={\"processed\": True, \"original\": data}  # Return this if successful\n    )\n</code></pre>"},{"location":"usage/#streaming-responses","title":"Streaming Responses","text":"<pre><code>@export(mod_name=MOD_NAME, api=True, version=\"1.0\")\nasync def stream_data():\n    async def generator():\n        for i in range(10):\n            yield {\"chunk\": i}\n            await asyncio.sleep(0.5)\n\n    async def cleanup():\n        # Cleanup resources when the stream closes\n        print(\"Stream closed, performing cleanup\")\n\n    return Result.stream(\n        stream_generator=generator(),\n        info=\"Streaming data chunks\",\n        cleanup_func=cleanup\n    )\n</code></pre>"},{"location":"usage/#advanced-features","title":"Advanced Features","text":""},{"location":"usage/#caching","title":"Caching","text":"<pre><code># Memory caching\n@export(mod_name=MOD_NAME, version=version, memory_cache=True,\n        memory_cache_max_size=100, memory_cache_ttl=300)\ndef cached_function(key):\n    # Expensive operation here\n    return Result.ok(data=compute_expensive_data(key))\n\n# File caching\n@export(mod_name=MOD_NAME, version=version, file_cache=True)\ndef file_cached_function(key):\n    # Expensive operation here\n    return Result.ok(data=compute_expensive_data(key))\n</code></pre>"},{"location":"usage/#background-functions","title":"Background Functions","text":"<pre><code># Memory caching\n@export(mod_name=MOD_NAME, version=version)\ndef function_with_log_running_bg_call():\n    # Expensive operation here\n    def sync_bg_function():\n        print(\"running in gb\")\n        compute_expensive_function()\n\n    return Result.ok(data=\"Starting processing\").task(sync_bg_function)\n\n# File caching\n@export(mod_name=MOD_NAME, version=version)\nasync def function_with_log_running_bg_call():\n    # Expensive operation here\n    async def bg_function():\n        print(\"running in gb\")\n        await compute_expensive_function()\n    return Result.ok(data=\"Starting processing\").task(bg_function())\n</code></pre>"},{"location":"usage/#lifecycle-management","title":"Lifecycle Management","text":"<pre><code># Initialization function\n@export(mod_name=MOD_NAME, version=version, initial=True)\ndef initialize_module(app: App):\n    # Called when the module is loaded\n    print(f\"Initializing {MOD_NAME} module\")\n    # Set up resources, connections, etc.\n    return Result.ok(info=\"Module initialized\")\n\n# Exit function\n@export(mod_name=MOD_NAME, version=version, exit_f=True)\ndef cleanup_module(app: App):\n    # Called when the application is shutting down\n    print(f\"Cleaning up {MOD_NAME} module\")\n    # Release resources, close connections, etc.\n    return Result.ok(info=\"Module cleaned up\")\n</code></pre>"},{"location":"usage/#prepost-compute-functions","title":"Pre/Post Compute Functions","text":"<pre><code>def log_before_execution(func, *args, **kwargs):\n    print(f\"Executing {func.__name__} with args: {args}, kwargs: {kwargs}\")\n    return args, kwargs\n\ndef log_after_execution(result, func, *args, **kwargs):\n    print(f\"Function {func.__name__} returned: {result}\")\n    return result\n\n@export(mod_name=MOD_NAME, version=version,\n        pre_compute=log_before_execution,\n        post_compute=log_after_execution)\ndef monitored_function(name):\n    # Function logic\n    return Result.ok(data=f\"Hello, {name}!\")\n</code></pre>"},{"location":"usage/#url-patterns-for-api-endpoints","title":"URL Patterns for API Endpoints","text":"<p>API endpoints are accessible using the following URL patterns:</p> <ul> <li>Regular API: <code>/api/MOD_NAME/{function_name}?param1=value1&amp;param2=value2</code></li> <li>Server-Sent Events (streaming): <code>/sse/MOD_NAME/{function_name}?param1=value1&amp;param2=value2</code></li> </ul>"},{"location":"utils/","title":"ToolBoxV2: The <code>App</code> Class","text":"<p>The <code>App</code> class is the central singleton instance in ToolBoxV2, responsible for managing the application's lifecycle, configuration, module loading, and core functionalities. It's typically accessed via the <code>get_app()</code> utility function.</p>"},{"location":"utils/#initialization","title":"Initialization","text":"<p>The <code>App</code> instance is initialized with a <code>prefix</code> and <code>AppArgs</code> (command-line arguments).</p> <pre><code>from toolboxv2 import App, AppArgs, get_app\n\n# Example: Initialize or get the App instance\n# The prefix helps differentiate multiple App instances if needed,\n# and is often used in directory naming.\nargs = AppArgs().default() # Or parsed from sys.argv in __main__.py\napp_instance = get_app(prefix=\"my_app_instance\", args=args)\n\n# Accessing key attributes:\nprint(f\"App ID: {app_instance.id}\")\nprint(f\"Version: {app_instance.version}\")\nprint(f\"Start Directory: {app_instance.start_dir}\")\nprint(f\"Data Directory: {app_instance.data_dir}\")\nprint(f\"Config Directory: {app_instance.config_dir}\")\nprint(f\"Debug Mode: {app_instance.debug}\")\n</code></pre>"},{"location":"utils/#key-initialization-steps","title":"Key Initialization Steps:","text":"<ol> <li> <p>System &amp; Paths:</p> <ul> <li>Determines the operating system (<code>system_flag</code>).</li> <li>Sets the <code>start_dir</code> to the application's root directory.</li> <li>Resolves the <code>prefix</code>:<ul> <li>If no prefix is provided, it attempts to load the last used prefix from <code>.data/last-app-prefix.txt</code>.</li> <li>If a prefix is provided, it's saved to this file for future use.</li> </ul> </li> <li>Constructs the <code>app_id</code> (e.g., <code>prefix-hostname</code>).</li> <li>Sets up <code>data_dir</code>, <code>config_dir</code>, and <code>info_dir</code> based on the <code>app_id</code> (e.g., <code>./.data/prefix-hostname/</code>).</li> <li>Sets up <code>appdata</code> directory (OS-specific application data folder).</li> </ul> </li> <li> <p>Logging:</p> <ul> <li>Initializes a logger (<code>app.logger</code>). The logging level and output (terminal/file) can vary based on the <code>prefix</code> (e.g., \"test\", \"live\", \"debug\") and the <code>--debug</code> CLI argument.</li> </ul> </li> <li> <p>Configuration:</p> <ul> <li>Loads application configuration using <code>FileHandler</code> from a file typically named <code>app_id.config</code> in the <code>config_dir</code>.</li> <li>Defines default configuration <code>keys</code> and <code>defaults</code> (e.g., for macros, helpers, debug status).</li> </ul> </li> <li> <p>Core Attributes:</p> <ul> <li><code>version</code>: ToolBoxV2 version, read from <code>pyproject.toml</code>.</li> <li><code>debug</code>: Boolean, controlled by CLI args and config.</li> <li><code>dev_modi</code>: Boolean, development mode status from config.</li> <li><code>functions</code>: A dictionary to store registered functions from modules.</li> <li><code>modules</code>: A dictionary to store loaded module objects.</li> <li><code>interface_type</code>: Default <code>ToolBoxInterfaces.native</code>.</li> <li><code>alive</code>: Boolean, controls the main application loop.</li> <li><code>args_sto</code>: Stores the parsed <code>AppArgs</code>.</li> <li><code>loop</code>: The asyncio event loop (initialized later or if already running).</li> <li><code>session</code>: A <code>Session</code> object for managing user/remote session state.</li> </ul> </li> <li> <p>Conditional Actions (based on <code>AppArgs</code>):</p> <ul> <li><code>args.init</code>: If true, adds <code>start_dir</code> to <code>sys.path</code>.</li> <li>The <code>__main__.py</code> script handles other arguments like <code>--update</code>, <code>--get-version</code>, etc., by calling <code>App</code> methods or other utilities.</li> </ul> </li> </ol>"},{"location":"utils/#core-functionalities","title":"Core Functionalities","text":""},{"location":"utils/#module-management","title":"Module Management","text":"<ul> <li> <p><code>load_mod(mod_name: str, spec='app', mlm='I', **kwargs)</code> / <code>save_load(modname, spec='app')</code>:</p> <ul> <li>Loads a module into the application.</li> <li><code>spec</code> (specification): Used to namespace or categorize the module instance (e.g., 'app' for general, or a specific session ID).</li> <li>Supports different loading mechanisms (<code>mlm</code>):<ul> <li><code>'I'</code>: In-place load (imports the Python module directly). This is the default.</li> <li><code>'C'</code>: Copies the module file to a runtime directory before loading (less common).</li> </ul> </li> <li>Handles <code>ModuleNotFoundError</code> by attempting to guide the user (e.g., install via <code>CloudM</code> or <code>pip</code>).</li> <li>Registers the module's exported functions and/or its <code>Tools</code> class instance.</li> <li>Can reload modules if they are already loaded. <pre><code># Load the 'MyModule'\nmy_module_instance = app_instance.load_mod(\"MyModule\")\n\n# Or if it's a Tool-based module:\n# my_tool_instance = app_instance.load_mod(\"MyToolModule\")\n</code></pre></li> </ul> </li> <li> <p><code>get_mod(name: str, spec='app') -&gt; ModuleType | MainToolType</code>:</p> <ul> <li>Retrieves a loaded module instance. If the module isn't loaded, it attempts to load it. <pre><code>db_mod = app_instance.get_mod(\"DB\")\nif db_mod:\n    db_mod.some_db_function()\n</code></pre></li> </ul> </li> <li> <p><code>remove_mod(mod_name: str, spec='app', delete=True)</code> / <code>a_remove_mod(...)</code> (async):</p> <ul> <li>Unloads a module, calling its <code>on_exit</code> functions if defined.</li> <li><code>delete=True</code> removes it completely from the <code>functions</code> registry.</li> </ul> </li> <li> <p><code>reload_mod(mod_name: str, spec='app', ...)</code>:</p> <ul> <li>Reloads an existing module. Useful for development.</li> </ul> </li> <li> <p><code>watch_mod(mod_name: str, ...)</code>:</p> <ul> <li>Monitors a module's source file(s) for changes and automatically reloads it. <pre><code># In development, watch 'MyDevModule' for changes\napp_instance.watch_mod(\"MyDevModule\")\n</code></pre></li> </ul> </li> <li> <p><code>load_all_mods_in_file(working_dir=\"mods\")</code> / <code>a_load_all_mods_in_file(...)</code> (async):</p> <ul> <li>Scans the specified directory (default <code>./mods/</code>) and loads all valid Python modules found.</li> </ul> </li> </ul>"},{"location":"utils/#function-registration-and-execution","title":"Function Registration and Execution","text":"<ul> <li> <p><code>@app.tb(...)</code> Decorator (via <code>_create_decorator</code>):</p> <ul> <li>The primary way functions are registered with ToolBoxV2. See <code>example_mod.md</code> for details on usage.</li> <li>This decorator populates the <code>app.functions</code> dictionary.</li> </ul> </li> <li> <p><code>get_function(name: Enum | tuple, metadata=False, state=True, specification='app', ...)</code>:</p> <ul> <li>Retrieves a registered function.</li> <li><code>name</code>: Can be an Enum (from <code>all_functions_enums.py</code>) or a <code>(module_name, function_name)</code> tuple.</li> <li><code>metadata=True</code>: Returns a tuple <code>(function_data_dict, callable_function)</code>.</li> <li><code>state=True</code>: Returns a stateful version of the function (bound to its module instance if applicable).</li> <li><code>state=False</code>: Returns the raw, stateless function.</li> <li><code>specification</code>: The context/instance spec to get the function for.</li> </ul> </li> <li> <p><code>run_any(mod_function_name, ..., get_results=False, **kwargs)</code> / <code>a_run_any(...)</code> (async):</p> <ul> <li>Executes a registered function by its name (Enum or tuple).</li> <li>Handles argument passing, stateful/stateless execution, and error wrapping into a <code>Result</code> object.</li> <li><code>get_results=True</code>: Returns the <code>Result</code> object itself.</li> <li><code>get_results=False</code> (default): Returns the <code>data</code> payload from the <code>Result</code> object if successful.</li> <li>Automatically handles running the function's pre/post compute hooks and caching if configured via <code>@app.tb</code>. <pre><code># Synchronous execution\nresult_data = app_instance.run_any((\"MyModule\", \"my_function\"), arg1=\"hello\")\nfull_result_obj = app_instance.run_any((\"MyModule\", \"my_function\"), arg1=\"hello\", get_results=True)\n\n# Asynchronous execution\nasync_result_data = await app_instance.a_run_any((\"MyAsyncModule\", \"my_async_function\"))\n</code></pre></li> </ul> </li> <li> <p><code>run_http(mod_function_name, function_name=None, method=\"GET\", ...)</code> (async):</p> <ul> <li>Executes a function on a remote ToolBoxV2 instance via HTTP, using the app's <code>session</code> object.</li> </ul> </li> </ul>"},{"location":"utils/#application-lifecycle","title":"Application Lifecycle","text":"<ul> <li><code>exit()</code> / <code>a_exit()</code> (async):<ul> <li>Gracefully shuts down the application.</li> <li>Calls <code>on_exit</code> functions for all loaded modules.</li> <li>Saves configuration.</li> <li>Stops the main application loop (<code>alive = False</code>).</li> <li>Cleans up threads and the event loop if applicable.</li> </ul> </li> </ul>"},{"location":"utils/#utilities","title":"Utilities","text":"<ul> <li><code>print(text, *args, **kwargs)</code> / <code>sprint(text, *args, **kwargs)</code>:<ul> <li>Styled print functions, prepending <code>System$[app_id]:</code>. <code>sprint</code> is often used for more verbose/system-level messages and can be silenced.</li> </ul> </li> <li><code>debug_rains(e: Exception)</code>: If <code>app.debug</code> is true, prints a full traceback and re-raises the exception.</li> <li><code>set_flows(r: dict)</code> / <code>run_flows(name: str, **kwargs)</code>: Manages and executes predefined application flows (sequences of operations).</li> <li><code>get_username()</code> / <code>set_username(username: str)</code>: Manages the application's user identity.</li> <li><code>save_autocompletion_dict()</code> / <code>get_autocompletion_dict()</code>: Saves/loads a dictionary of modules and their functions for autocompletion features.</li> <li><code>save_registry_as_enums(directory: str, filename: str)</code>: Generates an <code>all_functions_enums.py</code>-like file from the currently registered functions.</li> <li><code>execute_all_functions(...)</code> / <code>a_execute_all_functions(...)</code> (async):<ul> <li>Runs all registered testable functions (marked with <code>test=True</code> in <code>@app.tb</code> or having <code>samples</code>).</li> <li>Useful for integration testing and profiling.</li> <li>Can filter by module (<code>m_query</code>) and function (<code>f_query</code>).</li> <li>Supports profiling via <code>cProfile</code>.</li> </ul> </li> <li><code>run_bg_task(task: Callable)</code>:<ul> <li>Runs a synchronous or asynchronous task in a separate background thread with its own event loop. Ensures proper handling of nested asyncio operations within the task.</li> </ul> </li> </ul>"},{"location":"utils/#session-management-appsession","title":"Session Management (<code>app.session</code>)","text":"<p>The <code>app.session</code> attribute holds an instance of the <code>Session</code> class (from <code>toolboxv2.utils.system.session</code>). It's used for: *   Authenticating with a remote ToolBoxV2 service (e.g., SimpleCore Hub). *   Making authenticated HTTP requests (<code>session.fetch</code>, <code>session.upload_file</code>, <code>session.download_file</code>). *   Manages JWT claims and private key authentication.</p> <pre><code># Example: Making an authenticated API call\n# Assumes app.session is already authenticated\nresponse_data = await app_instance.session.fetch(\"/api/MyRemoteModule/get_info\")\njson_payload = await response_data.json()\n</code></pre> <pre><code>### 2. `cli.md` - Documenting the Command Line Interface\n\nThis should explain how to use the `tb` (or `python -m toolboxv2`) command-line tool, detailing its arguments and their effects.\n\n```markdown\n# ToolBoxV2: Command Line Interface (CLI)\n\nToolBoxV2 provides a command-line interface (CLI) for managing and running applications. It's typically invoked as `tb` (if installed globally or via an alias) or `python -m toolboxv2`.\n\n## General Usage\n\n```bash\npython -m toolboxv2 [options] [sub-commands]\n# or\ntb [options] [sub-commands]\n</code></pre> <p>The CLI script (<code>__main__.py</code>) performs the following main steps: 1.  Parses command-line arguments. 2.  Initializes the <code>App</code> instance via <code>setup_app()</code> (which calls <code>get_app()</code>). 3.  Handles various options to:     *   Manage application data and configuration.     *   Control application modes (background, proxy, debug).     *   Load modules and manage their state.     *   Run tests or profilers.     *   Execute specific application flows or commands.</p>"},{"location":"utils/#key-cli-arguments","title":"Key CLI Arguments","text":"<p>The following are some of the primary arguments available. Use <code>tb -h</code> or <code>python -m toolboxv2 -h</code> for a full list.</p> <ul> <li> <p>Instance and Mode:</p> <ul> <li><code>-init [name]</code>: Initializes ToolBoxV2 with a specific instance name (default: <code>main</code>).</li> <li><code>-n, --name &lt;name&gt;</code>: Specifies an ID for the ToolBox instance (default: <code>main</code>). This affects data/config directory names.</li> <li><code>-m, --modi &lt;mode&gt;</code>: Starts a ToolBoxV2 interface/flow (e.g., <code>cli</code>, <code>bg</code>, or custom flows). Default is usually \"cli\".</li> <li><code>--debug</code>: Starts the application in debug mode (more verbose logging, potentially different behavior).</li> <li><code>--remote</code>: Starts in remote mode, often for connecting to a proxy or another instance.</li> <li><code>-bg, --background-application</code>: Starts an interface in the background as a detached process.</li> <li><code>-bgr, --background-application-runner</code>: Runs the background application logic in the current terminal (for daemons).</li> <li><code>-fg, --live-application</code>: Starts a proxy interface, connecting to a background daemon.</li> <li><code>--kill</code>: Kills the currently running local ToolBoxV2 instance (matching the <code>-m &lt;mode&gt;</code> and <code>-n &lt;name&gt;</code>).</li> </ul> </li> <li> <p>Module and Version Management:</p> <ul> <li><code>-l, --load-all-mod-in-files</code>: Loads all modules found in the <code>mods/</code> directory on startup.</li> <li><code>-sfe, --save-function-enums-in-file</code>: Generates/updates the <code>all_functions_enums.py</code> file based on loaded modules. Often used with <code>-l</code>.</li> <li><code>-v, --get-version</code>: Prints the version of ToolBoxV2 and all loaded modules.</li> <li><code>-i, --install &lt;name&gt;</code>: Installs a module or interface (likely via <code>CloudM</code> module).</li> <li><code>-r, --remove &lt;name&gt;</code>: Uninstalls a module or interface.</li> <li><code>-u, --update &lt;name_or_main&gt;</code>: Updates a module/interface or the core ToolBoxV2 (<code>main</code>).</li> </ul> </li> <li> <p>Development and Testing:</p> <ul> <li><code>--test</code>: Runs all unit tests (typically discovers and runs tests in the <code>tests/</code> directory).</li> <li><code>--profiler</code>: Runs all registered testable functions and profiles their execution using <code>cProfile</code>.</li> <li><code>--ipy</code>: Starts an IPython session with the ToolBoxV2 app pre-loaded. Provides magic commands like <code>%tb save/loadX/load/open/inject</code>.</li> </ul> </li> <li> <p>Service Management (<code>--sm</code>):</p> <ul> <li>Provides a sub-menu for managing ToolBoxV2 as a system service (Linux/systemd or Windows Startup).</li> <li>Options: Init, Start/Stop/Restart, Status, Uninstall, Show/Hide console window (Windows).</li> </ul> </li> <li> <p>Log Management (<code>--lm</code>):</p> <ul> <li>Provides a sub-menu for managing log files (e.g., removing or unstyling logs by date/level).</li> </ul> </li> <li> <p>Data and Configuration Management:</p> <ul> <li><code>--delete-config-all</code>: Deletes all configuration files. Use with caution!</li> <li><code>--delete-data-all</code>: Deletes all data folders. Use with caution!</li> <li><code>--delete-config</code>: Deletes the configuration file for the named instance.</li> <li><code>--delete-data</code>: Deletes the data folder for the named instance.</li> </ul> </li> <li> <p>Network Configuration (for interfaces):</p> <ul> <li><code>-p, --port &lt;port&gt;</code>: Specifies the port for an interface (default: <code>5000</code> or <code>6587</code> for background).</li> <li><code>-w, --host &lt;host&gt;</code>: Specifies the host for an interface (default: <code>0.0.0.0</code>).</li> </ul> </li> <li> <p>Direct Command Execution:</p> <ul> <li><code>-c, --command &lt;module_name&gt; &lt;function_name&gt; [arg1 arg2 ...]</code> (can be repeated): Executes a specific function.</li> <li><code>--kwargs &lt;key=value&gt;</code> (can be repeated): Provides keyword arguments for commands specified with <code>-c</code>.</li> </ul> </li> <li> <p>Conda Integration:</p> <ul> <li><code>conda [conda_args...]</code>: Special argument to pass commands directly to a <code>conda_runner.py</code> script (e.g., <code>tb conda env list</code>).</li> </ul> </li> <li> <p>API Runner:</p> <ul> <li><code>api [api_args...]</code>: Special argument to invoke <code>cli_api_runner.py</code>, likely for direct API interactions or testing.</li> </ul> </li> <li> <p>GUI:</p> <ul> <li><code>gui</code>: Starts the GUI version of ToolBoxV2 (imports and runs <code>toolboxv2.__gui__.start</code>).</li> </ul> </li> </ul>"},{"location":"utils/#cli-execution-flow-__main__py","title":"CLI Execution Flow (<code>__main__.py</code>)","text":"<ol> <li>Argument Parsing: <code>parse_args()</code> uses <code>argparse</code> to define and parse all CLI arguments.</li> <li>App Setup (<code>setup_app()</code>):<ul> <li>Initializes the <code>App</code> instance using <code>get_app()</code> with the parsed arguments and name.</li> <li>Sets up PID file for the current process.</li> <li>Optionally silences <code>app.sprint</code> if not in debug/verbose mode.</li> <li>Loads all modules if <code>-l</code> is specified.</li> <li>Handles <code>--update</code> logic.</li> </ul> </li> <li>Background/Live Application Handling:<ul> <li>If <code>-bgr</code>: Initializes <code>DaemonApp</code>.</li> <li>If <code>-bg</code>: Starts the application as a detached background process using <code>subprocess.Popen</code>.</li> <li>If <code>-fg</code> (live-application): Attempts to connect to a background daemon using <code>ProxyApp</code>.</li> </ul> </li> <li>Action Dispatching: Based on the parsed arguments, it performs actions like:<ul> <li>Module installation (<code>--install</code>).</li> <li>Log management (<code>--lm</code>).</li> <li>Service management (<code>--sm</code>).</li> <li>Saving function enums (<code>-sfe</code>).</li> <li>Printing versions (<code>-v</code>).</li> <li>Running the profiler (<code>--profiler</code>).</li> <li>Running flows based on <code>--modi</code>.</li> <li>Handling Docker commands (<code>--docker</code>).</li> <li>Killing an existing instance (<code>--kill</code>).</li> <li>Executing direct commands (<code>-c</code>).</li> </ul> </li> <li>Cleanup: Removes the PID file and calls <code>app.a_exit()</code> before exiting.</li> </ol>"},{"location":"utils/#example-cli-usage","title":"Example CLI Usage","text":"<pre><code># Get version information\npython -m toolboxv2 -v\n\n# Load all modules and save function enums\npython -m toolboxv2 -l -sfe\n\n# Run a specific function in MyModule\npython -m toolboxv2 -c MyModule my_function arg_value --kwargs param_name=kwarg_value\n\n# Start the application with a custom flow named 'web_server' in debug mode\npython -m toolboxv2 -m web_server --debug -n my_web_instance\n\n# Start a background daemon for the 'bg_processing' flow\npython -m toolboxv2 -m bg_processing -bg -n background_processor\n\n# Connect to the background daemon with a live proxy application\npython -m toolboxv2 -m cli -fg -n background_processor\n\n# Kill the 'web_server' modi instance named 'my_web_instance'\npython -m toolboxv2 -m web_server --kill -n my_web_instance\n</code></pre> <pre><code>### 3. `example_mod.md` - Documenting Module Creation\n\nThis needs to be updated to accurately reflect the `@app.tb(...)` decorator from `toolbox.py` and the `Result` and `RequestData` classes from `types.py`.\n\n```markdown\n# ToolBoxV2: Creating Modules\n\nToolBoxV2 modules are Python files or packages that extend the framework's functionality. They can define simple functions, stateful tools (classes inheriting from `MainTool`), or API endpoints.\n\n## Basic Module Structure\n\nA typical ToolBoxV2 module (`.py` file) includes:\n\n1.  **Imports:** Necessary libraries and ToolBoxV2 components.\n2.  **Module Metadata (Optional but Recommended):**\n    *   `Name` (or `name`): A string defining the module's canonical name.\n    *   `version`: A string for the module's version (e.g., \"1.0.0\").\n3.  **Function/Class Definitions:** The core logic of your module.\n4.  **Exporting Functions:** Functions are made available to ToolBoxV2 using the `@export` decorator (which is an alias for `app.tb`).\n\n## The `@export` Decorator (`app.tb`)\n\nThe `@export` decorator is the primary mechanism for registering functions and configuring their behavior within ToolBoxV2. It's obtained from an `App` instance.\n\n```python\nfrom toolboxv2 import get_app, App, Result, RequestData, MainTool\nfrom toolboxv2.utils.system.types import ToolBoxInterfaces # For specific interface types\nfrom typing import Optional, Dict, Any, List\nimport asyncio\n\n# Get the application instance (singleton)\n# The 'prefix' for get_app here is often the module's own name,\n# though the decorator will use its 'mod_name' parameter.\napp = get_app(\"MyModule\")\nexport = app.tb # Alias the decorator for convenience\n\n# --- Module Metadata (Best Practice) ---\nName = \"MyModule\"  # Used by the decorator if mod_name is not specified\nversion = \"1.0.1\"\n\n# --- Example Functions ---\n\n@export(mod_name=Name, version=version, helper=\"A simple greeting function.\")\ndef greet(name: str) -&gt; str:\n    \"\"\"Returns a greeting message.\"\"\"\n    return f\"Hello, {name} from MyModule!\"\n\n@export(mod_name=Name, version=version, row=True, helper=\"Returns raw data without Result wrapping.\")\ndef get_raw_data() -&gt; dict:\n    \"\"\"Demonstrates returning raw data.\"\"\"\n    return {\"key\": \"value\", \"number\": 123}\n\n@export(mod_name=Name, version=version, initial=True, helper=\"Runs when the module is first loaded.\")\ndef on_module_load():\n    \"\"\"Initialization logic for this module.\"\"\"\n    app.print(f\"{Name} module has been loaded and initialized!\")\n    # return Result.ok(info=\"MyModule initialized successfully\") # Optional: return a Result\n\n@export(mod_name=Name, version=version, exit_f=True, helper=\"Runs when the application is shutting down.\")\nasync def on_module_exit():\n    \"\"\"Cleanup logic for this module.\"\"\"\n    await asyncio.sleep(0.1) # Simulate async cleanup\n    app.print(f\"{Name} module is cleaning up.\")\n    # return Result.ok(info=\"MyModule cleaned up.\") # Optional\n\n@export(mod_name=Name, version=version, api=True, api_methods=['GET'], request_as_kwarg=True,\n        helper=\"An example API endpoint.\")\nasync def my_api_endpoint(request: Optional[RequestData] = None) -&gt; Result:\n    \"\"\"\n    Handles GET requests to /api/MyModule/my_api_endpoint.\n    Accesses request details if provided.\n    \"\"\"\n    if request:\n        app.print(f\"API request received: {request.request.method} {request.request.path}\")\n        app.print(f\"Query Params: {request.request.query_params}\")\n        app.print(f\"User from session: {request.session.user_name}\")\n    return Result.json(data={\"message\": \"API call successful!\", \"module_version\": version})\n\n@export(mod_name=Name, version=version, memory_cache=True, memory_cache_ttl=60)\ndef expensive_calculation(param: int) -&gt; int:\n    \"\"\"\n    An example of a function whose results will be cached in memory for 60 seconds.\n    \"\"\"\n    app.print(f\"Performing expensive calculation for {param}...\")\n    time.sleep(2) # Simulate work\n    return param * param\n\n# Example of a more complex function using App instance and returning a Result\n@export(mod_name=Name, version=version)\ndef process_data_with_app(app_instance: App, data_id: int) -&gt; Result:\n    \"\"\"\n    This function automatically receives the 'App' instance if its first parameter is type-hinted as 'App'.\n    This is determined by the 'state=True' logic in the decorator if 'app' is the first param.\n    Alternatively, use state=False for stateless functions.\n    \"\"\"\n    if not isinstance(app_instance, App): # Should always be App if first param is 'app'\n        return Result.default_internal_error(\"App instance not correctly passed.\")\n\n    # Use app_instance for logging, accessing config, other modules, etc.\n    app_instance.logger.info(f\"Processing data_id: {data_id} in {Name}\")\n    if data_id &lt; 0:\n        return Result.default_user_error(info=\"Data ID cannot be negative.\")\n    return Result.ok(data={\"processed_id\": data_id, \"status\": \"completed\"})\n</code></pre>"},{"location":"utils/#export-decorator-parameters","title":"<code>@export</code> Decorator Parameters:","text":"<ul> <li><code>name</code> (str, optional): The name to register the function under. Defaults to the function's actual name.</li> <li><code>mod_name</code> (str): The name of the module this function belongs to. If not provided, it tries to infer from <code>func.__module__</code> or a global <code>Name</code> in the module.</li> <li><code>version</code> (str, optional): Version string for this function/feature. Combined with the app's version.</li> <li><code>helper</code> (str, optional): A docstring or help text for the function.</li> <li><code>api</code> (bool, default <code>False</code>): If <code>True</code>, exposes this function as an HTTP API endpoint.<ul> <li>The URL pattern is typically <code>/api/&lt;mod_name&gt;/&lt;func_name&gt;</code>.</li> <li>For streaming, <code>/sse/&lt;mod_name&gt;/&lt;func_name&gt;</code>.</li> </ul> </li> <li><code>api_methods</code> (List[str], optional): Specifies allowed HTTP methods (e.g., <code>['GET', 'POST']</code>). Defaults to <code>['AUTO']</code> (GET if no body params, POST if body params).</li> <li><code>request_as_kwarg</code> (bool, default <code>False</code>): If <code>True</code> and <code>api=True</code>, the function will receive a <code>request: RequestData</code> keyword argument if it's defined in its signature.</li> <li><code>row</code> (bool, default <code>False</code>): If <code>True</code>, the function's raw return value is used directly. If <code>False</code> (default), the return value is automatically wrapped in a <code>Result.ok()</code> object if it's not already a <code>Result</code> or <code>ApiResult</code>.</li> <li><code>initial</code> (bool, default <code>False</code>): If <code>True</code>, this function is added to the module's \"on_start\" list and is called when the module is loaded by the <code>App</code> instance (if the module instance is a <code>MainTool</code> or similar, or if called directly).</li> <li><code>exit_f</code> (bool, default <code>False</code>): If <code>True</code>, this function is added to the module's \"on_exit\" list and is called when the <code>App</code> instance is shutting down or the module is removed.</li> <li><code>state</code> (bool, optional):<ul> <li>If <code>None</code> (default): Automatically determined. If the first parameter of the decorated function is named <code>self</code> or <code>app</code> (and type-hinted as <code>App</code>), <code>state</code> is considered <code>True</code>. Otherwise <code>False</code>.</li> <li>If <code>True</code>: The function is considered stateful. If its first parameter is <code>self</code>, it's assumed to be a method of a class instance (e.g., a <code>MainTool</code> subclass). If <code>app</code>, the <code>App</code> instance is passed.</li> <li>If <code>False</code>: The function is treated as stateless.</li> </ul> </li> <li><code>test</code> (bool, default <code>True</code>): Marks the function as testable. Used by <code>app.execute_all_functions()</code>.</li> <li><code>samples</code> (List[Dict[str, Any]], optional): A list of sample keyword arguments to be used when testing the function with <code>app.execute_all_functions()</code>.</li> <li><code>memory_cache</code> (bool, default <code>False</code>): Enables in-memory caching for the function's results.</li> <li><code>memory_cache_ttl</code> (int, default <code>300</code>): Time-to-live in seconds for memory cache entries.</li> <li><code>memory_cache_max_size</code> (int, default <code>100</code>): Max number of entries in the memory cache.</li> <li><code>file_cache</code> (bool, default <code>False</code>): Enables file-based caching for the function's results. (Stored in <code>app.data_dir/cache/...</code>).</li> <li><code>restrict_in_virtual_mode</code> (bool, default <code>False</code>): If <code>True</code>, restricts function in certain virtualized/proxied modes.</li> <li><code>level</code> (int, default <code>-1</code>): A general-purpose level or priority for the function.</li> <li><code>pre_compute</code> (Callable, optional): A function <code>(func, *args, **kwargs) -&gt; (args, kwargs)</code> called before the main function executes. It can modify args/kwargs.</li> <li><code>post_compute</code> (Callable, optional): A function <code>(result, func, *args, **kwargs) -&gt; result</code> called after the main function executes. It can modify the result.</li> <li><code>interface</code> (ToolBoxInterfaces | str, optional): Specifies the intended interface type (e.g., <code>ToolBoxInterfaces.cli</code>, <code>ToolBoxInterfaces.api</code>). Defaults to \"tb\".</li> </ul>"},{"location":"utils/#result-and-apiresult-objects","title":"<code>Result</code> and <code>ApiResult</code> Objects","text":"<ul> <li>Modules should typically return <code>Result</code> objects (or <code>ApiResult</code> for API endpoints) to provide standardized responses including success/error status, data payload, and informational messages.</li> <li>The <code>toolboxv2.utils.system.types.Result</code> class provides factory methods:<ul> <li><code>Result.ok(data=..., info=...)</code></li> <li><code>Result.json(data=..., info=...)</code> (for <code>api=True</code> functions)</li> <li><code>Result.html(data=..., info=...)</code></li> <li><code>Result.text(text_data=..., info=...)</code></li> <li><code>Result.binary(data=..., content_type=..., download_name=...)</code></li> <li><code>Result.redirect(url=..., status_code=...)</code></li> <li><code>Result.stream(stream_generator=..., info=..., cleanup_func=...)</code> (for SSE)</li> <li><code>Result.default_user_error(info=..., exec_code=...)</code></li> <li><code>Result.default_internal_error(info=..., exec_code=...)</code></li> <li><code>Result.custom_error(data=..., info=..., exec_code=...)</code></li> </ul> </li> <li>The <code>Result</code> object has a <code>task(background_task_callable)</code> method to schedule a background task to run after the main function returns.</li> </ul>"},{"location":"utils/#requestdata-object","title":"<code>RequestData</code> Object","text":"<ul> <li>For API functions (<code>api=True</code>) with <code>request_as_kwarg=True</code>, if the function signature includes <code>request: Optional[RequestData] = None</code>, it will receive an instance of <code>toolboxv2.utils.system.types.RequestData</code>.</li> <li><code>RequestData</code> provides access to:<ul> <li><code>request.method</code>, <code>request.path</code></li> <li><code>request.headers</code> (an instance of <code>Headers</code>, e.g., <code>request.headers.user_agent</code>, <code>request.headers.hx_request</code>)</li> <li><code>request.query_params</code> (dict)</li> <li><code>request.form_data</code> (dict, if applicable)</li> <li><code>request.body</code> (parsed JSON if <code>content_type</code> is <code>application/json</code>, otherwise raw bytes/str)</li> <li><code>session.SiID</code>, <code>session.user_name</code>, <code>session.level</code> (from the current user's session)</li> </ul> </li> </ul>"},{"location":"utils/#creating-a-maintool-based-module","title":"Creating a <code>MainTool</code>-based Module","text":"<p>For more complex, stateful modules, you can create a class that inherits from <code>toolboxv2.utils.system.main_tool.MainTool</code>.</p> <pre><code>from toolboxv2 import get_app, App, Result, MainTool\nfrom toolboxv2.utils.system.types import ToolBoxError\n\napp = get_app(\"MyToolModule\")\nexport = app.tb\n\nName = \"MyToolModule\"\nversion = \"0.5.0\"\n\nclass Tools(MainTool): # The class must be named 'Tools' for auto-detection by older App versions\n                      # or ensure your module file directly uses @export on methods if not named Tools.\n    # Or, you can export methods directly from any class:\n    # class MyCustomTool(MainTool):\n    #    @export(...)\n    #    def my_method(self, ...): ...\n\n    async def __ainit__(self): # Asynchronous initialization\n        # self.app is automatically available\n        # self.name, self.version, self.logger are set by MainTool's __await__\n        await super().__ainit__(name=Name, v=version, tool={\n            \"process_item\": self.process_item, # For older compatibility if functions were in 'tools' dict\n            \"get_status\": self.get_status\n        })\n        self.internal_state = \"initialized\"\n        self.app.print(f\"{self.name} (Tool) has been initialized with state: {self.internal_state}\")\n\n    @export(mod_name=Name, version=version) # Decorate methods to export them\n    def process_item(self, item_id: int, details: str) -&gt; Result:\n        # 'self' provides access to app, logger, name, version, config\n        self.app.logger.info(f\"{self.name} processing item: {item_id} - {details}\")\n        self.internal_state = f\"last_processed_{item_id}\"\n        if item_id == 0:\n            return self.return_result( # Helper from MainTool\n                error=ToolBoxError.input_error,\n                exec_code=1, # Custom error code\n                help_text=\"Item ID cannot be zero.\",\n                data_info=\"Validation failed\"\n            )\n        return Result.ok(data={\"item_id\": item_id, \"status\": \"processed by tool\"})\n\n    @export(mod_name=Name, version=version)\n    async def get_status(self) -&gt; str: # Example async method\n        await asyncio.sleep(0.01)\n        return f\"Tool {self.name} current state: {self.internal_state}\"\n\n    async def on_exit(self): # Not automatically called unless also decorated or part of a convention\n        self.app.print(f\"Tool {self.name} is shutting down its internal state.\")\n        # Perform cleanup\n\n# To ensure on_exit is called by the App framework:\n@export(mod_name=Name, version=version, exit_f=True)\nasync def custom_tool_exit_function(app_instance: App):\n    tool_instance = app_instance.get_mod(Name)\n    if tool_instance and hasattr(tool_instance, 'on_exit') and callable(tool_instance.on_exit):\n        await tool_instance.on_exit()\n</code></pre> <p>Key aspects of <code>MainTool</code>: *   Asynchronous Initialization: Use <code>async def __ainit__(self)</code> for setup. The <code>MainTool</code> itself is awaitable, and <code>__ainit__</code> is called when the instance is first awaited (e.g., by <code>app.load_mod</code> or <code>app.get_mod</code>). *   <code>self.app</code>: The <code>App</code> instance is available as <code>self.app</code>. *   <code>self.name</code>, <code>self.version</code>, <code>self.logger</code>: These are automatically set up. *   <code>self.return_result(...)</code>: A helper method for creating <code>Result</code> objects. *   Methods intended to be called via <code>app.run_any</code> should be decorated with <code>@export</code>.</p>"},{"location":"utils/#steps-to-create-a-valid-toolboxv2-module","title":"Steps to Create a Valid Toolboxv2 Module:","text":"<ol> <li>Define Module Structure: Organize your code with imports, metadata, and function/class definitions.</li> <li>Clarify Dependencies: Import necessary libraries. Handle missing optional dependencies gracefully if needed.</li> <li>Export Functions/Methods: Use the <code>@export(...)</code> decorator (e.g., <code>app.tb(...)</code>) to mark functions/methods that ToolBoxV2 should recognize.<ul> <li>Provide <code>mod_name</code> and <code>version</code>.</li> <li>Use other parameters (<code>api</code>, <code>row</code>, <code>initial</code>, <code>exit_f</code>, <code>memory_cache</code>, etc.) as needed.</li> <li>Ensure clear signatures and document parameters/return types (Python type hints are highly recommended).</li> </ul> </li> <li>Documentation and Versioning: Document your module and its functions. Use semantic versioning.</li> <li>Testing: Test your module thoroughly, including how it integrates with the ToolBoxV2 app (<code>app.run_any</code>, <code>app.get_mod</code>, etc.). Use the <code>test=True</code> and <code>samples</code> parameters in <code>@export</code> to facilitate automated testing via <code>app.execute_all_functions()</code>.</li> </ol>"}]}