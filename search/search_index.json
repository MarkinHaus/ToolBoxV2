{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ToolBoxV2 \ud83e\uddf0","text":"<p>ToolBoxV2 is a flexible, modular framework designed for creating and managing a wide range of tools, functions, and complete applications. It supports deployment locally, on the web, or as cross-platform desktop/mobile applications.</p> <p>At its core, ToolBoxV2 integrates a Python backend with a Rust server and a Tauri-based UI, offering a powerful and versatile development experience.</p> <ul> <li>Free software: Custom License</li> <li>Officel Web page: https://simplecore.app/</li> <li>GitHub Repository: https://github.com/MarkinHaus/ToolBoxV2</li> </ul>"},{"location":"#key-goals-features","title":"Key Goals &amp; Features","text":"<p>ToolBoxV2 aims to simplify the development and usage of digital tools by:</p> <ul> <li>\ud83d\udd0c Modularity: Build applications from reusable Python modules (<code>mods</code>) and utilities (<code>utils</code>).</li> <li>\u2699\ufe0f Automation: Facilitate automation of tasks through CLI interactions and programmable APIs.</li> <li>\ud83c\udf10 Cross-Platform Interfaces:<ul> <li>Develop Desktop Applications using Tauri (Rust + Web UI).</li> <li>Create Web Applications with the <code>tbjs</code> frontend framework.</li> <li>Interact via a robust Command Line Interface (CLI).</li> </ul> </li> <li>\ud83d\ude80 Performance &amp; Safety: Leverage Rust for backend server components (Actix) and Python for scripting and application logic.</li> <li>\ud83e\udde9 Extensibility: Easily create and integrate new functions, tools, or full mini-applications.</li> <li>System Independence: Strives to make applications and tools runnable across different operating systems.</li> <li>Unified Development: Provides a cohesive environment for Python, Rust, and web technologies.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li> <p>Installation:     For detailed instructions on how to install the core Python library or set up the full-stack development environment, please see the Installation Guide.     <pre><code># Quick install for the Python package\npip install ToolBoxV2\n</code></pre></p> </li> <li> <p>Developer Guide:     To learn how to create modules, use the <code>App</code> class, and interact with the CLI, explore the full Developer Documentation. </p> </li> <li> <p>Explore the Code:     Dive into the GitHub Repository to see the project structure and contribute.</p> </li> </ul>"},{"location":"#example-use-cases","title":"Example Use Cases","text":"<p>ToolBoxV2 can be used for: *   Personal productivity tools (calendars, note-takers). *   Development utilities and automation scripts. *   Custom internal business applications. *   Interactive data processing and visualization tools. *   And much more!</p>"},{"location":"#credits","title":"Credits","text":"<p>This package was created with inspiration from project structures like those generated by Cookiecutter and templates such as giswqs/pypackage.</p> <p>\u00a9 2022\u20132025 Markin Hausmanns \u2013 All rights reserved.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"clis/","title":"CLIS","text":"<p>Of course. Here are three rich helper guides and usage examples for your enhanced CLI applications, formatted in Markdown.</p>"},{"location":"clis/#p2p-tunnel-manager-tmc_p2p_clipy-user-guide","title":"\ud83d\ude80 P2P Tunnel Manager (<code>tmc_p2p_cli.py</code>) - User Guide","text":"<p>This utility provides a robust command-line interface to manage instances of the P2P tunneling application. It creates isolated environments for each peer and relay, making it easy to configure, run, and debug complex network setups.</p>"},{"location":"clis/#quickstart","title":"Quickstart","text":"<ol> <li> <p>Build the application: <pre><code>tb p2p build\n</code></pre></p> </li> <li> <p>Start a Relay Server: <pre><code>tb p2p start-relay main-relay --password \"a-secure-password\"\n</code></pre></p> </li> <li> <p>Start Peers:</p> <ul> <li>Provider (exposes a local service):     <pre><code>tb p2p start-peer api-provider --peer-id service-A \\\n  --relay-addr 127.0.0.1:9000 --relay-pass \"a-secure-password\" \\\n  --forward 127.0.0.1:3000\n</code></pre></li> <li>Consumer (accesses the provider's service):     <pre><code>tb p2p start-peer api-consumer --target service-A \\\n  --relay-addr 127.0.0.1:9000 --relay-pass \"a-secure-password\" \\\n  --listen 127.0.0.1:8000\n</code></pre></li> </ul> </li> <li> <p>Check Status &amp; Logs: <pre><code>tb p2p status\ntb p2p logs api-provider\n</code></pre></p> </li> <li> <p>Stop Instances: <pre><code># Stop one instance\ntb p2p stop api-consumer\n\n# Stop all instances\ntb p2p stop\n</code></pre></p> </li> </ol>"},{"location":"clis/#blob-db-cluster-manager-db_clipy-user-guide","title":"\ud83d\ude80 Blob DB Cluster Manager (<code>db_cli.py</code>) - User Guide","text":"<p>This CLI is a powerful tool for managing a distributed cluster of <code>r_blob_db</code> instances. It handles configuration, state, and provides commands for starting, stopping, and health-checking the entire cluster or individual nodes.</p>"},{"location":"clis/#quickstart_1","title":"Quickstart","text":"<ol> <li> <p>Build the application: <pre><code>tb db build\n</code></pre></p> </li> <li> <p>Initialize the Cluster: The first time you run a command, a <code>cluster_config.json</code> is created with a default two-node setup. You can customize this file.</p> </li> <li> <p>Start the Cluster: <pre><code># Starts all instances defined in cluster_config.json\ntb db start\n</code></pre></p> </li> <li> <p>Check Status &amp; Health: <pre><code>tb db status\ntb db health\n</code></pre></p> </li> <li> <p>Stop the Cluster: <pre><code># Stop a single instance\ntb db stop --instance-id instance-01\n\n# Stop all instances\ntb db stop\n</code></pre></p> </li> </ol>"},{"location":"clis/#example-2-performing-a-rolling-update-on-the-live-cluster","title":"Example 2: Performing a Rolling Update on the Live Cluster","text":"<p>Scenario: You've developed <code>v1.1.0</code> of <code>r_blob_db</code> and need to update your running <code>v1.0.0</code> cluster without any downtime. The rolling update process updates one node at a time, ensuring the cluster remains available.</p> <ol> <li> <p>Check Current Cluster Health:     Before starting, ensure all nodes are healthy.     <pre><code>tb db health\n</code></pre> You should see all instances report <code>\u2705 OK</code>.</p> </li> <li> <p>Build the New Version:     Compile the new version of your application. The manager will automatically find the new binary.     <pre><code># Assuming your code is updated to v1.1.0\ntb db build\n</code></pre></p> </li> <li> <p>Initiate the Rolling Update:     Execute the <code>update</code> command, specifying the new version string.     <pre><code>tb db update --version \"v1.1.0\"\n</code></pre></p> </li> <li> <p>Monitor the Process:     The CLI will provide detailed, real-time feedback:     <pre><code>--- Starting Rolling Update to Version v1.1.0 ---\n\n[1/2] Updating instance 'instance-01'...\n\u23f9\ufe0f  Instance 'instance-01' stopped.\n\ud83d\ude80 Starting instance 'instance-01' on port 3001...\n\u2705 Instance 'instance-01' started successfully. (PID: 12346)\n...\n\u29d6 Waiting for 'instance-01' to become healthy...\n\u2705 Instance 'instance-01' is healthy with new version.\n\n[2/2] Updating instance 'instance-02'...\n...\n--- Rolling Update Complete ---\n</code></pre></p> </li> <li> <p>Verify the Update:     Run the health check again. All instances should now report <code>OK</code> and show <code>server_version: v1.1.0</code>.     <pre><code>tb db health\n</code></pre></p> </li> </ol>"},{"location":"clis/#api-server-manager-api_managerpy-user-guide","title":"\ud83d\ude80 API Server Manager (<code>api_manager.py</code>) - User Guide","text":"<p>This manager is designed for high-availability web services. Its standout feature is the ability to perform zero-downtime updates on POSIX systems (Linux/macOS) by passing the active network socket from the old process to the new one, ensuring no client requests are dropped during the update.</p>"},{"location":"clis/#quickstart_2","title":"Quickstart","text":"<ol> <li> <p>Build the application: <pre><code># Assuming the CLI entrypoint is mapped to `tb`\ntb api build\n</code></pre></p> </li> <li> <p>Start the Server:</p> <ul> <li>On Linux/macOS (with Zero-Downtime enabled): <pre><code>tb api start --posix-zdt\n</code></pre></li> <li>On Windows (uses graceful restart): <pre><code>tb api start\n</code></pre></li> </ul> </li> <li> <p>Check Status: <pre><code>tb api status\n</code></pre></p> </li> <li> <p>Update the Server: <pre><code># First, build the new version\ntb api build\n\n# Then, run the update\ntb api update --version \"v1.2.0\" --posix-zdt\n</code></pre></p> </li> <li> <p>Stop the Server: <pre><code>tb api stop\n</code></pre></p> </li> </ol>"},{"location":"clis/#example-3-zero-downtime-deployment-on-a-linux-server","title":"Example 3: Zero-Downtime Deployment on a Linux Server","text":"<p>Scenario: Your API server is handling live traffic. You need to deploy a critical security patch (<code>v1.0.1</code>) without interrupting any ongoing client connections.</p> <ol> <li> <p>Check Initial State:     Ensure the server is running correctly. The <code>--posix-zdt</code> flag confirms that the manager is aware of the socket file descriptor.     <pre><code>tb api status --posix-zdt\n</code></pre> Output: <pre><code>--- Server Status ---\n  \u2705 RUNNING\n    PID:        11223\n    Version:    v1.0.0\n    Executable: /path/to/project/src-core/simple-core-server\n    Listening FD: 4 (POSIX ZDT Active)\n</code></pre></p> </li> <li> <p>Build the New Version:     Compile the patched version of the code.     <pre><code>tb api build\n</code></pre></p> </li> <li> <p>Execute the Zero-Downtime Update:     Run the <code>update</code> command with the <code>--posix-zdt</code> flag.     <pre><code>tb api update --version \"v1.0.1\" --posix-zdt\n</code></pre></p> </li> <li> <p>Observe the Magic:     The manager performs the following sequence seamlessly:</p> <ul> <li>It finds the persistent socket file descriptor (<code>FD: 4</code>).</li> <li>It starts the new server process (<code>v1.0.1</code>), passing it ownership of the active socket. The new server begins accepting new connections on the same port immediately.</li> <li>Once the new server is running, the manager sends a <code>SIGTERM</code> signal to the old process (<code>v1.0.0</code>).</li> <li>The old process stops accepting new connections but finishes handling any in-flight requests before shutting down.</li> <li>The state file is updated with the new PID and version.</li> </ul> <p>Terminal Output: <pre><code>--- [POSIX] Starting Zero-Downtime Update to v1.0.1 ---\n\u2705 New server started (PID: 11255).\n\u23f9\ufe0f  Process 11223 stopped.\n--- Update Complete. New PID: 11255 ---\n</code></pre></p> </li> <li> <p>Final Verification:     Check the status again. The server is still <code>RUNNING</code>, but now with the new PID and version. No client would have noticed the switch.     <pre><code>tb api status --posix-zdt\n</code></pre></p> </li> </ol> <p>and wit h the same focus new the last ui update + use real Posix by global fag: import contextlib</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/MarkinHaus/ToolBoxV2/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>ToolBox could always use more documentation, whether as part of the official ToolBox docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/MarkinHaus/ToolBoxV2/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up ToolBoxV2 for local development.</p> <ol> <li> <p>Fork the ToolBoxV2 repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> </li> </ol> <pre><code>$ git clone git@github.com:MarkinHaus/ToolBoxV2.git\n</code></pre> <ol> <li>Install your local copy into a virtualenv. Assuming you have    virtualenvwrapper installed, this is how you set up your fork for    local development:</li> </ol> <pre><code>$ mkvirtualenv ToolBoxV2\n$ cd ToolBoxV2/\n$ python setup.py develop\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li>When you're done making changes, check that your changes pass flake8    and the tests, including testing other Python versions with tox:</li> </ol> <pre><code>$ flake8 ToolBoxV2 tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.    Put your new functionality into a function with a docstring, and add    the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and    for PyPy. Check https://github.com/MarkinHaus/ToolBoxV2/pull_requests and make sure that the tests pass for all    supported Python versions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#gei-isaa-redy-in-conda-with-cuda-conda-install-pytorch-torchvision-torchaudio-pytorch-cuda124-c-pytorch-c-nvidia","title":"Gei isaa redy in conda with cuda  # conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia","text":""},{"location":"faq/#errors","title":"Errors :","text":""},{"location":"faq/#modulenotfounderror-no-module-named-_cffi_backend-fix-pip-vvv-install-upgrade-force-reinstall-cffi","title":"ModuleNotFoundError: No module named '_cffi_backend' fix -&gt; pip -vvv install --upgrade --force-reinstall cffi","text":""},{"location":"faq/#extraes-langchain-experimental-astor-pyaudio-pebble-transformers-litellm-nltk-gpt4all-speechrecognition-chromadb-pydub-duckduckgo-search-langchain-groq-beautifulsoup4-langchain-huggingface-langchain-langchain-chroma-langchain-ollama-tiktoken","title":"extraes : langchain-experimental astor PyAudio Pebble transformers litellm nltk gpt4all SpeechRecognition chromadb pydub duckduckgo-search langchain-groq beautifulsoup4 langchain-huggingface langchain  langchain-chroma langchain-ollama tiktoken","text":""},{"location":"installation/","title":"ToolBoxV2: Installation Guide","text":"<p>This guide provides instructions for installing ToolBoxV2, whether you need just the core Python library or the full-stack application including the Rust server and Tauri/Web frontend.</p>"},{"location":"installation/#1-installing-the-core-python-library","title":"1. Installing the Core Python Library","text":"<p>This method is suitable if you primarily need to use ToolBoxV2 as a Python library within your own projects or want to develop Python-based modules for it.</p>"},{"location":"installation/#option-a-stable-release-from-pypi-recommended","title":"Option A: Stable Release from PyPI (Recommended)","text":"<p>This is the preferred method for installing the latest stable release of the ToolBoxV2 Python package.</p> <ol> <li> <p>Ensure you have Python and pip:     If you don't have Python and pip installed, this Python installation guide can help. We recommend Python 3.10 or newer.</p> </li> <li> <p>Install ToolBoxV2:     Open your terminal or command prompt and run:     <pre><code>pip install ToolBoxV2\n</code></pre> Consider using a virtual environment to manage project dependencies: <pre><code># Create a virtual environment (optional but recommended)\npython -m venv .venv\n# Activate it (Windows)\n# .venv\\Scripts\\activate\n# Activate it (macOS/Linux)\n# source .venv/bin/activate\n\npip install ToolBoxV2\n</code></pre></p> </li> </ol>"},{"location":"installation/#option-b-from-source-latest-development-version","title":"Option B: From Source (Latest Development Version)","text":"<p>This method allows you to get the very latest code from the GitHub repository, which might include new features or changes not yet in a stable release.</p> <ol> <li> <p>Clone the Repository: <pre><code>git clone https://github.com/MarkinHaus/ToolBoxV2.git\ncd ToolBoxV2\n</code></pre></p> </li> <li> <p>Install in Editable Mode:     This installs the package from your local clone, and any changes you make to the source code will be immediately reflected in your environment.</p> <ul> <li>Using pip: <pre><code># Recommended: Activate a virtual environment first\npip install -e .\n</code></pre></li> <li>Using <code>uv</code> (a fast Python package installer and resolver): <pre><code># Recommended: Activate a virtual environment first\nuv pip install -e .\n</code></pre></li> <li>Using the provided script (sets up environment):     This script creates a virtual environment and installs dependencies.     <pre><code>chmod +x install_python_env.sh\n./install_python_env.sh\n</code></pre></li> </ul> </li> </ol>"},{"location":"installation/#option-c-directly-from-github-with-pip","title":"Option C: Directly from GitHub with pip","text":"<p>You can also install directly from the GitHub repository without cloning it first: <pre><code>pip install git+https://github.com/MarkinHaus/ToolBoxV2.git\n</code></pre></p>"},{"location":"installation/#2-installing-the-full-stack-desktopweb-application","title":"2. Installing the Full Stack Desktop/Web Application","text":"<p>This setup is for developers who want to run or develop the complete ToolBoxV2 application, including the Python backend, Rust server (Actix), and the Tauri-based desktop application or <code>tbjs</code> web frontend.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed on your system:</p> <ul> <li>Python: Version 3.10 or higher.</li> <li>Rust and Cargo: Install from rust-lang.org.</li> <li>Node.js and npm/pnpm: Install from nodejs.org. We recommend <code>pnpm</code> for managing Node.js dependencies in this project.<ul> <li>Install <code>pnpm</code> globally: <code>npm install -g pnpm</code></li> </ul> </li> <li>Tauri CLI: Install using Cargo: <code>cargo install tauri-cli</code></li> </ul> <p>Ensure the virtual environment created by the script (or one you created manually) is activated for the subsequent steps.</p> <ol> <li>Install Node.js Dependencies and Build Rust Components:     From the root of the <code>ToolBoxV2</code> directory:     <pre><code>pnpm install  # Installs Node.js dependencies for tbjs and Tauri frontend\n</code></pre>     The Rust backend (<code>src-core/</code>) and Tauri components are typically built as part of the <code>pnpm</code> scripts defined in <code>package.json</code>. If you need to build the Rust core manually:     <pre><code># (Usually not needed if using pnpm scripts)\n# cargo build --release --manifest-path src-core/Cargo.toml\n</code></pre> the build step is Usually handled by the api flow</li> </ol>"},{"location":"installation/#running-the-application-in-cli","title":"Running the Application in CLI","text":"<ul> <li>Row python runner tb <pre><code>tb -c {MOD_NAME} {FUCTION_NAME} {AGRGS} --kwargs name:value\n</code></pre></li> <li>or run in ipython <pre><code>tb --ipy\n</code></pre></li> </ul>"},{"location":"installation/#running-the-application-in-server-mode-for-web-and-desktop","title":"Running the Application in Server mode for web and Desktop","text":"<p>Refer to the scripts in the <code>package.json</code> file for various ways to run and build the application. Common commands include:</p> <ul> <li> <p>Web Development Mode (tbjs frontend with hot-reloading): <pre><code>pnpm dev\n# or live\n</code></pre>     This typically starts the Rust server and the web frontend development server.</p> </li> <li> <p>Tauri Desktop Application (Development Mode): <pre><code>pnpm tauri dev\n</code></pre>     This will build and run the Tauri desktop application with hot-reloading for the frontend.</p> </li> <li> <p>Build Tauri Desktop Application (Production): <pre><code>pnpm tauri build # Or a custom script like `pnpm tauriB` if defined\n</code></pre>     This creates a distributable binary of the desktop application.</p> </li> </ul> <p>For more specific build and run commands, please consult the <code>scripts</code> section in the <code>package.json</code> file located in the <code>ToolBoxV2</code> repository root or use the CLI help: <pre><code>    tb --help\n    # or\n    python -m toolboxv2 --help\n</code></pre></p>"},{"location":"installation/#developing-tip-use-to-activate-all-hooks","title":"developing tip use to activate all hooks","text":"<pre><code>    bash .github/hooks/setup_hooks.sh\n</code></pre>"},{"location":"installation/#auto-version-commit-hook-add-to-the-commit-msg-and-for-auto-summary","title":"auto version commit hook add &lt;#&gt; to the commit msg and  for auto summary","text":""},{"location":"installation/#auto-tagging-of-version-dev-alpha-or-release-tagging-syntax-in-commit-msg","title":"auto tagging of version dev, alpha or release tagging syntax in commit msg <ul> <li>[t:d] for dev</li> <li>[t:a] for alpha and</li> <li>[t:r] for release</li> </ul> <p>all with auto versioning</p>","text":""},{"location":"installation/#pre-commit-hook","title":"pre-commit hook","text":"<p>runs Ruff Bandit Safety versions and on  in the commit msg auto summary of the changes crates an report in local-reports"},{"location":"installation/#_1","title":"????????? <p><pre><code>INSTALLER_URL=\"https://raw.githubusercontent.com/MarkinHaus/ToolBoxV2/refs/heads/master/installer.sh\"; (echo \"Fetching installer script...\" &amp;&amp; curl -sSL -o installer.sh \"$INSTALLER_URL\" &amp;&amp; echo \"Creating default 'init.config'...\" &amp;&amp; cat &lt;&lt;EOL &gt; init.config &amp;&amp; echo \"# ToolBoxV2 Installer Configuration\" &amp;&amp; echo \"# File will be located at: $(pwd)/init.config\" &amp;&amp; echo \"# Modify values below as needed before proceeding.\" &amp;&amp; echo \"# The installer (installer.sh) will use these if this file exists and no arguments are provided to it.\" &amp;&amp; echo \"# --- Example values (uncomment and change if needed): ---\" &amp;&amp; echo \"# TB_VERSION=latest\" &amp;&amp; echo \"# INSTALL_SOURCE=pip\" &amp;&amp; echo \"# PKG_MANAGER=pip\" &amp;&amp; echo \"# PYTHON_VERSION_TARGET=3.11\" &amp;&amp; echo \"# ISAA_EXTRA=false\" &amp;&amp; echo \"# DEV_EXTRA=false\" &amp;&amp; echo \"# INSTALL_LOCATION_TYPE=apps_folder\" &amp;&amp; EOL &amp;&amp; INIT_CONFIG_PATH=\"$(pwd)/init.config\" &amp;&amp; echo -e \"\\n\\033[0;32m\ud83d\udcc4 Default 'init.config' created at:\\033[0m \\033[1;33m$INIT_CONFIG_PATH\\033[0m\" &amp;&amp; echo -e \"   You can review or modify it now in another terminal if you wish.\" &amp;&amp; echo -e \"   The main script (installer.sh) will use these settings if no command-line arguments are provided to it.\" &amp;&amp; read -p \"\u23f3 Press [Enter] to make the installer executable and run it...\" REPLY &amp;&amp; chmod +x installer.sh &amp;&amp; echo \"\ud83d\ude80 Running installer...\" &amp;&amp; ./installer.sh) || echo -e \"\\033[0;31m\u274c An error occurred during the setup process. Please check messages above.\\033[0m\"\n</code></pre> onliner installer</p>","text":""},{"location":"isaa/","title":"ISAA (Intelligent System Agent Architecture) Module Documentation","text":"<p>Version: 0.2.0</p>"},{"location":"isaa/#1-overview","title":"1. Overview","text":"<p>The ISAA module provides a comprehensive framework for building, managing, and orchestrating AI agents. It leverages the <code>EnhancedAgent</code> and <code>EnhancedAgentBuilder</code> for creating sophisticated agents with capabilities like tool use, code execution, web interaction, and persistent memory. The module is designed to be extensible and configurable, allowing developers to create complex multi-agent systems and automated workflows.</p> <p>Key features include: *   Advanced Agent System: Based on <code>EnhancedAgent</code> for robust and production-ready agents. *   Flexible Agent Configuration: Uses <code>EnhancedAgentBuilder</code> for fluent and detailed agent setup. *   Task Chain Management: Define and execute sequences of agent actions or tool uses. *   Interactive Code Execution Pipelines: Stateful Python execution environments for agents. *   Semantic Memory: Persistent knowledge storage and retrieval using <code>AISemanticMemory</code>. *   Tool Integration: Supports ADK-compatible tools, custom functions, and has provisions for LangChain tools. *   Asynchronous Operations: Many core functionalities are <code>async</code> for better performance.</p>"},{"location":"isaa/#2-core-concepts","title":"2. Core Concepts","text":""},{"location":"isaa/#21-enhancedagent","title":"2.1. <code>EnhancedAgent</code>","text":"<p>The <code>EnhancedAgent</code> is the primary agent class in ISAA. It integrates: *   LiteLLM: For interaction with a wide range of LLMs. *   ADK (Agent Development Kit): For structured tool use, planning, and code execution (if ADK is available and configured). *   A2A (Agent-to-Agent): For inter-agent communication (if A2A is available). *   MCP (Model Context Protocol): For exposing agent capabilities (if MCP is available). *   World Model: A dictionary-like persistent state for the agent. *   Cost Tracking: Built-in user cost tracking. *   Callbacks: For streaming, progress, and post-run actions.</p>"},{"location":"isaa/#22-enhancedagentbuilder","title":"2.2. <code>EnhancedAgentBuilder</code>","text":"<p>The <code>EnhancedAgentBuilder</code> provides a fluent API to configure and construct <code>EnhancedAgent</code> instances. Key aspects: *   Configuration Model (<code>BuilderConfig</code>): A Pydantic model that holds all serializable configurations for an agent. This can be saved to and loaded from JSON. *   Model Configuration: Specify LLM model, API keys (via env vars), temperature, etc. *   Behavior: Streaming, logging, initial world model data. *   Framework Integrations: Enable and configure ADK, A2A, MCP. *   Tool Management: Add ADK tools (including wrapped functions). *   Cost Tracking: Configure persistence for user costs. *   Telemetry: Configure OpenTelemetry.</p>"},{"location":"isaa/#3-initialization-and-configuration-tools-class","title":"3. Initialization and Configuration (<code>Tools</code> Class)","text":"<p>The <code>Tools</code> class is the main entry point for interacting with the ISAA module.</p> <pre><code>from toolboxv2 import get_app\nfrom toolboxv2.mods.isaa.module import Tools\n\n# Get the application instance (if using toolboxv2 framework)\napp = get_app(\"my_application\")\nisaa = Tools(app=app) # or isaa = app.get_mod(\"isaa\") if registered\n\n# Initialize ISAA (loads configs, sets up defaults)\n# This is now an async operation if it involves building default agents\nasync def initialize_isaa():\n    await isaa.init_isaa()\n    print(\"ISAA initialized.\")\n\n# asyncio.run(initialize_isaa())\n</code></pre>"},{"location":"isaa/#31-configuration-isaaconfig","title":"3.1. Configuration (<code>isaa.config</code>)","text":"<p>The <code>isaa.config</code> dictionary holds various settings: *   <code>DEFAULTMODEL*</code>: Default LLM model identifiers for different agent types (e.g., <code>DEFAULTMODEL0</code>, <code>DEFAULTMODELCODE</code>). These can be overridden by environment variables. *   <code>agents-name-list</code>: A list of registered agent names. *   <code>controller_file</code>: Path to the JSON file for <code>ControllerManager</code> (LLM modes). *   Other internal states and paths.</p> <p>API keys (like <code>OPENAI_API_KEY</code>, <code>GEMINI_API_KEY</code>, etc.) are typically loaded from environment variables by LiteLLM or explicitly set in the <code>EnhancedAgentBuilder</code> via <code>with_api_key_from_env()</code>.</p>"},{"location":"isaa/#32-isaaon_exit","title":"3.2. <code>isaa.on_exit()</code>","text":"<p>Called when the application or module shuts down. It saves: *   Agent builder configurations (<code>BuilderConfig</code> dicts from <code>isaa.agent_data</code>). *   <code>ControllerManager</code> state. *   <code>AgentChain</code> definitions. *   <code>Scripts</code> definitions. *   ISAA <code>Tools</code> class configuration.</p>"},{"location":"isaa/#4-agent-management","title":"4. Agent Management","text":"<p>All agent management functions are now primarily <code>async</code>.</p>"},{"location":"isaa/#41-getting-an-agent-builder-async-get_agent_builder","title":"4.1. Getting an Agent Builder (<code>async get_agent_builder</code>)","text":"<p>This method returns a pre-configured <code>EnhancedAgentBuilder</code> instance.</p> <p><pre><code>async def manage_agent_builder():\n    # Get a default builder for an agent named \"coder_agent\"\n    coder_builder = await isaa.get_agent_builder(\"coder_agent\")\n\n    # Further configure the builder\n    coder_builder.with_model(\"anthropic/claude-3-haiku-20240229\")\n    coder_builder.with_system_message(\"You are a master Python programmer.\")\n    coder_builder.enable_adk_code_executor(\"adk_builtin\") # If ADK is available\n\n    # ... other configurations ...\n    return coder_builder\n</code></pre> Default builders come with common ISAA tools like <code>runAgent</code>, <code>memorySearch</code>, <code>searchWeb</code>, and <code>shell</code>.</p>"},{"location":"isaa/#42-registering-an-agent-async-register_agent","title":"4.2. Registering an Agent (<code>async register_agent</code>)","text":"<p>Once an <code>EnhancedAgentBuilder</code> is configured, its configuration can be registered with ISAA. The agent instance itself will be built on demand.</p> <p><pre><code>async def register_my_agent():\n    builder = await isaa.get_agent_builder(\"my_query_agent\")\n    builder.with_system_message(\"You answer questions based on internal memory.\")\n\n    await isaa.register_agent(builder)\n    print(\"Agent 'my_query_agent' configuration registered.\")\n</code></pre> This saves the builder's configuration to a JSON file (e.g., <code>.data/app_id/Agents/my_query_agent.agent.json</code>) and stores the config dictionary in <code>isaa.agent_data</code>.</p>"},{"location":"isaa/#43-retrieving-an-agent-instance-async-get_agent","title":"4.3. Retrieving an Agent Instance (<code>async get_agent</code>)","text":"<p>This <code>async</code> method retrieves (and builds if necessary) an <code>EnhancedAgent</code> instance.</p> <p><pre><code>async def retrieve_and_use_agent():\n    my_agent = await isaa.get_agent(\"my_query_agent\")\n    # my_agent is now an instance of EnhancedAgent\n\n    # If you need to override the model for this instance (will rebuild if different)\n    # my_agent_gpt4 = await isaa.get_agent(\"my_query_agent\", model_override=\"openai/gpt-4-turbo\")\n\n    response = await my_agent.a_run(\"What is the capital of France?\")\n    print(response)\n</code></pre> If an agent configuration exists, it's loaded. Otherwise, a default builder is used. The agent instance is cached in <code>isaa.config['agent-instance-{name}']</code>.</p>"},{"location":"isaa/#5-running-agents-and-tasks","title":"5. Running Agents and Tasks","text":""},{"location":"isaa/#51-running-an-agent-async-run_agent","title":"5.1. Running an Agent (<code>async run_agent</code>)","text":"<p>This is the primary method to interact with a registered agent.</p> <p><pre><code>async def run_specific_agent():\n    # Ensure agent is registered (e.g., during init_isaa or manually)\n    # For example, register a 'self' agent if not already present\n    if \"self\" not in isaa.config.get(\"agents-name-list\", []):\n        self_builder = await isaa.get_agent_builder(\"self\")\n        await isaa.register_agent(self_builder)\n\n    response = await isaa.run_agent(\"self\", \"Tell me a joke.\")\n    print(f\"Self agent says: {response}\")\n\n    # Example with session_id for persistent history with EnhancedAgent\n    session_id = \"user123_chat_session\"\n    response1 = await isaa.run_agent(\"self\", \"My name is Bob.\", session_id=session_id)\n    response2 = await isaa.run_agent(\"self\", \"What is my name?\", session_id=session_id)\n    print(f\"Agent remembers: {response2}\")\n</code></pre> The <code>run_agent</code> method now calls the <code>EnhancedAgent.a_run()</code> method, which supports features like session-based history, world model updates, ADK tool execution, etc.</p>"},{"location":"isaa/#52-mini-task-completion-async-mini_task_completion","title":"5.2. Mini Task Completion (<code>async mini_task_completion</code>)","text":"<p>For quick, one-off LLM calls without full agent capabilities.</p> <pre><code>async def run_mini_task():\n    translation = await isaa.mini_task_completion(\n        mini_task=\"Translate to German.\",\n        user_task=\"Hello, how are you?\"\n    )\n    print(f\"Translation: {translation}\")\n</code></pre>"},{"location":"isaa/#53-structured-output-async-format_class","title":"5.3. Structured Output (<code>async format_class</code>)","text":"<p>To get LLM output structured according to a Pydantic model.</p> <pre><code>from pydantic import BaseModel\n\nclass UserInfo(BaseModel):\n    name: str\n    age: int\n    city: Optional[str] = None\n\nasync def get_structured_info():\n    user_data_dict = await isaa.format_class(\n        UserInfo,\n        \"The user is Alice, 30 years old, from New York.\"\n    )\n    if user_data_dict:\n        user_info = UserInfo(**user_data_dict)\n        print(f\"Parsed User: {user_info.name}, Age: {user_info.age}\")\n</code></pre>"},{"location":"isaa/#6-task-chains","title":"6. Task Chains","text":"<p>Task chains allow defining a sequence of operations involving agents or tools.</p>"},{"location":"isaa/#61-defining-a-task-chain","title":"6.1. Defining a Task Chain","text":"<p>Task chains are defined as lists of dictionaries, where each dictionary represents a task. The <code>TaskChain</code> Pydantic model from <code>toolboxv2.mods.isaa.types</code> can be used for validation.</p> <p>Example structure for a task in the list: <pre><code>{\n  \"use\": \"agent\", // \"agent\", \"tool\", \"chain\"\n  \"name\": \"agent_name_or_tool_name_or_chain_name\",\n  \"args\": \"Prompt or arguments, can use $variable or $user-input\",\n  \"return_key\": \"my_result_variable\" // Result stored under this key\n}\n</code></pre></p>"},{"location":"isaa/#62-creating-a-task-chain-async-crate_task_chain","title":"6.2. Creating a Task Chain (<code>async crate_task_chain</code>)","text":"<p>Uses an LLM (typically \"TaskChainAgent\") to generate a task chain definition from a natural language prompt.</p> <pre><code>async def create_my_chain():\n    chain_name = await isaa.crate_task_chain(\n        \"Create a plan to research a topic: first search the web, then summarize findings.\"\n    )\n    if chain_name:\n        print(f\"Task chain '{chain_name}' created.\")\n        isaa.save_task(chain_name) # Save it\n    else:\n        print(\"Failed to create task chain.\")\n</code></pre>"},{"location":"isaa/#63-managing-task-chains","title":"6.3. Managing Task Chains","text":"<ul> <li><code>isaa.add_task(chain_name, task_definition_list)</code>: Manually add/update a chain.</li> <li><code>isaa.get_task(chain_name)</code>: Get the definition of a chain.</li> <li><code>isaa.list_task()</code>: List names of all available chains.</li> <li><code>isaa.save_task(chain_name=None)</code>: Save one or all chains to file.</li> <li><code>isaa.load_task(chain_name=None)</code>: Load one or all chains from file.</li> </ul>"},{"location":"isaa/#64-running-a-task-chain-async-run_task","title":"6.4. Running a Task Chain (<code>async run_task</code>)","text":"<p>Executes a defined task chain.</p> <p><pre><code>async def execute_my_chain():\n    # Assume \"research_topic_chain\" was created and saved earlier\n    isaa.load_task(\"research_topic_chain\") # Load if not already in memory\n    results = await isaa.run_task(\n        task_input=\"Quantum computing advancements in 2024\",\n        chain_name=\"research_topic_chain\"\n    )\n    print(\"Task chain execution results:\", results)\n</code></pre> The <code>ChainTreeExecutor</code> handles variable injection (<code>$variable_name</code>, <code>$user-input</code>) and result passing between tasks.</p>"},{"location":"isaa/#7-pipelines-for-code-execution-pipeline-class","title":"7. Pipelines for Code Execution (<code>Pipeline</code> Class)","text":"<p>The <code>Pipeline</code> class (from <code>toolboxv2.mods.isaa.CodingAgent.live</code>) provides a stateful environment for agents to execute Python code iteratively. It uses a mock IPython interface.</p>"},{"location":"isaa/#71-getting-a-pipeline-instance-async-get_pipe","title":"7.1. Getting a Pipeline Instance (<code>async get_pipe</code>)","text":"<p>Retrieves or creates a <code>Pipeline</code> instance associated with a specific ISAA agent. The agent's context (variables, potentially its LLM for thinking within the pipeline) can influence the pipeline's behavior.</p> <pre><code>async def setup_pipeline():\n    # Get a pipeline associated with the 'coder_agent'\n    # Ensure 'coder_agent' is registered\n    coder_agent_builder = await isaa.get_agent_builder(\"coder_agent\")\n    coder_agent_builder.with_system_message(\"You write and execute Python code to solve problems.\")\n    await isaa.register_agent(coder_agent_builder)\n\n    coder_pipeline = await isaa.get_pipe(\"coder_agent\", verbose=True)\n    return coder_pipeline\n</code></pre>"},{"location":"isaa/#72-running-a-pipeline-async-run_pipe","title":"7.2. Running a Pipeline (<code>async run_pipe</code>)","text":"<p>Executes a task within the pipeline. The agent associated with the pipeline will \"think\" and generate code or actions to be executed by the pipeline's IPython environment.</p> <p><pre><code>async def execute_pipeline_task():\n    coder_pipeline = await isaa.get_pipe(\"coder_agent\") # Assumes \"coder_agent\" is set up\n\n    task_description = \"Define a function to calculate factorial and test it with n=5.\"\n    pipeline_result = await coder_pipeline.run(task_description)\n\n    print(f\"Pipeline Final Result: {pipeline_result.result}\")\n    print(\"Execution History:\")\n    for record in pipeline_result.execution_history:\n        print(f\"  Code: {record.code[:50]}... -&gt; Result: {record.result}, Error: {record.error}\")\n    print(\"Final Variables in Pipeline:\", pipeline_result.variables)\n</code></pre> The <code>Pipeline.run</code> method involves multiple turns of the agent thinking, generating code/actions, and the pipeline executing them, until the task is marked \"done\" or iterations are exhausted.</p>"},{"location":"isaa/#8-semantic-memory-aisemanticmemory","title":"8. Semantic Memory (<code>AISemanticMemory</code>)","text":"<p>ISAA uses <code>AISemanticMemory</code> for persistent, semantic storage and retrieval of information.</p>"},{"location":"isaa/#81-accessing-memory-isaaget_memory","title":"8.1. Accessing Memory (<code>isaa.get_memory</code>)","text":"<pre><code># Get the global AISemanticMemory instance\nsemantic_memory = isaa.get_memory()\n\n# AISemanticMemory can manage multiple named \"memory spaces\" (KnowledgeBase instances)\n# To get a specific KnowledgeBase instance (e.g., for an agent):\nagent_kb = isaa.get_memory(name=\"my_agent_context\") # This will create if not exists\n</code></pre>"},{"location":"isaa/#82-interacting-with-memory","title":"8.2. Interacting with Memory","text":"<p>The <code>AISemanticMemory</code> class (and its underlying <code>KnowledgeBase</code> instances) provides methods like: *   <code>async add_data(memory_name: str, data: ..., metadata: ...)</code>: Adds data to a specific memory space. *   <code>async query(query: str, memory_names: ..., to_str: bool)</code>: Queries one or more memory spaces. *   <code>async unified_retrieve(...)</code>: A more comprehensive retrieval method.</p> <p><pre><code>async def use_semantic_memory():\n    mem = isaa.get_memory()\n    agent_name = \"researcher\"\n\n    # Ensure the agent's memory space exists (usually handled by agent init)\n    await mem.create_memory(agent_name) # Or rely on get_memory(name=...)\n\n    # Add data\n    await mem.add_data(\n        memory_name=agent_name,\n        data=\"Photosynthesis is a process used by plants to convert light energy into chemical energy.\",\n        metadata={\"source\": \"biology_notes\"}\n    )\n\n    # Query data\n    results = await mem.query(\n        query=\"How do plants get energy?\",\n        memory_names=[agent_name],\n        to_str=True\n    )\n    print(f\"Memory search results: {results}\")\n</code></pre> <code>EnhancedAgent</code> instances often have their world model, but can also interact with <code>AISemanticMemory</code> via tools for broader knowledge. <code>ChatSession</code> (used by <code>Pipeline</code>) also uses <code>AISemanticMemory</code>.</p>"},{"location":"isaa/#9-tool-integration","title":"9. Tool Integration","text":""},{"location":"isaa/#91-default-isaa-tools","title":"9.1. Default ISAA Tools","text":"<p>Agents created with <code>get_agent_builder</code> automatically get several tools: *   <code>runAgent</code>: To call other registered ISAA agents. *   <code>memorySearch</code>: To query the <code>AISemanticMemory</code>. *   <code>saveDataToMemory</code>: To save data into the agent's context in <code>AISemanticMemory</code>. *   <code>searchWeb</code>: Uses <code>WebScraper</code> to search the internet. *   <code>shell</code>: Executes shell commands using <code>shell_tool_function</code>. *   <code>runCodePipeline</code> (for agents like \"self\", \"code\"): To invoke a <code>Pipeline</code> task.</p> <p>These are added as ADK-compatible function tools to the <code>EnhancedAgentBuilder</code>.</p>"},{"location":"isaa/#92-adding-custom-and-langchain-tools-async-init_tools","title":"9.2. Adding Custom and LangChain Tools (<code>async init_tools</code>)","text":"<p>The <code>init_tools</code> method is intended for adding external tools, particularly LangChain tools, to an agent builder.</p> <p><pre><code># This is a conceptual example, as init_tools itself needs to be fully async\n# and adapt to how EnhancedAgentBuilder handles external tool definitions.\n\nasync def add_external_tools():\n    builder = await isaa.get_agent_builder(\"tool_user_agent\")\n\n    # Configuration for tools (example)\n    tools_config = {\n        \"lagChinTools\": [\"wikipedia\", \"ddg-search\"], # Example LangChain tool names\n        # \"huggingTools\": [], # HF tools are also LC tools\n        # \"Plugins\": [] # AIPluginTool\n    }\n\n    # init_tools would modify the builder by adding wrapped LangChain tools\n    await isaa.init_tools(tools_config, builder) # Pass the builder instance\n\n    await isaa.register_agent(builder)\n\n    agent_with_tools = await isaa.get_agent(\"tool_user_agent\")\n    response = await agent_with_tools.a_run(\"Search Wikipedia for 'Large Language Models'\")\n    print(response)\n</code></pre> Note: Wrapping arbitrary LangChain tools (which can have complex Pydantic <code>args_schema</code>) into ADK <code>FunctionTool</code>s (which prefer simpler schemas or Pydantic models for arguments) can be non-trivial. <code>init_tools</code> will need careful implementation to handle schema mapping or require tools to be provided as simple callables.</p>"},{"location":"isaa/#10-example-usage-flow","title":"10. Example Usage Flow","text":"<pre><code>import asyncio\nfrom toolboxv2 import get_app\nfrom toolboxv2.mods.isaa.module import Tools\nfrom pydantic import BaseModel\n\n# --- Setup ---\napp = get_app(\"isaa_demo_app\")\nisaa = Tools(app=app)\n\n# --- Pydantic Model for Structured Output ---\nclass AnalysisResult(BaseModel):\n    topic: str\n    key_points: List[str]\n    sentiment: Optional[str] = None\n\nasync def main_demo():\n    # 1. Initialize ISAA (loads configs, ControllerManager, etc.)\n    await isaa.init_isaa()\n    print(\"ISAA Initialized.\")\n\n    # 2. Create and Register a \"Researcher\" Agent\n    researcher_builder = await isaa.get_agent_builder(\"Researcher\")\n    researcher_builder.with_system_message(\n        \"You are a research assistant. Your job is to find information using web search and summarize it.\"\n    )\n    researcher_builder.with_model(\"openai/gpt-3.5-turbo\") # Example model\n    # The default builder already adds searchWeb and memory tools.\n    await isaa.register_agent(researcher_builder)\n    print(\"Researcher agent registered.\")\n\n    # 3. Create and Register a \"Summarizer\" Agent for structured output\n    summarizer_builder = await isaa.get_agent_builder(\"Summarizer\")\n    summarizer_builder.with_system_message(\n        \"You summarize text and extract key information into a structured format.\"\n    )\n    summarizer_builder.with_model(\"openai/gpt-4-turbo\") # Example model, good for JSON\n    await isaa.register_agent(summarizer_builder)\n    print(\"Summarizer agent registered.\")\n\n    # 4. Define a Task Chain\n    chain_name = \"WebResearchAndAnalyze\"\n    research_tasks = [\n        {\n            \"use\": \"agent\",\n            \"name\": \"Researcher\",\n            \"args\": \"Find recent news about AI in healthcare. Focus on the top 3 articles. User input: $user-input\", # $user-input will be main query\n            \"return_key\": \"research_findings\"\n        },\n        {\n            \"use\": \"agent\",\n            \"name\": \"Summarizer\",\n            # The 'Summarizer' agent will use its format_class capability internally if prompted correctly\n            # For this example, we assume 'Summarizer' is prompted to produce AnalysisResult\n            # A more robust way is to have a specific tool/agent that *only* does formatting.\n            \"args\": \"Analyze the following research findings and structure them: $research_findings. Extract topic, key points, and sentiment.\",\n            \"return_key\": \"structured_analysis\"\n        }\n    ]\n    isaa.agent_chain.add(chain_name, research_tasks)\n    isaa.save_task(chain_name)\n    print(f\"Task chain '{chain_name}' created and saved.\")\n\n    # 5. Run the Task Chain\n    user_query = \"Latest breakthroughs in AI-driven drug discovery\"\n    print(f\"\\nRunning task chain '{chain_name}' for query: '{user_query}'\")\n    chain_results = await isaa.run_task(user_query, chain_name)\n    print(\"\\n--- Task Chain Results ---\")\n    if chain_results and \"structured_analysis\" in chain_results:\n        analysis_output = chain_results[\"structured_analysis\"]\n        # If the summarizer agent directly returned a dict matching AnalysisResult:\n        try:\n            # The result from an agent run is typically a string.\n            # If the 'Summarizer' was specifically designed to output JSON string for AnalysisResult:\n            analysis_dict = json.loads(analysis_output) # Agent must output valid JSON string\n            structured_data = AnalysisResult(**analysis_dict)\n            print(f\"Topic: {structured_data.topic}\")\n            print(\"Key Points:\")\n            for point in structured_data.key_points:\n                print(f\"  - {point}\")\n            if structured_data.sentiment:\n                print(f\"Sentiment: {structured_data.sentiment}\")\n        except Exception as e:\n            print(f\"Could not parse structured analysis: {e}\")\n            print(\"Raw analysis output:\", analysis_output)\n    else:\n        print(\"Chain did not produce expected 'structured_analysis'. Full results:\", chain_results)\n\n    # 6. Example of using a Pipeline with a \"Coder\" agent\n    coder_builder = await isaa.get_agent_builder(\"PyCoder\")\n    coder_builder.with_system_message(\"You are a Python coding assistant. You write and execute Python code to solve problems. Ensure your code prints results or returns values.\")\n    coder_builder.enable_adk_code_executor(\"unsafe_simple\") # Or \"adk_builtin\" if model supports\n    await isaa.register_agent(coder_builder)\n\n    py_coder_pipeline = await isaa.get_pipe(\"PyCoder\", verbose=True) # Get pipeline for this agent\n    pipeline_task = \"Write a Python function that takes a list of numbers and returns their sum. Then, call this function with the list [1, 2, 3, 4, 5] and print the result.\"\n    print(f\"\\nRunning Pipeline for task: '{pipeline_task}'\")\n    pipeline_result = await py_coder_pipeline.run(pipeline_task)\n    print(\"\\n--- Pipeline Final Output ---\")\n    print(pipeline_result.result)\n    print(\"\\n--- Pipeline Variables ---\")\n    # Filter out internal IPython variables for clarity\n    final_vars = {k: v for k, v in pipeline_result.variables.items() if not k.startswith('_') and k not in ['In', 'Out', 'exit', 'quit', 'get_ipython', 'open']}\n    print(json.dumps(final_vars, default=str, indent=2))\n\n    # 7. Clean up (optional, as on_exit handles saving)\n    # isaa.on_exit()\n\nif __name__ == \"__main__\":\n    asyncio.run(main_demo())\n</code></pre>"},{"location":"isaa/#11-important-notes","title":"11. Important Notes","text":"<ul> <li>Asynchronous Nature: Most core methods of the <code>Tools</code> class are now <code>async</code>. Ensure your calling code uses <code>await</code> appropriately or runs within an asyncio event loop.</li> <li>Agent Configuration: Agent capabilities are primarily defined by their system message, the tools provided to them via the <code>EnhancedAgentBuilder</code>, and their underlying LLM model.</li> <li>Error Handling: Robust error handling should be implemented around <code>async</code> calls, especially for network-dependent operations like LLM calls or web interactions.</li> <li>ADK Integration: Full ADK functionality (planning, advanced tool schemas, long-running operations) requires Google ADK to be installed and properly configured. The <code>EnhancedAgentBuilder</code> provides methods like <code>with_adk_code_executor</code>, <code>with_adk_tool_instance</code>, etc.</li> <li>Security: Be cautious when enabling code execution (<code>unsafe_simple</code> executor is for development only) or shell access.</li> </ul> <p>This documentation provides a starting point for using the refactored ISAA module. As the module evolves, further details on specific component interactions (e.g., advanced ADK planner configurations, A2A/MCP server setup via builder) will be crucial.</p>"},{"location":"tbjs/","title":"TBjs","text":""},{"location":"tbjs/#tbjs-framework-comprehensive-guide-documentation","title":"tbjs Framework: Comprehensive Guide &amp; Documentation","text":"<p>Table of Contents</p> <ol> <li>Introduction<ul> <li>Key Design Principles &amp; Features</li> </ul> </li> <li>Getting Started<ul> <li>Prerequisites</li> <li>Installation</li> <li>HTML Setup</li> <li>Application Initialization (<code>TB.init</code>)</li> </ul> </li> <li>Core Modules (<code>TB.*</code>)<ul> <li><code>TB.config</code>: Configuration Management</li> <li><code>TB.logger</code>: Logging Utility</li> <li><code>TB.state</code>: Global State Management</li> <li><code>TB.events</code>: Event Bus / Pub/Sub</li> <li><code>TB.env</code>: Environment Detection</li> <li><code>TB.api</code>: Backend Communication</li> <li><code>TB.router</code>: SPA Routing</li> <li><code>TB.crypto</code>: Cryptographic Utilities</li> <li><code>TB.user</code>: User Session &amp; Authentication</li> <li><code>TB.sse</code>: Server-Sent Events</li> <li><code>TB.sw</code>: Service Worker Management</li> <li><code>TB.utils</code>: General Utilities</li> <li><code>TB.graphics</code>: 3D Graphics (THREE.js)</li> </ul> </li> <li>UI System (<code>TB.ui.*</code>)<ul> <li><code>TB.ui.theme</code>: Theming (Light/Dark Mode, Backgrounds)</li> <li><code>TB.ui.htmxIntegration</code>: HTMX Event Handling</li> <li><code>TB.ui.processDynamicContent</code>: Handling New DOM Content</li> <li>UI Components:<ul> <li><code>TB.ui.Modal</code></li> <li><code>TB.ui.Toast</code></li> <li><code>TB.ui.Loader</code></li> <li><code>TB.ui.Button</code></li> <li><code>TB.ui.DarkModeToggle</code></li> <li><code>TB.ui.CookieBanner</code></li> <li><code>TB.ui.MarkdownRenderer</code></li> <li><code>TB.ui.NavMenu</code></li> <li><code>TB.ui.AutocompleteWidget</code></li> </ul> </li> </ul> </li> <li>Styling with Tailwind CSS<ul> <li>Prefixing and CSS Variables</li> <li>Using <code>tbjs</code> Tailwind Config in Your Project</li> </ul> </li> <li>Advanced Topics<ul> <li>Tauri Integration</li> <li>Working with 3D Graphics</li> </ul> </li> <li>Example: Login Flow Walkthrough</li> <li>Building <code>tbjs</code> (For Developers)</li> </ol>"},{"location":"tbjs/#1-introduction","title":"1. Introduction","text":"<p><code>tbjs</code> is a modular frontend framework designed for building modern web applications, with special consideration for integration with Tauri for desktop applications and tools like HTMX and Three.js. It provides a comprehensive set of tools for managing configuration, state, API communication, routing, UI components, user authentication, and more.</p> <p>Key Design Principles &amp; Features:</p> <ul> <li>Modularity: Clear separation of concerns into <code>core</code> and <code>ui</code> modules. You can use only the parts you need.</li> <li>Event-Driven: Facilitates decoupled communication between modules via an event bus.</li> <li>Configuration-Centric: Application behavior is heavily influenced by a central configuration object.</li> <li>State Management: Centralized application state with optional persistence.</li> <li>SPA Router: Handles client-side navigation and view loading.</li> <li>API Abstraction: Simplifies backend communication, supporting both HTTP and Tauri <code>invoke</code> calls.</li> <li>UI System: Includes theme management (light/dark mode), dynamic backgrounds, and reusable UI components.</li> <li>3D Graphics Integration: Built-in support for THREE.js for dynamic backgrounds or scenes, managed by <code>TB.graphics</code>.</li> <li>User Authentication: Robust support for various authentication flows, including device key (asymmetric crypto) and WebAuthn (passkeys).</li> <li>HTMX Friendly: Designed to work seamlessly alongside HTMX for enhancing HTML with dynamic behaviors.</li> <li>Tauri-Aware: Core functionalities can adapt to run optimally in a Tauri environment.</li> <li>Modern Tooling: Built with Webpack, Babel, PostCSS, and Tailwind CSS.</li> </ul>"},{"location":"tbjs/#2-getting-started","title":"2. Getting Started","text":""},{"location":"tbjs/#prerequisites","title":"Prerequisites","text":"<p>Before using <code>tbjs</code>, ensure you have the following (or plan to include them if using related features):</p> <ol> <li>HTMX (Recommended): <code>tbjs</code> integrates well with HTMX for server-rendered partials and dynamic updates.     <pre><code>&lt;script defer src=\"https://unpkg.com/htmx.org@2.0.2/dist/htmx.min.js\"&gt;&lt;/script&gt;\n</code></pre></li> <li>Three.js (Optional, if using <code>TB.graphics</code>):     <pre><code>&lt;script defer src=\"https://cdnjs.cloudflare.com/ajax/libs/three.js/0.153.0/three.min.js\"&gt;&lt;/script&gt;\n</code></pre></li> <li> <p>Marked &amp; Highlight.js (Optional, if using <code>TB.ui.MarkdownRenderer</code>): For rendering Markdown to HTML with syntax highlighting.     <pre><code>&lt;script defer src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"&gt;&lt;/script&gt;\n&lt;script defer src=\"https://cdn.jsdelivr.net/npm/marked-highlight/lib/index.umd.min.js\"&gt;&lt;/script&gt;\n&lt;script defer src=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js\"&gt;&lt;/script&gt;\n&lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css\"&gt;\n</code></pre> Ensure <code>window.marked</code>, <code>window.markedHighlight</code>, and <code>window.hljs</code> are available before <code>TB.ui.MarkdownRenderer</code> is used.</p> </li> <li> <p>Material Symbols (Optional, used by some default UI components):     <pre><code>&lt;link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200\" /&gt;\n</code></pre></p> </li> </ol>"},{"location":"tbjs/#installation","title":"Installation","text":"<ol> <li> <p>Add <code>tbjs</code> to your project:     If <code>tbjs</code> were published to npm:     <pre><code>npm install tbjs\n# or\nyarn add tbjs\n</code></pre>     Since it's often used locally or as part of a larger monorepo, you'd typically:</p> <ul> <li>Build <code>tbjs</code> from source (see Building <code>tbjs</code>) to get <code>dist/tbjs.js</code> and <code>dist/tbjs.css</code>.</li> <li>Or, if integrating into a build system, import directly from its source path (e.g., <code>import TB from 'path/to/tbjs/src/index.js';</code>).</li> </ul> </li> <li> <p>Include files in your HTML (if using pre-built dist files): <pre><code>&lt;link rel=\"stylesheet\" href=\"path/to/your/tbjs/dist/tbjs.css\"&gt;\n&lt;!-- Load tbjs.js as a module or a global script depending on its build --&gt;\n&lt;script defer type=\"module\" src=\"path/to/your/tbjs/dist/tbjs.js\"&gt;&lt;/script&gt; &lt;!-- If ES Module build --&gt;\n&lt;!-- &lt;script defer src=\"path/to/your/tbjs/dist/tbjs.js\"&gt;&lt;/script&gt; --&gt; &lt;!-- If UMD build --&gt;\n</code></pre></p> </li> <li> <p>Peer Dependencies (Reminder):     Ensure you have <code>htmx.org</code> and <code>three</code> installed/included if you plan to use features that depend on them.     <pre><code>npm install htmx.org three # Or yarn add\n</code></pre></p> </li> </ol>"},{"location":"tbjs/#3-core-modules-tb","title":"3. Core Modules (<code>TB.*</code>)","text":""},{"location":"tbjs/#tbconfig-configuration-management","title":"<code>TB.config</code>: Configuration Management","text":"<p>Manages application-wide settings. It's initialized by <code>TB.init()</code> with default values merged with your provided configuration.</p> <ul> <li>Initialization: <code>TB.config.init(userAppConfig)</code> is called by <code>TB.init</code>.<ul> <li><code>userAppConfig</code> options:<ul> <li><code>appRootId</code> (string): ID of the main DOM element for router views. Default: <code>app-root</code>.</li> <li><code>baseApiUrl</code> (string): Base URL for API calls. Default: <code>/api</code>. Normalized to be absolute (e.g., <code>/api</code> becomes <code>window.location.origin/api</code>).</li> <li><code>baseFileUrl</code> (string): Base URL for fetching static HTML files for routing. Default: <code>window.location.origin</code>. Normalized to end with <code>/</code> if it has a path, and ensures it doesn't include file names.</li> <li><code>initialState</code> (object): Initial state for <code>TB.state</code>.</li> <li><code>themeSettings</code> (object): See <code>TB.ui.theme</code> section.</li> <li><code>routes</code> (array): Predefined routes for <code>TB.router</code> (currently for reference/future use).</li> <li><code>logLevel</code> (string): <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code>, <code>none</code>. Default: <code>info</code>.</li> <li><code>isProduction</code> (boolean): Inferred based on hostname (<code>localhost</code>, <code>127.0.0.1</code>) if not explicitly set.</li> <li><code>serviceWorker</code> (object): <code>{ enabled: boolean, url: string, scope: string }</code>.</li> </ul> </li> </ul> </li> <li>Getting Configuration: <pre><code>const apiUrl = TB.config.get('baseApiUrl');\nconst logLevel = TB.config.get('logLevel');\nconst themePref = TB.config.get('themeSettings.defaultPreference'); // Dot notation for nested\nconst allConfig = TB.config.getAll(); // Returns a copy of the entire config\n</code></pre></li> <li>Setting Configuration (dynamically, use with caution after init): <pre><code>TB.config.set('myCustomSetting', 'someValue');\nTB.config.set('featureFlags.newFeature', true);\n</code></pre></li> </ul>"},{"location":"tbjs/#tblogger-logging-utility","title":"<code>TB.logger</code>: Logging Utility","text":"<p>Provides leveled, prefixed, and timestamped logging to the console.</p> <ul> <li>Initialization: <code>TB.logger.init({ logLevel: '...' })</code> is called by <code>TB.init</code> based on <code>TB.config</code>.</li> <li>Setting Log Level: <pre><code>TB.logger.setLevel('debug'); // 'debug', 'info', 'warn', 'error', 'none'\n</code></pre></li> <li>Logging Messages: <pre><code>TB.logger.debug('Detailed debug message:', { data: 123 });\nTB.logger.info('Informational message.'); // Alias: TB.logger.log()\nTB.logger.warn('Potential issue warning.');\nTB.logger.error('An error occurred:', new Error('Something went wrong'));\n</code></pre>     Output includes a timestamp, <code>[tbjs]</code>, and the log level (e.g., <code>[DEBUG]</code>).</li> </ul>"},{"location":"tbjs/#tbstate-global-state-management","title":"<code>TB.state</code>: Global State Management","text":"<p>A simple key-value store for global application state with optional persistence to <code>localStorage</code>.</p> <ul> <li>Initialization: <code>TB.state.init(initialState)</code> is called by <code>TB.init</code> with <code>TB.config.get('initialState')</code>. Loads any persisted state.</li> <li>Getting State: <pre><code>const username = TB.state.get('user.username'); // Dot notation for nested\nconst allState = TB.state.get(); // Returns a copy of the entire state\n</code></pre></li> <li>Setting State: <pre><code>// Set a simple value\nTB.state.set('ui.darkMode', true);\n\n// Set a nested value, creating intermediate objects if they don't exist\nTB.state.set('user.profile.avatarUrl', '/path/to/avatar.png');\n\n// Persist the top-level key 'user' to localStorage\nTB.state.set('user.isLoggedIn', true, { persist: true });\n// Any change under 'user' (e.g., 'user.settings.notifications') will now persist 'user'.\n</code></pre><ul> <li>Emits <code>state:changed</code> event with <code>{ key, value, fullState }</code>.</li> <li>Emits specific event like <code>state:changed:user:profile:avatarUrl</code> with the <code>value</code>.</li> </ul> </li> <li>Deleting State: <pre><code>TB.state.delete('user.profile.temporaryToken');\nTB.state.delete('featureFlags.oldFlag', { persist: true }); // Will update persisted 'featureFlags'\n</code></pre></li> <li>Legacy \"Var\" Methods (for simple key-value persistence, prefer structured state with <code>persist</code> option):<ul> <li><code>TB.state.initVar('myVar', 'defaultValue')</code>: Sets if not already defined, persists.</li> <li><code>TB.state.delVar('myVar')</code>: Deletes and updates persisted state.</li> <li><code>TB.state.getVar('myVar')</code>: Gets value.</li> <li><code>TB.state.setVar('myVar', 'newValue')</code>: Sets value, persists.</li> </ul> </li> </ul>"},{"location":"tbjs/#tbevents-event-bus-pubsub","title":"<code>TB.events</code>: Event Bus / Pub/Sub","text":"<p>Allows modules to communicate without direct dependencies.</p> <ul> <li>Subscribing to Events: <pre><code>function handleThemeChange(eventData) {\n    console.log('Theme changed to:', eventData.mode);\n}\nTB.events.on('theme:changed', handleThemeChange);\n\n// Subscribe only once\nTB.events.once('app:firstLogin', (userData) =&gt; { /* ... */ });\n</code></pre></li> <li>Unsubscribing from Events: <pre><code>TB.events.off('theme:changed', handleThemeChange);\n</code></pre></li> <li>Emitting Events: <pre><code>TB.events.emit('user:loggedIn', { userId: 123, username: 'testuser' });\n</code></pre>     If a listener throws an error, <code>TB.logger.error</code> is called, and other listeners still execute.</li> <li>Common Framework Events: <code>tbjs:initialized</code>, <code>state:changed</code>, <code>router:navigationSuccess</code>, <code>theme:changed</code>, <code>api:networkError</code>, <code>graphics:initialized</code>, <code>user:loggedOut</code>, etc.</li> </ul>"},{"location":"tbjs/#tbenv-environment-detection","title":"<code>TB.env</code>: Environment Detection","text":"<p>Provides information about the runtime environment.</p> <ul> <li>Initialization: <code>TB.env.detect()</code> is called by <code>TB.init</code>.</li> <li>Checking Environment: <pre><code>if (TB.env.isTauri()) {\n    console.log('Running in Tauri environment.');\n} else if (TB.env.isWeb()) {\n    console.log('Running in a web browser.');\n}\nif (TB.env.isMobile()) { // Currently implies Tauri mobile if detected\n    console.log('Running on a mobile platform (Tauri).');\n}\n</code></pre></li> </ul>"},{"location":"tbjs/#tbapi-backend-communication","title":"<code>TB.api</code>: Backend Communication","text":"<p>Handles all HTTP and Tauri <code>invoke</code> calls, standardizing responses.</p> <ul> <li>Core <code>Result</code> Object: All <code>TB.api</code> methods (and Tauri invokes) aim to return or be wrapped into a <code>Result</code> object:     <pre><code>// Structure of a Result object (simplified)\n// const result = {\n//   origin: Array&lt;string&gt;,    // e.g., ['http'], ['tauri']\n//   error: string,           // From TB.ToolBoxError (e.g., 'none', 'InternalError')\n//   result: {                // Instance of ToolBoxResult\n//     data_to: string,       // From TB.ToolBoxInterfaces (e.g., 'API', 'NATIVE')\n//     data_info: string|null,// Additional info\n//     data: any              // The actual payload\n//   },\n//   info: {                  // Instance of ToolBoxInfo\n//     exec_code: number,     // HTTP status or custom code (0 for success)\n//     help_text: string      // Descriptive message\n//   },\n//   get: function() { return this.result.data; }, // Helper to get payload\n//   log: function() { /* console logs details */ },\n//   html: function() { /* returns an HTML representation for debugging */ }\n// };\n</code></pre></li> <li> <p><code>TB.api.request(moduleName, functionName, payload, method, useTauri, isSpecialAuthRoute)</code>:     The primary method for making backend requests.</p> <ul> <li><code>moduleName</code> (string): Backend module/class OR full path (e.g., <code>/validateSession</code>).</li> <li><code>functionName</code> (string|object): Backend function/method OR query params object if <code>moduleName</code> is a full path (for GET/DELETE).</li> <li><code>payload</code> (object|string|null): Data to send. Object for JSON POST/PUT; string can be form-urlencoded or query params.</li> <li><code>method</code> (string): HTTP method (<code>GET</code>, <code>POST</code>, etc.). Default: <code>POST</code>.</li> <li><code>useTauri</code> (string): <code>auto</code> (default), <code>force</code> (Tauri only), <code>never</code> (HTTP only).</li> <li><code>isSpecialAuthRoute</code> (boolean): If <code>true</code>, influences token handling (rarely needed).</li> </ul> <p>URL Construction (HTTP): *   Standard: <code>baseApiUrl/moduleName/functionName</code> *   Full path: <code>baseApiUrl</code> + <code>moduleName</code> (where <code>moduleName</code> starts with <code>/</code>, e.g., <code>/custom/endpoint</code>)</p> <p><pre><code>// POST request (HTTP or Tauri if available and 'auto')\nconst userData = { name: 'John Doe', email: 'john@example.com' };\nlet response = await TB.api.request('UserModule', 'createUser', userData); // Defaults to POST\n\nif (response.error === TB.ToolBoxError.none) {\n    console.log('User created:', response.get());\n} else {\n    TB.logger.error('Failed to create user:', response.info.help_text);\n}\n\n// GET request with query parameters from an object\nresponse = await TB.api.request('ProductModule', 'getProduct', { id: 123 }, 'GET');\n// URL: /api/ProductModule/getProduct?id=123\n\n// Full path GET (functionName is query params object)\nresponse = await TB.api.request('/custom/data', { type: 'summary' }, null, 'GET');\n// URL: /api/custom/data?type=summary (if baseApiUrl is /api)\n</code></pre> *   <code>TB.api.fetchHtml(path)</code>: Fetches HTML content, typically for router views. Path is relative to <code>baseFileUrl</code>. <pre><code>const htmlResult = await TB.api.fetchHtml('/about.html'); // Fetches /web/pages/about.html\nif (!htmlResult.startsWith('HTTP error!')) { /* ... */ }\n</code></pre> *   <code>TB.api.AuthHttpPostData(username)</code>: Specific method for validating a session. Calls <code>/validateSession</code>. *   <code>TB.api.logoutServer()</code>: Notifies the backend to invalidate the current user's session token (calls <code>/web/logoutS</code>). *   Events: *   <code>api:networkError</code>: Emitted on fetch network failures. Payload: <code>{ url, error }</code>.</p> </li> </ul>"},{"location":"tbjs/#tbrouter-spa-routing","title":"<code>TB.router</code>: SPA Routing","text":"<p>Manages client-side navigation and view rendering.</p> <ul> <li>Initialization: <code>TB.router.init(rootElement, predefinedRoutes)</code> called by <code>TB.init</code>.<ul> <li><code>rootElement</code>: The DOM element where views will be rendered (from <code>TB.config.appRootId</code>).</li> <li>Automatically navigates to the initial URL (or <code>/index.html</code>).</li> </ul> </li> <li>Navigating: <pre><code>// Navigate to a new path, updating browser history\nTB.router.navigateTo('/products/123'); // Fetches baseFileUrl + /products/123.html\n\n// Navigate and replace current history entry\nTB.router.navigateTo('/profile/settings', true);\n</code></pre><ul> <li>Fetches HTML from <code>TB.config.get('baseFileUrl') + path + '.html'</code> (by default, unless path includes an extension).</li> <li>Handles script loading within new views (external once, inline executed, <code>unsave</code> attribute for fresh execution, <code>global=\"true\"</code> for potential preservation).</li> <li>Updates <code>appRootElement.innerHTML</code> with fetched content.</li> <li>Calls <code>TB.ui.processDynamicContent()</code> on the new content.</li> <li>Handles 404 errors by trying to navigate to <code>/web/assets/404.html</code>.</li> <li>Handles 401 errors by trying to navigate to <code>/web/assets/401.html</code>.</li> </ul> </li> <li>Getting Current Path: <code>TB.router.getCurrentPath()</code></li> <li>Cache Management:<ul> <li><code>TB.router.clearCache(path)</code>: Clears HTML cache for a specific path or all if <code>path</code> is omitted (uses <code>sessionStorage</code> if <code>USE_SESSION_CACHE</code> is true in router.js).</li> <li><code>scriptCache</code> (Set of script <code>src</code> URLs) prevents re-fetching external scripts.</li> </ul> </li> <li>Events:<ul> <li><code>router:beforeNavigation</code>: <code>{ from, to }</code></li> <li><code>router:navigationSuccess</code>: <code>{ path, contentSource }</code> ('cache' or 'fetched')</li> <li><code>router:navigationError</code>: <code>{ path, error }</code></li> <li><code>router:contentProcessed</code>: <code>{ path, element }</code></li> </ul> </li> </ul>"},{"location":"tbjs/#tbcrypto-cryptographic-utilities","title":"<code>TB.crypto</code>: Cryptographic Utilities","text":"<p>Provides functions for various cryptographic operations, including WebAuthn. Relies on browser's Web Crypto API.</p> <ul> <li>Key Management &amp; Signing:<ul> <li><code>TB.crypto.generateAsymmetricKeys()</code>: Generates RSA-OAEP key pair (PEM &amp; Base64).</li> <li><code>TB.crypto.decryptAsymmetric(encryptedBase64Data, privateKeyBase64, convertHex = false)</code>: Decrypts RSA-OAEP encrypted data.</li> <li><code>TB.crypto.signMessage(privateKeyBase64, message)</code>: Signs a message using RSA-PSS.</li> <li><code>TB.crypto.storePrivateKey(privateKeyBase64, username)</code>: Stores private key in <code>localStorage</code>.</li> <li><code>TB.crypto.retrievePrivateKey(username)</code>: Retrieves private key.</li> </ul> </li> <li>Symmetric Encryption/Decryption:<ul> <li><code>TB.crypto.generateSymmetricKey()</code>: Generates an AES-GCM key (Base64 of raw key).</li> <li><code>TB.crypto.decryptSymmetric(encryptedDataB64, password)</code>: Decrypts AES-GCM data (assumes IV is prepended to ciphertext, password used for key derivation via PBKDF2).</li> </ul> </li> <li>WebAuthn (Passkeys):<ul> <li>The Relying Party ID (<code>rpId</code>) is determined from <code>window.location.hostname</code> (or \"localhost\").</li> <li><code>TB.crypto.registerWebAuthnCredential(registrationData, singData)</code>:<ul> <li><code>registrationData</code>: <code>{ challenge, userId, username }</code> from server.</li> <li><code>singData</code>: Additional data (e.g., session token) to associate.</li> <li>Calls <code>navigator.credentials.create()</code>. Returns payload for server verification.</li> </ul> </li> <li><code>TB.crypto.authorizeWebAuthnCredential(rawIdAsBase64, challenge, username)</code>:<ul> <li><code>rawIdAsBase64</code>, <code>challenge</code>, <code>username</code> from server.</li> <li>Calls <code>navigator.credentials.get()</code>. Returns assertion payload for server verification.</li> </ul> </li> </ul> </li> <li>Data Conversions: <code>arrayBufferToBase64</code>, <code>base64ToArrayBuffer</code>, <code>strToBase64</code>, etc.</li> </ul>"},{"location":"tbjs/#tbuser-user-session-authentication","title":"<code>TB.user</code>: User Session &amp; Authentication","text":"<p>Manages user state, authentication flows, and user-specific data. User state is stored under <code>TB.state.get('user')</code>.</p> <ul> <li>Initialization: <code>TB.user.init(forceServerFetch = false)</code>:<ul> <li>Called by <code>TB.init</code>. Loads session, validates with backend, synchronizes user data.</li> </ul> </li> <li>Authentication State: <code>TB.user.isAuthenticated()</code>, <code>TB.user.getUsername()</code>, <code>TB.user.getToken()</code>, etc.</li> <li>Login Methods:<ul> <li><code>async TB.user.signup(username, email, initiationKey, registerAsPersona = false)</code>: Initiates user creation.</li> <li><code>async TB.user.loginWithDeviceKey(username)</code>: Login using locally stored asymmetric key.</li> <li><code>async TB.user.loginWithWebAuthn(username)</code>: Login using WebAuthn (passkey).</li> <li><code>async TB.user.requestMagicLink(username)</code>: Requests a magic link email.</li> <li><code>async TB.user.registerDeviceWithInvitation(username, invitationKey)</code>: Registers a new device.</li> <li><code>async TB.user.registerWebAuthnForCurrentUser(username)</code>: Adds a WebAuthn credential for an authenticated user.</li> </ul> </li> <li>Session Management:<ul> <li><code>async TB.user.checkSessionValidity()</code>: Validates current token with server.</li> <li><code>async TB.user.logout(notifyServer = true)</code>: Clears local session, notifies server.</li> </ul> </li> <li>User-Specific Data: <code>TB.user.getUserData(key)</code>, <code>TB.user.setUserData(keyOrObject, value, syncToServer = false)</code>, <code>async TB.user.syncUserData()</code>, <code>async TB.user.fetchUserData()</code>.</li> <li>Events: <code>user:stateChanged</code>, <code>user:loggedOut</code>.</li> </ul>"},{"location":"tbjs/#tbsse-server-sent-events","title":"<code>TB.sse</code>: Server-Sent Events","text":"<p>Manages connections to Server-Sent Event streams.</p> <ul> <li>Connecting: <code>TB.sse.connect(url, options = {})</code><ul> <li><code>options</code>: <code>{ onOpen, onError, onMessage, listeners: { eventName: handler }, eventSourceOptions }</code>. <pre><code>TB.sse.connect('/api/sse/updates', {\n    listeners: {\n        'user-update': (data) =&gt; TB.state.set('user.profile', data),\n        'new-notification': (data) =&gt; TB.ui.Toast.showInfo(data.message)\n    }\n});\n</code></pre></li> </ul> </li> <li>Disconnecting: <code>TB.sse.disconnect(url)</code>, <code>TB.sse.disconnectAll()</code></li> <li>Getting Connection: <code>TB.sse.getConnection(url)</code></li> <li>Events Emitted: <code>sse:open:&lt;url&gt;</code>, <code>sse:error:&lt;url&gt;</code>, <code>sse:event:&lt;url&gt;:&lt;eventName&gt;</code>, etc.</li> </ul>"},{"location":"tbjs/#tbsw-service-worker-management","title":"<code>TB.sw</code>: Service Worker Management","text":"<p>Handles registration and communication with your application's service worker.</p> <ul> <li>Configuration (<code>TB.config.get('serviceWorker')</code>): <code>enabled</code>, <code>url</code>, <code>scope</code>.</li> <li>Registration: Called automatically by <code>TB.init</code> if enabled. Manual: <code>await TB.sw.register()</code>.</li> <li>Unregistration: <code>await TB.sw.unregister()</code></li> <li>Sending Messages: <code>await TB.sw.sendMessage({ type: 'GET_VERSION' })</code></li> <li>Events Emitted: <code>sw:updateAvailable</code>, <code>sw:contentCached</code>.     <pre><code>TB.events.on('sw:updateAvailable', ({ registration }) =&gt; {\n    if (confirm('New version available. Reload?')) {\n        registration.waiting.postMessage({ type: 'SKIP_WAITING' });\n        // Listen for controllerchange to reload\n        navigator.serviceWorker.addEventListener('controllerchange', () =&gt; window.location.reload());\n    }\n});\n</code></pre></li> </ul>"},{"location":"tbjs/#tbutils-general-utilities","title":"<code>TB.utils</code>: General Utilities","text":"<p>A collection of helper functions.</p> <ul> <li><code>TB.utils.autocomplete(inputElement, arrayOrFunctionSource)</code>: Basic autocomplete (prefer <code>TB.ui.AutocompleteWidget</code>).</li> <li><code>TB.utils.debounce(func, delay)</code></li> <li><code>TB.utils.throttle(func, limit)</code></li> <li><code>TB.utils.uniqueId(prefix = 'id-')</code></li> <li><code>TB.utils.deepClone(obj)</code></li> <li><code>TB.utils.cleanUrl(url)</code>: Basic URL cleaning.</li> </ul>"},{"location":"tbjs/#tbgraphics-3d-graphics-threejs","title":"<code>TB.graphics</code>: 3D Graphics (THREE.js)","text":"<p>Manages a THREE.js scene, typically for background effects.</p> <ul> <li>Initialization: <code>TB.graphics.init(canvasContainerSelector, options = {})</code><ul> <li><code>canvasContainerSelector</code>: CSS selector for the DOM element (e.g., <code>'#threeDScene'</code>).</li> <li><code>options</code>: <code>{ cameraY, cameraZ, sierpinskiDepth, loaderHideDelay }</code>.</li> <li>Typically called if <code>themeSettings.background.type</code> is <code>'3d'</code>.</li> </ul> </li> <li>Control Methods:<ul> <li><code>TB.graphics.dispose()</code>, <code>TB.graphics.pause()</code>, <code>TB.graphics.resume()</code>.</li> <li><code>TB.graphics.updateTheme(themeMode)</code>: Called by <code>TB.ui.theme</code>.</li> <li><code>TB.graphics.setSierpinskiDepth(newDepth)</code>.</li> <li><code>TB.graphics.setAnimationSpeed(x, y, z, factor)</code>.</li> <li><code>TB.graphics.adjustCameraZoom(delta)</code>, <code>TB.graphics.setCameraZoom(absoluteZoomValue)</code>.</li> </ul> </li> <li>Programmed Animation Sequences:<ul> <li><code>TB.graphics.playAnimationSequence(sequenceString, onCompleteCallback, baseSpeedOverride, speedFactorOverride)</code><ul> <li><code>sequenceString</code>: e.g., <code>\"R1+32:P2-14\"</code> (Type, Repeat, Direction, Speed, Complexity).</li> </ul> </li> <li><code>TB.graphics.stopAnimationSequence()</code>.</li> </ul> </li> <li>Events: <code>graphics:initialized</code>, <code>graphics:disposed</code>.</li> </ul>"},{"location":"tbjs/#4-ui-system-tbui","title":"4. UI System (<code>TB.ui.*</code>)","text":""},{"location":"tbjs/#tbuitheme-theming","title":"<code>TB.ui.theme</code>: Theming","text":"<p>Manages light/dark mode and application background.</p> <ul> <li>Initialization: <code>TB.ui.theme.init(themeSettings)</code> called by <code>TB.init</code>.<ul> <li><code>themeSettings</code>: <code>{ defaultPreference ('light'|'dark'|'system'), background: { type, light, dark, placeholder } }</code>.</li> <li><code>background.type</code>: <code>'3d'</code>, <code>'image'</code>, <code>'color'</code>, <code>'none'</code>.</li> <li><code>background.light/dark</code>: <code>{ color: string, image: string|null }</code>.</li> <li><code>background.placeholder</code>: <code>{ image_light, image_dark, displayUntil3DReady }</code>.</li> </ul> </li> <li>Interacting with Theme:<ul> <li><code>TB.ui.theme.setPreference('dark')</code>, <code>TB.ui.theme.togglePreference()</code>.</li> <li><code>TB.ui.theme.getCurrentMode()</code> ('light' or 'dark').</li> <li><code>TB.ui.theme.getPreference()</code> ('light', 'dark', or 'system').</li> </ul> </li> <li>Background Management:<ul> <li>Uses <code>#appBackgroundContainer</code> for image/color and <code>#threeDScene</code> for 3D.</li> </ul> </li> <li>Events: <code>theme:changed</code> (payload: <code>{ mode: 'light' | 'dark' }</code>).</li> </ul>"},{"location":"tbjs/#tbuihtmxintegration-htmx-event-handling","title":"<code>TB.ui.htmxIntegration</code>: HTMX Event Handling","text":"<p>Listens to HTMX events to integrate <code>tbjs</code> functionalities.</p> <ul> <li>Initialization: <code>TB.ui.htmxIntegration.init()</code> is called by <code>TB.init</code>.</li> <li><code>htmx:afterSwap</code>: Calls <code>TB.ui.processDynamicContent</code> on the new HTMX target element.</li> <li><code>htmx:afterRequest</code>:<ul> <li>Inspects XHR response. If JSON, wraps in <code>TB.api.Result</code>, shows toasts for errors.</li> <li>Handles <code>REMOTE</code> render commands.</li> <li>Emits <code>htmx:jsonResponse</code>.</li> </ul> </li> </ul>"},{"location":"tbjs/#tbuiprocessdynamiccontentparentelement-options","title":"<code>TB.ui.processDynamicContent(parentElement, options = {})</code>","text":"<p>Initializes <code>tbjs</code> features/components within newly added DOM content.</p> <ul> <li><code>parentElement</code>: The container of the new content.</li> <li><code>options</code>: <code>{ addScripts (default true), scriptCache }</code>.</li> <li>Actions: Calls <code>window.htmx.process()</code>, handles scripts, calls <code>TB.ui.MarkdownRenderer.renderAllIn()</code>, initializes data-attribute driven components like <code>AutocompleteWidget</code>.</li> </ul>"},{"location":"tbjs/#ui-components","title":"UI Components","text":""},{"location":"tbjs/#tbuimodal","title":"<code>TB.ui.Modal</code>","text":"<p>Displays modal dialogs.</p> <ul> <li>Static Usage: <code>TB.ui.Modal.show({ title, content, maxWidth, buttons, onOpen, onClose, ... })</code><ul> <li><code>buttons</code>: <code>[{ text, action: (modalInstance) =&gt; {}, variant, className }]</code></li> </ul> </li> <li>Styling: Uses Tailwind CSS, \"milk glass\" effect.</li> <li> <p>Events: <code>modal:shown</code>, <code>modal:closed</code>.</p> </li> <li> <p>Static Usage: `TB.ui.Modal.confirm({         title,         content,         confirmButtonText = 'OK',         cancelButtonText = 'Cancel',         confirmButtonVariant = 'primary',         cancelButtonVariant = 'secondary',         confirmButtonClass = '',         cancelButtonClass = '',         hideCancelButton = false,         resolveOnClose = false,         ...extraModalOptions // Collects any other options passed to confirm</p> </li> </ul>"},{"location":"tbjs/#tbuitoast","title":"<code>TB.ui.Toast</code>","text":"<p>Displays \"speech balloon\" style toast notifications.</p> <ul> <li>Static Usage:<ul> <li><code>TB.ui.Toast.showInfo(message, options)</code></li> <li><code>TB.ui.Toast.showSuccess(message, options)</code></li> <li><code>TB.ui.Toast.showWarning(message, options)</code></li> <li><code>TB.ui.Toast.showError(message, options)</code></li> </ul> </li> <li>Options: <code>{ title, duration, position, actions, icon, closable, showDotOnHide, dotDuration }</code>.</li> <li>Events: <code>toast:shown</code>, <code>toast:hidden</code>.</li> </ul>"},{"location":"tbjs/#tbuiloader","title":"<code>TB.ui.Loader</code>","text":"<p>Displays a loading indicator.</p> <ul> <li>Static Usage (Global Page Loader):<ul> <li><code>const loaderElement = TB.ui.Loader.show('Processing...');</code></li> <li><code>TB.ui.Loader.hide(loaderElement);</code> (or <code>TB.ui.Loader.hide()</code> for default).</li> </ul> </li> <li>Options: <code>{ text, fullscreen, customSpinnerHtml }</code>.</li> </ul>"},{"location":"tbjs/#tbuibutton","title":"<code>TB.ui.Button</code>","text":"<p>Creates styled button elements programmatically.</p> <ul> <li>Static Usage: <code>const myButtonElement = TB.ui.Button.create(text, onClickCallback, options)</code></li> <li>Options: <code>{ variant, size, iconLeft, iconRight, type, disabled, isLoading, ... }</code>.</li> <li>Instance Methods: <code>setLoading(true)</code>, <code>setDisabled(true)</code>.</li> </ul>"},{"location":"tbjs/#tbuidarkmodetoggle","title":"<code>TB.ui.DarkModeToggle</code>","text":"<p>UI component for switching themes, syncing with <code>TB.ui.theme</code>.</p> <ul> <li>HTML (Example): <pre><code>&lt;div id=\"darkModeToggleContainer\"&gt;\n    &lt;label for=\"darkModeSwitch\"&gt;&lt;span class=\"tb-toggle-icon material-symbols-outlined\"&gt;light_mode&lt;/span&gt;&lt;/label&gt;\n    &lt;input type=\"checkbox\" id=\"darkModeSwitch\" class=\"tb-sr-only\"&gt;\n&lt;/div&gt;\n</code></pre></li> <li>Initialization: <code>TB.ui.DarkModeToggle.init({ containerSelector, iconSelector, checkboxSelector, ... })</code>. Default init uses common selectors.</li> <li>Updates icon and checkbox based on <code>theme:changed</code> event.</li> </ul>"},{"location":"tbjs/#tbuicookiebanner","title":"<code>TB.ui.CookieBanner</code>","text":"<p>Displays a cookie consent banner and settings modal.</p> <ul> <li>Static Usage: <code>TB.ui.CookieBanner.show({ title, message, termsLink, onConsent, ... })</code></li> <li><code>onConsent</code> callback receives <code>{ essential, preferences, analytics, source }</code>.</li> <li>Methods: <code>CookieBanner.getConsent()</code>, <code>CookieBanner.clearConsent()</code>.</li> <li>Events: <code>cookieConsent:updated</code>, <code>cookieBanner:shown</code>/<code>hidden</code>.</li> </ul>"},{"location":"tbjs/#tbuimarkdownrenderer","title":"<code>TB.ui.MarkdownRenderer</code>","text":"<p>Renders Markdown to HTML, with optional <code>highlight.js</code> syntax highlighting.</p> <ul> <li>Dependencies: <code>marked</code>, <code>highlight.js</code>, <code>marked-highlight</code> (global or loaded).</li> <li>Methods:<ul> <li><code>TB.ui.MarkdownRenderer.render(markdownString)</code></li> <li><code>TB.ui.MarkdownRenderer.renderAllIn(parentElement)</code> (for elements with <code>.markdown</code> class)</li> <li><code>TB.ui.MarkdownRenderer.renderElement(element)</code></li> </ul> </li> <li>Adds Tailwind Prose classes (<code>prose dark:prose-invert</code>) for styling.</li> </ul>"},{"location":"tbjs/#tbuinavmenu","title":"<code>TB.ui.NavMenu</code>","text":"<p>A slide-in (or modal-style) navigation menu.</p> <ul> <li>HTML Trigger (Example): <pre><code>&lt;div id=\"Nav-Main\"&gt; &lt;!-- Menu is appended here --&gt;\n    &lt;div id=\"links\"&gt;&lt;span class=\"material-symbols-outlined\"&gt;menu&lt;/span&gt;&lt;/div&gt;\n&lt;/div&gt;\n</code></pre></li> <li>Initialization: <code>TB.ui.NavMenu.init({ triggerSelector, menuContentHtml, ... })</code>.</li> <li>Events: <code>navMenu:opened</code>, <code>navMenu:closed</code>.</li> </ul>"},{"location":"tbjs/#tbuiautocompletewidget","title":"<code>TB.ui.AutocompleteWidget</code>","text":"<p>Provides autocomplete suggestions for input fields.</p> <ul> <li>HTML (Declarative): <pre><code>&lt;input type=\"text\" data-tb-autocomplete data-tb-autocomplete-source='[\"Apple\", \"Banana\"]'&gt;\n&lt;!-- Or data-tb-autocomplete-source=\"myGlobalFunctionName\" --&gt;\n</code></pre></li> <li>Initialization:<ul> <li>Automatic: <code>TB.ui.AutocompleteWidget.initAll()</code> (called by <code>processDynamicContent</code>).</li> <li>Manual: <code>new TB.ui.AutocompleteWidget(inputEl, { source, minLength, onSelect, ... })</code>.</li> </ul> </li> <li>Features: Keyboard navigation, ARIA attributes.</li> </ul>"},{"location":"tbjs/#5-styling-with-tailwind-css","title":"5. Styling with Tailwind CSS","text":"<p><code>tbjs</code> components are primarily styled using Tailwind CSS utility classes.</p>"},{"location":"tbjs/#prefixing-and-css-variables","title":"Prefixing and CSS Variables","text":"<ul> <li>Prefix: <code>tbjs</code>'s internal Tailwind configuration uses a <code>tb-</code> prefix (e.g., <code>tb-bg-primary-500</code>, <code>tb-text-lg</code>). This is crucial to avoid conflicts if your main application also uses Tailwind without a prefix or with a different one.</li> <li>Main CSS (<code>tbjs.css</code> or <code>tbjs-main.css</code>):<ul> <li>Imports Tailwind utilities generated with the <code>tb-</code> prefix.</li> <li>Defines CSS custom properties (variables) for theming (e.g., <code>--tb-color-primary-500</code>, <code>--theme-bg</code>, <code>--glass-bg</code>). These are used by the prefixed Tailwind classes.</li> <li>Includes light and dark theme definitions typically applied to <code>body[data-theme=\"dark\"]</code> or <code>body.dark-mode</code>.</li> <li>Provides base styles and some component-specific styles hard to achieve with utilities alone (e.g., toast speech balloon tail).</li> </ul> </li> <li>Customization:<ul> <li>Applications can override the CSS variables defined in <code>tbjs.css</code> in their own stylesheets to customize the look and feel.</li> <li>For deeper Tailwind customization (new colors, variants specific to <code>tbjs</code>), you would edit <code>tbjs/tailwind.config.js</code> and rebuild <code>tbjs</code>.</li> </ul> </li> </ul>"},{"location":"tbjs/#using-tbjs-tailwind-config-in-your-project","title":"Using <code>tbjs</code> Tailwind Config in Your Project","text":"<p>If your project also uses Tailwind CSS, you have a few options:</p> <ol> <li> <p>Separate Builds (Recommended for Isolation):</p> <ul> <li>Build <code>tbjs.css</code> using its own Tailwind configuration (with the <code>tb-</code> prefix).</li> <li>Build your application's CSS using its Tailwind configuration.</li> <li>Include both CSS files in your HTML. The <code>tb-</code> prefix prevents most conflicts.</li> </ul> </li> <li> <p>Merging Configurations (Advanced):     If you want a single Tailwind build process, you might try to merge configurations. This can be complex due to prefixing and potential conflicts.</p> <ul> <li>You would need to ensure your main <code>tailwind.config.js</code> includes the <code>content</code> paths for <code>tbjs</code> source files.</li> <li>You'd also need to decide how to handle the <code>tb-</code> prefix. If your app doesn't use a prefix, <code>tbjs</code> components might not be styled correctly unless you manually adapt their classes or adjust the <code>tbjs</code> source.</li> <li>A simpler merge might involve including <code>tbjs</code>'s Tailwind plugin or preset if it were structured that way, but this is not the default.</li> </ul> <p>Example (Conceptual - requires careful setup): <pre><code>// your-app/tailwind.config.js\n// const tbjsTailwindConfig = require('path/to/tbjs/tailwind.config.js'); // If CJS\n\nexport default {\n  content: [\n    './src/**/*.{html,js,svelte,vue,jsx,tsx}', // Your app's content\n    './node_modules/tbjs/src/**/*.{html,js}', // Or path to tbjs source\n  ],\n  // If your app uses a prefix, it might conflict or work alongside tb-\n  // prefix: 'app-',\n  theme: {\n    extend: {\n      // You might try to extend with tbjs colors if they are defined without prefix in its config\n      // This part is tricky due to the 'tb-' prefix baked into tbjs's own build\n    },\n  },\n  plugins: [],\n};\n</code></pre> Generally, keeping <code>tbjs.css</code> separate with its <code>tb-</code> prefix is the most straightforward way to avoid styling conflicts.</p> </li> </ol>"},{"location":"tbjs/#6-advanced-topics","title":"6. Advanced Topics","text":""},{"location":"tbjs/#tauri-integration","title":"Tauri Integration","text":"<ul> <li>Environment Check: Use <code>TB.env.isTauri()</code> to execute Tauri-specific code.</li> <li>API Calls: <code>TB.api.request()</code> automatically uses <code>window.__TAURI__.invoke</code> if <code>useTauri</code> is <code>'auto'</code> (default) or <code>'force'</code> and the environment is Tauri.<ul> <li>The Tauri command invoked is typically <code>moduleName.functionName</code> (e.g., <code>MyRustModule.my_function</code>). <pre><code>if (TB.env.isTauri()) {\n    const result = await TB.api.request('my_rust_command', 'sub_command_or_payload_key', { data: 'payload' });\n    // Effective Tauri invoke: window.__TAURI__.invoke('my_rust_command.sub_command_or_payload_key', { data: 'payload' });\n}\n</code></pre></li> </ul> </li> <li>Platform-Specific Features: The <code>initializeApp</code> function shows a pattern for loading Tauri-specific listeners or UI adjustments.</li> </ul>"},{"location":"tbjs/#working-with-3d-graphics","title":"Working with 3D Graphics","text":"<ul> <li>The <code>TB.graphics</code> module manages a THREE.js scene, typically for background effects.</li> <li>Integration with Theme: If <code>themeSettings.background.type</code> is <code>'3d'</code>, <code>TB.ui.theme</code> will initialize <code>TB.graphics</code> (targeting <code>#threeDScene</code>) and call <code>TB.graphics.updateTheme()</code> on light/dark mode changes.</li> <li>Manual Control: <pre><code>TB.graphics.setSierpinskiDepth(3);\nTB.graphics.playAnimationSequence(\"R2+52:P1-31\", () =&gt; console.log(\"3D Animation done!\"));\n// Mouse/touch drag for interaction is usually enabled by default.\n</code></pre></li> </ul>"},{"location":"tbjs/#7-example-login-flow-walkthrough","title":"7. Example: Login Flow Walkthrough","text":"<p>This conceptual example (based on a typical <code>login.js</code> implementation with <code>tbjs</code>) demonstrates how various modules work together:</p> <ol> <li> <p>Initialization (e.g., in a <code>setupLogin</code> function called when the login page loads):</p> <ul> <li>Wait for <code>tbjs:initialized</code> or check <code>TB.isInitialized</code>.</li> <li>Optionally, play an initial graphics animation: <code>TB.graphics.playAnimationSequence(\"Z0+12\")</code>.</li> <li>Check session validity: <code>TB.user.checkSessionValidity()</code>. If valid, show a toast and offer navigation to a dashboard.</li> </ul> </li> <li> <p>Form Submission (e.g., on login button click):</p> <ul> <li>Prevent default form submission.</li> <li>Get username from input. Validate locally (show info/toast on error).</li> <li>Play a \"login attempt\" graphics animation: <code>TB.graphics.playAnimationSequence(\"R1+11:P1-11\")</code>.</li> <li>Show a global loader: <code>TB.ui.Loader.show('Attempting login...')</code>.</li> <li>Authentication Logic (Conditional):<ul> <li>If user opts for WebAuthn/Passkey: <code>await TB.user.loginWithWebAuthn(username)</code>.</li> <li>Else (e.g., device key): <code>await TB.user.loginWithDeviceKey(username)</code>.<ul> <li>If <code>loginWithDeviceKey</code> fails due to no key: Show a sticky error toast with actions:<ul> <li>\"Try Passkey/WebAuthn\": Calls <code>TB.user.loginWithWebAuthn()</code>.</li> <li>\"Register with Invitation\": Prompts for key, calls <code>TB.user.registerDeviceWithInvitation()</code>.</li> <li>\"Send Magic Link\": Calls <code>TB.user.requestMagicLink()</code>.</li> <li>Each action would have its own loader management and graphics animations.</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Result Handling:</p> <ul> <li>Based on <code>result.success</code> from <code>TB.user</code> login methods:<ul> <li>Success:<ul> <li>Show success toast: <code>TB.ui.Toast.showSuccess('Login successful!')</code>.</li> <li>Play success animation: <code>TB.graphics.playAnimationSequence(\"Z1+32:R0+50\")</code>.</li> <li>Navigate: <code>TB.router.navigateTo('/dashboard')</code>.</li> </ul> </li> <li>Failure:<ul> <li>Show error toast: <code>TB.ui.Toast.showError(result.message)</code>.</li> <li>Play failure animation: <code>TB.graphics.playAnimationSequence(\"P2-42\")</code>.</li> </ul> </li> </ul> </li> <li>Use <code>TB.logger</code> for detailed console logging throughout the process.</li> <li>Hide loader (<code>TB.ui.Loader.hide()</code>) and stop animations (<code>TB.graphics.stopAnimationSequence()</code>) in a <code>finally</code> block or after completion.</li> </ul> </li> </ol> <p>This flow showcases: *   Event-driven UI: Graphics and toasts respond to login states. *   Module Orchestration: <code>TB.user</code>, <code>TB.graphics</code>, <code>TB.ui.Toast</code>, <code>TB.ui.Loader</code>, <code>TB.router</code>, <code>TB.logger</code> working in concert. *   User Feedback: Clear messages and visual cues for different scenarios.</p>"},{"location":"tbjs/#8-building-tbjs-for-developers","title":"8. Building <code>tbjs</code> (For Developers)","text":"<p>If you are modifying the <code>tbjs</code> framework itself or need to build it from source:</p> <ol> <li>Prerequisites:<ul> <li>Node.js and npm (or yarn) installed.</li> </ul> </li> <li>Install Dependencies:     Navigate to the <code>tbjs</code> root directory in your terminal and run:     <pre><code>npm install\n# or\n# yarn install\n</code></pre></li> <li>Build Scripts (examples from a typical <code>package.json</code>):<ul> <li>Production Build: <pre><code>npm run build\n</code></pre>     This usually creates optimized, minified files in a <code>dist/</code> directory (e.g., <code>dist/tbjs.js</code> and <code>dist/tbjs.css</code>). The build process uses Webpack, configured in <code>webpack.config.js</code>.</li> <li>Development Watch Mode: <pre><code>npm run watch\n# or npm run dev\n</code></pre>     This watches source files for changes and automatically rebuilds, often in a non-minified format for easier debugging.</li> <li>Linting: <pre><code>npm run lint\n</code></pre>     Checks the JavaScript code for style consistency and potential errors using a linter like ESLint.</li> </ul> </li> </ol>"},{"location":"toolboxv2/","title":"toolboxv2 API Reference","text":"<p>This section provides an API reference for key components directly available from the <code>toolboxv2</code> package.</p>"},{"location":"toolboxv2/#core-application-tooling","title":"Core Application &amp; Tooling","text":""},{"location":"toolboxv2/#toolboxv2.AppType","title":"<code>toolboxv2.AppType</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class AppType:\n    prefix: str\n    id: str\n    globals: dict[str, Any] = {\"root\": dict, }\n    locals: dict[str, Any] = {\"user\": {'app': \"self\"}, }\n\n    local_test: bool = False\n    start_dir: str\n    data_dir: str\n    config_dir: str\n    info_dir: str\n\n    logger: logging.Logger\n    logging_filename: str\n\n    api_allowed_mods_list: list[str] = []\n\n    version: str\n    loop: asyncio.AbstractEventLoop\n\n    keys: dict[str, str] = {\n        \"MACRO\": \"macro~~~~:\",\n        \"MACRO_C\": \"m_color~~:\",\n        \"HELPER\": \"helper~~~:\",\n        \"debug\": \"debug~~~~:\",\n        \"id\": \"name-spa~:\",\n        \"st-load\": \"mute~load:\",\n        \"comm-his\": \"comm-his~:\",\n        \"develop-mode\": \"dev~mode~:\",\n        \"provider::\": \"provider::\",\n    }\n\n    defaults: dict[str, (bool or dict or dict[str, dict[str, str]] or str or list[str] or list[list]) | None] = {\n        \"MACRO\": list[str],\n        \"MACRO_C\": dict,\n        \"HELPER\": dict,\n        \"debug\": str,\n        \"id\": str,\n        \"st-load\": False,\n        \"comm-his\": list[list],\n        \"develop-mode\": bool,\n    }\n\n    cluster_manager: ClusterManager\n    root_blob_storage: BlobStorage\n    config_fh: FileHandler\n    _debug: bool\n    flows: dict[str, Callable]\n    dev_modi: bool\n    functions: dict[str, Any]\n    modules: dict[str, Any]\n\n    interface_type: ToolBoxInterfaces\n    REFIX: str\n\n    alive: bool\n    called_exit: tuple[bool, float]\n    args_sto: AppArgs\n    system_flag = None\n    session = None\n    appdata = None\n    exit_tasks = []\n\n    enable_profiling: bool = False\n    sto = None\n\n    def __init__(self, prefix: None | str= None, args: AppArgs | None = None):\n        self.args_sto = args\n        self.prefix = prefix\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def exit_main(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def hide_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def show_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def disconnect(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def set_logger(self, debug=False):\n        \"\"\"proxi attr\"\"\"\n\n    @property\n    def debug(self):\n        \"\"\"proxi attr\"\"\"\n        return self._debug\n\n    def debug_rains(self, e):\n        \"\"\"proxi attr\"\"\"\n\n    def set_flows(self, r):\n        \"\"\"proxi attr\"\"\"\n\n    def run_flows(self, name, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def rrun_flows(self, name, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def idle(self):\n        import time\n        self.print(\"idle\")\n        try:\n            while self.alive:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            pass\n        self.print(\"idle done\")\n\n    async def a_idle(self):\n        self.print(\"a idle\")\n        try:\n            if hasattr(self, 'daemon_app'):\n                self.print(\"serving daemon\")\n                await self.daemon_app.connect(self)\n            else:\n                self.print(\"serving default\")\n                while self.alive:\n                    await asyncio.sleep(1)\n        except KeyboardInterrupt:\n            pass\n        self.print(\"a idle done\")\n\n    @debug.setter\n    def debug(self, value):\n        \"\"\"proxi attr\"\"\"\n\n    def _coppy_mod(self, content, new_mod_dir, mod_name, file_type='py'):\n        \"\"\"proxi attr\"\"\"\n\n    def _pre_lib_mod(self, mod_name, path_to=\"./runtime\", file_type='py'):\n        \"\"\"proxi attr\"\"\"\n\n    def _copy_load(self, mod_name, file_type='py', **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True):\n        \"\"\"proxi attr\"\"\"\n\n    def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n        \"\"\"proxi attr\"\"\"\n\n    def save_initialized_module(self, tools_class, spec):\n        \"\"\"proxi attr\"\"\"\n\n    def mod_online(self, mod_name, installed=False):\n        \"\"\"proxi attr\"\"\"\n\n    def _get_function(self,\n                      name: Enum or None,\n                      state: bool = True,\n                      specification: str = \"app\",\n                      metadata=False, as_str: tuple or None = None, r=0):\n        \"\"\"proxi attr\"\"\"\n\n    def save_exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def load_mod(self, mod_name: str, mlm='I', **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    async def init_module(self, modular):\n        return await self.load_mod(modular)\n\n    async def load_all_mods_in_file(self, working_dir=\"mods\"):\n        \"\"\"proxi attr\"\"\"\n\n    def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\"):\n        \"\"\"proxi attr\"\"\"\n\n    def remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            self.remove_mod(mod, delete=delete)\n\n    async def a_remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            await self.a_remove_mod(mod, delete=delete)\n\n    def print_ok(self):\n        \"\"\"proxi attr\"\"\"\n        self.logger.info(\"OK\")\n\n    def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n        \"\"\"proxi attr\"\"\"\n\n    def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None):\n        \"\"\"proxi attr\"\"\"\n\n    def remove_mod(self, mod_name, spec='app', delete=True):\n        \"\"\"proxi attr\"\"\"\n\n    async def a_remove_mod(self, mod_name, spec='app', delete=True):\n        \"\"\"proxi attr\"\"\"\n\n    def exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def web_context(self) -&gt; str:\n        \"\"\"returns the build index ( toolbox web component )\"\"\"\n\n    async def a_exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def save_load(self, modname, spec='app'):\n        \"\"\"proxi attr\"\"\"\n\n    def get_function(self, name: Enum or tuple, **kwargs):\n        \"\"\"\n        Kwargs for _get_function\n            metadata:: return the registered function dictionary\n                stateless: (function_data, None), 0\n                stateful: (function_data, higher_order_function), 0\n            state::boolean\n                specification::str default app\n        \"\"\"\n\n    def run_a_from_sync(self, function, *args):\n        \"\"\"\n        run a async fuction\n        \"\"\"\n\n    def run_bg_task_advanced(self, task, *args, **kwargs):\n        \"\"\"\n        proxi attr\n        \"\"\"\n\n    def wait_for_bg_tasks(self, timeout=None):\n        \"\"\"\n        proxi attr\n        \"\"\"\n\n    def run_bg_task(self, task):\n        \"\"\"\n                run a async fuction\n                \"\"\"\n    def run_function(self, mod_function_name: Enum or tuple,\n                     tb_run_function_with_state=True,\n                     tb_run_with_specification='app',\n                     args_=None,\n                     kwargs_=None,\n                     *args,\n                     **kwargs) -&gt; Result:\n\n        \"\"\"proxi attr\"\"\"\n\n    async def a_run_function(self, mod_function_name: Enum or tuple,\n                             tb_run_function_with_state=True,\n                             tb_run_with_specification='app',\n                             args_=None,\n                             kwargs_=None,\n                             *args,\n                             **kwargs) -&gt; Result:\n\n        \"\"\"proxi attr\"\"\"\n\n    def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n        \"\"\"\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        proxi attr\n        \"\"\"\n\n    async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict):\n        \"\"\"\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        proxi attr\n        \"\"\"\n\n    async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None, method=\"GET\",\n                       args_=None,\n                       kwargs_=None,\n                       *args, **kwargs):\n        \"\"\"run a function remote via http / https\"\"\"\n\n    def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n                get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                kwargs_=None,\n                *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                        backwords_compability_variabel_string_holder=None,\n                        get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                        kwargs_=None,\n                        *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def print(text, *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def sprint(text, *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    # ----------------------------------------------------------------\n    # Decorators for the toolbox\n\n    def _register_function(self, module_name, func_name, data):\n        \"\"\"proxi attr\"\"\"\n\n    def _create_decorator(self, type_: str,\n                          name: str = \"\",\n                          mod_name: str = \"\",\n                          level: int = -1,\n                          restrict_in_virtual_mode: bool = False,\n                          api: bool = False,\n                          helper: str = \"\",\n                          version: str or None = None,\n                          initial=False,\n                          exit_f=False,\n                          test=True,\n                          samples=None,\n                          state=None,\n                          pre_compute=None,\n                          post_compute=None,\n                          memory_cache=False,\n                          file_cache=False,\n                          row=False,\n                          request_as_kwarg=False,\n                          memory_cache_max_size=100,\n                          memory_cache_ttl=300):\n        \"\"\"proxi attr\"\"\"\n\n        # data = {\n        #     \"type\": type_,\n        #     \"module_name\": module_name,\n        #     \"func_name\": func_name,\n        #     \"level\": level,\n        #     \"restrict_in_virtual_mode\": restrict_in_virtual_mode,\n        #     \"func\": func,\n        #     \"api\": api,\n        #     \"helper\": helper,\n        #     \"version\": version,\n        #     \"initial\": initial,\n        #     \"exit_f\": exit_f,\n        #     \"__module__\": func.__module__,\n        #     \"signature\": sig,\n        #     \"params\": params,\n        #     \"state\": (\n        #         False if len(params) == 0 else params[0] in ['self', 'state', 'app']) if state is None else state,\n        #     \"do_test\": test,\n        #     \"samples\": samples,\n        #     \"request_as_kwarg\": request_as_kwarg,\n\n    def tb(self, name=None,\n           mod_name: str = \"\",\n           helper: str = \"\",\n           version: str or None = None,\n           test: bool = True,\n           restrict_in_virtual_mode: bool = False,\n           api: bool = False,\n           initial: bool = False,\n           exit_f: bool = False,\n           test_only: bool = False,\n           memory_cache: bool = False,\n           file_cache: bool = False,\n           row=False,\n           request_as_kwarg: bool = False,\n           state: bool or None = None,\n           level: int = 0,\n           memory_cache_max_size: int = 100,\n           memory_cache_ttl: int = 300,\n           samples: list or dict or None = None,\n           interface: ToolBoxInterfaces or None or str = None,\n           pre_compute=None,\n           post_compute=None,\n           api_methods=None,\n           ):\n        \"\"\"\n    A decorator for registering and configuring functions within a module.\n\n    This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\n    Args:\n        name (str, optional): The name to register the function under. Defaults to the function's own name.\n        mod_name (str, optional): The name of the module the function belongs to.\n        helper (str, optional): A helper string providing additional information about the function.\n        version (str or None, optional): The version of the function or module.\n        test (bool, optional): Flag to indicate if the function is for testing purposes.\n        restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n        api (bool, optional): Flag to indicate if the function is part of an API.\n        initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n        exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n        test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n        memory_cache (bool, optional): Flag to enable memory caching for the function.\n        request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n        file_cache (bool, optional): Flag to enable file caching for the function.\n        row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n        state (bool or None, optional): Flag to indicate if the function maintains state.\n        level (int, optional): The level of the function, used for prioritization or categorization.\n        memory_cache_max_size (int, optional): Maximum size of the memory cache.\n        memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n        samples (list or dict or None, optional): Samples or examples of function usage.\n        interface (str, optional): The interface type for the function.\n        pre_compute (callable, optional): A function to be called before the main function.\n        post_compute (callable, optional): A function to be called after the main function.\n        api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\n    Returns:\n        function: The decorated function with additional processing and registration capabilities.\n    \"\"\"\n        if interface is None:\n            interface = \"tb\"\n        if test_only and 'test' not in self.id:\n            return lambda *args, **kwargs: args\n        return self._create_decorator(interface,\n                                      name,\n                                      mod_name,\n                                      level=level,\n                                      restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                      helper=helper,\n                                      api=api,\n                                      version=version,\n                                      initial=initial,\n                                      exit_f=exit_f,\n                                      test=test,\n                                      samples=samples,\n                                      state=state,\n                                      pre_compute=pre_compute,\n                                      post_compute=post_compute,\n                                      memory_cache=memory_cache,\n                                      file_cache=file_cache,\n                                      row=row,\n                                      request_as_kwarg=request_as_kwarg,\n                                      memory_cache_max_size=memory_cache_max_size,\n                                      memory_cache_ttl=memory_cache_ttl)\n\n    def print_functions(self, name=None):\n\n\n        if not self.functions:\n            print(\"Nothing to see\")\n            return\n\n        def helper(_functions):\n            for func_name, data in _functions.items():\n                if not isinstance(data, dict):\n                    continue\n\n                func_type = data.get('type', 'Unknown')\n                func_level = 'r' if data['level'] == -1 else data['level']\n                api_status = 'Api' if data.get('api', False) else 'Non-Api'\n\n                print(f\"  Function: {func_name}{data.get('signature', '()')}; \"\n                      f\"Type: {func_type}, Level: {func_level}, {api_status}\")\n\n        if name is not None:\n            functions = self.functions.get(name)\n            if functions is not None:\n                print(f\"\\nModule: {name}; Type: {functions.get('app_instance_type', 'Unknown')}\")\n                helper(functions)\n                return\n        for module, functions in self.functions.items():\n            print(f\"\\nModule: {module}; Type: {functions.get('app_instance_type', 'Unknown')}\")\n            helper(functions)\n\n    def save_autocompletion_dict(self):\n        \"\"\"proxi attr\"\"\"\n\n    def get_autocompletion_dict(self):\n        \"\"\"proxi attr\"\"\"\n\n    def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n        \"\"\"proxi attr\"\"\"\n\n    def save_registry_as_enums(self, directory: str, filename: str):\n        \"\"\"proxi attr\"\"\"\n\n    async def execute_all_functions_(self, m_query='', f_query=''):\n        print(\"Executing all functions\")\n        from ..extras import generate_test_cases\n        all_data = {\n            \"modular_run\": 0,\n            \"modular_fatal_error\": 0,\n            \"errors\": 0,\n            \"modular_sug\": 0,\n            \"coverage\": [],\n            \"total_coverage\": {},\n        }\n        items = list(self.functions.items()).copy()\n        for module_name, functions in items:\n            infos = {\n                \"functions_run\": 0,\n                \"functions_fatal_error\": 0,\n                \"error\": 0,\n                \"functions_sug\": 0,\n                'calls': {},\n                'callse': {},\n                \"coverage\": [0, 0],\n            }\n            all_data['modular_run'] += 1\n            if not module_name.startswith(m_query):\n                all_data['modular_sug'] += 1\n                continue\n\n            with Spinner(message=f\"In {module_name}| \"):\n                f_items = list(functions.items()).copy()\n                for function_name, function_data in f_items:\n                    if not isinstance(function_data, dict):\n                        continue\n                    if not function_name.startswith(f_query):\n                        continue\n                    test: list = function_data.get('do_test')\n                    # print(test, module_name, function_name, function_data)\n                    infos[\"coverage\"][0] += 1\n                    if test is False:\n                        continue\n\n                    with Spinner(message=f\"\\t\\t\\t\\t\\t\\tfuction {function_name}...\"):\n                        params: list = function_data.get('params')\n                        sig: signature = function_data.get('signature')\n                        state: bool = function_data.get('state')\n                        samples: bool = function_data.get('samples')\n\n                        test_kwargs_list = [{}]\n\n                        if params is not None:\n                            test_kwargs_list = samples if samples is not None else generate_test_cases(sig=sig)\n                            # print(test_kwargs)\n                            # print(test_kwargs[0])\n                            # test_kwargs = test_kwargs_list[0]\n                        # print(module_name, function_name, test_kwargs_list)\n                        infos[\"coverage\"][1] += 1\n                        for test_kwargs in test_kwargs_list:\n                            try:\n                                # print(f\"test Running {state=} |{module_name}.{function_name}\")\n                                result = await self.a_run_function((module_name, function_name),\n                                                                   tb_run_function_with_state=state,\n                                                                   **test_kwargs)\n                                if not isinstance(result, Result):\n                                    result = Result.ok(result)\n                                if result.info.exec_code == 0:\n                                    infos['calls'][function_name] = [test_kwargs, str(result)]\n                                    infos['functions_sug'] += 1\n                                else:\n                                    infos['functions_sug'] += 1\n                                    infos['error'] += 1\n                                    infos['callse'][function_name] = [test_kwargs, str(result)]\n                            except Exception as e:\n                                infos['functions_fatal_error'] += 1\n                                infos['callse'][function_name] = [test_kwargs, str(e)]\n                            finally:\n                                infos['functions_run'] += 1\n\n                if infos['functions_run'] == infos['functions_sug']:\n                    all_data['modular_sug'] += 1\n                else:\n                    all_data['modular_fatal_error'] += 1\n                if infos['error'] &gt; 0:\n                    all_data['errors'] += infos['error']\n\n                all_data[module_name] = infos\n                if infos['coverage'][0] == 0:\n                    c = 0\n                else:\n                    c = infos['coverage'][1] / infos['coverage'][0]\n                all_data[\"coverage\"].append(f\"{module_name}:{c:.2f}\\n\")\n        total_coverage = sum([float(t.split(\":\")[-1]) for t in all_data[\"coverage\"]]) / len(all_data[\"coverage\"])\n        print(\n            f\"\\n{all_data['modular_run']=}\\n{all_data['modular_sug']=}\\n{all_data['modular_fatal_error']=}\\n{total_coverage=}\")\n        d = analyze_data(all_data)\n        return Result.ok(data=all_data, data_info=d)\n\n    @staticmethod\n    def calculate_complexity(filename_or_code):\n        from radon.complexity import cc_rank, cc_visit\n        if os.path.exists(filename_or_code):\n            with open(filename_or_code) as file:\n                code = file.read()\n        else:\n            code = filename_or_code\n\n        # Calculate and print Cyclomatic Complexity\n        complexity_results = cc_visit(code)\n        i = -1\n        avg_complexity = 0\n        for block in complexity_results:\n            complexity = block.complexity\n            i += 1\n            print(f\"block: {block.name} {i} Class/Fuction/Methode : {block.letter}\")\n            print(f\"    fullname: {block.fullname}\")\n            print(f\"    Cyclomatic Complexity: {complexity}\")\n            # Optional: Get complexity rank\n            avg_complexity += complexity\n            rank = cc_rank(complexity)\n            print(f\"    Complexity Rank: {rank}\")\n            # print(f\"    lineno: {block.lineno}\")\n            print(f\"    endline: {block.endline}\")\n            print(f\"    col_offset: {block.col_offset}\\n\")\n        if i &lt;= 0:\n            i += 2\n        avg_complexity = avg_complexity / i\n        print(f\"\\nAVG Complexity: {avg_complexity:.2f}\")\n        print(f\"Total Rank: {cc_rank(int(avg_complexity + i // 10))}\")\n\n    async def execute_function_test(self, module_name: str, function_name: str,\n                                    function_data: dict, test_kwargs: dict,\n                                    profiler: cProfile.Profile) -&gt; tuple[bool, str, dict, float]:\n        start_time = time.time()\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            try:\n                result = await self.a_run_function(\n                    (module_name, function_name),\n                    tb_run_function_with_state=function_data.get('state'),\n                    **test_kwargs\n                )\n\n                if not isinstance(result, Result):\n                    result = Result.ok(result)\n\n                success = result.info.exec_code == 0\n                execution_time = time.time() - start_time\n                return success, str(result), test_kwargs, execution_time\n            except Exception as e:\n                execution_time = time.time() - start_time\n                return False, str(e), test_kwargs, execution_time\n\n    async def process_function(self, module_name: str, function_name: str,\n                               function_data: dict, profiler: cProfile.Profile) -&gt; tuple[str, ModuleInfo]:\n        start_time = time.time()\n        info = ModuleInfo()\n\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            if not isinstance(function_data, dict):\n                return function_name, info\n\n            test = function_data.get('do_test')\n            info.coverage[0] += 1\n\n            if test is False:\n                return function_name, info\n\n            params = function_data.get('params')\n            sig = function_data.get('signature')\n            samples = function_data.get('samples')\n\n            test_kwargs_list = [{}] if params is None else (\n                samples if samples is not None else generate_test_cases(sig=sig)\n            )\n\n            info.coverage[1] += 1\n\n            # Create tasks for all test cases\n            tasks = [\n                self.execute_function_test(module_name, function_name, function_data, test_kwargs, profiler)\n                for test_kwargs in test_kwargs_list\n            ]\n\n            # Execute all tests concurrently\n            results = await asyncio.gather(*tasks)\n\n            total_execution_time = 0\n            for success, result_str, test_kwargs, execution_time in results:\n                info.functions_run += 1\n                total_execution_time += execution_time\n\n                if success:\n                    info.functions_sug += 1\n                    info.calls[function_name] = [test_kwargs, result_str]\n                else:\n                    info.functions_sug += 1\n                    info.error += 1\n                    info.callse[function_name] = [test_kwargs, result_str]\n\n            info.execution_time = time.time() - start_time\n            return function_name, info\n\n    async def process_module(self, module_name: str, functions: dict,\n                             f_query: str, profiler: cProfile.Profile) -&gt; tuple[str, ModuleInfo]:\n        start_time = time.time()\n\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            async with asyncio.Semaphore(mp.cpu_count()):\n                tasks = [\n                    self.process_function(module_name, fname, fdata, profiler)\n                    for fname, fdata in functions.items()\n                    if fname.startswith(f_query)\n                ]\n\n                if not tasks:\n                    return module_name, ModuleInfo()\n\n                results = await asyncio.gather(*tasks)\n\n                # Combine results from all functions in the module\n                combined_info = ModuleInfo()\n                total_execution_time = 0\n\n                for _, info in results:\n                    combined_info.functions_run += info.functions_run\n                    combined_info.functions_fatal_error += info.functions_fatal_error\n                    combined_info.error += info.error\n                    combined_info.functions_sug += info.functions_sug\n                    combined_info.calls.update(info.calls)\n                    combined_info.callse.update(info.callse)\n                    combined_info.coverage[0] += info.coverage[0]\n                    combined_info.coverage[1] += info.coverage[1]\n                    total_execution_time += info.execution_time\n\n                combined_info.execution_time = time.time() - start_time\n                return module_name, combined_info\n\n    async def execute_all_functions(self, m_query='', f_query='', enable_profiling=True):\n        \"\"\"\n        Execute all functions with parallel processing and optional profiling.\n\n        Args:\n            m_query (str): Module name query filter\n            f_query (str): Function name query filter\n            enable_profiling (bool): Enable detailed profiling information\n        \"\"\"\n        print(\"Executing all functions in parallel\" + (\" with profiling\" if enable_profiling else \"\"))\n\n        start_time = time.time()\n        stats = ExecutionStats()\n        items = list(self.functions.items()).copy()\n\n        # Set up profiling\n        self.enable_profiling = enable_profiling\n        profiler = cProfile.Profile()\n\n        with profile_section(profiler, enable_profiling):\n            # Filter modules based on query\n            filtered_modules = [\n                (mname, mfuncs) for mname, mfuncs in items\n                if mname.startswith(m_query)\n            ]\n\n            stats.modular_run = len(filtered_modules)\n\n            # Process all modules concurrently\n            async with asyncio.Semaphore(mp.cpu_count()):\n                tasks = [\n                    self.process_module(mname, mfuncs, f_query, profiler)\n                    for mname, mfuncs in filtered_modules\n                ]\n\n                results = await asyncio.gather(*tasks)\n\n            # Combine results and calculate statistics\n            for module_name, info in results:\n                if info.functions_run == info.functions_sug:\n                    stats.modular_sug += 1\n                else:\n                    stats.modular_fatal_error += 1\n\n                stats.errors += info.error\n\n                # Calculate coverage\n                coverage = (info.coverage[1] / info.coverage[0]) if info.coverage[0] &gt; 0 else 0\n                stats.coverage.append(f\"{module_name}:{coverage:.2f}\\n\")\n\n                # Store module info\n                stats.__dict__[module_name] = info\n\n            # Calculate total coverage\n            total_coverage = (\n                sum(float(t.split(\":\")[-1]) for t in stats.coverage) / len(stats.coverage)\n                if stats.coverage else 0\n            )\n\n            stats.total_execution_time = time.time() - start_time\n\n            # Generate profiling stats if enabled\n            if enable_profiling:\n                s = io.StringIO()\n                ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n                ps.print_stats()\n                stats.profiling_data = {\n                    'detailed_stats': s.getvalue(),\n                    'total_time': stats.total_execution_time,\n                    'function_count': stats.modular_run,\n                    'successful_functions': stats.modular_sug\n                }\n\n            print(\n                f\"\\n{stats.modular_run=}\"\n                f\"\\n{stats.modular_sug=}\"\n                f\"\\n{stats.modular_fatal_error=}\"\n                f\"\\n{total_coverage=}\"\n                f\"\\nTotal execution time: {stats.total_execution_time:.2f}s\"\n            )\n\n            if enable_profiling:\n                print(\"\\nProfiling Summary:\")\n                print(f\"{'=' * 50}\")\n                print(\"Top 10 time-consuming functions:\")\n                ps.print_stats(10)\n\n            analyzed_data = analyze_data(stats.__dict__)\n            return Result.ok(data=stats.__dict__, data_info=analyzed_data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.debug","title":"<code>debug</code>  <code>property</code> <code>writable</code>","text":"<p>proxi attr</p>"},{"location":"toolboxv2/#toolboxv2.AppType.prefix","title":"<code>prefix = prefix</code>  <code>instance-attribute</code>","text":"<p>proxi attr</p>"},{"location":"toolboxv2/#toolboxv2.AppType.a_exit","title":"<code>a_exit()</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.a_fuction_runner","title":"<code>a_fuction_runner(function, function_data, args, kwargs)</code>  <code>async</code>","text":"<p>parameters = function_data.get('params') modular_name = function_data.get('module_name') function_name = function_data.get('func_name') mod_function_name = f\"{modular_name}.{function_name}\"</p> <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict):\n    \"\"\"\n    parameters = function_data.get('params')\n    modular_name = function_data.get('module_name')\n    function_name = function_data.get('func_name')\n    mod_function_name = f\"{modular_name}.{function_name}\"\n\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.a_remove_mod","title":"<code>a_remove_mod(mod_name, spec='app', delete=True)</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_remove_mod(self, mod_name, spec='app', delete=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.a_run_any","title":"<code>a_run_any(mod_function_name, backwords_compability_variabel_string_holder=None, get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                    backwords_compability_variabel_string_holder=None,\n                    get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                    kwargs_=None,\n                    *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.a_run_function","title":"<code>a_run_function(mod_function_name, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_run_function(self, mod_function_name: Enum or tuple,\n                         tb_run_function_with_state=True,\n                         tb_run_with_specification='app',\n                         args_=None,\n                         kwargs_=None,\n                         *args,\n                         **kwargs) -&gt; Result:\n\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.debug_rains","title":"<code>debug_rains(e)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def debug_rains(self, e):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.disconnect","title":"<code>disconnect(*args, **kwargs)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def disconnect(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.execute_all_functions","title":"<code>execute_all_functions(m_query='', f_query='', enable_profiling=True)</code>  <code>async</code>","text":"<p>Execute all functions with parallel processing and optional profiling.</p> <p>Parameters:</p> Name Type Description Default <code>m_query</code> <code>str</code> <p>Module name query filter</p> <code>''</code> <code>f_query</code> <code>str</code> <p>Function name query filter</p> <code>''</code> <code>enable_profiling</code> <code>bool</code> <p>Enable detailed profiling information</p> <code>True</code> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def execute_all_functions(self, m_query='', f_query='', enable_profiling=True):\n    \"\"\"\n    Execute all functions with parallel processing and optional profiling.\n\n    Args:\n        m_query (str): Module name query filter\n        f_query (str): Function name query filter\n        enable_profiling (bool): Enable detailed profiling information\n    \"\"\"\n    print(\"Executing all functions in parallel\" + (\" with profiling\" if enable_profiling else \"\"))\n\n    start_time = time.time()\n    stats = ExecutionStats()\n    items = list(self.functions.items()).copy()\n\n    # Set up profiling\n    self.enable_profiling = enable_profiling\n    profiler = cProfile.Profile()\n\n    with profile_section(profiler, enable_profiling):\n        # Filter modules based on query\n        filtered_modules = [\n            (mname, mfuncs) for mname, mfuncs in items\n            if mname.startswith(m_query)\n        ]\n\n        stats.modular_run = len(filtered_modules)\n\n        # Process all modules concurrently\n        async with asyncio.Semaphore(mp.cpu_count()):\n            tasks = [\n                self.process_module(mname, mfuncs, f_query, profiler)\n                for mname, mfuncs in filtered_modules\n            ]\n\n            results = await asyncio.gather(*tasks)\n\n        # Combine results and calculate statistics\n        for module_name, info in results:\n            if info.functions_run == info.functions_sug:\n                stats.modular_sug += 1\n            else:\n                stats.modular_fatal_error += 1\n\n            stats.errors += info.error\n\n            # Calculate coverage\n            coverage = (info.coverage[1] / info.coverage[0]) if info.coverage[0] &gt; 0 else 0\n            stats.coverage.append(f\"{module_name}:{coverage:.2f}\\n\")\n\n            # Store module info\n            stats.__dict__[module_name] = info\n\n        # Calculate total coverage\n        total_coverage = (\n            sum(float(t.split(\":\")[-1]) for t in stats.coverage) / len(stats.coverage)\n            if stats.coverage else 0\n        )\n\n        stats.total_execution_time = time.time() - start_time\n\n        # Generate profiling stats if enabled\n        if enable_profiling:\n            s = io.StringIO()\n            ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n            ps.print_stats()\n            stats.profiling_data = {\n                'detailed_stats': s.getvalue(),\n                'total_time': stats.total_execution_time,\n                'function_count': stats.modular_run,\n                'successful_functions': stats.modular_sug\n            }\n\n        print(\n            f\"\\n{stats.modular_run=}\"\n            f\"\\n{stats.modular_sug=}\"\n            f\"\\n{stats.modular_fatal_error=}\"\n            f\"\\n{total_coverage=}\"\n            f\"\\nTotal execution time: {stats.total_execution_time:.2f}s\"\n        )\n\n        if enable_profiling:\n            print(\"\\nProfiling Summary:\")\n            print(f\"{'=' * 50}\")\n            print(\"Top 10 time-consuming functions:\")\n            ps.print_stats(10)\n\n        analyzed_data = analyze_data(stats.__dict__)\n        return Result.ok(data=stats.__dict__, data_info=analyzed_data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.exit","title":"<code>exit()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.exit_main","title":"<code>exit_main(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef exit_main(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.fuction_runner","title":"<code>fuction_runner(function, function_data, args, kwargs, t0=0.0)</code>","text":"<p>parameters = function_data.get('params') modular_name = function_data.get('module_name') function_name = function_data.get('func_name') mod_function_name = f\"{modular_name}.{function_name}\"</p> <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n    \"\"\"\n    parameters = function_data.get('params')\n    modular_name = function_data.get('module_name')\n    function_name = function_data.get('func_name')\n    mod_function_name = f\"{modular_name}.{function_name}\"\n\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.get_all_mods","title":"<code>get_all_mods(working_dir='mods', path_to='./runtime')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.get_autocompletion_dict","title":"<code>get_autocompletion_dict()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_autocompletion_dict(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.get_function","title":"<code>get_function(name, **kwargs)</code>","text":"<p>Kwargs for _get_function     metadata:: return the registered function dictionary         stateless: (function_data, None), 0         stateful: (function_data, higher_order_function), 0     state::boolean         specification::str default app</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_function(self, name: Enum or tuple, **kwargs):\n    \"\"\"\n    Kwargs for _get_function\n        metadata:: return the registered function dictionary\n            stateless: (function_data, None), 0\n            stateful: (function_data, higher_order_function), 0\n        state::boolean\n            specification::str default app\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.get_mod","title":"<code>get_mod(name, spec='app')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.get_username","title":"<code>get_username(get_input=False, default='loot')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.hide_console","title":"<code>hide_console(*args, **kwargs)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def hide_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.inplace_load_instance","title":"<code>inplace_load_instance(mod_name, loc='toolboxv2.mods.', spec='app', save=True)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.load_all_mods_in_file","title":"<code>load_all_mods_in_file(working_dir='mods')</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def load_all_mods_in_file(self, working_dir=\"mods\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.load_mod","title":"<code>load_mod(mod_name, mlm='I', **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def load_mod(self, mod_name: str, mlm='I', **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.mod_online","title":"<code>mod_online(mod_name, installed=False)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def mod_online(self, mod_name, installed=False):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.print","title":"<code>print(text, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef print(text, *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.print_ok","title":"<code>print_ok()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def print_ok(self):\n    \"\"\"proxi attr\"\"\"\n    self.logger.info(\"OK\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.reload_mod","title":"<code>reload_mod(mod_name, spec='app', is_file=True, loc='toolboxv2.mods.')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.remove_mod","title":"<code>remove_mod(mod_name, spec='app', delete=True)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def remove_mod(self, mod_name, spec='app', delete=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.rrun_flows","title":"<code>rrun_flows(name, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def rrun_flows(self, name, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_a_from_sync","title":"<code>run_a_from_sync(function, *args)</code>","text":"<p>run a async fuction</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_a_from_sync(self, function, *args):\n    \"\"\"\n    run a async fuction\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_any","title":"<code>run_any(mod_function_name, backwords_compability_variabel_string_holder=None, get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n            get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n            kwargs_=None,\n            *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_bg_task","title":"<code>run_bg_task(task)</code>","text":"<p>run a async fuction</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_bg_task(self, task):\n    \"\"\"\n            run a async fuction\n            \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_bg_task_advanced","title":"<code>run_bg_task_advanced(task, *args, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_bg_task_advanced(self, task, *args, **kwargs):\n    \"\"\"\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_flows","title":"<code>run_flows(name, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_flows(self, name, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_function","title":"<code>run_function(mod_function_name, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_function(self, mod_function_name: Enum or tuple,\n                 tb_run_function_with_state=True,\n                 tb_run_with_specification='app',\n                 args_=None,\n                 kwargs_=None,\n                 *args,\n                 **kwargs) -&gt; Result:\n\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_http","title":"<code>run_http(mod_function_name, function_name=None, method='GET', args_=None, kwargs_=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>run a function remote via http / https</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None, method=\"GET\",\n                   args_=None,\n                   kwargs_=None,\n                   *args, **kwargs):\n    \"\"\"run a function remote via http / https\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.save_autocompletion_dict","title":"<code>save_autocompletion_dict()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_autocompletion_dict(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.save_exit","title":"<code>save_exit()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.save_initialized_module","title":"<code>save_initialized_module(tools_class, spec)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_initialized_module(self, tools_class, spec):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.save_instance","title":"<code>save_instance(instance, modular_id, spec='app', instance_type='file/application', tools_class=None)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.save_load","title":"<code>save_load(modname, spec='app')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_load(self, modname, spec='app'):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.save_registry_as_enums","title":"<code>save_registry_as_enums(directory, filename)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_registry_as_enums(self, directory: str, filename: str):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.set_flows","title":"<code>set_flows(r)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def set_flows(self, r):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.set_logger","title":"<code>set_logger(debug=False)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def set_logger(self, debug=False):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.show_console","title":"<code>show_console(*args, **kwargs)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def show_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.sprint","title":"<code>sprint(text, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef sprint(text, *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.tb","title":"<code>tb(name=None, mod_name='', helper='', version=None, test=True, restrict_in_virtual_mode=False, api=False, initial=False, exit_f=False, test_only=False, memory_cache=False, file_cache=False, row=False, request_as_kwarg=False, state=None, level=0, memory_cache_max_size=100, memory_cache_ttl=300, samples=None, interface=None, pre_compute=None, post_compute=None, api_methods=None)</code>","text":"<p>A decorator for registering and configuring functions within a module.</p> <p>This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to register the function under. Defaults to the function's own name.</p> <code>None</code> <code>mod_name</code> <code>str</code> <p>The name of the module the function belongs to.</p> <code>''</code> <code>helper</code> <code>str</code> <p>A helper string providing additional information about the function.</p> <code>''</code> <code>version</code> <code>str or None</code> <p>The version of the function or module.</p> <code>None</code> <code>test</code> <code>bool</code> <p>Flag to indicate if the function is for testing purposes.</p> <code>True</code> <code>restrict_in_virtual_mode</code> <code>bool</code> <p>Flag to restrict the function in virtual mode.</p> <code>False</code> <code>api</code> <code>bool</code> <p>Flag to indicate if the function is part of an API.</p> <code>False</code> <code>initial</code> <code>bool</code> <p>Flag to indicate if the function should be executed at initialization.</p> <code>False</code> <code>exit_f</code> <code>bool</code> <p>Flag to indicate if the function should be executed at exit.</p> <code>False</code> <code>test_only</code> <code>bool</code> <p>Flag to indicate if the function should only be used for testing.</p> <code>False</code> <code>memory_cache</code> <code>bool</code> <p>Flag to enable memory caching for the function.</p> <code>False</code> <code>request_as_kwarg</code> <code>bool</code> <p>Flag to get request if the fuction is calld from api.</p> <code>False</code> <code>file_cache</code> <code>bool</code> <p>Flag to enable file caching for the function.</p> <code>False</code> <code>row</code> <code>bool</code> <p>rather to auto wrap the result in Result type default False means no row data aka result type</p> <code>False</code> <code>state</code> <code>bool or None</code> <p>Flag to indicate if the function maintains state.</p> <code>None</code> <code>level</code> <code>int</code> <p>The level of the function, used for prioritization or categorization.</p> <code>0</code> <code>memory_cache_max_size</code> <code>int</code> <p>Maximum size of the memory cache.</p> <code>100</code> <code>memory_cache_ttl</code> <code>int</code> <p>Time-to-live for the memory cache entries.</p> <code>300</code> <code>samples</code> <code>list or dict or None</code> <p>Samples or examples of function usage.</p> <code>None</code> <code>interface</code> <code>str</code> <p>The interface type for the function.</p> <code>None</code> <code>pre_compute</code> <code>callable</code> <p>A function to be called before the main function.</p> <code>None</code> <code>post_compute</code> <code>callable</code> <p>A function to be called after the main function.</p> <code>None</code> <code>api_methods</code> <code>list[str]</code> <p>default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>function</code> <p>The decorated function with additional processing and registration capabilities.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def tb(self, name=None,\n       mod_name: str = \"\",\n       helper: str = \"\",\n       version: str or None = None,\n       test: bool = True,\n       restrict_in_virtual_mode: bool = False,\n       api: bool = False,\n       initial: bool = False,\n       exit_f: bool = False,\n       test_only: bool = False,\n       memory_cache: bool = False,\n       file_cache: bool = False,\n       row=False,\n       request_as_kwarg: bool = False,\n       state: bool or None = None,\n       level: int = 0,\n       memory_cache_max_size: int = 100,\n       memory_cache_ttl: int = 300,\n       samples: list or dict or None = None,\n       interface: ToolBoxInterfaces or None or str = None,\n       pre_compute=None,\n       post_compute=None,\n       api_methods=None,\n       ):\n    \"\"\"\nA decorator for registering and configuring functions within a module.\n\nThis decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\nArgs:\n    name (str, optional): The name to register the function under. Defaults to the function's own name.\n    mod_name (str, optional): The name of the module the function belongs to.\n    helper (str, optional): A helper string providing additional information about the function.\n    version (str or None, optional): The version of the function or module.\n    test (bool, optional): Flag to indicate if the function is for testing purposes.\n    restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n    api (bool, optional): Flag to indicate if the function is part of an API.\n    initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n    exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n    test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n    memory_cache (bool, optional): Flag to enable memory caching for the function.\n    request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n    file_cache (bool, optional): Flag to enable file caching for the function.\n    row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n    state (bool or None, optional): Flag to indicate if the function maintains state.\n    level (int, optional): The level of the function, used for prioritization or categorization.\n    memory_cache_max_size (int, optional): Maximum size of the memory cache.\n    memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n    samples (list or dict or None, optional): Samples or examples of function usage.\n    interface (str, optional): The interface type for the function.\n    pre_compute (callable, optional): A function to be called before the main function.\n    post_compute (callable, optional): A function to be called after the main function.\n    api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\nReturns:\n    function: The decorated function with additional processing and registration capabilities.\n\"\"\"\n    if interface is None:\n        interface = \"tb\"\n    if test_only and 'test' not in self.id:\n        return lambda *args, **kwargs: args\n    return self._create_decorator(interface,\n                                  name,\n                                  mod_name,\n                                  level=level,\n                                  restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                  helper=helper,\n                                  api=api,\n                                  version=version,\n                                  initial=initial,\n                                  exit_f=exit_f,\n                                  test=test,\n                                  samples=samples,\n                                  state=state,\n                                  pre_compute=pre_compute,\n                                  post_compute=post_compute,\n                                  memory_cache=memory_cache,\n                                  file_cache=file_cache,\n                                  row=row,\n                                  request_as_kwarg=request_as_kwarg,\n                                  memory_cache_max_size=memory_cache_max_size,\n                                  memory_cache_ttl=memory_cache_ttl)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.wait_for_bg_tasks","title":"<code>wait_for_bg_tasks(timeout=None)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def wait_for_bg_tasks(self, timeout=None):\n    \"\"\"\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.watch_mod","title":"<code>watch_mod(mod_name, spec='app', loc='toolboxv2.mods.', use_thread=True, path_name=None)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.web_context","title":"<code>web_context()</code>","text":"<p>returns the build index ( toolbox web component )</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def web_context(self) -&gt; str:\n    \"\"\"returns the build index ( toolbox web component )\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.MainTool","title":"<code>toolboxv2.MainTool</code>","text":"Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>class MainTool:\n    toolID: str = \"\"\n    # app = None\n    interface = None\n    spec = \"app\"\n    name = \"\"\n    color = \"Bold\"\n    stuf = False\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.__storedargs = args, kwargs\n        self.tools = kwargs.get(\"tool\", {})\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n        if \"on_exit\" in kwargs and isinstance(kwargs.get(\"on_exit\"), Callable):\n            self.on_exit =self.app.tb(\n                mod_name=self.name,\n                name=kwargs.get(\"on_exit\").__name__,\n                version=self.version if hasattr(self, 'version') else \"0.0.0\",\n            )(kwargs.get(\"on_exit\"))\n        self.async_initialized = False\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    pass\n                else:\n                    self.todo()\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\"Error loading mod {self.name} {e}\")\n                if self.app.debug:\n                    import traceback\n                    traceback.print_exc()\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n\n    async def __ainit__(self, *args, **kwargs):\n        self.version = kwargs[\"v\"]\n        self.tools = kwargs.get(\"tool\", {})\n        self.name = kwargs[\"name\"]\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", None))\n        if not hasattr(self, 'config'):\n            self.config = {}\n        self.user = None\n        self.description = \"A toolbox mod\" if kwargs.get(\"description\") is None else kwargs.get(\"description\")\n        if MainTool.interface is None:\n            MainTool.interface = self.app.interface_type\n        # Result.default(self.app.interface)\n\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    await self.todo()\n                else:\n                    pass\n                await asyncio.sleep(0.1)\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\"Error loading mod {self.name} {e}\")\n                if self.app.debug:\n                    import traceback\n                    traceback.print_exc()\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n        self.app.print(f\"TOOL : {self.spec}.{self.name} online\")\n\n\n\n    @property\n    def app(self):\n        return get_app(\n            from_=f\"{self.spec}.{self.name}|{self.toolID if self.toolID else '*' + MainTool.toolID} {self.interface if self.interface else MainTool.interface}\")\n\n    @app.setter\n    def app(self, v):\n        raise PermissionError(f\"You cannot set the App Instance! {v=}\")\n\n    @staticmethod\n    def return_result(error: ToolBoxError = ToolBoxError.none,\n                      exec_code: int = 0,\n                      help_text: str = \"\",\n                      data_info=None,\n                      data=None,\n                      data_to=None):\n\n        if data_to is None:\n            data_to = MainTool.interface if MainTool.interface is not None else ToolBoxInterfaces.cli\n\n        if data is None:\n            data = {}\n\n        if data_info is None:\n            data_info = {}\n\n        return Result(\n            error,\n            ToolBoxResult(data_info=data_info, data=data, data_to=data_to),\n            ToolBoxInfo(exec_code=exec_code, help_text=help_text)\n        )\n\n    def print(self, message, end=\"\\n\", **kwargs):\n        if self.stuf:\n            return\n\n        self.app.print(Style.style_dic[self.color] + self.name + Style.style_dic[\"END\"] + \":\", message, end=end,\n                       **kwargs)\n\n    def add_str_to_config(self, command):\n        if len(command) != 2:\n            self.logger.error('Invalid command must be key value')\n            return False\n        self.config[command[0]] = command[1]\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n\n    def get_version(self) -&gt; str:\n        \"\"\"\"Returns the version\"\"\"\n        return self.version\n\n    async def get_user(self, username: str) -&gt; Result:\n        return await self.app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME, username=username, get_results=True)\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.MainTool.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.__storedargs = args, kwargs\n    self.tools = kwargs.get(\"tool\", {})\n    self.logger = kwargs.get(\"logs\", get_logger())\n    self.color = kwargs.get(\"color\", \"WHITE\")\n    self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n    if \"on_exit\" in kwargs and isinstance(kwargs.get(\"on_exit\"), Callable):\n        self.on_exit =self.app.tb(\n            mod_name=self.name,\n            name=kwargs.get(\"on_exit\").__name__,\n            version=self.version if hasattr(self, 'version') else \"0.0.0\",\n        )(kwargs.get(\"on_exit\"))\n    self.async_initialized = False\n    if self.todo:\n        try:\n            if inspect.iscoroutinefunction(self.todo):\n                pass\n            else:\n                self.todo()\n            get_logger().info(f\"{self.name} on load suspended\")\n        except Exception as e:\n            get_logger().error(f\"Error loading mod {self.name} {e}\")\n            if self.app.debug:\n                import traceback\n                traceback.print_exc()\n    else:\n        get_logger().info(f\"{self.name} no load require\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.MainTool.__initobj","title":"<code>__initobj()</code>  <code>async</code>","text":"<p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.MainTool.get_version","title":"<code>get_version()</code>","text":"<p>\"Returns the version</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\"Returns the version\"\"\"\n    return self.version\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.MainTool.webInstall","title":"<code>webInstall(user_instance, construct_render)</code>","text":"<p>\"Returns a web installer for the given user instance and construct render template</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def webInstall(self, user_instance, construct_render) -&gt; str:\n    \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.get_app","title":"<code>toolboxv2.get_app(from_=None, name=None, args=AppArgs().default(), app_con=None, sync=False)</code>","text":"Source code in <code>toolboxv2/utils/system/getting_and_closing_app.py</code> <pre><code>def get_app(from_=None, name=None, args=AppArgs().default(), app_con=None, sync=False) -&gt; AppType:\n    global registered_apps\n    # name = None\n    # print(f\"get app requested from: {from_} withe name: {name}\")\n    logger = get_logger()\n    logger.info(Style.GREYBG(f\"get app requested from: {from_}\"))\n    if registered_apps[0] is not None:\n        return registered_apps[0]\n\n    if app_con is None:\n        from ... import App\n        app_con = App\n    app = app_con(name, args=args) if name else app_con()\n    logger.info(Style.Bold(f\"App instance, returned ID: {app.id}\"))\n\n    registered_apps[0] = app\n    return app\n</code></pre>"},{"location":"toolboxv2/#system-utilities-configuration","title":"System Utilities &amp; Configuration","text":""},{"location":"toolboxv2/#toolboxv2.FileHandler","title":"<code>toolboxv2.FileHandler</code>","text":"<p>               Bases: <code>Code</code></p> Source code in <code>toolboxv2/utils/system/file_handler.py</code> <pre><code>class FileHandler(Code):\n\n    def __init__(self, filename, name='mainTool', keys=None, defaults=None):\n        if defaults is None:\n            defaults = {}\n        if keys is None:\n            keys = {}\n        assert filename.endswith(\".config\") or filename.endswith(\".data\"), \\\n            f\"filename must end with .config or .data {filename=}\"\n        self.file_handler_save = {}\n        self.file_handler_load = {}\n        self.file_handler_key_mapper = {}\n        self.file_handler_filename = filename\n        self.file_handler_storage = None\n        self.file_handler_max_loaded_index_ = 0\n        self.file_handler_file_prefix = (f\".{filename.split('.')[1]}/\"\n                                         f\"{name.replace('.', '-')}/\")\n        # self.load_file_handler()\n        self.set_defaults_keys_file_handler(keys, defaults)\n\n    def _open_file_handler(self, mode: str, rdu):\n        logger = get_logger()\n        logger.info(Style.Bold(Style.YELLOW(f\"Opening file in mode : {mode}\")))\n        if self.file_handler_storage:\n            self.file_handler_storage.close()\n            self.file_handler_storage = None\n        try:\n            self.file_handler_storage = open(self.file_handler_file_prefix + self.file_handler_filename, mode)\n            self.file_handler_max_loaded_index_ += 1\n        except FileNotFoundError:\n            if self.file_handler_max_loaded_index_ == 2:\n                os.makedirs(self.file_handler_file_prefix, exist_ok=True)\n            if self.file_handler_max_loaded_index_ == 3:\n                os.makedirs(\".config/mainTool\", exist_ok=True)\n            if self.file_handler_max_loaded_index_ &gt;= 5:\n                print(Style.RED(f\"pleas create this file to prosed : {self.file_handler_file_prefix}\"\n                                f\"{self.file_handler_filename}\"))\n                logger.critical(f\"{self.file_handler_file_prefix} {self.file_handler_filename} FileNotFoundError cannot\"\n                                f\" be Created\")\n                exit(0)\n            self.file_handler_max_loaded_index_ += 1\n            logger.info(Style.YELLOW(f\"Try Creating File: {self.file_handler_file_prefix}{self.file_handler_filename}\"))\n\n            if not os.path.exists(f\"{self.file_handler_file_prefix}\"):\n                os.makedirs(f\"{self.file_handler_file_prefix}\")\n\n            with open(self.file_handler_file_prefix + self.file_handler_filename, 'a'):\n                logger.info(Style.GREEN(\"File created successfully\"))\n                self.file_handler_max_loaded_index_ = -1\n            rdu()\n        except OSError and PermissionError as e:\n            raise e\n\n    def open_s_file_handler(self):\n        self._open_file_handler('w+', self.open_s_file_handler)\n        return self\n\n    def open_l_file_handler(self):\n        self._open_file_handler('r+', self.open_l_file_handler)\n        return self\n\n    def save_file_handler(self):\n        get_logger().info(\n            Style.BLUE(\n                f\"init Saving (S) {self.file_handler_filename} \"\n            )\n        )\n        if self.file_handler_storage:\n            get_logger().warning(\n                f\"WARNING file is already open (S): {self.file_handler_filename} {self.file_handler_storage}\")\n\n        self.open_s_file_handler()\n\n        get_logger().info(\n            Style.BLUE(\n                f\"Elements to save : ({len(self.file_handler_save.keys())})\"\n            )\n        )\n\n        self.file_handler_storage.write(json.dumps(self.file_handler_save))\n\n        self.file_handler_storage.close()\n        self.file_handler_storage = None\n\n        get_logger().info(\n            Style.BLUE(\n                f\"closing file : {self.file_handler_filename} \"\n            )\n        )\n\n        return self\n\n    def add_to_save_file_handler(self, key: str, value: str):\n        if len(key) != 10:\n            get_logger(). \\\n                warning(\n                Style.YELLOW(\n                    'WARNING: key length is not 10 characters'\n                )\n            )\n            return False\n        if key not in self.file_handler_load:\n            if key in self.file_handler_key_mapper:\n                key = self.file_handler_key_mapper[key]\n\n        self.file_handler_load[key] = value\n        self.file_handler_save[key] = self.encode_code(value)\n        return True\n\n    def remove_key_file_handler(self, key: str):\n        if key == 'Pka7237327':\n            print(\"Cant remove Root Key\")\n            return\n        if key in self.file_handler_load:\n            del self.file_handler_load[key]\n        if key in self.file_handler_save:\n            del self.file_handler_save[key]\n\n    def load_file_handler(self):\n        get_logger().info(\n            Style.BLUE(\n                f\"loading {self.file_handler_filename} \"\n            )\n        )\n        if self.file_handler_storage:\n            get_logger().warning(\n                Style.YELLOW(\n                    f\"WARNING file is already open (L) {self.file_handler_filename}\"\n                )\n            )\n        self.open_l_file_handler()\n\n        try:\n\n            self.file_handler_save = json.load(self.file_handler_storage)\n            for key, line in self.file_handler_save.items():\n                self.file_handler_load[key] = self.decode_code(line)\n\n        except json.decoder.JSONDecodeError and Exception:\n\n            for line in self.file_handler_storage:\n                line = line[:-1]\n                heda = line[:10]\n                self.file_handler_save[heda] = line[10:]\n                enc = self.decode_code(line[10:])\n                self.file_handler_load[heda] = enc\n\n            self.file_handler_save = {}\n\n        self.file_handler_storage.close()\n        self.file_handler_storage = None\n\n        return self\n\n    def get_file_handler(self, obj: str, default=None) -&gt; str or None:\n        logger = get_logger()\n        if obj not in self.file_handler_load:\n            if obj in self.file_handler_key_mapper:\n                obj = self.file_handler_key_mapper[obj]\n        logger.info(Style.ITALIC(Style.GREY(f\"Collecting data from storage key : {obj}\")))\n        self.file_handler_max_loaded_index_ = -1\n        for objects in self.file_handler_load.items():\n            self.file_handler_max_loaded_index_ += 1\n            if obj == objects[0]:\n\n                try:\n                    if len(objects[1]) &gt; 0:\n                        return ast.literal_eval(objects[1]) if isinstance(objects[1], str) else objects[1]\n                    logger.warning(\n                        Style.YELLOW(\n                            f\"No data  {obj}  ; {self.file_handler_filename}\"\n                        )\n                    )\n                except ValueError:\n                    logger.error(f\"ValueError Loading {obj} ; {self.file_handler_filename}\")\n                except SyntaxError:\n                    if isinstance(objects[1], str):\n                        return objects[1]\n                    logger.warning(\n                        Style.YELLOW(\n                            f\"Possible SyntaxError Loading {obj} ; {self.file_handler_filename}\"\n                            f\" {len(objects[1])} {type(objects[1])}\"\n                        )\n                    )\n                    return objects[1]\n                except NameError:\n                    return str(objects[1])\n\n        if obj in list(self.file_handler_save.keys()):\n            r = self.decode_code(self.file_handler_save[obj])\n            logger.info(f\"returning Default for {obj}\")\n            return r\n\n        if default is None:\n            default = self.file_handler_load.get(obj)\n\n        logger.info(\"no data found\")\n        return default\n\n    def set_defaults_keys_file_handler(self, keys: dict, defaults: dict):\n        list_keys = iter(list(keys.keys()))\n        df_keys = defaults.keys()\n        for key in list_keys:\n            self.file_handler_key_mapper[key] = keys[key]\n            self.file_handler_key_mapper[keys[key]] = key\n            if key in df_keys:\n                self.file_handler_load[keys[key]] = str(defaults[key])\n                self.file_handler_save[keys[key]] = self.encode_code(defaults[key])\n            else:\n                self.file_handler_load[keys[key]] = \"None\"\n\n    def delete_file(self):\n        os.remove(self.file_handler_file_prefix + self.file_handler_filename)\n        get_logger().warning(Style.GREEN(f\"File deleted {self.file_handler_file_prefix + self.file_handler_filename}\"))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils","title":"<code>toolboxv2.utils</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.App","title":"<code>App</code>","text":"Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>class App(AppType, metaclass=Singleton):\n\n    def __init__(self, prefix: str = \"\", args=AppArgs().default()):\n        super().__init__(prefix, args)\n        self._web_context = None\n        t0 = time.perf_counter()\n        abspath = os.path.abspath(__file__)\n        self.system_flag = system()  # Linux: Linux Mac: Darwin Windows: Windows\n\n        self.appdata = os.getenv('APPDATA') if os.name == 'nt' else os.getenv('XDG_CONFIG_HOME') or os.path.expanduser(\n                '~/.config') if os.name == 'posix' else None\n\n        if self.system_flag == \"Darwin\" or self.system_flag == \"Linux\":\n            dir_name = os.path.dirname(abspath).replace(\"/utils\", \"\")\n        else:\n            dir_name = os.path.dirname(abspath).replace(\"\\\\utils\", \"\")\n\n        self.start_dir = str(dir_name)\n\n        self.bg_tasks = []\n\n        lapp = dir_name + '\\\\.data\\\\'\n\n        if not prefix:\n            if not os.path.exists(f\"{lapp}last-app-prefix.txt\"):\n                os.makedirs(lapp, exist_ok=True)\n                open(f\"{lapp}last-app-prefix.txt\", \"a\").close()\n            with open(f\"{lapp}last-app-prefix.txt\") as prefix_file:\n                cont = prefix_file.read()\n                if cont:\n                    prefix = cont.rstrip()\n        else:\n            if not os.path.exists(f\"{lapp}last-app-prefix.txt\"):\n                os.makedirs(lapp, exist_ok=True)\n                open(f\"{lapp}last-app-prefix.txt\", \"a\").close()\n            with open(f\"{lapp}last-app-prefix.txt\", \"w\") as prefix_file:\n                prefix_file.write(prefix)\n\n        self.prefix = prefix\n\n        node_ = node()\n\n        if 'localhost' in node_ and (host := os.getenv('HOSTNAME', 'localhost')) != 'localhost':\n            node_ = node_.replace('localhost', host)\n        self.id = prefix + '-' + node_\n        self.globals = {\n            \"root\": {**globals()},\n        }\n        self.locals = {\n            \"user\": {'app': self, **locals()},\n        }\n\n        identification = self.id\n\n        if \"test\" in prefix:\n            if self.system_flag == \"Darwin\" or self.system_flag == \"Linux\":\n                start_dir = self.start_dir.replace(\"ToolBoxV2/toolboxv2\", \"toolboxv2\")\n            else:\n                start_dir = self.start_dir.replace(\"ToolBoxV2\\\\toolboxv2\", \"toolboxv2\")\n            self.data_dir = start_dir + '\\\\.data\\\\' + \"test\"\n            self.config_dir = start_dir + '\\\\.config\\\\' + \"test\"\n            self.info_dir = start_dir + '\\\\.info\\\\' + \"test\"\n        elif identification.startswith('collective-'):\n            collective_identification = identification.split('-')[1]\n            self.data_dir = self.start_dir + '\\\\.data\\\\' + collective_identification\n            self.config_dir = self.start_dir + '\\\\.config\\\\' + collective_identification\n            self.info_dir = self.start_dir + '\\\\.info\\\\' + collective_identification\n        else:\n            self.data_dir = self.start_dir + '\\\\.data\\\\' + identification\n            self.config_dir = self.start_dir + '\\\\.config\\\\' + identification\n            self.info_dir = self.start_dir + '\\\\.info\\\\' + identification\n\n        if self.appdata is None:\n            self.appdata = self.data_dir\n        else:\n            self.appdata += \"/ToolBoxV2\"\n\n        if not os.path.exists(self.appdata):\n            os.makedirs(self.appdata, exist_ok=True)\n        if not os.path.exists(self.data_dir):\n            os.makedirs(self.data_dir, exist_ok=True)\n        if not os.path.exists(self.config_dir):\n            os.makedirs(self.config_dir, exist_ok=True)\n        if not os.path.exists(self.info_dir):\n            os.makedirs(self.info_dir, exist_ok=True)\n\n        print(f\"Starting ToolBox as {prefix} from :\", Style.Bold(Style.CYAN(f\"{os.getcwd()}\")))\n\n        logger_info_str, self.logger, self.logging_filename = self.set_logger(args.debug)\n\n        print(\"Logger \" + logger_info_str)\n        print(\"================================\")\n        self.logger.info(\"Logger initialized\")\n        get_logger().info(Style.GREEN(\"Starting Application instance\"))\n        if args.init and args.init is not None and self.start_dir not in sys.path:\n            sys.path.append(self.start_dir)\n\n\n        __version__ = get_version_from_pyproject()\n\n        self.version = __version__\n\n        self.keys = {\n            \"MACRO\": \"macro~~~~:\",\n            \"MACRO_C\": \"m_color~~:\",\n            \"HELPER\": \"helper~~~:\",\n            \"debug\": \"debug~~~~:\",\n            \"id\": \"name-spa~:\",\n            \"st-load\": \"mute~load:\",\n            \"comm-his\": \"comm-his~:\",\n            \"develop-mode\": \"dev~mode~:\",\n            \"provider::\": \"provider::\",\n        }\n\n        defaults = {\n            \"MACRO\": ['Exit'],\n            \"MACRO_C\": {},\n            \"HELPER\": {},\n            \"debug\": args.debug,\n            \"id\": self.id,\n            \"st-load\": False,\n            \"comm-his\": [[]],\n            \"develop-mode\": False,\n        }\n        self.config_fh = FileHandler(self.id + \".config\", keys=self.keys, defaults=defaults)\n        self.config_fh.load_file_handler()\n        self._debug = args.debug\n        self.flows = {}\n        self.dev_modi = self.config_fh.get_file_handler(self.keys[\"develop-mode\"])\n        if self.config_fh.get_file_handler(\"provider::\") is None:\n            self.config_fh.add_to_save_file_handler(\"provider::\", \"http://localhost:\" + str(\n                self.args_sto.port) if os.environ.get(\"HOSTNAME\",\n                                                                     \"localhost\") == \"localhost\" else \"https://simplecore.app\")\n        self.functions = {}\n        self.modules = {}\n\n        self.interface_type = ToolBoxInterfaces.native\n        self.PREFIX = Style.CYAN(f\"~{node()}@&gt;\")\n        self.alive = True\n        self.called_exit = False, time.time()\n\n        self.print(f\"Infos:\\n  {'Name':&lt;8} -&gt; {node()}\\n  {'ID':&lt;8} -&gt; {self.id}\\n  {'Version':&lt;8} -&gt; {self.version}\\n\")\n\n        self.logger.info(\n            Style.GREEN(\n                f\"Finish init up in {time.perf_counter() - t0:.2f}s\"\n            )\n        )\n\n        self.args_sto = args\n        self.loop = None\n\n        from .system.session import Session\n        self.session: Session = Session(self.get_username())\n        if len(sys.argv) &gt; 2 and \"db\" == sys.argv[1]:\n            return\n        from .system.db_cli_manager import ClusterManager, get_executable_path\n        self.cluster_manager = ClusterManager()\n        online_list, server_list = self.cluster_manager.status_all(silent=True)\n        if not server_list:\n            self.cluster_manager.start_all(get_executable_path(), self.version)\n            _, server_list = self.cluster_manager.status_all()\n        from .extras.blobs import BlobStorage\n        self.root_blob_storage = BlobStorage(servers=server_list, storage_directory=self.data_dir+ '\\\\blob_cache\\\\')\n\n    def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n        user_name = self.config_fh.get_file_handler(\"ac_user:::\")\n        if get_input and user_name is None:\n            user_name = input(\"Input your username: \")\n            self.config_fh.add_to_save_file_handler(\"ac_user:::\", user_name)\n        if user_name is None:\n            user_name = default\n            self.config_fh.add_to_save_file_handler(\"ac_user:::\", user_name)\n        return user_name\n\n    def set_username(self, username):\n        return self.config_fh.add_to_save_file_handler(\"ac_user:::\", username)\n\n    @staticmethod\n    def exit_main(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def hide_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def show_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def disconnect(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def set_logger(self, debug=False):\n        if \"test\" in self.prefix and not debug:\n            logger, logging_filename = setup_logging(logging.NOTSET, name=\"toolbox-test\", interminal=True,\n                                                     file_level=logging.NOTSET, app_name=self.id)\n            logger_info_str = \"in Test Mode\"\n        elif \"live\" in self.prefix and not debug:\n            logger, logging_filename = setup_logging(logging.DEBUG, name=\"toolbox-live\", interminal=False,\n                                                     file_level=logging.WARNING, app_name=self.id)\n            logger_info_str = \"in Live Mode\"\n            # setup_logging(logging.WARNING, name=\"toolbox-live\", is_online=True\n            #              , online_level=logging.WARNING).info(\"Logger initialized\")\n        elif \"debug\" in self.prefix or self.prefix.endswith(\"D\"):\n            self.prefix = self.prefix.replace(\"-debug\", '').replace(\"debug\", '')\n            logger, logging_filename = setup_logging(logging.DEBUG, name=\"toolbox-debug\", interminal=True,\n                                                     file_level=logging.WARNING, app_name=self.id)\n            logger_info_str = \"in debug Mode\"\n            self.debug = True\n        elif debug:\n            logger, logging_filename = setup_logging(logging.DEBUG, name=f\"toolbox-{self.prefix}-debug\",\n                                                     interminal=True,\n                                                     file_level=logging.DEBUG, app_name=self.id)\n            logger_info_str = \"in args debug Mode\"\n        else:\n            logger, logging_filename = setup_logging(logging.ERROR, name=f\"toolbox-{self.prefix}\", app_name=self.id)\n            logger_info_str = \"in Default\"\n\n        return logger_info_str, logger, logging_filename\n\n    @property\n    def debug(self):\n        return self._debug\n\n    @debug.setter\n    def debug(self, value):\n        if not isinstance(value, bool):\n            self.logger.debug(f\"Value must be an boolean. is : {value} type of {type(value)}\")\n            raise ValueError(\"Value must be an boolean.\")\n\n        # self.logger.info(f\"Setting debug {value}\")\n        self._debug = value\n\n    def debug_rains(self, e):\n        if self.debug:\n            import traceback\n            x = \"=\"*5\n            x += \" DEBUG \"\n            x += \"=\"*5\n            self.print(x)\n            self.print(traceback.format_exc())\n            self.print(x)\n            raise e\n        else:\n            self.logger.error(f\"Error: {e}\")\n            import traceback\n            x = \"=\"*5\n            x += \" DEBUG \"\n            x += \"=\"*5\n            self.print(x)\n            self.print(traceback.format_exc())\n            self.print(x)\n\n    def set_flows(self, r):\n        self.flows = r\n\n    async def run_flows(self, name, **kwargs):\n        from ..flows import flows_dict as flows_dict_func\n        if name not in self.flows:\n            self.flows = {**self.flows, **flows_dict_func(s=name, remote=True)}\n        if name in self.flows:\n            if asyncio.iscoroutinefunction(self.flows[name]):\n                return await self.flows[name](get_app(from_=\"runner\"), self.args_sto, **kwargs)\n            else:\n                return self.flows[name](get_app(from_=\"runner\"), self.args_sto, **kwargs)\n        else:\n            print(\"Flow not found, active flows:\", len(self.flows.keys()))\n\n    def _coppy_mod(self, content, new_mod_dir, mod_name, file_type='py'):\n\n        mode = 'xb'\n        self.logger.info(f\" coppy mod {mod_name} to {new_mod_dir} size : {sys.getsizeof(content) / 8388608:.3f} mb\")\n\n        if not os.path.exists(new_mod_dir):\n            os.makedirs(new_mod_dir)\n            with open(f\"{new_mod_dir}/__init__.py\", \"w\") as nmd:\n                nmd.write(f\"__version__ = '{self.version}'\")\n\n        if os.path.exists(f\"{new_mod_dir}/{mod_name}.{file_type}\"):\n            mode = False\n\n            with open(f\"{new_mod_dir}/{mod_name}.{file_type}\", 'rb') as d:\n                runtime_mod = d.read()  # Testing version but not efficient\n\n            if len(content) != len(runtime_mod):\n                mode = 'wb'\n\n        if mode:\n            with open(f\"{new_mod_dir}/{mod_name}.{file_type}\", mode) as f:\n                f.write(content)\n\n    def _pre_lib_mod(self, mod_name, path_to=\"./runtime\", file_type='py'):\n        working_dir = self.id.replace(\".\", \"_\")\n        lib_mod_dir = f\"toolboxv2.runtime.{working_dir}.mod_lib.\"\n\n        self.logger.info(f\"pre_lib_mod {mod_name} from {lib_mod_dir}\")\n\n        postfix = \"_dev\" if self.dev_modi else \"\"\n        mod_file_dir = f\"./mods{postfix}/{mod_name}.{file_type}\"\n        new_mod_dir = f\"{path_to}/{working_dir}/mod_lib\"\n        with open(mod_file_dir, \"rb\") as c:\n            content = c.read()\n        self._coppy_mod(content, new_mod_dir, mod_name, file_type=file_type)\n        return lib_mod_dir\n\n    def _copy_load(self, mod_name, file_type='py', **kwargs):\n        loc = self._pre_lib_mod(mod_name, file_type)\n        return self.inplace_load_instance(mod_name, loc=loc, **kwargs)\n\n    def helper_install_pip_module(self, module_name):\n        if 'main' in self.id:\n            return\n        self.print(f\"Installing {module_name} GREEDY\")\n        os.system(f\"{sys.executable} -m pip install {module_name}\")\n\n    def python_module_import_classifier(self, mod_name, error_message):\n\n        if error_message.startswith(\"No module named 'toolboxv2.utils\"):\n            return Result.default_internal_error(f\"404 {error_message.split('utils')[1]} not found\")\n        if error_message.startswith(\"No module named 'toolboxv2.mods\"):\n            if mod_name.startswith('.'):\n                return\n            return self.run_a_from_sync(self.a_run_any, (\"CloudM\", \"install\"), module_name=mod_name)\n        if error_message.startswith(\"No module named '\"):\n            pip_requ = error_message.split(\"'\")[1].replace(\"'\", \"\").strip()\n            # if 'y' in input(f\"\\t\\t\\tAuto install {pip_requ} Y/n\").lower:\n            return self.helper_install_pip_module(pip_requ)\n            # return Result.default_internal_error(f\"404 {pip_requ} not found\")\n\n    def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True, mfo=None):\n        if self.dev_modi and loc == \"toolboxv2.mods.\":\n            loc = \"toolboxv2.mods_dev.\"\n        if spec=='app' and self.mod_online(mod_name):\n            self.logger.info(f\"Reloading mod from : {loc + mod_name}\")\n            self.remove_mod(mod_name, spec=spec, delete=False)\n\n        if (os.path.exists(self.start_dir + '/mods/' + mod_name) or os.path.exists(\n            self.start_dir + '/mods/' + mod_name + '.py')) and (\n            os.path.isdir(self.start_dir + '/mods/' + mod_name) or os.path.isfile(\n            self.start_dir + '/mods/' + mod_name + '.py')):\n            try:\n                if mfo is None:\n                    modular_file_object = import_module(loc + mod_name)\n                else:\n                    modular_file_object = mfo\n                self.modules[mod_name] = modular_file_object\n            except ModuleNotFoundError as e:\n                self.logger.error(Style.RED(f\"module {loc + mod_name} not found is type sensitive {e}\"))\n                self.print(Style.RED(f\"module {loc + mod_name} not found is type sensitive {e}\"))\n                if self.debug or self.args_sto.sysPrint:\n                    self.python_module_import_classifier(mod_name, str(e))\n                self.debug_rains(e)\n                return None\n        else:\n            self.print(f\"module {loc + mod_name} is not valid\")\n            return None\n        if hasattr(modular_file_object, \"Tools\"):\n            tools_class = modular_file_object.Tools\n        else:\n            if hasattr(modular_file_object, \"name\"):\n                tools_class = modular_file_object\n                modular_file_object = import_module(loc + mod_name)\n            else:\n                tools_class = None\n\n        modular_id = None\n        instance = modular_file_object\n        app_instance_type = \"file/application\"\n\n        if tools_class is None:\n            modular_id = modular_file_object.Name if hasattr(modular_file_object, \"Name\") else mod_name\n\n        if tools_class is None and modular_id is None:\n            modular_id = str(modular_file_object.__name__)\n            self.logger.warning(f\"Unknown instance loaded {mod_name}\")\n            return modular_file_object\n\n        if tools_class is not None:\n            tools_class = self.save_initialized_module(tools_class, spec)\n            modular_id = tools_class.name\n            app_instance_type = \"functions/class\"\n        else:\n            instance.spec = spec\n        # if private:\n        #     self.functions[modular_id][f\"{spec}_private\"] = private\n\n        if not save:\n            return instance if tools_class is None else tools_class\n\n        return self.save_instance(instance, modular_id, spec, app_instance_type, tools_class=tools_class)\n\n    def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n\n        if modular_id in self.functions and tools_class is None:\n            if self.functions[modular_id].get(f\"{spec}_instance\", None) is None:\n                self.functions[modular_id][f\"{spec}_instance\"] = instance\n                self.functions[modular_id][f\"{spec}_instance_type\"] = instance_type\n            else:\n                self.print(\"ERROR OVERRIDE\")\n                raise ImportError(f\"Module already known {modular_id}\")\n\n        elif tools_class is not None:\n            if modular_id not in self.functions:\n                self.functions[modular_id] = {}\n            self.functions[modular_id][f\"{spec}_instance\"] = tools_class\n            self.functions[modular_id][f\"{spec}_instance_type\"] = instance_type\n\n            try:\n                if not hasattr(tools_class, 'tools'):\n                    tools_class.tools = {\"Version\": tools_class.get_version, 'name': tools_class.name}\n                for function_name in list(tools_class.tools.keys()):\n                    t_function_name = function_name.lower()\n                    if t_function_name != \"all\" and t_function_name != \"name\":\n                        self.tb(function_name, mod_name=modular_id)(tools_class.tools.get(function_name))\n                self.functions[modular_id][f\"{spec}_instance_type\"] += \"/BC\"\n                if hasattr(tools_class, 'on_exit'):\n                    if \"on_exit\" in self.functions[modular_id]:\n                        self.functions[modular_id][\"on_exit\"].append(tools_class.on_exit)\n                    else:\n                        self.functions[modular_id][\"on_exit\"] = [tools_class.on_exit]\n            except Exception as e:\n                self.logger.error(f\"Starting Module {modular_id} compatibility failed with : {e}\")\n                pass\n        elif modular_id not in self.functions and tools_class is None:\n            self.functions[modular_id] = {}\n            self.functions[modular_id][f\"{spec}_instance\"] = instance\n            self.functions[modular_id][f\"{spec}_instance_type\"] = instance_type\n\n        else:\n            raise ImportError(f\"Modular {modular_id} is not a valid mod\")\n        on_start = self.functions[modular_id].get(\"on_start\")\n        if on_start is not None:\n            i = 1\n            for f in on_start:\n                try:\n                    f_, e = self.get_function((modular_id, f), state=True, specification=spec)\n                    if e == 0:\n                        self.logger.info(Style.GREY(f\"Running On start {f} {i}/{len(on_start)}\"))\n                        if asyncio.iscoroutinefunction(f_):\n                            self.print(f\"Async on start is only in Tool claas supported for {modular_id}.{f}\" if tools_class is None else f\"initialization starting soon for {modular_id}.{f}\")\n                            self.run_bg_task_advanced(f_)\n                        else:\n                            o = f_()\n                            if o is not None:\n                                self.print(f\"Function {modular_id} On start result: {o}\")\n                    else:\n                        self.logger.warning(f\"starting function not found {e}\")\n                except Exception as e:\n                    self.logger.debug(Style.YELLOW(\n                        Style.Bold(f\"modular:{modular_id}.{f} on_start error {i}/{len(on_start)} -&gt; {e}\")))\n                    self.debug_rains(e)\n                finally:\n                    i += 1\n        return instance if tools_class is None else tools_class\n\n    def save_initialized_module(self, tools_class, spec):\n        tools_class.spec = spec\n        live_tools_class = tools_class(app=self)\n        return live_tools_class\n\n    def mod_online(self, mod_name, installed=False):\n        if installed and mod_name not in self.functions:\n            self.save_load(mod_name)\n        return mod_name in self.functions\n\n    def _get_function(self,\n                      name: Enum or None,\n                      state: bool = True,\n                      specification: str = \"app\",\n                      metadata=False, as_str: tuple or None = None, r=0, **kwargs):\n\n        if as_str is None and isinstance(name, Enum):\n            modular_id = str(name.NAME.value)\n            function_id = str(name.value)\n        elif as_str is None and isinstance(name, list):\n            modular_id, function_id = name[0], name[1]\n        else:\n            modular_id, function_id = as_str\n\n        self.logger.info(f\"getting function : {specification}.{modular_id}.{function_id}\")\n\n        if modular_id not in self.functions:\n            if r == 0:\n                self.save_load(modular_id, spec=specification)\n                return self.get_function(name=(modular_id, function_id),\n                                         state=state,\n                                         specification=specification,\n                                         metadata=metadata,\n                                         r=1)\n            self.logger.warning(f\"function modular not found {modular_id} 404\")\n            return \"404\", 404\n\n        if function_id not in self.functions[modular_id]:\n            self.logger.warning(f\"function data not found {modular_id}.{function_id} 404\")\n            return \"404\", 404\n\n        function_data = self.functions[modular_id][function_id]\n\n        if isinstance(function_data, list):\n            print(f\"functions {function_id} : {function_data}\")\n            function_data = self.functions[modular_id][function_data[kwargs.get('i', -1)]]\n            print(f\"functions {modular_id} : {function_data}\")\n        function = function_data.get(\"func\")\n        params = function_data.get(\"params\")\n\n        state_ = function_data.get(\"state\")\n        if state_ is not None and state != state_:\n            state = state_\n\n        if function is None:\n            self.logger.warning(\"No function found\")\n            return \"404\", 404\n\n        if params is None:\n            self.logger.warning(\"No function (params) found\")\n            return \"404\", 301\n\n        if metadata and not state:\n            self.logger.info(\"returning metadata stateless\")\n            return (function_data, function), 0\n\n        if not state:  # mens a stateless function\n            self.logger.info(\"returning stateless function\")\n            return function, 0\n\n        instance = self.functions[modular_id].get(f\"{specification}_instance\")\n\n        # instance_type = self.functions[modular_id].get(f\"{specification}_instance_type\", \"functions/class\")\n\n        if params[0] == 'app':\n            instance = get_app(from_=f\"fuction {specification}.{modular_id}.{function_id}\")\n\n        if instance is None and self.alive:\n            self.inplace_load_instance(modular_id, spec=specification)\n            instance = self.functions[modular_id].get(f\"{specification}_instance\")\n\n        if instance is None:\n            self.logger.warning(\"No live Instance found\")\n            return \"404\", 400\n\n        # if instance_type.endswith(\"/BC\"):  # for backwards compatibility  functions/class/BC old modules\n        #     # returning as stateless\n        #     # return \"422\", -1\n        #     self.logger.info(\n        #         f\"returning stateless function, cant find tools class for state handling found {instance_type}\")\n        #     if metadata:\n        #         self.logger.info(f\"returning metadata stateless\")\n        #         return (function_data, function), 0\n        #     return function, 0\n\n        self.logger.info(\"wrapping in higher_order_function\")\n\n        self.logger.info(f\"returned fuction {specification}.{modular_id}.{function_id}\")\n        higher_order_function = partial(function, instance)\n\n        if metadata:\n            self.logger.info(\"returning metadata stateful\")\n            return (function_data, higher_order_function), 0\n\n        self.logger.info(\"returning stateful function\")\n        return higher_order_function, 0\n\n    def save_exit(self):\n        self.logger.info(f\"save exiting saving data to {self.config_fh.file_handler_filename} states of {self.debug=}\")\n        self.config_fh.add_to_save_file_handler(self.keys[\"debug\"], str(self.debug))\n\n    def init_mod(self, mod_name, spec='app'):\n        if '.' in mod_name:\n            mod_name = mod_name.split('.')[0]\n        return self.loop_gard().run_until_complete(self.a_init_mod(mod_name, spec))\n\n    def run_bg_task(self, task: Callable, *args, **kwargs) -&gt; Optional[asyncio.Task]:\n        \"\"\"\n        Runs a coroutine in the background without blocking the caller.\n\n        This is the primary method for \"fire-and-forget\" async tasks. It schedules\n        the coroutine to run on the application's main event loop.\n\n        Args:\n            task: The coroutine function to run.\n            *args: Arguments to pass to the coroutine function.\n            **kwargs: Keyword arguments to pass to the coroutine function.\n\n        Returns:\n            An asyncio.Task object representing the scheduled task, or None if\n            the task could not be scheduled.\n        \"\"\"\n        if not callable(task):\n            self.logger.warning(\"Task passed to run_bg_task is not callable!\")\n            return None\n\n        if not asyncio.iscoroutinefunction(task) and not asyncio.iscoroutine(task):\n            self.logger.warning(f\"Task '{getattr(task, '__name__', 'unknown')}' is not a coroutine. \"\n                                f\"Use run_bg_task_advanced for synchronous functions.\")\n            # Fallback to advanced runner for convenience\n            self.run_bg_task_advanced(task, *args, **kwargs)\n            return None\n\n        try:\n            loop = self.loop_gard()\n            if not loop.is_running():\n                # If the main loop isn't running, we can't create a task on it.\n                # This scenario is handled by run_bg_task_advanced.\n                self.logger.info(\"Main event loop not running. Delegating to advanced background runner.\")\n                return self.run_bg_task_advanced(task, *args, **kwargs)\n\n            # Create the coroutine if it's a function\n            coro = task(*args, **kwargs) if asyncio.iscoroutinefunction(task) else task\n\n            # Create a task on the running event loop\n            bg_task = loop.create_task(coro)\n\n            # Add a callback to log exceptions from the background task\n            def _log_exception(the_task: asyncio.Task):\n                if not the_task.cancelled() and the_task.exception():\n                    self.logger.error(f\"Exception in background task '{the_task.get_name()}':\",\n                                      exc_info=the_task.exception())\n\n            bg_task.add_done_callback(_log_exception)\n            self.bg_tasks.append(bg_task)\n            return bg_task\n\n        except Exception as e:\n            self.logger.error(f\"Failed to schedule background task: {e}\", exc_info=True)\n            return None\n\n    def run_bg_task_advanced(self, task: Callable, *args, **kwargs) -&gt; threading.Thread:\n        \"\"\"\n        Runs a task in a separate, dedicated background thread with its own event loop.\n\n        This is ideal for:\n        1. Running an async task from a synchronous context.\n        2. Launching a long-running, independent operation that should not\n           interfere with the main application's event loop.\n\n        Args:\n            task: The function to run (can be sync or async).\n            *args: Arguments for the task.\n            **kwargs: Keyword arguments for the task.\n\n        Returns:\n            The threading.Thread object managing the background execution.\n        \"\"\"\n        if not callable(task):\n            self.logger.warning(\"Task for run_bg_task_advanced is not callable!\")\n            return None\n\n        def thread_target():\n            # Each thread gets its own event loop.\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n\n            try:\n                # Prepare the coroutine we need to run\n                if asyncio.iscoroutinefunction(task):\n                    coro = task(*args, **kwargs)\n                elif asyncio.iscoroutine(task):\n                    # It's already a coroutine object\n                    coro = task\n                else:\n                    # It's a synchronous function, run it in an executor\n                    # to avoid blocking the new event loop.\n                    coro = loop.run_in_executor(None, lambda: task(*args, **kwargs))\n\n                # Run the coroutine to completion\n                result = loop.run_until_complete(coro)\n                self.logger.debug(f\"Advanced background task '{getattr(task, '__name__', 'unknown')}' completed.\")\n                if result is not None:\n                    self.logger.debug(f\"Task result: {str(result)[:100]}\")\n\n            except Exception as e:\n                self.logger.error(f\"Error in advanced background task '{getattr(task, '__name__', 'unknown')}':\",\n                                  exc_info=e)\n            finally:\n                # Cleanly shut down the event loop in this thread.\n                try:\n                    all_tasks = asyncio.all_tasks(loop=loop)\n                    if all_tasks:\n                        for t in all_tasks:\n                            t.cancel()\n                        loop.run_until_complete(asyncio.gather(*all_tasks, return_exceptions=True))\n                finally:\n                    loop.close()\n                    asyncio.set_event_loop(None)\n\n        # Create, start, and return the thread.\n        # It's a daemon thread so it won't prevent the main app from exiting.\n        t = threading.Thread(target=thread_target, daemon=True, name=f\"BGTask-{getattr(task, '__name__', 'unknown')}\")\n        self.bg_tasks.append(t)\n        t.start()\n        return t\n\n    # Helper method to wait for background tasks to complete (optional)\n    def wait_for_bg_tasks(self, timeout=None):\n        \"\"\"\n        Wait for all background tasks to complete.\n\n        Args:\n            timeout: Maximum time to wait (in seconds) for all tasks to complete.\n                     None means wait indefinitely.\n\n        Returns:\n            bool: True if all tasks completed, False if timeout occurred\n        \"\"\"\n        active_tasks = [t for t in self.bg_tasks if t.is_alive()]\n\n        for task in active_tasks:\n            task.join(timeout=timeout)\n            if task.is_alive():\n                return False\n\n        return True\n\n    def __call__(self, *args, **kwargs):\n        return self.run(*args, **kwargs)\n\n    def run(self, *args, request=None, running_function_coro=None, **kwargs):\n        \"\"\"\n        Run a function with support for SSE streaming in both\n        threaded and non-threaded contexts.\n        \"\"\"\n        if running_function_coro is None:\n            mn, fn = args[0]\n            if self.functions.get(mn, {}).get(fn, {}).get('request_as_kwarg', False):\n                kwargs[\"request\"] = RequestData.from_dict(request)\n                if 'data' in kwargs and 'data' not in self.functions.get(mn, {}).get(fn, {}).get('params', []):\n                    kwargs[\"request\"].data = kwargs[\"request\"].body = kwargs['data']\n                    del kwargs['data']\n                if 'form_data' in kwargs and 'form_data' not in self.functions.get(mn, {}).get(fn, {}).get('params',\n                                                                                                           []):\n                    kwargs[\"request\"].form_data = kwargs[\"request\"].body = kwargs['form_data']\n                    del kwargs['form_data']\n\n        # Create the coroutine\n        coro = running_function_coro or self.a_run_any(*args, **kwargs)\n\n        # Get or create an event loop\n        try:\n            loop = asyncio.get_event_loop()\n            is_running = loop.is_running()\n        except RuntimeError:\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n            is_running = False\n\n        # If the loop is already running, run in a separate thread\n        if is_running:\n            # Create thread pool executor as needed\n            if not hasattr(self.__class__, '_executor'):\n                self.__class__._executor = ThreadPoolExecutor(max_workers=4)\n\n            def run_in_new_thread():\n                # Set up a new loop in this thread\n                new_loop = asyncio.new_event_loop()\n                asyncio.set_event_loop(new_loop)\n\n                try:\n                    # Run the coroutine\n                    return new_loop.run_until_complete(coro)\n                finally:\n                    new_loop.close()\n\n            # Run in thread and get result\n            thread_result = self.__class__._executor.submit(run_in_new_thread).result()\n\n            # Handle streaming results from thread\n            if isinstance(thread_result, dict) and thread_result.get(\"is_stream\"):\n                # Create a new SSE stream in the main thread\n                async def stream_from_function():\n                    # Re-run the function with direct async access\n                    stream_result = await self.a_run_any(*args, **kwargs)\n\n                    if (isinstance(stream_result, Result) and\n                        getattr(stream_result.result, 'data_type', None) == \"stream\"):\n                        # Get and forward data from the original generator\n                        original_gen = stream_result.result.data.get(\"generator\")\n                        if inspect.isasyncgen(original_gen):\n                            async for item in original_gen:\n                                yield item\n\n                # Return a new streaming Result\n                return Result.stream(\n                    stream_generator=stream_from_function(),\n                    headers=thread_result.get(\"headers\", {})\n                )\n\n            result = thread_result\n        else:\n            # Direct execution when loop is not running\n            result = loop.run_until_complete(coro)\n\n        # Process the final result\n        if isinstance(result, Result):\n            if 'debug' in self.id:\n                result.print()\n            if getattr(result.result, 'data_type', None) == \"stream\":\n                return result\n            return result.to_api_result().model_dump(mode='json')\n\n        return result\n\n    def loop_gard(self):\n        if self.loop is None:\n            self.loop = asyncio.get_event_loop()\n        if self.loop.is_closed():\n            self.loop = asyncio.get_event_loop()\n        return self.loop\n\n    async def a_init_mod(self, mod_name, spec='app'):\n        mod = self.save_load(mod_name, spec=spec)\n        if hasattr(mod, \"__initobj\") and not mod.async_initialized:\n            await mod\n        return mod\n\n\n    def load_mod(self, mod_name: str, mlm='I', **kwargs):\n\n        action_list_helper = ['I (inplace load dill on error python)',\n                              # 'C (coppy py file to runtime dir)',\n                              # 'S (save py file to dill)',\n                              # 'CS (coppy and save py file)',\n                              # 'D (development mode, inplace load py file)'\n                              ]\n        action_list = {\"I\": lambda: self.inplace_load_instance(mod_name, **kwargs),\n                       \"C\": lambda: self._copy_load(mod_name, **kwargs)\n                       }\n\n        try:\n            if mlm in action_list:\n\n                return action_list.get(mlm)()\n            else:\n                self.logger.critical(\n                    f\"config mlm must be {' or '.join(action_list_helper)} is {mlm=}\")\n                raise ValueError(f\"config mlm must be {' or '.join(action_list_helper)} is {mlm=}\")\n        except ValueError as e:\n            self.logger.warning(Style.YELLOW(f\"Error Loading Module '{mod_name}', with error :{e}\"))\n            self.debug_rains(e)\n        except ImportError as e:\n            self.logger.error(Style.YELLOW(f\"Error Loading Module '{mod_name}', with error :{e}\"))\n            self.debug_rains(e)\n        except Exception as e:\n            self.logger.critical(Style.RED(f\"Error Loading Module '{mod_name}', with critical error :{e}\"))\n            print(Style.RED(f\"Error Loading Module '{mod_name}'\"))\n            self.debug_rains(e)\n\n        return Result.default_internal_error(info=\"info's in logs.\")\n\n    async def load_all_mods_in_file(self, working_dir=\"mods\"):\n        print(f\"LOADING ALL MODS FROM FOLDER : {working_dir}\")\n        t0 = time.perf_counter()\n        # Get the list of all modules\n        module_list = self.get_all_mods(working_dir)\n        open_modules = self.functions.keys()\n        start_len = len(open_modules)\n\n        for om in open_modules:\n            if om in module_list:\n                module_list.remove(om)\n\n        tasks: set[Task] = set()\n\n        _ = {tasks.add(asyncio.create_task(asyncio.to_thread(self.save_load, mod, 'app'))) for mod in module_list}\n        for t in asyncio.as_completed(tasks):\n            try:\n                result = await t\n                if hasattr(result, 'Name'):\n                    print('Opened :', result.Name)\n                elif hasattr(result, 'name'):\n                    if hasattr(result, 'async_initialized'):\n                        if not result.async_initialized:\n                            async def _():\n                                try:\n                                    if asyncio.iscoroutine(result):\n                                        await result\n                                    if hasattr(result, 'Name'):\n                                        print('Opened :', result.Name)\n                                    elif hasattr(result, 'name'):\n                                        print('Opened :', result.name)\n                                except Exception as e:\n                                    self.debug_rains(e)\n                                    if hasattr(result, 'Name'):\n                                        print('Error opening :', result.Name)\n                                    elif hasattr(result, 'name'):\n                                        print('Error opening :', result.name)\n                            asyncio.create_task(_())\n                        else:\n                            print('Opened :', result.name)\n                else:\n                    print('Opened :', result)\n            except Exception as e:\n                self.logger.error(Style.RED(f\"An Error occurred while opening all modules error: {str(e)}\"))\n                self.debug_rains(e)\n        opened = len(self.functions.keys()) - start_len\n\n        self.logger.info(f\"Opened {opened} modules in {time.perf_counter() - t0:.2f}s\")\n        return f\"Opened {opened} modules in {time.perf_counter() - t0:.2f}s\"\n\n    def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\", use_wd=True):\n        self.logger.info(f\"collating all mods in working directory {working_dir}\")\n\n        pr = \"_dev\" if self.dev_modi else \"\"\n        if working_dir == \"mods\" and use_wd:\n            working_dir = f\"{self.start_dir}/mods{pr}\"\n        elif use_wd:\n            pass\n        else:\n            w_dir = self.id.replace(\".\", \"_\")\n            working_dir = f\"{path_to}/{w_dir}/mod_lib{pr}/\"\n        res = os.listdir(working_dir)\n\n        self.logger.info(f\"found : {len(res)} files\")\n\n        def do_helper(_mod):\n            if \"mainTool\" in _mod:\n                return False\n            # if not _mod.endswith(\".py\"):\n            #     return False\n            if _mod.startswith(\"__\"):\n                return False\n            if _mod.startswith(\".\"):\n                return False\n            return not _mod.startswith(\"test_\")\n\n        def r_endings(word: str):\n            if word.endswith(\".py\"):\n                return word[:-3]\n            return word\n\n        mods_list = list(map(r_endings, filter(do_helper, res)))\n\n        self.logger.info(f\"found : {len(mods_list)} Modules\")\n        return mods_list\n\n    def remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            self.remove_mod(mod, delete=delete)\n\n    def remove_mod(self, mod_name, spec='app', delete=True):\n        if mod_name not in self.functions:\n            self.logger.info(f\"mod not active {mod_name}\")\n            return\n\n        on_exit = self.functions[mod_name].get(\"on_exit\")\n        self.logger.info(f\"closing: {on_exit}\")\n        def helper():\n            if f\"{spec}_instance\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance\"]\n            if f\"{spec}_instance_type\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance_type\"]\n\n        if on_exit is None and self.functions[mod_name].get(f\"{spec}_instance_type\", \"\").endswith(\"/BC\"):\n            instance = self.functions[mod_name].get(f\"{spec}_instance\", None)\n            if instance is not None and hasattr(instance, 'on_exit'):\n                if asyncio.iscoroutinefunction(instance.on_exit):\n                    self.exit_tasks.append(instance.on_exit)\n                else:\n                    instance.on_exit()\n\n        if on_exit is None and delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n            return\n        if on_exit is None:\n            helper()\n            return\n\n        i = 1\n\n        for j, f in enumerate(on_exit):\n            try:\n                f_, e = self.get_function((mod_name, f), state=True, specification=spec, i=j)\n                if e == 0:\n                    self.logger.info(Style.GREY(f\"Running On exit {f} {i}/{len(on_exit)}\"))\n                    if asyncio.iscoroutinefunction(f_):\n                        self.exit_tasks.append(f_)\n                        o = None\n                    else:\n                        o = f_()\n                    if o is not None:\n                        self.print(f\"Function On Exit result: {o}\")\n                else:\n                    self.logger.warning(\"closing function not found\")\n            except Exception as e:\n                self.logger.debug(\n                    Style.YELLOW(Style.Bold(f\"modular:{mod_name}.{f} on_exit error {i}/{len(on_exit)} -&gt; {e}\")))\n\n                self.debug_rains(e)\n            finally:\n                i += 1\n\n        helper()\n\n        if delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n\n    async def a_remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            await self.a_remove_mod(mod, delete=delete)\n\n    async def a_remove_mod(self, mod_name, spec='app', delete=True):\n        if mod_name not in self.functions:\n            self.logger.info(f\"mod not active {mod_name}\")\n            return\n        on_exit = self.functions[mod_name].get(\"on_exit\")\n        self.logger.info(f\"closing: {on_exit}\")\n        def helper():\n            if f\"{spec}_instance\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance\"]\n            if f\"{spec}_instance_type\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance_type\"]\n\n        if on_exit is None and self.functions[mod_name].get(f\"{spec}_instance_type\", \"\").endswith(\"/BC\"):\n            instance = self.functions[mod_name].get(f\"{spec}_instance\", None)\n            if instance is not None and hasattr(instance, 'on_exit'):\n                if asyncio.iscoroutinefunction(instance.on_exit):\n                    await instance.on_exit()\n                else:\n                    instance.on_exit()\n\n        if on_exit is None and delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n            return\n        if on_exit is None:\n            helper()\n            return\n\n        i = 1\n        for f in on_exit:\n            try:\n                e = 1\n                if isinstance(f, str):\n                    f_, e = self.get_function((mod_name, f), state=True, specification=spec)\n                elif isinstance(f, Callable):\n                    f_, e, f  = f, 0, f.__name__\n                if e == 0:\n                    self.logger.info(Style.GREY(f\"Running On exit {f} {i}/{len(on_exit)}\"))\n                    if asyncio.iscoroutinefunction(f_):\n                        o = await f_()\n                    else:\n                        o = f_()\n                    if o is not None:\n                        self.print(f\"Function On Exit result: {o}\")\n                else:\n                    self.logger.warning(\"closing function not found\")\n            except Exception as e:\n                self.logger.debug(\n                    Style.YELLOW(Style.Bold(f\"modular:{mod_name}.{f} on_exit error {i}/{len(on_exit)} -&gt; {e}\")))\n                self.debug_rains(e)\n            finally:\n                i += 1\n\n        helper()\n\n        if delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n\n    def exit(self, remove_all=True):\n        if not self.alive:\n            return\n        if self.args_sto.debug:\n            self.hide_console()\n        self.disconnect()\n        if remove_all:\n            self.remove_all_modules()\n        self.logger.info(\"Exiting ToolBox interface\")\n        self.alive = False\n        self.called_exit = True, time.time()\n        self.save_exit()\n        if hasattr(self, 'root_blob_storage') and self.root_blob_storage:\n            self.root_blob_storage.exit()\n        try:\n            self.config_fh.save_file_handler()\n        except SystemExit:\n            print(\"If u ar testing this is fine else ...\")\n\n        if hasattr(self, 'daemon_app'):\n            import threading\n\n            for thread in threading.enumerate()[::-1]:\n                if thread.name == \"MainThread\":\n                    continue\n                try:\n                    with Spinner(f\"closing Thread {thread.name:^50}|\", symbols=\"s\", count_down=True,\n                                 time_in_s=0.751 if not self.debug else 0.6):\n                        thread.join(timeout=0.751 if not self.debug else 0.6)\n                except TimeoutError as e:\n                    self.logger.error(f\"Timeout error on exit {thread.name} {str(e)}\")\n                    print(str(e), f\"Timeout {thread.name}\")\n                except KeyboardInterrupt:\n                    print(\"Unsave Exit\")\n                    break\n        if hasattr(self, 'loop') and self.loop is not None:\n            with Spinner(\"closing Event loop:\", symbols=\"+\"):\n                self.loop.stop()\n\n    async def a_exit(self):\n        await self.a_remove_all_modules(delete=True)\n        results = await asyncio.gather(\n            *[asyncio.create_task(f()) for f in self.exit_tasks if asyncio.iscoroutinefunction(f)])\n        for result in results:\n            self.print(f\"Function On Exit result: {result}\")\n        self.exit(remove_all=False)\n\n    def save_load(self, modname, spec='app'):\n        self.logger.debug(f\"Save load module {modname}\")\n        if not modname:\n            self.logger.warning(\"no filename specified\")\n            return False\n        try:\n            return self.load_mod(modname, spec=spec)\n        except ModuleNotFoundError as e:\n            self.logger.error(Style.RED(f\"Module {modname} not found\"))\n            self.debug_rains(e)\n\n        return False\n\n    def get_function(self, name: Enum or tuple, **kwargs):\n        \"\"\"\n        Kwargs for _get_function\n            metadata:: return the registered function dictionary\n                stateless: (function_data, None), 0\n                stateful: (function_data, higher_order_function), 0\n            state::boolean\n                specification::str default app\n        \"\"\"\n        if isinstance(name, tuple):\n            return self._get_function(None, as_str=name, **kwargs)\n        else:\n            return self._get_function(name, **kwargs)\n\n    async def a_run_function(self, mod_function_name: Enum or tuple,\n                             tb_run_function_with_state=True,\n                             tb_run_with_specification='app',\n                             args_=None,\n                             kwargs_=None,\n                             *args,\n                             **kwargs) -&gt; Result:\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n        if isinstance(mod_function_name, tuple):\n            modular_name, function_name = mod_function_name\n        elif isinstance(mod_function_name, list):\n            modular_name, function_name = mod_function_name[0], mod_function_name[1]\n        elif isinstance(mod_function_name, Enum):\n            modular_name, function_name = mod_function_name.__class__.NAME.value, mod_function_name.value\n        else:\n            raise TypeError(\"Unknown function type\")\n\n        if not self.mod_online(modular_name, installed=True):\n            self.get_mod(modular_name)\n\n        function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                      metadata=True, specification=tb_run_with_specification)\n        self.logger.info(f\"Received fuction : {mod_function_name}, with execode: {error_code}\")\n        if error_code == 404:\n            mod = self.get_mod(modular_name)\n            if hasattr(mod, \"async_initialized\") and not mod.async_initialized:\n                await mod\n            function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                          metadata=True, specification=tb_run_with_specification)\n\n        if error_code == 404:\n            self.logger.warning(Style.RED(\"Function Not Found\"))\n            return (Result.default_user_error(interface=self.interface_type,\n                                              exec_code=404,\n                                              info=\"function not found function is not decorated\").\n                    set_origin(mod_function_name))\n\n        if error_code == 300:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 info=f\"module {modular_name}\"\n                                                      f\" has no state (instance)\").set_origin(mod_function_name)\n\n        if error_code != 0:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=error_code,\n                                                 info=f\"Internal error\"\n                                                      f\" {modular_name}.\"\n                                                      f\"{function_name}\").set_origin(mod_function_name)\n\n        if not tb_run_function_with_state:\n            function_data, _ = function_data\n            function = function_data.get('func')\n        else:\n            function_data, function = function_data\n\n        if not function:\n            self.logger.warning(Style.RED(f\"Function {function_name} not found\"))\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=404,\n                                                 info=\"function not found function\").set_origin(mod_function_name)\n\n        self.logger.info(\"Profiling function\")\n        t0 = time.perf_counter()\n        if asyncio.iscoroutinefunction(function):\n            return await self.a_fuction_runner(function, function_data, args, kwargs, t0)\n        else:\n            return self.fuction_runner(function, function_data, args, kwargs, t0)\n\n    def run_function(self, mod_function_name: Enum or tuple,\n                     tb_run_function_with_state=True,\n                     tb_run_with_specification='app',\n                     args_=None,\n                     kwargs_=None,\n                     *args,\n                     **kwargs) -&gt; Result:\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n        if isinstance(mod_function_name, tuple):\n            modular_name, function_name = mod_function_name\n        elif isinstance(mod_function_name, list):\n            modular_name, function_name = mod_function_name[0], mod_function_name[1]\n        elif isinstance(mod_function_name, Enum):\n            modular_name, function_name = mod_function_name.__class__.NAME.value, mod_function_name.value\n        else:\n            raise TypeError(\"Unknown function type\")\n\n        if not self.mod_online(modular_name, installed=True):\n            self.get_mod(modular_name)\n\n        function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                      metadata=True, specification=tb_run_with_specification)\n        self.logger.info(f\"Received fuction : {mod_function_name}, with execode: {error_code}\")\n        if error_code == 1 or error_code == 3 or error_code == 400:\n            self.get_mod(modular_name)\n            function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                          metadata=True, specification=tb_run_with_specification)\n\n        if error_code == 2:\n            self.logger.warning(Style.RED(\"Function Not Found\"))\n            return (Result.default_user_error(interface=self.interface_type,\n                                              exec_code=404,\n                                              info=\"function not found function is not decorated\").\n                    set_origin(mod_function_name))\n\n        if error_code == -1:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 info=f\"module {modular_name}\"\n                                                      f\" has no state (instance)\").set_origin(mod_function_name)\n\n        if error_code != 0:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=error_code,\n                                                 info=f\"Internal error\"\n                                                      f\" {modular_name}.\"\n                                                      f\"{function_name}\").set_origin(mod_function_name)\n\n        if not tb_run_function_with_state:\n            function_data, _ = function_data\n            function = function_data.get('func')\n        else:\n            function_data, function = function_data\n\n        if not function:\n            self.logger.warning(Style.RED(f\"Function {function_name} not found\"))\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=404,\n                                                 info=\"function not found function\").set_origin(mod_function_name)\n\n        self.logger.info(\"Profiling function\")\n        t0 = time.perf_counter()\n        if asyncio.iscoroutinefunction(function):\n            raise ValueError(f\"Fuction {function_name} is Async use a_run_any\")\n        else:\n            return self.fuction_runner(function, function_data, args, kwargs, t0)\n\n    def run_a_from_sync(self, function, *args, **kwargs):\n        # Initialize self.loop if not already set.\n        if self.loop is None:\n            try:\n                self.loop = asyncio.get_running_loop()\n            except RuntimeError:\n                self.loop = asyncio.new_event_loop()\n\n        # If the loop is running, offload the coroutine to a new thread.\n        if self.loop.is_running():\n            result_future = Future()\n\n            def run_in_new_loop():\n                new_loop = asyncio.new_event_loop()\n                asyncio.set_event_loop(new_loop)\n                try:\n                    result = new_loop.run_until_complete(function(*args, **kwargs))\n                    result_future.set_result(result)\n                except Exception as e:\n                    result_future.set_exception(e)\n                finally:\n                    new_loop.close()\n\n            thread = threading.Thread(target=run_in_new_loop)\n            thread.start()\n            thread.join()  # Block until the thread completes.\n            return result_future.result()\n        else:\n            # If the loop is not running, schedule and run the coroutine directly.\n            future = self.loop.create_task(function(*args, **kwargs))\n            return self.loop.run_until_complete(future)\n\n    def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        row = function_data.get('row')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        if_self_state = 1 if 'self' in parameters else 0\n\n        try:\n            if len(parameters) == 0:\n                res = function()\n            elif len(parameters) == len(args) + if_self_state:\n                res = function(*args)\n            elif len(parameters) == len(kwargs.keys()) + if_self_state:\n                res = function(**kwargs)\n            else:\n                res = function(*args, **kwargs)\n            self.logger.info(f\"Execution done in {time.perf_counter()-t0:.4f}\")\n            if isinstance(res, Result):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.set_origin(mod_function_name)\n            elif isinstance(res, ApiResult):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.as_result().set_origin(mod_function_name).to_api_result()\n            elif row:\n                formatted_result = res\n            else:\n                # Wrap the result in a Result object\n                formatted_result = Result.ok(\n                    interface=self.interface_type,\n                    data_info=\"Auto generated result\",\n                    data=res,\n                    info=\"Function executed successfully\"\n                ).set_origin(mod_function_name)\n            if not row:\n                self.logger.info(\n                    f\"Function Exec code: {formatted_result.info.exec_code} Info's: {formatted_result.info.help_text}\")\n            else:\n                self.logger.info(\n                    f\"Function Exec data: {formatted_result}\")\n        except Exception as e:\n            self.logger.error(\n                Style.YELLOW(Style.Bold(\n                    f\"! Function ERROR: in {modular_name}.{function_name}\")))\n            # Wrap the exception in a Result object\n            formatted_result = Result.default_internal_error(info=str(e)).set_origin(mod_function_name)\n            # res = formatted_result\n            self.logger.error(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed wit an error {str(e)}, {type(e)}\")\n            self.debug_rains(e)\n            self.print(f\"! Function ERROR: in {modular_name}.{function_name} \")\n\n\n\n        else:\n            self.print_ok()\n\n            self.logger.info(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed successfully\")\n\n        return formatted_result\n\n    async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        row = function_data.get('row')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        if_self_state = 1 if 'self' in parameters else 0\n\n        try:\n            if len(parameters) == 0:\n                res = await function()\n            elif len(parameters) == len(args) + if_self_state:\n                res = await function(*args)\n            elif len(parameters) == len(kwargs.keys()) + if_self_state:\n                res = await function(**kwargs)\n            else:\n                res = await function(*args, **kwargs)\n            self.logger.info(f\"Execution done in {time.perf_counter()-t0:.4f}\")\n            if isinstance(res, Result):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.set_origin(mod_function_name)\n            elif isinstance(res, ApiResult):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.as_result().set_origin(mod_function_name).to_api_result()\n            elif row:\n                formatted_result = res\n            else:\n                # Wrap the result in a Result object\n                formatted_result = Result.ok(\n                    interface=self.interface_type,\n                    data_info=\"Auto generated result\",\n                    data=res,\n                    info=\"Function executed successfully\"\n                ).set_origin(mod_function_name)\n            if not row:\n                self.logger.info(\n                    f\"Function Exec code: {formatted_result.info.exec_code} Info's: {formatted_result.info.help_text}\")\n            else:\n                self.logger.info(\n                    f\"Function Exec data: {formatted_result}\")\n        except Exception as e:\n            self.logger.error(\n                Style.YELLOW(Style.Bold(\n                    f\"! Function ERROR: in {modular_name}.{function_name}\")))\n            # Wrap the exception in a Result object\n            formatted_result = Result.default_internal_error(info=str(e)).set_origin(mod_function_name)\n            # res = formatted_result\n            self.logger.error(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed wit an error {str(e)}, {type(e)}\")\n            self.debug_rains(e)\n\n        else:\n            self.print_ok()\n\n            self.logger.info(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed successfully\")\n\n        return formatted_result\n\n    async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None,\n                       args_=None,\n                       kwargs_=None, method=\"GET\",\n                       *args, **kwargs):\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n\n        modular_name = mod_function_name\n        function_name = function_name\n\n        if isinstance(mod_function_name, str) and isinstance(function_name, str):\n            mod_function_name = (mod_function_name, function_name)\n\n        if isinstance(mod_function_name, tuple):\n            modular_name, function_name = mod_function_name\n        elif isinstance(mod_function_name, list):\n            modular_name, function_name = mod_function_name[0], mod_function_name[1]\n        elif isinstance(mod_function_name, Enum):\n            modular_name, function_name = mod_function_name.__class__.NAME.value, mod_function_name.value\n\n        r = await self.session.fetch(f\"/api/{modular_name}/{function_name}{'?' + args_ if args_ is not None else ''}\",\n                                     data=kwargs, method=method)\n        try:\n            if not r:\n                print(\"\u00a7 Session server Offline!\", self.session.base)\n                return Result.default_internal_error(info=\"Session fetch failed\").as_dict()\n\n            content_type = r.headers.get('Content-Type', '').lower()\n            raw = await r.read()\n            encoding = r.get_encoding() or 'utf-8'\n            text = raw.decode(encoding, errors='ignore')\n\n            # Attempt JSON\n            if 'application/json' in content_type:\n                try:\n                    return await r.json()\n                except Exception as e:\n                    print(\"\u26a0 JSON decode error:\", e)\n\n            # Attempt YAML\n            if 'yaml' in content_type or text.strip().startswith('---'):\n                try:\n                    import yaml\n                    return yaml.safe_load(text)\n                except Exception as e:\n                    print(\"\u26a0 YAML decode error:\", e)\n\n            # Attempt XML\n            if 'xml' in content_type or text.strip().startswith('&lt;?xml'):\n                try:\n                    import xmltodict\n                    return xmltodict.parse(text)\n                except Exception as e:\n                    print(\"\u26a0 XML decode error:\", e)\n\n            # Fallback: return plain text\n            return Result.default_internal_error(data={'raw_text': text, 'content_type': content_type}).as_dict()\n\n        except Exception as e:\n            print(\"\u274c Fatal error during API call:\", e)\n            return Result.default_internal_error(str(e)).as_dict()\n\n    def run_local(self, *args, **kwargs):\n        return self.run_any(*args, **kwargs)\n\n    async def a_run_local(self, *args, **kwargs):\n        return await self.a_run_any(*args, **kwargs)\n\n    def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n                get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                kwargs_=None,\n                *args, **kwargs):\n\n        # if self.debug:\n        #     self.logger.info(f'Called from: {getouterframes(currentframe(), 2)}')\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n\n        if isinstance(mod_function_name, str) and backwords_compability_variabel_string_holder is None:\n            backwords_compability_variabel_string_holder = mod_function_name.split('.')[-1]\n            mod_function_name = mod_function_name.replace(f\".{backwords_compability_variabel_string_holder}\", \"\")\n\n        if isinstance(mod_function_name, str) and isinstance(backwords_compability_variabel_string_holder, str):\n            mod_function_name = (mod_function_name, backwords_compability_variabel_string_holder)\n\n        res: Result = self.run_function(mod_function_name,\n                                        tb_run_function_with_state=tb_run_function_with_state,\n                                        tb_run_with_specification=tb_run_with_specification,\n                                        args_=args, kwargs_=kwargs).as_result()\n        if isinstance(res, ApiResult):\n            res = res.as_result()\n\n        if isinstance(res, Result) and res.bg_task is not None:\n            self.run_bg_task(res.bg_task)\n\n        if self.debug:\n            res.log(show_data=False)\n\n        if not get_results and isinstance(res, Result):\n            return res.get()\n\n        if get_results and not isinstance(res, Result):\n            return Result.ok(data=res)\n\n        return res\n\n    async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                        backwords_compability_variabel_string_holder=None,\n                        get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                        kwargs_=None,\n                        *args, **kwargs):\n\n        # if self.debug:\n        #     self.logger.info(f'Called from: {getouterframes(currentframe(), 2)}')\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n\n        if isinstance(mod_function_name, str) and backwords_compability_variabel_string_holder is None:\n            backwords_compability_variabel_string_holder = mod_function_name.split('.')[-1]\n            mod_function_name = mod_function_name.replace(f\".{backwords_compability_variabel_string_holder}\", \"\")\n\n        if isinstance(mod_function_name, str) and isinstance(backwords_compability_variabel_string_holder, str):\n            mod_function_name = (mod_function_name, backwords_compability_variabel_string_holder)\n\n        res: Result = await self.a_run_function(mod_function_name,\n                                                tb_run_function_with_state=tb_run_function_with_state,\n                                                tb_run_with_specification=tb_run_with_specification,\n                                                args_=args, kwargs_=kwargs)\n        if isinstance(res, ApiResult):\n            res = res.as_result()\n\n        if isinstance(res, Result) and res.bg_task is not None:\n            self.run_bg_task(res.bg_task)\n\n        if self.debug:\n            res.print()\n            res.log(show_data=False) if isinstance(res, Result) else self.logger.debug(res)\n        if not get_results and isinstance(res, Result):\n            return res.get()\n\n        if get_results and not isinstance(res, Result):\n            return Result.ok(data=res)\n\n        return res\n\n\n    def web_context(self):\n        if self._web_context is None:\n            try:\n                self._web_context = open(\"./dist/helper.html\", encoding=\"utf-8\").read()\n            except Exception as e:\n                self.logger.error(f\"Could not load web context: {e}\")\n                self._web_context = \"&lt;div&gt;&lt;h1&gt;Web Context not found&lt;/h1&gt;&lt;/div&gt;\"\n        return self._web_context\n\n    def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n        if spec != \"app\":\n            self.print(f\"Getting Module {name} spec: {spec}\")\n        if name not in self.functions:\n            mod = self.save_load(name, spec=spec)\n            if mod is False or (isinstance(mod, Result) and mod.is_error()):\n                self.logger.warning(f\"Could not find {name} in {list(self.functions.keys())}\")\n                raise ValueError(f\"Could not find {name} in {list(self.functions.keys())} pleas install the module, or its posibly broken use --debug for infos\")\n        # private = self.functions[name].get(f\"{spec}_private\")\n        # if private is not None:\n        #     if private and spec != 'app':\n        #         raise ValueError(\"Module is private\")\n        if name not in self.functions:\n            self.logger.warning(f\"Module '{name}' is not found\")\n            return None\n        instance = self.functions[name].get(f\"{spec}_instance\")\n        if instance is None:\n            return self.load_mod(name, spec=spec)\n        return self.functions[name].get(f\"{spec}_instance\")\n\n    def print(self, text, *args, **kwargs):\n        # self.logger.info(f\"Output : {text}\")\n        if 'live' in self.id:\n            return\n        if self.sprint(None):\n            print(Style.CYAN(f\"System${self.id}:\"), end=\" \")\n        print(text, *args, **kwargs)\n\n    def sprint(self, text, *args, **kwargs):\n        if text is None:\n            return True\n        if 'live' in self.id:\n            return\n        # self.logger.info(f\"Output : {text}\")\n        print(Style.CYAN(f\"System${self.id}:\"), end=\" \")\n        if isinstance(text, str) and kwargs == {} and text:\n            stram_print(text + ' '.join(args))\n            print()\n        else:\n            print(text, *args, **kwargs)\n\n    # ----------------------------------------------------------------\n    # Decorators for the toolbox\n\n    def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n        self.remove_mod(mod_name, delete=True)\n        if mod_name not in self.modules:\n            self.logger.warning(f\"Module '{mod_name}' is not found\")\n            return\n        if hasattr(self.modules[mod_name], 'reload_save') and self.modules[mod_name].reload_save:\n            def reexecute_module_code(x):\n                return x\n        else:\n            def reexecute_module_code(module_name):\n                if isinstance(module_name, str):\n                    module = import_module(module_name)\n                else:\n                    module = module_name\n                # Get the source code of the module\n                try:\n                    source = inspect.getsource(module)\n                except Exception:\n                    # print(f\"No source for {str(module_name).split('from')[0]}: {e}\")\n                    return module\n                # Compile the source code\n                try:\n                    code = compile(source, module.__file__, 'exec')\n                    # Execute the code in the module's namespace\n                    exec(code, module.__dict__)\n                except Exception:\n                    # print(f\"No source for {str(module_name).split('from')[0]}: {e}\")\n                    pass\n                return module\n\n        if not is_file:\n            mods = self.get_all_mods(\"./mods/\" + mod_name)\n            def recursive_reload(package_name):\n                package = import_module(package_name)\n\n                # First, reload all submodules\n                if hasattr(package, '__path__'):\n                    for _finder, name, _ispkg in pkgutil.walk_packages(package.__path__, package.__name__ + \".\"):\n                        try:\n                            mod = import_module(name)\n                            reexecute_module_code(mod)\n                            reload(mod)\n                        except Exception as e:\n                            print(f\"Error reloading module {name}: {e}\")\n                            break\n\n                # Finally, reload the package itself\n                reexecute_module_code(package)\n                reload(package)\n\n            for mod in mods:\n                if mod.endswith(\".txt\") or mod.endswith(\".yaml\"):\n                    continue\n                try:\n                    recursive_reload(loc + mod_name + '.' + mod)\n                    self.print(f\"Reloaded {mod_name}.{mod}\")\n                except ImportError:\n                    self.print(f\"Could not load {mod_name}.{mod}\")\n        reexecute_module_code(self.modules[mod_name])\n        if mod_name in self.functions:\n            if \"on_exit\" in self.functions[mod_name]:\n                self.functions[mod_name][\"on_exit\"] = []\n            if \"on_start\" in self.functions[mod_name]:\n                self.functions[mod_name][\"on_start\"] = []\n        self.inplace_load_instance(mod_name, spec=spec, mfo=reload(self.modules[mod_name]) if mod_name in self.modules else None)\n\n    def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None, on_reload=None):\n        if path_name is None:\n            path_name = mod_name\n        is_file = os.path.isfile(self.start_dir + '/mods/' + path_name + '.py')\n        import watchfiles\n        def helper():\n            paths = f'mods/{path_name}' + ('.py' if is_file else '')\n            self.print(f'Watching Path: {paths}')\n            for changes in watchfiles.watch(paths):\n                if not changes:\n                    continue\n                self.reload_mod(mod_name, spec, is_file, loc)\n                if on_reload:\n                    on_reload()\n\n        if not use_thread:\n            helper()\n        else:\n            threading.Thread(target=helper, daemon=True).start()\n\n    def _register_function(self, module_name, func_name, data):\n        if module_name not in self.functions:\n            self.functions[module_name] = {}\n        if func_name in self.functions[module_name]:\n            self.print(f\"Overriding function {func_name} from {module_name}\", end=\"\\r\")\n            self.functions[module_name][func_name] = data\n        else:\n            self.functions[module_name][func_name] = data\n\n    def _create_decorator(self, type_: str,\n                          name: str = \"\",\n                          mod_name: str = \"\",\n                          level: int = -1,\n                          restrict_in_virtual_mode: bool = False,\n                          api: bool = False,\n                          helper: str = \"\",\n                          version: str or None = None,\n                          initial: bool=False,\n                          exit_f: bool=False,\n                          test: bool=True,\n                          samples:list[dict[str, Any]] | None=None,\n                          state:bool | None=None,\n                          pre_compute:Callable | None=None,\n                          post_compute:Callable[[], Result] | None=None,\n                          api_methods:list[str] | None=None,\n                          memory_cache: bool=False,\n                          file_cache: bool=False,\n                          request_as_kwarg: bool=False,\n                          row: bool=False,\n                          memory_cache_max_size:int=100,\n                          memory_cache_ttl:int=300):\n\n        if isinstance(type_, Enum):\n            type_ = type_.value\n\n        if memory_cache and file_cache:\n            raise ValueError(\"Don't use both cash at the same time for the same fuction\")\n\n        use_cache = memory_cache or file_cache\n        cache = {}\n        if file_cache:\n            cache = FileCache(folder=self.data_dir + f'\\\\cache\\\\{mod_name}\\\\',\n                              filename=self.data_dir + f'\\\\cache\\\\{mod_name}\\\\{name}cache.db')\n        if memory_cache:\n            cache = MemoryCache(maxsize=memory_cache_max_size, ttl=memory_cache_ttl)\n\n        version = self.version if version is None else self.version + ':' + version\n\n        def a_additional_process(func):\n\n            async def executor(*args, **kwargs):\n\n                if pre_compute is not None:\n                    args, kwargs = await pre_compute(*args, **kwargs)\n                if asyncio.iscoroutinefunction(func):\n                    result = await func(*args, **kwargs)\n                else:\n                    result = func(*args, **kwargs)\n                if post_compute is not None:\n                    result = await post_compute(result)\n                if row:\n                    return result\n                if not isinstance(result, Result):\n                    result = Result.ok(data=result)\n                if result.origin is None:\n                    result.set_origin((mod_name if mod_name else func.__module__.split('.')[-1]\n                                       , name if name else func.__name__\n                                       , type_))\n                if result.result.data_to == ToolBoxInterfaces.native.name:\n                    result.result.data_to = ToolBoxInterfaces.remote if api else ToolBoxInterfaces.native\n                # Wenden Sie die to_api_result Methode auf das Ergebnis an, falls verf\u00fcgbar\n                if api and hasattr(result, 'to_api_result'):\n                    return result.to_api_result()\n                return result\n\n            @wraps(func)\n            async def wrapper(*args, **kwargs):\n\n                if not use_cache:\n                    return await executor(*args, **kwargs)\n\n                try:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{str(args)},{str(kwargs.items())}\")\n                except ValueError:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{bytes(args)},{str(kwargs.items())}\")\n\n                result = cache.get(cache_key)\n                if result is not None:\n                    return result\n\n                result = await executor(*args, **kwargs)\n\n                cache.set(cache_key, result)\n\n                return result\n\n            return wrapper\n\n        def additional_process(func):\n\n            def executor(*args, **kwargs):\n\n                if pre_compute is not None:\n                    args, kwargs = pre_compute(*args, **kwargs)\n                if asyncio.iscoroutinefunction(func):\n                    result = func(*args, **kwargs)\n                else:\n                    result = func(*args, **kwargs)\n                if post_compute is not None:\n                    result = post_compute(result)\n                if row:\n                    return result\n                if not isinstance(result, Result):\n                    result = Result.ok(data=result)\n                if result.origin is None:\n                    result.set_origin((mod_name if mod_name else func.__module__.split('.')[-1]\n                                       , name if name else func.__name__\n                                       , type_))\n                if result.result.data_to == ToolBoxInterfaces.native.name:\n                    result.result.data_to = ToolBoxInterfaces.remote if api else ToolBoxInterfaces.native\n                # Wenden Sie die to_api_result Methode auf das Ergebnis an, falls verf\u00fcgbar\n                if api and hasattr(result, 'to_api_result'):\n                    return result.to_api_result()\n                return result\n\n            @wraps(func)\n            def wrapper(*args, **kwargs):\n\n                if not use_cache:\n                    return executor(*args, **kwargs)\n\n                try:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{str(args)},{str(kwargs.items())}\")\n                except ValueError:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{bytes(args)},{str(kwargs.items())}\")\n\n                result = cache.get(cache_key)\n                if result is not None:\n                    return result\n\n                result = executor(*args, **kwargs)\n\n                cache.set(cache_key, result)\n\n                return result\n\n            return wrapper\n\n        def decorator(func):\n            sig = signature(func)\n            params = list(sig.parameters)\n            module_name = mod_name if mod_name else func.__module__.split('.')[-1]\n            func_name = name if name else func.__name__\n            if func_name == 'on_start':\n                func_name = 'on_startup'\n            if func_name == 'on_exit':\n                func_name = 'on_close'\n            if api or pre_compute is not None or post_compute is not None or memory_cache or file_cache:\n                if asyncio.iscoroutinefunction(func):\n                    func = a_additional_process(func)\n                else:\n                    func = additional_process(func)\n            if api and str(sig.return_annotation) == 'Result':\n                raise ValueError(f\"Fuction {module_name}.{func_name} registered as \"\n                                 f\"Api fuction but uses {str(sig.return_annotation)}\\n\"\n                                 f\"Please change the sig from ..)-&gt; Result to ..)-&gt; ApiResult\")\n            data = {\n                \"type\": type_,\n                \"module_name\": module_name,\n                \"func_name\": func_name,\n                \"level\": level,\n                \"restrict_in_virtual_mode\": restrict_in_virtual_mode,\n                \"func\": func,\n                \"api\": api,\n                \"helper\": helper,\n                \"version\": version,\n                \"initial\": initial,\n                \"exit_f\": exit_f,\n                \"api_methods\": api_methods if api_methods is not None else [\"AUTO\"],\n                \"__module__\": func.__module__,\n                \"signature\": sig,\n                \"params\": params,\n                \"row\": row,\n                \"state\": (\n                    False if len(params) == 0 else params[0] in ['self', 'state', 'app']) if state is None else state,\n                \"do_test\": test,\n                \"samples\": samples,\n                \"request_as_kwarg\": request_as_kwarg,\n\n            }\n            self._register_function(module_name, func_name, data)\n            if exit_f:\n                if \"on_exit\" not in self.functions[module_name]:\n                    self.functions[module_name][\"on_exit\"] = []\n                self.functions[module_name][\"on_exit\"].append(data)\n            if initial:\n                if \"on_start\" not in self.functions[module_name]:\n                    self.functions[module_name][\"on_start\"] = []\n                self.functions[module_name][\"on_start\"].append(func_name)\n\n            return func\n\n        decorator.tb_init = True\n\n        return decorator\n\n    def tb(self, name=None,\n           mod_name: str = \"\",\n           helper: str = \"\",\n           version: str | None = None,\n           test: bool = True,\n           restrict_in_virtual_mode: bool = False,\n           api: bool = False,\n           initial: bool = False,\n           exit_f: bool = False,\n           test_only: bool = False,\n           memory_cache: bool = False,\n           file_cache: bool = False,\n           request_as_kwarg: bool = False,\n           row: bool = False,\n           state: bool | None = None,\n           level: int = -1,\n           memory_cache_max_size: int = 100,\n           memory_cache_ttl: int = 300,\n           samples: list or dict or None = None,\n           interface: ToolBoxInterfaces or None or str = None,\n           pre_compute=None,\n           post_compute=None,\n           api_methods=None,\n           ):\n        \"\"\"\n    A decorator for registering and configuring functions within a module.\n\n    This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\n    Args:\n        name (str, optional): The name to register the function under. Defaults to the function's own name.\n        mod_name (str, optional): The name of the module the function belongs to.\n        helper (str, optional): A helper string providing additional information about the function.\n        version (str or None, optional): The version of the function or module.\n        test (bool, optional): Flag to indicate if the function is for testing purposes.\n        restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n        api (bool, optional): Flag to indicate if the function is part of an API.\n        initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n        exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n        test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n        memory_cache (bool, optional): Flag to enable memory caching for the function.\n        request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n        file_cache (bool, optional): Flag to enable file caching for the function.\n        row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n        state (bool or None, optional): Flag to indicate if the function maintains state.\n        level (int, optional): The level of the function, used for prioritization or categorization.\n        memory_cache_max_size (int, optional): Maximum size of the memory cache.\n        memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n        samples (list or dict or None, optional): Samples or examples of function usage.\n        interface (str, optional): The interface type for the function.\n        pre_compute (callable, optional): A function to be called before the main function.\n        post_compute (callable, optional): A function to be called after the main function.\n        api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\n    Returns:\n        function: The decorated function with additional processing and registration capabilities.\n    \"\"\"\n        if interface is None:\n            interface = \"tb\"\n        if test_only and 'test' not in self.id:\n            return lambda *args, **kwargs: args\n        return self._create_decorator(interface,\n                                      name,\n                                      mod_name,\n                                      level=level,\n                                      restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                      helper=helper,\n                                      api=api,\n                                      version=version,\n                                      initial=initial,\n                                      exit_f=exit_f,\n                                      test=test,\n                                      samples=samples,\n                                      state=state,\n                                      pre_compute=pre_compute,\n                                      post_compute=post_compute,\n                                      memory_cache=memory_cache,\n                                      file_cache=file_cache,\n                                      request_as_kwarg=request_as_kwarg,\n                                      row=row,\n                                      api_methods=api_methods,\n                                      memory_cache_max_size=memory_cache_max_size,\n                                      memory_cache_ttl=memory_cache_ttl)\n\n    def save_autocompletion_dict(self):\n        autocompletion_dict = {}\n        for module_name, _module in self.functions.items():\n            data = {}\n            for function_name, function_data in self.functions[module_name].items():\n                if not isinstance(function_data, dict):\n                    continue\n                data[function_name] = {arg: None for arg in\n                                       function_data.get(\"params\", [])}\n                if len(data[function_name].keys()) == 0:\n                    data[function_name] = None\n            autocompletion_dict[module_name] = data if len(data.keys()) &gt; 0 else None\n        self.config_fh.add_to_save_file_handler(\"auto~~~~~~\", str(autocompletion_dict))\n\n    def get_autocompletion_dict(self):\n        return self.config_fh.get_file_handler(\"auto~~~~~~\")\n\n    def save_registry_as_enums(self, directory: str, filename: str):\n        # Ordner erstellen, falls nicht vorhanden\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n        # Dateipfad vorbereiten\n        filepath = os.path.join(directory, filename)\n\n        # Enum-Klassen als Strings generieren\n        enum_classes = [f'\"\"\"Automatic generated by ToolBox v = {self.version}\"\"\"'\n                        f'\\nfrom enum import Enum\\nfrom dataclasses import dataclass'\n                        f'\\n\\n\\n']\n        for module, functions in self.functions.items():\n            if module.startswith(\"APP_INSTANCE\"):\n                continue\n            class_name = module\n            enum_members = \"\\n    \".join(\n                [\n                    f\"{func_name.upper().replace('-', '')}\"\n                    f\" = '{func_name}' \"\n                    f\"# Input: ({fuction_data['params'] if isinstance(fuction_data, dict) else ''}),\"\n                    f\" Output: {fuction_data['signature'].return_annotation if isinstance(fuction_data, dict) else 'None'}\"\n                    for func_name, fuction_data in functions.items()])\n            enum_class = (f'@dataclass\\nclass {class_name.upper().replace(\".\", \"_\").replace(\"-\", \"\")}(Enum):'\n                          f\"\\n    NAME = '{class_name}'\\n    {enum_members}\")\n            enum_classes.append(enum_class)\n\n        # Enums in die Datei schreiben\n        data = \"\\n\\n\\n\".join(enum_classes)\n        if len(data) &lt; 12:\n            raise ValueError(\n                \"Invalid Enums Loosing content pleas delete it ur self in the (utils/system/all_functions_enums.py) or add mor new stuff :}\")\n        with open(filepath, 'w') as file:\n            file.write(data)\n\n        print(Style.Bold(Style.BLUE(f\"Enums gespeichert in {filepath}\")))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.disconnect","title":"<code>disconnect(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef disconnect(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.exit_main","title":"<code>exit_main(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef exit_main(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.get_function","title":"<code>get_function(name, **kwargs)</code>","text":"<p>Kwargs for _get_function     metadata:: return the registered function dictionary         stateless: (function_data, None), 0         stateful: (function_data, higher_order_function), 0     state::boolean         specification::str default app</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def get_function(self, name: Enum or tuple, **kwargs):\n    \"\"\"\n    Kwargs for _get_function\n        metadata:: return the registered function dictionary\n            stateless: (function_data, None), 0\n            stateful: (function_data, higher_order_function), 0\n        state::boolean\n            specification::str default app\n    \"\"\"\n    if isinstance(name, tuple):\n        return self._get_function(None, as_str=name, **kwargs)\n    else:\n        return self._get_function(name, **kwargs)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.hide_console","title":"<code>hide_console(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef hide_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.run","title":"<code>run(*args, request=None, running_function_coro=None, **kwargs)</code>","text":"<p>Run a function with support for SSE streaming in both threaded and non-threaded contexts.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def run(self, *args, request=None, running_function_coro=None, **kwargs):\n    \"\"\"\n    Run a function with support for SSE streaming in both\n    threaded and non-threaded contexts.\n    \"\"\"\n    if running_function_coro is None:\n        mn, fn = args[0]\n        if self.functions.get(mn, {}).get(fn, {}).get('request_as_kwarg', False):\n            kwargs[\"request\"] = RequestData.from_dict(request)\n            if 'data' in kwargs and 'data' not in self.functions.get(mn, {}).get(fn, {}).get('params', []):\n                kwargs[\"request\"].data = kwargs[\"request\"].body = kwargs['data']\n                del kwargs['data']\n            if 'form_data' in kwargs and 'form_data' not in self.functions.get(mn, {}).get(fn, {}).get('params',\n                                                                                                       []):\n                kwargs[\"request\"].form_data = kwargs[\"request\"].body = kwargs['form_data']\n                del kwargs['form_data']\n\n    # Create the coroutine\n    coro = running_function_coro or self.a_run_any(*args, **kwargs)\n\n    # Get or create an event loop\n    try:\n        loop = asyncio.get_event_loop()\n        is_running = loop.is_running()\n    except RuntimeError:\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        is_running = False\n\n    # If the loop is already running, run in a separate thread\n    if is_running:\n        # Create thread pool executor as needed\n        if not hasattr(self.__class__, '_executor'):\n            self.__class__._executor = ThreadPoolExecutor(max_workers=4)\n\n        def run_in_new_thread():\n            # Set up a new loop in this thread\n            new_loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(new_loop)\n\n            try:\n                # Run the coroutine\n                return new_loop.run_until_complete(coro)\n            finally:\n                new_loop.close()\n\n        # Run in thread and get result\n        thread_result = self.__class__._executor.submit(run_in_new_thread).result()\n\n        # Handle streaming results from thread\n        if isinstance(thread_result, dict) and thread_result.get(\"is_stream\"):\n            # Create a new SSE stream in the main thread\n            async def stream_from_function():\n                # Re-run the function with direct async access\n                stream_result = await self.a_run_any(*args, **kwargs)\n\n                if (isinstance(stream_result, Result) and\n                    getattr(stream_result.result, 'data_type', None) == \"stream\"):\n                    # Get and forward data from the original generator\n                    original_gen = stream_result.result.data.get(\"generator\")\n                    if inspect.isasyncgen(original_gen):\n                        async for item in original_gen:\n                            yield item\n\n            # Return a new streaming Result\n            return Result.stream(\n                stream_generator=stream_from_function(),\n                headers=thread_result.get(\"headers\", {})\n            )\n\n        result = thread_result\n    else:\n        # Direct execution when loop is not running\n        result = loop.run_until_complete(coro)\n\n    # Process the final result\n    if isinstance(result, Result):\n        if 'debug' in self.id:\n            result.print()\n        if getattr(result.result, 'data_type', None) == \"stream\":\n            return result\n        return result.to_api_result().model_dump(mode='json')\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.run_bg_task","title":"<code>run_bg_task(task, *args, **kwargs)</code>","text":"<p>Runs a coroutine in the background without blocking the caller.</p> <p>This is the primary method for \"fire-and-forget\" async tasks. It schedules the coroutine to run on the application's main event loop.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Callable</code> <p>The coroutine function to run.</p> required <code>*args</code> <p>Arguments to pass to the coroutine function.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the coroutine function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Task]</code> <p>An asyncio.Task object representing the scheduled task, or None if</p> <code>Optional[Task]</code> <p>the task could not be scheduled.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def run_bg_task(self, task: Callable, *args, **kwargs) -&gt; Optional[asyncio.Task]:\n    \"\"\"\n    Runs a coroutine in the background without blocking the caller.\n\n    This is the primary method for \"fire-and-forget\" async tasks. It schedules\n    the coroutine to run on the application's main event loop.\n\n    Args:\n        task: The coroutine function to run.\n        *args: Arguments to pass to the coroutine function.\n        **kwargs: Keyword arguments to pass to the coroutine function.\n\n    Returns:\n        An asyncio.Task object representing the scheduled task, or None if\n        the task could not be scheduled.\n    \"\"\"\n    if not callable(task):\n        self.logger.warning(\"Task passed to run_bg_task is not callable!\")\n        return None\n\n    if not asyncio.iscoroutinefunction(task) and not asyncio.iscoroutine(task):\n        self.logger.warning(f\"Task '{getattr(task, '__name__', 'unknown')}' is not a coroutine. \"\n                            f\"Use run_bg_task_advanced for synchronous functions.\")\n        # Fallback to advanced runner for convenience\n        self.run_bg_task_advanced(task, *args, **kwargs)\n        return None\n\n    try:\n        loop = self.loop_gard()\n        if not loop.is_running():\n            # If the main loop isn't running, we can't create a task on it.\n            # This scenario is handled by run_bg_task_advanced.\n            self.logger.info(\"Main event loop not running. Delegating to advanced background runner.\")\n            return self.run_bg_task_advanced(task, *args, **kwargs)\n\n        # Create the coroutine if it's a function\n        coro = task(*args, **kwargs) if asyncio.iscoroutinefunction(task) else task\n\n        # Create a task on the running event loop\n        bg_task = loop.create_task(coro)\n\n        # Add a callback to log exceptions from the background task\n        def _log_exception(the_task: asyncio.Task):\n            if not the_task.cancelled() and the_task.exception():\n                self.logger.error(f\"Exception in background task '{the_task.get_name()}':\",\n                                  exc_info=the_task.exception())\n\n        bg_task.add_done_callback(_log_exception)\n        self.bg_tasks.append(bg_task)\n        return bg_task\n\n    except Exception as e:\n        self.logger.error(f\"Failed to schedule background task: {e}\", exc_info=True)\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.run_bg_task_advanced","title":"<code>run_bg_task_advanced(task, *args, **kwargs)</code>","text":"<p>Runs a task in a separate, dedicated background thread with its own event loop.</p> <p>This is ideal for: 1. Running an async task from a synchronous context. 2. Launching a long-running, independent operation that should not    interfere with the main application's event loop.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Callable</code> <p>The function to run (can be sync or async).</p> required <code>*args</code> <p>Arguments for the task.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments for the task.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Thread</code> <p>The threading.Thread object managing the background execution.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def run_bg_task_advanced(self, task: Callable, *args, **kwargs) -&gt; threading.Thread:\n    \"\"\"\n    Runs a task in a separate, dedicated background thread with its own event loop.\n\n    This is ideal for:\n    1. Running an async task from a synchronous context.\n    2. Launching a long-running, independent operation that should not\n       interfere with the main application's event loop.\n\n    Args:\n        task: The function to run (can be sync or async).\n        *args: Arguments for the task.\n        **kwargs: Keyword arguments for the task.\n\n    Returns:\n        The threading.Thread object managing the background execution.\n    \"\"\"\n    if not callable(task):\n        self.logger.warning(\"Task for run_bg_task_advanced is not callable!\")\n        return None\n\n    def thread_target():\n        # Each thread gets its own event loop.\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n\n        try:\n            # Prepare the coroutine we need to run\n            if asyncio.iscoroutinefunction(task):\n                coro = task(*args, **kwargs)\n            elif asyncio.iscoroutine(task):\n                # It's already a coroutine object\n                coro = task\n            else:\n                # It's a synchronous function, run it in an executor\n                # to avoid blocking the new event loop.\n                coro = loop.run_in_executor(None, lambda: task(*args, **kwargs))\n\n            # Run the coroutine to completion\n            result = loop.run_until_complete(coro)\n            self.logger.debug(f\"Advanced background task '{getattr(task, '__name__', 'unknown')}' completed.\")\n            if result is not None:\n                self.logger.debug(f\"Task result: {str(result)[:100]}\")\n\n        except Exception as e:\n            self.logger.error(f\"Error in advanced background task '{getattr(task, '__name__', 'unknown')}':\",\n                              exc_info=e)\n        finally:\n            # Cleanly shut down the event loop in this thread.\n            try:\n                all_tasks = asyncio.all_tasks(loop=loop)\n                if all_tasks:\n                    for t in all_tasks:\n                        t.cancel()\n                    loop.run_until_complete(asyncio.gather(*all_tasks, return_exceptions=True))\n            finally:\n                loop.close()\n                asyncio.set_event_loop(None)\n\n    # Create, start, and return the thread.\n    # It's a daemon thread so it won't prevent the main app from exiting.\n    t = threading.Thread(target=thread_target, daemon=True, name=f\"BGTask-{getattr(task, '__name__', 'unknown')}\")\n    self.bg_tasks.append(t)\n    t.start()\n    return t\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.show_console","title":"<code>show_console(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef show_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.tb","title":"<code>tb(name=None, mod_name='', helper='', version=None, test=True, restrict_in_virtual_mode=False, api=False, initial=False, exit_f=False, test_only=False, memory_cache=False, file_cache=False, request_as_kwarg=False, row=False, state=None, level=-1, memory_cache_max_size=100, memory_cache_ttl=300, samples=None, interface=None, pre_compute=None, post_compute=None, api_methods=None)</code>","text":"<p>A decorator for registering and configuring functions within a module.</p> <p>This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to register the function under. Defaults to the function's own name.</p> <code>None</code> <code>mod_name</code> <code>str</code> <p>The name of the module the function belongs to.</p> <code>''</code> <code>helper</code> <code>str</code> <p>A helper string providing additional information about the function.</p> <code>''</code> <code>version</code> <code>str or None</code> <p>The version of the function or module.</p> <code>None</code> <code>test</code> <code>bool</code> <p>Flag to indicate if the function is for testing purposes.</p> <code>True</code> <code>restrict_in_virtual_mode</code> <code>bool</code> <p>Flag to restrict the function in virtual mode.</p> <code>False</code> <code>api</code> <code>bool</code> <p>Flag to indicate if the function is part of an API.</p> <code>False</code> <code>initial</code> <code>bool</code> <p>Flag to indicate if the function should be executed at initialization.</p> <code>False</code> <code>exit_f</code> <code>bool</code> <p>Flag to indicate if the function should be executed at exit.</p> <code>False</code> <code>test_only</code> <code>bool</code> <p>Flag to indicate if the function should only be used for testing.</p> <code>False</code> <code>memory_cache</code> <code>bool</code> <p>Flag to enable memory caching for the function.</p> <code>False</code> <code>request_as_kwarg</code> <code>bool</code> <p>Flag to get request if the fuction is calld from api.</p> <code>False</code> <code>file_cache</code> <code>bool</code> <p>Flag to enable file caching for the function.</p> <code>False</code> <code>row</code> <code>bool</code> <p>rather to auto wrap the result in Result type default False means no row data aka result type</p> <code>False</code> <code>state</code> <code>bool or None</code> <p>Flag to indicate if the function maintains state.</p> <code>None</code> <code>level</code> <code>int</code> <p>The level of the function, used for prioritization or categorization.</p> <code>-1</code> <code>memory_cache_max_size</code> <code>int</code> <p>Maximum size of the memory cache.</p> <code>100</code> <code>memory_cache_ttl</code> <code>int</code> <p>Time-to-live for the memory cache entries.</p> <code>300</code> <code>samples</code> <code>list or dict or None</code> <p>Samples or examples of function usage.</p> <code>None</code> <code>interface</code> <code>str</code> <p>The interface type for the function.</p> <code>None</code> <code>pre_compute</code> <code>callable</code> <p>A function to be called before the main function.</p> <code>None</code> <code>post_compute</code> <code>callable</code> <p>A function to be called after the main function.</p> <code>None</code> <code>api_methods</code> <code>list[str]</code> <p>default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>function</code> <p>The decorated function with additional processing and registration capabilities.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def tb(self, name=None,\n       mod_name: str = \"\",\n       helper: str = \"\",\n       version: str | None = None,\n       test: bool = True,\n       restrict_in_virtual_mode: bool = False,\n       api: bool = False,\n       initial: bool = False,\n       exit_f: bool = False,\n       test_only: bool = False,\n       memory_cache: bool = False,\n       file_cache: bool = False,\n       request_as_kwarg: bool = False,\n       row: bool = False,\n       state: bool | None = None,\n       level: int = -1,\n       memory_cache_max_size: int = 100,\n       memory_cache_ttl: int = 300,\n       samples: list or dict or None = None,\n       interface: ToolBoxInterfaces or None or str = None,\n       pre_compute=None,\n       post_compute=None,\n       api_methods=None,\n       ):\n    \"\"\"\nA decorator for registering and configuring functions within a module.\n\nThis decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\nArgs:\n    name (str, optional): The name to register the function under. Defaults to the function's own name.\n    mod_name (str, optional): The name of the module the function belongs to.\n    helper (str, optional): A helper string providing additional information about the function.\n    version (str or None, optional): The version of the function or module.\n    test (bool, optional): Flag to indicate if the function is for testing purposes.\n    restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n    api (bool, optional): Flag to indicate if the function is part of an API.\n    initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n    exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n    test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n    memory_cache (bool, optional): Flag to enable memory caching for the function.\n    request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n    file_cache (bool, optional): Flag to enable file caching for the function.\n    row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n    state (bool or None, optional): Flag to indicate if the function maintains state.\n    level (int, optional): The level of the function, used for prioritization or categorization.\n    memory_cache_max_size (int, optional): Maximum size of the memory cache.\n    memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n    samples (list or dict or None, optional): Samples or examples of function usage.\n    interface (str, optional): The interface type for the function.\n    pre_compute (callable, optional): A function to be called before the main function.\n    post_compute (callable, optional): A function to be called after the main function.\n    api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\nReturns:\n    function: The decorated function with additional processing and registration capabilities.\n\"\"\"\n    if interface is None:\n        interface = \"tb\"\n    if test_only and 'test' not in self.id:\n        return lambda *args, **kwargs: args\n    return self._create_decorator(interface,\n                                  name,\n                                  mod_name,\n                                  level=level,\n                                  restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                  helper=helper,\n                                  api=api,\n                                  version=version,\n                                  initial=initial,\n                                  exit_f=exit_f,\n                                  test=test,\n                                  samples=samples,\n                                  state=state,\n                                  pre_compute=pre_compute,\n                                  post_compute=post_compute,\n                                  memory_cache=memory_cache,\n                                  file_cache=file_cache,\n                                  request_as_kwarg=request_as_kwarg,\n                                  row=row,\n                                  api_methods=api_methods,\n                                  memory_cache_max_size=memory_cache_max_size,\n                                  memory_cache_ttl=memory_cache_ttl)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.wait_for_bg_tasks","title":"<code>wait_for_bg_tasks(timeout=None)</code>","text":"<p>Wait for all background tasks to complete.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <p>Maximum time to wait (in seconds) for all tasks to complete.      None means wait indefinitely.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if all tasks completed, False if timeout occurred</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def wait_for_bg_tasks(self, timeout=None):\n    \"\"\"\n    Wait for all background tasks to complete.\n\n    Args:\n        timeout: Maximum time to wait (in seconds) for all tasks to complete.\n                 None means wait indefinitely.\n\n    Returns:\n        bool: True if all tasks completed, False if timeout occurred\n    \"\"\"\n    active_tasks = [t for t in self.bg_tasks if t.is_alive()]\n\n    for task in active_tasks:\n        task.join(timeout=timeout)\n        if task.is_alive():\n            return False\n\n    return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code","title":"<code>Code</code>","text":"Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>class Code:\n\n    @staticmethod\n    def DK():\n        return DEVICE_KEY\n\n    def decode_code(self, encrypted_data, key=None):\n\n        if not isinstance(encrypted_data, str):\n            encrypted_data = str(encrypted_data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.decrypt_symmetric(encrypted_data, key)\n\n    def encode_code(self, data, key=None):\n\n        if not isinstance(data, str):\n            data = str(data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.encrypt_symmetric(data, key)\n\n    @staticmethod\n    def generate_seed() -&gt; int:\n        \"\"\"\n        Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n        Returns:\n            int: Eine zuf\u00e4llige Zahl.\n        \"\"\"\n        return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n\n    @staticmethod\n    def one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n        \"\"\"\n        Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n        Args:\n            text (str): Der zu hashende Text.\n            salt (str): Der Salt-Wert.\n            pepper (str): Der Pepper-Wert.\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            str: Der resultierende Hash-Wert.\n        \"\"\"\n        return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n\n    @staticmethod\n    def generate_symmetric_key(as_str=True) -&gt; str or bytes:\n        \"\"\"\n        Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n        Returns:\n            str: Der generierte Schl\u00fcssel.\n        \"\"\"\n        key = Fernet.generate_key()\n        if as_str:\n            key = key.decode()\n        return key\n\n    @staticmethod\n    def encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        if isinstance(text, str):\n            text = text.encode()\n\n        try:\n            fernet = Fernet(key.encode())\n            return fernet.encrypt(text).decode()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n            return \"Error encrypt\"\n\n    @staticmethod\n    def decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n            to_str (bool): default true returns str if false returns bytes\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n\n        if isinstance(key, str):\n            key = key.encode()\n\n        #try:\n        fernet = Fernet(key)\n        text_b = fernet.decrypt(encrypted_text)\n        if not to_str:\n            return text_b\n        return text_b.decode()\n        # except Exception as e:\n        #     get_logger().error(f\"Error decrypt_symmetric {e}\")\n        #     if not mute:\n        #         raise e\n        #     if not to_str:\n        #         return f\"Error decoding\".encode()\n        #     return f\"Error decoding\"\n\n    @staticmethod\n    def generate_asymmetric_keys() -&gt; (str, str):\n        \"\"\"\n        Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n        Args:\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n        \"\"\"\n        private_key = rsa.generate_private_key(\n            public_exponent=65537,\n            key_size=2048 * 3,\n        )\n        public_key = private_key.public_key()\n\n        # Serialisieren der Schl\u00fcssel\n        pem_private_key = private_key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.NoEncryption()\n        ).decode()\n\n        pem_public_key = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        ).decode()\n\n        return pem_public_key, pem_private_key\n\n    @staticmethod\n    def save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n        \"\"\"\n        Speichert die generierten Schl\u00fcssel in separate Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n        Args:\n            public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n            private_key (str): Der private Schl\u00fcssel im PEM-Format\n            directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n        \"\"\"\n        # Erstelle das Verzeichnis, falls es nicht existiert\n        os.makedirs(directory, exist_ok=True)\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n        encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n        # Speichere den \u00f6ffentlichen Schl\u00fcssel\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        with open(public_key_path, \"w\") as f:\n            f.write(public_key)\n\n        # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n        with open(private_key_path, \"w\") as f:\n            f.write(encrypted_private_key)\n\n        print(\"Saved keys in \", public_key_path)\n\n    @staticmethod\n    def load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n        \"\"\"\n        L\u00e4dt die Schl\u00fcssel aus den Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n        Args:\n            directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n        Raises:\n            FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n        \"\"\"\n        # Pfade zu den Schl\u00fcsseldateien\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n\n        # Pr\u00fcfe ob die Dateien existieren\n        if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n            return \"\", \"\"\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Lade den \u00f6ffentlichen Schl\u00fcssel\n        with open(public_key_path) as f:\n            public_key = f.read()\n\n        # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n        with open(private_key_path) as f:\n            encrypted_private_key = f.read()\n            private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n        return public_key, private_key\n\n    @staticmethod\n    def encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        # try:\n        #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        #  except Exception as e:\n        #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            encrypted = public_key.encrypt(\n                text.encode(),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return encrypted.hex()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_asymmetric {e}\")\n            return \"Invalid\"\n\n    @staticmethod\n    def decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n        Args:\n            encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n            private_key_str (str): Der private Schl\u00fcssel als String.\n\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            decrypted = private_key.decrypt(\n                bytes.fromhex(encrypted_text_hex),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return decrypted.decode()\n\n        except Exception as e:\n            get_logger().error(f\"Error decrypt_asymmetric {e}\")\n        return \"Invalid\"\n\n    @staticmethod\n    def verify_signature(signature: str or bytes, message: str or bytes, public_key_str: str,\n                         salt_length=padding.PSS.MAX_LENGTH) -&gt; bool:\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                padding=padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                algorithm=hashes.SHA512()\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def verify_signature_web_algo(signature: str or bytes, message: str or bytes, public_key_str: str,\n                                  algo: int = -512) -&gt; bool:\n        signature_algorithm = ECDSA(hashes.SHA512())\n        if algo != -512:\n            signature_algorithm = ECDSA(hashes.SHA256())\n\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                # padding=padding.PSS(\n                #    mgf=padding.MGF1(hashes.SHA512()),\n                #    salt_length=padding.PSS.MAX_LENGTH\n                # ),\n                signature_algorithm=signature_algorithm\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def create_signature(message: str, private_key_str: str, salt_length=padding.PSS.MAX_LENGTH,\n                         row=False) -&gt; str or bytes:\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            signature = private_key.sign(\n                message.encode(),\n                padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                hashes.SHA512()\n            )\n            if row:\n                return signature\n            return base64.b64encode(signature).decode()\n        except Exception as e:\n            get_logger().error(f\"Error create_signature {e}\")\n            print(e)\n        return \"Invalid Key\"\n\n    @staticmethod\n    def pem_to_public_key(pem_key: str):\n        \"\"\"\n        Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n        Args:\n            pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n        Returns:\n            PublicKey: Das PublicKey-Objekt.\n        \"\"\"\n        public_key = serialization.load_pem_public_key(pem_key.encode())\n        return public_key\n\n    @staticmethod\n    def public_key_to_pem(public_key: RSAPublicKey):\n        \"\"\"\n        Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n        Args:\n            public_key (PublicKey): Das PublicKey-Objekt.\n\n        Returns:\n            str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n        \"\"\"\n        pem = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n        return pem.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.decrypt_asymmetric","title":"<code>decrypt_asymmetric(encrypted_text_hex, private_key_str)</code>  <code>staticmethod</code>","text":"<p>Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text_hex</code> <code>str</code> <p>Der verschl\u00fcsselte Text als Hex-String.</p> required <code>private_key_str</code> <code>str</code> <p>Der private Schl\u00fcssel als String.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n    Args:\n        encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n        private_key_str (str): Der private Schl\u00fcssel als String.\n\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n    try:\n        private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n        decrypted = private_key.decrypt(\n            bytes.fromhex(encrypted_text_hex),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return decrypted.decode()\n\n    except Exception as e:\n        get_logger().error(f\"Error decrypt_asymmetric {e}\")\n    return \"Invalid\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.decrypt_symmetric","title":"<code>decrypt_symmetric(encrypted_text, key, to_str=True, mute=False)</code>  <code>staticmethod</code>","text":"<p>Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text</code> <code>str</code> <p>Der zu entschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <code>to_str</code> <code>bool</code> <p>default true returns str if false returns bytes</p> <code>True</code> <p>Returns:     str: Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n        to_str (bool): default true returns str if false returns bytes\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n\n    if isinstance(key, str):\n        key = key.encode()\n\n    #try:\n    fernet = Fernet(key)\n    text_b = fernet.decrypt(encrypted_text)\n    if not to_str:\n        return text_b\n    return text_b.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.encrypt_asymmetric","title":"<code>encrypt_asymmetric(text, public_key_str)</code>  <code>staticmethod</code>","text":"<p>Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>public_key_str</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    # try:\n    #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n    #  except Exception as e:\n    #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n    try:\n        public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        encrypted = public_key.encrypt(\n            text.encode(),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return encrypted.hex()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        return \"Invalid\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.encrypt_symmetric","title":"<code>encrypt_symmetric(text, key)</code>  <code>staticmethod</code>","text":"<p>Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    if isinstance(text, str):\n        text = text.encode()\n\n    try:\n        fernet = Fernet(key.encode())\n        return fernet.encrypt(text).decode()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n        return \"Error encrypt\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.generate_asymmetric_keys","title":"<code>generate_asymmetric_keys()</code>  <code>staticmethod</code>","text":"<p>Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_asymmetric_keys() -&gt; (str, str):\n    \"\"\"\n    Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n    Args:\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n    \"\"\"\n    private_key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048 * 3,\n    )\n    public_key = private_key.public_key()\n\n    # Serialisieren der Schl\u00fcssel\n    pem_private_key = private_key.private_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PrivateFormat.PKCS8,\n        encryption_algorithm=serialization.NoEncryption()\n    ).decode()\n\n    pem_public_key = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    ).decode()\n\n    return pem_public_key, pem_private_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.generate_seed","title":"<code>generate_seed()</code>  <code>staticmethod</code>","text":"<p>Erzeugt eine zuf\u00e4llige Zahl als Seed.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Eine zuf\u00e4llige Zahl.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_seed() -&gt; int:\n    \"\"\"\n    Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n    Returns:\n        int: Eine zuf\u00e4llige Zahl.\n    \"\"\"\n    return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.generate_symmetric_key","title":"<code>generate_symmetric_key(as_str=True)</code>  <code>staticmethod</code>","text":"<p>Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str or bytes</code> <p>Der generierte Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_symmetric_key(as_str=True) -&gt; str or bytes:\n    \"\"\"\n    Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n    Returns:\n        str: Der generierte Schl\u00fcssel.\n    \"\"\"\n    key = Fernet.generate_key()\n    if as_str:\n        key = key.decode()\n    return key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.load_keys_from_files","title":"<code>load_keys_from_files(directory='keys')</code>  <code>staticmethod</code>","text":"<p>L\u00e4dt die Schl\u00fcssel aus den Dateien. Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen</p> <code>'keys'</code> <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n    \"\"\"\n    L\u00e4dt die Schl\u00fcssel aus den Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n    Args:\n        directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n    Raises:\n        FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n    \"\"\"\n    # Pfade zu den Schl\u00fcsseldateien\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n\n    # Pr\u00fcfe ob die Dateien existieren\n    if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n        return \"\", \"\"\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Lade den \u00f6ffentlichen Schl\u00fcssel\n    with open(public_key_path) as f:\n        public_key = f.read()\n\n    # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n    with open(private_key_path) as f:\n        encrypted_private_key = f.read()\n        private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n    return public_key, private_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.one_way_hash","title":"<code>one_way_hash(text, salt='', pepper='')</code>  <code>staticmethod</code>","text":"<p>Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu hashende Text.</p> required <code>salt</code> <code>str</code> <p>Der Salt-Wert.</p> <code>''</code> <code>pepper</code> <code>str</code> <p>Der Pepper-Wert.</p> <code>''</code> <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der resultierende Hash-Wert.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n    \"\"\"\n    Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n    Args:\n        text (str): Der zu hashende Text.\n        salt (str): Der Salt-Wert.\n        pepper (str): Der Pepper-Wert.\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        str: Der resultierende Hash-Wert.\n    \"\"\"\n    return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.pem_to_public_key","title":"<code>pem_to_public_key(pem_key)</code>  <code>staticmethod</code>","text":"<p>Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.</p> <p>Parameters:</p> Name Type Description Default <code>pem_key</code> <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef pem_to_public_key(pem_key: str):\n    \"\"\"\n    Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n    Args:\n        pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n    Returns:\n        PublicKey: Das PublicKey-Objekt.\n    \"\"\"\n    public_key = serialization.load_pem_public_key(pem_key.encode())\n    return public_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.public_key_to_pem","title":"<code>public_key_to_pem(public_key)</code>  <code>staticmethod</code>","text":"<p>Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef public_key_to_pem(public_key: RSAPublicKey):\n    \"\"\"\n    Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n    Args:\n        public_key (PublicKey): Das PublicKey-Objekt.\n\n    Returns:\n        str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n    \"\"\"\n    pem = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    )\n    return pem.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.save_keys_to_files","title":"<code>save_keys_to_files(public_key, private_key, directory='keys')</code>  <code>staticmethod</code>","text":"<p>Speichert die generierten Schl\u00fcssel in separate Dateien. Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel im PEM-Format</p> required <code>private_key</code> <code>str</code> <p>Der private Schl\u00fcssel im PEM-Format</p> required <code>directory</code> <code>str</code> <p>Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen</p> <code>'keys'</code> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n    \"\"\"\n    Speichert die generierten Schl\u00fcssel in separate Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n    Args:\n        public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n        private_key (str): Der private Schl\u00fcssel im PEM-Format\n        directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n    \"\"\"\n    # Erstelle das Verzeichnis, falls es nicht existiert\n    os.makedirs(directory, exist_ok=True)\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n    encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n    # Speichere den \u00f6ffentlichen Schl\u00fcssel\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    with open(public_key_path, \"w\") as f:\n        f.write(public_key)\n\n    # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n    with open(private_key_path, \"w\") as f:\n        f.write(encrypted_private_key)\n\n    print(\"Saved keys in \", public_key_path)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.MainTool","title":"<code>MainTool</code>","text":"Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>class MainTool:\n    toolID: str = \"\"\n    # app = None\n    interface = None\n    spec = \"app\"\n    name = \"\"\n    color = \"Bold\"\n    stuf = False\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.__storedargs = args, kwargs\n        self.tools = kwargs.get(\"tool\", {})\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n        if \"on_exit\" in kwargs and isinstance(kwargs.get(\"on_exit\"), Callable):\n            self.on_exit =self.app.tb(\n                mod_name=self.name,\n                name=kwargs.get(\"on_exit\").__name__,\n                version=self.version if hasattr(self, 'version') else \"0.0.0\",\n            )(kwargs.get(\"on_exit\"))\n        self.async_initialized = False\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    pass\n                else:\n                    self.todo()\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\"Error loading mod {self.name} {e}\")\n                if self.app.debug:\n                    import traceback\n                    traceback.print_exc()\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n\n    async def __ainit__(self, *args, **kwargs):\n        self.version = kwargs[\"v\"]\n        self.tools = kwargs.get(\"tool\", {})\n        self.name = kwargs[\"name\"]\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", None))\n        if not hasattr(self, 'config'):\n            self.config = {}\n        self.user = None\n        self.description = \"A toolbox mod\" if kwargs.get(\"description\") is None else kwargs.get(\"description\")\n        if MainTool.interface is None:\n            MainTool.interface = self.app.interface_type\n        # Result.default(self.app.interface)\n\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    await self.todo()\n                else:\n                    pass\n                await asyncio.sleep(0.1)\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\"Error loading mod {self.name} {e}\")\n                if self.app.debug:\n                    import traceback\n                    traceback.print_exc()\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n        self.app.print(f\"TOOL : {self.spec}.{self.name} online\")\n\n\n\n    @property\n    def app(self):\n        return get_app(\n            from_=f\"{self.spec}.{self.name}|{self.toolID if self.toolID else '*' + MainTool.toolID} {self.interface if self.interface else MainTool.interface}\")\n\n    @app.setter\n    def app(self, v):\n        raise PermissionError(f\"You cannot set the App Instance! {v=}\")\n\n    @staticmethod\n    def return_result(error: ToolBoxError = ToolBoxError.none,\n                      exec_code: int = 0,\n                      help_text: str = \"\",\n                      data_info=None,\n                      data=None,\n                      data_to=None):\n\n        if data_to is None:\n            data_to = MainTool.interface if MainTool.interface is not None else ToolBoxInterfaces.cli\n\n        if data is None:\n            data = {}\n\n        if data_info is None:\n            data_info = {}\n\n        return Result(\n            error,\n            ToolBoxResult(data_info=data_info, data=data, data_to=data_to),\n            ToolBoxInfo(exec_code=exec_code, help_text=help_text)\n        )\n\n    def print(self, message, end=\"\\n\", **kwargs):\n        if self.stuf:\n            return\n\n        self.app.print(Style.style_dic[self.color] + self.name + Style.style_dic[\"END\"] + \":\", message, end=end,\n                       **kwargs)\n\n    def add_str_to_config(self, command):\n        if len(command) != 2:\n            self.logger.error('Invalid command must be key value')\n            return False\n        self.config[command[0]] = command[1]\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n\n    def get_version(self) -&gt; str:\n        \"\"\"\"Returns the version\"\"\"\n        return self.version\n\n    async def get_user(self, username: str) -&gt; Result:\n        return await self.app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME, username=username, get_results=True)\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.MainTool.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.__storedargs = args, kwargs\n    self.tools = kwargs.get(\"tool\", {})\n    self.logger = kwargs.get(\"logs\", get_logger())\n    self.color = kwargs.get(\"color\", \"WHITE\")\n    self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n    if \"on_exit\" in kwargs and isinstance(kwargs.get(\"on_exit\"), Callable):\n        self.on_exit =self.app.tb(\n            mod_name=self.name,\n            name=kwargs.get(\"on_exit\").__name__,\n            version=self.version if hasattr(self, 'version') else \"0.0.0\",\n        )(kwargs.get(\"on_exit\"))\n    self.async_initialized = False\n    if self.todo:\n        try:\n            if inspect.iscoroutinefunction(self.todo):\n                pass\n            else:\n                self.todo()\n            get_logger().info(f\"{self.name} on load suspended\")\n        except Exception as e:\n            get_logger().error(f\"Error loading mod {self.name} {e}\")\n            if self.app.debug:\n                import traceback\n                traceback.print_exc()\n    else:\n        get_logger().info(f\"{self.name} no load require\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.MainTool.__initobj","title":"<code>__initobj()</code>  <code>async</code>","text":"<p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.MainTool.get_version","title":"<code>get_version()</code>","text":"<p>\"Returns the version</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\"Returns the version\"\"\"\n    return self.version\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.MainTool.webInstall","title":"<code>webInstall(user_instance, construct_render)</code>","text":"<p>\"Returns a web installer for the given user instance and construct render template</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def webInstall(self, user_instance, construct_render) -&gt; str:\n    \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result","title":"<code>Result</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class Result:\n    _task = None\n    def __init__(self,\n                 error: ToolBoxError,\n                 result: ToolBoxResult,\n                 info: ToolBoxInfo,\n                 origin: Any | None = None,\n                 ):\n        self.error: ToolBoxError = error\n        self.result: ToolBoxResult = result\n        self.info: ToolBoxInfo = info\n        self.origin = origin\n\n    def as_result(self):\n        return self\n\n    def as_dict(self):\n        return {\n            \"error\":self.error.value if isinstance(self.error, Enum) else self.error,\n        \"result\" : {\n            \"data_to\":self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n            \"data_info\":self.result.data_info,\n            \"data\":self.result.data,\n            \"data_type\":self.result.data_type\n        } if self.result else None,\n        \"info\" : {\n            \"exec_code\" : self.info.exec_code,  # exec_code umwandel in http resposn codes\n        \"help_text\" : self.info.help_text\n        } if self.info else None,\n        \"origin\" : self.origin\n        }\n\n    def set_origin(self, origin):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = origin\n        return self\n\n    def set_dir_origin(self, name, extras=\"assets/\"):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = f\"mods/{name}/{extras}\"\n        return self\n\n    def is_error(self):\n        if _test_is_result(self.result.data):\n            return self.result.data.is_error()\n        if self.error == ToolBoxError.none:\n            return False\n        if self.info.exec_code == 0:\n            return False\n        if self.info.exec_code == 200:\n            return False\n        return True\n\n    def is_data(self):\n        return self.result.data is not None\n\n    def to_api_result(self):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=self.error.value if isinstance(self.error, Enum) else self.error,\n            result=ToolBoxResultBM(\n                data_to=self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n                data_info=self.result.data_info,\n                data=self.result.data,\n                data_type=self.result.data_type\n            ) if self.result else None,\n            info=ToolBoxInfoBM(\n                exec_code=self.info.exec_code,  # exec_code umwandel in http resposn codes\n                help_text=self.info.help_text\n            ) if self.info else None,\n            origin=self.origin\n        )\n\n    def task(self, task):\n        self._task = task\n        return self\n\n    @staticmethod\n    def result_from_dict(error: str, result: dict, info: dict, origin: list or None or str):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=error if isinstance(error, Enum) else error,\n            result=ToolBoxResultBM(\n                data_to=result.get('data_to') if isinstance(result.get('data_to'), Enum) else result.get('data_to'),\n                data_info=result.get('data_info', '404'),\n                data=result.get('data'),\n                data_type=result.get('data_type', '404'),\n            ) if result else ToolBoxResultBM(\n                data_to=ToolBoxInterfaces.cli.value,\n                data_info='',\n                data='404',\n                data_type='404',\n            ),\n            info=ToolBoxInfoBM(\n                exec_code=info.get('exec_code', 404),\n                help_text=info.get('help_text', '404')\n            ) if info else ToolBoxInfoBM(\n                exec_code=404,\n                help_text='404'\n            ),\n            origin=origin\n        ).as_result()\n\n    @classmethod\n    def stream(cls,\n               stream_generator: Any,  # Renamed from source for clarity\n               content_type: str = \"text/event-stream\",  # Default to SSE\n               headers: Union[dict, None] = None,\n               info: str = \"OK\",\n               interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n               cleanup_func: Union[\n                   Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None):\n        \"\"\"\n        Create a streaming response Result. Handles SSE and other stream types.\n\n        Args:\n            stream_generator: Any stream source (async generator, sync generator, iterable, or single item).\n            content_type: Content-Type header (default: text/event-stream for SSE).\n            headers: Additional HTTP headers for the response.\n            info: Help text for the result.\n            interface: Interface to send data to.\n            cleanup_func: Optional function for cleanup.\n\n        Returns:\n            A Result object configured for streaming.\n        \"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        final_generator: AsyncGenerator[str, None]\n\n        if content_type == \"text/event-stream\":\n            # For SSE, always use SSEGenerator.create_sse_stream to wrap the source.\n            # SSEGenerator.create_sse_stream handles various types of stream_generator internally.\n            final_generator = SSEGenerator.create_sse_stream(source=stream_generator, cleanup_func=cleanup_func)\n\n            # Standard SSE headers for the HTTP response itself\n            # These will be stored in the Result object. Rust side decides how to use them.\n            standard_sse_headers = {\n                \"Cache-Control\": \"no-cache\",  # SSE specific\n                \"Connection\": \"keep-alive\",  # SSE specific\n                \"X-Accel-Buffering\": \"no\",  # Useful for proxies with SSE\n                # Content-Type is implicitly text/event-stream, will be in streaming_data below\n            }\n            all_response_headers = standard_sse_headers.copy()\n            if headers:\n                all_response_headers.update(headers)\n        else:\n            # For non-SSE streams.\n            # If stream_generator is sync, wrap it to be async.\n            # If already async or single item, it will be handled.\n            # Rust's stream_generator in ToolboxClient seems to handle both sync/async Python generators.\n            # For consistency with how SSEGenerator does it, we can wrap sync ones.\n            if inspect.isgenerator(stream_generator) or \\\n                (not isinstance(stream_generator, str) and hasattr(stream_generator, '__iter__')):\n                final_generator = SSEGenerator.wrap_sync_generator(stream_generator)  # Simple async wrapper\n            elif inspect.isasyncgen(stream_generator):\n                final_generator = stream_generator\n            else:  # Single item or string\n                async def _single_item_gen():\n                    yield stream_generator\n\n                final_generator = _single_item_gen()\n            all_response_headers = headers if headers else {}\n\n        # Prepare streaming data to be stored in the Result object\n        streaming_data = {\n            \"type\": \"stream\",  # Indicator for Rust side\n            \"generator\": final_generator,\n            \"content_type\": content_type,  # Let Rust know the intended content type\n            \"headers\": all_response_headers  # Intended HTTP headers for the overall response\n        }\n\n        result_payload = ToolBoxResult(\n            data_to=interface,\n            data=streaming_data,\n            data_info=\"Streaming response\" if content_type != \"text/event-stream\" else \"SSE Event Stream\",\n            data_type=\"stream\"  # Generic type for Rust to identify it needs to stream from 'generator'\n        )\n\n        return cls(error=error, info=info_obj, result=result_payload)\n\n    @classmethod\n    def sse(cls,\n            stream_generator: Any,\n            info: str = \"OK\",\n            interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n            cleanup_func: Union[\n                Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None,\n            # http_headers: Optional[dict] = None # If we want to allow overriding default SSE HTTP headers\n            ):\n        \"\"\"\n        Create an Server-Sent Events (SSE) streaming response Result.\n\n        Args:\n            stream_generator: A source yielding individual data items. This can be an\n                              async generator, sync generator, iterable, or a single item.\n                              Each item will be formatted as an SSE event.\n            info: Optional help text for the Result.\n            interface: Optional ToolBoxInterface to target.\n            cleanup_func: Optional cleanup function to run when the stream ends or is cancelled.\n            #http_headers: Optional dictionary of custom HTTP headers for the SSE response.\n\n        Returns:\n            A Result object configured for SSE streaming.\n        \"\"\"\n        # Result.stream will handle calling SSEGenerator.create_sse_stream\n        # and setting appropriate default headers for SSE when content_type is \"text/event-stream\".\n        return cls.stream(\n            stream_generator=stream_generator,\n            content_type=\"text/event-stream\",\n            # headers=http_headers, # Pass if we add http_headers param\n            info=info,\n            interface=interface,\n            cleanup_func=cleanup_func\n        )\n\n    @classmethod\n    def default(cls, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=-1, help_text=\"\")\n        result = ToolBoxResult(data_to=interface)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None):\n        \"\"\"Create a JSON response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code or exec_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=data,\n            data_info=\"JSON response\",\n            data_type=\"json\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n        \"\"\"Create a text response Result with specific content type.\"\"\"\n        if headers is not None:\n            return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=text_data,\n            data_info=\"Text response\",\n            data_type=content_type\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n               interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a binary data response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        # Create a dictionary with binary data and metadata\n        binary_data = {\n            \"data\": data,\n            \"content_type\": content_type,\n            \"filename\": download_name\n        }\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=binary_data,\n            data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n            data_type=\"binary\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def file(cls, data, filename, content_type=None, info=\"OK\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a file download response Result.\n\n        Args:\n            data: File data as bytes or base64 string\n            filename: Name of the file for download\n            content_type: MIME type of the file (auto-detected if None)\n            info: Response info text\n            interface: Target interface\n\n        Returns:\n            Result object configured for file download\n        \"\"\"\n        import base64\n        import mimetypes\n\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=200, help_text=info)\n\n        # Auto-detect content type if not provided\n        if content_type is None:\n            content_type, _ = mimetypes.guess_type(filename)\n            if content_type is None:\n                content_type = \"application/octet-stream\"\n\n        # Ensure data is base64 encoded string (as expected by Rust server)\n        if isinstance(data, bytes):\n            base64_data = base64.b64encode(data).decode('utf-8')\n        elif isinstance(data, str):\n            # Assume it's already base64 encoded\n            base64_data = data\n        else:\n            raise ValueError(\"File data must be bytes or base64 string\")\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=base64_data,  # Rust expects base64 string for \"file\" type\n            data_info=f\"File download: {filename}\",\n            data_type=\"file\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a redirect response.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=url,\n            data_info=\"Redirect response\",\n            data_type=\"redirect\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def ok(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def html(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.remote, data_type=\"html\",status=200, headers=None, row=False):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=status, help_text=info)\n        from ...utils.system.getting_and_closing_app import get_app\n\n        if not row and not '\"&lt;div class=\"main-content\"\"' in data:\n            data = f'&lt;div class=\"main-content frosted-glass\"&gt;{data}&lt;div&gt;'\n        if not row and not get_app().web_context() in data:\n            data = get_app().web_context() + data\n\n        if isinstance(headers, dict):\n            result = ToolBoxResult(data_to=interface, data={'html':data,'headers':headers}, data_info=data_info,\n                                   data_type=\"special_html\")\n        else:\n            result = ToolBoxResult(data_to=interface, data=data, data_info=data_info,\n                                   data_type=data_type if data_type is not None else type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def future(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.future):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=\"future\")\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def custom_error(cls, data=None, data_info=\"\", info=\"\", exec_code=-1, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def error(cls, data=None, data_info=\"\", info=\"\", exec_code=450, interface=ToolBoxInterfaces.remote):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_user_error(cls, info=\"\", exec_code=-3, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.input_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_internal_error(cls, info=\"\", exec_code=-2, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.internal_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    def print(self, show=True, show_data=True, prifix=\"\"):\n        data = '\\n' + f\"{((prifix + 'Data: ' + str(self.result.data) if self.result.data is not None else 'NO Data') if not isinstance(self.result.data, Result) else self.result.data.print(show=False, show_data=show_data, prifix=prifix + '-')) if show_data else 'Data: private'}\"\n        origin = '\\n' + f\"{prifix + 'Origin: ' + str(self.origin) if self.origin is not None else 'NO Origin'}\"\n        text = (f\"Function Exec code: {self.info.exec_code}\"\n                f\"\\n{prifix}Info's:\"\n                f\" {self.info.help_text} {'&lt;|&gt; ' + str(self.result.data_info) if self.result.data_info is not None else ''}\"\n                f\"{origin}{data if not data.endswith('NO Data') else ''}\")\n        if not show:\n            return text\n        print(\"\\n======== Result ========\\n\" + text + \"\\n------- EndOfD -------\")\n        return self\n\n    def log(self, show_data=True, prifix=\"\"):\n        from toolboxv2 import get_logger\n        get_logger().debug(self.print(show=False, show_data=show_data, prifix=prifix).replace(\"\\n\", \" - \"))\n        return self\n\n    def __str__(self):\n        return self.print(show=False, show_data=True)\n\n    def get(self, key=None, default=None):\n        data = self.result.data\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    async def aget(self, key=None, default=None):\n        if asyncio.isfuture(self.result.data) or asyncio.iscoroutine(self.result.data) or (\n            isinstance(self.result.data_to, Enum) and self.result.data_to.name == ToolBoxInterfaces.future.name):\n            data = await self.result.data\n        else:\n            data = self.get(key=None, default=None)\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    def lazy_return(self, _=0, data=None, **kwargs):\n        flags = ['raise', 'logg', 'user', 'intern']\n        flag = flags[_] if isinstance(_, int) else _\n        if self.info.exec_code == 0:\n            return self if data is None else data if _test_is_result(data) else self.ok(data=data, **kwargs)\n        if flag == 'raise':\n            raise ValueError(self.print(show=False))\n        if flag == 'logg':\n            from .. import get_logger\n            get_logger().error(self.print(show=False))\n\n        if flag == 'user':\n            return self if data is None else data if _test_is_result(data) else self.default_user_error(data=data,\n                                                                                                        **kwargs)\n        if flag == 'intern':\n            return self if data is None else data if _test_is_result(data) else self.default_internal_error(data=data,\n                                                                                                            **kwargs)\n\n        return self if data is None else data if _test_is_result(data) else self.custom_error(data=data, **kwargs)\n\n    @property\n    def bg_task(self):\n        return self._task\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.binary","title":"<code>binary(data, content_type='application/octet-stream', download_name=None, info='OK', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a binary data response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n           interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a binary data response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    # Create a dictionary with binary data and metadata\n    binary_data = {\n        \"data\": data,\n        \"content_type\": content_type,\n        \"filename\": download_name\n    }\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=binary_data,\n        data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n        data_type=\"binary\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.file","title":"<code>file(data, filename, content_type=None, info='OK', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a file download response Result.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>File data as bytes or base64 string</p> required <code>filename</code> <p>Name of the file for download</p> required <code>content_type</code> <p>MIME type of the file (auto-detected if None)</p> <code>None</code> <code>info</code> <p>Response info text</p> <code>'OK'</code> <code>interface</code> <p>Target interface</p> <code>remote</code> <p>Returns:</p> Type Description <p>Result object configured for file download</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef file(cls, data, filename, content_type=None, info=\"OK\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a file download response Result.\n\n    Args:\n        data: File data as bytes or base64 string\n        filename: Name of the file for download\n        content_type: MIME type of the file (auto-detected if None)\n        info: Response info text\n        interface: Target interface\n\n    Returns:\n        Result object configured for file download\n    \"\"\"\n    import base64\n    import mimetypes\n\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=200, help_text=info)\n\n    # Auto-detect content type if not provided\n    if content_type is None:\n        content_type, _ = mimetypes.guess_type(filename)\n        if content_type is None:\n            content_type = \"application/octet-stream\"\n\n    # Ensure data is base64 encoded string (as expected by Rust server)\n    if isinstance(data, bytes):\n        base64_data = base64.b64encode(data).decode('utf-8')\n    elif isinstance(data, str):\n        # Assume it's already base64 encoded\n        base64_data = data\n    else:\n        raise ValueError(\"File data must be bytes or base64 string\")\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=base64_data,  # Rust expects base64 string for \"file\" type\n        data_info=f\"File download: {filename}\",\n        data_type=\"file\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.json","title":"<code>json(data, info='OK', interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None)</code>  <code>classmethod</code>","text":"<p>Create a JSON response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None):\n    \"\"\"Create a JSON response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code or exec_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=data,\n        data_info=\"JSON response\",\n        data_type=\"json\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.redirect","title":"<code>redirect(url, status_code=302, info='Redirect', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a redirect response.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a redirect response.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=url,\n        data_info=\"Redirect response\",\n        data_type=\"redirect\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.sse","title":"<code>sse(stream_generator, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code>  <code>classmethod</code>","text":"<p>Create an Server-Sent Events (SSE) streaming response Result.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <code>Any</code> <p>A source yielding individual data items. This can be an               async generator, sync generator, iterable, or a single item.               Each item will be formatted as an SSE event.</p> required <code>info</code> <code>str</code> <p>Optional help text for the Result.</p> <code>'OK'</code> <code>interface</code> <code>ToolBoxInterfaces</code> <p>Optional ToolBoxInterface to target.</p> <code>remote</code> <code>cleanup_func</code> <code>Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None]</code> <p>Optional cleanup function to run when the stream ends or is cancelled.</p> <code>None</code> <code>#http_headers</code> <p>Optional dictionary of custom HTTP headers for the SSE response.</p> required <p>Returns:</p> Type Description <p>A Result object configured for SSE streaming.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef sse(cls,\n        stream_generator: Any,\n        info: str = \"OK\",\n        interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n        cleanup_func: Union[\n            Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None,\n        # http_headers: Optional[dict] = None # If we want to allow overriding default SSE HTTP headers\n        ):\n    \"\"\"\n    Create an Server-Sent Events (SSE) streaming response Result.\n\n    Args:\n        stream_generator: A source yielding individual data items. This can be an\n                          async generator, sync generator, iterable, or a single item.\n                          Each item will be formatted as an SSE event.\n        info: Optional help text for the Result.\n        interface: Optional ToolBoxInterface to target.\n        cleanup_func: Optional cleanup function to run when the stream ends or is cancelled.\n        #http_headers: Optional dictionary of custom HTTP headers for the SSE response.\n\n    Returns:\n        A Result object configured for SSE streaming.\n    \"\"\"\n    # Result.stream will handle calling SSEGenerator.create_sse_stream\n    # and setting appropriate default headers for SSE when content_type is \"text/event-stream\".\n    return cls.stream(\n        stream_generator=stream_generator,\n        content_type=\"text/event-stream\",\n        # headers=http_headers, # Pass if we add http_headers param\n        info=info,\n        interface=interface,\n        cleanup_func=cleanup_func\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.stream","title":"<code>stream(stream_generator, content_type='text/event-stream', headers=None, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code>  <code>classmethod</code>","text":"<p>Create a streaming response Result. Handles SSE and other stream types.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <code>Any</code> <p>Any stream source (async generator, sync generator, iterable, or single item).</p> required <code>content_type</code> <code>str</code> <p>Content-Type header (default: text/event-stream for SSE).</p> <code>'text/event-stream'</code> <code>headers</code> <code>Union[dict, None]</code> <p>Additional HTTP headers for the response.</p> <code>None</code> <code>info</code> <code>str</code> <p>Help text for the result.</p> <code>'OK'</code> <code>interface</code> <code>ToolBoxInterfaces</code> <p>Interface to send data to.</p> <code>remote</code> <code>cleanup_func</code> <code>Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None]</code> <p>Optional function for cleanup.</p> <code>None</code> <p>Returns:</p> Type Description <p>A Result object configured for streaming.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef stream(cls,\n           stream_generator: Any,  # Renamed from source for clarity\n           content_type: str = \"text/event-stream\",  # Default to SSE\n           headers: Union[dict, None] = None,\n           info: str = \"OK\",\n           interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n           cleanup_func: Union[\n               Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None):\n    \"\"\"\n    Create a streaming response Result. Handles SSE and other stream types.\n\n    Args:\n        stream_generator: Any stream source (async generator, sync generator, iterable, or single item).\n        content_type: Content-Type header (default: text/event-stream for SSE).\n        headers: Additional HTTP headers for the response.\n        info: Help text for the result.\n        interface: Interface to send data to.\n        cleanup_func: Optional function for cleanup.\n\n    Returns:\n        A Result object configured for streaming.\n    \"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    final_generator: AsyncGenerator[str, None]\n\n    if content_type == \"text/event-stream\":\n        # For SSE, always use SSEGenerator.create_sse_stream to wrap the source.\n        # SSEGenerator.create_sse_stream handles various types of stream_generator internally.\n        final_generator = SSEGenerator.create_sse_stream(source=stream_generator, cleanup_func=cleanup_func)\n\n        # Standard SSE headers for the HTTP response itself\n        # These will be stored in the Result object. Rust side decides how to use them.\n        standard_sse_headers = {\n            \"Cache-Control\": \"no-cache\",  # SSE specific\n            \"Connection\": \"keep-alive\",  # SSE specific\n            \"X-Accel-Buffering\": \"no\",  # Useful for proxies with SSE\n            # Content-Type is implicitly text/event-stream, will be in streaming_data below\n        }\n        all_response_headers = standard_sse_headers.copy()\n        if headers:\n            all_response_headers.update(headers)\n    else:\n        # For non-SSE streams.\n        # If stream_generator is sync, wrap it to be async.\n        # If already async or single item, it will be handled.\n        # Rust's stream_generator in ToolboxClient seems to handle both sync/async Python generators.\n        # For consistency with how SSEGenerator does it, we can wrap sync ones.\n        if inspect.isgenerator(stream_generator) or \\\n            (not isinstance(stream_generator, str) and hasattr(stream_generator, '__iter__')):\n            final_generator = SSEGenerator.wrap_sync_generator(stream_generator)  # Simple async wrapper\n        elif inspect.isasyncgen(stream_generator):\n            final_generator = stream_generator\n        else:  # Single item or string\n            async def _single_item_gen():\n                yield stream_generator\n\n            final_generator = _single_item_gen()\n        all_response_headers = headers if headers else {}\n\n    # Prepare streaming data to be stored in the Result object\n    streaming_data = {\n        \"type\": \"stream\",  # Indicator for Rust side\n        \"generator\": final_generator,\n        \"content_type\": content_type,  # Let Rust know the intended content type\n        \"headers\": all_response_headers  # Intended HTTP headers for the overall response\n    }\n\n    result_payload = ToolBoxResult(\n        data_to=interface,\n        data=streaming_data,\n        data_info=\"Streaming response\" if content_type != \"text/event-stream\" else \"SSE Event Stream\",\n        data_type=\"stream\"  # Generic type for Rust to identify it needs to stream from 'generator'\n    )\n\n    return cls(error=error, info=info_obj, result=result_payload)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.text","title":"<code>text(text_data, content_type='text/plain', exec_code=None, status=200, info='OK', interface=ToolBoxInterfaces.remote, headers=None)</code>  <code>classmethod</code>","text":"<p>Create a text response Result with specific content type.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n    \"\"\"Create a text response Result with specific content type.\"\"\"\n    if headers is not None:\n        return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=text_data,\n        data_info=\"Text response\",\n        data_type=content_type\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Singleton","title":"<code>Singleton</code>","text":"<p>Singleton metaclass for ensuring only one instance of a class.</p> Source code in <code>toolboxv2/utils/singelton_class.py</code> <pre><code>class Singleton(type):\n    \"\"\"\n    Singleton metaclass for ensuring only one instance of a class.\n    \"\"\"\n\n    _instances = {}\n    _kwargs = {}\n    _args = {}\n\n    def __call__(cls, *args, **kwargs):\n        if cls not in cls._instances:\n            cls._instances[cls] = super().__call__(*args, **kwargs)\n            cls._args[cls] = args\n            cls._kwargs[cls] = kwargs\n        return cls._instances[cls]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Spinner","title":"<code>Spinner</code>","text":"<p>Enhanced Spinner with tqdm-like line rendering.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>class Spinner:\n    \"\"\"\n    Enhanced Spinner with tqdm-like line rendering.\n    \"\"\"\n    SYMBOL_SETS = {\n        \"c\": [\"\u25d0\", \"\u25d3\", \"\u25d1\", \"\u25d2\"],\n        \"b\": [\"\u2581\", \"\u2583\", \"\u2584\", \"\u2585\", \"\u2586\", \"\u2587\", \"\u2588\", \"\u2587\", \"\u2586\", \"\u2585\", \"\u2584\", \"\u2583\"],\n        \"d\": [\"\u28fe\", \"\u28fd\", \"\u28fb\", \"\u28bf\", \"\u287f\", \"\u28df\", \"\u28ef\", \"\u28f7\"],\n        \"w\": [\"\ud83c\udf0d\", \"\ud83c\udf0e\", \"\ud83c\udf0f\"],\n        \"s\": [\"\ud83c\udf00   \", \" \ud83c\udf00  \", \"  \ud83c\udf00 \", \"   \ud83c\udf00\", \"  \ud83c\udf00 \", \" \ud83c\udf00  \"],\n        \"+\": [\"+\", \"x\"],\n        \"t\": [\"\u2736\", \"\u2738\", \"\u2739\", \"\u273a\", \"\u2739\", \"\u2737\"]\n    }\n\n    def __init__(\n        self,\n        message: str = \"Loading...\",\n        delay: float = 0.1,\n        symbols=None,\n        count_down: bool = False,\n        time_in_s: float = 0\n    ):\n        \"\"\"Initialize spinner with flexible configuration.\"\"\"\n        # Resolve symbol set.\n        if isinstance(symbols, str):\n            symbols = self.SYMBOL_SETS.get(symbols, None)\n\n        # Default symbols if not provided.\n        if symbols is None:\n            symbols = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n\n        # Test mode symbol set.\n        if 'unittest' in sys.argv[0]:\n            symbols = ['#', '=', '-']\n\n        self.spinner = itertools.cycle(symbols)\n        self.delay = delay\n        self.message = message\n        self.running = False\n        self.spinner_thread = None\n        self.max_t = time_in_s\n        self.contd = count_down\n\n        # Rendering management.\n        self._is_primary = False\n        self._start_time = 0\n\n        # Central manager.\n        self.manager = SpinnerManager()\n\n    def _generate_render_line(self):\n        \"\"\"Generate the primary render line.\"\"\"\n        current_time = time.time()\n        if self.contd:\n            remaining = max(0, self.max_t - (current_time - self._start_time))\n            time_display = f\"{remaining:.2f}\"\n        else:\n            time_display = f\"{current_time - self._start_time:.2f}\"\n\n        symbol = next(self.spinner)\n        return f\"{symbol} {self.message} | {time_display}\"\n\n    def _generate_secondary_info(self):\n        \"\"\"Generate secondary spinner info for additional spinners.\"\"\"\n        return f\"{self.message}\"\n\n    def __enter__(self):\n        \"\"\"Start the spinner.\"\"\"\n        self.running = True\n        self._start_time = time.time()\n        self.manager.register_spinner(self)\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        \"\"\"Stop the spinner.\"\"\"\n        self.running = False\n        self.manager.unregister_spinner(self)\n        # Clear the spinner's line if it was the primary spinner.\n        if self._is_primary:\n            sys.stdout.write(\"\\r\\033[K\")\n            sys.stdout.flush()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Spinner.__enter__","title":"<code>__enter__()</code>","text":"<p>Start the spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __enter__(self):\n    \"\"\"Start the spinner.\"\"\"\n    self.running = True\n    self._start_time = time.time()\n    self.manager.register_spinner(self)\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Spinner.__exit__","title":"<code>__exit__(exc_type, exc_value, exc_traceback)</code>","text":"<p>Stop the spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __exit__(self, exc_type, exc_value, exc_traceback):\n    \"\"\"Stop the spinner.\"\"\"\n    self.running = False\n    self.manager.unregister_spinner(self)\n    # Clear the spinner's line if it was the primary spinner.\n    if self._is_primary:\n        sys.stdout.write(\"\\r\\033[K\")\n        sys.stdout.flush()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Spinner.__init__","title":"<code>__init__(message='Loading...', delay=0.1, symbols=None, count_down=False, time_in_s=0)</code>","text":"<p>Initialize spinner with flexible configuration.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __init__(\n    self,\n    message: str = \"Loading...\",\n    delay: float = 0.1,\n    symbols=None,\n    count_down: bool = False,\n    time_in_s: float = 0\n):\n    \"\"\"Initialize spinner with flexible configuration.\"\"\"\n    # Resolve symbol set.\n    if isinstance(symbols, str):\n        symbols = self.SYMBOL_SETS.get(symbols, None)\n\n    # Default symbols if not provided.\n    if symbols is None:\n        symbols = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n\n    # Test mode symbol set.\n    if 'unittest' in sys.argv[0]:\n        symbols = ['#', '=', '-']\n\n    self.spinner = itertools.cycle(symbols)\n    self.delay = delay\n    self.message = message\n    self.running = False\n    self.spinner_thread = None\n    self.max_t = time_in_s\n    self.contd = count_down\n\n    # Rendering management.\n    self._is_primary = False\n    self._start_time = 0\n\n    # Central manager.\n    self.manager = SpinnerManager()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.TBEF","title":"<code>TBEF</code>","text":"<p>Automatic generated by ToolBox v = 0.1.21</p>"},{"location":"toolboxv2/#toolboxv2.utils.daemon","title":"<code>daemon</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.daemon.DaemonUtil","title":"<code>DaemonUtil</code>","text":"Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>class DaemonUtil:\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.server = None\n        self.alive = False\n        self.__storedargs = args, kwargs\n        self.async_initialized = False\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n\n    async def __ainit__(self, class_instance: Any, host='0.0.0.0', port=6587, t=False,\n                        app: (App or AppType) | None = None,\n                        peer=False, name='daemonApp-server', on_register=None, on_client_exit=None, on_server_exit=None,\n                        unix_socket=False, test_override=False):\n        from toolboxv2.mods.SocketManager import SocketType\n        self.class_instance = class_instance\n        self.server = None\n        self.port = port\n        self.host = host\n        self.alive = False\n        self.test_override = test_override\n        self._name = name\n        if on_register is None:\n            def on_register(*args):\n                return None\n        self._on_register = on_register\n        if on_client_exit is None:\n            def on_client_exit(*args):\n                return None\n        self.on_client_exit = on_client_exit\n        if on_server_exit is None:\n            def on_server_exit():\n                return None\n        self.on_server_exit = on_server_exit\n        self.unix_socket = unix_socket\n        self.online = None\n        connection_type = SocketType.server\n        if peer:\n            connection_type = SocketType.peer\n\n        await self.start_server(connection_type)\n        app = app if app is not None else get_app(from_=f\"DaemonUtil.{self._name}\")\n        self.online = await asyncio.to_thread(self.connect, app)\n        if t:\n            await self.online\n\n    async def start_server(self, connection_type=None):\n        \"\"\"Start the server using app and the socket manager\"\"\"\n        from toolboxv2.mods.SocketManager import SocketType\n        if connection_type is None:\n            connection_type = SocketType.server\n        app = get_app(from_=\"Starting.Daemon\")\n        print(app.mod_online(\"SocketManager\"), \"SocketManager\")\n        if not app.mod_online(\"SocketManager\"):\n            await app.load_mod(\"SocketManager\")\n        server_result = await app.a_run_any(SOCKETMANAGER.CREATE_SOCKET,\n                                            get_results=True,\n                                            name=self._name,\n                                            host=self.host,\n                                            port=self.port,\n                                            type_id=connection_type,\n                                            max_connections=-1,\n                                            return_full_object=True,\n                                            test_override=self.test_override,\n                                            unix_file=self.unix_socket)\n        if server_result.is_error():\n            raise Exception(f\"Server error: {server_result.print(False)}\")\n        if not server_result.is_data():\n            raise Exception(f\"Server error: {server_result.print(False)}\")\n        self.alive = True\n        self.server = server_result\n        # 'socket': socket,\n        # 'receiver_socket': r_socket,\n        # 'host': host,\n        # 'port': port,\n        # 'p2p-port': endpoint_port,\n        # 'sender': send,\n        # 'receiver_queue': receiver_queue,\n        # 'connection_error': connection_error,\n        # 'receiver_thread': s_thread,\n        # 'keepalive_thread': keep_alive_thread,\n        # 'running_dict': running_dict,\n        # 'client_to_receiver_thread': to_receive,\n        # 'client_receiver_threads': threeds,\n\n    async def send(self, data: dict or bytes or str, identifier: tuple[str, int] or str = \"main\"):\n        result = await self.server.aget()\n        sender = result.get('sender')\n        await sender(data, identifier)\n        return \"Data Transmitted\"\n\n    @staticmethod\n    async def runner_co(fuction, *args, **kwargs):\n        if asyncio.iscoroutinefunction(fuction):\n            return await fuction(*args, **kwargs)\n        return fuction(*args, **kwargs)\n\n    async def connect(self, app):\n        result = await self.server.aget()\n        if not isinstance(result, dict) or result.get('connection_error') != 0:\n            raise Exception(f\"Server error: {result}\")\n        self.server = Result.ok(result)\n        receiver_queue: queue.Queue = self.server.get('receiver_queue')\n        client_to_receiver_thread = self.server.get('client_to_receiver_thread')\n        running_dict = self.server.get('running_dict')\n        sender = self.server.get('sender')\n        known_clients = {}\n        valid_clients = {}\n        app.print(f\"Starting Demon {self._name}\")\n\n        while self.alive:\n\n            if not receiver_queue.empty():\n                data = receiver_queue.get()\n                if not data:\n                    continue\n                if 'identifier' not in data:\n                    continue\n\n                identifier = data.get('identifier', 'unknown')\n                try:\n                    if identifier == \"new_con\":\n                        client, address = data.get('data')\n                        get_logger().info(f\"New connection: {address}\")\n                        known_clients[str(address)] = client\n                        await client_to_receiver_thread(client, str(address))\n\n                        await self.runner_co(self._on_register, identifier, address)\n                        identifier = str(address)\n                        # await sender({'ok': 0}, identifier)\n\n                    print(\"Receiver queue\", identifier, identifier in known_clients, identifier in valid_clients)\n                    # validation\n                    if identifier in known_clients:\n                        get_logger().info(identifier)\n                        if identifier.startswith(\"('127.0.0.1'\"):\n                            valid_clients[identifier] = known_clients[identifier]\n                            await self.runner_co(self._on_register, identifier, data)\n                        elif data.get(\"claim\", False):\n                            do = app.run_any((\"CloudM.UserInstances\", \"validate_ws_id\"),\n                                             ws_id=data.get(\"claim\"))[0]\n                            get_logger().info(do)\n                            if do:\n                                valid_clients[identifier] = known_clients[identifier]\n                                await self.runner_co(self._on_register, identifier, data)\n                        elif data.get(\"key\", False) == os.getenv(\"TB_R_KEY\"):\n                            valid_clients[identifier] = known_clients[identifier]\n                            await self.runner_co(self._on_register, identifier, data)\n                        else:\n                            get_logger().warning(f\"Validating Failed: {identifier}\")\n                            # sender({'Validating Failed': -1}, eval(identifier))\n                        get_logger().info(f\"Validating New: {identifier}\")\n                        del known_clients[identifier]\n\n                    elif identifier in valid_clients:\n                        get_logger().info(f\"New valid Request: {identifier}\")\n                        name = data.get('name')\n                        args = data.get('args')\n                        kwargs = data.get('kwargs')\n\n                        get_logger().info(f\"Request data: {name=}{args=}{kwargs=}{identifier=}\")\n\n                        if name == 'exit_main':\n                            self.alive = False\n                            break\n\n                        if name == 'show_console':\n                            show_console(True)\n                            await sender({'ok': 0}, identifier)\n                            continue\n\n                        if name == 'hide_console':\n                            show_console(False)\n                            await sender({'ok': 0}, identifier)\n                            continue\n\n                        if name == 'rrun_flow':\n                            show_console(True)\n                            runnner = self.class_instance.run_flow\n                            threading.Thread(target=runnner, args=args, kwargs=kwargs, daemon=True).start()\n                            await sender({'ok': 0}, identifier)\n                            show_console(False)\n                            continue\n\n                        async def _helper_runner():\n                            try:\n                                attr_f = getattr(self.class_instance, name)\n\n                                if asyncio.iscoroutinefunction(attr_f):\n                                    res = await attr_f(*args, **kwargs)\n                                else:\n                                    res = attr_f(*args, **kwargs)\n\n                                if res is None:\n                                    res = {'data': res}\n                                elif isinstance(res, Result):\n                                    if asyncio.iscoroutine(res.get()) or isinstance(res.get(), asyncio.Task):\n                                        res_ = await res.aget()\n                                        res.result.data = res_\n                                    res = json.loads(res.to_api_result().json())\n                                elif isinstance(res, bytes | dict):\n                                    pass\n                                else:\n                                    res = {'data': 'unsupported type', 'type': str(type(res))}\n\n                                get_logger().info(f\"sending response {res} {type(res)}\")\n\n                                await sender(res, identifier)\n                            except Exception as e:\n                                await sender({\"data\": str(e)}, identifier)\n\n                        await _helper_runner()\n                    else:\n                        print(\"Unknown connection data:\", data)\n\n                except Exception as e:\n                    get_logger().warning(Style.RED(f\"An error occurred on {identifier} {str(e)}\"))\n                    if identifier != \"unknown\":\n                        running_dict[\"receive\"][str(identifier)] = False\n                        await self.runner_co(self.on_client_exit,  identifier)\n            await asyncio.sleep(0.1)\n        running_dict[\"server_receiver\"] = False\n        for x in running_dict[\"receive\"]:\n            running_dict[\"receive\"][x] = False\n        running_dict[\"keep_alive_var\"] = False\n        await self.runner_co(self.on_server_exit)\n        app.print(f\"Closing Demon {self._name}\")\n        return Result.ok()\n\n    async def a_exit(self):\n        result = await self.server.aget()\n        await result.get(\"close\")()\n        self.alive = False\n        if asyncio.iscoroutine(self.online):\n            await self.online\n        print(\"Connection result :\", result.get(\"host\"), result.get(\"port\"),\n              \"total connections:\", result.get(\"connections\"))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.daemon.DaemonUtil.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.server = None\n    self.alive = False\n    self.__storedargs = args, kwargs\n    self.async_initialized = False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.daemon.DaemonUtil.__initobj","title":"<code>__initobj()</code>  <code>async</code>","text":"<p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.daemon.DaemonUtil.start_server","title":"<code>start_server(connection_type=None)</code>  <code>async</code>","text":"<p>Start the server using app and the socket manager</p> Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>async def start_server(self, connection_type=None):\n    \"\"\"Start the server using app and the socket manager\"\"\"\n    from toolboxv2.mods.SocketManager import SocketType\n    if connection_type is None:\n        connection_type = SocketType.server\n    app = get_app(from_=\"Starting.Daemon\")\n    print(app.mod_online(\"SocketManager\"), \"SocketManager\")\n    if not app.mod_online(\"SocketManager\"):\n        await app.load_mod(\"SocketManager\")\n    server_result = await app.a_run_any(SOCKETMANAGER.CREATE_SOCKET,\n                                        get_results=True,\n                                        name=self._name,\n                                        host=self.host,\n                                        port=self.port,\n                                        type_id=connection_type,\n                                        max_connections=-1,\n                                        return_full_object=True,\n                                        test_override=self.test_override,\n                                        unix_file=self.unix_socket)\n    if server_result.is_error():\n        raise Exception(f\"Server error: {server_result.print(False)}\")\n    if not server_result.is_data():\n        raise Exception(f\"Server error: {server_result.print(False)}\")\n    self.alive = True\n    self.server = server_result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.daemon.daemon_util","title":"<code>daemon_util</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.daemon.daemon_util.DaemonUtil","title":"<code>DaemonUtil</code>","text":"Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>class DaemonUtil:\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.server = None\n        self.alive = False\n        self.__storedargs = args, kwargs\n        self.async_initialized = False\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n\n    async def __ainit__(self, class_instance: Any, host='0.0.0.0', port=6587, t=False,\n                        app: (App or AppType) | None = None,\n                        peer=False, name='daemonApp-server', on_register=None, on_client_exit=None, on_server_exit=None,\n                        unix_socket=False, test_override=False):\n        from toolboxv2.mods.SocketManager import SocketType\n        self.class_instance = class_instance\n        self.server = None\n        self.port = port\n        self.host = host\n        self.alive = False\n        self.test_override = test_override\n        self._name = name\n        if on_register is None:\n            def on_register(*args):\n                return None\n        self._on_register = on_register\n        if on_client_exit is None:\n            def on_client_exit(*args):\n                return None\n        self.on_client_exit = on_client_exit\n        if on_server_exit is None:\n            def on_server_exit():\n                return None\n        self.on_server_exit = on_server_exit\n        self.unix_socket = unix_socket\n        self.online = None\n        connection_type = SocketType.server\n        if peer:\n            connection_type = SocketType.peer\n\n        await self.start_server(connection_type)\n        app = app if app is not None else get_app(from_=f\"DaemonUtil.{self._name}\")\n        self.online = await asyncio.to_thread(self.connect, app)\n        if t:\n            await self.online\n\n    async def start_server(self, connection_type=None):\n        \"\"\"Start the server using app and the socket manager\"\"\"\n        from toolboxv2.mods.SocketManager import SocketType\n        if connection_type is None:\n            connection_type = SocketType.server\n        app = get_app(from_=\"Starting.Daemon\")\n        print(app.mod_online(\"SocketManager\"), \"SocketManager\")\n        if not app.mod_online(\"SocketManager\"):\n            await app.load_mod(\"SocketManager\")\n        server_result = await app.a_run_any(SOCKETMANAGER.CREATE_SOCKET,\n                                            get_results=True,\n                                            name=self._name,\n                                            host=self.host,\n                                            port=self.port,\n                                            type_id=connection_type,\n                                            max_connections=-1,\n                                            return_full_object=True,\n                                            test_override=self.test_override,\n                                            unix_file=self.unix_socket)\n        if server_result.is_error():\n            raise Exception(f\"Server error: {server_result.print(False)}\")\n        if not server_result.is_data():\n            raise Exception(f\"Server error: {server_result.print(False)}\")\n        self.alive = True\n        self.server = server_result\n        # 'socket': socket,\n        # 'receiver_socket': r_socket,\n        # 'host': host,\n        # 'port': port,\n        # 'p2p-port': endpoint_port,\n        # 'sender': send,\n        # 'receiver_queue': receiver_queue,\n        # 'connection_error': connection_error,\n        # 'receiver_thread': s_thread,\n        # 'keepalive_thread': keep_alive_thread,\n        # 'running_dict': running_dict,\n        # 'client_to_receiver_thread': to_receive,\n        # 'client_receiver_threads': threeds,\n\n    async def send(self, data: dict or bytes or str, identifier: tuple[str, int] or str = \"main\"):\n        result = await self.server.aget()\n        sender = result.get('sender')\n        await sender(data, identifier)\n        return \"Data Transmitted\"\n\n    @staticmethod\n    async def runner_co(fuction, *args, **kwargs):\n        if asyncio.iscoroutinefunction(fuction):\n            return await fuction(*args, **kwargs)\n        return fuction(*args, **kwargs)\n\n    async def connect(self, app):\n        result = await self.server.aget()\n        if not isinstance(result, dict) or result.get('connection_error') != 0:\n            raise Exception(f\"Server error: {result}\")\n        self.server = Result.ok(result)\n        receiver_queue: queue.Queue = self.server.get('receiver_queue')\n        client_to_receiver_thread = self.server.get('client_to_receiver_thread')\n        running_dict = self.server.get('running_dict')\n        sender = self.server.get('sender')\n        known_clients = {}\n        valid_clients = {}\n        app.print(f\"Starting Demon {self._name}\")\n\n        while self.alive:\n\n            if not receiver_queue.empty():\n                data = receiver_queue.get()\n                if not data:\n                    continue\n                if 'identifier' not in data:\n                    continue\n\n                identifier = data.get('identifier', 'unknown')\n                try:\n                    if identifier == \"new_con\":\n                        client, address = data.get('data')\n                        get_logger().info(f\"New connection: {address}\")\n                        known_clients[str(address)] = client\n                        await client_to_receiver_thread(client, str(address))\n\n                        await self.runner_co(self._on_register, identifier, address)\n                        identifier = str(address)\n                        # await sender({'ok': 0}, identifier)\n\n                    print(\"Receiver queue\", identifier, identifier in known_clients, identifier in valid_clients)\n                    # validation\n                    if identifier in known_clients:\n                        get_logger().info(identifier)\n                        if identifier.startswith(\"('127.0.0.1'\"):\n                            valid_clients[identifier] = known_clients[identifier]\n                            await self.runner_co(self._on_register, identifier, data)\n                        elif data.get(\"claim\", False):\n                            do = app.run_any((\"CloudM.UserInstances\", \"validate_ws_id\"),\n                                             ws_id=data.get(\"claim\"))[0]\n                            get_logger().info(do)\n                            if do:\n                                valid_clients[identifier] = known_clients[identifier]\n                                await self.runner_co(self._on_register, identifier, data)\n                        elif data.get(\"key\", False) == os.getenv(\"TB_R_KEY\"):\n                            valid_clients[identifier] = known_clients[identifier]\n                            await self.runner_co(self._on_register, identifier, data)\n                        else:\n                            get_logger().warning(f\"Validating Failed: {identifier}\")\n                            # sender({'Validating Failed': -1}, eval(identifier))\n                        get_logger().info(f\"Validating New: {identifier}\")\n                        del known_clients[identifier]\n\n                    elif identifier in valid_clients:\n                        get_logger().info(f\"New valid Request: {identifier}\")\n                        name = data.get('name')\n                        args = data.get('args')\n                        kwargs = data.get('kwargs')\n\n                        get_logger().info(f\"Request data: {name=}{args=}{kwargs=}{identifier=}\")\n\n                        if name == 'exit_main':\n                            self.alive = False\n                            break\n\n                        if name == 'show_console':\n                            show_console(True)\n                            await sender({'ok': 0}, identifier)\n                            continue\n\n                        if name == 'hide_console':\n                            show_console(False)\n                            await sender({'ok': 0}, identifier)\n                            continue\n\n                        if name == 'rrun_flow':\n                            show_console(True)\n                            runnner = self.class_instance.run_flow\n                            threading.Thread(target=runnner, args=args, kwargs=kwargs, daemon=True).start()\n                            await sender({'ok': 0}, identifier)\n                            show_console(False)\n                            continue\n\n                        async def _helper_runner():\n                            try:\n                                attr_f = getattr(self.class_instance, name)\n\n                                if asyncio.iscoroutinefunction(attr_f):\n                                    res = await attr_f(*args, **kwargs)\n                                else:\n                                    res = attr_f(*args, **kwargs)\n\n                                if res is None:\n                                    res = {'data': res}\n                                elif isinstance(res, Result):\n                                    if asyncio.iscoroutine(res.get()) or isinstance(res.get(), asyncio.Task):\n                                        res_ = await res.aget()\n                                        res.result.data = res_\n                                    res = json.loads(res.to_api_result().json())\n                                elif isinstance(res, bytes | dict):\n                                    pass\n                                else:\n                                    res = {'data': 'unsupported type', 'type': str(type(res))}\n\n                                get_logger().info(f\"sending response {res} {type(res)}\")\n\n                                await sender(res, identifier)\n                            except Exception as e:\n                                await sender({\"data\": str(e)}, identifier)\n\n                        await _helper_runner()\n                    else:\n                        print(\"Unknown connection data:\", data)\n\n                except Exception as e:\n                    get_logger().warning(Style.RED(f\"An error occurred on {identifier} {str(e)}\"))\n                    if identifier != \"unknown\":\n                        running_dict[\"receive\"][str(identifier)] = False\n                        await self.runner_co(self.on_client_exit,  identifier)\n            await asyncio.sleep(0.1)\n        running_dict[\"server_receiver\"] = False\n        for x in running_dict[\"receive\"]:\n            running_dict[\"receive\"][x] = False\n        running_dict[\"keep_alive_var\"] = False\n        await self.runner_co(self.on_server_exit)\n        app.print(f\"Closing Demon {self._name}\")\n        return Result.ok()\n\n    async def a_exit(self):\n        result = await self.server.aget()\n        await result.get(\"close\")()\n        self.alive = False\n        if asyncio.iscoroutine(self.online):\n            await self.online\n        print(\"Connection result :\", result.get(\"host\"), result.get(\"port\"),\n              \"total connections:\", result.get(\"connections\"))\n</code></pre> <code>__init__(*args, **kwargs)</code> \u00b6 <p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.server = None\n    self.alive = False\n    self.__storedargs = args, kwargs\n    self.async_initialized = False\n</code></pre> <code>__initobj()</code> <code>async</code> \u00b6 <p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre> <code>start_server(connection_type=None)</code> <code>async</code> \u00b6 <p>Start the server using app and the socket manager</p> Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>async def start_server(self, connection_type=None):\n    \"\"\"Start the server using app and the socket manager\"\"\"\n    from toolboxv2.mods.SocketManager import SocketType\n    if connection_type is None:\n        connection_type = SocketType.server\n    app = get_app(from_=\"Starting.Daemon\")\n    print(app.mod_online(\"SocketManager\"), \"SocketManager\")\n    if not app.mod_online(\"SocketManager\"):\n        await app.load_mod(\"SocketManager\")\n    server_result = await app.a_run_any(SOCKETMANAGER.CREATE_SOCKET,\n                                        get_results=True,\n                                        name=self._name,\n                                        host=self.host,\n                                        port=self.port,\n                                        type_id=connection_type,\n                                        max_connections=-1,\n                                        return_full_object=True,\n                                        test_override=self.test_override,\n                                        unix_file=self.unix_socket)\n    if server_result.is_error():\n        raise Exception(f\"Server error: {server_result.print(False)}\")\n    if not server_result.is_data():\n        raise Exception(f\"Server error: {server_result.print(False)}\")\n    self.alive = True\n    self.server = server_result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras","title":"<code>extras</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.BaseWidget","title":"<code>BaseWidget</code>","text":"Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>class BaseWidget:\n    def __init__(self, name: str):\n        self.name = name\n        self.openWidgetsIDs = {}\n        self.onReload = []\n        self.iframes = {}\n\n    def register(self, app, fuction, version=None, name=\"get_widget\", level=1, **kwargs):\n        if version is None:\n            version = app.version\n        app.tb(mod_name=self.name, version=version, request_as_kwarg=True, level=level, api=True, name=name, **kwargs)(\n            fuction)\n\n    def modify_iterator(self, iterator, replace):\n        \"\"\"\n        ['a', 'b'] -&gt; [{replace[0]: 'a',..., replace[len(replace)-1]: 'a'},\n        {replace[0]: 'b',..., replace[len(replace)-1]: 'b'}, ]\n        \"\"\"\n\n        for item in iterator:\n            modified_item = {replace[i]: (self.name if replace[i] == \"name\" else '') + item for i in\n                             range(len(replace))}\n            yield modified_item\n\n    def register2reload(self, *functions):\n        for fuction in functions:\n            def x(r):\n                return fuction(request=r)\n            self.onReload.append(x)\n\n    def reload_guard(self, function):\n        c = None\n        if len(self.onReload) == 0:\n            c = function()\n        return c\n\n    async def oa_reload_guard(self, function):\n        c = None\n        if len(self.onReload) == 0:\n            c = await function() if asyncio.iscoroutinefunction(function) else function()\n        return c\n\n    @staticmethod\n    def get_a_group(asset_name, template=None, file_path=None, a_kwargs=None):\n        if a_kwargs is None:\n            raise ValueError(\"a_kwargs must be specified\")\n        return [{'name': asset_name,\n                 'file_path': file_path,\n                 'kwargs': a_kwargs\n                 } if file_path is not None else {'name': asset_name,\n                                                  'template': template,\n                                                  'kwargs': a_kwargs\n                                                  }]\n\n    def group_generator(self, asset_name: str, iterator: iter, template=None, file_path=None, a_kwargs=None):\n        groups = []\n        work_kwargs = a_kwargs\n        for _i, data in enumerate(iterator):\n            if isinstance(data, dict):\n                work_kwargs = {**a_kwargs, **data}\n            groups.append(self.get_a_group(asset_name, template=template, file_path=file_path, a_kwargs=work_kwargs))\n        return groups\n\n    def asset_loder(self, app, name, asset_id, file_path=None, template=None, iterator=None, **kwargs):\n        a_kwargs = {**{\n            'root': f\"/api/{self.name}\",\n            'WidgetID': asset_id},\n                    **kwargs}\n        asset_name = f\"{name}-{asset_id}\"\n        if iterator is None:\n            group = self.get_a_group(asset_name,\n                                     template=template,\n                                     file_path=file_path,\n                                     a_kwargs=a_kwargs)\n        else:\n            group = self.group_generator(asset_name,\n                                         iterator=iterator,\n                                         template=template,\n                                         file_path=file_path,\n                                         a_kwargs=a_kwargs)\n\n        asset = app.run_any(MINIMALHTML.ADD_COLLECTION_TO_GROUP,\n                            group_name=self.name,\n                            collection={'name': f\"{asset_name}\",\n                                        'group': group},\n                            get_results=True)\n        if asset.is_error():\n            app.run_any(MINIMALHTML.ADD_GROUP, command=self.name)\n            asset = app.run_any(MINIMALHTML.ADD_COLLECTION_TO_GROUP,\n                                group_name=self.name,\n                                collection={'name': f\"{self.name}-{asset_name}\",\n                                            'group': group},\n                                get_results=True)\n        return asset\n\n    def generate_html(self, app, name=\"MainWidget\", asset_id=str(uuid.uuid4())[:4]):\n        return app.run_any(MINIMALHTML.GENERATE_HTML,\n                           group_name=self.name,\n                           collection_name=f\"{name}-{asset_id}\")\n\n    def load_widget(self, app, request, name=\"MainWidget\", asset_id=str(uuid.uuid4())[:4]):\n        app.run_any(MINIMALHTML.ADD_GROUP, command=self.name)\n        self.reload(request)\n        html_widget = self.generate_html(app, name, asset_id)\n        return html_widget[0]['html_element']\n\n    @staticmethod\n    async def get_user_from_request(app, request):\n        from toolboxv2.mods.CloudM import User\n        if request is None:\n            return User()\n        return await get_current_user_from_request(app, request)\n\n    @staticmethod\n    def get_s_id(request):\n        from ..system.types import Result\n        if request is None:\n            return Result.default_internal_error(\"No request specified\")\n        return Result.ok(request.session.get('ID', ''))\n\n    def reload(self, request):\n        [_(request) for _ in self.onReload]\n\n    async def oa_reload(self, request):\n        [_(request) if not asyncio.iscoroutinefunction(_) else await _(request) for _ in self.onReload]\n\n    async def get_widget(self, request, **kwargs):\n        raise NotImplementedError\n\n    def hash_wrapper(self, _id, _salt=''):\n        from ..security.cryp import Code\n        return Code.one_way_hash(text=_id, salt=_salt, pepper=self.name)\n\n    def register_iframe(self, iframe_id: str, src: str, width: str = \"100%\", height: str = \"500px\", **kwargs):\n        \"\"\"\n        Registriert einen iframe mit gegebener ID und Quelle\n\n        Args:\n            iframe_id: Eindeutige ID f\u00fcr den iframe\n            src: URL oder Pfad zur Quelle des iframes\n            width: Breite des iframes (default: \"100%\")\n            height: H\u00f6he des iframes (default: \"500px\")\n            **kwargs: Weitere iframe-Attribute\n        \"\"\"\n        iframe_config = {\n            'src': src,\n            'width': width,\n            'height': height,\n            **kwargs\n        }\n        self.iframes[iframe_id] = iframe_config\n\n    def create_iframe_asset(self, app, iframe_id: str, asset_id: str = None):\n        \"\"\"\n        Erstellt ein Asset f\u00fcr einen registrierten iframe\n\n        Args:\n            app: App-Instanz\n            iframe_id: ID des registrierten iframes\n            asset_id: Optional, spezifische Asset-ID\n        \"\"\"\n        if iframe_id not in self.iframes:\n            raise ValueError(f\"iframe mit ID {iframe_id} nicht registriert\")\n\n        if asset_id is None:\n            asset_id = str(uuid.uuid4())[:4]\n\n        iframe_config = self.iframes[iframe_id]\n        iframe_template = \"\"\"\n        &lt;iframe id=\"{iframe_id}\"\n                src=\"{src}\"\n                width=\"{width}\"\n                height=\"{height}\"\n                frameborder=\"0\"\n                {additional_attrs}&gt;&lt;/iframe&gt;\n        \"\"\".strip()\n\n        # Filtere bekannte Attribute heraus und erstelle String f\u00fcr zus\u00e4tzliche Attribute\n        known_attrs = {'src', 'width', 'height'}\n        additional_attrs = ' '.join(\n            f'{k}=\"{v}\"' for k, v in iframe_config.items()\n            if k not in known_attrs\n        )\n\n        iframe_html = iframe_template.format(\n            iframe_id=iframe_id,\n            src=iframe_config['src'],\n            width=iframe_config['width'],\n            height=iframe_config['height'],\n            additional_attrs=additional_attrs\n        )\n\n        return self.asset_loder(\n            app=app,\n            name=f\"iframe-{iframe_id}\",\n            asset_id=asset_id,\n            template=iframe_html\n        )\n\n    def load_iframe(self, app, iframe_id: str, asset_id: str = None):\n        \"\"\"\n        L\u00e4dt einen registrierten iframe und gibt das HTML-Element zur\u00fcck\n\n        Args:\n            app: App-Instanz\n            iframe_id: ID des registrierten iframes\n            asset_id: Optional, spezifische Asset-ID\n        \"\"\"\n        self.create_iframe_asset(app, iframe_id, asset_id)\n        return self.generate_html(app, f\"iframe-{iframe_id}\", asset_id)[0]['html_element']\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.BaseWidget.create_iframe_asset","title":"<code>create_iframe_asset(app, iframe_id, asset_id=None)</code>","text":"<p>Erstellt ein Asset f\u00fcr einen registrierten iframe</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <p>App-Instanz</p> required <code>iframe_id</code> <code>str</code> <p>ID des registrierten iframes</p> required <code>asset_id</code> <code>str</code> <p>Optional, spezifische Asset-ID</p> <code>None</code> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def create_iframe_asset(self, app, iframe_id: str, asset_id: str = None):\n    \"\"\"\n    Erstellt ein Asset f\u00fcr einen registrierten iframe\n\n    Args:\n        app: App-Instanz\n        iframe_id: ID des registrierten iframes\n        asset_id: Optional, spezifische Asset-ID\n    \"\"\"\n    if iframe_id not in self.iframes:\n        raise ValueError(f\"iframe mit ID {iframe_id} nicht registriert\")\n\n    if asset_id is None:\n        asset_id = str(uuid.uuid4())[:4]\n\n    iframe_config = self.iframes[iframe_id]\n    iframe_template = \"\"\"\n    &lt;iframe id=\"{iframe_id}\"\n            src=\"{src}\"\n            width=\"{width}\"\n            height=\"{height}\"\n            frameborder=\"0\"\n            {additional_attrs}&gt;&lt;/iframe&gt;\n    \"\"\".strip()\n\n    # Filtere bekannte Attribute heraus und erstelle String f\u00fcr zus\u00e4tzliche Attribute\n    known_attrs = {'src', 'width', 'height'}\n    additional_attrs = ' '.join(\n        f'{k}=\"{v}\"' for k, v in iframe_config.items()\n        if k not in known_attrs\n    )\n\n    iframe_html = iframe_template.format(\n        iframe_id=iframe_id,\n        src=iframe_config['src'],\n        width=iframe_config['width'],\n        height=iframe_config['height'],\n        additional_attrs=additional_attrs\n    )\n\n    return self.asset_loder(\n        app=app,\n        name=f\"iframe-{iframe_id}\",\n        asset_id=asset_id,\n        template=iframe_html\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.BaseWidget.load_iframe","title":"<code>load_iframe(app, iframe_id, asset_id=None)</code>","text":"<p>L\u00e4dt einen registrierten iframe und gibt das HTML-Element zur\u00fcck</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <p>App-Instanz</p> required <code>iframe_id</code> <code>str</code> <p>ID des registrierten iframes</p> required <code>asset_id</code> <code>str</code> <p>Optional, spezifische Asset-ID</p> <code>None</code> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def load_iframe(self, app, iframe_id: str, asset_id: str = None):\n    \"\"\"\n    L\u00e4dt einen registrierten iframe und gibt das HTML-Element zur\u00fcck\n\n    Args:\n        app: App-Instanz\n        iframe_id: ID des registrierten iframes\n        asset_id: Optional, spezifische Asset-ID\n    \"\"\"\n    self.create_iframe_asset(app, iframe_id, asset_id)\n    return self.generate_html(app, f\"iframe-{iframe_id}\", asset_id)[0]['html_element']\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.BaseWidget.modify_iterator","title":"<code>modify_iterator(iterator, replace)</code>","text":"<p>['a', 'b'] -&gt; [{replace[0]: 'a',..., replace[len(replace)-1]: 'a'}, {replace[0]: 'b',..., replace[len(replace)-1]: 'b'}, ]</p> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def modify_iterator(self, iterator, replace):\n    \"\"\"\n    ['a', 'b'] -&gt; [{replace[0]: 'a',..., replace[len(replace)-1]: 'a'},\n    {replace[0]: 'b',..., replace[len(replace)-1]: 'b'}, ]\n    \"\"\"\n\n    for item in iterator:\n        modified_item = {replace[i]: (self.name if replace[i] == \"name\" else '') + item for i in\n                         range(len(replace))}\n        yield modified_item\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.BaseWidget.register_iframe","title":"<code>register_iframe(iframe_id, src, width='100%', height='500px', **kwargs)</code>","text":"<p>Registriert einen iframe mit gegebener ID und Quelle</p> <p>Parameters:</p> Name Type Description Default <code>iframe_id</code> <code>str</code> <p>Eindeutige ID f\u00fcr den iframe</p> required <code>src</code> <code>str</code> <p>URL oder Pfad zur Quelle des iframes</p> required <code>width</code> <code>str</code> <p>Breite des iframes (default: \"100%\")</p> <code>'100%'</code> <code>height</code> <code>str</code> <p>H\u00f6he des iframes (default: \"500px\")</p> <code>'500px'</code> <code>**kwargs</code> <p>Weitere iframe-Attribute</p> <code>{}</code> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def register_iframe(self, iframe_id: str, src: str, width: str = \"100%\", height: str = \"500px\", **kwargs):\n    \"\"\"\n    Registriert einen iframe mit gegebener ID und Quelle\n\n    Args:\n        iframe_id: Eindeutige ID f\u00fcr den iframe\n        src: URL oder Pfad zur Quelle des iframes\n        width: Breite des iframes (default: \"100%\")\n        height: H\u00f6he des iframes (default: \"500px\")\n        **kwargs: Weitere iframe-Attribute\n    \"\"\"\n    iframe_config = {\n        'src': src,\n        'width': width,\n        'height': height,\n        **kwargs\n    }\n    self.iframes[iframe_id] = iframe_config\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.Style","title":"<code>Style</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.Style.Spinner","title":"<code>Spinner</code>","text":"<p>Enhanced Spinner with tqdm-like line rendering.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>class Spinner:\n    \"\"\"\n    Enhanced Spinner with tqdm-like line rendering.\n    \"\"\"\n    SYMBOL_SETS = {\n        \"c\": [\"\u25d0\", \"\u25d3\", \"\u25d1\", \"\u25d2\"],\n        \"b\": [\"\u2581\", \"\u2583\", \"\u2584\", \"\u2585\", \"\u2586\", \"\u2587\", \"\u2588\", \"\u2587\", \"\u2586\", \"\u2585\", \"\u2584\", \"\u2583\"],\n        \"d\": [\"\u28fe\", \"\u28fd\", \"\u28fb\", \"\u28bf\", \"\u287f\", \"\u28df\", \"\u28ef\", \"\u28f7\"],\n        \"w\": [\"\ud83c\udf0d\", \"\ud83c\udf0e\", \"\ud83c\udf0f\"],\n        \"s\": [\"\ud83c\udf00   \", \" \ud83c\udf00  \", \"  \ud83c\udf00 \", \"   \ud83c\udf00\", \"  \ud83c\udf00 \", \" \ud83c\udf00  \"],\n        \"+\": [\"+\", \"x\"],\n        \"t\": [\"\u2736\", \"\u2738\", \"\u2739\", \"\u273a\", \"\u2739\", \"\u2737\"]\n    }\n\n    def __init__(\n        self,\n        message: str = \"Loading...\",\n        delay: float = 0.1,\n        symbols=None,\n        count_down: bool = False,\n        time_in_s: float = 0\n    ):\n        \"\"\"Initialize spinner with flexible configuration.\"\"\"\n        # Resolve symbol set.\n        if isinstance(symbols, str):\n            symbols = self.SYMBOL_SETS.get(symbols, None)\n\n        # Default symbols if not provided.\n        if symbols is None:\n            symbols = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n\n        # Test mode symbol set.\n        if 'unittest' in sys.argv[0]:\n            symbols = ['#', '=', '-']\n\n        self.spinner = itertools.cycle(symbols)\n        self.delay = delay\n        self.message = message\n        self.running = False\n        self.spinner_thread = None\n        self.max_t = time_in_s\n        self.contd = count_down\n\n        # Rendering management.\n        self._is_primary = False\n        self._start_time = 0\n\n        # Central manager.\n        self.manager = SpinnerManager()\n\n    def _generate_render_line(self):\n        \"\"\"Generate the primary render line.\"\"\"\n        current_time = time.time()\n        if self.contd:\n            remaining = max(0, self.max_t - (current_time - self._start_time))\n            time_display = f\"{remaining:.2f}\"\n        else:\n            time_display = f\"{current_time - self._start_time:.2f}\"\n\n        symbol = next(self.spinner)\n        return f\"{symbol} {self.message} | {time_display}\"\n\n    def _generate_secondary_info(self):\n        \"\"\"Generate secondary spinner info for additional spinners.\"\"\"\n        return f\"{self.message}\"\n\n    def __enter__(self):\n        \"\"\"Start the spinner.\"\"\"\n        self.running = True\n        self._start_time = time.time()\n        self.manager.register_spinner(self)\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        \"\"\"Stop the spinner.\"\"\"\n        self.running = False\n        self.manager.unregister_spinner(self)\n        # Clear the spinner's line if it was the primary spinner.\n        if self._is_primary:\n            sys.stdout.write(\"\\r\\033[K\")\n            sys.stdout.flush()\n</code></pre> <code>__enter__()</code> \u00b6 <p>Start the spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __enter__(self):\n    \"\"\"Start the spinner.\"\"\"\n    self.running = True\n    self._start_time = time.time()\n    self.manager.register_spinner(self)\n    return self\n</code></pre> <code>__exit__(exc_type, exc_value, exc_traceback)</code> \u00b6 <p>Stop the spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __exit__(self, exc_type, exc_value, exc_traceback):\n    \"\"\"Stop the spinner.\"\"\"\n    self.running = False\n    self.manager.unregister_spinner(self)\n    # Clear the spinner's line if it was the primary spinner.\n    if self._is_primary:\n        sys.stdout.write(\"\\r\\033[K\")\n        sys.stdout.flush()\n</code></pre> <code>__init__(message='Loading...', delay=0.1, symbols=None, count_down=False, time_in_s=0)</code> \u00b6 <p>Initialize spinner with flexible configuration.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __init__(\n    self,\n    message: str = \"Loading...\",\n    delay: float = 0.1,\n    symbols=None,\n    count_down: bool = False,\n    time_in_s: float = 0\n):\n    \"\"\"Initialize spinner with flexible configuration.\"\"\"\n    # Resolve symbol set.\n    if isinstance(symbols, str):\n        symbols = self.SYMBOL_SETS.get(symbols, None)\n\n    # Default symbols if not provided.\n    if symbols is None:\n        symbols = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n\n    # Test mode symbol set.\n    if 'unittest' in sys.argv[0]:\n        symbols = ['#', '=', '-']\n\n    self.spinner = itertools.cycle(symbols)\n    self.delay = delay\n    self.message = message\n    self.running = False\n    self.spinner_thread = None\n    self.max_t = time_in_s\n    self.contd = count_down\n\n    # Rendering management.\n    self._is_primary = False\n    self._start_time = 0\n\n    # Central manager.\n    self.manager = SpinnerManager()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.Style.SpinnerManager","title":"<code>SpinnerManager</code>","text":"<p>Manages multiple spinners to ensure tqdm-like line rendering. Automatically captures SIGINT (Ctrl+C) to stop all spinners.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>class SpinnerManager(metaclass=Singleton):\n    \"\"\"\n    Manages multiple spinners to ensure tqdm-like line rendering.\n    Automatically captures SIGINT (Ctrl+C) to stop all spinners.\n    \"\"\"\n    _instance = None\n\n    def __new__(cls):\n        if not cls._instance:\n            cls._instance = super().__new__(cls)\n            cls._instance._init_manager()\n        return cls._instance\n\n    def _init_manager(self):\n        \"\"\"Initialize spinner management resources and register SIGINT handler.\"\"\"\n        self._spinners = []\n        self._lock = threading.Lock()\n        self._render_thread = None\n        self._should_run = False\n        try:\n            signal.signal(signal.SIGINT, self._signal_handler)\n        except ValueError:\n            print(\"Spinner Manager not in the min Thread no signal possible\")\n            pass\n\n    def _signal_handler(self, signum, frame):\n        \"\"\"Handle SIGINT by stopping all spinners gracefully.\"\"\"\n        with self._lock:\n            for spinner in self._spinners:\n                spinner.running = False\n            self._spinners.clear()\n        self._should_run = False\n        sys.stdout.write(\"\\r\\033[K\")  # Clear the spinner's line.\n        sys.stdout.flush()\n        sys.exit(0)\n\n    def register_spinner(self, spinner):\n        \"\"\"Register a new spinner.\"\"\"\n        with self._lock:\n            # First spinner defines the rendering line.\n            if not self._spinners:\n                spinner._is_primary = True\n            self._spinners.append(spinner)\n            # Start rendering if not already running.\n            if not self._should_run:\n                self._should_run = True\n                self._render_thread = threading.Thread(\n                    target=self._render_loop,\n                    daemon=True\n                )\n                self._render_thread.start()\n\n    def unregister_spinner(self, spinner):\n        \"\"\"Unregister a completed spinner.\"\"\"\n        with self._lock:\n            if spinner in self._spinners:\n                self._spinners.remove(spinner)\n\n    def _render_loop(self):\n        \"\"\"Continuous rendering loop for all active spinners.\"\"\"\n        while self._should_run:\n            if not self._spinners:\n                self._should_run = False\n                break\n\n            with self._lock:\n                # Find primary spinner (first registered).\n                primary_spinner = next((s for s in self._spinners if s._is_primary), None)\n\n                if primary_spinner and primary_spinner.running:\n                    # Render in the same line.\n                    render_line = primary_spinner._generate_render_line()\n\n                    # Append additional spinner info if multiple exist.\n                    if len(self._spinners) &gt; 1:\n                        secondary_info = \" | \".join(\n                            s._generate_secondary_info()\n                            for s in self._spinners\n                            if s is not primary_spinner and s.running\n                        )\n                        render_line += f\" [{secondary_info}]\"\n\n                    # Clear line and write.\n                    try:\n                        sys.stdout.write(\"\\r\" + render_line + \"\\033[K\")\n                        sys.stdout.flush()\n                    except Exception:\n                        self._should_run = False\n\n            time.sleep(0.1)  # Render interval.\n</code></pre> <code>register_spinner(spinner)</code> \u00b6 <p>Register a new spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def register_spinner(self, spinner):\n    \"\"\"Register a new spinner.\"\"\"\n    with self._lock:\n        # First spinner defines the rendering line.\n        if not self._spinners:\n            spinner._is_primary = True\n        self._spinners.append(spinner)\n        # Start rendering if not already running.\n        if not self._should_run:\n            self._should_run = True\n            self._render_thread = threading.Thread(\n                target=self._render_loop,\n                daemon=True\n            )\n            self._render_thread.start()\n</code></pre> <code>unregister_spinner(spinner)</code> \u00b6 <p>Unregister a completed spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def unregister_spinner(self, spinner):\n    \"\"\"Unregister a completed spinner.\"\"\"\n    with self._lock:\n        if spinner in self._spinners:\n            self._spinners.remove(spinner)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.base_widget","title":"<code>base_widget</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.base_widget.BaseWidget","title":"<code>BaseWidget</code>","text":"Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>class BaseWidget:\n    def __init__(self, name: str):\n        self.name = name\n        self.openWidgetsIDs = {}\n        self.onReload = []\n        self.iframes = {}\n\n    def register(self, app, fuction, version=None, name=\"get_widget\", level=1, **kwargs):\n        if version is None:\n            version = app.version\n        app.tb(mod_name=self.name, version=version, request_as_kwarg=True, level=level, api=True, name=name, **kwargs)(\n            fuction)\n\n    def modify_iterator(self, iterator, replace):\n        \"\"\"\n        ['a', 'b'] -&gt; [{replace[0]: 'a',..., replace[len(replace)-1]: 'a'},\n        {replace[0]: 'b',..., replace[len(replace)-1]: 'b'}, ]\n        \"\"\"\n\n        for item in iterator:\n            modified_item = {replace[i]: (self.name if replace[i] == \"name\" else '') + item for i in\n                             range(len(replace))}\n            yield modified_item\n\n    def register2reload(self, *functions):\n        for fuction in functions:\n            def x(r):\n                return fuction(request=r)\n            self.onReload.append(x)\n\n    def reload_guard(self, function):\n        c = None\n        if len(self.onReload) == 0:\n            c = function()\n        return c\n\n    async def oa_reload_guard(self, function):\n        c = None\n        if len(self.onReload) == 0:\n            c = await function() if asyncio.iscoroutinefunction(function) else function()\n        return c\n\n    @staticmethod\n    def get_a_group(asset_name, template=None, file_path=None, a_kwargs=None):\n        if a_kwargs is None:\n            raise ValueError(\"a_kwargs must be specified\")\n        return [{'name': asset_name,\n                 'file_path': file_path,\n                 'kwargs': a_kwargs\n                 } if file_path is not None else {'name': asset_name,\n                                                  'template': template,\n                                                  'kwargs': a_kwargs\n                                                  }]\n\n    def group_generator(self, asset_name: str, iterator: iter, template=None, file_path=None, a_kwargs=None):\n        groups = []\n        work_kwargs = a_kwargs\n        for _i, data in enumerate(iterator):\n            if isinstance(data, dict):\n                work_kwargs = {**a_kwargs, **data}\n            groups.append(self.get_a_group(asset_name, template=template, file_path=file_path, a_kwargs=work_kwargs))\n        return groups\n\n    def asset_loder(self, app, name, asset_id, file_path=None, template=None, iterator=None, **kwargs):\n        a_kwargs = {**{\n            'root': f\"/api/{self.name}\",\n            'WidgetID': asset_id},\n                    **kwargs}\n        asset_name = f\"{name}-{asset_id}\"\n        if iterator is None:\n            group = self.get_a_group(asset_name,\n                                     template=template,\n                                     file_path=file_path,\n                                     a_kwargs=a_kwargs)\n        else:\n            group = self.group_generator(asset_name,\n                                         iterator=iterator,\n                                         template=template,\n                                         file_path=file_path,\n                                         a_kwargs=a_kwargs)\n\n        asset = app.run_any(MINIMALHTML.ADD_COLLECTION_TO_GROUP,\n                            group_name=self.name,\n                            collection={'name': f\"{asset_name}\",\n                                        'group': group},\n                            get_results=True)\n        if asset.is_error():\n            app.run_any(MINIMALHTML.ADD_GROUP, command=self.name)\n            asset = app.run_any(MINIMALHTML.ADD_COLLECTION_TO_GROUP,\n                                group_name=self.name,\n                                collection={'name': f\"{self.name}-{asset_name}\",\n                                            'group': group},\n                                get_results=True)\n        return asset\n\n    def generate_html(self, app, name=\"MainWidget\", asset_id=str(uuid.uuid4())[:4]):\n        return app.run_any(MINIMALHTML.GENERATE_HTML,\n                           group_name=self.name,\n                           collection_name=f\"{name}-{asset_id}\")\n\n    def load_widget(self, app, request, name=\"MainWidget\", asset_id=str(uuid.uuid4())[:4]):\n        app.run_any(MINIMALHTML.ADD_GROUP, command=self.name)\n        self.reload(request)\n        html_widget = self.generate_html(app, name, asset_id)\n        return html_widget[0]['html_element']\n\n    @staticmethod\n    async def get_user_from_request(app, request):\n        from toolboxv2.mods.CloudM import User\n        if request is None:\n            return User()\n        return await get_current_user_from_request(app, request)\n\n    @staticmethod\n    def get_s_id(request):\n        from ..system.types import Result\n        if request is None:\n            return Result.default_internal_error(\"No request specified\")\n        return Result.ok(request.session.get('ID', ''))\n\n    def reload(self, request):\n        [_(request) for _ in self.onReload]\n\n    async def oa_reload(self, request):\n        [_(request) if not asyncio.iscoroutinefunction(_) else await _(request) for _ in self.onReload]\n\n    async def get_widget(self, request, **kwargs):\n        raise NotImplementedError\n\n    def hash_wrapper(self, _id, _salt=''):\n        from ..security.cryp import Code\n        return Code.one_way_hash(text=_id, salt=_salt, pepper=self.name)\n\n    def register_iframe(self, iframe_id: str, src: str, width: str = \"100%\", height: str = \"500px\", **kwargs):\n        \"\"\"\n        Registriert einen iframe mit gegebener ID und Quelle\n\n        Args:\n            iframe_id: Eindeutige ID f\u00fcr den iframe\n            src: URL oder Pfad zur Quelle des iframes\n            width: Breite des iframes (default: \"100%\")\n            height: H\u00f6he des iframes (default: \"500px\")\n            **kwargs: Weitere iframe-Attribute\n        \"\"\"\n        iframe_config = {\n            'src': src,\n            'width': width,\n            'height': height,\n            **kwargs\n        }\n        self.iframes[iframe_id] = iframe_config\n\n    def create_iframe_asset(self, app, iframe_id: str, asset_id: str = None):\n        \"\"\"\n        Erstellt ein Asset f\u00fcr einen registrierten iframe\n\n        Args:\n            app: App-Instanz\n            iframe_id: ID des registrierten iframes\n            asset_id: Optional, spezifische Asset-ID\n        \"\"\"\n        if iframe_id not in self.iframes:\n            raise ValueError(f\"iframe mit ID {iframe_id} nicht registriert\")\n\n        if asset_id is None:\n            asset_id = str(uuid.uuid4())[:4]\n\n        iframe_config = self.iframes[iframe_id]\n        iframe_template = \"\"\"\n        &lt;iframe id=\"{iframe_id}\"\n                src=\"{src}\"\n                width=\"{width}\"\n                height=\"{height}\"\n                frameborder=\"0\"\n                {additional_attrs}&gt;&lt;/iframe&gt;\n        \"\"\".strip()\n\n        # Filtere bekannte Attribute heraus und erstelle String f\u00fcr zus\u00e4tzliche Attribute\n        known_attrs = {'src', 'width', 'height'}\n        additional_attrs = ' '.join(\n            f'{k}=\"{v}\"' for k, v in iframe_config.items()\n            if k not in known_attrs\n        )\n\n        iframe_html = iframe_template.format(\n            iframe_id=iframe_id,\n            src=iframe_config['src'],\n            width=iframe_config['width'],\n            height=iframe_config['height'],\n            additional_attrs=additional_attrs\n        )\n\n        return self.asset_loder(\n            app=app,\n            name=f\"iframe-{iframe_id}\",\n            asset_id=asset_id,\n            template=iframe_html\n        )\n\n    def load_iframe(self, app, iframe_id: str, asset_id: str = None):\n        \"\"\"\n        L\u00e4dt einen registrierten iframe und gibt das HTML-Element zur\u00fcck\n\n        Args:\n            app: App-Instanz\n            iframe_id: ID des registrierten iframes\n            asset_id: Optional, spezifische Asset-ID\n        \"\"\"\n        self.create_iframe_asset(app, iframe_id, asset_id)\n        return self.generate_html(app, f\"iframe-{iframe_id}\", asset_id)[0]['html_element']\n</code></pre> <code>create_iframe_asset(app, iframe_id, asset_id=None)</code> \u00b6 <p>Erstellt ein Asset f\u00fcr einen registrierten iframe</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <p>App-Instanz</p> required <code>iframe_id</code> <code>str</code> <p>ID des registrierten iframes</p> required <code>asset_id</code> <code>str</code> <p>Optional, spezifische Asset-ID</p> <code>None</code> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def create_iframe_asset(self, app, iframe_id: str, asset_id: str = None):\n    \"\"\"\n    Erstellt ein Asset f\u00fcr einen registrierten iframe\n\n    Args:\n        app: App-Instanz\n        iframe_id: ID des registrierten iframes\n        asset_id: Optional, spezifische Asset-ID\n    \"\"\"\n    if iframe_id not in self.iframes:\n        raise ValueError(f\"iframe mit ID {iframe_id} nicht registriert\")\n\n    if asset_id is None:\n        asset_id = str(uuid.uuid4())[:4]\n\n    iframe_config = self.iframes[iframe_id]\n    iframe_template = \"\"\"\n    &lt;iframe id=\"{iframe_id}\"\n            src=\"{src}\"\n            width=\"{width}\"\n            height=\"{height}\"\n            frameborder=\"0\"\n            {additional_attrs}&gt;&lt;/iframe&gt;\n    \"\"\".strip()\n\n    # Filtere bekannte Attribute heraus und erstelle String f\u00fcr zus\u00e4tzliche Attribute\n    known_attrs = {'src', 'width', 'height'}\n    additional_attrs = ' '.join(\n        f'{k}=\"{v}\"' for k, v in iframe_config.items()\n        if k not in known_attrs\n    )\n\n    iframe_html = iframe_template.format(\n        iframe_id=iframe_id,\n        src=iframe_config['src'],\n        width=iframe_config['width'],\n        height=iframe_config['height'],\n        additional_attrs=additional_attrs\n    )\n\n    return self.asset_loder(\n        app=app,\n        name=f\"iframe-{iframe_id}\",\n        asset_id=asset_id,\n        template=iframe_html\n    )\n</code></pre> <code>load_iframe(app, iframe_id, asset_id=None)</code> \u00b6 <p>L\u00e4dt einen registrierten iframe und gibt das HTML-Element zur\u00fcck</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <p>App-Instanz</p> required <code>iframe_id</code> <code>str</code> <p>ID des registrierten iframes</p> required <code>asset_id</code> <code>str</code> <p>Optional, spezifische Asset-ID</p> <code>None</code> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def load_iframe(self, app, iframe_id: str, asset_id: str = None):\n    \"\"\"\n    L\u00e4dt einen registrierten iframe und gibt das HTML-Element zur\u00fcck\n\n    Args:\n        app: App-Instanz\n        iframe_id: ID des registrierten iframes\n        asset_id: Optional, spezifische Asset-ID\n    \"\"\"\n    self.create_iframe_asset(app, iframe_id, asset_id)\n    return self.generate_html(app, f\"iframe-{iframe_id}\", asset_id)[0]['html_element']\n</code></pre> <code>modify_iterator(iterator, replace)</code> \u00b6 <p>['a', 'b'] -&gt; [{replace[0]: 'a',..., replace[len(replace)-1]: 'a'}, {replace[0]: 'b',..., replace[len(replace)-1]: 'b'}, ]</p> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def modify_iterator(self, iterator, replace):\n    \"\"\"\n    ['a', 'b'] -&gt; [{replace[0]: 'a',..., replace[len(replace)-1]: 'a'},\n    {replace[0]: 'b',..., replace[len(replace)-1]: 'b'}, ]\n    \"\"\"\n\n    for item in iterator:\n        modified_item = {replace[i]: (self.name if replace[i] == \"name\" else '') + item for i in\n                         range(len(replace))}\n        yield modified_item\n</code></pre> <code>register_iframe(iframe_id, src, width='100%', height='500px', **kwargs)</code> \u00b6 <p>Registriert einen iframe mit gegebener ID und Quelle</p> <p>Parameters:</p> Name Type Description Default <code>iframe_id</code> <code>str</code> <p>Eindeutige ID f\u00fcr den iframe</p> required <code>src</code> <code>str</code> <p>URL oder Pfad zur Quelle des iframes</p> required <code>width</code> <code>str</code> <p>Breite des iframes (default: \"100%\")</p> <code>'100%'</code> <code>height</code> <code>str</code> <p>H\u00f6he des iframes (default: \"500px\")</p> <code>'500px'</code> <code>**kwargs</code> <p>Weitere iframe-Attribute</p> <code>{}</code> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def register_iframe(self, iframe_id: str, src: str, width: str = \"100%\", height: str = \"500px\", **kwargs):\n    \"\"\"\n    Registriert einen iframe mit gegebener ID und Quelle\n\n    Args:\n        iframe_id: Eindeutige ID f\u00fcr den iframe\n        src: URL oder Pfad zur Quelle des iframes\n        width: Breite des iframes (default: \"100%\")\n        height: H\u00f6he des iframes (default: \"500px\")\n        **kwargs: Weitere iframe-Attribute\n    \"\"\"\n    iframe_config = {\n        'src': src,\n        'width': width,\n        'height': height,\n        **kwargs\n    }\n    self.iframes[iframe_id] = iframe_config\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.blobs","title":"<code>blobs</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.blobs.BlobFile","title":"<code>BlobFile</code>","text":"Source code in <code>toolboxv2/utils/extras/blobs.py</code> <pre><code>class BlobFile(io.IOBase):\n    def __init__(self, filename: str, mode: str = 'r', storage: BlobStorage = None, key: str = None,\n                 servers: list[str] = None):\n        if not isinstance(filename, str) or not filename:\n            raise ValueError(\"Filename must be a non-empty string.\")\n        if not filename.startswith('/'): filename = '/' + filename\n        self.filename = filename.lstrip('/\\\\')\n        self.blob_id, self.folder, self.datei = self._path_splitter(self.filename)\n        self.mode = mode\n\n        if storage is None:\n            # In a real app, dependency injection or a global factory would be better\n            # but this provides a fallback for simple scripts.\n            if not servers:\n                from toolboxv2 import get_app\n                storage = get_app(from_=\"BlobStorage\").root_blob_storage\n            else:\n                storage = BlobStorage(servers=servers)\n\n        self.storage = storage\n        self.data_buffer = b\"\"\n        self.key = key\n        if key:\n            try:\n                assert Code.decrypt_symmetric(Code.encrypt_symmetric(b\"test\", key), key, to_str=False) == b\"test\"\n            except Exception:\n                raise ValueError(\"Invalid symmetric key provided.\")\n\n    @staticmethod\n    def _path_splitter(filename):\n        parts = Path(filename).parts\n        if not parts: raise ValueError(\"Filename cannot be empty.\")\n        blob_id = parts[0]\n        if len(parts) == 1: raise ValueError(\"Filename must include a path within the blob, e.g., 'blob_id/file.txt'\")\n        datei = parts[-1]\n        folder = '|'.join(parts[1:-1])\n        return blob_id, folder, datei\n\n    def create(self):\n        self.storage.create_blob(pickle.dumps({}), self.blob_id)\n        return self\n\n    def __enter__(self):\n        try:\n            raw_blob_data = self.storage.read_blob(self.blob_id)\n            blob_content = pickle.loads(raw_blob_data)\n        except (requests.exceptions.HTTPError, EOFError, pickle.UnpicklingError, ConnectionError) as e:\n            if isinstance(e, requests.exceptions.HTTPError) and e.response.status_code == 404:\n                blob_content = {}  # Blob doesn't exist yet, treat as empty\n            elif isinstance(e, (EOFError, pickle.UnpicklingError)):\n                blob_content = {}  # Blob is empty or corrupt, treat as empty for writing\n            else:\n                self.storage.create_blob(blob_id=self.blob_id, data=pickle.dumps({}))\n                blob_content = {}\n\n        if 'r' in self.mode:\n            path_key = self.folder if self.folder else self.datei\n            if self.folder:\n                file_data = blob_content.get(self.folder, {}).get(self.datei)\n            else:\n                file_data = blob_content.get(self.datei)\n\n            if file_data:\n                self.data_buffer = file_data\n                if self.key:\n                    self.data_buffer = Code.decrypt_symmetric(self.data_buffer, self.key, to_str=False)\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if 'w' in self.mode:\n            final_data = self.data_buffer\n            if self.key:\n                final_data = Code.encrypt_symmetric(final_data, self.key)\n\n            try:\n                raw_blob_data = self.storage.read_blob(self.blob_id)\n                blob_content = pickle.loads(raw_blob_data)\n            except Exception:\n                blob_content = {}\n\n            # Safely navigate and create path\n            current_level = blob_content\n            if self.folder:\n                if self.folder not in current_level:\n                    current_level[self.folder] = {}\n                current_level = current_level[self.folder]\n\n            current_level[self.datei] = final_data\n            self.storage.update_blob(self.blob_id, pickle.dumps(blob_content))\n\n\n\n\n    def exists(self) -&gt; bool:\n        \"\"\"\n        Checks if the specific file path exists within the blob without reading its content.\n        This is an efficient, read-only operation.\n\n        Returns:\n            bool: True if the file exists within the blob, False otherwise.\n        \"\"\"\n        try:\n            # Fetch the raw blob data. This leverages the local cache if available.\n            raw_blob_data = self.storage.read_blob(self.blob_id)\n            # Unpickle the directory structure.\n            if raw_blob_data:\n                blob_content = pickle.loads(raw_blob_data)\n            else:\n                return False\n        except (requests.exceptions.HTTPError, EOFError, pickle.UnpicklingError, ConnectionError):\n            # If the blob itself doesn't exist, is empty, or can't be reached,\n            # then the file within it cannot exist.\n            return False\n\n        # Navigate the dictionary to check for the file's existence.\n        current_level = blob_content\n        if self.folder:\n            if self.folder not in current_level:\n                return False\n            current_level = current_level[self.folder]\n\n        return self.datei in current_level\n\n    def clear(self):\n        self.data_buffer = b''\n\n    def write(self, data):\n        if 'w' not in self.mode: raise IOError(\"File not opened in write mode.\")\n        if isinstance(data, str):\n            self.data_buffer += data.encode()\n        elif isinstance(data, bytes):\n            self.data_buffer += data\n        else:\n            raise TypeError(\"write() argument must be str or bytes\")\n\n    def read(self):\n        if 'r' not in self.mode: raise IOError(\"File not opened in read mode.\")\n        return self.data_buffer\n\n    def read_json(self):\n        if 'r' not in self.mode: raise ValueError(\"File not opened in read mode.\")\n        if self.data_buffer == b\"\": return {}\n        return json.loads(self.data_buffer.decode())\n\n    def write_json(self, data):\n        if 'w' not in self.mode: raise ValueError(\"File not opened in write mode.\")\n        self.data_buffer += json.dumps(data).encode()\n\n    def read_pickle(self):\n        if 'r' not in self.mode: raise ValueError(\"File not opened in read mode.\")\n        if self.data_buffer == b\"\": return {}\n        return pickle.loads(self.data_buffer)\n\n    def write_pickle(self, data):\n        if 'w' not in self.mode: raise ValueError(\"File not opened in write mode.\")\n        self.data_buffer += pickle.dumps(data)\n\n    def read_yaml(self):\n        if 'r' not in self.mode: raise ValueError(\"File not opened in read mode.\")\n        if self.data_buffer == b\"\": return {}\n        return yaml.safe_load(self.data_buffer)\n\n    def write_yaml(self, data):\n        if 'w' not in self.mode: raise ValueError(\"File not opened in write mode.\")\n        yaml.dump(data, self)\n</code></pre> <code>exists()</code> \u00b6 <p>Checks if the specific file path exists within the blob without reading its content. This is an efficient, read-only operation.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file exists within the blob, False otherwise.</p> Source code in <code>toolboxv2/utils/extras/blobs.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"\n    Checks if the specific file path exists within the blob without reading its content.\n    This is an efficient, read-only operation.\n\n    Returns:\n        bool: True if the file exists within the blob, False otherwise.\n    \"\"\"\n    try:\n        # Fetch the raw blob data. This leverages the local cache if available.\n        raw_blob_data = self.storage.read_blob(self.blob_id)\n        # Unpickle the directory structure.\n        if raw_blob_data:\n            blob_content = pickle.loads(raw_blob_data)\n        else:\n            return False\n    except (requests.exceptions.HTTPError, EOFError, pickle.UnpicklingError, ConnectionError):\n        # If the blob itself doesn't exist, is empty, or can't be reached,\n        # then the file within it cannot exist.\n        return False\n\n    # Navigate the dictionary to check for the file's existence.\n    current_level = blob_content\n    if self.folder:\n        if self.folder not in current_level:\n            return False\n        current_level = current_level[self.folder]\n\n    return self.datei in current_level\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.blobs.BlobStorage","title":"<code>BlobStorage</code>","text":"<p>A production-ready client for the distributed blob storage server. It handles communication with a list of server instances, manages a local cache, and implements backoff/retry logic for resilience.</p> Source code in <code>toolboxv2/utils/extras/blobs.py</code> <pre><code>class BlobStorage:\n    \"\"\"\n    A production-ready client for the distributed blob storage server.\n    It handles communication with a list of server instances, manages a local cache,\n    and implements backoff/retry logic for resilience.\n    \"\"\"\n\n    def __init__(self, servers: list[str], storage_directory: str = './.data/blob_cache'):\n\n\n        self.servers = servers\n        self.session = requests.Session()\n        self.storage_directory = storage_directory\n        self.blob_ids = []\n        os.makedirs(storage_directory, exist_ok=True)\n\n        # Initialize the consistent hash ring\n        self.hash_ring = ConsistentHashRing()\n        for server in self.servers:\n            self.hash_ring.add_node(server)\n\n    def _make_request(self, method, endpoint, blob_id: str = None, max_retries=2, **kwargs):\n        \"\"\"\n        Makes a resilient HTTP request to the server cluster.\n        - If a blob_id is provided, it uses the consistent hash ring to find the\n          primary server and subsequent backup servers in a predictable order.\n        - If no blob_id is given (e.g., for broadcast actions), it tries servers randomly.\n        - Implements exponential backoff on server errors.\n        \"\"\"\n        if not self.servers:\n            res = requests.Response()\n            res.status_code = 503\n            res.reason = \"No servers available\"\n            return res\n\n        if blob_id:\n            # Get the ordered list of servers for this specific blob\n            preferred_servers = self.hash_ring.get_nodes_for_key(blob_id)\n        else:\n            # For non-specific requests, shuffle all servers\n            preferred_servers = random.sample(self.servers, len(self.servers))\n\n        last_error = None\n        for attempt in range(max_retries):\n            for server in preferred_servers:\n                url = f\"{server.rstrip('/')}{endpoint}\"\n                try:\n                    # In a targeted request, print which server we are trying\n                    response = self.session.request(method, url, timeout=10, **kwargs)\n\n                    if 500 &lt;= response.status_code &lt; 600:\n                        get_logger().warning(f\"Warning: Server {server} returned status {response.status_code}. Retrying...\")\n                        continue\n                    response.raise_for_status()\n                    return response\n                except requests.exceptions.RequestException as e:\n                    last_error = e\n                    get_logger().warning(f\"Warning: Could not connect to server {server}: {e}. Trying next server.\")\n\n            if attempt &lt; max_retries - 1:\n                wait_time = 2 ** (attempt*0.1)\n                get_logger().warning(f\"Warning: All preferred servers failed. Retrying in {wait_time} seconds...\")\n                time.sleep(wait_time)\n                if len(preferred_servers) == 1 and len(self.servers) &gt; 1:\n                    preferred_servers = random.sample(self.servers, len(self.servers))\n\n        raise ConnectionError(f\"Failed to execute request after {max_retries} attempts. Last error: {last_error}\")\n\n\n    def create_blob(self, data: bytes, blob_id=None) -&gt; str:\n        \"\"\"\n        Creates a new blob. The blob_id is calculated client-side by hashing\n        the content, and the data is sent to the correct server determined\n        by the consistent hash ring. This uses a PUT request, making creation\n        idempotent.\n        \"\"\"\n        # The blob ID is the hash of its content, ensuring content-addressable storage.\n        if not blob_id:\n            blob_id = hashlib.sha256(data).hexdigest()\n\n        # Use PUT, as we now know the blob's final ID/URL.\n        # Pass blob_id to _make_request so it uses the hash ring.\n        print(f\"Creating blob {blob_id} on {self._make_request('PUT', f'/blob/{blob_id}',blob_id=blob_id, data=data).status_code}\")\n        # blob_id = response.text\n        self._save_blob_to_cache(blob_id, data)\n        return blob_id\n\n    def read_blob(self, blob_id: str) -&gt; bytes:\n        cached_data = self._load_blob_from_cache(blob_id)\n        if cached_data is not None:\n            return cached_data\n\n        get_logger().info(f\"Info: Blob '{blob_id}' not in cache, fetching from network.\")\n        # Pass blob_id to _make_request to target the correct server(s).\n        response = self._make_request('GET', f'/blob/{blob_id}', blob_id=blob_id)\n\n        blob_data = response.content\n        self._save_blob_to_cache(blob_id, blob_data)\n        return blob_data\n\n    def update_blob(self, blob_id: str, data: bytes):\n        # Pass blob_id to _make_request to target the correct server(s).\n        response = self._make_request('PUT', f'/blob/{blob_id}', blob_id=blob_id, data=data)\n        self._save_blob_to_cache(blob_id, data)\n        return response\n\n    def delete_blob(self, blob_id: str):\n        # Pass blob_id to _make_request to target the correct server(s).\n        self._make_request('DELETE', f'/blob/{blob_id}', blob_id=blob_id)\n        cache_file = self._get_blob_cache_filename(blob_id)\n        if os.path.exists(cache_file):\n            os.remove(cache_file)\n\n    # NOTE: share_blobs and recover_blob are coordination endpoints. They do not\n    # act on a single blob, so they will continue to use the non-targeted (random)\n    # request mode to contact any available server to act as a coordinator.\n    def share_blobs(self, blob_ids: list[str]):\n        get_logger().info(f\"Info: Instructing a server to share blobs for recovery: {blob_ids}\")\n        payload = {\"blob_ids\": blob_ids}\n        # No blob_id passed, will try any server as a coordinator.\n        self._make_request('POST', '/share', json=payload)\n        get_logger().info(\"Info: Sharing command sent successfully.\")\n\n    def recover_blob(self, lost_blob_id: str) -&gt; bytes:\n        get_logger().info(f\"Info: Attempting to recover '{lost_blob_id}' from the cluster.\")\n        payload = {\"blob_id\": lost_blob_id}\n        # No blob_id passed, recovery can be initiated by any server.\n        response = self._make_request('POST', '/recover', json=payload)\n\n        recovered_data = response.content\n        get_logger().info(f\"Info: Successfully recovered blob '{lost_blob_id}'.\")\n        self._save_blob_to_cache(lost_blob_id, recovered_data)\n        return recovered_data\n\n    def _get_blob_cache_filename(self, blob_id: str) -&gt; str:\n        return os.path.join(self.storage_directory, blob_id + '.blobcache')\n\n    def _save_blob_to_cache(self, blob_id: str, data: bytes):\n        if blob_id not in self.blob_ids:\n            self.blob_ids.append(blob_id)\n        with open(self._get_blob_cache_filename(blob_id), 'wb') as f:\n            f.write(data)\n\n    def _load_blob_from_cache(self, blob_id: str) -&gt; bytes | None:\n        cache_file = self._get_blob_cache_filename(blob_id)\n        if not os.path.exists(cache_file):\n            return None\n        with open(cache_file, 'rb') as f:\n            return f.read()\n\n    def exit(self):\n        if len(self.blob_ids) &lt; 5:\n            return\n        for i in range(len(self.servers)//2+1):\n            self.share_blobs(self.blob_ids)\n</code></pre> <code>create_blob(data, blob_id=None)</code> \u00b6 <p>Creates a new blob. The blob_id is calculated client-side by hashing the content, and the data is sent to the correct server determined by the consistent hash ring. This uses a PUT request, making creation idempotent.</p> Source code in <code>toolboxv2/utils/extras/blobs.py</code> <pre><code>def create_blob(self, data: bytes, blob_id=None) -&gt; str:\n    \"\"\"\n    Creates a new blob. The blob_id is calculated client-side by hashing\n    the content, and the data is sent to the correct server determined\n    by the consistent hash ring. This uses a PUT request, making creation\n    idempotent.\n    \"\"\"\n    # The blob ID is the hash of its content, ensuring content-addressable storage.\n    if not blob_id:\n        blob_id = hashlib.sha256(data).hexdigest()\n\n    # Use PUT, as we now know the blob's final ID/URL.\n    # Pass blob_id to _make_request so it uses the hash ring.\n    print(f\"Creating blob {blob_id} on {self._make_request('PUT', f'/blob/{blob_id}',blob_id=blob_id, data=data).status_code}\")\n    # blob_id = response.text\n    self._save_blob_to_cache(blob_id, data)\n    return blob_id\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.blobs.ConsistentHashRing","title":"<code>ConsistentHashRing</code>","text":"<p>A consistent hash ring implementation to map keys (blob_ids) to nodes (servers). It uses virtual nodes (replicas) to ensure a more uniform distribution of keys.</p> Source code in <code>toolboxv2/utils/extras/blobs.py</code> <pre><code>class ConsistentHashRing:\n    \"\"\"\n    A consistent hash ring implementation to map keys (blob_ids) to nodes (servers).\n    It uses virtual nodes (replicas) to ensure a more uniform distribution of keys.\n    \"\"\"\n    def __init__(self, replicas=100):\n        \"\"\"\n        :param replicas: The number of virtual nodes for each physical node.\n                         Higher values lead to more balanced distribution.\n        \"\"\"\n        self.replicas = replicas\n        self._keys = []  # Sorted list of hash values (the ring)\n        self._nodes = {} # Maps hash values to physical node URLs\n\n    def _hash(self, key: str) -&gt; int:\n        \"\"\"Hashes a key to an integer using md5 for speed and distribution.\"\"\"\n        return int(hashlib.md5(key.encode('utf-8')).hexdigest(), 16)\n\n    def add_node(self, node: str):\n        \"\"\"Adds a physical node to the hash ring.\"\"\"\n        for i in range(self.replicas):\n            vnode_key = f\"{node}:{i}\"\n            h = self._hash(vnode_key)\n            bisect.insort(self._keys, h)\n            self._nodes[h] = node\n\n    def get_nodes_for_key(self, key: str) -&gt; list[str]:\n        \"\"\"\n        Returns an ordered list of nodes responsible for the given key.\n        The first node in the list is the primary, the rest are failover candidates\n        in preferential order.\n        \"\"\"\n        if not self._nodes:\n            return []\n\n        h = self._hash(key)\n        start_idx = bisect.bisect_left(self._keys, h)\n\n        # Collect unique physical nodes by iterating around the ring\n        found_nodes = []\n        for i in range(len(self._keys)):\n            idx = (start_idx + i) % len(self._keys)\n            node_hash = self._keys[idx]\n            physical_node = self._nodes[node_hash]\n            if physical_node not in found_nodes:\n                found_nodes.append(physical_node)\n            # Stop when we have found all unique physical nodes\n            if len(found_nodes) == len(set(self._nodes.values())):\n                break\n        return found_nodes\n</code></pre> <code>__init__(replicas=100)</code> \u00b6 <p>:param replicas: The number of virtual nodes for each physical node.                  Higher values lead to more balanced distribution.</p> Source code in <code>toolboxv2/utils/extras/blobs.py</code> <pre><code>def __init__(self, replicas=100):\n    \"\"\"\n    :param replicas: The number of virtual nodes for each physical node.\n                     Higher values lead to more balanced distribution.\n    \"\"\"\n    self.replicas = replicas\n    self._keys = []  # Sorted list of hash values (the ring)\n    self._nodes = {} # Maps hash values to physical node URLs\n</code></pre> <code>add_node(node)</code> \u00b6 <p>Adds a physical node to the hash ring.</p> Source code in <code>toolboxv2/utils/extras/blobs.py</code> <pre><code>def add_node(self, node: str):\n    \"\"\"Adds a physical node to the hash ring.\"\"\"\n    for i in range(self.replicas):\n        vnode_key = f\"{node}:{i}\"\n        h = self._hash(vnode_key)\n        bisect.insort(self._keys, h)\n        self._nodes[h] = node\n</code></pre> <code>get_nodes_for_key(key)</code> \u00b6 <p>Returns an ordered list of nodes responsible for the given key. The first node in the list is the primary, the rest are failover candidates in preferential order.</p> Source code in <code>toolboxv2/utils/extras/blobs.py</code> <pre><code>def get_nodes_for_key(self, key: str) -&gt; list[str]:\n    \"\"\"\n    Returns an ordered list of nodes responsible for the given key.\n    The first node in the list is the primary, the rest are failover candidates\n    in preferential order.\n    \"\"\"\n    if not self._nodes:\n        return []\n\n    h = self._hash(key)\n    start_idx = bisect.bisect_left(self._keys, h)\n\n    # Collect unique physical nodes by iterating around the ring\n    found_nodes = []\n    for i in range(len(self._keys)):\n        idx = (start_idx + i) % len(self._keys)\n        node_hash = self._keys[idx]\n        physical_node = self._nodes[node_hash]\n        if physical_node not in found_nodes:\n            found_nodes.append(physical_node)\n        # Stop when we have found all unique physical nodes\n        if len(found_nodes) == len(set(self._nodes.values())):\n            break\n    return found_nodes\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.gist_control","title":"<code>gist_control</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.gist_control.GistLoader","title":"<code>GistLoader</code>","text":"Source code in <code>toolboxv2/utils/extras/gist_control.py</code> <pre><code>class GistLoader:\n    def __init__(self, gist_url):\n        self.gist_url = gist_url\n        self.module_code = None\n\n    def load_module(self, module_name):\n        \"\"\"L\u00e4dt das Modul mit dem gegebenen Namen.\"\"\"\n        if self.module_code is None:\n            self.module_code = self._fetch_gist_content()\n\n        # Erstelle ein neues Modul\n        module = importlib.util.module_from_spec(self.get_spec(module_name))\n        exec(self.module_code, module.__dict__)\n        return module\n\n    def get_spec(self, module_name):\n        \"\"\"Gibt die Modul-Specifikation zur\u00fcck.\"\"\"\n        return ModuleSpec(module_name, self)\n\n    def get_filename(self, module_name):\n        return f\"&lt;gist:{self.gist_url}&gt;\"\n\n    def _fetch_gist_content(self):\n        \"\"\"L\u00e4dt den Inhalt des Gists von der GitHub API herunter.\"\"\"\n        gist_id = self.gist_url.split('/')[-1]\n        api_url = f\"https://api.github.com/gists/{gist_id}\"\n\n        response = requests.get(api_url)\n\n        if response.status_code == 200:\n            gist_data = response.json()\n            first_file = next(iter(gist_data['files'].values()))\n            return first_file['content']\n        else:\n            raise Exception(f\"Failed to fetch gist: {response.status_code}\")\n</code></pre> <code>get_spec(module_name)</code> \u00b6 <p>Gibt die Modul-Specifikation zur\u00fcck.</p> Source code in <code>toolboxv2/utils/extras/gist_control.py</code> <pre><code>def get_spec(self, module_name):\n    \"\"\"Gibt die Modul-Specifikation zur\u00fcck.\"\"\"\n    return ModuleSpec(module_name, self)\n</code></pre> <code>load_module(module_name)</code> \u00b6 <p>L\u00e4dt das Modul mit dem gegebenen Namen.</p> Source code in <code>toolboxv2/utils/extras/gist_control.py</code> <pre><code>def load_module(self, module_name):\n    \"\"\"L\u00e4dt das Modul mit dem gegebenen Namen.\"\"\"\n    if self.module_code is None:\n        self.module_code = self._fetch_gist_content()\n\n    # Erstelle ein neues Modul\n    module = importlib.util.module_from_spec(self.get_spec(module_name))\n    exec(self.module_code, module.__dict__)\n    return module\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.helper_test_functions","title":"<code>helper_test_functions</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.helper_test_functions.generate_edge_value","title":"<code>generate_edge_value(param_type)</code>","text":"<p>Generiert Edge-Case-Werte basierend auf dem Parametertyp.</p> Source code in <code>toolboxv2/utils/extras/helper_test_functions.py</code> <pre><code>def generate_edge_value(param_type: Any) -&gt; Any:\n    \"\"\"\n    Generiert Edge-Case-Werte basierend auf dem Parametertyp.\n    \"\"\"\n    if param_type in [int, float]:\n        return -999  # Beispiel f\u00fcr negative Zahlen\n    elif param_type == str:\n        return \"test \" * 100  # Lange zuf\u00e4llige Strings\n    # F\u00fcgen Sie hier weitere Bedingungen f\u00fcr andere Datentypen hinzu\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.helper_test_functions.generate_normal_value","title":"<code>generate_normal_value(param_type)</code>","text":"<p>Generiert normale Werte basierend auf dem Parametertyp.</p> Source code in <code>toolboxv2/utils/extras/helper_test_functions.py</code> <pre><code>def generate_normal_value(param_type: Any) -&gt; Any:\n    \"\"\"\n    Generiert normale Werte basierend auf dem Parametertyp.\n    \"\"\"\n    from toolboxv2 import RequestData\n    if param_type in [int, float]:\n        return random.randint(0, 100)  # Zuf\u00e4llige normale Zahlen\n    elif param_type == str:\n        return \"test\" # Zuf\u00e4lliges Wort\n    elif param_type == RequestData:\n        return RequestData.moc()\n    # F\u00fcgen Sie hier weitere Bedingungen f\u00fcr andere Datentypen hinzu\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.keword_matcher","title":"<code>keword_matcher</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.keword_matcher.calculate_keyword_score","title":"<code>calculate_keyword_score(text, keywords)</code>","text":"<p>Berechnet den Keyword-Score basierend auf der H\u00e4ufigkeit der Keywords im Text. Case-insensitive und optimiert f\u00fcr Geschwindigkeit.</p> <p>:param text: Eingabetext als String :param keywords: Set von Keywords :return: Gesamt-Score als Integer</p> Source code in <code>toolboxv2/utils/extras/keword_matcher.py</code> <pre><code>def calculate_keyword_score(text: str, keywords: set[str]) -&gt; int:\n    \"\"\"\n    Berechnet den Keyword-Score basierend auf der H\u00e4ufigkeit der Keywords im Text.\n    Case-insensitive und optimiert f\u00fcr Geschwindigkeit.\n\n    :param text: Eingabetext als String\n    :param keywords: Set von Keywords\n    :return: Gesamt-Score als Integer\n    \"\"\"\n    # Vorverarbeitung der Keywords\n    keyword_pattern = re.compile(\n        r'\\b(' + '|'.join(re.escape(k.lower()) for k in keywords) + r')\\b',\n        flags=re.IGNORECASE\n    )\n\n    # Erstelle Frequenz-W\u00f6rterbuch\n    freq_dict = defaultdict(int)\n\n    # Finde alle \u00dcbereinstimmungen\n    matches = keyword_pattern.findall(text.lower())\n\n    # Z\u00e4hle die Treffer\n    for match in matches:\n        freq_dict[match.lower()] += 1\n\n    # Berechne Gesamt-Score\n    total_score = sum(freq_dict.values())\n\n    return total_score\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.keword_matcher.calculate_weighted_score","title":"<code>calculate_weighted_score(text, keyword_weights)</code>","text":"<p>Berechnet gewichteten Score mit unterschiedlichen Gewichten pro Keyword</p> <p>:param text: Eingabetext :param keyword_weights: Dictionary mit {Keyword: Gewicht} :return: Gewichteter Gesamt-Score</p> Source code in <code>toolboxv2/utils/extras/keword_matcher.py</code> <pre><code>def calculate_weighted_score(text: str, keyword_weights: dict or list) -&gt; float:\n    \"\"\"\n    Berechnet gewichteten Score mit unterschiedlichen Gewichten pro Keyword\n\n    :param text: Eingabetext\n    :param keyword_weights: Dictionary mit {Keyword: Gewicht}\n    :return: Gewichteter Gesamt-Score\n    \"\"\"\n    total = 0.0\n    text_lower = text.lower()\n\n    if isinstance(keyword_weights, list):\n        keyword_weights = {k:v for k, v in keyword_weights}\n\n    for keyword, weight in keyword_weights.items():\n        count = len(re.findall(r'\\b' + re.escape(keyword.lower()) + r'\\b', text_lower))\n        total += count * weight\n\n    return round(total, 2)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.keword_matcher.extract_keywords","title":"<code>extract_keywords(text, max_len=-1, min_word_length=3, with_weights=False, remove_stopwords=True, stopwords=True)</code>","text":"<p>Extrahiert Keywords mit optionaler Frequenzgewichtung</p> <p>:param text: Eingabetext :param max_len: Maximale Anzahl Keywords (-1 = alle) :param min_word_length: Minimale Wortl\u00e4nge :param with_weights: Gibt Wort+Frequenz zur\u00fcck wenn True :param remove_stopwords: Filtert deutsche Stopw\u00f6rter :param german_stopwords: Verwendet deutsche Standard-Stopw\u00f6rter :return: Keywords oder (Keyword, H\u00e4ufigkeit) Paare</p> Source code in <code>toolboxv2/utils/extras/keword_matcher.py</code> <pre><code>def extract_keywords(\n    text: str,\n    max_len: int = -1,\n    min_word_length: int = 3,\n    with_weights: bool = False,\n    remove_stopwords: bool = True,\n    stopwords: bool = True\n) -&gt; list[str] | list[tuple[str, int]]:\n    \"\"\"\n    Extrahiert Keywords mit optionaler Frequenzgewichtung\n\n    :param text: Eingabetext\n    :param max_len: Maximale Anzahl Keywords (-1 = alle)\n    :param min_word_length: Minimale Wortl\u00e4nge\n    :param with_weights: Gibt Wort+Frequenz zur\u00fcck wenn True\n    :param remove_stopwords: Filtert deutsche Stopw\u00f6rter\n    :param german_stopwords: Verwendet deutsche Standard-Stopw\u00f6rter\n    :return: Keywords oder (Keyword, H\u00e4ufigkeit) Paare\n    \"\"\"\n\n    # Deutsche Basis-Stopw\u00f6rter\n    DEFAULT_STOPWORDS = STOPWORDS if stopwords else set()\n\n    # Text vorverarbeiten\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Worte filtern\n    filtered_words = [\n        word for word in words\n        if len(word) &gt; min_word_length\n           and (not remove_stopwords or word not in DEFAULT_STOPWORDS)\n    ]\n\n    # Frequenzanalyse\n    word_counts = defaultdict(int)\n    for word in filtered_words:\n        word_counts[word] += 1\n\n    # Sortierung: Zuerst H\u00e4ufigkeit, dann alphabetisch\n    sorted_words = sorted(\n        word_counts.items(),\n        key=lambda x: (-x[1], x[0])\n    )\n\n    # L\u00e4ngenbegrenzung\n    if max_len == -1:\n        max_len = None\n    result = sorted_words[:max_len]\n\n    return result if with_weights else [word for word, _ in result]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.reqbuilder","title":"<code>reqbuilder</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.reqbuilder.generate_requirements","title":"<code>generate_requirements(folder, output_file)</code>","text":"<p>Generates requirements.txt for the specified folder using pipreqs.</p> Source code in <code>toolboxv2/utils/extras/reqbuilder.py</code> <pre><code>def generate_requirements(folder: str, output_file: str):\n    \"\"\"Generates requirements.txt for the specified folder using pipreqs.\"\"\"\n    print(folder, output_file, os.path.abspath(os.curdir))\n    try:\n        from pipreqs.pipreqs import get_all_imports\n    except ImportError:\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pipreqs\"], check=True)\n    from pipreqs.pipreqs import get_all_imports\n    imports = set(get_all_imports(os.path.abspath(folder)))\n    imports.remove('toolboxv2') if 'toolboxv2' in imports else None\n    with open(os.path.abspath(output_file), \"w\") as f:\n        f.write(\"\\n\".join(imports))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.reqbuilder.run_pipeline","title":"<code>run_pipeline(base_dir)</code>","text":"<p>Runs the entire pipeline to generate requirements files.</p> Source code in <code>toolboxv2/utils/extras/reqbuilder.py</code> <pre><code>def run_pipeline(base_dir: str):\n    \"\"\"Runs the entire pipeline to generate requirements files.\"\"\"\n    toolbox_path = os.path.join(base_dir, \"toolboxv2\")\n    utils_path = os.path.join(toolbox_path, \"utils\")\n    mini_req_file = os.path.join(base_dir, \"requirements_mini.txt\")\n    extras_req_file = os.path.join(base_dir, \"requirements_tests.txt\")\n\n    # Step 1: Generate minimal requirements\n    print(\"Step 1/2: \")\n    generate_requirements(utils_path, mini_req_file)\n\n    # Step 2: Generate extended requirements\n    print(\"Step 2/2: \")\n    extras_path = os.path.join(toolbox_path, \"tests\")\n    generate_requirements(extras_path, extras_req_file)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.proxy","title":"<code>proxy</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.proxy.ProxyUtil","title":"<code>ProxyUtil</code>","text":"Source code in <code>toolboxv2/utils/proxy/prox_util.py</code> <pre><code>class ProxyUtil:\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.__storedargs = args, kwargs\n        self.async_initialized = False\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        # assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n\n    async def __ainit__(self, class_instance: Any, host='0.0.0.0', port=6587, timeout=6,\n                        app: (App or AppType) | None = None,\n                        remote_functions=None, peer=False, name='ProxyApp-client', do_connect=True, unix_socket=False,\n                        test_override=False):\n        self.class_instance = class_instance\n        self.client = None\n        self.test_override = test_override\n        self.port = port\n        self.host = host\n        self.timeout = timeout\n        if app is None:\n            app = get_app(\"ProxyUtil\")\n        self.app = app\n        self._name = name\n        self.unix_socket = unix_socket\n        if remote_functions is None:\n            remote_functions = [\"run_any\", \"a_run_any\", \"remove_mod\", \"save_load\", \"exit_main\", \"show_console\", \"hide_console\",\n                                \"rrun_flow\",\n                                \"get_autocompletion_dict\",\n                                \"exit_main\", \"watch_mod\"]\n        self.remote_functions = remote_functions\n\n        from toolboxv2.mods.SocketManager import SocketType\n        self.connection_type = SocketType.client\n        if peer:\n            self.connection_type = SocketType.peer\n        if do_connect:\n            await self.connect()\n\n    async def connect(self):\n        client_result = await self.app.a_run_local(SOCKETMANAGER.CREATE_SOCKET,\n                                           get_results=True,\n                                           name=self._name,\n                                           host=self.host,\n                                           port=self.port,\n                                           type_id=self.connection_type,\n                                           max_connections=-1,\n                                           return_full_object=True,\n                                           test_override=self.test_override,\n                                           unix_file=self.unix_socket)\n\n        if client_result.is_error():\n            raise Exception(f\"Client {self._name} error: {client_result.print(False)}\")\n        if not client_result.is_data():\n            raise Exception(f\"Client {self._name} error: {client_result.print(False)}\")\n        # 'socket': socket,\n        # 'receiver_socket': r_socket,\n        # 'host': host,\n        # 'port': port,\n        # 'p2p-port': endpoint_port,\n        # 'sender': send,\n        # 'receiver_queue': receiver_queue,\n        # 'connection_error': connection_error,\n        # 'receiver_thread': s_thread,\n        # 'keepalive_thread': keep_alive_thread,\n        # 'running_dict': running_dict,\n        # 'client_to_receiver_thread': to_receive,\n        # 'client_receiver_threads': threeds,\n        result = await client_result.aget()\n        if result is None or result.get('connection_error') != 0:\n            raise Exception(f\"Client {self._name} error: {client_result.print(False)}\")\n        self.client = Result.ok(result)\n\n    async def disconnect(self):\n        time.sleep(1)\n        close = self.client.get(\"close\")\n        await close()\n        self.client = None\n\n    async def reconnect(self):\n        if self.client is not None:\n            await self.disconnect()\n        await self.connect()\n\n    async def verify(self, message=b\"verify\"):\n        await asyncio.sleep(1)\n        # self.client.get('sender')({'keepalive': 0})\n        await self.client.get('sender')(message)\n\n    def __getattr__(self, name):\n\n        # print(f\"ProxyApp: {name}, {self.client is None}\")\n        if name == \"on_exit\":\n            return self.disconnect\n        if name == \"rc\":\n            return self.reconnect\n\n        if name == \"r\":\n            try:\n                return self.client.get('receiver_queue').get(timeout=self.timeout)\n            except:\n                return \"No data\"\n\n        app_attr = getattr(self.class_instance, name)\n\n        async def method(*args, **kwargs):\n            # if name == 'run_any':\n            #     print(\"method\", name, kwargs.get('get_results', False), args[0])\n            if self.client is None:\n                await self.reconnect()\n            if kwargs.get('spec', '-') == 'app':\n                if asyncio.iscoroutinefunction(app_attr):\n                    return await app_attr(*args, **kwargs)\n                return app_attr(*args, **kwargs)\n            try:\n                if name in self.remote_functions:\n                    if (name == 'run_any' or name == 'a_run_any') and not kwargs.get('get_results', False):\n                        if asyncio.iscoroutinefunction(app_attr):\n                            return await app_attr(*args, **kwargs)\n                        return app_attr(*args, **kwargs)\n                    if (name == 'run_any' or name == 'a_run_any') and kwargs.get('get_results', False):\n                        if isinstance(args[0], Enum):\n                            args = (args[0].__class__.NAME.value, args[0].value), args[1:]\n                    self.app.sprint(f\"Calling method {name}, {args=}, {kwargs}=\")\n                    await self.client.get('sender')({'name': name, 'args': args, 'kwargs': kwargs})\n                    while Spinner(\"Waiting for result\"):\n                        try:\n                            data = self.client.get('receiver_queue').get(timeout=self.timeout)\n                            if isinstance(data, dict) and 'identifier' in data:\n                                del data[\"identifier\"]\n                            if 'error' in data and 'origin' in data and 'result' in data and 'info' in data:\n                                data = ApiResult(**data).as_result()\n                            return data\n                        except:\n                            print(\"No data look later with class_instance.r\")\n                            return Result.default_internal_error(\"No data received from Demon.\"\n                                                                 \" uns class_instance.r to get data later\")\n            except:\n                if self.client.get('socket') is None:\n                    self.client = None\n            return app_attr(*args, **kwargs)\n\n        if callable(app_attr) and name in self.remote_functions and self.client is not None:\n            return method\n        return app_attr\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.proxy.ProxyUtil.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/proxy/prox_util.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.__storedargs = args, kwargs\n    self.async_initialized = False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.proxy.ProxyUtil.__initobj","title":"<code>__initobj()</code>  <code>async</code>","text":"<p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/proxy/prox_util.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    # assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.proxy.prox_util","title":"<code>prox_util</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.proxy.prox_util.ProxyUtil","title":"<code>ProxyUtil</code>","text":"Source code in <code>toolboxv2/utils/proxy/prox_util.py</code> <pre><code>class ProxyUtil:\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.__storedargs = args, kwargs\n        self.async_initialized = False\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        # assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n\n    async def __ainit__(self, class_instance: Any, host='0.0.0.0', port=6587, timeout=6,\n                        app: (App or AppType) | None = None,\n                        remote_functions=None, peer=False, name='ProxyApp-client', do_connect=True, unix_socket=False,\n                        test_override=False):\n        self.class_instance = class_instance\n        self.client = None\n        self.test_override = test_override\n        self.port = port\n        self.host = host\n        self.timeout = timeout\n        if app is None:\n            app = get_app(\"ProxyUtil\")\n        self.app = app\n        self._name = name\n        self.unix_socket = unix_socket\n        if remote_functions is None:\n            remote_functions = [\"run_any\", \"a_run_any\", \"remove_mod\", \"save_load\", \"exit_main\", \"show_console\", \"hide_console\",\n                                \"rrun_flow\",\n                                \"get_autocompletion_dict\",\n                                \"exit_main\", \"watch_mod\"]\n        self.remote_functions = remote_functions\n\n        from toolboxv2.mods.SocketManager import SocketType\n        self.connection_type = SocketType.client\n        if peer:\n            self.connection_type = SocketType.peer\n        if do_connect:\n            await self.connect()\n\n    async def connect(self):\n        client_result = await self.app.a_run_local(SOCKETMANAGER.CREATE_SOCKET,\n                                           get_results=True,\n                                           name=self._name,\n                                           host=self.host,\n                                           port=self.port,\n                                           type_id=self.connection_type,\n                                           max_connections=-1,\n                                           return_full_object=True,\n                                           test_override=self.test_override,\n                                           unix_file=self.unix_socket)\n\n        if client_result.is_error():\n            raise Exception(f\"Client {self._name} error: {client_result.print(False)}\")\n        if not client_result.is_data():\n            raise Exception(f\"Client {self._name} error: {client_result.print(False)}\")\n        # 'socket': socket,\n        # 'receiver_socket': r_socket,\n        # 'host': host,\n        # 'port': port,\n        # 'p2p-port': endpoint_port,\n        # 'sender': send,\n        # 'receiver_queue': receiver_queue,\n        # 'connection_error': connection_error,\n        # 'receiver_thread': s_thread,\n        # 'keepalive_thread': keep_alive_thread,\n        # 'running_dict': running_dict,\n        # 'client_to_receiver_thread': to_receive,\n        # 'client_receiver_threads': threeds,\n        result = await client_result.aget()\n        if result is None or result.get('connection_error') != 0:\n            raise Exception(f\"Client {self._name} error: {client_result.print(False)}\")\n        self.client = Result.ok(result)\n\n    async def disconnect(self):\n        time.sleep(1)\n        close = self.client.get(\"close\")\n        await close()\n        self.client = None\n\n    async def reconnect(self):\n        if self.client is not None:\n            await self.disconnect()\n        await self.connect()\n\n    async def verify(self, message=b\"verify\"):\n        await asyncio.sleep(1)\n        # self.client.get('sender')({'keepalive': 0})\n        await self.client.get('sender')(message)\n\n    def __getattr__(self, name):\n\n        # print(f\"ProxyApp: {name}, {self.client is None}\")\n        if name == \"on_exit\":\n            return self.disconnect\n        if name == \"rc\":\n            return self.reconnect\n\n        if name == \"r\":\n            try:\n                return self.client.get('receiver_queue').get(timeout=self.timeout)\n            except:\n                return \"No data\"\n\n        app_attr = getattr(self.class_instance, name)\n\n        async def method(*args, **kwargs):\n            # if name == 'run_any':\n            #     print(\"method\", name, kwargs.get('get_results', False), args[0])\n            if self.client is None:\n                await self.reconnect()\n            if kwargs.get('spec', '-') == 'app':\n                if asyncio.iscoroutinefunction(app_attr):\n                    return await app_attr(*args, **kwargs)\n                return app_attr(*args, **kwargs)\n            try:\n                if name in self.remote_functions:\n                    if (name == 'run_any' or name == 'a_run_any') and not kwargs.get('get_results', False):\n                        if asyncio.iscoroutinefunction(app_attr):\n                            return await app_attr(*args, **kwargs)\n                        return app_attr(*args, **kwargs)\n                    if (name == 'run_any' or name == 'a_run_any') and kwargs.get('get_results', False):\n                        if isinstance(args[0], Enum):\n                            args = (args[0].__class__.NAME.value, args[0].value), args[1:]\n                    self.app.sprint(f\"Calling method {name}, {args=}, {kwargs}=\")\n                    await self.client.get('sender')({'name': name, 'args': args, 'kwargs': kwargs})\n                    while Spinner(\"Waiting for result\"):\n                        try:\n                            data = self.client.get('receiver_queue').get(timeout=self.timeout)\n                            if isinstance(data, dict) and 'identifier' in data:\n                                del data[\"identifier\"]\n                            if 'error' in data and 'origin' in data and 'result' in data and 'info' in data:\n                                data = ApiResult(**data).as_result()\n                            return data\n                        except:\n                            print(\"No data look later with class_instance.r\")\n                            return Result.default_internal_error(\"No data received from Demon.\"\n                                                                 \" uns class_instance.r to get data later\")\n            except:\n                if self.client.get('socket') is None:\n                    self.client = None\n            return app_attr(*args, **kwargs)\n\n        if callable(app_attr) and name in self.remote_functions and self.client is not None:\n            return method\n        return app_attr\n</code></pre> <code>__init__(*args, **kwargs)</code> \u00b6 <p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/proxy/prox_util.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.__storedargs = args, kwargs\n    self.async_initialized = False\n</code></pre> <code>__initobj()</code> <code>async</code> \u00b6 <p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/proxy/prox_util.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    # assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security","title":"<code>security</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.security.Code","title":"<code>Code</code>","text":"Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>class Code:\n\n    @staticmethod\n    def DK():\n        return DEVICE_KEY\n\n    def decode_code(self, encrypted_data, key=None):\n\n        if not isinstance(encrypted_data, str):\n            encrypted_data = str(encrypted_data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.decrypt_symmetric(encrypted_data, key)\n\n    def encode_code(self, data, key=None):\n\n        if not isinstance(data, str):\n            data = str(data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.encrypt_symmetric(data, key)\n\n    @staticmethod\n    def generate_seed() -&gt; int:\n        \"\"\"\n        Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n        Returns:\n            int: Eine zuf\u00e4llige Zahl.\n        \"\"\"\n        return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n\n    @staticmethod\n    def one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n        \"\"\"\n        Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n        Args:\n            text (str): Der zu hashende Text.\n            salt (str): Der Salt-Wert.\n            pepper (str): Der Pepper-Wert.\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            str: Der resultierende Hash-Wert.\n        \"\"\"\n        return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n\n    @staticmethod\n    def generate_symmetric_key(as_str=True) -&gt; str or bytes:\n        \"\"\"\n        Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n        Returns:\n            str: Der generierte Schl\u00fcssel.\n        \"\"\"\n        key = Fernet.generate_key()\n        if as_str:\n            key = key.decode()\n        return key\n\n    @staticmethod\n    def encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        if isinstance(text, str):\n            text = text.encode()\n\n        try:\n            fernet = Fernet(key.encode())\n            return fernet.encrypt(text).decode()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n            return \"Error encrypt\"\n\n    @staticmethod\n    def decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n            to_str (bool): default true returns str if false returns bytes\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n\n        if isinstance(key, str):\n            key = key.encode()\n\n        #try:\n        fernet = Fernet(key)\n        text_b = fernet.decrypt(encrypted_text)\n        if not to_str:\n            return text_b\n        return text_b.decode()\n        # except Exception as e:\n        #     get_logger().error(f\"Error decrypt_symmetric {e}\")\n        #     if not mute:\n        #         raise e\n        #     if not to_str:\n        #         return f\"Error decoding\".encode()\n        #     return f\"Error decoding\"\n\n    @staticmethod\n    def generate_asymmetric_keys() -&gt; (str, str):\n        \"\"\"\n        Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n        Args:\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n        \"\"\"\n        private_key = rsa.generate_private_key(\n            public_exponent=65537,\n            key_size=2048 * 3,\n        )\n        public_key = private_key.public_key()\n\n        # Serialisieren der Schl\u00fcssel\n        pem_private_key = private_key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.NoEncryption()\n        ).decode()\n\n        pem_public_key = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        ).decode()\n\n        return pem_public_key, pem_private_key\n\n    @staticmethod\n    def save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n        \"\"\"\n        Speichert die generierten Schl\u00fcssel in separate Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n        Args:\n            public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n            private_key (str): Der private Schl\u00fcssel im PEM-Format\n            directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n        \"\"\"\n        # Erstelle das Verzeichnis, falls es nicht existiert\n        os.makedirs(directory, exist_ok=True)\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n        encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n        # Speichere den \u00f6ffentlichen Schl\u00fcssel\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        with open(public_key_path, \"w\") as f:\n            f.write(public_key)\n\n        # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n        with open(private_key_path, \"w\") as f:\n            f.write(encrypted_private_key)\n\n        print(\"Saved keys in \", public_key_path)\n\n    @staticmethod\n    def load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n        \"\"\"\n        L\u00e4dt die Schl\u00fcssel aus den Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n        Args:\n            directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n        Raises:\n            FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n        \"\"\"\n        # Pfade zu den Schl\u00fcsseldateien\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n\n        # Pr\u00fcfe ob die Dateien existieren\n        if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n            return \"\", \"\"\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Lade den \u00f6ffentlichen Schl\u00fcssel\n        with open(public_key_path) as f:\n            public_key = f.read()\n\n        # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n        with open(private_key_path) as f:\n            encrypted_private_key = f.read()\n            private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n        return public_key, private_key\n\n    @staticmethod\n    def encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        # try:\n        #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        #  except Exception as e:\n        #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            encrypted = public_key.encrypt(\n                text.encode(),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return encrypted.hex()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_asymmetric {e}\")\n            return \"Invalid\"\n\n    @staticmethod\n    def decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n        Args:\n            encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n            private_key_str (str): Der private Schl\u00fcssel als String.\n\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            decrypted = private_key.decrypt(\n                bytes.fromhex(encrypted_text_hex),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return decrypted.decode()\n\n        except Exception as e:\n            get_logger().error(f\"Error decrypt_asymmetric {e}\")\n        return \"Invalid\"\n\n    @staticmethod\n    def verify_signature(signature: str or bytes, message: str or bytes, public_key_str: str,\n                         salt_length=padding.PSS.MAX_LENGTH) -&gt; bool:\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                padding=padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                algorithm=hashes.SHA512()\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def verify_signature_web_algo(signature: str or bytes, message: str or bytes, public_key_str: str,\n                                  algo: int = -512) -&gt; bool:\n        signature_algorithm = ECDSA(hashes.SHA512())\n        if algo != -512:\n            signature_algorithm = ECDSA(hashes.SHA256())\n\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                # padding=padding.PSS(\n                #    mgf=padding.MGF1(hashes.SHA512()),\n                #    salt_length=padding.PSS.MAX_LENGTH\n                # ),\n                signature_algorithm=signature_algorithm\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def create_signature(message: str, private_key_str: str, salt_length=padding.PSS.MAX_LENGTH,\n                         row=False) -&gt; str or bytes:\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            signature = private_key.sign(\n                message.encode(),\n                padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                hashes.SHA512()\n            )\n            if row:\n                return signature\n            return base64.b64encode(signature).decode()\n        except Exception as e:\n            get_logger().error(f\"Error create_signature {e}\")\n            print(e)\n        return \"Invalid Key\"\n\n    @staticmethod\n    def pem_to_public_key(pem_key: str):\n        \"\"\"\n        Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n        Args:\n            pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n        Returns:\n            PublicKey: Das PublicKey-Objekt.\n        \"\"\"\n        public_key = serialization.load_pem_public_key(pem_key.encode())\n        return public_key\n\n    @staticmethod\n    def public_key_to_pem(public_key: RSAPublicKey):\n        \"\"\"\n        Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n        Args:\n            public_key (PublicKey): Das PublicKey-Objekt.\n\n        Returns:\n            str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n        \"\"\"\n        pem = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n        return pem.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.decrypt_asymmetric","title":"<code>decrypt_asymmetric(encrypted_text_hex, private_key_str)</code>  <code>staticmethod</code>","text":"<p>Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text_hex</code> <code>str</code> <p>Der verschl\u00fcsselte Text als Hex-String.</p> required <code>private_key_str</code> <code>str</code> <p>Der private Schl\u00fcssel als String.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n    Args:\n        encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n        private_key_str (str): Der private Schl\u00fcssel als String.\n\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n    try:\n        private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n        decrypted = private_key.decrypt(\n            bytes.fromhex(encrypted_text_hex),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return decrypted.decode()\n\n    except Exception as e:\n        get_logger().error(f\"Error decrypt_asymmetric {e}\")\n    return \"Invalid\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.decrypt_symmetric","title":"<code>decrypt_symmetric(encrypted_text, key, to_str=True, mute=False)</code>  <code>staticmethod</code>","text":"<p>Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text</code> <code>str</code> <p>Der zu entschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <code>to_str</code> <code>bool</code> <p>default true returns str if false returns bytes</p> <code>True</code> <p>Returns:     str: Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n        to_str (bool): default true returns str if false returns bytes\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n\n    if isinstance(key, str):\n        key = key.encode()\n\n    #try:\n    fernet = Fernet(key)\n    text_b = fernet.decrypt(encrypted_text)\n    if not to_str:\n        return text_b\n    return text_b.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.encrypt_asymmetric","title":"<code>encrypt_asymmetric(text, public_key_str)</code>  <code>staticmethod</code>","text":"<p>Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>public_key_str</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    # try:\n    #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n    #  except Exception as e:\n    #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n    try:\n        public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        encrypted = public_key.encrypt(\n            text.encode(),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return encrypted.hex()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        return \"Invalid\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.encrypt_symmetric","title":"<code>encrypt_symmetric(text, key)</code>  <code>staticmethod</code>","text":"<p>Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    if isinstance(text, str):\n        text = text.encode()\n\n    try:\n        fernet = Fernet(key.encode())\n        return fernet.encrypt(text).decode()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n        return \"Error encrypt\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.generate_asymmetric_keys","title":"<code>generate_asymmetric_keys()</code>  <code>staticmethod</code>","text":"<p>Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_asymmetric_keys() -&gt; (str, str):\n    \"\"\"\n    Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n    Args:\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n    \"\"\"\n    private_key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048 * 3,\n    )\n    public_key = private_key.public_key()\n\n    # Serialisieren der Schl\u00fcssel\n    pem_private_key = private_key.private_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PrivateFormat.PKCS8,\n        encryption_algorithm=serialization.NoEncryption()\n    ).decode()\n\n    pem_public_key = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    ).decode()\n\n    return pem_public_key, pem_private_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.generate_seed","title":"<code>generate_seed()</code>  <code>staticmethod</code>","text":"<p>Erzeugt eine zuf\u00e4llige Zahl als Seed.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Eine zuf\u00e4llige Zahl.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_seed() -&gt; int:\n    \"\"\"\n    Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n    Returns:\n        int: Eine zuf\u00e4llige Zahl.\n    \"\"\"\n    return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.generate_symmetric_key","title":"<code>generate_symmetric_key(as_str=True)</code>  <code>staticmethod</code>","text":"<p>Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str or bytes</code> <p>Der generierte Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_symmetric_key(as_str=True) -&gt; str or bytes:\n    \"\"\"\n    Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n    Returns:\n        str: Der generierte Schl\u00fcssel.\n    \"\"\"\n    key = Fernet.generate_key()\n    if as_str:\n        key = key.decode()\n    return key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.load_keys_from_files","title":"<code>load_keys_from_files(directory='keys')</code>  <code>staticmethod</code>","text":"<p>L\u00e4dt die Schl\u00fcssel aus den Dateien. Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen</p> <code>'keys'</code> <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n    \"\"\"\n    L\u00e4dt die Schl\u00fcssel aus den Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n    Args:\n        directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n    Raises:\n        FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n    \"\"\"\n    # Pfade zu den Schl\u00fcsseldateien\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n\n    # Pr\u00fcfe ob die Dateien existieren\n    if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n        return \"\", \"\"\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Lade den \u00f6ffentlichen Schl\u00fcssel\n    with open(public_key_path) as f:\n        public_key = f.read()\n\n    # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n    with open(private_key_path) as f:\n        encrypted_private_key = f.read()\n        private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n    return public_key, private_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.one_way_hash","title":"<code>one_way_hash(text, salt='', pepper='')</code>  <code>staticmethod</code>","text":"<p>Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu hashende Text.</p> required <code>salt</code> <code>str</code> <p>Der Salt-Wert.</p> <code>''</code> <code>pepper</code> <code>str</code> <p>Der Pepper-Wert.</p> <code>''</code> <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der resultierende Hash-Wert.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n    \"\"\"\n    Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n    Args:\n        text (str): Der zu hashende Text.\n        salt (str): Der Salt-Wert.\n        pepper (str): Der Pepper-Wert.\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        str: Der resultierende Hash-Wert.\n    \"\"\"\n    return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.pem_to_public_key","title":"<code>pem_to_public_key(pem_key)</code>  <code>staticmethod</code>","text":"<p>Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.</p> <p>Parameters:</p> Name Type Description Default <code>pem_key</code> <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef pem_to_public_key(pem_key: str):\n    \"\"\"\n    Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n    Args:\n        pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n    Returns:\n        PublicKey: Das PublicKey-Objekt.\n    \"\"\"\n    public_key = serialization.load_pem_public_key(pem_key.encode())\n    return public_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.public_key_to_pem","title":"<code>public_key_to_pem(public_key)</code>  <code>staticmethod</code>","text":"<p>Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef public_key_to_pem(public_key: RSAPublicKey):\n    \"\"\"\n    Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n    Args:\n        public_key (PublicKey): Das PublicKey-Objekt.\n\n    Returns:\n        str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n    \"\"\"\n    pem = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    )\n    return pem.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.save_keys_to_files","title":"<code>save_keys_to_files(public_key, private_key, directory='keys')</code>  <code>staticmethod</code>","text":"<p>Speichert die generierten Schl\u00fcssel in separate Dateien. Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel im PEM-Format</p> required <code>private_key</code> <code>str</code> <p>Der private Schl\u00fcssel im PEM-Format</p> required <code>directory</code> <code>str</code> <p>Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen</p> <code>'keys'</code> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n    \"\"\"\n    Speichert die generierten Schl\u00fcssel in separate Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n    Args:\n        public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n        private_key (str): Der private Schl\u00fcssel im PEM-Format\n        directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n    \"\"\"\n    # Erstelle das Verzeichnis, falls es nicht existiert\n    os.makedirs(directory, exist_ok=True)\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n    encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n    # Speichere den \u00f6ffentlichen Schl\u00fcssel\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    with open(public_key_path, \"w\") as f:\n        f.write(public_key)\n\n    # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n    with open(private_key_path, \"w\") as f:\n        f.write(encrypted_private_key)\n\n    print(\"Saved keys in \", public_key_path)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.cryp","title":"<code>cryp</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.security.cryp.Code","title":"<code>Code</code>","text":"Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>class Code:\n\n    @staticmethod\n    def DK():\n        return DEVICE_KEY\n\n    def decode_code(self, encrypted_data, key=None):\n\n        if not isinstance(encrypted_data, str):\n            encrypted_data = str(encrypted_data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.decrypt_symmetric(encrypted_data, key)\n\n    def encode_code(self, data, key=None):\n\n        if not isinstance(data, str):\n            data = str(data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.encrypt_symmetric(data, key)\n\n    @staticmethod\n    def generate_seed() -&gt; int:\n        \"\"\"\n        Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n        Returns:\n            int: Eine zuf\u00e4llige Zahl.\n        \"\"\"\n        return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n\n    @staticmethod\n    def one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n        \"\"\"\n        Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n        Args:\n            text (str): Der zu hashende Text.\n            salt (str): Der Salt-Wert.\n            pepper (str): Der Pepper-Wert.\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            str: Der resultierende Hash-Wert.\n        \"\"\"\n        return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n\n    @staticmethod\n    def generate_symmetric_key(as_str=True) -&gt; str or bytes:\n        \"\"\"\n        Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n        Returns:\n            str: Der generierte Schl\u00fcssel.\n        \"\"\"\n        key = Fernet.generate_key()\n        if as_str:\n            key = key.decode()\n        return key\n\n    @staticmethod\n    def encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        if isinstance(text, str):\n            text = text.encode()\n\n        try:\n            fernet = Fernet(key.encode())\n            return fernet.encrypt(text).decode()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n            return \"Error encrypt\"\n\n    @staticmethod\n    def decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n            to_str (bool): default true returns str if false returns bytes\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n\n        if isinstance(key, str):\n            key = key.encode()\n\n        #try:\n        fernet = Fernet(key)\n        text_b = fernet.decrypt(encrypted_text)\n        if not to_str:\n            return text_b\n        return text_b.decode()\n        # except Exception as e:\n        #     get_logger().error(f\"Error decrypt_symmetric {e}\")\n        #     if not mute:\n        #         raise e\n        #     if not to_str:\n        #         return f\"Error decoding\".encode()\n        #     return f\"Error decoding\"\n\n    @staticmethod\n    def generate_asymmetric_keys() -&gt; (str, str):\n        \"\"\"\n        Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n        Args:\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n        \"\"\"\n        private_key = rsa.generate_private_key(\n            public_exponent=65537,\n            key_size=2048 * 3,\n        )\n        public_key = private_key.public_key()\n\n        # Serialisieren der Schl\u00fcssel\n        pem_private_key = private_key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.NoEncryption()\n        ).decode()\n\n        pem_public_key = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        ).decode()\n\n        return pem_public_key, pem_private_key\n\n    @staticmethod\n    def save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n        \"\"\"\n        Speichert die generierten Schl\u00fcssel in separate Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n        Args:\n            public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n            private_key (str): Der private Schl\u00fcssel im PEM-Format\n            directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n        \"\"\"\n        # Erstelle das Verzeichnis, falls es nicht existiert\n        os.makedirs(directory, exist_ok=True)\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n        encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n        # Speichere den \u00f6ffentlichen Schl\u00fcssel\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        with open(public_key_path, \"w\") as f:\n            f.write(public_key)\n\n        # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n        with open(private_key_path, \"w\") as f:\n            f.write(encrypted_private_key)\n\n        print(\"Saved keys in \", public_key_path)\n\n    @staticmethod\n    def load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n        \"\"\"\n        L\u00e4dt die Schl\u00fcssel aus den Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n        Args:\n            directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n        Raises:\n            FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n        \"\"\"\n        # Pfade zu den Schl\u00fcsseldateien\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n\n        # Pr\u00fcfe ob die Dateien existieren\n        if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n            return \"\", \"\"\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Lade den \u00f6ffentlichen Schl\u00fcssel\n        with open(public_key_path) as f:\n            public_key = f.read()\n\n        # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n        with open(private_key_path) as f:\n            encrypted_private_key = f.read()\n            private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n        return public_key, private_key\n\n    @staticmethod\n    def encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        # try:\n        #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        #  except Exception as e:\n        #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            encrypted = public_key.encrypt(\n                text.encode(),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return encrypted.hex()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_asymmetric {e}\")\n            return \"Invalid\"\n\n    @staticmethod\n    def decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n        Args:\n            encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n            private_key_str (str): Der private Schl\u00fcssel als String.\n\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            decrypted = private_key.decrypt(\n                bytes.fromhex(encrypted_text_hex),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return decrypted.decode()\n\n        except Exception as e:\n            get_logger().error(f\"Error decrypt_asymmetric {e}\")\n        return \"Invalid\"\n\n    @staticmethod\n    def verify_signature(signature: str or bytes, message: str or bytes, public_key_str: str,\n                         salt_length=padding.PSS.MAX_LENGTH) -&gt; bool:\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                padding=padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                algorithm=hashes.SHA512()\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def verify_signature_web_algo(signature: str or bytes, message: str or bytes, public_key_str: str,\n                                  algo: int = -512) -&gt; bool:\n        signature_algorithm = ECDSA(hashes.SHA512())\n        if algo != -512:\n            signature_algorithm = ECDSA(hashes.SHA256())\n\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                # padding=padding.PSS(\n                #    mgf=padding.MGF1(hashes.SHA512()),\n                #    salt_length=padding.PSS.MAX_LENGTH\n                # ),\n                signature_algorithm=signature_algorithm\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def create_signature(message: str, private_key_str: str, salt_length=padding.PSS.MAX_LENGTH,\n                         row=False) -&gt; str or bytes:\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            signature = private_key.sign(\n                message.encode(),\n                padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                hashes.SHA512()\n            )\n            if row:\n                return signature\n            return base64.b64encode(signature).decode()\n        except Exception as e:\n            get_logger().error(f\"Error create_signature {e}\")\n            print(e)\n        return \"Invalid Key\"\n\n    @staticmethod\n    def pem_to_public_key(pem_key: str):\n        \"\"\"\n        Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n        Args:\n            pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n        Returns:\n            PublicKey: Das PublicKey-Objekt.\n        \"\"\"\n        public_key = serialization.load_pem_public_key(pem_key.encode())\n        return public_key\n\n    @staticmethod\n    def public_key_to_pem(public_key: RSAPublicKey):\n        \"\"\"\n        Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n        Args:\n            public_key (PublicKey): Das PublicKey-Objekt.\n\n        Returns:\n            str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n        \"\"\"\n        pem = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n        return pem.decode()\n</code></pre> <code>decrypt_asymmetric(encrypted_text_hex, private_key_str)</code> <code>staticmethod</code> \u00b6 <p>Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text_hex</code> <code>str</code> <p>Der verschl\u00fcsselte Text als Hex-String.</p> required <code>private_key_str</code> <code>str</code> <p>Der private Schl\u00fcssel als String.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n    Args:\n        encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n        private_key_str (str): Der private Schl\u00fcssel als String.\n\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n    try:\n        private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n        decrypted = private_key.decrypt(\n            bytes.fromhex(encrypted_text_hex),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return decrypted.decode()\n\n    except Exception as e:\n        get_logger().error(f\"Error decrypt_asymmetric {e}\")\n    return \"Invalid\"\n</code></pre> <code>decrypt_symmetric(encrypted_text, key, to_str=True, mute=False)</code> <code>staticmethod</code> \u00b6 <p>Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text</code> <code>str</code> <p>Der zu entschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <code>to_str</code> <code>bool</code> <p>default true returns str if false returns bytes</p> <code>True</code> <p>Returns:     str: Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n        to_str (bool): default true returns str if false returns bytes\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n\n    if isinstance(key, str):\n        key = key.encode()\n\n    #try:\n    fernet = Fernet(key)\n    text_b = fernet.decrypt(encrypted_text)\n    if not to_str:\n        return text_b\n    return text_b.decode()\n</code></pre> <code>encrypt_asymmetric(text, public_key_str)</code> <code>staticmethod</code> \u00b6 <p>Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>public_key_str</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    # try:\n    #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n    #  except Exception as e:\n    #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n    try:\n        public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        encrypted = public_key.encrypt(\n            text.encode(),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return encrypted.hex()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        return \"Invalid\"\n</code></pre> <code>encrypt_symmetric(text, key)</code> <code>staticmethod</code> \u00b6 <p>Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    if isinstance(text, str):\n        text = text.encode()\n\n    try:\n        fernet = Fernet(key.encode())\n        return fernet.encrypt(text).decode()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n        return \"Error encrypt\"\n</code></pre> <code>generate_asymmetric_keys()</code> <code>staticmethod</code> \u00b6 <p>Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_asymmetric_keys() -&gt; (str, str):\n    \"\"\"\n    Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n    Args:\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n    \"\"\"\n    private_key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048 * 3,\n    )\n    public_key = private_key.public_key()\n\n    # Serialisieren der Schl\u00fcssel\n    pem_private_key = private_key.private_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PrivateFormat.PKCS8,\n        encryption_algorithm=serialization.NoEncryption()\n    ).decode()\n\n    pem_public_key = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    ).decode()\n\n    return pem_public_key, pem_private_key\n</code></pre> <code>generate_seed()</code> <code>staticmethod</code> \u00b6 <p>Erzeugt eine zuf\u00e4llige Zahl als Seed.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Eine zuf\u00e4llige Zahl.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_seed() -&gt; int:\n    \"\"\"\n    Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n    Returns:\n        int: Eine zuf\u00e4llige Zahl.\n    \"\"\"\n    return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n</code></pre> <code>generate_symmetric_key(as_str=True)</code> <code>staticmethod</code> \u00b6 <p>Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str or bytes</code> <p>Der generierte Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_symmetric_key(as_str=True) -&gt; str or bytes:\n    \"\"\"\n    Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n    Returns:\n        str: Der generierte Schl\u00fcssel.\n    \"\"\"\n    key = Fernet.generate_key()\n    if as_str:\n        key = key.decode()\n    return key\n</code></pre> <code>load_keys_from_files(directory='keys')</code> <code>staticmethod</code> \u00b6 <p>L\u00e4dt die Schl\u00fcssel aus den Dateien. Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen</p> <code>'keys'</code> <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n    \"\"\"\n    L\u00e4dt die Schl\u00fcssel aus den Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n    Args:\n        directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n    Raises:\n        FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n    \"\"\"\n    # Pfade zu den Schl\u00fcsseldateien\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n\n    # Pr\u00fcfe ob die Dateien existieren\n    if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n        return \"\", \"\"\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Lade den \u00f6ffentlichen Schl\u00fcssel\n    with open(public_key_path) as f:\n        public_key = f.read()\n\n    # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n    with open(private_key_path) as f:\n        encrypted_private_key = f.read()\n        private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n    return public_key, private_key\n</code></pre> <code>one_way_hash(text, salt='', pepper='')</code> <code>staticmethod</code> \u00b6 <p>Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu hashende Text.</p> required <code>salt</code> <code>str</code> <p>Der Salt-Wert.</p> <code>''</code> <code>pepper</code> <code>str</code> <p>Der Pepper-Wert.</p> <code>''</code> <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der resultierende Hash-Wert.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n    \"\"\"\n    Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n    Args:\n        text (str): Der zu hashende Text.\n        salt (str): Der Salt-Wert.\n        pepper (str): Der Pepper-Wert.\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        str: Der resultierende Hash-Wert.\n    \"\"\"\n    return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n</code></pre> <code>pem_to_public_key(pem_key)</code> <code>staticmethod</code> \u00b6 <p>Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.</p> <p>Parameters:</p> Name Type Description Default <code>pem_key</code> <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef pem_to_public_key(pem_key: str):\n    \"\"\"\n    Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n    Args:\n        pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n    Returns:\n        PublicKey: Das PublicKey-Objekt.\n    \"\"\"\n    public_key = serialization.load_pem_public_key(pem_key.encode())\n    return public_key\n</code></pre> <code>public_key_to_pem(public_key)</code> <code>staticmethod</code> \u00b6 <p>Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef public_key_to_pem(public_key: RSAPublicKey):\n    \"\"\"\n    Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n    Args:\n        public_key (PublicKey): Das PublicKey-Objekt.\n\n    Returns:\n        str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n    \"\"\"\n    pem = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    )\n    return pem.decode()\n</code></pre> <code>save_keys_to_files(public_key, private_key, directory='keys')</code> <code>staticmethod</code> \u00b6 <p>Speichert die generierten Schl\u00fcssel in separate Dateien. Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel im PEM-Format</p> required <code>private_key</code> <code>str</code> <p>Der private Schl\u00fcssel im PEM-Format</p> required <code>directory</code> <code>str</code> <p>Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen</p> <code>'keys'</code> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n    \"\"\"\n    Speichert die generierten Schl\u00fcssel in separate Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n    Args:\n        public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n        private_key (str): Der private Schl\u00fcssel im PEM-Format\n        directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n    \"\"\"\n    # Erstelle das Verzeichnis, falls es nicht existiert\n    os.makedirs(directory, exist_ok=True)\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n    encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n    # Speichere den \u00f6ffentlichen Schl\u00fcssel\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    with open(public_key_path, \"w\") as f:\n        f.write(public_key)\n\n    # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n    with open(private_key_path, \"w\") as f:\n        f.write(encrypted_private_key)\n\n    print(\"Saved keys in \", public_key_path)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.singelton_class","title":"<code>singelton_class</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.singelton_class.Singleton","title":"<code>Singleton</code>","text":"<p>Singleton metaclass for ensuring only one instance of a class.</p> Source code in <code>toolboxv2/utils/singelton_class.py</code> <pre><code>class Singleton(type):\n    \"\"\"\n    Singleton metaclass for ensuring only one instance of a class.\n    \"\"\"\n\n    _instances = {}\n    _kwargs = {}\n    _args = {}\n\n    def __call__(cls, *args, **kwargs):\n        if cls not in cls._instances:\n            cls._instances[cls] = super().__call__(*args, **kwargs)\n            cls._args[cls] = args\n            cls._kwargs[cls] = kwargs\n        return cls._instances[cls]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system","title":"<code>system</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.AppType","title":"<code>AppType</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class AppType:\n    prefix: str\n    id: str\n    globals: dict[str, Any] = {\"root\": dict, }\n    locals: dict[str, Any] = {\"user\": {'app': \"self\"}, }\n\n    local_test: bool = False\n    start_dir: str\n    data_dir: str\n    config_dir: str\n    info_dir: str\n\n    logger: logging.Logger\n    logging_filename: str\n\n    api_allowed_mods_list: list[str] = []\n\n    version: str\n    loop: asyncio.AbstractEventLoop\n\n    keys: dict[str, str] = {\n        \"MACRO\": \"macro~~~~:\",\n        \"MACRO_C\": \"m_color~~:\",\n        \"HELPER\": \"helper~~~:\",\n        \"debug\": \"debug~~~~:\",\n        \"id\": \"name-spa~:\",\n        \"st-load\": \"mute~load:\",\n        \"comm-his\": \"comm-his~:\",\n        \"develop-mode\": \"dev~mode~:\",\n        \"provider::\": \"provider::\",\n    }\n\n    defaults: dict[str, (bool or dict or dict[str, dict[str, str]] or str or list[str] or list[list]) | None] = {\n        \"MACRO\": list[str],\n        \"MACRO_C\": dict,\n        \"HELPER\": dict,\n        \"debug\": str,\n        \"id\": str,\n        \"st-load\": False,\n        \"comm-his\": list[list],\n        \"develop-mode\": bool,\n    }\n\n    cluster_manager: ClusterManager\n    root_blob_storage: BlobStorage\n    config_fh: FileHandler\n    _debug: bool\n    flows: dict[str, Callable]\n    dev_modi: bool\n    functions: dict[str, Any]\n    modules: dict[str, Any]\n\n    interface_type: ToolBoxInterfaces\n    REFIX: str\n\n    alive: bool\n    called_exit: tuple[bool, float]\n    args_sto: AppArgs\n    system_flag = None\n    session = None\n    appdata = None\n    exit_tasks = []\n\n    enable_profiling: bool = False\n    sto = None\n\n    def __init__(self, prefix: None | str= None, args: AppArgs | None = None):\n        self.args_sto = args\n        self.prefix = prefix\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def exit_main(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def hide_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def show_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def disconnect(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def set_logger(self, debug=False):\n        \"\"\"proxi attr\"\"\"\n\n    @property\n    def debug(self):\n        \"\"\"proxi attr\"\"\"\n        return self._debug\n\n    def debug_rains(self, e):\n        \"\"\"proxi attr\"\"\"\n\n    def set_flows(self, r):\n        \"\"\"proxi attr\"\"\"\n\n    def run_flows(self, name, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def rrun_flows(self, name, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def idle(self):\n        import time\n        self.print(\"idle\")\n        try:\n            while self.alive:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            pass\n        self.print(\"idle done\")\n\n    async def a_idle(self):\n        self.print(\"a idle\")\n        try:\n            if hasattr(self, 'daemon_app'):\n                self.print(\"serving daemon\")\n                await self.daemon_app.connect(self)\n            else:\n                self.print(\"serving default\")\n                while self.alive:\n                    await asyncio.sleep(1)\n        except KeyboardInterrupt:\n            pass\n        self.print(\"a idle done\")\n\n    @debug.setter\n    def debug(self, value):\n        \"\"\"proxi attr\"\"\"\n\n    def _coppy_mod(self, content, new_mod_dir, mod_name, file_type='py'):\n        \"\"\"proxi attr\"\"\"\n\n    def _pre_lib_mod(self, mod_name, path_to=\"./runtime\", file_type='py'):\n        \"\"\"proxi attr\"\"\"\n\n    def _copy_load(self, mod_name, file_type='py', **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True):\n        \"\"\"proxi attr\"\"\"\n\n    def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n        \"\"\"proxi attr\"\"\"\n\n    def save_initialized_module(self, tools_class, spec):\n        \"\"\"proxi attr\"\"\"\n\n    def mod_online(self, mod_name, installed=False):\n        \"\"\"proxi attr\"\"\"\n\n    def _get_function(self,\n                      name: Enum or None,\n                      state: bool = True,\n                      specification: str = \"app\",\n                      metadata=False, as_str: tuple or None = None, r=0):\n        \"\"\"proxi attr\"\"\"\n\n    def save_exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def load_mod(self, mod_name: str, mlm='I', **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    async def init_module(self, modular):\n        return await self.load_mod(modular)\n\n    async def load_all_mods_in_file(self, working_dir=\"mods\"):\n        \"\"\"proxi attr\"\"\"\n\n    def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\"):\n        \"\"\"proxi attr\"\"\"\n\n    def remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            self.remove_mod(mod, delete=delete)\n\n    async def a_remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            await self.a_remove_mod(mod, delete=delete)\n\n    def print_ok(self):\n        \"\"\"proxi attr\"\"\"\n        self.logger.info(\"OK\")\n\n    def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n        \"\"\"proxi attr\"\"\"\n\n    def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None):\n        \"\"\"proxi attr\"\"\"\n\n    def remove_mod(self, mod_name, spec='app', delete=True):\n        \"\"\"proxi attr\"\"\"\n\n    async def a_remove_mod(self, mod_name, spec='app', delete=True):\n        \"\"\"proxi attr\"\"\"\n\n    def exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def web_context(self) -&gt; str:\n        \"\"\"returns the build index ( toolbox web component )\"\"\"\n\n    async def a_exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def save_load(self, modname, spec='app'):\n        \"\"\"proxi attr\"\"\"\n\n    def get_function(self, name: Enum or tuple, **kwargs):\n        \"\"\"\n        Kwargs for _get_function\n            metadata:: return the registered function dictionary\n                stateless: (function_data, None), 0\n                stateful: (function_data, higher_order_function), 0\n            state::boolean\n                specification::str default app\n        \"\"\"\n\n    def run_a_from_sync(self, function, *args):\n        \"\"\"\n        run a async fuction\n        \"\"\"\n\n    def run_bg_task_advanced(self, task, *args, **kwargs):\n        \"\"\"\n        proxi attr\n        \"\"\"\n\n    def wait_for_bg_tasks(self, timeout=None):\n        \"\"\"\n        proxi attr\n        \"\"\"\n\n    def run_bg_task(self, task):\n        \"\"\"\n                run a async fuction\n                \"\"\"\n    def run_function(self, mod_function_name: Enum or tuple,\n                     tb_run_function_with_state=True,\n                     tb_run_with_specification='app',\n                     args_=None,\n                     kwargs_=None,\n                     *args,\n                     **kwargs) -&gt; Result:\n\n        \"\"\"proxi attr\"\"\"\n\n    async def a_run_function(self, mod_function_name: Enum or tuple,\n                             tb_run_function_with_state=True,\n                             tb_run_with_specification='app',\n                             args_=None,\n                             kwargs_=None,\n                             *args,\n                             **kwargs) -&gt; Result:\n\n        \"\"\"proxi attr\"\"\"\n\n    def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n        \"\"\"\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        proxi attr\n        \"\"\"\n\n    async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict):\n        \"\"\"\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        proxi attr\n        \"\"\"\n\n    async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None, method=\"GET\",\n                       args_=None,\n                       kwargs_=None,\n                       *args, **kwargs):\n        \"\"\"run a function remote via http / https\"\"\"\n\n    def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n                get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                kwargs_=None,\n                *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                        backwords_compability_variabel_string_holder=None,\n                        get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                        kwargs_=None,\n                        *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def print(text, *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def sprint(text, *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    # ----------------------------------------------------------------\n    # Decorators for the toolbox\n\n    def _register_function(self, module_name, func_name, data):\n        \"\"\"proxi attr\"\"\"\n\n    def _create_decorator(self, type_: str,\n                          name: str = \"\",\n                          mod_name: str = \"\",\n                          level: int = -1,\n                          restrict_in_virtual_mode: bool = False,\n                          api: bool = False,\n                          helper: str = \"\",\n                          version: str or None = None,\n                          initial=False,\n                          exit_f=False,\n                          test=True,\n                          samples=None,\n                          state=None,\n                          pre_compute=None,\n                          post_compute=None,\n                          memory_cache=False,\n                          file_cache=False,\n                          row=False,\n                          request_as_kwarg=False,\n                          memory_cache_max_size=100,\n                          memory_cache_ttl=300):\n        \"\"\"proxi attr\"\"\"\n\n        # data = {\n        #     \"type\": type_,\n        #     \"module_name\": module_name,\n        #     \"func_name\": func_name,\n        #     \"level\": level,\n        #     \"restrict_in_virtual_mode\": restrict_in_virtual_mode,\n        #     \"func\": func,\n        #     \"api\": api,\n        #     \"helper\": helper,\n        #     \"version\": version,\n        #     \"initial\": initial,\n        #     \"exit_f\": exit_f,\n        #     \"__module__\": func.__module__,\n        #     \"signature\": sig,\n        #     \"params\": params,\n        #     \"state\": (\n        #         False if len(params) == 0 else params[0] in ['self', 'state', 'app']) if state is None else state,\n        #     \"do_test\": test,\n        #     \"samples\": samples,\n        #     \"request_as_kwarg\": request_as_kwarg,\n\n    def tb(self, name=None,\n           mod_name: str = \"\",\n           helper: str = \"\",\n           version: str or None = None,\n           test: bool = True,\n           restrict_in_virtual_mode: bool = False,\n           api: bool = False,\n           initial: bool = False,\n           exit_f: bool = False,\n           test_only: bool = False,\n           memory_cache: bool = False,\n           file_cache: bool = False,\n           row=False,\n           request_as_kwarg: bool = False,\n           state: bool or None = None,\n           level: int = 0,\n           memory_cache_max_size: int = 100,\n           memory_cache_ttl: int = 300,\n           samples: list or dict or None = None,\n           interface: ToolBoxInterfaces or None or str = None,\n           pre_compute=None,\n           post_compute=None,\n           api_methods=None,\n           ):\n        \"\"\"\n    A decorator for registering and configuring functions within a module.\n\n    This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\n    Args:\n        name (str, optional): The name to register the function under. Defaults to the function's own name.\n        mod_name (str, optional): The name of the module the function belongs to.\n        helper (str, optional): A helper string providing additional information about the function.\n        version (str or None, optional): The version of the function or module.\n        test (bool, optional): Flag to indicate if the function is for testing purposes.\n        restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n        api (bool, optional): Flag to indicate if the function is part of an API.\n        initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n        exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n        test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n        memory_cache (bool, optional): Flag to enable memory caching for the function.\n        request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n        file_cache (bool, optional): Flag to enable file caching for the function.\n        row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n        state (bool or None, optional): Flag to indicate if the function maintains state.\n        level (int, optional): The level of the function, used for prioritization or categorization.\n        memory_cache_max_size (int, optional): Maximum size of the memory cache.\n        memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n        samples (list or dict or None, optional): Samples or examples of function usage.\n        interface (str, optional): The interface type for the function.\n        pre_compute (callable, optional): A function to be called before the main function.\n        post_compute (callable, optional): A function to be called after the main function.\n        api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\n    Returns:\n        function: The decorated function with additional processing and registration capabilities.\n    \"\"\"\n        if interface is None:\n            interface = \"tb\"\n        if test_only and 'test' not in self.id:\n            return lambda *args, **kwargs: args\n        return self._create_decorator(interface,\n                                      name,\n                                      mod_name,\n                                      level=level,\n                                      restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                      helper=helper,\n                                      api=api,\n                                      version=version,\n                                      initial=initial,\n                                      exit_f=exit_f,\n                                      test=test,\n                                      samples=samples,\n                                      state=state,\n                                      pre_compute=pre_compute,\n                                      post_compute=post_compute,\n                                      memory_cache=memory_cache,\n                                      file_cache=file_cache,\n                                      row=row,\n                                      request_as_kwarg=request_as_kwarg,\n                                      memory_cache_max_size=memory_cache_max_size,\n                                      memory_cache_ttl=memory_cache_ttl)\n\n    def print_functions(self, name=None):\n\n\n        if not self.functions:\n            print(\"Nothing to see\")\n            return\n\n        def helper(_functions):\n            for func_name, data in _functions.items():\n                if not isinstance(data, dict):\n                    continue\n\n                func_type = data.get('type', 'Unknown')\n                func_level = 'r' if data['level'] == -1 else data['level']\n                api_status = 'Api' if data.get('api', False) else 'Non-Api'\n\n                print(f\"  Function: {func_name}{data.get('signature', '()')}; \"\n                      f\"Type: {func_type}, Level: {func_level}, {api_status}\")\n\n        if name is not None:\n            functions = self.functions.get(name)\n            if functions is not None:\n                print(f\"\\nModule: {name}; Type: {functions.get('app_instance_type', 'Unknown')}\")\n                helper(functions)\n                return\n        for module, functions in self.functions.items():\n            print(f\"\\nModule: {module}; Type: {functions.get('app_instance_type', 'Unknown')}\")\n            helper(functions)\n\n    def save_autocompletion_dict(self):\n        \"\"\"proxi attr\"\"\"\n\n    def get_autocompletion_dict(self):\n        \"\"\"proxi attr\"\"\"\n\n    def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n        \"\"\"proxi attr\"\"\"\n\n    def save_registry_as_enums(self, directory: str, filename: str):\n        \"\"\"proxi attr\"\"\"\n\n    async def execute_all_functions_(self, m_query='', f_query=''):\n        print(\"Executing all functions\")\n        from ..extras import generate_test_cases\n        all_data = {\n            \"modular_run\": 0,\n            \"modular_fatal_error\": 0,\n            \"errors\": 0,\n            \"modular_sug\": 0,\n            \"coverage\": [],\n            \"total_coverage\": {},\n        }\n        items = list(self.functions.items()).copy()\n        for module_name, functions in items:\n            infos = {\n                \"functions_run\": 0,\n                \"functions_fatal_error\": 0,\n                \"error\": 0,\n                \"functions_sug\": 0,\n                'calls': {},\n                'callse': {},\n                \"coverage\": [0, 0],\n            }\n            all_data['modular_run'] += 1\n            if not module_name.startswith(m_query):\n                all_data['modular_sug'] += 1\n                continue\n\n            with Spinner(message=f\"In {module_name}| \"):\n                f_items = list(functions.items()).copy()\n                for function_name, function_data in f_items:\n                    if not isinstance(function_data, dict):\n                        continue\n                    if not function_name.startswith(f_query):\n                        continue\n                    test: list = function_data.get('do_test')\n                    # print(test, module_name, function_name, function_data)\n                    infos[\"coverage\"][0] += 1\n                    if test is False:\n                        continue\n\n                    with Spinner(message=f\"\\t\\t\\t\\t\\t\\tfuction {function_name}...\"):\n                        params: list = function_data.get('params')\n                        sig: signature = function_data.get('signature')\n                        state: bool = function_data.get('state')\n                        samples: bool = function_data.get('samples')\n\n                        test_kwargs_list = [{}]\n\n                        if params is not None:\n                            test_kwargs_list = samples if samples is not None else generate_test_cases(sig=sig)\n                            # print(test_kwargs)\n                            # print(test_kwargs[0])\n                            # test_kwargs = test_kwargs_list[0]\n                        # print(module_name, function_name, test_kwargs_list)\n                        infos[\"coverage\"][1] += 1\n                        for test_kwargs in test_kwargs_list:\n                            try:\n                                # print(f\"test Running {state=} |{module_name}.{function_name}\")\n                                result = await self.a_run_function((module_name, function_name),\n                                                                   tb_run_function_with_state=state,\n                                                                   **test_kwargs)\n                                if not isinstance(result, Result):\n                                    result = Result.ok(result)\n                                if result.info.exec_code == 0:\n                                    infos['calls'][function_name] = [test_kwargs, str(result)]\n                                    infos['functions_sug'] += 1\n                                else:\n                                    infos['functions_sug'] += 1\n                                    infos['error'] += 1\n                                    infos['callse'][function_name] = [test_kwargs, str(result)]\n                            except Exception as e:\n                                infos['functions_fatal_error'] += 1\n                                infos['callse'][function_name] = [test_kwargs, str(e)]\n                            finally:\n                                infos['functions_run'] += 1\n\n                if infos['functions_run'] == infos['functions_sug']:\n                    all_data['modular_sug'] += 1\n                else:\n                    all_data['modular_fatal_error'] += 1\n                if infos['error'] &gt; 0:\n                    all_data['errors'] += infos['error']\n\n                all_data[module_name] = infos\n                if infos['coverage'][0] == 0:\n                    c = 0\n                else:\n                    c = infos['coverage'][1] / infos['coverage'][0]\n                all_data[\"coverage\"].append(f\"{module_name}:{c:.2f}\\n\")\n        total_coverage = sum([float(t.split(\":\")[-1]) for t in all_data[\"coverage\"]]) / len(all_data[\"coverage\"])\n        print(\n            f\"\\n{all_data['modular_run']=}\\n{all_data['modular_sug']=}\\n{all_data['modular_fatal_error']=}\\n{total_coverage=}\")\n        d = analyze_data(all_data)\n        return Result.ok(data=all_data, data_info=d)\n\n    @staticmethod\n    def calculate_complexity(filename_or_code):\n        from radon.complexity import cc_rank, cc_visit\n        if os.path.exists(filename_or_code):\n            with open(filename_or_code) as file:\n                code = file.read()\n        else:\n            code = filename_or_code\n\n        # Calculate and print Cyclomatic Complexity\n        complexity_results = cc_visit(code)\n        i = -1\n        avg_complexity = 0\n        for block in complexity_results:\n            complexity = block.complexity\n            i += 1\n            print(f\"block: {block.name} {i} Class/Fuction/Methode : {block.letter}\")\n            print(f\"    fullname: {block.fullname}\")\n            print(f\"    Cyclomatic Complexity: {complexity}\")\n            # Optional: Get complexity rank\n            avg_complexity += complexity\n            rank = cc_rank(complexity)\n            print(f\"    Complexity Rank: {rank}\")\n            # print(f\"    lineno: {block.lineno}\")\n            print(f\"    endline: {block.endline}\")\n            print(f\"    col_offset: {block.col_offset}\\n\")\n        if i &lt;= 0:\n            i += 2\n        avg_complexity = avg_complexity / i\n        print(f\"\\nAVG Complexity: {avg_complexity:.2f}\")\n        print(f\"Total Rank: {cc_rank(int(avg_complexity + i // 10))}\")\n\n    async def execute_function_test(self, module_name: str, function_name: str,\n                                    function_data: dict, test_kwargs: dict,\n                                    profiler: cProfile.Profile) -&gt; tuple[bool, str, dict, float]:\n        start_time = time.time()\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            try:\n                result = await self.a_run_function(\n                    (module_name, function_name),\n                    tb_run_function_with_state=function_data.get('state'),\n                    **test_kwargs\n                )\n\n                if not isinstance(result, Result):\n                    result = Result.ok(result)\n\n                success = result.info.exec_code == 0\n                execution_time = time.time() - start_time\n                return success, str(result), test_kwargs, execution_time\n            except Exception as e:\n                execution_time = time.time() - start_time\n                return False, str(e), test_kwargs, execution_time\n\n    async def process_function(self, module_name: str, function_name: str,\n                               function_data: dict, profiler: cProfile.Profile) -&gt; tuple[str, ModuleInfo]:\n        start_time = time.time()\n        info = ModuleInfo()\n\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            if not isinstance(function_data, dict):\n                return function_name, info\n\n            test = function_data.get('do_test')\n            info.coverage[0] += 1\n\n            if test is False:\n                return function_name, info\n\n            params = function_data.get('params')\n            sig = function_data.get('signature')\n            samples = function_data.get('samples')\n\n            test_kwargs_list = [{}] if params is None else (\n                samples if samples is not None else generate_test_cases(sig=sig)\n            )\n\n            info.coverage[1] += 1\n\n            # Create tasks for all test cases\n            tasks = [\n                self.execute_function_test(module_name, function_name, function_data, test_kwargs, profiler)\n                for test_kwargs in test_kwargs_list\n            ]\n\n            # Execute all tests concurrently\n            results = await asyncio.gather(*tasks)\n\n            total_execution_time = 0\n            for success, result_str, test_kwargs, execution_time in results:\n                info.functions_run += 1\n                total_execution_time += execution_time\n\n                if success:\n                    info.functions_sug += 1\n                    info.calls[function_name] = [test_kwargs, result_str]\n                else:\n                    info.functions_sug += 1\n                    info.error += 1\n                    info.callse[function_name] = [test_kwargs, result_str]\n\n            info.execution_time = time.time() - start_time\n            return function_name, info\n\n    async def process_module(self, module_name: str, functions: dict,\n                             f_query: str, profiler: cProfile.Profile) -&gt; tuple[str, ModuleInfo]:\n        start_time = time.time()\n\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            async with asyncio.Semaphore(mp.cpu_count()):\n                tasks = [\n                    self.process_function(module_name, fname, fdata, profiler)\n                    for fname, fdata in functions.items()\n                    if fname.startswith(f_query)\n                ]\n\n                if not tasks:\n                    return module_name, ModuleInfo()\n\n                results = await asyncio.gather(*tasks)\n\n                # Combine results from all functions in the module\n                combined_info = ModuleInfo()\n                total_execution_time = 0\n\n                for _, info in results:\n                    combined_info.functions_run += info.functions_run\n                    combined_info.functions_fatal_error += info.functions_fatal_error\n                    combined_info.error += info.error\n                    combined_info.functions_sug += info.functions_sug\n                    combined_info.calls.update(info.calls)\n                    combined_info.callse.update(info.callse)\n                    combined_info.coverage[0] += info.coverage[0]\n                    combined_info.coverage[1] += info.coverage[1]\n                    total_execution_time += info.execution_time\n\n                combined_info.execution_time = time.time() - start_time\n                return module_name, combined_info\n\n    async def execute_all_functions(self, m_query='', f_query='', enable_profiling=True):\n        \"\"\"\n        Execute all functions with parallel processing and optional profiling.\n\n        Args:\n            m_query (str): Module name query filter\n            f_query (str): Function name query filter\n            enable_profiling (bool): Enable detailed profiling information\n        \"\"\"\n        print(\"Executing all functions in parallel\" + (\" with profiling\" if enable_profiling else \"\"))\n\n        start_time = time.time()\n        stats = ExecutionStats()\n        items = list(self.functions.items()).copy()\n\n        # Set up profiling\n        self.enable_profiling = enable_profiling\n        profiler = cProfile.Profile()\n\n        with profile_section(profiler, enable_profiling):\n            # Filter modules based on query\n            filtered_modules = [\n                (mname, mfuncs) for mname, mfuncs in items\n                if mname.startswith(m_query)\n            ]\n\n            stats.modular_run = len(filtered_modules)\n\n            # Process all modules concurrently\n            async with asyncio.Semaphore(mp.cpu_count()):\n                tasks = [\n                    self.process_module(mname, mfuncs, f_query, profiler)\n                    for mname, mfuncs in filtered_modules\n                ]\n\n                results = await asyncio.gather(*tasks)\n\n            # Combine results and calculate statistics\n            for module_name, info in results:\n                if info.functions_run == info.functions_sug:\n                    stats.modular_sug += 1\n                else:\n                    stats.modular_fatal_error += 1\n\n                stats.errors += info.error\n\n                # Calculate coverage\n                coverage = (info.coverage[1] / info.coverage[0]) if info.coverage[0] &gt; 0 else 0\n                stats.coverage.append(f\"{module_name}:{coverage:.2f}\\n\")\n\n                # Store module info\n                stats.__dict__[module_name] = info\n\n            # Calculate total coverage\n            total_coverage = (\n                sum(float(t.split(\":\")[-1]) for t in stats.coverage) / len(stats.coverage)\n                if stats.coverage else 0\n            )\n\n            stats.total_execution_time = time.time() - start_time\n\n            # Generate profiling stats if enabled\n            if enable_profiling:\n                s = io.StringIO()\n                ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n                ps.print_stats()\n                stats.profiling_data = {\n                    'detailed_stats': s.getvalue(),\n                    'total_time': stats.total_execution_time,\n                    'function_count': stats.modular_run,\n                    'successful_functions': stats.modular_sug\n                }\n\n            print(\n                f\"\\n{stats.modular_run=}\"\n                f\"\\n{stats.modular_sug=}\"\n                f\"\\n{stats.modular_fatal_error=}\"\n                f\"\\n{total_coverage=}\"\n                f\"\\nTotal execution time: {stats.total_execution_time:.2f}s\"\n            )\n\n            if enable_profiling:\n                print(\"\\nProfiling Summary:\")\n                print(f\"{'=' * 50}\")\n                print(\"Top 10 time-consuming functions:\")\n                ps.print_stats(10)\n\n            analyzed_data = analyze_data(stats.__dict__)\n            return Result.ok(data=stats.__dict__, data_info=analyzed_data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.debug","title":"<code>debug</code>  <code>property</code> <code>writable</code>","text":"<p>proxi attr</p>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.prefix","title":"<code>prefix = prefix</code>  <code>instance-attribute</code>","text":"<p>proxi attr</p>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.a_exit","title":"<code>a_exit()</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.a_fuction_runner","title":"<code>a_fuction_runner(function, function_data, args, kwargs)</code>  <code>async</code>","text":"<p>parameters = function_data.get('params') modular_name = function_data.get('module_name') function_name = function_data.get('func_name') mod_function_name = f\"{modular_name}.{function_name}\"</p> <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict):\n    \"\"\"\n    parameters = function_data.get('params')\n    modular_name = function_data.get('module_name')\n    function_name = function_data.get('func_name')\n    mod_function_name = f\"{modular_name}.{function_name}\"\n\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.a_remove_mod","title":"<code>a_remove_mod(mod_name, spec='app', delete=True)</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_remove_mod(self, mod_name, spec='app', delete=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.a_run_any","title":"<code>a_run_any(mod_function_name, backwords_compability_variabel_string_holder=None, get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                    backwords_compability_variabel_string_holder=None,\n                    get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                    kwargs_=None,\n                    *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.a_run_function","title":"<code>a_run_function(mod_function_name, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_run_function(self, mod_function_name: Enum or tuple,\n                         tb_run_function_with_state=True,\n                         tb_run_with_specification='app',\n                         args_=None,\n                         kwargs_=None,\n                         *args,\n                         **kwargs) -&gt; Result:\n\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.debug_rains","title":"<code>debug_rains(e)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def debug_rains(self, e):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.disconnect","title":"<code>disconnect(*args, **kwargs)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def disconnect(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.execute_all_functions","title":"<code>execute_all_functions(m_query='', f_query='', enable_profiling=True)</code>  <code>async</code>","text":"<p>Execute all functions with parallel processing and optional profiling.</p> <p>Parameters:</p> Name Type Description Default <code>m_query</code> <code>str</code> <p>Module name query filter</p> <code>''</code> <code>f_query</code> <code>str</code> <p>Function name query filter</p> <code>''</code> <code>enable_profiling</code> <code>bool</code> <p>Enable detailed profiling information</p> <code>True</code> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def execute_all_functions(self, m_query='', f_query='', enable_profiling=True):\n    \"\"\"\n    Execute all functions with parallel processing and optional profiling.\n\n    Args:\n        m_query (str): Module name query filter\n        f_query (str): Function name query filter\n        enable_profiling (bool): Enable detailed profiling information\n    \"\"\"\n    print(\"Executing all functions in parallel\" + (\" with profiling\" if enable_profiling else \"\"))\n\n    start_time = time.time()\n    stats = ExecutionStats()\n    items = list(self.functions.items()).copy()\n\n    # Set up profiling\n    self.enable_profiling = enable_profiling\n    profiler = cProfile.Profile()\n\n    with profile_section(profiler, enable_profiling):\n        # Filter modules based on query\n        filtered_modules = [\n            (mname, mfuncs) for mname, mfuncs in items\n            if mname.startswith(m_query)\n        ]\n\n        stats.modular_run = len(filtered_modules)\n\n        # Process all modules concurrently\n        async with asyncio.Semaphore(mp.cpu_count()):\n            tasks = [\n                self.process_module(mname, mfuncs, f_query, profiler)\n                for mname, mfuncs in filtered_modules\n            ]\n\n            results = await asyncio.gather(*tasks)\n\n        # Combine results and calculate statistics\n        for module_name, info in results:\n            if info.functions_run == info.functions_sug:\n                stats.modular_sug += 1\n            else:\n                stats.modular_fatal_error += 1\n\n            stats.errors += info.error\n\n            # Calculate coverage\n            coverage = (info.coverage[1] / info.coverage[0]) if info.coverage[0] &gt; 0 else 0\n            stats.coverage.append(f\"{module_name}:{coverage:.2f}\\n\")\n\n            # Store module info\n            stats.__dict__[module_name] = info\n\n        # Calculate total coverage\n        total_coverage = (\n            sum(float(t.split(\":\")[-1]) for t in stats.coverage) / len(stats.coverage)\n            if stats.coverage else 0\n        )\n\n        stats.total_execution_time = time.time() - start_time\n\n        # Generate profiling stats if enabled\n        if enable_profiling:\n            s = io.StringIO()\n            ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n            ps.print_stats()\n            stats.profiling_data = {\n                'detailed_stats': s.getvalue(),\n                'total_time': stats.total_execution_time,\n                'function_count': stats.modular_run,\n                'successful_functions': stats.modular_sug\n            }\n\n        print(\n            f\"\\n{stats.modular_run=}\"\n            f\"\\n{stats.modular_sug=}\"\n            f\"\\n{stats.modular_fatal_error=}\"\n            f\"\\n{total_coverage=}\"\n            f\"\\nTotal execution time: {stats.total_execution_time:.2f}s\"\n        )\n\n        if enable_profiling:\n            print(\"\\nProfiling Summary:\")\n            print(f\"{'=' * 50}\")\n            print(\"Top 10 time-consuming functions:\")\n            ps.print_stats(10)\n\n        analyzed_data = analyze_data(stats.__dict__)\n        return Result.ok(data=stats.__dict__, data_info=analyzed_data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.exit","title":"<code>exit()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.exit_main","title":"<code>exit_main(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef exit_main(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.fuction_runner","title":"<code>fuction_runner(function, function_data, args, kwargs, t0=0.0)</code>","text":"<p>parameters = function_data.get('params') modular_name = function_data.get('module_name') function_name = function_data.get('func_name') mod_function_name = f\"{modular_name}.{function_name}\"</p> <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n    \"\"\"\n    parameters = function_data.get('params')\n    modular_name = function_data.get('module_name')\n    function_name = function_data.get('func_name')\n    mod_function_name = f\"{modular_name}.{function_name}\"\n\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.get_all_mods","title":"<code>get_all_mods(working_dir='mods', path_to='./runtime')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.get_autocompletion_dict","title":"<code>get_autocompletion_dict()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_autocompletion_dict(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.get_function","title":"<code>get_function(name, **kwargs)</code>","text":"<p>Kwargs for _get_function     metadata:: return the registered function dictionary         stateless: (function_data, None), 0         stateful: (function_data, higher_order_function), 0     state::boolean         specification::str default app</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_function(self, name: Enum or tuple, **kwargs):\n    \"\"\"\n    Kwargs for _get_function\n        metadata:: return the registered function dictionary\n            stateless: (function_data, None), 0\n            stateful: (function_data, higher_order_function), 0\n        state::boolean\n            specification::str default app\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.get_mod","title":"<code>get_mod(name, spec='app')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.get_username","title":"<code>get_username(get_input=False, default='loot')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.hide_console","title":"<code>hide_console(*args, **kwargs)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def hide_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.inplace_load_instance","title":"<code>inplace_load_instance(mod_name, loc='toolboxv2.mods.', spec='app', save=True)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.load_all_mods_in_file","title":"<code>load_all_mods_in_file(working_dir='mods')</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def load_all_mods_in_file(self, working_dir=\"mods\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.load_mod","title":"<code>load_mod(mod_name, mlm='I', **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def load_mod(self, mod_name: str, mlm='I', **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.mod_online","title":"<code>mod_online(mod_name, installed=False)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def mod_online(self, mod_name, installed=False):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.print","title":"<code>print(text, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef print(text, *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.print_ok","title":"<code>print_ok()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def print_ok(self):\n    \"\"\"proxi attr\"\"\"\n    self.logger.info(\"OK\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.reload_mod","title":"<code>reload_mod(mod_name, spec='app', is_file=True, loc='toolboxv2.mods.')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.remove_mod","title":"<code>remove_mod(mod_name, spec='app', delete=True)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def remove_mod(self, mod_name, spec='app', delete=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.rrun_flows","title":"<code>rrun_flows(name, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def rrun_flows(self, name, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_a_from_sync","title":"<code>run_a_from_sync(function, *args)</code>","text":"<p>run a async fuction</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_a_from_sync(self, function, *args):\n    \"\"\"\n    run a async fuction\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_any","title":"<code>run_any(mod_function_name, backwords_compability_variabel_string_holder=None, get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n            get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n            kwargs_=None,\n            *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_bg_task","title":"<code>run_bg_task(task)</code>","text":"<p>run a async fuction</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_bg_task(self, task):\n    \"\"\"\n            run a async fuction\n            \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_bg_task_advanced","title":"<code>run_bg_task_advanced(task, *args, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_bg_task_advanced(self, task, *args, **kwargs):\n    \"\"\"\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_flows","title":"<code>run_flows(name, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_flows(self, name, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_function","title":"<code>run_function(mod_function_name, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_function(self, mod_function_name: Enum or tuple,\n                 tb_run_function_with_state=True,\n                 tb_run_with_specification='app',\n                 args_=None,\n                 kwargs_=None,\n                 *args,\n                 **kwargs) -&gt; Result:\n\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_http","title":"<code>run_http(mod_function_name, function_name=None, method='GET', args_=None, kwargs_=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>run a function remote via http / https</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None, method=\"GET\",\n                   args_=None,\n                   kwargs_=None,\n                   *args, **kwargs):\n    \"\"\"run a function remote via http / https\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.save_autocompletion_dict","title":"<code>save_autocompletion_dict()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_autocompletion_dict(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.save_exit","title":"<code>save_exit()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.save_initialized_module","title":"<code>save_initialized_module(tools_class, spec)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_initialized_module(self, tools_class, spec):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.save_instance","title":"<code>save_instance(instance, modular_id, spec='app', instance_type='file/application', tools_class=None)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.save_load","title":"<code>save_load(modname, spec='app')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_load(self, modname, spec='app'):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.save_registry_as_enums","title":"<code>save_registry_as_enums(directory, filename)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_registry_as_enums(self, directory: str, filename: str):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.set_flows","title":"<code>set_flows(r)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def set_flows(self, r):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.set_logger","title":"<code>set_logger(debug=False)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def set_logger(self, debug=False):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.show_console","title":"<code>show_console(*args, **kwargs)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def show_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.sprint","title":"<code>sprint(text, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef sprint(text, *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.tb","title":"<code>tb(name=None, mod_name='', helper='', version=None, test=True, restrict_in_virtual_mode=False, api=False, initial=False, exit_f=False, test_only=False, memory_cache=False, file_cache=False, row=False, request_as_kwarg=False, state=None, level=0, memory_cache_max_size=100, memory_cache_ttl=300, samples=None, interface=None, pre_compute=None, post_compute=None, api_methods=None)</code>","text":"<p>A decorator for registering and configuring functions within a module.</p> <p>This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to register the function under. Defaults to the function's own name.</p> <code>None</code> <code>mod_name</code> <code>str</code> <p>The name of the module the function belongs to.</p> <code>''</code> <code>helper</code> <code>str</code> <p>A helper string providing additional information about the function.</p> <code>''</code> <code>version</code> <code>str or None</code> <p>The version of the function or module.</p> <code>None</code> <code>test</code> <code>bool</code> <p>Flag to indicate if the function is for testing purposes.</p> <code>True</code> <code>restrict_in_virtual_mode</code> <code>bool</code> <p>Flag to restrict the function in virtual mode.</p> <code>False</code> <code>api</code> <code>bool</code> <p>Flag to indicate if the function is part of an API.</p> <code>False</code> <code>initial</code> <code>bool</code> <p>Flag to indicate if the function should be executed at initialization.</p> <code>False</code> <code>exit_f</code> <code>bool</code> <p>Flag to indicate if the function should be executed at exit.</p> <code>False</code> <code>test_only</code> <code>bool</code> <p>Flag to indicate if the function should only be used for testing.</p> <code>False</code> <code>memory_cache</code> <code>bool</code> <p>Flag to enable memory caching for the function.</p> <code>False</code> <code>request_as_kwarg</code> <code>bool</code> <p>Flag to get request if the fuction is calld from api.</p> <code>False</code> <code>file_cache</code> <code>bool</code> <p>Flag to enable file caching for the function.</p> <code>False</code> <code>row</code> <code>bool</code> <p>rather to auto wrap the result in Result type default False means no row data aka result type</p> <code>False</code> <code>state</code> <code>bool or None</code> <p>Flag to indicate if the function maintains state.</p> <code>None</code> <code>level</code> <code>int</code> <p>The level of the function, used for prioritization or categorization.</p> <code>0</code> <code>memory_cache_max_size</code> <code>int</code> <p>Maximum size of the memory cache.</p> <code>100</code> <code>memory_cache_ttl</code> <code>int</code> <p>Time-to-live for the memory cache entries.</p> <code>300</code> <code>samples</code> <code>list or dict or None</code> <p>Samples or examples of function usage.</p> <code>None</code> <code>interface</code> <code>str</code> <p>The interface type for the function.</p> <code>None</code> <code>pre_compute</code> <code>callable</code> <p>A function to be called before the main function.</p> <code>None</code> <code>post_compute</code> <code>callable</code> <p>A function to be called after the main function.</p> <code>None</code> <code>api_methods</code> <code>list[str]</code> <p>default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>function</code> <p>The decorated function with additional processing and registration capabilities.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def tb(self, name=None,\n       mod_name: str = \"\",\n       helper: str = \"\",\n       version: str or None = None,\n       test: bool = True,\n       restrict_in_virtual_mode: bool = False,\n       api: bool = False,\n       initial: bool = False,\n       exit_f: bool = False,\n       test_only: bool = False,\n       memory_cache: bool = False,\n       file_cache: bool = False,\n       row=False,\n       request_as_kwarg: bool = False,\n       state: bool or None = None,\n       level: int = 0,\n       memory_cache_max_size: int = 100,\n       memory_cache_ttl: int = 300,\n       samples: list or dict or None = None,\n       interface: ToolBoxInterfaces or None or str = None,\n       pre_compute=None,\n       post_compute=None,\n       api_methods=None,\n       ):\n    \"\"\"\nA decorator for registering and configuring functions within a module.\n\nThis decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\nArgs:\n    name (str, optional): The name to register the function under. Defaults to the function's own name.\n    mod_name (str, optional): The name of the module the function belongs to.\n    helper (str, optional): A helper string providing additional information about the function.\n    version (str or None, optional): The version of the function or module.\n    test (bool, optional): Flag to indicate if the function is for testing purposes.\n    restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n    api (bool, optional): Flag to indicate if the function is part of an API.\n    initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n    exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n    test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n    memory_cache (bool, optional): Flag to enable memory caching for the function.\n    request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n    file_cache (bool, optional): Flag to enable file caching for the function.\n    row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n    state (bool or None, optional): Flag to indicate if the function maintains state.\n    level (int, optional): The level of the function, used for prioritization or categorization.\n    memory_cache_max_size (int, optional): Maximum size of the memory cache.\n    memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n    samples (list or dict or None, optional): Samples or examples of function usage.\n    interface (str, optional): The interface type for the function.\n    pre_compute (callable, optional): A function to be called before the main function.\n    post_compute (callable, optional): A function to be called after the main function.\n    api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\nReturns:\n    function: The decorated function with additional processing and registration capabilities.\n\"\"\"\n    if interface is None:\n        interface = \"tb\"\n    if test_only and 'test' not in self.id:\n        return lambda *args, **kwargs: args\n    return self._create_decorator(interface,\n                                  name,\n                                  mod_name,\n                                  level=level,\n                                  restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                  helper=helper,\n                                  api=api,\n                                  version=version,\n                                  initial=initial,\n                                  exit_f=exit_f,\n                                  test=test,\n                                  samples=samples,\n                                  state=state,\n                                  pre_compute=pre_compute,\n                                  post_compute=post_compute,\n                                  memory_cache=memory_cache,\n                                  file_cache=file_cache,\n                                  row=row,\n                                  request_as_kwarg=request_as_kwarg,\n                                  memory_cache_max_size=memory_cache_max_size,\n                                  memory_cache_ttl=memory_cache_ttl)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.wait_for_bg_tasks","title":"<code>wait_for_bg_tasks(timeout=None)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def wait_for_bg_tasks(self, timeout=None):\n    \"\"\"\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.watch_mod","title":"<code>watch_mod(mod_name, spec='app', loc='toolboxv2.mods.', use_thread=True, path_name=None)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.web_context","title":"<code>web_context()</code>","text":"<p>returns the build index ( toolbox web component )</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def web_context(self) -&gt; str:\n    \"\"\"returns the build index ( toolbox web component )\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainTool","title":"<code>MainTool</code>","text":"Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>class MainTool:\n    toolID: str = \"\"\n    # app = None\n    interface = None\n    spec = \"app\"\n    name = \"\"\n    color = \"Bold\"\n    stuf = False\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.__storedargs = args, kwargs\n        self.tools = kwargs.get(\"tool\", {})\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n        if \"on_exit\" in kwargs and isinstance(kwargs.get(\"on_exit\"), Callable):\n            self.on_exit =self.app.tb(\n                mod_name=self.name,\n                name=kwargs.get(\"on_exit\").__name__,\n                version=self.version if hasattr(self, 'version') else \"0.0.0\",\n            )(kwargs.get(\"on_exit\"))\n        self.async_initialized = False\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    pass\n                else:\n                    self.todo()\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\"Error loading mod {self.name} {e}\")\n                if self.app.debug:\n                    import traceback\n                    traceback.print_exc()\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n\n    async def __ainit__(self, *args, **kwargs):\n        self.version = kwargs[\"v\"]\n        self.tools = kwargs.get(\"tool\", {})\n        self.name = kwargs[\"name\"]\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", None))\n        if not hasattr(self, 'config'):\n            self.config = {}\n        self.user = None\n        self.description = \"A toolbox mod\" if kwargs.get(\"description\") is None else kwargs.get(\"description\")\n        if MainTool.interface is None:\n            MainTool.interface = self.app.interface_type\n        # Result.default(self.app.interface)\n\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    await self.todo()\n                else:\n                    pass\n                await asyncio.sleep(0.1)\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\"Error loading mod {self.name} {e}\")\n                if self.app.debug:\n                    import traceback\n                    traceback.print_exc()\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n        self.app.print(f\"TOOL : {self.spec}.{self.name} online\")\n\n\n\n    @property\n    def app(self):\n        return get_app(\n            from_=f\"{self.spec}.{self.name}|{self.toolID if self.toolID else '*' + MainTool.toolID} {self.interface if self.interface else MainTool.interface}\")\n\n    @app.setter\n    def app(self, v):\n        raise PermissionError(f\"You cannot set the App Instance! {v=}\")\n\n    @staticmethod\n    def return_result(error: ToolBoxError = ToolBoxError.none,\n                      exec_code: int = 0,\n                      help_text: str = \"\",\n                      data_info=None,\n                      data=None,\n                      data_to=None):\n\n        if data_to is None:\n            data_to = MainTool.interface if MainTool.interface is not None else ToolBoxInterfaces.cli\n\n        if data is None:\n            data = {}\n\n        if data_info is None:\n            data_info = {}\n\n        return Result(\n            error,\n            ToolBoxResult(data_info=data_info, data=data, data_to=data_to),\n            ToolBoxInfo(exec_code=exec_code, help_text=help_text)\n        )\n\n    def print(self, message, end=\"\\n\", **kwargs):\n        if self.stuf:\n            return\n\n        self.app.print(Style.style_dic[self.color] + self.name + Style.style_dic[\"END\"] + \":\", message, end=end,\n                       **kwargs)\n\n    def add_str_to_config(self, command):\n        if len(command) != 2:\n            self.logger.error('Invalid command must be key value')\n            return False\n        self.config[command[0]] = command[1]\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n\n    def get_version(self) -&gt; str:\n        \"\"\"\"Returns the version\"\"\"\n        return self.version\n\n    async def get_user(self, username: str) -&gt; Result:\n        return await self.app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME, username=username, get_results=True)\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainTool.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.__storedargs = args, kwargs\n    self.tools = kwargs.get(\"tool\", {})\n    self.logger = kwargs.get(\"logs\", get_logger())\n    self.color = kwargs.get(\"color\", \"WHITE\")\n    self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n    if \"on_exit\" in kwargs and isinstance(kwargs.get(\"on_exit\"), Callable):\n        self.on_exit =self.app.tb(\n            mod_name=self.name,\n            name=kwargs.get(\"on_exit\").__name__,\n            version=self.version if hasattr(self, 'version') else \"0.0.0\",\n        )(kwargs.get(\"on_exit\"))\n    self.async_initialized = False\n    if self.todo:\n        try:\n            if inspect.iscoroutinefunction(self.todo):\n                pass\n            else:\n                self.todo()\n            get_logger().info(f\"{self.name} on load suspended\")\n        except Exception as e:\n            get_logger().error(f\"Error loading mod {self.name} {e}\")\n            if self.app.debug:\n                import traceback\n                traceback.print_exc()\n    else:\n        get_logger().info(f\"{self.name} no load require\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainTool.__initobj","title":"<code>__initobj()</code>  <code>async</code>","text":"<p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainTool.get_version","title":"<code>get_version()</code>","text":"<p>\"Returns the version</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\"Returns the version\"\"\"\n    return self.version\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainTool.webInstall","title":"<code>webInstall(user_instance, construct_render)</code>","text":"<p>\"Returns a web installer for the given user instance and construct render template</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def webInstall(self, user_instance, construct_render) -&gt; str:\n    \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainToolType","title":"<code>MainToolType</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class MainToolType:\n    toolID: str\n    app: A\n    interface: ToolBoxInterfaces\n    spec: str\n\n    version: str\n    tools: dict  # legacy\n    name: str\n    logger: logging\n    color: str\n    todo: Callable\n    _on_exit: Callable\n    stuf: bool\n    config: dict\n    user: U | None\n    description: str\n\n    @staticmethod\n    def return_result(error: ToolBoxError = ToolBoxError.none,\n                      exec_code: int = 0,\n                      help_text: str = \"\",\n                      data_info=None,\n                      data=None,\n                      data_to=None) -&gt; Result:\n        \"\"\"proxi attr\"\"\"\n\n    def load(self):\n        \"\"\"proxi attr\"\"\"\n\n    def print(self, message, end=\"\\n\", **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def add_str_to_config(self, command):\n        if len(command) != 2:\n            self.logger.error('Invalid command must be key value')\n            return False\n        self.config[command[0]] = command[1]\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n\n    async def get_user(self, username: str) -&gt; Result:\n        return self.app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME, username=username, get_results=True)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainToolType.load","title":"<code>load()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def load(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainToolType.print","title":"<code>print(message, end='\\n', **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def print(self, message, end=\"\\n\", **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainToolType.return_result","title":"<code>return_result(error=ToolBoxError.none, exec_code=0, help_text='', data_info=None, data=None, data_to=None)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef return_result(error: ToolBoxError = ToolBoxError.none,\n                  exec_code: int = 0,\n                  help_text: str = \"\",\n                  data_info=None,\n                  data=None,\n                  data_to=None) -&gt; Result:\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainToolType.webInstall","title":"<code>webInstall(user_instance, construct_render)</code>","text":"<p>\"Returns a web installer for the given user instance and construct render template</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def webInstall(self, user_instance, construct_render) -&gt; str:\n    \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result","title":"<code>Result</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class Result:\n    _task = None\n    def __init__(self,\n                 error: ToolBoxError,\n                 result: ToolBoxResult,\n                 info: ToolBoxInfo,\n                 origin: Any | None = None,\n                 ):\n        self.error: ToolBoxError = error\n        self.result: ToolBoxResult = result\n        self.info: ToolBoxInfo = info\n        self.origin = origin\n\n    def as_result(self):\n        return self\n\n    def as_dict(self):\n        return {\n            \"error\":self.error.value if isinstance(self.error, Enum) else self.error,\n        \"result\" : {\n            \"data_to\":self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n            \"data_info\":self.result.data_info,\n            \"data\":self.result.data,\n            \"data_type\":self.result.data_type\n        } if self.result else None,\n        \"info\" : {\n            \"exec_code\" : self.info.exec_code,  # exec_code umwandel in http resposn codes\n        \"help_text\" : self.info.help_text\n        } if self.info else None,\n        \"origin\" : self.origin\n        }\n\n    def set_origin(self, origin):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = origin\n        return self\n\n    def set_dir_origin(self, name, extras=\"assets/\"):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = f\"mods/{name}/{extras}\"\n        return self\n\n    def is_error(self):\n        if _test_is_result(self.result.data):\n            return self.result.data.is_error()\n        if self.error == ToolBoxError.none:\n            return False\n        if self.info.exec_code == 0:\n            return False\n        if self.info.exec_code == 200:\n            return False\n        return True\n\n    def is_data(self):\n        return self.result.data is not None\n\n    def to_api_result(self):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=self.error.value if isinstance(self.error, Enum) else self.error,\n            result=ToolBoxResultBM(\n                data_to=self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n                data_info=self.result.data_info,\n                data=self.result.data,\n                data_type=self.result.data_type\n            ) if self.result else None,\n            info=ToolBoxInfoBM(\n                exec_code=self.info.exec_code,  # exec_code umwandel in http resposn codes\n                help_text=self.info.help_text\n            ) if self.info else None,\n            origin=self.origin\n        )\n\n    def task(self, task):\n        self._task = task\n        return self\n\n    @staticmethod\n    def result_from_dict(error: str, result: dict, info: dict, origin: list or None or str):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=error if isinstance(error, Enum) else error,\n            result=ToolBoxResultBM(\n                data_to=result.get('data_to') if isinstance(result.get('data_to'), Enum) else result.get('data_to'),\n                data_info=result.get('data_info', '404'),\n                data=result.get('data'),\n                data_type=result.get('data_type', '404'),\n            ) if result else ToolBoxResultBM(\n                data_to=ToolBoxInterfaces.cli.value,\n                data_info='',\n                data='404',\n                data_type='404',\n            ),\n            info=ToolBoxInfoBM(\n                exec_code=info.get('exec_code', 404),\n                help_text=info.get('help_text', '404')\n            ) if info else ToolBoxInfoBM(\n                exec_code=404,\n                help_text='404'\n            ),\n            origin=origin\n        ).as_result()\n\n    @classmethod\n    def stream(cls,\n               stream_generator: Any,  # Renamed from source for clarity\n               content_type: str = \"text/event-stream\",  # Default to SSE\n               headers: Union[dict, None] = None,\n               info: str = \"OK\",\n               interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n               cleanup_func: Union[\n                   Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None):\n        \"\"\"\n        Create a streaming response Result. Handles SSE and other stream types.\n\n        Args:\n            stream_generator: Any stream source (async generator, sync generator, iterable, or single item).\n            content_type: Content-Type header (default: text/event-stream for SSE).\n            headers: Additional HTTP headers for the response.\n            info: Help text for the result.\n            interface: Interface to send data to.\n            cleanup_func: Optional function for cleanup.\n\n        Returns:\n            A Result object configured for streaming.\n        \"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        final_generator: AsyncGenerator[str, None]\n\n        if content_type == \"text/event-stream\":\n            # For SSE, always use SSEGenerator.create_sse_stream to wrap the source.\n            # SSEGenerator.create_sse_stream handles various types of stream_generator internally.\n            final_generator = SSEGenerator.create_sse_stream(source=stream_generator, cleanup_func=cleanup_func)\n\n            # Standard SSE headers for the HTTP response itself\n            # These will be stored in the Result object. Rust side decides how to use them.\n            standard_sse_headers = {\n                \"Cache-Control\": \"no-cache\",  # SSE specific\n                \"Connection\": \"keep-alive\",  # SSE specific\n                \"X-Accel-Buffering\": \"no\",  # Useful for proxies with SSE\n                # Content-Type is implicitly text/event-stream, will be in streaming_data below\n            }\n            all_response_headers = standard_sse_headers.copy()\n            if headers:\n                all_response_headers.update(headers)\n        else:\n            # For non-SSE streams.\n            # If stream_generator is sync, wrap it to be async.\n            # If already async or single item, it will be handled.\n            # Rust's stream_generator in ToolboxClient seems to handle both sync/async Python generators.\n            # For consistency with how SSEGenerator does it, we can wrap sync ones.\n            if inspect.isgenerator(stream_generator) or \\\n                (not isinstance(stream_generator, str) and hasattr(stream_generator, '__iter__')):\n                final_generator = SSEGenerator.wrap_sync_generator(stream_generator)  # Simple async wrapper\n            elif inspect.isasyncgen(stream_generator):\n                final_generator = stream_generator\n            else:  # Single item or string\n                async def _single_item_gen():\n                    yield stream_generator\n\n                final_generator = _single_item_gen()\n            all_response_headers = headers if headers else {}\n\n        # Prepare streaming data to be stored in the Result object\n        streaming_data = {\n            \"type\": \"stream\",  # Indicator for Rust side\n            \"generator\": final_generator,\n            \"content_type\": content_type,  # Let Rust know the intended content type\n            \"headers\": all_response_headers  # Intended HTTP headers for the overall response\n        }\n\n        result_payload = ToolBoxResult(\n            data_to=interface,\n            data=streaming_data,\n            data_info=\"Streaming response\" if content_type != \"text/event-stream\" else \"SSE Event Stream\",\n            data_type=\"stream\"  # Generic type for Rust to identify it needs to stream from 'generator'\n        )\n\n        return cls(error=error, info=info_obj, result=result_payload)\n\n    @classmethod\n    def sse(cls,\n            stream_generator: Any,\n            info: str = \"OK\",\n            interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n            cleanup_func: Union[\n                Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None,\n            # http_headers: Optional[dict] = None # If we want to allow overriding default SSE HTTP headers\n            ):\n        \"\"\"\n        Create an Server-Sent Events (SSE) streaming response Result.\n\n        Args:\n            stream_generator: A source yielding individual data items. This can be an\n                              async generator, sync generator, iterable, or a single item.\n                              Each item will be formatted as an SSE event.\n            info: Optional help text for the Result.\n            interface: Optional ToolBoxInterface to target.\n            cleanup_func: Optional cleanup function to run when the stream ends or is cancelled.\n            #http_headers: Optional dictionary of custom HTTP headers for the SSE response.\n\n        Returns:\n            A Result object configured for SSE streaming.\n        \"\"\"\n        # Result.stream will handle calling SSEGenerator.create_sse_stream\n        # and setting appropriate default headers for SSE when content_type is \"text/event-stream\".\n        return cls.stream(\n            stream_generator=stream_generator,\n            content_type=\"text/event-stream\",\n            # headers=http_headers, # Pass if we add http_headers param\n            info=info,\n            interface=interface,\n            cleanup_func=cleanup_func\n        )\n\n    @classmethod\n    def default(cls, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=-1, help_text=\"\")\n        result = ToolBoxResult(data_to=interface)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None):\n        \"\"\"Create a JSON response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code or exec_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=data,\n            data_info=\"JSON response\",\n            data_type=\"json\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n        \"\"\"Create a text response Result with specific content type.\"\"\"\n        if headers is not None:\n            return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=text_data,\n            data_info=\"Text response\",\n            data_type=content_type\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n               interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a binary data response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        # Create a dictionary with binary data and metadata\n        binary_data = {\n            \"data\": data,\n            \"content_type\": content_type,\n            \"filename\": download_name\n        }\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=binary_data,\n            data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n            data_type=\"binary\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def file(cls, data, filename, content_type=None, info=\"OK\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a file download response Result.\n\n        Args:\n            data: File data as bytes or base64 string\n            filename: Name of the file for download\n            content_type: MIME type of the file (auto-detected if None)\n            info: Response info text\n            interface: Target interface\n\n        Returns:\n            Result object configured for file download\n        \"\"\"\n        import base64\n        import mimetypes\n\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=200, help_text=info)\n\n        # Auto-detect content type if not provided\n        if content_type is None:\n            content_type, _ = mimetypes.guess_type(filename)\n            if content_type is None:\n                content_type = \"application/octet-stream\"\n\n        # Ensure data is base64 encoded string (as expected by Rust server)\n        if isinstance(data, bytes):\n            base64_data = base64.b64encode(data).decode('utf-8')\n        elif isinstance(data, str):\n            # Assume it's already base64 encoded\n            base64_data = data\n        else:\n            raise ValueError(\"File data must be bytes or base64 string\")\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=base64_data,  # Rust expects base64 string for \"file\" type\n            data_info=f\"File download: {filename}\",\n            data_type=\"file\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a redirect response.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=url,\n            data_info=\"Redirect response\",\n            data_type=\"redirect\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def ok(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def html(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.remote, data_type=\"html\",status=200, headers=None, row=False):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=status, help_text=info)\n        from ...utils.system.getting_and_closing_app import get_app\n\n        if not row and not '\"&lt;div class=\"main-content\"\"' in data:\n            data = f'&lt;div class=\"main-content frosted-glass\"&gt;{data}&lt;div&gt;'\n        if not row and not get_app().web_context() in data:\n            data = get_app().web_context() + data\n\n        if isinstance(headers, dict):\n            result = ToolBoxResult(data_to=interface, data={'html':data,'headers':headers}, data_info=data_info,\n                                   data_type=\"special_html\")\n        else:\n            result = ToolBoxResult(data_to=interface, data=data, data_info=data_info,\n                                   data_type=data_type if data_type is not None else type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def future(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.future):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=\"future\")\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def custom_error(cls, data=None, data_info=\"\", info=\"\", exec_code=-1, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def error(cls, data=None, data_info=\"\", info=\"\", exec_code=450, interface=ToolBoxInterfaces.remote):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_user_error(cls, info=\"\", exec_code=-3, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.input_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_internal_error(cls, info=\"\", exec_code=-2, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.internal_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    def print(self, show=True, show_data=True, prifix=\"\"):\n        data = '\\n' + f\"{((prifix + 'Data: ' + str(self.result.data) if self.result.data is not None else 'NO Data') if not isinstance(self.result.data, Result) else self.result.data.print(show=False, show_data=show_data, prifix=prifix + '-')) if show_data else 'Data: private'}\"\n        origin = '\\n' + f\"{prifix + 'Origin: ' + str(self.origin) if self.origin is not None else 'NO Origin'}\"\n        text = (f\"Function Exec code: {self.info.exec_code}\"\n                f\"\\n{prifix}Info's:\"\n                f\" {self.info.help_text} {'&lt;|&gt; ' + str(self.result.data_info) if self.result.data_info is not None else ''}\"\n                f\"{origin}{data if not data.endswith('NO Data') else ''}\")\n        if not show:\n            return text\n        print(\"\\n======== Result ========\\n\" + text + \"\\n------- EndOfD -------\")\n        return self\n\n    def log(self, show_data=True, prifix=\"\"):\n        from toolboxv2 import get_logger\n        get_logger().debug(self.print(show=False, show_data=show_data, prifix=prifix).replace(\"\\n\", \" - \"))\n        return self\n\n    def __str__(self):\n        return self.print(show=False, show_data=True)\n\n    def get(self, key=None, default=None):\n        data = self.result.data\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    async def aget(self, key=None, default=None):\n        if asyncio.isfuture(self.result.data) or asyncio.iscoroutine(self.result.data) or (\n            isinstance(self.result.data_to, Enum) and self.result.data_to.name == ToolBoxInterfaces.future.name):\n            data = await self.result.data\n        else:\n            data = self.get(key=None, default=None)\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    def lazy_return(self, _=0, data=None, **kwargs):\n        flags = ['raise', 'logg', 'user', 'intern']\n        flag = flags[_] if isinstance(_, int) else _\n        if self.info.exec_code == 0:\n            return self if data is None else data if _test_is_result(data) else self.ok(data=data, **kwargs)\n        if flag == 'raise':\n            raise ValueError(self.print(show=False))\n        if flag == 'logg':\n            from .. import get_logger\n            get_logger().error(self.print(show=False))\n\n        if flag == 'user':\n            return self if data is None else data if _test_is_result(data) else self.default_user_error(data=data,\n                                                                                                        **kwargs)\n        if flag == 'intern':\n            return self if data is None else data if _test_is_result(data) else self.default_internal_error(data=data,\n                                                                                                            **kwargs)\n\n        return self if data is None else data if _test_is_result(data) else self.custom_error(data=data, **kwargs)\n\n    @property\n    def bg_task(self):\n        return self._task\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.binary","title":"<code>binary(data, content_type='application/octet-stream', download_name=None, info='OK', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a binary data response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n           interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a binary data response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    # Create a dictionary with binary data and metadata\n    binary_data = {\n        \"data\": data,\n        \"content_type\": content_type,\n        \"filename\": download_name\n    }\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=binary_data,\n        data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n        data_type=\"binary\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.file","title":"<code>file(data, filename, content_type=None, info='OK', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a file download response Result.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>File data as bytes or base64 string</p> required <code>filename</code> <p>Name of the file for download</p> required <code>content_type</code> <p>MIME type of the file (auto-detected if None)</p> <code>None</code> <code>info</code> <p>Response info text</p> <code>'OK'</code> <code>interface</code> <p>Target interface</p> <code>remote</code> <p>Returns:</p> Type Description <p>Result object configured for file download</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef file(cls, data, filename, content_type=None, info=\"OK\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a file download response Result.\n\n    Args:\n        data: File data as bytes or base64 string\n        filename: Name of the file for download\n        content_type: MIME type of the file (auto-detected if None)\n        info: Response info text\n        interface: Target interface\n\n    Returns:\n        Result object configured for file download\n    \"\"\"\n    import base64\n    import mimetypes\n\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=200, help_text=info)\n\n    # Auto-detect content type if not provided\n    if content_type is None:\n        content_type, _ = mimetypes.guess_type(filename)\n        if content_type is None:\n            content_type = \"application/octet-stream\"\n\n    # Ensure data is base64 encoded string (as expected by Rust server)\n    if isinstance(data, bytes):\n        base64_data = base64.b64encode(data).decode('utf-8')\n    elif isinstance(data, str):\n        # Assume it's already base64 encoded\n        base64_data = data\n    else:\n        raise ValueError(\"File data must be bytes or base64 string\")\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=base64_data,  # Rust expects base64 string for \"file\" type\n        data_info=f\"File download: {filename}\",\n        data_type=\"file\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.json","title":"<code>json(data, info='OK', interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None)</code>  <code>classmethod</code>","text":"<p>Create a JSON response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None):\n    \"\"\"Create a JSON response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code or exec_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=data,\n        data_info=\"JSON response\",\n        data_type=\"json\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.redirect","title":"<code>redirect(url, status_code=302, info='Redirect', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a redirect response.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a redirect response.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=url,\n        data_info=\"Redirect response\",\n        data_type=\"redirect\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.sse","title":"<code>sse(stream_generator, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code>  <code>classmethod</code>","text":"<p>Create an Server-Sent Events (SSE) streaming response Result.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <code>Any</code> <p>A source yielding individual data items. This can be an               async generator, sync generator, iterable, or a single item.               Each item will be formatted as an SSE event.</p> required <code>info</code> <code>str</code> <p>Optional help text for the Result.</p> <code>'OK'</code> <code>interface</code> <code>ToolBoxInterfaces</code> <p>Optional ToolBoxInterface to target.</p> <code>remote</code> <code>cleanup_func</code> <code>Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None]</code> <p>Optional cleanup function to run when the stream ends or is cancelled.</p> <code>None</code> <code>#http_headers</code> <p>Optional dictionary of custom HTTP headers for the SSE response.</p> required <p>Returns:</p> Type Description <p>A Result object configured for SSE streaming.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef sse(cls,\n        stream_generator: Any,\n        info: str = \"OK\",\n        interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n        cleanup_func: Union[\n            Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None,\n        # http_headers: Optional[dict] = None # If we want to allow overriding default SSE HTTP headers\n        ):\n    \"\"\"\n    Create an Server-Sent Events (SSE) streaming response Result.\n\n    Args:\n        stream_generator: A source yielding individual data items. This can be an\n                          async generator, sync generator, iterable, or a single item.\n                          Each item will be formatted as an SSE event.\n        info: Optional help text for the Result.\n        interface: Optional ToolBoxInterface to target.\n        cleanup_func: Optional cleanup function to run when the stream ends or is cancelled.\n        #http_headers: Optional dictionary of custom HTTP headers for the SSE response.\n\n    Returns:\n        A Result object configured for SSE streaming.\n    \"\"\"\n    # Result.stream will handle calling SSEGenerator.create_sse_stream\n    # and setting appropriate default headers for SSE when content_type is \"text/event-stream\".\n    return cls.stream(\n        stream_generator=stream_generator,\n        content_type=\"text/event-stream\",\n        # headers=http_headers, # Pass if we add http_headers param\n        info=info,\n        interface=interface,\n        cleanup_func=cleanup_func\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.stream","title":"<code>stream(stream_generator, content_type='text/event-stream', headers=None, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code>  <code>classmethod</code>","text":"<p>Create a streaming response Result. Handles SSE and other stream types.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <code>Any</code> <p>Any stream source (async generator, sync generator, iterable, or single item).</p> required <code>content_type</code> <code>str</code> <p>Content-Type header (default: text/event-stream for SSE).</p> <code>'text/event-stream'</code> <code>headers</code> <code>Union[dict, None]</code> <p>Additional HTTP headers for the response.</p> <code>None</code> <code>info</code> <code>str</code> <p>Help text for the result.</p> <code>'OK'</code> <code>interface</code> <code>ToolBoxInterfaces</code> <p>Interface to send data to.</p> <code>remote</code> <code>cleanup_func</code> <code>Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None]</code> <p>Optional function for cleanup.</p> <code>None</code> <p>Returns:</p> Type Description <p>A Result object configured for streaming.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef stream(cls,\n           stream_generator: Any,  # Renamed from source for clarity\n           content_type: str = \"text/event-stream\",  # Default to SSE\n           headers: Union[dict, None] = None,\n           info: str = \"OK\",\n           interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n           cleanup_func: Union[\n               Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None):\n    \"\"\"\n    Create a streaming response Result. Handles SSE and other stream types.\n\n    Args:\n        stream_generator: Any stream source (async generator, sync generator, iterable, or single item).\n        content_type: Content-Type header (default: text/event-stream for SSE).\n        headers: Additional HTTP headers for the response.\n        info: Help text for the result.\n        interface: Interface to send data to.\n        cleanup_func: Optional function for cleanup.\n\n    Returns:\n        A Result object configured for streaming.\n    \"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    final_generator: AsyncGenerator[str, None]\n\n    if content_type == \"text/event-stream\":\n        # For SSE, always use SSEGenerator.create_sse_stream to wrap the source.\n        # SSEGenerator.create_sse_stream handles various types of stream_generator internally.\n        final_generator = SSEGenerator.create_sse_stream(source=stream_generator, cleanup_func=cleanup_func)\n\n        # Standard SSE headers for the HTTP response itself\n        # These will be stored in the Result object. Rust side decides how to use them.\n        standard_sse_headers = {\n            \"Cache-Control\": \"no-cache\",  # SSE specific\n            \"Connection\": \"keep-alive\",  # SSE specific\n            \"X-Accel-Buffering\": \"no\",  # Useful for proxies with SSE\n            # Content-Type is implicitly text/event-stream, will be in streaming_data below\n        }\n        all_response_headers = standard_sse_headers.copy()\n        if headers:\n            all_response_headers.update(headers)\n    else:\n        # For non-SSE streams.\n        # If stream_generator is sync, wrap it to be async.\n        # If already async or single item, it will be handled.\n        # Rust's stream_generator in ToolboxClient seems to handle both sync/async Python generators.\n        # For consistency with how SSEGenerator does it, we can wrap sync ones.\n        if inspect.isgenerator(stream_generator) or \\\n            (not isinstance(stream_generator, str) and hasattr(stream_generator, '__iter__')):\n            final_generator = SSEGenerator.wrap_sync_generator(stream_generator)  # Simple async wrapper\n        elif inspect.isasyncgen(stream_generator):\n            final_generator = stream_generator\n        else:  # Single item or string\n            async def _single_item_gen():\n                yield stream_generator\n\n            final_generator = _single_item_gen()\n        all_response_headers = headers if headers else {}\n\n    # Prepare streaming data to be stored in the Result object\n    streaming_data = {\n        \"type\": \"stream\",  # Indicator for Rust side\n        \"generator\": final_generator,\n        \"content_type\": content_type,  # Let Rust know the intended content type\n        \"headers\": all_response_headers  # Intended HTTP headers for the overall response\n    }\n\n    result_payload = ToolBoxResult(\n        data_to=interface,\n        data=streaming_data,\n        data_info=\"Streaming response\" if content_type != \"text/event-stream\" else \"SSE Event Stream\",\n        data_type=\"stream\"  # Generic type for Rust to identify it needs to stream from 'generator'\n    )\n\n    return cls(error=error, info=info_obj, result=result_payload)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.text","title":"<code>text(text_data, content_type='text/plain', exec_code=None, status=200, info='OK', interface=ToolBoxInterfaces.remote, headers=None)</code>  <code>classmethod</code>","text":"<p>Create a text response Result with specific content type.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n    \"\"\"Create a text response Result with specific content type.\"\"\"\n    if headers is not None:\n        return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=text_data,\n        data_info=\"Text response\",\n        data_type=content_type\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.all_functions_enums","title":"<code>all_functions_enums</code>","text":"<p>Automatic generated by ToolBox v = 0.1.21</p>"},{"location":"toolboxv2/#toolboxv2.utils.system.api","title":"<code>api</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.api.build_cargo_project","title":"<code>build_cargo_project(debug=False)</code>","text":"<p>Build the Cargo project, optionally in debug mode.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def build_cargo_project(debug=False):\n    \"\"\"Build the Cargo project, optionally in debug mode.\"\"\"\n    mode = \"debug\" if debug else \"release\"\n    args = [\"cargo\", \"build\"]\n    if not debug:\n        args.append(\"--release\")\n\n    print(f\"Building in {mode} mode...\")\n    try:\n        subprocess.run(args, cwd=os.path.join(\".\", \"src-core\"), check=True)\n        exe_path = get_executable_name_with_extension()\n        if exe_path:\n            bin_dir = tb_root_dir / \"bin\"\n            bin_dir.mkdir(exist_ok=True)\n            exe_path = Path(exe_path)\n            try:\n                shutil.copy(exe_path, bin_dir / exe_path.name)\n            except Exception as e:\n                bin_dir = tb_root_dir / \"ubin\"\n                bin_dir.mkdir(exist_ok=True)\n                (bin_dir / exe_path.name).unlink(missing_ok=True)\n                try:\n                    shutil.copy(exe_path, bin_dir / exe_path.name)\n                except Exception as e:\n                    print(f\"Failed to copy executable: {e}\")\n            print(f\"Copied executable to '{bin_dir.resolve()}'\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Cargo build failed: {e}\")\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.check_cargo_installed","title":"<code>check_cargo_installed()</code>","text":"<p>Check if Cargo (Rust package manager) is installed on the system.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def check_cargo_installed():\n    \"\"\"Check if Cargo (Rust package manager) is installed on the system.\"\"\"\n    try:\n        subprocess.run([\"cargo\", \"--version\"], check=True, capture_output=True)\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.cleanup_build_files","title":"<code>cleanup_build_files()</code>","text":"<p>Cleans up build files.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def cleanup_build_files():\n    \"\"\"Cleans up build files.\"\"\"\n    src_core_path = os.path.join(\".\", \"src-core\")\n    target_path = os.path.join(src_core_path, \"target\")\n\n    if os.path.exists(target_path):\n        try:\n            print(f\"Cleaning up build files in {target_path}...\")\n            # First try using cargo clean\n            try:\n                subprocess.run([\"cargo\", \"clean\"], cwd=src_core_path, check=True)\n                print(\"Successfully cleaned up build files with cargo clean\")\n            except subprocess.CalledProcessError:\n                # If cargo clean fails, manually remove directories\n                print(\"Cargo clean failed, manually removing build directories...\")\n                for item in os.listdir(target_path):\n                    item_path = os.path.join(target_path, item)\n                    if os.path.isdir(item_path) and item != \".rustc_info.json\":\n                        shutil.rmtree(item_path)\n                        print(f\"Removed {item_path}\")\n            return True\n        except Exception as e:\n            print(f\"Failed to clean up build files: {e}\")\n            return False\n    else:\n        print(f\"Build directory {target_path} not found\")\n        return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.detect_os_and_arch","title":"<code>detect_os_and_arch()</code>","text":"<p>Detect the current operating system and architecture.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def detect_os_and_arch():\n    \"\"\"Detect the current operating system and architecture.\"\"\"\n    current_os = platform.system().lower()  # e.g., 'windows', 'linux', 'darwin'\n    machine = platform.machine().lower()  # e.g., 'x86_64', 'amd64'\n    return current_os, machine\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.download_executable","title":"<code>download_executable(url, file_name)</code>","text":"<p>Attempt to download the executable from the provided URL.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def download_executable(url, file_name):\n    \"\"\"Attempt to download the executable from the provided URL.\"\"\"\n    try:\n        import requests\n    except ImportError:\n        print(\"The 'requests' library is required. Please install it via pip install requests\")\n        sys.exit(1)\n\n    print(f\"Attempting to download executable from {url}...\")\n    try:\n        response = requests.get(url, stream=True)\n    except Exception as e:\n        print(f\"Download error: {e}\")\n        return None\n\n    if response.status_code == 200:\n        with open(file_name, \"wb\") as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:\n                    f.write(chunk)\n        # Make the file executable on non-Windows systems\n        if platform.system().lower() != \"windows\":\n            os.chmod(file_name, 0o755)\n        return file_name\n    else:\n        print(\"Download failed. Status code:\", response.status_code)\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.find_highest_zip_version","title":"<code>find_highest_zip_version(name_filter, app_version=None, root_dir='mods_sto', version_only=False)</code>","text":"<p>Findet die h\u00f6chste verf\u00fcgbare ZIP-Version in einem Verzeichnis basierend auf einem Namensfilter.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Wurzelverzeichnis f\u00fcr die Suche</p> <code>'mods_sto'</code> <code>name_filter</code> <code>str</code> <p>Namensfilter f\u00fcr die ZIP-Dateien</p> required <code>app_version</code> <code>str</code> <p>Aktuelle App-Version f\u00fcr Kompatibilit\u00e4tspr\u00fcfung</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Pfad zur ZIP-Datei mit der h\u00f6chsten Version oder None wenn keine gefunden</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def find_highest_zip_version(name_filter: str, app_version: str = None, root_dir: str = \"mods_sto\", version_only=False) -&gt; str:\n    \"\"\"\n    Findet die h\u00f6chste verf\u00fcgbare ZIP-Version in einem Verzeichnis basierend auf einem Namensfilter.\n\n    Args:\n        root_dir (str): Wurzelverzeichnis f\u00fcr die Suche\n        name_filter (str): Namensfilter f\u00fcr die ZIP-Dateien\n        app_version (str, optional): Aktuelle App-Version f\u00fcr Kompatibilit\u00e4tspr\u00fcfung\n\n    Returns:\n        str: Pfad zur ZIP-Datei mit der h\u00f6chsten Version oder None wenn keine gefunden\n    \"\"\"\n\n    # Kompiliere den Regex-Pattern f\u00fcr die Dateinamen\n    pattern = fr\"{name_filter}&amp;v[0-9.]+\u00a7([0-9.]+)\\.zip$\"\n\n    highest_version = None\n    highest_version_file = None\n\n    # Durchsuche das Verzeichnis\n    root_path = Path(root_dir)\n    for file_path in root_path.rglob(\"*.zip\"):\n        if \"RST$\"+name_filter not in str(file_path):\n            continue\n        match = re.search(pattern, str(file_path).split(\"RST$\")[-1].strip())\n        if match:\n            zip_version = match.group(1)\n\n            # Pr\u00fcfe App-Version Kompatibilit\u00e4t falls angegeben\n            if app_version:\n                file_app_version = re.search(r\"&amp;v([0-9.]+)\u00a7\", str(file_path)).group(1)\n                if version.parse(file_app_version) &gt; version.parse(app_version):\n                    continue\n\n            # Vergleiche Versionen\n            current_version = version.parse(zip_version)\n            if highest_version is None or current_version &gt; highest_version:\n                highest_version = current_version\n                highest_version_file = str(file_path)\n    if version_only:\n        return str(highest_version)\n    return highest_version_file\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.find_highest_zip_version_entry","title":"<code>find_highest_zip_version_entry(name, target_app_version=None, filepath='tbState.yaml')</code>","text":"<p>Findet den Eintrag mit der h\u00f6chsten ZIP-Version f\u00fcr einen gegebenen Namen und eine optionale Ziel-App-Version in einer YAML-Datei.</p> <p>:param name: Der Name des gesuchten Eintrags. :param target_app_version: Die Zielversion der App als String (optional). :param filepath: Der Pfad zur YAML-Datei. :return: Den Eintrag mit der h\u00f6chsten ZIP-Version innerhalb der Ziel-App-Version oder None, falls nicht gefunden.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def find_highest_zip_version_entry(name, target_app_version=None, filepath='tbState.yaml'):\n    \"\"\"\n    Findet den Eintrag mit der h\u00f6chsten ZIP-Version f\u00fcr einen gegebenen Namen und eine optionale Ziel-App-Version in einer YAML-Datei.\n\n    :param name: Der Name des gesuchten Eintrags.\n    :param target_app_version: Die Zielversion der App als String (optional).\n    :param filepath: Der Pfad zur YAML-Datei.\n    :return: Den Eintrag mit der h\u00f6chsten ZIP-Version innerhalb der Ziel-App-Version oder None, falls nicht gefunden.\n    \"\"\"\n    import yaml\n    highest_zip_ver = None\n    highest_entry = {}\n\n    with open(filepath) as file:\n        data = yaml.safe_load(file)\n        # print(data)\n        app_ver_h = None\n        for key, value in list(data.get('installable', {}).items())[::-1]:\n            # Pr\u00fcfe, ob der Name im Schl\u00fcssel enthalten ist\n\n            if name in key:\n                v = value['version']\n                if len(v) == 1:\n                    app_ver = v[0].split('v')[-1]\n                    zip_ver = \"0.0.0\"\n                else:\n                    app_ver, zip_ver = v\n                    app_ver = app_ver.split('v')[-1]\n                app_ver = version.parse(app_ver)\n                # Wenn eine Ziel-App-Version angegeben ist, vergleiche sie\n                if target_app_version is None or app_ver == version.parse(target_app_version):\n                    current_zip_ver = version.parse(zip_ver)\n                    # print(current_zip_ver, highest_zip_ver)\n\n                    if highest_zip_ver is None or current_zip_ver &gt; highest_zip_ver:\n                        highest_zip_ver = current_zip_ver\n                        highest_entry = value\n\n                    if app_ver_h is None or app_ver &gt; app_ver_h:\n                        app_ver_h = app_ver\n                        highest_zip_ver = current_zip_ver\n                        highest_entry = value\n    return highest_entry\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.get_executable_path","title":"<code>get_executable_path()</code>","text":"<p>Find the release executable in standard locations.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def get_executable_path():\n    \"\"\"Find the release executable in standard locations.\"\"\"\n    # This function is simplified from your example to match this script's scope\n    exe_name = get_executable_name_with_extension()\n    from toolboxv2 import tb_root_dir\n    search_paths = [\n        tb_root_dir / Path(\"bin\") / exe_name,\n        tb_root_dir / Path(\"src-core\") / exe_name,\n        tb_root_dir / exe_name,\n        tb_root_dir / Path(\"src-core\") / \"target\" / \"release\" / exe_name,\n    ]\n    for path in search_paths:\n        print(path)\n        if path.exists() and path.is_file():\n            return path.resolve()\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.query_executable_url","title":"<code>query_executable_url(current_os, machine)</code>","text":"<p>Query a remote URL for a matching executable based on OS and architecture. The file name is built dynamically based on parameters.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def query_executable_url(current_os, machine):\n    \"\"\"\n    Query a remote URL for a matching executable based on OS and architecture.\n    The file name is built dynamically based on parameters.\n    \"\"\"\n    base_url = \"https://example.com/downloads\"  # Replace with the actual URL\n    # Windows executables have .exe extension\n    if current_os == \"windows\":\n        file_name = f\"server_{current_os}_{machine}.exe\"\n    else:\n        file_name = f\"server_{current_os}_{machine}\"\n    full_url = f\"{base_url}/{file_name}\"\n    return full_url, file_name\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.remove_release_executable","title":"<code>remove_release_executable()</code>","text":"<p>Removes the release executable.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def remove_release_executable():\n    \"\"\"Removes the release executable.\"\"\"\n    src_core_path = os.path.join(\".\", \"src-core\")\n    expected_name = \"simple-core-server.exe\" if platform.system().lower() == \"windows\" else \"simple-core-server\"\n\n    # Remove from src-core root\n    direct_path = os.path.join(src_core_path, expected_name)\n    if os.path.exists(direct_path):\n        try:\n            os.remove(direct_path)\n            print(f\"Removed release executable: {direct_path}\")\n        except Exception as e:\n            print(f\"Failed to remove {direct_path}: {e}\")\n\n    # Remove from target/release\n    release_path = os.path.join(src_core_path, \"target\", \"release\", expected_name)\n    if os.path.exists(release_path):\n        try:\n            os.remove(release_path)\n            print(f\"Removed release executable: {release_path}\")\n        except Exception as e:\n            print(f\"Failed to remove {release_path}: {e}\")\n\n    return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.run_executable","title":"<code>run_executable(file_path)</code>","text":"<p>Run the executable file.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def run_executable(file_path):\n    \"\"\"Run the executable file.\"\"\"\n    try:\n        print(\"Running it.\")\n        subprocess.run([os.path.abspath(file_path)], check=True)\n    except subprocess.CalledProcessError as e:\n        print(f\"Failed to execute {file_path}: {e}\")\n    except KeyboardInterrupt:\n        print(\"Exiting call from:\", file_path)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.run_in_debug_mode","title":"<code>run_in_debug_mode()</code>","text":"<p>Run the Cargo project in debug mode.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def run_in_debug_mode():\n    \"\"\"Run the Cargo project in debug mode.\"\"\"\n    src_core_path = os.path.join(\".\", \"src-core\")\n    print(\"Running in debug mode...\")\n    try:\n        subprocess.run([\"cargo\", \"run\"], cwd=src_core_path)\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Debug execution failed: {e}\")\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.run_with_hot_reload","title":"<code>run_with_hot_reload()</code>","text":"<p>Run the Cargo project with hot reloading.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def run_with_hot_reload():\n    \"\"\"Run the Cargo project with hot reloading.\"\"\"\n    src_core_path = os.path.join(\".\", \"src-core\")\n\n    # Check if cargo-watch is installed\n    try:\n        subprocess.run([\"cargo\", \"watch\", \"--version\"], check=True, capture_output=True)\n    except Exception:\n        print(\"cargo-watch is not installed. Installing now...\")\n        try:\n            subprocess.run([\"cargo\", \"install\", \"cargo-watch\"], check=True)\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to install cargo-watch: {e}\")\n            print(\"Running without hot reload\")\n            return run_in_debug_mode()\n\n    print(\"Running with hot reload in debug mode...\")\n    try:\n        subprocess.run([\"cargo\", \"watch\", \"-x\", \"run\"], cwd=src_core_path)\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Hot reload execution failed: {e}\")\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.update_server","title":"<code>update_server(new_executable_path, new_version, use_posix_zdt)</code>","text":"<p>High-level update function, calls platform-specific logic.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def update_server(new_executable_path: str, new_version: str, use_posix_zdt: bool):\n    \"\"\"High-level update function, calls platform-specific logic.\"\"\"\n    # Only use POSIX ZDT if flag is set AND on a non-windows system\n    is_posix = platform.system().lower() != \"windows\"\n    if is_posix and use_posix_zdt:\n        return update_server_posix(new_executable_path, new_version)\n    else:\n        if use_posix_zdt and not is_posix:\n            print(Style.YELLOW(\"Warning: --posix-zdt flag ignored on Windows. Using graceful restart.\"))\n        return update_server_graceful_restart(new_executable_path, new_version)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.conda_runner","title":"<code>conda_runner</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.conda_runner.create_env_registry","title":"<code>create_env_registry(env_name)</code>","text":"<p>Create a JSON registry of all packages installed in the specified conda environment.</p> <p>Args: env_name (str): Name of the conda environment</p> <p>Returns: bool: True if registry creation was successful, False otherwise</p> Source code in <code>toolboxv2/utils/system/conda_runner.py</code> <pre><code>def create_env_registry(env_name: str) -&gt; bool:\n    \"\"\"\n    Create a JSON registry of all packages installed in the specified conda environment.\n\n    Args:\n    env_name (str): Name of the conda environment\n\n    Returns:\n    bool: True if registry creation was successful, False otherwise\n    \"\"\"\n    # Get list of installed packages\n    command = f\"conda list -n {env_name} --json\"\n    success, output = run_command(command, live=False)\n\n    if not success or output is None:\n        print(f\"Failed to get package list for environment {env_name}\")\n        return False\n\n    try:\n        # Parse the JSON output\n        packages = json.loads(output)\n\n        # Create a simplified registry with package names and versions\n        registry = [{\"name\": pkg[\"name\"], \"version\": pkg[\"version\"]} for pkg in packages]\n\n        # Write the registry to a JSON file\n        registry_file = f\"{env_name}_registry.json\"\n        with open(registry_file, 'w') as f:\n            json.dump(registry, f, indent=2)\n\n        print(f\"Registry created successfully: {registry_file}\")\n        return True\n\n    except json.JSONDecodeError:\n        print(f\"Failed to parse package list for environment {env_name}\")\n        return False\n    except OSError:\n        print(f\"Failed to write registry file for environment {env_name}\")\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.db_cli_manager","title":"<code>db_cli_manager</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.db_cli_manager.ClusterManager","title":"<code>ClusterManager</code>","text":"<p>Manages a cluster of r_blob_db instances defined in a config file.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>class ClusterManager:\n    \"\"\"Manages a cluster of r_blob_db instances defined in a config file.\"\"\"\n\n    def __init__(self, config_path: str = CLUSTER_CONFIG_FILE):\n        self.config_path = Path(config_path)\n        self.instances: Dict[str, DBInstanceManager] = self._load_config()\n\n    def _load_config(self) -&gt; Dict[str, DBInstanceManager]:\n        \"\"\"Loads and validates the cluster configuration.\"\"\"\n        from toolboxv2 import tb_root_dir\n        if not self.config_path.is_absolute():\n            self.config_path = tb_root_dir / self.config_path\n\n        if not self.config_path.exists():\n            print(Style.YELLOW(f\"Warning: Cluster config '{self.config_path}' not found. Creating a default example.\"))\n            default_config_dir = (tb_root_dir / \".data/db_data/\").resolve()\n            default_config = {\n                \"instance-01\": {\"port\": 3001, \"data_dir\": str(default_config_dir / \"01\")},\n                \"instance-02\": {\"port\": 3002, \"data_dir\": str(default_config_dir / \"02\")},\n            }\n            with open(self.config_path, 'w') as f:\n                json.dump(default_config, f, indent=4)\n            config_data = default_config\n        else:\n            with open(self.config_path, 'r') as f:\n                config_data = json.load(f)\n\n        return {id: DBInstanceManager(id, cfg) for id, cfg in config_data.items()}\n\n    def get_instances(self, instance_id: Optional[str] = None) -&gt; List[DBInstanceManager]:\n        \"\"\"Returns a list of instances to operate on.\"\"\"\n        if instance_id:\n            if instance_id not in self.instances:\n                raise ValueError(f\"Instance ID '{instance_id}' not found in '{self.config_path}'.\")\n            return [self.instances[instance_id]]\n        return list(self.instances.values())\n\n    def start_all(self, executable_path: Path, version: str, instance_id: Optional[str] = None):\n        for instance in self.get_instances(instance_id):\n            instance.start(executable_path, version)\n\n    def stop_all(self, instance_id: Optional[str] = None):\n        for instance in self.get_instances(instance_id):\n            instance.stop()\n\n    def status_all(self, instance_id: Optional[str] = None, silent=False):\n        if not silent:\n            header = f\"--- {Style.Bold('Cluster Status')} ---\"\n            print(header)\n            print(\n                f\"{Style.Underline('INSTANCE ID'):&lt;18} {Style.Underline('STATUS'):&lt;20} {Style.Underline('PID'):&lt;8} {Style.Underline('VERSION'):&lt;12} {Style.Underline('PORT')}\")\n\n        services_online = 0\n        server_list = []\n        for instance in self.get_instances(instance_id):\n            pid, version = instance.read_state()\n            is_running = instance.is_running()\n            if is_running:\n                server_list.append(f\"http://{instance.host}:{instance.port}\")\n                services_online += 1\n            if not silent:\n                status_str = \"\u2705 RUNNING\" if is_running else \"\u274c STOPPED\"\n                status_color = Style.GREEN2 if is_running else Style.RED2\n                print(\n                    f\"  {Style.WHITE(instance.id):&lt;16} {status_color(status_str):&lt;20} {Style.GREY(str(pid or 'N/A')):&lt;8} {Style.BLUE2(version or 'N/A'):&lt;12} {Style.YELLOW(str(instance.port))}\"\n                )\n        if not silent:\n            print(\"-\" * len(header))\n        return services_online, server_list\n\n    def health_check_all(self, instance_id: Optional[str] = None):\n        header = f\"--- {Style.Bold('Cluster Health Check')} ---\"\n        print(header)\n        print(\n            f\"{Style.Underline('INSTANCE ID'):&lt;18} {Style.Underline('STATUS'):&lt;22} {Style.Underline('PID'):&lt;8} {Style.Underline('LATENCY'):&lt;12} {Style.Underline('DETAILS')}\")\n\n        for instance in self.get_instances(instance_id):\n            health = instance.get_health()\n            status = health.get('status', 'UNKNOWN')\n            pid = health.get('pid', 'N/A')\n            details = \"\"\n\n            if status == 'OK':\n                status_str, color = \"\u2705 OK\", Style.GREEN2\n                latency = f\"{health['latency_ms']}ms\"\n                details = f\"Blobs: {Style.YELLOW(str(health['blobs_managed']))} | Version: {Style.BLUE2(health['server_version'])}\"\n            elif status == 'STOPPED':\n                status_str, color = \"\u274c STOPPED\", Style.RED2\n                latency = \"N/A\"\n            else:\n                status_str, color = f\"\ud83d\udd25 {status}\", Style.RED\n                latency = \"N/A\"\n                details = Style.GREY(str(health.get('error', 'N/A')))\n\n            print(\n                f\"  {Style.WHITE(instance.id):&lt;16} {color(status_str):&lt;22} {Style.GREY(str(pid)):&lt;8} {Style.GREEN(latency):&lt;12} {details}\")\n        print(\"-\" * len(header))\n\n    def update_all_rolling(self, new_executable_path: Path, new_version: str, instance_id: Optional[str] = None):\n        \"\"\"Performs a zero-downtime rolling update of the cluster.\"\"\"\n        print(f\"--- {Style.Bold(f'Starting Rolling Update to Version {Style.YELLOW(new_version)}')} ---\")\n        instances_to_update = self.get_instances(instance_id)\n        for i, instance in enumerate(instances_to_update):\n            print(\n                f\"\\n{Style.CYAN(f'[{i + 1}/{len(instances_to_update)}] Updating instance')} '{Style.WHITE(instance.id)}'...\")\n\n            if not instance.stop():\n                print(Style.RED2(f\"CRITICAL: Failed to stop old instance '{instance.id}'. Aborting update.\"))\n                return\n\n            if not instance.start(new_executable_path, new_version):\n                print(Style.RED2(f\"CRITICAL: Failed to start new version for '{instance.id}'. Update halted.\"))\n                print(Style.YELLOW(\"The cluster might be in a partially updated state. Please investigate.\"))\n                return\n\n            with Spinner(f\"Waiting for '{instance.id}' to become healthy\", symbols=\"t\") as s:\n                for attempt in range(5):\n                    s.message = f\"Waiting for '{instance.id}' to become healthy (attempt {attempt + 1}/5)\"\n                    time.sleep(2)\n                    health = instance.get_health()\n                    if health.get('status') == 'OK':\n                        print(\n                            f\"\\n{Style.GREEN('\u2705 Instance')} '{instance.id}' {Style.GREEN('is healthy with new version.')}\")\n                        break\n                else:\n                    print(\n                        f\"\\n{Style.RED2('CRITICAL:')} Instance '{instance.id}' did not become healthy after update. Update halted.\")\n                    return\n\n        print(f\"\\n--- {Style.GREEN2('Rolling Update Complete')} ---\")\n</code></pre> <code>get_instances(instance_id=None)</code> \u00b6 <p>Returns a list of instances to operate on.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def get_instances(self, instance_id: Optional[str] = None) -&gt; List[DBInstanceManager]:\n    \"\"\"Returns a list of instances to operate on.\"\"\"\n    if instance_id:\n        if instance_id not in self.instances:\n            raise ValueError(f\"Instance ID '{instance_id}' not found in '{self.config_path}'.\")\n        return [self.instances[instance_id]]\n    return list(self.instances.values())\n</code></pre> <code>update_all_rolling(new_executable_path, new_version, instance_id=None)</code> \u00b6 <p>Performs a zero-downtime rolling update of the cluster.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def update_all_rolling(self, new_executable_path: Path, new_version: str, instance_id: Optional[str] = None):\n    \"\"\"Performs a zero-downtime rolling update of the cluster.\"\"\"\n    print(f\"--- {Style.Bold(f'Starting Rolling Update to Version {Style.YELLOW(new_version)}')} ---\")\n    instances_to_update = self.get_instances(instance_id)\n    for i, instance in enumerate(instances_to_update):\n        print(\n            f\"\\n{Style.CYAN(f'[{i + 1}/{len(instances_to_update)}] Updating instance')} '{Style.WHITE(instance.id)}'...\")\n\n        if not instance.stop():\n            print(Style.RED2(f\"CRITICAL: Failed to stop old instance '{instance.id}'. Aborting update.\"))\n            return\n\n        if not instance.start(new_executable_path, new_version):\n            print(Style.RED2(f\"CRITICAL: Failed to start new version for '{instance.id}'. Update halted.\"))\n            print(Style.YELLOW(\"The cluster might be in a partially updated state. Please investigate.\"))\n            return\n\n        with Spinner(f\"Waiting for '{instance.id}' to become healthy\", symbols=\"t\") as s:\n            for attempt in range(5):\n                s.message = f\"Waiting for '{instance.id}' to become healthy (attempt {attempt + 1}/5)\"\n                time.sleep(2)\n                health = instance.get_health()\n                if health.get('status') == 'OK':\n                    print(\n                        f\"\\n{Style.GREEN('\u2705 Instance')} '{instance.id}' {Style.GREEN('is healthy with new version.')}\")\n                    break\n            else:\n                print(\n                    f\"\\n{Style.RED2('CRITICAL:')} Instance '{instance.id}' did not become healthy after update. Update halted.\")\n                return\n\n    print(f\"\\n--- {Style.GREEN2('Rolling Update Complete')} ---\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.db_cli_manager.DBInstanceManager","title":"<code>DBInstanceManager</code>","text":"<p>Manages a single r_blob_db instance.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>class DBInstanceManager:\n    \"\"\"Manages a single r_blob_db instance.\"\"\"\n\n    def __init__(self, instance_id: str, config: dict):\n        self.id = instance_id\n        self.port = config['port']\n        self.host = config.get('host', '127.0.0.1')\n        self.data_dir = Path(config['data_dir'])\n        self.state_file = self.data_dir / \"instance_state.json\"\n        self.log_file = self.data_dir / \"instance.log\"  # Added for better logging info\n\n    def read_state(self) -&gt; Tuple[Optional[int], Optional[str]]:\n        \"\"\"Reads the PID and version from the instance's state file.\"\"\"\n        if not self.state_file.exists():\n            return None, None\n        try:\n            with open(self.state_file, 'r') as f:\n                state = json.load(f)\n            return state.get('pid'), state.get('version')\n        except (json.JSONDecodeError, ValueError, FileNotFoundError):\n            return None, None\n\n    def write_state(self, pid: Optional[int], version: Optional[str]):\n        \"\"\"Writes the PID and version to the state file.\"\"\"\n        self.data_dir.mkdir(parents=True, exist_ok=True)\n        state = {'pid': pid, 'version': version}\n        with open(self.state_file, 'w') as f:\n            json.dump(state, f, indent=4)\n\n    def is_running(self) -&gt; bool:\n        \"\"\"Checks if the process associated with this instance is running.\"\"\"\n        pid, _ = self.read_state()\n        return psutil.pid_exists(pid) if pid else False\n\n    def start(self, executable_path: Path, version: str) -&gt; bool:\n        \"\"\"Starts the instance process and detaches, redirecting output to a log file.\"\"\"\n        if self.is_running():\n            print(Style.YELLOW(f\"Instance '{self.id}' is already running.\"))\n            return True\n\n        print(Style.CYAN(f\"\ud83d\ude80 Starting instance '{self.id}' on port {self.port}...\"))\n        self.data_dir.mkdir(parents=True, exist_ok=True)\n        log_handle = open(self.log_file, 'a')\n\n        env = os.environ.copy()\n        env[\"R_BLOB_DB_CLEAN\"] = os.getenv(\"R_BLOB_DB_CLEAN\", \"false\")\n        env[\"R_BLOB_DB_PORT\"] = str(self.port)\n        env[\"R_BLOB_DB_DATA_DIR\"] = str(self.data_dir.resolve())\n        env[\"RUST_LOG\"] = \"info,tower_http=debug\" # \"error\"\n\n        try:\n            if executable_path is None:\n                raise ValueError(f\"\\n{Style.RED2('\u274c ERROR:')} Executable not found. Build it first.\")\n            with Spinner(f\"Launching process for '{self.id}'\", symbols=\"d\"):\n                process = subprocess.Popen(\n                    [str(executable_path.resolve())],\n                    env=env,\n                    stdout=log_handle,\n                    stderr=log_handle,\n                    creationflags=subprocess.DETACHED_PROCESS if platform.system() == \"Windows\" else 0\n                )\n                time.sleep(1.5)\n\n            if process.poll() is not None:\n                print(f\"\\n{Style.RED2('\u274c ERROR:')} Instance '{self.id}' failed to start. Check logs:\")\n                print(f\"    {Style.GREY(self.log_file)}\")\n                return False\n\n            self.write_state(process.pid, version)\n            print(\n                f\"\\n{Style.GREEN2('\u2705 Instance')} '{Style.Bold(self.id)}' {Style.GREEN2('started successfully.')} {Style.GREY(f'(PID: {process.pid})')}\")\n            print(f\"   {Style.BLUE('Logging to:')} {Style.GREY(self.log_file)}\")\n            return True\n        except Exception as e:\n            print(f\"\\n{Style.RED2('\u274c ERROR:')} Failed to launch instance '{self.id}': {e}\")\n            log_handle.close()\n            return False\n\n    def stop(self, timeout: int = 10) -&gt; bool:\n        \"\"\"Stops the instance process gracefully.\"\"\"\n        if not self.is_running():\n            print(Style.YELLOW(f\"Instance '{self.id}' is not running.\"))\n            self.write_state(None, None)\n            return True\n\n        pid, _ = self.read_state()\n        with Spinner(f\"Stopping '{self.id}' (PID: {pid})\", symbols=\"+\", time_in_s=timeout, count_down=True) as s:\n            try:\n                proc = psutil.Process(pid)\n                proc.terminate()\n                proc.wait(timeout)\n            except psutil.TimeoutExpired:\n                s.message = f\"Force killing '{self.id}'\"\n                proc.kill()\n            except psutil.NoSuchProcess:\n                pass\n            except Exception as e:\n                print(f\"\\n{Style.RED2('\u274c ERROR:')} Failed to stop instance '{self.id}': {e}\")\n                return False\n\n        self.write_state(None, None)\n        print(f\"\\n{Style.VIOLET2('\u23f9\ufe0f  Instance')} '{Style.Bold(self.id)}' {Style.VIOLET2('stopped.')}\")\n        return True\n\n    def get_health(self) -&gt; dict:\n        \"\"\"Performs a health check on the running instance.\"\"\"\n        if not self.is_running():\n            return {'id': self.id, 'status': 'STOPPED', 'error': 'Process not running'}\n\n        pid, version = self.read_state()\n        health_url = f\"http://{self.host}:{self.port}/health\"\n        start_time = time.monotonic()\n        try:\n            response = requests.get(health_url, timeout=2)\n            latency_ms = (time.monotonic() - start_time) * 1000\n            response.raise_for_status()\n            health_data = response.json()\n            health_data.update({\n                'id': self.id, 'pid': pid, 'latency_ms': round(latency_ms),\n                'server_version': health_data.pop('version', 'unknown'),\n                'manager_known_version': version\n            })\n            return health_data\n        except requests.exceptions.RequestException as e:\n            return {'id': self.id, 'status': 'UNREACHABLE', 'pid': pid, 'error': str(e)}\n        except Exception as e:\n            return {'id': self.id, 'status': 'ERROR', 'pid': pid, 'error': f'Failed to parse health response: {e}'}\n</code></pre> <code>get_health()</code> \u00b6 <p>Performs a health check on the running instance.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def get_health(self) -&gt; dict:\n    \"\"\"Performs a health check on the running instance.\"\"\"\n    if not self.is_running():\n        return {'id': self.id, 'status': 'STOPPED', 'error': 'Process not running'}\n\n    pid, version = self.read_state()\n    health_url = f\"http://{self.host}:{self.port}/health\"\n    start_time = time.monotonic()\n    try:\n        response = requests.get(health_url, timeout=2)\n        latency_ms = (time.monotonic() - start_time) * 1000\n        response.raise_for_status()\n        health_data = response.json()\n        health_data.update({\n            'id': self.id, 'pid': pid, 'latency_ms': round(latency_ms),\n            'server_version': health_data.pop('version', 'unknown'),\n            'manager_known_version': version\n        })\n        return health_data\n    except requests.exceptions.RequestException as e:\n        return {'id': self.id, 'status': 'UNREACHABLE', 'pid': pid, 'error': str(e)}\n    except Exception as e:\n        return {'id': self.id, 'status': 'ERROR', 'pid': pid, 'error': f'Failed to parse health response: {e}'}\n</code></pre> <code>is_running()</code> \u00b6 <p>Checks if the process associated with this instance is running.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def is_running(self) -&gt; bool:\n    \"\"\"Checks if the process associated with this instance is running.\"\"\"\n    pid, _ = self.read_state()\n    return psutil.pid_exists(pid) if pid else False\n</code></pre> <code>read_state()</code> \u00b6 <p>Reads the PID and version from the instance's state file.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def read_state(self) -&gt; Tuple[Optional[int], Optional[str]]:\n    \"\"\"Reads the PID and version from the instance's state file.\"\"\"\n    if not self.state_file.exists():\n        return None, None\n    try:\n        with open(self.state_file, 'r') as f:\n            state = json.load(f)\n        return state.get('pid'), state.get('version')\n    except (json.JSONDecodeError, ValueError, FileNotFoundError):\n        return None, None\n</code></pre> <code>start(executable_path, version)</code> \u00b6 <p>Starts the instance process and detaches, redirecting output to a log file.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def start(self, executable_path: Path, version: str) -&gt; bool:\n    \"\"\"Starts the instance process and detaches, redirecting output to a log file.\"\"\"\n    if self.is_running():\n        print(Style.YELLOW(f\"Instance '{self.id}' is already running.\"))\n        return True\n\n    print(Style.CYAN(f\"\ud83d\ude80 Starting instance '{self.id}' on port {self.port}...\"))\n    self.data_dir.mkdir(parents=True, exist_ok=True)\n    log_handle = open(self.log_file, 'a')\n\n    env = os.environ.copy()\n    env[\"R_BLOB_DB_CLEAN\"] = os.getenv(\"R_BLOB_DB_CLEAN\", \"false\")\n    env[\"R_BLOB_DB_PORT\"] = str(self.port)\n    env[\"R_BLOB_DB_DATA_DIR\"] = str(self.data_dir.resolve())\n    env[\"RUST_LOG\"] = \"info,tower_http=debug\" # \"error\"\n\n    try:\n        if executable_path is None:\n            raise ValueError(f\"\\n{Style.RED2('\u274c ERROR:')} Executable not found. Build it first.\")\n        with Spinner(f\"Launching process for '{self.id}'\", symbols=\"d\"):\n            process = subprocess.Popen(\n                [str(executable_path.resolve())],\n                env=env,\n                stdout=log_handle,\n                stderr=log_handle,\n                creationflags=subprocess.DETACHED_PROCESS if platform.system() == \"Windows\" else 0\n            )\n            time.sleep(1.5)\n\n        if process.poll() is not None:\n            print(f\"\\n{Style.RED2('\u274c ERROR:')} Instance '{self.id}' failed to start. Check logs:\")\n            print(f\"    {Style.GREY(self.log_file)}\")\n            return False\n\n        self.write_state(process.pid, version)\n        print(\n            f\"\\n{Style.GREEN2('\u2705 Instance')} '{Style.Bold(self.id)}' {Style.GREEN2('started successfully.')} {Style.GREY(f'(PID: {process.pid})')}\")\n        print(f\"   {Style.BLUE('Logging to:')} {Style.GREY(self.log_file)}\")\n        return True\n    except Exception as e:\n        print(f\"\\n{Style.RED2('\u274c ERROR:')} Failed to launch instance '{self.id}': {e}\")\n        log_handle.close()\n        return False\n</code></pre> <code>stop(timeout=10)</code> \u00b6 <p>Stops the instance process gracefully.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def stop(self, timeout: int = 10) -&gt; bool:\n    \"\"\"Stops the instance process gracefully.\"\"\"\n    if not self.is_running():\n        print(Style.YELLOW(f\"Instance '{self.id}' is not running.\"))\n        self.write_state(None, None)\n        return True\n\n    pid, _ = self.read_state()\n    with Spinner(f\"Stopping '{self.id}' (PID: {pid})\", symbols=\"+\", time_in_s=timeout, count_down=True) as s:\n        try:\n            proc = psutil.Process(pid)\n            proc.terminate()\n            proc.wait(timeout)\n        except psutil.TimeoutExpired:\n            s.message = f\"Force killing '{self.id}'\"\n            proc.kill()\n        except psutil.NoSuchProcess:\n            pass\n        except Exception as e:\n            print(f\"\\n{Style.RED2('\u274c ERROR:')} Failed to stop instance '{self.id}': {e}\")\n            return False\n\n    self.write_state(None, None)\n    print(f\"\\n{Style.VIOLET2('\u23f9\ufe0f  Instance')} '{Style.Bold(self.id)}' {Style.VIOLET2('stopped.')}\")\n    return True\n</code></pre> <code>write_state(pid, version)</code> \u00b6 <p>Writes the PID and version to the state file.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def write_state(self, pid: Optional[int], version: Optional[str]):\n    \"\"\"Writes the PID and version to the state file.\"\"\"\n    self.data_dir.mkdir(parents=True, exist_ok=True)\n    state = {'pid': pid, 'version': version}\n    with open(self.state_file, 'w') as f:\n        json.dump(state, f, indent=4)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.db_cli_manager.cli_db_runner","title":"<code>cli_db_runner()</code>","text":"<p>The main entry point for the CLI application.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def cli_db_runner():\n    \"\"\"The main entry point for the CLI application.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=f\"\ud83d\ude80 {Style.Bold('A manager for r_blob_db instances and clusters.')}\",\n        formatter_class=argparse.RawTextHelpFormatter\n    )\n    subparsers = parser.add_subparsers(dest=\"action\", required=True, help=\"Available actions\")\n\n    # Define common arguments\n    instance_arg = {'name_or_flags': ['--instance-id'], 'type': str,\n                    'help': 'Target a specific instance ID. If omitted, action applies to the whole cluster.',\n                    'default': None}\n    version_arg = {'name_or_flags': ['--version'], 'type': str,\n                   'help': 'Specify a version string for the executable (e.g., \"1.2.0\").', 'default': 'dev'}\n\n    # --- Define Commands ---\n    p_start = subparsers.add_parser('start', help='Start instance(s).')\n    p_start.add_argument(*instance_arg['name_or_flags'],\n                         **{k: v for k, v in instance_arg.items() if k != 'name_or_flags'})\n    p_start.add_argument(*version_arg['name_or_flags'],\n                         **{k: v for k, v in version_arg.items() if k != 'name_or_flags'})\n\n    p_stop = subparsers.add_parser('stop', help='Stop instance(s).')\n    p_stop.add_argument(*instance_arg['name_or_flags'],\n                        **{k: v for k, v in instance_arg.items() if k != 'name_or_flags'})\n\n    p_status = subparsers.add_parser('status', help='Show the running status of instance(s).')\n    p_status.add_argument(*instance_arg['name_or_flags'],\n                          **{k: v for k, v in instance_arg.items() if k != 'name_or_flags'})\n\n    p_health = subparsers.add_parser('health', help='Perform a health check on instance(s).')\n    p_health.add_argument(*instance_arg['name_or_flags'],\n                          **{k: v for k, v in instance_arg.items() if k != 'name_or_flags'})\n\n    p_update = subparsers.add_parser('update', help='Perform a rolling update on the cluster.')\n    p_update.add_argument(*instance_arg['name_or_flags'],\n                          **{k: v for k, v in instance_arg.items() if k != 'name_or_flags'})\n    version_arg_update = {**version_arg, 'required': True}\n    p_update.add_argument(*version_arg_update['name_or_flags'],\n                          **{k: v for k, v in version_arg_update.items() if k != 'name_or_flags'})\n\n    subparsers.add_parser('build', help='Build the Rust executable from source.')\n    subparsers.add_parser('clean', help='Clean the Rust build artifacts.')\n\n    # --- Execute Command ---\n    args = parser.parse_args()\n\n    if args.action == 'build':\n        handle_build()\n        return\n    if args.action == 'clean':\n        handle_clean()\n        return\n\n    manager = ClusterManager()\n\n    if args.action in ['start', 'update']:\n        executable_path = get_executable_path(update=(args.action == 'update'))\n        if not executable_path:\n            print(Style.RED(f\"ERROR: Could not find the {EXECUTABLE_NAME} executable.\"))\n            print(Style.YELLOW(\"Please build it first with: python -m toolboxv2.r_blob_db.db_cli build\"))\n            return\n\n    if args.action == 'start':\n        manager.start_all(executable_path, args.version, args.instance_id)\n    elif args.action == 'stop':\n        manager.stop_all(args.instance_id)\n    elif args.action == 'status':\n        manager.status_all(args.instance_id)\n    elif args.action == 'health':\n        manager.health_check_all(args.instance_id)\n    elif args.action == 'update':\n        manager.update_all_rolling(executable_path, args.version, args.instance_id)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.db_cli_manager.get_executable_path","title":"<code>get_executable_path(base_name=EXECUTABLE_NAME, update=False)</code>","text":"<p>Finds the release executable in standard locations.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def get_executable_path(base_name: str = EXECUTABLE_NAME, update=False) -&gt; Optional[Path]:\n    \"\"\"Finds the release executable in standard locations.\"\"\"\n    name_with_ext = f\"{base_name}.exe\" if platform.system() == \"Windows\" else base_name\n    from toolboxv2 import tb_root_dir\n    search_paths = [\n        tb_root_dir / \"bin\" / name_with_ext,\n        tb_root_dir / \"r_blob_db\" / \"target\" / \"release\" / name_with_ext,\n    ]\n    if update:\n        search_paths = search_paths[::-1]\n    for path in search_paths:\n        if path.is_file():\n            return path.resolve()\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.main_tool","title":"<code>main_tool</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.main_tool.MainTool","title":"<code>MainTool</code>","text":"Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>class MainTool:\n    toolID: str = \"\"\n    # app = None\n    interface = None\n    spec = \"app\"\n    name = \"\"\n    color = \"Bold\"\n    stuf = False\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.__storedargs = args, kwargs\n        self.tools = kwargs.get(\"tool\", {})\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n        if \"on_exit\" in kwargs and isinstance(kwargs.get(\"on_exit\"), Callable):\n            self.on_exit =self.app.tb(\n                mod_name=self.name,\n                name=kwargs.get(\"on_exit\").__name__,\n                version=self.version if hasattr(self, 'version') else \"0.0.0\",\n            )(kwargs.get(\"on_exit\"))\n        self.async_initialized = False\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    pass\n                else:\n                    self.todo()\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\"Error loading mod {self.name} {e}\")\n                if self.app.debug:\n                    import traceback\n                    traceback.print_exc()\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n\n    async def __ainit__(self, *args, **kwargs):\n        self.version = kwargs[\"v\"]\n        self.tools = kwargs.get(\"tool\", {})\n        self.name = kwargs[\"name\"]\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", None))\n        if not hasattr(self, 'config'):\n            self.config = {}\n        self.user = None\n        self.description = \"A toolbox mod\" if kwargs.get(\"description\") is None else kwargs.get(\"description\")\n        if MainTool.interface is None:\n            MainTool.interface = self.app.interface_type\n        # Result.default(self.app.interface)\n\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    await self.todo()\n                else:\n                    pass\n                await asyncio.sleep(0.1)\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\"Error loading mod {self.name} {e}\")\n                if self.app.debug:\n                    import traceback\n                    traceback.print_exc()\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n        self.app.print(f\"TOOL : {self.spec}.{self.name} online\")\n\n\n\n    @property\n    def app(self):\n        return get_app(\n            from_=f\"{self.spec}.{self.name}|{self.toolID if self.toolID else '*' + MainTool.toolID} {self.interface if self.interface else MainTool.interface}\")\n\n    @app.setter\n    def app(self, v):\n        raise PermissionError(f\"You cannot set the App Instance! {v=}\")\n\n    @staticmethod\n    def return_result(error: ToolBoxError = ToolBoxError.none,\n                      exec_code: int = 0,\n                      help_text: str = \"\",\n                      data_info=None,\n                      data=None,\n                      data_to=None):\n\n        if data_to is None:\n            data_to = MainTool.interface if MainTool.interface is not None else ToolBoxInterfaces.cli\n\n        if data is None:\n            data = {}\n\n        if data_info is None:\n            data_info = {}\n\n        return Result(\n            error,\n            ToolBoxResult(data_info=data_info, data=data, data_to=data_to),\n            ToolBoxInfo(exec_code=exec_code, help_text=help_text)\n        )\n\n    def print(self, message, end=\"\\n\", **kwargs):\n        if self.stuf:\n            return\n\n        self.app.print(Style.style_dic[self.color] + self.name + Style.style_dic[\"END\"] + \":\", message, end=end,\n                       **kwargs)\n\n    def add_str_to_config(self, command):\n        if len(command) != 2:\n            self.logger.error('Invalid command must be key value')\n            return False\n        self.config[command[0]] = command[1]\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n\n    def get_version(self) -&gt; str:\n        \"\"\"\"Returns the version\"\"\"\n        return self.version\n\n    async def get_user(self, username: str) -&gt; Result:\n        return await self.app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME, username=username, get_results=True)\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n</code></pre> <code>__init__(*args, **kwargs)</code> \u00b6 <p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.__storedargs = args, kwargs\n    self.tools = kwargs.get(\"tool\", {})\n    self.logger = kwargs.get(\"logs\", get_logger())\n    self.color = kwargs.get(\"color\", \"WHITE\")\n    self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n    if \"on_exit\" in kwargs and isinstance(kwargs.get(\"on_exit\"), Callable):\n        self.on_exit =self.app.tb(\n            mod_name=self.name,\n            name=kwargs.get(\"on_exit\").__name__,\n            version=self.version if hasattr(self, 'version') else \"0.0.0\",\n        )(kwargs.get(\"on_exit\"))\n    self.async_initialized = False\n    if self.todo:\n        try:\n            if inspect.iscoroutinefunction(self.todo):\n                pass\n            else:\n                self.todo()\n            get_logger().info(f\"{self.name} on load suspended\")\n        except Exception as e:\n            get_logger().error(f\"Error loading mod {self.name} {e}\")\n            if self.app.debug:\n                import traceback\n                traceback.print_exc()\n    else:\n        get_logger().info(f\"{self.name} no load require\")\n</code></pre> <code>__initobj()</code> <code>async</code> \u00b6 <p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre> <code>get_version()</code> \u00b6 <p>\"Returns the version</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\"Returns the version\"\"\"\n    return self.version\n</code></pre> <code>webInstall(user_instance, construct_render)</code> \u00b6 <p>\"Returns a web installer for the given user instance and construct render template</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def webInstall(self, user_instance, construct_render) -&gt; str:\n    \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.main_tool.get_version_from_pyproject","title":"<code>get_version_from_pyproject(pyproject_path='../pyproject.toml')</code>","text":"<p>Reads the version from the pyproject.toml file.</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def get_version_from_pyproject(pyproject_path='../pyproject.toml'):\n    \"\"\"Reads the version from the pyproject.toml file.\"\"\"\n    if not os.path.exists(pyproject_path) and pyproject_path=='../pyproject.toml':\n        pyproject_path = 'pyproject.toml'\n    if not os.path.exists(pyproject_path) and pyproject_path=='pyproject.toml':\n        return \"0.1.21\"\n\n    try:\n        import toml\n        # Load the pyproject.toml file\n        with open(pyproject_path) as file:\n            pyproject_data = toml.load(file)\n\n        # Extract the version from the 'project' section\n        version = pyproject_data.get('project', {}).get('version')\n\n        if version is None:\n            raise ValueError(f\"Version not found in {pyproject_path}\")\n\n        return version\n    except Exception as e:\n        print(f\"Error reading version: {e}\")\n        return \"0.0.0\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.state_system","title":"<code>state_system</code>","text":"<p>The Task of the State System is : 1 Kep trak of the current state of the ToolBox and its dependency's 2 tracks the shasum of all mod and runnabael 3 the version of all mod</p> <p>The state : {\"utils\":{\"file_name\": {\"version\":##,\"shasum\"}} ,\"mods\":{\"file_name\": {\"version\":##,\"shasum\":##,\"src-url\":##}} ,\"runnable\":{\"file_name\": {\"version\":##,\"shasum\":##,\"src-url\":##}} ,\"api\":{\"file_name\": {\"version\":##,\"shasum\"}} ,\"app\":{\"file_name\": {\"version\":##,\"shasum\":##,\"src-url\":##}} }</p> <p>trans form state from on to an other.</p>"},{"location":"toolboxv2/#toolboxv2.utils.system.tcm_p2p_cli","title":"<code>tcm_p2p_cli</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.tcm_p2p_cli.InstanceManager","title":"<code>InstanceManager</code>","text":"<p>Manages a single named instance (relay or peer) of the P2P application.</p> Source code in <code>toolboxv2/utils/system/tcm_p2p_cli.py</code> <pre><code>class InstanceManager:\n    \"\"\"Manages a single named instance (relay or peer) of the P2P application.\"\"\"\n\n    def __init__(self, name: str):\n        self.name = name\n        self.instance_dir = INSTANCES_ROOT_DIR / self.name\n        self.state_file = self.instance_dir / \"state.json\"\n        self.config_file = self.instance_dir / \"config.toml\"\n        self.log_file = self.instance_dir / \"instance.log\"\n\n    def read_state(self) -&gt; Dict:\n        \"\"\"Reads the instance's state (pid, mode, etc.) from its state file.\"\"\"\n        if not self.state_file.exists():\n            return {}\n        try:\n            with open(self.state_file, 'r') as f:\n                return json.load(f)\n        except (json.JSONDecodeError, FileNotFoundError):\n            return {}\n\n    def write_state(self, state_data: Dict):\n        \"\"\"Writes the instance's state to its state file.\"\"\"\n        self.instance_dir.mkdir(parents=True, exist_ok=True)\n        with open(self.state_file, 'w') as f:\n            json.dump(state_data, f, indent=2)\n\n    def is_running(self) -&gt; bool:\n        \"\"\"Checks if the process associated with this instance is active.\"\"\"\n        pid = self.read_state().get('pid')\n        return psutil.pid_exists(pid) if pid else False\n\n    def generate_config(self, mode: str, config_data: Dict):\n        \"\"\"Generates the config.toml file for this specific instance.\"\"\"\n        content = f'mode = \"{mode}\"\\n\\n'\n\n        if mode == \"relay\":\n            content += \"[relay]\\n\"\n            content += f'bind_address = \"{config_data.get(\"bind_address\", \"0.0.0.0:9000\")}\"\\n'\n            content += f'password = \"{config_data.get(\"password\", \"\")}\"\\n'\n\n        elif mode == \"peer\":\n            content += \"[peer]\\n\"\n            content += f'relay_address = \"{config_data.get(\"relay_address\", \"127.0.0.1:9000\")}\"\\n'\n            content += f'relay_password = \"{config_data.get(\"relay_password\", \"\")}\"\\n'\n            content += f'peer_id = \"{config_data.get(\"peer_id\", \"default-peer\")}\"\\n'\n            content += f'listen_address = \"{config_data.get(\"listen_address\", \"127.0.0.1:8000\")}\"\\n'\n            content += f'forward_to_address = \"{config_data.get(\"forward_to_address\", \"127.0.0.1:3000\")}\"\\n'\n            if config_data.get(\"target_peer_id\"):\n                content += f'target_peer_id = \"{config_data.get(\"target_peer_id\")}\"\\n'\n\n        self.instance_dir.mkdir(parents=True, exist_ok=True)\n        with open(self.config_file, \"w\") as f:\n            f.write(content)\n        print(f\"    {Style.GREEN('Generated config:')} {Style.GREY(str(self.config_file))}\")\n\n    def start(self, executable_path: Path, mode: str, config_data: dict) -&gt; bool:\n        \"\"\"Starts the instance process, detaches it, and logs its state.\"\"\"\n        if self.is_running():\n            print(Style.YELLOW(f\"Instance '{self.name}' is already running.\"))\n            return True\n\n        print(Style.CYAN(f\"\ud83d\ude80 Starting instance '{self.name}'...\"))\n        self.generate_config(mode, config_data)\n        log_handle = open(self.log_file, 'a')\n\n        try:\n            with Spinner(f\"Launching process for '{self.name}'\", symbols=\"d\"):\n                process = subprocess.Popen(\n                    [str(executable_path)],\n                    cwd=str(self.instance_dir),\n                    stdout=log_handle,\n                    stderr=log_handle,\n                    creationflags=subprocess.DETACHED_PROCESS if platform.system() == \"Windows\" else 0\n                )\n                time.sleep(1.5)  # Give it a moment to stabilize or crash\n\n            if process.poll() is not None:\n                print(f\"\\n{Style.RED2('\u274c ERROR:')} Instance '{self.name}' failed to start. Check logs for details:\")\n                print(f\"    {Style.GREY(self.log_file)}\")\n                return False\n\n            state = {'pid': process.pid, 'mode': mode, 'config': config_data}\n            self.write_state(state)\n            print(\n                f\"\\n{Style.GREEN2('\u2705 Instance')} '{Style.Bold(self.name)}' {Style.GREEN2('started successfully.')} {Style.GREY(f'(PID: {process.pid})')}\")\n            print(f\"   {Style.BLUE('Logging to:')} {Style.GREY(self.log_file)}\")\n            return True\n        except Exception as e:\n            print(f\"\\n{Style.RED2('\u274c ERROR:')} Failed to launch instance '{self.name}': {e}\")\n            log_handle.close()\n            return False\n\n    def stop(self, timeout: int = 10) -&gt; bool:\n        \"\"\"Stops the instance process gracefully with a forced kill fallback.\"\"\"\n        if not self.is_running():\n            print(Style.YELLOW(f\"Instance '{self.name}' is not running.\"))\n            self.write_state({})\n            return True\n\n        pid = self.read_state().get('pid')\n        with Spinner(f\"Stopping '{self.name}' (PID: {pid})\", symbols=\"+\", time_in_s=timeout, count_down=True) as s:\n            try:\n                proc = psutil.Process(pid)\n                proc.terminate()\n                proc.wait(timeout)\n            except psutil.TimeoutExpired:\n                s.message = f\"Force killing '{self.name}'\"\n                proc.kill()\n            except psutil.NoSuchProcess:\n                pass\n            except Exception as e:\n                print(f\"\\n{Style.RED2('\u274c ERROR:')} Failed to stop instance '{self.name}': {e}\")\n                return False\n\n        self.write_state({})\n        print(f\"\\n{Style.VIOLET2('\u23f9\ufe0f  Instance')} '{Style.Bold(self.name)}' {Style.VIOLET2('stopped.')}\")\n        return True\n</code></pre> <code>generate_config(mode, config_data)</code> \u00b6 <p>Generates the config.toml file for this specific instance.</p> Source code in <code>toolboxv2/utils/system/tcm_p2p_cli.py</code> <pre><code>def generate_config(self, mode: str, config_data: Dict):\n    \"\"\"Generates the config.toml file for this specific instance.\"\"\"\n    content = f'mode = \"{mode}\"\\n\\n'\n\n    if mode == \"relay\":\n        content += \"[relay]\\n\"\n        content += f'bind_address = \"{config_data.get(\"bind_address\", \"0.0.0.0:9000\")}\"\\n'\n        content += f'password = \"{config_data.get(\"password\", \"\")}\"\\n'\n\n    elif mode == \"peer\":\n        content += \"[peer]\\n\"\n        content += f'relay_address = \"{config_data.get(\"relay_address\", \"127.0.0.1:9000\")}\"\\n'\n        content += f'relay_password = \"{config_data.get(\"relay_password\", \"\")}\"\\n'\n        content += f'peer_id = \"{config_data.get(\"peer_id\", \"default-peer\")}\"\\n'\n        content += f'listen_address = \"{config_data.get(\"listen_address\", \"127.0.0.1:8000\")}\"\\n'\n        content += f'forward_to_address = \"{config_data.get(\"forward_to_address\", \"127.0.0.1:3000\")}\"\\n'\n        if config_data.get(\"target_peer_id\"):\n            content += f'target_peer_id = \"{config_data.get(\"target_peer_id\")}\"\\n'\n\n    self.instance_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.config_file, \"w\") as f:\n        f.write(content)\n    print(f\"    {Style.GREEN('Generated config:')} {Style.GREY(str(self.config_file))}\")\n</code></pre> <code>is_running()</code> \u00b6 <p>Checks if the process associated with this instance is active.</p> Source code in <code>toolboxv2/utils/system/tcm_p2p_cli.py</code> <pre><code>def is_running(self) -&gt; bool:\n    \"\"\"Checks if the process associated with this instance is active.\"\"\"\n    pid = self.read_state().get('pid')\n    return psutil.pid_exists(pid) if pid else False\n</code></pre> <code>read_state()</code> \u00b6 <p>Reads the instance's state (pid, mode, etc.) from its state file.</p> Source code in <code>toolboxv2/utils/system/tcm_p2p_cli.py</code> <pre><code>def read_state(self) -&gt; Dict:\n    \"\"\"Reads the instance's state (pid, mode, etc.) from its state file.\"\"\"\n    if not self.state_file.exists():\n        return {}\n    try:\n        with open(self.state_file, 'r') as f:\n            return json.load(f)\n    except (json.JSONDecodeError, FileNotFoundError):\n        return {}\n</code></pre> <code>start(executable_path, mode, config_data)</code> \u00b6 <p>Starts the instance process, detaches it, and logs its state.</p> Source code in <code>toolboxv2/utils/system/tcm_p2p_cli.py</code> <pre><code>def start(self, executable_path: Path, mode: str, config_data: dict) -&gt; bool:\n    \"\"\"Starts the instance process, detaches it, and logs its state.\"\"\"\n    if self.is_running():\n        print(Style.YELLOW(f\"Instance '{self.name}' is already running.\"))\n        return True\n\n    print(Style.CYAN(f\"\ud83d\ude80 Starting instance '{self.name}'...\"))\n    self.generate_config(mode, config_data)\n    log_handle = open(self.log_file, 'a')\n\n    try:\n        with Spinner(f\"Launching process for '{self.name}'\", symbols=\"d\"):\n            process = subprocess.Popen(\n                [str(executable_path)],\n                cwd=str(self.instance_dir),\n                stdout=log_handle,\n                stderr=log_handle,\n                creationflags=subprocess.DETACHED_PROCESS if platform.system() == \"Windows\" else 0\n            )\n            time.sleep(1.5)  # Give it a moment to stabilize or crash\n\n        if process.poll() is not None:\n            print(f\"\\n{Style.RED2('\u274c ERROR:')} Instance '{self.name}' failed to start. Check logs for details:\")\n            print(f\"    {Style.GREY(self.log_file)}\")\n            return False\n\n        state = {'pid': process.pid, 'mode': mode, 'config': config_data}\n        self.write_state(state)\n        print(\n            f\"\\n{Style.GREEN2('\u2705 Instance')} '{Style.Bold(self.name)}' {Style.GREEN2('started successfully.')} {Style.GREY(f'(PID: {process.pid})')}\")\n        print(f\"   {Style.BLUE('Logging to:')} {Style.GREY(self.log_file)}\")\n        return True\n    except Exception as e:\n        print(f\"\\n{Style.RED2('\u274c ERROR:')} Failed to launch instance '{self.name}': {e}\")\n        log_handle.close()\n        return False\n</code></pre> <code>stop(timeout=10)</code> \u00b6 <p>Stops the instance process gracefully with a forced kill fallback.</p> Source code in <code>toolboxv2/utils/system/tcm_p2p_cli.py</code> <pre><code>def stop(self, timeout: int = 10) -&gt; bool:\n    \"\"\"Stops the instance process gracefully with a forced kill fallback.\"\"\"\n    if not self.is_running():\n        print(Style.YELLOW(f\"Instance '{self.name}' is not running.\"))\n        self.write_state({})\n        return True\n\n    pid = self.read_state().get('pid')\n    with Spinner(f\"Stopping '{self.name}' (PID: {pid})\", symbols=\"+\", time_in_s=timeout, count_down=True) as s:\n        try:\n            proc = psutil.Process(pid)\n            proc.terminate()\n            proc.wait(timeout)\n        except psutil.TimeoutExpired:\n            s.message = f\"Force killing '{self.name}'\"\n            proc.kill()\n        except psutil.NoSuchProcess:\n            pass\n        except Exception as e:\n            print(f\"\\n{Style.RED2('\u274c ERROR:')} Failed to stop instance '{self.name}': {e}\")\n            return False\n\n    self.write_state({})\n    print(f\"\\n{Style.VIOLET2('\u23f9\ufe0f  Instance')} '{Style.Bold(self.name)}' {Style.VIOLET2('stopped.')}\")\n    return True\n</code></pre> <code>write_state(state_data)</code> \u00b6 <p>Writes the instance's state to its state file.</p> Source code in <code>toolboxv2/utils/system/tcm_p2p_cli.py</code> <pre><code>def write_state(self, state_data: Dict):\n    \"\"\"Writes the instance's state to its state file.\"\"\"\n    self.instance_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.state_file, 'w') as f:\n        json.dump(state_data, f, indent=2)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.tcm_p2p_cli.find_instances","title":"<code>find_instances()</code>","text":"<p>Discovers all managed instances by scanning the instances directory.</p> Source code in <code>toolboxv2/utils/system/tcm_p2p_cli.py</code> <pre><code>def find_instances() -&gt; List['InstanceManager']:\n    \"\"\"Discovers all managed instances by scanning the instances directory.\"\"\"\n    if not INSTANCES_ROOT_DIR.is_dir():\n        return []\n\n    instance_managers = []\n    for instance_dir in INSTANCES_ROOT_DIR.iterdir():\n        if instance_dir.is_dir():\n            instance_managers.append(InstanceManager(instance_dir.name))\n    return instance_managers\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.tcm_p2p_cli.get_executable_path","title":"<code>get_executable_path(update=False)</code>","text":"<p>Finds the release executable in standard locations.</p> Source code in <code>toolboxv2/utils/system/tcm_p2p_cli.py</code> <pre><code>def get_executable_path(update=False) -&gt; Path | None:\n    \"\"\"Finds the release executable in standard locations.\"\"\"\n    # Look in a dedicated 'bin' folder first, then cargo's default\n    from toolboxv2 import tb_root_dir\n    search_paths = [\n        tb_root_dir /\"bin\" / EXECUTABLE_NAME,\n        tb_root_dir / \"tcm\"/ \"target\" / \"release\" / EXECUTABLE_NAME,\n    ]\n    if update:\n        search_paths = search_paths[::-1]\n    for path in search_paths:\n        print(path)\n        if path.is_file():\n            return path.resolve()\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types","title":"<code>types</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.types.AppType","title":"<code>AppType</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class AppType:\n    prefix: str\n    id: str\n    globals: dict[str, Any] = {\"root\": dict, }\n    locals: dict[str, Any] = {\"user\": {'app': \"self\"}, }\n\n    local_test: bool = False\n    start_dir: str\n    data_dir: str\n    config_dir: str\n    info_dir: str\n\n    logger: logging.Logger\n    logging_filename: str\n\n    api_allowed_mods_list: list[str] = []\n\n    version: str\n    loop: asyncio.AbstractEventLoop\n\n    keys: dict[str, str] = {\n        \"MACRO\": \"macro~~~~:\",\n        \"MACRO_C\": \"m_color~~:\",\n        \"HELPER\": \"helper~~~:\",\n        \"debug\": \"debug~~~~:\",\n        \"id\": \"name-spa~:\",\n        \"st-load\": \"mute~load:\",\n        \"comm-his\": \"comm-his~:\",\n        \"develop-mode\": \"dev~mode~:\",\n        \"provider::\": \"provider::\",\n    }\n\n    defaults: dict[str, (bool or dict or dict[str, dict[str, str]] or str or list[str] or list[list]) | None] = {\n        \"MACRO\": list[str],\n        \"MACRO_C\": dict,\n        \"HELPER\": dict,\n        \"debug\": str,\n        \"id\": str,\n        \"st-load\": False,\n        \"comm-his\": list[list],\n        \"develop-mode\": bool,\n    }\n\n    cluster_manager: ClusterManager\n    root_blob_storage: BlobStorage\n    config_fh: FileHandler\n    _debug: bool\n    flows: dict[str, Callable]\n    dev_modi: bool\n    functions: dict[str, Any]\n    modules: dict[str, Any]\n\n    interface_type: ToolBoxInterfaces\n    REFIX: str\n\n    alive: bool\n    called_exit: tuple[bool, float]\n    args_sto: AppArgs\n    system_flag = None\n    session = None\n    appdata = None\n    exit_tasks = []\n\n    enable_profiling: bool = False\n    sto = None\n\n    def __init__(self, prefix: None | str= None, args: AppArgs | None = None):\n        self.args_sto = args\n        self.prefix = prefix\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def exit_main(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def hide_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def show_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def disconnect(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def set_logger(self, debug=False):\n        \"\"\"proxi attr\"\"\"\n\n    @property\n    def debug(self):\n        \"\"\"proxi attr\"\"\"\n        return self._debug\n\n    def debug_rains(self, e):\n        \"\"\"proxi attr\"\"\"\n\n    def set_flows(self, r):\n        \"\"\"proxi attr\"\"\"\n\n    def run_flows(self, name, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def rrun_flows(self, name, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def idle(self):\n        import time\n        self.print(\"idle\")\n        try:\n            while self.alive:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            pass\n        self.print(\"idle done\")\n\n    async def a_idle(self):\n        self.print(\"a idle\")\n        try:\n            if hasattr(self, 'daemon_app'):\n                self.print(\"serving daemon\")\n                await self.daemon_app.connect(self)\n            else:\n                self.print(\"serving default\")\n                while self.alive:\n                    await asyncio.sleep(1)\n        except KeyboardInterrupt:\n            pass\n        self.print(\"a idle done\")\n\n    @debug.setter\n    def debug(self, value):\n        \"\"\"proxi attr\"\"\"\n\n    def _coppy_mod(self, content, new_mod_dir, mod_name, file_type='py'):\n        \"\"\"proxi attr\"\"\"\n\n    def _pre_lib_mod(self, mod_name, path_to=\"./runtime\", file_type='py'):\n        \"\"\"proxi attr\"\"\"\n\n    def _copy_load(self, mod_name, file_type='py', **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True):\n        \"\"\"proxi attr\"\"\"\n\n    def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n        \"\"\"proxi attr\"\"\"\n\n    def save_initialized_module(self, tools_class, spec):\n        \"\"\"proxi attr\"\"\"\n\n    def mod_online(self, mod_name, installed=False):\n        \"\"\"proxi attr\"\"\"\n\n    def _get_function(self,\n                      name: Enum or None,\n                      state: bool = True,\n                      specification: str = \"app\",\n                      metadata=False, as_str: tuple or None = None, r=0):\n        \"\"\"proxi attr\"\"\"\n\n    def save_exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def load_mod(self, mod_name: str, mlm='I', **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    async def init_module(self, modular):\n        return await self.load_mod(modular)\n\n    async def load_all_mods_in_file(self, working_dir=\"mods\"):\n        \"\"\"proxi attr\"\"\"\n\n    def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\"):\n        \"\"\"proxi attr\"\"\"\n\n    def remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            self.remove_mod(mod, delete=delete)\n\n    async def a_remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            await self.a_remove_mod(mod, delete=delete)\n\n    def print_ok(self):\n        \"\"\"proxi attr\"\"\"\n        self.logger.info(\"OK\")\n\n    def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n        \"\"\"proxi attr\"\"\"\n\n    def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None):\n        \"\"\"proxi attr\"\"\"\n\n    def remove_mod(self, mod_name, spec='app', delete=True):\n        \"\"\"proxi attr\"\"\"\n\n    async def a_remove_mod(self, mod_name, spec='app', delete=True):\n        \"\"\"proxi attr\"\"\"\n\n    def exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def web_context(self) -&gt; str:\n        \"\"\"returns the build index ( toolbox web component )\"\"\"\n\n    async def a_exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def save_load(self, modname, spec='app'):\n        \"\"\"proxi attr\"\"\"\n\n    def get_function(self, name: Enum or tuple, **kwargs):\n        \"\"\"\n        Kwargs for _get_function\n            metadata:: return the registered function dictionary\n                stateless: (function_data, None), 0\n                stateful: (function_data, higher_order_function), 0\n            state::boolean\n                specification::str default app\n        \"\"\"\n\n    def run_a_from_sync(self, function, *args):\n        \"\"\"\n        run a async fuction\n        \"\"\"\n\n    def run_bg_task_advanced(self, task, *args, **kwargs):\n        \"\"\"\n        proxi attr\n        \"\"\"\n\n    def wait_for_bg_tasks(self, timeout=None):\n        \"\"\"\n        proxi attr\n        \"\"\"\n\n    def run_bg_task(self, task):\n        \"\"\"\n                run a async fuction\n                \"\"\"\n    def run_function(self, mod_function_name: Enum or tuple,\n                     tb_run_function_with_state=True,\n                     tb_run_with_specification='app',\n                     args_=None,\n                     kwargs_=None,\n                     *args,\n                     **kwargs) -&gt; Result:\n\n        \"\"\"proxi attr\"\"\"\n\n    async def a_run_function(self, mod_function_name: Enum or tuple,\n                             tb_run_function_with_state=True,\n                             tb_run_with_specification='app',\n                             args_=None,\n                             kwargs_=None,\n                             *args,\n                             **kwargs) -&gt; Result:\n\n        \"\"\"proxi attr\"\"\"\n\n    def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n        \"\"\"\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        proxi attr\n        \"\"\"\n\n    async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict):\n        \"\"\"\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        proxi attr\n        \"\"\"\n\n    async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None, method=\"GET\",\n                       args_=None,\n                       kwargs_=None,\n                       *args, **kwargs):\n        \"\"\"run a function remote via http / https\"\"\"\n\n    def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n                get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                kwargs_=None,\n                *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                        backwords_compability_variabel_string_holder=None,\n                        get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                        kwargs_=None,\n                        *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def print(text, *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def sprint(text, *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    # ----------------------------------------------------------------\n    # Decorators for the toolbox\n\n    def _register_function(self, module_name, func_name, data):\n        \"\"\"proxi attr\"\"\"\n\n    def _create_decorator(self, type_: str,\n                          name: str = \"\",\n                          mod_name: str = \"\",\n                          level: int = -1,\n                          restrict_in_virtual_mode: bool = False,\n                          api: bool = False,\n                          helper: str = \"\",\n                          version: str or None = None,\n                          initial=False,\n                          exit_f=False,\n                          test=True,\n                          samples=None,\n                          state=None,\n                          pre_compute=None,\n                          post_compute=None,\n                          memory_cache=False,\n                          file_cache=False,\n                          row=False,\n                          request_as_kwarg=False,\n                          memory_cache_max_size=100,\n                          memory_cache_ttl=300):\n        \"\"\"proxi attr\"\"\"\n\n        # data = {\n        #     \"type\": type_,\n        #     \"module_name\": module_name,\n        #     \"func_name\": func_name,\n        #     \"level\": level,\n        #     \"restrict_in_virtual_mode\": restrict_in_virtual_mode,\n        #     \"func\": func,\n        #     \"api\": api,\n        #     \"helper\": helper,\n        #     \"version\": version,\n        #     \"initial\": initial,\n        #     \"exit_f\": exit_f,\n        #     \"__module__\": func.__module__,\n        #     \"signature\": sig,\n        #     \"params\": params,\n        #     \"state\": (\n        #         False if len(params) == 0 else params[0] in ['self', 'state', 'app']) if state is None else state,\n        #     \"do_test\": test,\n        #     \"samples\": samples,\n        #     \"request_as_kwarg\": request_as_kwarg,\n\n    def tb(self, name=None,\n           mod_name: str = \"\",\n           helper: str = \"\",\n           version: str or None = None,\n           test: bool = True,\n           restrict_in_virtual_mode: bool = False,\n           api: bool = False,\n           initial: bool = False,\n           exit_f: bool = False,\n           test_only: bool = False,\n           memory_cache: bool = False,\n           file_cache: bool = False,\n           row=False,\n           request_as_kwarg: bool = False,\n           state: bool or None = None,\n           level: int = 0,\n           memory_cache_max_size: int = 100,\n           memory_cache_ttl: int = 300,\n           samples: list or dict or None = None,\n           interface: ToolBoxInterfaces or None or str = None,\n           pre_compute=None,\n           post_compute=None,\n           api_methods=None,\n           ):\n        \"\"\"\n    A decorator for registering and configuring functions within a module.\n\n    This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\n    Args:\n        name (str, optional): The name to register the function under. Defaults to the function's own name.\n        mod_name (str, optional): The name of the module the function belongs to.\n        helper (str, optional): A helper string providing additional information about the function.\n        version (str or None, optional): The version of the function or module.\n        test (bool, optional): Flag to indicate if the function is for testing purposes.\n        restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n        api (bool, optional): Flag to indicate if the function is part of an API.\n        initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n        exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n        test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n        memory_cache (bool, optional): Flag to enable memory caching for the function.\n        request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n        file_cache (bool, optional): Flag to enable file caching for the function.\n        row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n        state (bool or None, optional): Flag to indicate if the function maintains state.\n        level (int, optional): The level of the function, used for prioritization or categorization.\n        memory_cache_max_size (int, optional): Maximum size of the memory cache.\n        memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n        samples (list or dict or None, optional): Samples or examples of function usage.\n        interface (str, optional): The interface type for the function.\n        pre_compute (callable, optional): A function to be called before the main function.\n        post_compute (callable, optional): A function to be called after the main function.\n        api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\n    Returns:\n        function: The decorated function with additional processing and registration capabilities.\n    \"\"\"\n        if interface is None:\n            interface = \"tb\"\n        if test_only and 'test' not in self.id:\n            return lambda *args, **kwargs: args\n        return self._create_decorator(interface,\n                                      name,\n                                      mod_name,\n                                      level=level,\n                                      restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                      helper=helper,\n                                      api=api,\n                                      version=version,\n                                      initial=initial,\n                                      exit_f=exit_f,\n                                      test=test,\n                                      samples=samples,\n                                      state=state,\n                                      pre_compute=pre_compute,\n                                      post_compute=post_compute,\n                                      memory_cache=memory_cache,\n                                      file_cache=file_cache,\n                                      row=row,\n                                      request_as_kwarg=request_as_kwarg,\n                                      memory_cache_max_size=memory_cache_max_size,\n                                      memory_cache_ttl=memory_cache_ttl)\n\n    def print_functions(self, name=None):\n\n\n        if not self.functions:\n            print(\"Nothing to see\")\n            return\n\n        def helper(_functions):\n            for func_name, data in _functions.items():\n                if not isinstance(data, dict):\n                    continue\n\n                func_type = data.get('type', 'Unknown')\n                func_level = 'r' if data['level'] == -1 else data['level']\n                api_status = 'Api' if data.get('api', False) else 'Non-Api'\n\n                print(f\"  Function: {func_name}{data.get('signature', '()')}; \"\n                      f\"Type: {func_type}, Level: {func_level}, {api_status}\")\n\n        if name is not None:\n            functions = self.functions.get(name)\n            if functions is not None:\n                print(f\"\\nModule: {name}; Type: {functions.get('app_instance_type', 'Unknown')}\")\n                helper(functions)\n                return\n        for module, functions in self.functions.items():\n            print(f\"\\nModule: {module}; Type: {functions.get('app_instance_type', 'Unknown')}\")\n            helper(functions)\n\n    def save_autocompletion_dict(self):\n        \"\"\"proxi attr\"\"\"\n\n    def get_autocompletion_dict(self):\n        \"\"\"proxi attr\"\"\"\n\n    def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n        \"\"\"proxi attr\"\"\"\n\n    def save_registry_as_enums(self, directory: str, filename: str):\n        \"\"\"proxi attr\"\"\"\n\n    async def execute_all_functions_(self, m_query='', f_query=''):\n        print(\"Executing all functions\")\n        from ..extras import generate_test_cases\n        all_data = {\n            \"modular_run\": 0,\n            \"modular_fatal_error\": 0,\n            \"errors\": 0,\n            \"modular_sug\": 0,\n            \"coverage\": [],\n            \"total_coverage\": {},\n        }\n        items = list(self.functions.items()).copy()\n        for module_name, functions in items:\n            infos = {\n                \"functions_run\": 0,\n                \"functions_fatal_error\": 0,\n                \"error\": 0,\n                \"functions_sug\": 0,\n                'calls': {},\n                'callse': {},\n                \"coverage\": [0, 0],\n            }\n            all_data['modular_run'] += 1\n            if not module_name.startswith(m_query):\n                all_data['modular_sug'] += 1\n                continue\n\n            with Spinner(message=f\"In {module_name}| \"):\n                f_items = list(functions.items()).copy()\n                for function_name, function_data in f_items:\n                    if not isinstance(function_data, dict):\n                        continue\n                    if not function_name.startswith(f_query):\n                        continue\n                    test: list = function_data.get('do_test')\n                    # print(test, module_name, function_name, function_data)\n                    infos[\"coverage\"][0] += 1\n                    if test is False:\n                        continue\n\n                    with Spinner(message=f\"\\t\\t\\t\\t\\t\\tfuction {function_name}...\"):\n                        params: list = function_data.get('params')\n                        sig: signature = function_data.get('signature')\n                        state: bool = function_data.get('state')\n                        samples: bool = function_data.get('samples')\n\n                        test_kwargs_list = [{}]\n\n                        if params is not None:\n                            test_kwargs_list = samples if samples is not None else generate_test_cases(sig=sig)\n                            # print(test_kwargs)\n                            # print(test_kwargs[0])\n                            # test_kwargs = test_kwargs_list[0]\n                        # print(module_name, function_name, test_kwargs_list)\n                        infos[\"coverage\"][1] += 1\n                        for test_kwargs in test_kwargs_list:\n                            try:\n                                # print(f\"test Running {state=} |{module_name}.{function_name}\")\n                                result = await self.a_run_function((module_name, function_name),\n                                                                   tb_run_function_with_state=state,\n                                                                   **test_kwargs)\n                                if not isinstance(result, Result):\n                                    result = Result.ok(result)\n                                if result.info.exec_code == 0:\n                                    infos['calls'][function_name] = [test_kwargs, str(result)]\n                                    infos['functions_sug'] += 1\n                                else:\n                                    infos['functions_sug'] += 1\n                                    infos['error'] += 1\n                                    infos['callse'][function_name] = [test_kwargs, str(result)]\n                            except Exception as e:\n                                infos['functions_fatal_error'] += 1\n                                infos['callse'][function_name] = [test_kwargs, str(e)]\n                            finally:\n                                infos['functions_run'] += 1\n\n                if infos['functions_run'] == infos['functions_sug']:\n                    all_data['modular_sug'] += 1\n                else:\n                    all_data['modular_fatal_error'] += 1\n                if infos['error'] &gt; 0:\n                    all_data['errors'] += infos['error']\n\n                all_data[module_name] = infos\n                if infos['coverage'][0] == 0:\n                    c = 0\n                else:\n                    c = infos['coverage'][1] / infos['coverage'][0]\n                all_data[\"coverage\"].append(f\"{module_name}:{c:.2f}\\n\")\n        total_coverage = sum([float(t.split(\":\")[-1]) for t in all_data[\"coverage\"]]) / len(all_data[\"coverage\"])\n        print(\n            f\"\\n{all_data['modular_run']=}\\n{all_data['modular_sug']=}\\n{all_data['modular_fatal_error']=}\\n{total_coverage=}\")\n        d = analyze_data(all_data)\n        return Result.ok(data=all_data, data_info=d)\n\n    @staticmethod\n    def calculate_complexity(filename_or_code):\n        from radon.complexity import cc_rank, cc_visit\n        if os.path.exists(filename_or_code):\n            with open(filename_or_code) as file:\n                code = file.read()\n        else:\n            code = filename_or_code\n\n        # Calculate and print Cyclomatic Complexity\n        complexity_results = cc_visit(code)\n        i = -1\n        avg_complexity = 0\n        for block in complexity_results:\n            complexity = block.complexity\n            i += 1\n            print(f\"block: {block.name} {i} Class/Fuction/Methode : {block.letter}\")\n            print(f\"    fullname: {block.fullname}\")\n            print(f\"    Cyclomatic Complexity: {complexity}\")\n            # Optional: Get complexity rank\n            avg_complexity += complexity\n            rank = cc_rank(complexity)\n            print(f\"    Complexity Rank: {rank}\")\n            # print(f\"    lineno: {block.lineno}\")\n            print(f\"    endline: {block.endline}\")\n            print(f\"    col_offset: {block.col_offset}\\n\")\n        if i &lt;= 0:\n            i += 2\n        avg_complexity = avg_complexity / i\n        print(f\"\\nAVG Complexity: {avg_complexity:.2f}\")\n        print(f\"Total Rank: {cc_rank(int(avg_complexity + i // 10))}\")\n\n    async def execute_function_test(self, module_name: str, function_name: str,\n                                    function_data: dict, test_kwargs: dict,\n                                    profiler: cProfile.Profile) -&gt; tuple[bool, str, dict, float]:\n        start_time = time.time()\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            try:\n                result = await self.a_run_function(\n                    (module_name, function_name),\n                    tb_run_function_with_state=function_data.get('state'),\n                    **test_kwargs\n                )\n\n                if not isinstance(result, Result):\n                    result = Result.ok(result)\n\n                success = result.info.exec_code == 0\n                execution_time = time.time() - start_time\n                return success, str(result), test_kwargs, execution_time\n            except Exception as e:\n                execution_time = time.time() - start_time\n                return False, str(e), test_kwargs, execution_time\n\n    async def process_function(self, module_name: str, function_name: str,\n                               function_data: dict, profiler: cProfile.Profile) -&gt; tuple[str, ModuleInfo]:\n        start_time = time.time()\n        info = ModuleInfo()\n\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            if not isinstance(function_data, dict):\n                return function_name, info\n\n            test = function_data.get('do_test')\n            info.coverage[0] += 1\n\n            if test is False:\n                return function_name, info\n\n            params = function_data.get('params')\n            sig = function_data.get('signature')\n            samples = function_data.get('samples')\n\n            test_kwargs_list = [{}] if params is None else (\n                samples if samples is not None else generate_test_cases(sig=sig)\n            )\n\n            info.coverage[1] += 1\n\n            # Create tasks for all test cases\n            tasks = [\n                self.execute_function_test(module_name, function_name, function_data, test_kwargs, profiler)\n                for test_kwargs in test_kwargs_list\n            ]\n\n            # Execute all tests concurrently\n            results = await asyncio.gather(*tasks)\n\n            total_execution_time = 0\n            for success, result_str, test_kwargs, execution_time in results:\n                info.functions_run += 1\n                total_execution_time += execution_time\n\n                if success:\n                    info.functions_sug += 1\n                    info.calls[function_name] = [test_kwargs, result_str]\n                else:\n                    info.functions_sug += 1\n                    info.error += 1\n                    info.callse[function_name] = [test_kwargs, result_str]\n\n            info.execution_time = time.time() - start_time\n            return function_name, info\n\n    async def process_module(self, module_name: str, functions: dict,\n                             f_query: str, profiler: cProfile.Profile) -&gt; tuple[str, ModuleInfo]:\n        start_time = time.time()\n\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            async with asyncio.Semaphore(mp.cpu_count()):\n                tasks = [\n                    self.process_function(module_name, fname, fdata, profiler)\n                    for fname, fdata in functions.items()\n                    if fname.startswith(f_query)\n                ]\n\n                if not tasks:\n                    return module_name, ModuleInfo()\n\n                results = await asyncio.gather(*tasks)\n\n                # Combine results from all functions in the module\n                combined_info = ModuleInfo()\n                total_execution_time = 0\n\n                for _, info in results:\n                    combined_info.functions_run += info.functions_run\n                    combined_info.functions_fatal_error += info.functions_fatal_error\n                    combined_info.error += info.error\n                    combined_info.functions_sug += info.functions_sug\n                    combined_info.calls.update(info.calls)\n                    combined_info.callse.update(info.callse)\n                    combined_info.coverage[0] += info.coverage[0]\n                    combined_info.coverage[1] += info.coverage[1]\n                    total_execution_time += info.execution_time\n\n                combined_info.execution_time = time.time() - start_time\n                return module_name, combined_info\n\n    async def execute_all_functions(self, m_query='', f_query='', enable_profiling=True):\n        \"\"\"\n        Execute all functions with parallel processing and optional profiling.\n\n        Args:\n            m_query (str): Module name query filter\n            f_query (str): Function name query filter\n            enable_profiling (bool): Enable detailed profiling information\n        \"\"\"\n        print(\"Executing all functions in parallel\" + (\" with profiling\" if enable_profiling else \"\"))\n\n        start_time = time.time()\n        stats = ExecutionStats()\n        items = list(self.functions.items()).copy()\n\n        # Set up profiling\n        self.enable_profiling = enable_profiling\n        profiler = cProfile.Profile()\n\n        with profile_section(profiler, enable_profiling):\n            # Filter modules based on query\n            filtered_modules = [\n                (mname, mfuncs) for mname, mfuncs in items\n                if mname.startswith(m_query)\n            ]\n\n            stats.modular_run = len(filtered_modules)\n\n            # Process all modules concurrently\n            async with asyncio.Semaphore(mp.cpu_count()):\n                tasks = [\n                    self.process_module(mname, mfuncs, f_query, profiler)\n                    for mname, mfuncs in filtered_modules\n                ]\n\n                results = await asyncio.gather(*tasks)\n\n            # Combine results and calculate statistics\n            for module_name, info in results:\n                if info.functions_run == info.functions_sug:\n                    stats.modular_sug += 1\n                else:\n                    stats.modular_fatal_error += 1\n\n                stats.errors += info.error\n\n                # Calculate coverage\n                coverage = (info.coverage[1] / info.coverage[0]) if info.coverage[0] &gt; 0 else 0\n                stats.coverage.append(f\"{module_name}:{coverage:.2f}\\n\")\n\n                # Store module info\n                stats.__dict__[module_name] = info\n\n            # Calculate total coverage\n            total_coverage = (\n                sum(float(t.split(\":\")[-1]) for t in stats.coverage) / len(stats.coverage)\n                if stats.coverage else 0\n            )\n\n            stats.total_execution_time = time.time() - start_time\n\n            # Generate profiling stats if enabled\n            if enable_profiling:\n                s = io.StringIO()\n                ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n                ps.print_stats()\n                stats.profiling_data = {\n                    'detailed_stats': s.getvalue(),\n                    'total_time': stats.total_execution_time,\n                    'function_count': stats.modular_run,\n                    'successful_functions': stats.modular_sug\n                }\n\n            print(\n                f\"\\n{stats.modular_run=}\"\n                f\"\\n{stats.modular_sug=}\"\n                f\"\\n{stats.modular_fatal_error=}\"\n                f\"\\n{total_coverage=}\"\n                f\"\\nTotal execution time: {stats.total_execution_time:.2f}s\"\n            )\n\n            if enable_profiling:\n                print(\"\\nProfiling Summary:\")\n                print(f\"{'=' * 50}\")\n                print(\"Top 10 time-consuming functions:\")\n                ps.print_stats(10)\n\n            analyzed_data = analyze_data(stats.__dict__)\n            return Result.ok(data=stats.__dict__, data_info=analyzed_data)\n</code></pre> <code>debug</code> <code>property</code> <code>writable</code> \u00b6 <p>proxi attr</p> <code>prefix = prefix</code> <code>instance-attribute</code> \u00b6 <p>proxi attr</p> <code>a_exit()</code> <code>async</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>a_fuction_runner(function, function_data, args, kwargs)</code> <code>async</code> \u00b6 <p>parameters = function_data.get('params') modular_name = function_data.get('module_name') function_name = function_data.get('func_name') mod_function_name = f\"{modular_name}.{function_name}\"</p> <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict):\n    \"\"\"\n    parameters = function_data.get('params')\n    modular_name = function_data.get('module_name')\n    function_name = function_data.get('func_name')\n    mod_function_name = f\"{modular_name}.{function_name}\"\n\n    proxi attr\n    \"\"\"\n</code></pre> <code>a_remove_mod(mod_name, spec='app', delete=True)</code> <code>async</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_remove_mod(self, mod_name, spec='app', delete=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>a_run_any(mod_function_name, backwords_compability_variabel_string_holder=None, get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code> <code>async</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                    backwords_compability_variabel_string_holder=None,\n                    get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                    kwargs_=None,\n                    *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>a_run_function(mod_function_name, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code> <code>async</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_run_function(self, mod_function_name: Enum or tuple,\n                         tb_run_function_with_state=True,\n                         tb_run_with_specification='app',\n                         args_=None,\n                         kwargs_=None,\n                         *args,\n                         **kwargs) -&gt; Result:\n\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>debug_rains(e)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def debug_rains(self, e):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>disconnect(*args, **kwargs)</code> <code>async</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def disconnect(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>execute_all_functions(m_query='', f_query='', enable_profiling=True)</code> <code>async</code> \u00b6 <p>Execute all functions with parallel processing and optional profiling.</p> <p>Parameters:</p> Name Type Description Default <code>m_query</code> <code>str</code> <p>Module name query filter</p> <code>''</code> <code>f_query</code> <code>str</code> <p>Function name query filter</p> <code>''</code> <code>enable_profiling</code> <code>bool</code> <p>Enable detailed profiling information</p> <code>True</code> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def execute_all_functions(self, m_query='', f_query='', enable_profiling=True):\n    \"\"\"\n    Execute all functions with parallel processing and optional profiling.\n\n    Args:\n        m_query (str): Module name query filter\n        f_query (str): Function name query filter\n        enable_profiling (bool): Enable detailed profiling information\n    \"\"\"\n    print(\"Executing all functions in parallel\" + (\" with profiling\" if enable_profiling else \"\"))\n\n    start_time = time.time()\n    stats = ExecutionStats()\n    items = list(self.functions.items()).copy()\n\n    # Set up profiling\n    self.enable_profiling = enable_profiling\n    profiler = cProfile.Profile()\n\n    with profile_section(profiler, enable_profiling):\n        # Filter modules based on query\n        filtered_modules = [\n            (mname, mfuncs) for mname, mfuncs in items\n            if mname.startswith(m_query)\n        ]\n\n        stats.modular_run = len(filtered_modules)\n\n        # Process all modules concurrently\n        async with asyncio.Semaphore(mp.cpu_count()):\n            tasks = [\n                self.process_module(mname, mfuncs, f_query, profiler)\n                for mname, mfuncs in filtered_modules\n            ]\n\n            results = await asyncio.gather(*tasks)\n\n        # Combine results and calculate statistics\n        for module_name, info in results:\n            if info.functions_run == info.functions_sug:\n                stats.modular_sug += 1\n            else:\n                stats.modular_fatal_error += 1\n\n            stats.errors += info.error\n\n            # Calculate coverage\n            coverage = (info.coverage[1] / info.coverage[0]) if info.coverage[0] &gt; 0 else 0\n            stats.coverage.append(f\"{module_name}:{coverage:.2f}\\n\")\n\n            # Store module info\n            stats.__dict__[module_name] = info\n\n        # Calculate total coverage\n        total_coverage = (\n            sum(float(t.split(\":\")[-1]) for t in stats.coverage) / len(stats.coverage)\n            if stats.coverage else 0\n        )\n\n        stats.total_execution_time = time.time() - start_time\n\n        # Generate profiling stats if enabled\n        if enable_profiling:\n            s = io.StringIO()\n            ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n            ps.print_stats()\n            stats.profiling_data = {\n                'detailed_stats': s.getvalue(),\n                'total_time': stats.total_execution_time,\n                'function_count': stats.modular_run,\n                'successful_functions': stats.modular_sug\n            }\n\n        print(\n            f\"\\n{stats.modular_run=}\"\n            f\"\\n{stats.modular_sug=}\"\n            f\"\\n{stats.modular_fatal_error=}\"\n            f\"\\n{total_coverage=}\"\n            f\"\\nTotal execution time: {stats.total_execution_time:.2f}s\"\n        )\n\n        if enable_profiling:\n            print(\"\\nProfiling Summary:\")\n            print(f\"{'=' * 50}\")\n            print(\"Top 10 time-consuming functions:\")\n            ps.print_stats(10)\n\n        analyzed_data = analyze_data(stats.__dict__)\n        return Result.ok(data=stats.__dict__, data_info=analyzed_data)\n</code></pre> <code>exit()</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>exit_main(*args, **kwargs)</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef exit_main(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>fuction_runner(function, function_data, args, kwargs, t0=0.0)</code> \u00b6 <p>parameters = function_data.get('params') modular_name = function_data.get('module_name') function_name = function_data.get('func_name') mod_function_name = f\"{modular_name}.{function_name}\"</p> <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n    \"\"\"\n    parameters = function_data.get('params')\n    modular_name = function_data.get('module_name')\n    function_name = function_data.get('func_name')\n    mod_function_name = f\"{modular_name}.{function_name}\"\n\n    proxi attr\n    \"\"\"\n</code></pre> <code>get_all_mods(working_dir='mods', path_to='./runtime')</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>get_autocompletion_dict()</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_autocompletion_dict(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>get_function(name, **kwargs)</code> \u00b6 <p>Kwargs for _get_function     metadata:: return the registered function dictionary         stateless: (function_data, None), 0         stateful: (function_data, higher_order_function), 0     state::boolean         specification::str default app</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_function(self, name: Enum or tuple, **kwargs):\n    \"\"\"\n    Kwargs for _get_function\n        metadata:: return the registered function dictionary\n            stateless: (function_data, None), 0\n            stateful: (function_data, higher_order_function), 0\n        state::boolean\n            specification::str default app\n    \"\"\"\n</code></pre> <code>get_mod(name, spec='app')</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>get_username(get_input=False, default='loot')</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>hide_console(*args, **kwargs)</code> <code>async</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def hide_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>inplace_load_instance(mod_name, loc='toolboxv2.mods.', spec='app', save=True)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>load_all_mods_in_file(working_dir='mods')</code> <code>async</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def load_all_mods_in_file(self, working_dir=\"mods\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>load_mod(mod_name, mlm='I', **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def load_mod(self, mod_name: str, mlm='I', **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>mod_online(mod_name, installed=False)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def mod_online(self, mod_name, installed=False):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>print(text, *args, **kwargs)</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef print(text, *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>print_ok()</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def print_ok(self):\n    \"\"\"proxi attr\"\"\"\n    self.logger.info(\"OK\")\n</code></pre> <code>reload_mod(mod_name, spec='app', is_file=True, loc='toolboxv2.mods.')</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>remove_mod(mod_name, spec='app', delete=True)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def remove_mod(self, mod_name, spec='app', delete=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>rrun_flows(name, **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def rrun_flows(self, name, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>run_a_from_sync(function, *args)</code> \u00b6 <p>run a async fuction</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_a_from_sync(self, function, *args):\n    \"\"\"\n    run a async fuction\n    \"\"\"\n</code></pre> <code>run_any(mod_function_name, backwords_compability_variabel_string_holder=None, get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n            get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n            kwargs_=None,\n            *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>run_bg_task(task)</code> \u00b6 <p>run a async fuction</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_bg_task(self, task):\n    \"\"\"\n            run a async fuction\n            \"\"\"\n</code></pre> <code>run_bg_task_advanced(task, *args, **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_bg_task_advanced(self, task, *args, **kwargs):\n    \"\"\"\n    proxi attr\n    \"\"\"\n</code></pre> <code>run_flows(name, **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_flows(self, name, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>run_function(mod_function_name, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_function(self, mod_function_name: Enum or tuple,\n                 tb_run_function_with_state=True,\n                 tb_run_with_specification='app',\n                 args_=None,\n                 kwargs_=None,\n                 *args,\n                 **kwargs) -&gt; Result:\n\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>run_http(mod_function_name, function_name=None, method='GET', args_=None, kwargs_=None, *args, **kwargs)</code> <code>async</code> \u00b6 <p>run a function remote via http / https</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None, method=\"GET\",\n                   args_=None,\n                   kwargs_=None,\n                   *args, **kwargs):\n    \"\"\"run a function remote via http / https\"\"\"\n</code></pre> <code>save_autocompletion_dict()</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_autocompletion_dict(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>save_exit()</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>save_initialized_module(tools_class, spec)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_initialized_module(self, tools_class, spec):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>save_instance(instance, modular_id, spec='app', instance_type='file/application', tools_class=None)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>save_load(modname, spec='app')</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_load(self, modname, spec='app'):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>save_registry_as_enums(directory, filename)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_registry_as_enums(self, directory: str, filename: str):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>set_flows(r)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def set_flows(self, r):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>set_logger(debug=False)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def set_logger(self, debug=False):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>show_console(*args, **kwargs)</code> <code>async</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def show_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>sprint(text, *args, **kwargs)</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef sprint(text, *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>tb(name=None, mod_name='', helper='', version=None, test=True, restrict_in_virtual_mode=False, api=False, initial=False, exit_f=False, test_only=False, memory_cache=False, file_cache=False, row=False, request_as_kwarg=False, state=None, level=0, memory_cache_max_size=100, memory_cache_ttl=300, samples=None, interface=None, pre_compute=None, post_compute=None, api_methods=None)</code> \u00b6 <p>A decorator for registering and configuring functions within a module.</p> <p>This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to register the function under. Defaults to the function's own name.</p> <code>None</code> <code>mod_name</code> <code>str</code> <p>The name of the module the function belongs to.</p> <code>''</code> <code>helper</code> <code>str</code> <p>A helper string providing additional information about the function.</p> <code>''</code> <code>version</code> <code>str or None</code> <p>The version of the function or module.</p> <code>None</code> <code>test</code> <code>bool</code> <p>Flag to indicate if the function is for testing purposes.</p> <code>True</code> <code>restrict_in_virtual_mode</code> <code>bool</code> <p>Flag to restrict the function in virtual mode.</p> <code>False</code> <code>api</code> <code>bool</code> <p>Flag to indicate if the function is part of an API.</p> <code>False</code> <code>initial</code> <code>bool</code> <p>Flag to indicate if the function should be executed at initialization.</p> <code>False</code> <code>exit_f</code> <code>bool</code> <p>Flag to indicate if the function should be executed at exit.</p> <code>False</code> <code>test_only</code> <code>bool</code> <p>Flag to indicate if the function should only be used for testing.</p> <code>False</code> <code>memory_cache</code> <code>bool</code> <p>Flag to enable memory caching for the function.</p> <code>False</code> <code>request_as_kwarg</code> <code>bool</code> <p>Flag to get request if the fuction is calld from api.</p> <code>False</code> <code>file_cache</code> <code>bool</code> <p>Flag to enable file caching for the function.</p> <code>False</code> <code>row</code> <code>bool</code> <p>rather to auto wrap the result in Result type default False means no row data aka result type</p> <code>False</code> <code>state</code> <code>bool or None</code> <p>Flag to indicate if the function maintains state.</p> <code>None</code> <code>level</code> <code>int</code> <p>The level of the function, used for prioritization or categorization.</p> <code>0</code> <code>memory_cache_max_size</code> <code>int</code> <p>Maximum size of the memory cache.</p> <code>100</code> <code>memory_cache_ttl</code> <code>int</code> <p>Time-to-live for the memory cache entries.</p> <code>300</code> <code>samples</code> <code>list or dict or None</code> <p>Samples or examples of function usage.</p> <code>None</code> <code>interface</code> <code>str</code> <p>The interface type for the function.</p> <code>None</code> <code>pre_compute</code> <code>callable</code> <p>A function to be called before the main function.</p> <code>None</code> <code>post_compute</code> <code>callable</code> <p>A function to be called after the main function.</p> <code>None</code> <code>api_methods</code> <code>list[str]</code> <p>default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>function</code> <p>The decorated function with additional processing and registration capabilities.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def tb(self, name=None,\n       mod_name: str = \"\",\n       helper: str = \"\",\n       version: str or None = None,\n       test: bool = True,\n       restrict_in_virtual_mode: bool = False,\n       api: bool = False,\n       initial: bool = False,\n       exit_f: bool = False,\n       test_only: bool = False,\n       memory_cache: bool = False,\n       file_cache: bool = False,\n       row=False,\n       request_as_kwarg: bool = False,\n       state: bool or None = None,\n       level: int = 0,\n       memory_cache_max_size: int = 100,\n       memory_cache_ttl: int = 300,\n       samples: list or dict or None = None,\n       interface: ToolBoxInterfaces or None or str = None,\n       pre_compute=None,\n       post_compute=None,\n       api_methods=None,\n       ):\n    \"\"\"\nA decorator for registering and configuring functions within a module.\n\nThis decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\nArgs:\n    name (str, optional): The name to register the function under. Defaults to the function's own name.\n    mod_name (str, optional): The name of the module the function belongs to.\n    helper (str, optional): A helper string providing additional information about the function.\n    version (str or None, optional): The version of the function or module.\n    test (bool, optional): Flag to indicate if the function is for testing purposes.\n    restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n    api (bool, optional): Flag to indicate if the function is part of an API.\n    initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n    exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n    test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n    memory_cache (bool, optional): Flag to enable memory caching for the function.\n    request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n    file_cache (bool, optional): Flag to enable file caching for the function.\n    row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n    state (bool or None, optional): Flag to indicate if the function maintains state.\n    level (int, optional): The level of the function, used for prioritization or categorization.\n    memory_cache_max_size (int, optional): Maximum size of the memory cache.\n    memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n    samples (list or dict or None, optional): Samples or examples of function usage.\n    interface (str, optional): The interface type for the function.\n    pre_compute (callable, optional): A function to be called before the main function.\n    post_compute (callable, optional): A function to be called after the main function.\n    api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\nReturns:\n    function: The decorated function with additional processing and registration capabilities.\n\"\"\"\n    if interface is None:\n        interface = \"tb\"\n    if test_only and 'test' not in self.id:\n        return lambda *args, **kwargs: args\n    return self._create_decorator(interface,\n                                  name,\n                                  mod_name,\n                                  level=level,\n                                  restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                  helper=helper,\n                                  api=api,\n                                  version=version,\n                                  initial=initial,\n                                  exit_f=exit_f,\n                                  test=test,\n                                  samples=samples,\n                                  state=state,\n                                  pre_compute=pre_compute,\n                                  post_compute=post_compute,\n                                  memory_cache=memory_cache,\n                                  file_cache=file_cache,\n                                  row=row,\n                                  request_as_kwarg=request_as_kwarg,\n                                  memory_cache_max_size=memory_cache_max_size,\n                                  memory_cache_ttl=memory_cache_ttl)\n</code></pre> <code>wait_for_bg_tasks(timeout=None)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def wait_for_bg_tasks(self, timeout=None):\n    \"\"\"\n    proxi attr\n    \"\"\"\n</code></pre> <code>watch_mod(mod_name, spec='app', loc='toolboxv2.mods.', use_thread=True, path_name=None)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>web_context()</code> \u00b6 <p>returns the build index ( toolbox web component )</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def web_context(self) -&gt; str:\n    \"\"\"returns the build index ( toolbox web component )\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.Headers","title":"<code>Headers</code>  <code>dataclass</code>","text":"<p>Class representing HTTP headers with strongly typed common fields.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@dataclass\nclass Headers:\n    \"\"\"Class representing HTTP headers with strongly typed common fields.\"\"\"\n    # General Headers\n    accept: None | str= None\n    accept_charset: None | str= None\n    accept_encoding: None | str= None\n    accept_language: None | str= None\n    accept_ranges: None | str= None\n    access_control_allow_credentials: None | str= None\n    access_control_allow_headers: None | str= None\n    access_control_allow_methods: None | str= None\n    access_control_allow_origin: None | str= None\n    access_control_expose_headers: None | str= None\n    access_control_max_age: None | str= None\n    access_control_request_headers: None | str= None\n    access_control_request_method: None | str= None\n    age: None | str= None\n    allow: None | str= None\n    alt_svc: None | str= None\n    authorization: None | str= None\n    cache_control: None | str= None\n    clear_site_data: None | str= None\n    connection: None | str= None\n    content_disposition: None | str= None\n    content_encoding: None | str= None\n    content_language: None | str= None\n    content_length: None | str= None\n    content_location: None | str= None\n    content_range: None | str= None\n    content_security_policy: None | str= None\n    content_security_policy_report_only: None | str= None\n    content_type: None | str= None\n    cookie: None | str= None\n    cross_origin_embedder_policy: None | str= None\n    cross_origin_opener_policy: None | str= None\n    cross_origin_resource_policy: None | str= None\n    date: None | str= None\n    device_memory: None | str= None\n    digest: None | str= None\n    dnt: None | str= None\n    dpr: None | str= None\n    etag: None | str= None\n    expect: None | str= None\n    expires: None | str= None\n    feature_policy: None | str= None\n    forwarded: None | str= None\n    from_header: None | str= None  # 'from' is a Python keyword\n    host: None | str= None\n    if_match: None | str= None\n    if_modified_since: None | str= None\n    if_none_match: None | str= None\n    if_range: None | str= None\n    if_unmodified_since: None | str= None\n    keep_alive: None | str= None\n    large_allocation: None | str= None\n    last_modified: None | str= None\n    link: None | str= None\n    location: None | str= None\n    max_forwards: None | str= None\n    origin: None | str= None\n    pragma: None | str= None\n    proxy_authenticate: None | str= None\n    proxy_authorization: None | str= None\n    public_key_pins: None | str= None\n    public_key_pins_report_only: None | str= None\n    range: None | str= None\n    referer: None | str= None\n    referrer_policy: None | str= None\n    retry_after: None | str= None\n    save_data: None | str= None\n    sec_fetch_dest: None | str= None\n    sec_fetch_mode: None | str= None\n    sec_fetch_site: None | str= None\n    sec_fetch_user: None | str= None\n    sec_websocket_accept: None | str= None\n    sec_websocket_extensions: None | str= None\n    sec_websocket_key: None | str= None\n    sec_websocket_protocol: None | str= None\n    sec_websocket_version: None | str= None\n    server: None | str= None\n    server_timing: None | str= None\n    service_worker_allowed: None | str= None\n    set_cookie: None | str= None\n    sourcemap: None | str= None\n    strict_transport_security: None | str= None\n    te: None | str= None\n    timing_allow_origin: None | str= None\n    tk: None | str= None\n    trailer: None | str= None\n    transfer_encoding: None | str= None\n    upgrade: None | str= None\n    upgrade_insecure_requests: None | str= None\n    user_agent: None | str= None\n    vary: None | str= None\n    via: None | str= None\n    warning: None | str= None\n    www_authenticate: None | str= None\n    x_content_type_options: None | str= None\n    x_dns_prefetch_control: None | str= None\n    x_forwarded_for: None | str= None\n    x_forwarded_host: None | str= None\n    x_forwarded_proto: None | str= None\n    x_frame_options: None | str= None\n    x_xss_protection: None | str= None\n\n    # Browser-specific and custom headers\n    sec_ch_ua: None | str= None\n    sec_ch_ua_mobile: None | str= None\n    sec_ch_ua_platform: None | str= None\n    sec_ch_ua_arch: None | str= None\n    sec_ch_ua_bitness: None | str= None\n    sec_ch_ua_full_version: None | str= None\n    sec_ch_ua_full_version_list: None | str= None\n    sec_ch_ua_platform_version: None | str= None\n\n    # HTMX specific headers\n    hx_boosted: None | str= None\n    hx_current_url: None | str= None\n    hx_history_restore_request: None | str= None\n    hx_prompt: None | str= None\n    hx_request: None | str= None\n    hx_target: None | str= None\n    hx_trigger: None | str= None\n    hx_trigger_name: None | str= None\n\n    # Additional fields can be stored in extra_headers\n    extra_headers: dict[str, str] = field(default_factory=dict)\n\n    def __post_init__(self):\n        \"\"\"Convert header keys with hyphens to underscores for attribute access.\"\"\"\n        # Handle the 'from' header specifically since it's a Python keyword\n        if 'from' in self.__dict__:\n            self.from_header = self.__dict__.pop('from')\n\n        # Store any attributes that weren't explicitly defined in extra_headers\n        all_attrs = self.__annotations__.keys()\n        for key in list(self.__dict__.keys()):\n            if key not in all_attrs and key != \"extra_headers\":\n                self.extra_headers[key.replace(\"_\", \"-\")] = getattr(self, key)\n                delattr(self, key)\n\n    @classmethod\n    def from_dict(cls, headers_dict: dict[str, str]) -&gt; 'Headers':\n        \"\"\"Create a Headers instance from a dictionary.\"\"\"\n        # Convert header keys from hyphenated to underscore format for Python attributes\n        processed_headers = {}\n        extra_headers = {}\n\n        for key, value in headers_dict.items():\n            # Handle 'from' header specifically\n            if key.lower() == 'from':\n                processed_headers['from_header'] = value\n                continue\n\n            python_key = key.replace(\"-\", \"_\").lower()\n            if python_key in cls.__annotations__ and python_key != \"extra_headers\":\n                processed_headers[python_key] = value\n            else:\n                extra_headers[key] = value\n\n        return cls(**processed_headers, extra_headers=extra_headers)\n\n    def to_dict(self) -&gt; dict[str, str]:\n        \"\"\"Convert the Headers object back to a dictionary.\"\"\"\n        result = {}\n\n        # Add regular attributes\n        for key, value in self.__dict__.items():\n            if key != \"extra_headers\" and value is not None:\n                # Handle from_header specially\n                if key == \"from_header\":\n                    result[\"from\"] = value\n                else:\n                    result[key.replace(\"_\", \"-\")] = value\n\n        # Add extra headers\n        result.update(self.extra_headers)\n\n        return result\n</code></pre> <code>__post_init__()</code> \u00b6 <p>Convert header keys with hyphens to underscores for attribute access.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Convert header keys with hyphens to underscores for attribute access.\"\"\"\n    # Handle the 'from' header specifically since it's a Python keyword\n    if 'from' in self.__dict__:\n        self.from_header = self.__dict__.pop('from')\n\n    # Store any attributes that weren't explicitly defined in extra_headers\n    all_attrs = self.__annotations__.keys()\n    for key in list(self.__dict__.keys()):\n        if key not in all_attrs and key != \"extra_headers\":\n            self.extra_headers[key.replace(\"_\", \"-\")] = getattr(self, key)\n            delattr(self, key)\n</code></pre> <code>from_dict(headers_dict)</code> <code>classmethod</code> \u00b6 <p>Create a Headers instance from a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef from_dict(cls, headers_dict: dict[str, str]) -&gt; 'Headers':\n    \"\"\"Create a Headers instance from a dictionary.\"\"\"\n    # Convert header keys from hyphenated to underscore format for Python attributes\n    processed_headers = {}\n    extra_headers = {}\n\n    for key, value in headers_dict.items():\n        # Handle 'from' header specifically\n        if key.lower() == 'from':\n            processed_headers['from_header'] = value\n            continue\n\n        python_key = key.replace(\"-\", \"_\").lower()\n        if python_key in cls.__annotations__ and python_key != \"extra_headers\":\n            processed_headers[python_key] = value\n        else:\n            extra_headers[key] = value\n\n    return cls(**processed_headers, extra_headers=extra_headers)\n</code></pre> <code>to_dict()</code> \u00b6 <p>Convert the Headers object back to a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, str]:\n    \"\"\"Convert the Headers object back to a dictionary.\"\"\"\n    result = {}\n\n    # Add regular attributes\n    for key, value in self.__dict__.items():\n        if key != \"extra_headers\" and value is not None:\n            # Handle from_header specially\n            if key == \"from_header\":\n                result[\"from\"] = value\n            else:\n                result[key.replace(\"_\", \"-\")] = value\n\n    # Add extra headers\n    result.update(self.extra_headers)\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.MainToolType","title":"<code>MainToolType</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class MainToolType:\n    toolID: str\n    app: A\n    interface: ToolBoxInterfaces\n    spec: str\n\n    version: str\n    tools: dict  # legacy\n    name: str\n    logger: logging\n    color: str\n    todo: Callable\n    _on_exit: Callable\n    stuf: bool\n    config: dict\n    user: U | None\n    description: str\n\n    @staticmethod\n    def return_result(error: ToolBoxError = ToolBoxError.none,\n                      exec_code: int = 0,\n                      help_text: str = \"\",\n                      data_info=None,\n                      data=None,\n                      data_to=None) -&gt; Result:\n        \"\"\"proxi attr\"\"\"\n\n    def load(self):\n        \"\"\"proxi attr\"\"\"\n\n    def print(self, message, end=\"\\n\", **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def add_str_to_config(self, command):\n        if len(command) != 2:\n            self.logger.error('Invalid command must be key value')\n            return False\n        self.config[command[0]] = command[1]\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n\n    async def get_user(self, username: str) -&gt; Result:\n        return self.app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME, username=username, get_results=True)\n</code></pre> <code>load()</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def load(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>print(message, end='\\n', **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def print(self, message, end=\"\\n\", **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>return_result(error=ToolBoxError.none, exec_code=0, help_text='', data_info=None, data=None, data_to=None)</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef return_result(error: ToolBoxError = ToolBoxError.none,\n                  exec_code: int = 0,\n                  help_text: str = \"\",\n                  data_info=None,\n                  data=None,\n                  data_to=None) -&gt; Result:\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>webInstall(user_instance, construct_render)</code> \u00b6 <p>\"Returns a web installer for the given user instance and construct render template</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def webInstall(self, user_instance, construct_render) -&gt; str:\n    \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.Request","title":"<code>Request</code>  <code>dataclass</code>","text":"<p>Class representing an HTTP request.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@dataclass\nclass Request:\n    \"\"\"Class representing an HTTP request.\"\"\"\n    content_type: str\n    headers: Headers\n    method: str\n    path: str\n    query_params: dict[str, Any] = field(default_factory=dict)\n    form_data: dict[str, Any] | None = None\n    body: Any | None = None\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any]) -&gt; 'Request':\n        \"\"\"Create a Request instance from a dictionary.\"\"\"\n        headers = Headers.from_dict(data.get('headers', {}))\n\n        # Extract other fields\n        return cls(\n            content_type=data.get('content_type', ''),\n            headers=headers,\n            method=data.get('method', ''),\n            path=data.get('path', ''),\n            query_params=data.get('query_params', {}),\n            form_data=data.get('form_data'),\n            body=data.get('body')\n        )\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert the Request object back to a dictionary.\"\"\"\n        result = {\n            'content_type': self.content_type,\n            'headers': self.headers.to_dict(),\n            'method': self.method,\n            'path': self.path,\n            'query_params': self.query_params,\n        }\n\n        if self.form_data is not None:\n            result['form_data'] = self.form_data\n\n        if self.body is not None:\n            result['body'] = self.body\n\n        return result\n</code></pre> <code>from_dict(data)</code> <code>classmethod</code> \u00b6 <p>Create a Request instance from a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; 'Request':\n    \"\"\"Create a Request instance from a dictionary.\"\"\"\n    headers = Headers.from_dict(data.get('headers', {}))\n\n    # Extract other fields\n    return cls(\n        content_type=data.get('content_type', ''),\n        headers=headers,\n        method=data.get('method', ''),\n        path=data.get('path', ''),\n        query_params=data.get('query_params', {}),\n        form_data=data.get('form_data'),\n        body=data.get('body')\n    )\n</code></pre> <code>to_dict()</code> \u00b6 <p>Convert the Request object back to a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert the Request object back to a dictionary.\"\"\"\n    result = {\n        'content_type': self.content_type,\n        'headers': self.headers.to_dict(),\n        'method': self.method,\n        'path': self.path,\n        'query_params': self.query_params,\n    }\n\n    if self.form_data is not None:\n        result['form_data'] = self.form_data\n\n    if self.body is not None:\n        result['body'] = self.body\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.RequestData","title":"<code>RequestData</code>  <code>dataclass</code>","text":"<p>Main class representing the complete request data structure.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@dataclass\nclass RequestData:\n    \"\"\"Main class representing the complete request data structure.\"\"\"\n    request: Request\n    session: Session\n    session_id: str\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any]) -&gt; 'RequestData':\n        \"\"\"Create a RequestData instance from a dictionary.\"\"\"\n        return cls(\n            request=Request.from_dict(data.get('request', {})),\n            session=Session.from_dict(data.get('session', {})),\n            session_id=data.get('session_id', '')\n        )\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert the RequestData object back to a dictionary.\"\"\"\n        return {\n            'request': self.request.to_dict(),\n            'session': self.session.to_dict(),\n            'session_id': self.session_id\n        }\n\n    def __getattr__(self, name: str) -&gt; Any:\n        \"\"\"Delegate unknown attributes to the `request` object.\"\"\"\n        # Nur wenn das Attribut nicht direkt in RequestData existiert\n        # und auch nicht `session` oder `session_id` ist\n        if hasattr(self.request, name):\n            return getattr(self.request, name)\n        raise AttributeError(f\"'RequestData' object has no attribute '{name}'\")\n\n    @classmethod\n    def moc(cls):\n        return cls(\n            request=Request.from_dict({\n                'content_type': 'application/x-www-form-urlencoded',\n                'headers': {\n                    'accept': '*/*',\n                    'accept-encoding': 'gzip, deflate, br, zstd',\n                    'accept-language': 'de-DE,de;q=0.9,en-US;q=0.8,en;q=0.7',\n                    'connection': 'keep-alive',\n                    'content-length': '107',\n                    'content-type': 'application/x-www-form-urlencoded',\n                    'cookie': 'session=abc123',\n                    'host': 'localhost:8080',\n                    'hx-current-url': 'http://localhost:8080/api/TruthSeeker/get_main_ui',\n                    'hx-request': 'true',\n                    'hx-target': 'estimates-guest_1fc2c9',\n                    'hx-trigger': 'config-form-guest_1fc2c9',\n                    'origin': 'http://localhost:8080',\n                    'referer': 'http://localhost:8080/api/TruthSeeker/get_main_ui',\n                    'sec-ch-ua': '\"Chromium\";v=\"134\", \"Not:A-Brand\";v=\"24\", \"Google Chrome\";v=\"134\"',\n                    'sec-ch-ua-mobile': '?0',\n                    'sec-ch-ua-platform': '\"Windows\"',\n                    'sec-fetch-dest': 'empty',\n                    'sec-fetch-mode': 'cors',\n                    'sec-fetch-site': 'same-origin',\n                    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n                },\n                'method': 'POST',\n                'path': '/api/TruthSeeker/update_estimates',\n                'query_params': {},\n                'form_data': {\n                    'param1': 'value1',\n                    'param2': 'value2'\n                }\n            }),\n            session=Session.from_dict({\n                'SiID': '29a2e258e18252e2afd5ff943523f09c82f1bb9adfe382a6f33fc6a8381de898',\n                'level': '1',\n                'spec': '74eed1c8de06886842e235486c3c2fd6bcd60586998ac5beb87f13c0d1750e1d',\n                'user_name': 'root',\n                'custom_field': 'custom_value'\n            }),\n            session_id='0x29dd1ac0d1e30d3f'\n        )\n</code></pre> <code>__getattr__(name)</code> \u00b6 <p>Delegate unknown attributes to the <code>request</code> object.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"Delegate unknown attributes to the `request` object.\"\"\"\n    # Nur wenn das Attribut nicht direkt in RequestData existiert\n    # und auch nicht `session` oder `session_id` ist\n    if hasattr(self.request, name):\n        return getattr(self.request, name)\n    raise AttributeError(f\"'RequestData' object has no attribute '{name}'\")\n</code></pre> <code>from_dict(data)</code> <code>classmethod</code> \u00b6 <p>Create a RequestData instance from a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; 'RequestData':\n    \"\"\"Create a RequestData instance from a dictionary.\"\"\"\n    return cls(\n        request=Request.from_dict(data.get('request', {})),\n        session=Session.from_dict(data.get('session', {})),\n        session_id=data.get('session_id', '')\n    )\n</code></pre> <code>to_dict()</code> \u00b6 <p>Convert the RequestData object back to a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert the RequestData object back to a dictionary.\"\"\"\n    return {\n        'request': self.request.to_dict(),\n        'session': self.session.to_dict(),\n        'session_id': self.session_id\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.Result","title":"<code>Result</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class Result:\n    _task = None\n    def __init__(self,\n                 error: ToolBoxError,\n                 result: ToolBoxResult,\n                 info: ToolBoxInfo,\n                 origin: Any | None = None,\n                 ):\n        self.error: ToolBoxError = error\n        self.result: ToolBoxResult = result\n        self.info: ToolBoxInfo = info\n        self.origin = origin\n\n    def as_result(self):\n        return self\n\n    def as_dict(self):\n        return {\n            \"error\":self.error.value if isinstance(self.error, Enum) else self.error,\n        \"result\" : {\n            \"data_to\":self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n            \"data_info\":self.result.data_info,\n            \"data\":self.result.data,\n            \"data_type\":self.result.data_type\n        } if self.result else None,\n        \"info\" : {\n            \"exec_code\" : self.info.exec_code,  # exec_code umwandel in http resposn codes\n        \"help_text\" : self.info.help_text\n        } if self.info else None,\n        \"origin\" : self.origin\n        }\n\n    def set_origin(self, origin):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = origin\n        return self\n\n    def set_dir_origin(self, name, extras=\"assets/\"):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = f\"mods/{name}/{extras}\"\n        return self\n\n    def is_error(self):\n        if _test_is_result(self.result.data):\n            return self.result.data.is_error()\n        if self.error == ToolBoxError.none:\n            return False\n        if self.info.exec_code == 0:\n            return False\n        if self.info.exec_code == 200:\n            return False\n        return True\n\n    def is_data(self):\n        return self.result.data is not None\n\n    def to_api_result(self):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=self.error.value if isinstance(self.error, Enum) else self.error,\n            result=ToolBoxResultBM(\n                data_to=self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n                data_info=self.result.data_info,\n                data=self.result.data,\n                data_type=self.result.data_type\n            ) if self.result else None,\n            info=ToolBoxInfoBM(\n                exec_code=self.info.exec_code,  # exec_code umwandel in http resposn codes\n                help_text=self.info.help_text\n            ) if self.info else None,\n            origin=self.origin\n        )\n\n    def task(self, task):\n        self._task = task\n        return self\n\n    @staticmethod\n    def result_from_dict(error: str, result: dict, info: dict, origin: list or None or str):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=error if isinstance(error, Enum) else error,\n            result=ToolBoxResultBM(\n                data_to=result.get('data_to') if isinstance(result.get('data_to'), Enum) else result.get('data_to'),\n                data_info=result.get('data_info', '404'),\n                data=result.get('data'),\n                data_type=result.get('data_type', '404'),\n            ) if result else ToolBoxResultBM(\n                data_to=ToolBoxInterfaces.cli.value,\n                data_info='',\n                data='404',\n                data_type='404',\n            ),\n            info=ToolBoxInfoBM(\n                exec_code=info.get('exec_code', 404),\n                help_text=info.get('help_text', '404')\n            ) if info else ToolBoxInfoBM(\n                exec_code=404,\n                help_text='404'\n            ),\n            origin=origin\n        ).as_result()\n\n    @classmethod\n    def stream(cls,\n               stream_generator: Any,  # Renamed from source for clarity\n               content_type: str = \"text/event-stream\",  # Default to SSE\n               headers: Union[dict, None] = None,\n               info: str = \"OK\",\n               interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n               cleanup_func: Union[\n                   Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None):\n        \"\"\"\n        Create a streaming response Result. Handles SSE and other stream types.\n\n        Args:\n            stream_generator: Any stream source (async generator, sync generator, iterable, or single item).\n            content_type: Content-Type header (default: text/event-stream for SSE).\n            headers: Additional HTTP headers for the response.\n            info: Help text for the result.\n            interface: Interface to send data to.\n            cleanup_func: Optional function for cleanup.\n\n        Returns:\n            A Result object configured for streaming.\n        \"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        final_generator: AsyncGenerator[str, None]\n\n        if content_type == \"text/event-stream\":\n            # For SSE, always use SSEGenerator.create_sse_stream to wrap the source.\n            # SSEGenerator.create_sse_stream handles various types of stream_generator internally.\n            final_generator = SSEGenerator.create_sse_stream(source=stream_generator, cleanup_func=cleanup_func)\n\n            # Standard SSE headers for the HTTP response itself\n            # These will be stored in the Result object. Rust side decides how to use them.\n            standard_sse_headers = {\n                \"Cache-Control\": \"no-cache\",  # SSE specific\n                \"Connection\": \"keep-alive\",  # SSE specific\n                \"X-Accel-Buffering\": \"no\",  # Useful for proxies with SSE\n                # Content-Type is implicitly text/event-stream, will be in streaming_data below\n            }\n            all_response_headers = standard_sse_headers.copy()\n            if headers:\n                all_response_headers.update(headers)\n        else:\n            # For non-SSE streams.\n            # If stream_generator is sync, wrap it to be async.\n            # If already async or single item, it will be handled.\n            # Rust's stream_generator in ToolboxClient seems to handle both sync/async Python generators.\n            # For consistency with how SSEGenerator does it, we can wrap sync ones.\n            if inspect.isgenerator(stream_generator) or \\\n                (not isinstance(stream_generator, str) and hasattr(stream_generator, '__iter__')):\n                final_generator = SSEGenerator.wrap_sync_generator(stream_generator)  # Simple async wrapper\n            elif inspect.isasyncgen(stream_generator):\n                final_generator = stream_generator\n            else:  # Single item or string\n                async def _single_item_gen():\n                    yield stream_generator\n\n                final_generator = _single_item_gen()\n            all_response_headers = headers if headers else {}\n\n        # Prepare streaming data to be stored in the Result object\n        streaming_data = {\n            \"type\": \"stream\",  # Indicator for Rust side\n            \"generator\": final_generator,\n            \"content_type\": content_type,  # Let Rust know the intended content type\n            \"headers\": all_response_headers  # Intended HTTP headers for the overall response\n        }\n\n        result_payload = ToolBoxResult(\n            data_to=interface,\n            data=streaming_data,\n            data_info=\"Streaming response\" if content_type != \"text/event-stream\" else \"SSE Event Stream\",\n            data_type=\"stream\"  # Generic type for Rust to identify it needs to stream from 'generator'\n        )\n\n        return cls(error=error, info=info_obj, result=result_payload)\n\n    @classmethod\n    def sse(cls,\n            stream_generator: Any,\n            info: str = \"OK\",\n            interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n            cleanup_func: Union[\n                Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None,\n            # http_headers: Optional[dict] = None # If we want to allow overriding default SSE HTTP headers\n            ):\n        \"\"\"\n        Create an Server-Sent Events (SSE) streaming response Result.\n\n        Args:\n            stream_generator: A source yielding individual data items. This can be an\n                              async generator, sync generator, iterable, or a single item.\n                              Each item will be formatted as an SSE event.\n            info: Optional help text for the Result.\n            interface: Optional ToolBoxInterface to target.\n            cleanup_func: Optional cleanup function to run when the stream ends or is cancelled.\n            #http_headers: Optional dictionary of custom HTTP headers for the SSE response.\n\n        Returns:\n            A Result object configured for SSE streaming.\n        \"\"\"\n        # Result.stream will handle calling SSEGenerator.create_sse_stream\n        # and setting appropriate default headers for SSE when content_type is \"text/event-stream\".\n        return cls.stream(\n            stream_generator=stream_generator,\n            content_type=\"text/event-stream\",\n            # headers=http_headers, # Pass if we add http_headers param\n            info=info,\n            interface=interface,\n            cleanup_func=cleanup_func\n        )\n\n    @classmethod\n    def default(cls, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=-1, help_text=\"\")\n        result = ToolBoxResult(data_to=interface)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None):\n        \"\"\"Create a JSON response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code or exec_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=data,\n            data_info=\"JSON response\",\n            data_type=\"json\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n        \"\"\"Create a text response Result with specific content type.\"\"\"\n        if headers is not None:\n            return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=text_data,\n            data_info=\"Text response\",\n            data_type=content_type\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n               interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a binary data response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        # Create a dictionary with binary data and metadata\n        binary_data = {\n            \"data\": data,\n            \"content_type\": content_type,\n            \"filename\": download_name\n        }\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=binary_data,\n            data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n            data_type=\"binary\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def file(cls, data, filename, content_type=None, info=\"OK\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a file download response Result.\n\n        Args:\n            data: File data as bytes or base64 string\n            filename: Name of the file for download\n            content_type: MIME type of the file (auto-detected if None)\n            info: Response info text\n            interface: Target interface\n\n        Returns:\n            Result object configured for file download\n        \"\"\"\n        import base64\n        import mimetypes\n\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=200, help_text=info)\n\n        # Auto-detect content type if not provided\n        if content_type is None:\n            content_type, _ = mimetypes.guess_type(filename)\n            if content_type is None:\n                content_type = \"application/octet-stream\"\n\n        # Ensure data is base64 encoded string (as expected by Rust server)\n        if isinstance(data, bytes):\n            base64_data = base64.b64encode(data).decode('utf-8')\n        elif isinstance(data, str):\n            # Assume it's already base64 encoded\n            base64_data = data\n        else:\n            raise ValueError(\"File data must be bytes or base64 string\")\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=base64_data,  # Rust expects base64 string for \"file\" type\n            data_info=f\"File download: {filename}\",\n            data_type=\"file\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a redirect response.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=url,\n            data_info=\"Redirect response\",\n            data_type=\"redirect\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def ok(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def html(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.remote, data_type=\"html\",status=200, headers=None, row=False):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=status, help_text=info)\n        from ...utils.system.getting_and_closing_app import get_app\n\n        if not row and not '\"&lt;div class=\"main-content\"\"' in data:\n            data = f'&lt;div class=\"main-content frosted-glass\"&gt;{data}&lt;div&gt;'\n        if not row and not get_app().web_context() in data:\n            data = get_app().web_context() + data\n\n        if isinstance(headers, dict):\n            result = ToolBoxResult(data_to=interface, data={'html':data,'headers':headers}, data_info=data_info,\n                                   data_type=\"special_html\")\n        else:\n            result = ToolBoxResult(data_to=interface, data=data, data_info=data_info,\n                                   data_type=data_type if data_type is not None else type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def future(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.future):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=\"future\")\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def custom_error(cls, data=None, data_info=\"\", info=\"\", exec_code=-1, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def error(cls, data=None, data_info=\"\", info=\"\", exec_code=450, interface=ToolBoxInterfaces.remote):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_user_error(cls, info=\"\", exec_code=-3, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.input_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_internal_error(cls, info=\"\", exec_code=-2, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.internal_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    def print(self, show=True, show_data=True, prifix=\"\"):\n        data = '\\n' + f\"{((prifix + 'Data: ' + str(self.result.data) if self.result.data is not None else 'NO Data') if not isinstance(self.result.data, Result) else self.result.data.print(show=False, show_data=show_data, prifix=prifix + '-')) if show_data else 'Data: private'}\"\n        origin = '\\n' + f\"{prifix + 'Origin: ' + str(self.origin) if self.origin is not None else 'NO Origin'}\"\n        text = (f\"Function Exec code: {self.info.exec_code}\"\n                f\"\\n{prifix}Info's:\"\n                f\" {self.info.help_text} {'&lt;|&gt; ' + str(self.result.data_info) if self.result.data_info is not None else ''}\"\n                f\"{origin}{data if not data.endswith('NO Data') else ''}\")\n        if not show:\n            return text\n        print(\"\\n======== Result ========\\n\" + text + \"\\n------- EndOfD -------\")\n        return self\n\n    def log(self, show_data=True, prifix=\"\"):\n        from toolboxv2 import get_logger\n        get_logger().debug(self.print(show=False, show_data=show_data, prifix=prifix).replace(\"\\n\", \" - \"))\n        return self\n\n    def __str__(self):\n        return self.print(show=False, show_data=True)\n\n    def get(self, key=None, default=None):\n        data = self.result.data\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    async def aget(self, key=None, default=None):\n        if asyncio.isfuture(self.result.data) or asyncio.iscoroutine(self.result.data) or (\n            isinstance(self.result.data_to, Enum) and self.result.data_to.name == ToolBoxInterfaces.future.name):\n            data = await self.result.data\n        else:\n            data = self.get(key=None, default=None)\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    def lazy_return(self, _=0, data=None, **kwargs):\n        flags = ['raise', 'logg', 'user', 'intern']\n        flag = flags[_] if isinstance(_, int) else _\n        if self.info.exec_code == 0:\n            return self if data is None else data if _test_is_result(data) else self.ok(data=data, **kwargs)\n        if flag == 'raise':\n            raise ValueError(self.print(show=False))\n        if flag == 'logg':\n            from .. import get_logger\n            get_logger().error(self.print(show=False))\n\n        if flag == 'user':\n            return self if data is None else data if _test_is_result(data) else self.default_user_error(data=data,\n                                                                                                        **kwargs)\n        if flag == 'intern':\n            return self if data is None else data if _test_is_result(data) else self.default_internal_error(data=data,\n                                                                                                            **kwargs)\n\n        return self if data is None else data if _test_is_result(data) else self.custom_error(data=data, **kwargs)\n\n    @property\n    def bg_task(self):\n        return self._task\n</code></pre> <code>binary(data, content_type='application/octet-stream', download_name=None, info='OK', interface=ToolBoxInterfaces.remote)</code> <code>classmethod</code> \u00b6 <p>Create a binary data response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n           interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a binary data response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    # Create a dictionary with binary data and metadata\n    binary_data = {\n        \"data\": data,\n        \"content_type\": content_type,\n        \"filename\": download_name\n    }\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=binary_data,\n        data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n        data_type=\"binary\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre> <code>file(data, filename, content_type=None, info='OK', interface=ToolBoxInterfaces.remote)</code> <code>classmethod</code> \u00b6 <p>Create a file download response Result.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>File data as bytes or base64 string</p> required <code>filename</code> <p>Name of the file for download</p> required <code>content_type</code> <p>MIME type of the file (auto-detected if None)</p> <code>None</code> <code>info</code> <p>Response info text</p> <code>'OK'</code> <code>interface</code> <p>Target interface</p> <code>remote</code> <p>Returns:</p> Type Description <p>Result object configured for file download</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef file(cls, data, filename, content_type=None, info=\"OK\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a file download response Result.\n\n    Args:\n        data: File data as bytes or base64 string\n        filename: Name of the file for download\n        content_type: MIME type of the file (auto-detected if None)\n        info: Response info text\n        interface: Target interface\n\n    Returns:\n        Result object configured for file download\n    \"\"\"\n    import base64\n    import mimetypes\n\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=200, help_text=info)\n\n    # Auto-detect content type if not provided\n    if content_type is None:\n        content_type, _ = mimetypes.guess_type(filename)\n        if content_type is None:\n            content_type = \"application/octet-stream\"\n\n    # Ensure data is base64 encoded string (as expected by Rust server)\n    if isinstance(data, bytes):\n        base64_data = base64.b64encode(data).decode('utf-8')\n    elif isinstance(data, str):\n        # Assume it's already base64 encoded\n        base64_data = data\n    else:\n        raise ValueError(\"File data must be bytes or base64 string\")\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=base64_data,  # Rust expects base64 string for \"file\" type\n        data_info=f\"File download: {filename}\",\n        data_type=\"file\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre> <code>json(data, info='OK', interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None)</code> <code>classmethod</code> \u00b6 <p>Create a JSON response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None):\n    \"\"\"Create a JSON response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code or exec_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=data,\n        data_info=\"JSON response\",\n        data_type=\"json\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre> <code>redirect(url, status_code=302, info='Redirect', interface=ToolBoxInterfaces.remote)</code> <code>classmethod</code> \u00b6 <p>Create a redirect response.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a redirect response.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=url,\n        data_info=\"Redirect response\",\n        data_type=\"redirect\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre> <code>sse(stream_generator, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code> <code>classmethod</code> \u00b6 <p>Create an Server-Sent Events (SSE) streaming response Result.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <code>Any</code> <p>A source yielding individual data items. This can be an               async generator, sync generator, iterable, or a single item.               Each item will be formatted as an SSE event.</p> required <code>info</code> <code>str</code> <p>Optional help text for the Result.</p> <code>'OK'</code> <code>interface</code> <code>ToolBoxInterfaces</code> <p>Optional ToolBoxInterface to target.</p> <code>remote</code> <code>cleanup_func</code> <code>Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None]</code> <p>Optional cleanup function to run when the stream ends or is cancelled.</p> <code>None</code> <code>#http_headers</code> <p>Optional dictionary of custom HTTP headers for the SSE response.</p> required <p>Returns:</p> Type Description <p>A Result object configured for SSE streaming.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef sse(cls,\n        stream_generator: Any,\n        info: str = \"OK\",\n        interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n        cleanup_func: Union[\n            Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None,\n        # http_headers: Optional[dict] = None # If we want to allow overriding default SSE HTTP headers\n        ):\n    \"\"\"\n    Create an Server-Sent Events (SSE) streaming response Result.\n\n    Args:\n        stream_generator: A source yielding individual data items. This can be an\n                          async generator, sync generator, iterable, or a single item.\n                          Each item will be formatted as an SSE event.\n        info: Optional help text for the Result.\n        interface: Optional ToolBoxInterface to target.\n        cleanup_func: Optional cleanup function to run when the stream ends or is cancelled.\n        #http_headers: Optional dictionary of custom HTTP headers for the SSE response.\n\n    Returns:\n        A Result object configured for SSE streaming.\n    \"\"\"\n    # Result.stream will handle calling SSEGenerator.create_sse_stream\n    # and setting appropriate default headers for SSE when content_type is \"text/event-stream\".\n    return cls.stream(\n        stream_generator=stream_generator,\n        content_type=\"text/event-stream\",\n        # headers=http_headers, # Pass if we add http_headers param\n        info=info,\n        interface=interface,\n        cleanup_func=cleanup_func\n    )\n</code></pre> <code>stream(stream_generator, content_type='text/event-stream', headers=None, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code> <code>classmethod</code> \u00b6 <p>Create a streaming response Result. Handles SSE and other stream types.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <code>Any</code> <p>Any stream source (async generator, sync generator, iterable, or single item).</p> required <code>content_type</code> <code>str</code> <p>Content-Type header (default: text/event-stream for SSE).</p> <code>'text/event-stream'</code> <code>headers</code> <code>Union[dict, None]</code> <p>Additional HTTP headers for the response.</p> <code>None</code> <code>info</code> <code>str</code> <p>Help text for the result.</p> <code>'OK'</code> <code>interface</code> <code>ToolBoxInterfaces</code> <p>Interface to send data to.</p> <code>remote</code> <code>cleanup_func</code> <code>Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None]</code> <p>Optional function for cleanup.</p> <code>None</code> <p>Returns:</p> Type Description <p>A Result object configured for streaming.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef stream(cls,\n           stream_generator: Any,  # Renamed from source for clarity\n           content_type: str = \"text/event-stream\",  # Default to SSE\n           headers: Union[dict, None] = None,\n           info: str = \"OK\",\n           interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n           cleanup_func: Union[\n               Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None):\n    \"\"\"\n    Create a streaming response Result. Handles SSE and other stream types.\n\n    Args:\n        stream_generator: Any stream source (async generator, sync generator, iterable, or single item).\n        content_type: Content-Type header (default: text/event-stream for SSE).\n        headers: Additional HTTP headers for the response.\n        info: Help text for the result.\n        interface: Interface to send data to.\n        cleanup_func: Optional function for cleanup.\n\n    Returns:\n        A Result object configured for streaming.\n    \"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    final_generator: AsyncGenerator[str, None]\n\n    if content_type == \"text/event-stream\":\n        # For SSE, always use SSEGenerator.create_sse_stream to wrap the source.\n        # SSEGenerator.create_sse_stream handles various types of stream_generator internally.\n        final_generator = SSEGenerator.create_sse_stream(source=stream_generator, cleanup_func=cleanup_func)\n\n        # Standard SSE headers for the HTTP response itself\n        # These will be stored in the Result object. Rust side decides how to use them.\n        standard_sse_headers = {\n            \"Cache-Control\": \"no-cache\",  # SSE specific\n            \"Connection\": \"keep-alive\",  # SSE specific\n            \"X-Accel-Buffering\": \"no\",  # Useful for proxies with SSE\n            # Content-Type is implicitly text/event-stream, will be in streaming_data below\n        }\n        all_response_headers = standard_sse_headers.copy()\n        if headers:\n            all_response_headers.update(headers)\n    else:\n        # For non-SSE streams.\n        # If stream_generator is sync, wrap it to be async.\n        # If already async or single item, it will be handled.\n        # Rust's stream_generator in ToolboxClient seems to handle both sync/async Python generators.\n        # For consistency with how SSEGenerator does it, we can wrap sync ones.\n        if inspect.isgenerator(stream_generator) or \\\n            (not isinstance(stream_generator, str) and hasattr(stream_generator, '__iter__')):\n            final_generator = SSEGenerator.wrap_sync_generator(stream_generator)  # Simple async wrapper\n        elif inspect.isasyncgen(stream_generator):\n            final_generator = stream_generator\n        else:  # Single item or string\n            async def _single_item_gen():\n                yield stream_generator\n\n            final_generator = _single_item_gen()\n        all_response_headers = headers if headers else {}\n\n    # Prepare streaming data to be stored in the Result object\n    streaming_data = {\n        \"type\": \"stream\",  # Indicator for Rust side\n        \"generator\": final_generator,\n        \"content_type\": content_type,  # Let Rust know the intended content type\n        \"headers\": all_response_headers  # Intended HTTP headers for the overall response\n    }\n\n    result_payload = ToolBoxResult(\n        data_to=interface,\n        data=streaming_data,\n        data_info=\"Streaming response\" if content_type != \"text/event-stream\" else \"SSE Event Stream\",\n        data_type=\"stream\"  # Generic type for Rust to identify it needs to stream from 'generator'\n    )\n\n    return cls(error=error, info=info_obj, result=result_payload)\n</code></pre> <code>text(text_data, content_type='text/plain', exec_code=None, status=200, info='OK', interface=ToolBoxInterfaces.remote, headers=None)</code> <code>classmethod</code> \u00b6 <p>Create a text response Result with specific content type.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n    \"\"\"Create a text response Result with specific content type.\"\"\"\n    if headers is not None:\n        return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=text_data,\n        data_info=\"Text response\",\n        data_type=content_type\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.SSEGenerator","title":"<code>SSEGenerator</code>","text":"<p>Production-ready SSE generator that converts any data source to properly formatted Server-Sent Events compatible with browsers.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class SSEGenerator:\n    \"\"\"\n    Production-ready SSE generator that converts any data source to\n    properly formatted Server-Sent Events compatible with browsers.\n    \"\"\"\n\n    @staticmethod\n    def format_sse_event(data: Any) -&gt; str:\n        \"\"\"Format any data as a proper SSE event message.\"\"\"\n        # Already formatted as SSE\n        if isinstance(data, str) and (data.startswith('data:') or data.startswith('event:')) and '\\n\\n' in data:\n            return data\n\n        # Handle bytes (binary data)\n        if isinstance(data, bytes):\n            try:\n                # Try to decode as UTF-8 first\n                decoded_data_str = data.decode('utf-8')\n                # If decoding works, treat it as a string for further processing\n                # This allows binary data that is valid UTF-8 JSON to be processed as JSON.\n                data = decoded_data_str\n            except UnicodeDecodeError:\n                # Binary data that is not UTF-8, encode as base64\n                b64_data = base64.b64encode(data).decode('utf-8')\n                return f\"event: binary\\ndata: {b64_data}\\n\\n\"\n\n        # Convert non-string objects (that are not already bytes) to JSON string\n        # If data was bytes and successfully decoded to UTF-8 string, it will be processed here.\n        original_data_type_was_complex = False\n        if not isinstance(data, str):\n            original_data_type_was_complex = True\n            try:\n                data_str = json.dumps(data)\n            except Exception:\n                data_str = str(data)  # Fallback to string representation\n        else:\n            data_str = data  # data is already a string\n\n        # Handle JSON data with special event formatting\n        # data_str now holds the string representation (either original string or JSON string)\n        if data_str.strip().startswith('{'):\n            try:\n                json_data = json.loads(data_str)\n                if isinstance(json_data, dict) and 'event' in json_data:\n                    event_type = json_data['event']\n                    event_id = json_data.get('id', None)  # Use None to distinguish from empty string\n\n                    # Determine the actual data payload for the SSE 'data:' field\n                    # If 'data' key exists in json_data, use its content.\n                    # Otherwise, use the original data_str (which is the JSON of json_data).\n                    if 'data' in json_data:\n                        payload_content = json_data['data']\n                        # If payload_content is complex, re-serialize it to JSON string\n                        if isinstance(payload_content, (dict, list)):\n                            sse_data_field = json.dumps(payload_content)\n                        else:  # Simple type (string, number, bool)\n                            sse_data_field = str(payload_content)\n                    else:\n                        # If original data was complex (e.g. dict) and became json_data,\n                        # and no 'data' key in it, then use the full json_data as payload.\n                        # If original data was a simple string that happened to be JSON parsable\n                        # but without 'event' key, it would have been handled by \"Regular JSON without event\"\n                        # or \"Plain text\" later.\n                        # This path implies original data was a dict with 'event' key.\n                        sse_data_field = data_str\n\n                    sse_lines = []\n                    if event_type:  # Should always be true here\n                        sse_lines.append(f\"event: {event_type}\")\n                    if event_id is not None:  # Check for None, allow empty string id\n                        sse_lines.append(f\"id: {event_id}\")\n\n                    # Handle multi-line data for the data field\n                    for line in sse_data_field.splitlines():\n                        sse_lines.append(f\"data: {line}\")\n\n                    return \"\\n\".join(sse_lines) + \"\\n\\n\"\n                else:\n                    # Regular JSON without special 'event' key\n                    sse_lines = []\n                    for line in data_str.splitlines():\n                        sse_lines.append(f\"data: {line}\")\n                    return \"\\n\".join(sse_lines) + \"\\n\\n\"\n            except json.JSONDecodeError:\n                # Not valid JSON, treat as plain text\n                sse_lines = []\n                for line in data_str.splitlines():\n                    sse_lines.append(f\"data: {line}\")\n                return \"\\n\".join(sse_lines) + \"\\n\\n\"\n        else:\n            # Plain text\n            sse_lines = []\n            for line in data_str.splitlines():\n                sse_lines.append(f\"data: {line}\")\n            return \"\\n\".join(sse_lines) + \"\\n\\n\"\n\n    @classmethod\n    async def wrap_sync_generator(cls, generator):\n        \"\"\"Convert a synchronous generator to an async generator.\"\"\"\n        for item in generator:\n            yield item\n            # Allow other tasks to run\n            await asyncio.sleep(0)\n\n    @classmethod\n    async def create_sse_stream(\n        cls,\n        source: Any,  # Changed from positional arg to keyword for clarity in Result.stream\n        cleanup_func: Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None\n    ) -&gt; AsyncGenerator[str, None]:\n        \"\"\"\n        Convert any source to a properly formatted SSE stream.\n\n        Args:\n            source: Can be async generator, sync generator, iterable, or a single item.\n            cleanup_func: Optional function to call when the stream ends or is cancelled.\n                          Can be a synchronous function, async function, or async generator.\n\n        Yields:\n            Properly formatted SSE messages (strings).\n        \"\"\"\n        # Send stream start event\n        # This structure ensures data field contains {\"id\":\"0\"}\n        yield cls.format_sse_event({\"event\": \"stream_start\", \"data\": {\"id\": \"0\"}})\n\n        try:\n            # Handle different types of sources\n            if inspect.isasyncgen(source):\n                # Source is already an async generator\n                async for item in source:\n                    yield cls.format_sse_event(item)\n            elif inspect.isgenerator(source) or (not isinstance(source, str) and hasattr(source, '__iter__')):\n                # Source is a sync generator or iterable (but not a string)\n                # Strings are iterable but should be treated as single items unless explicitly made a generator\n                async for item in cls.wrap_sync_generator(source):\n                    yield cls.format_sse_event(item)\n            else:\n                # Single item (including strings)\n                yield cls.format_sse_event(source)\n        except asyncio.CancelledError:\n            # Client disconnected\n            yield cls.format_sse_event({\"event\": \"cancelled\", \"data\": {\"id\": \"cancelled\"}})\n            raise\n        except Exception as e:\n            # Error in stream\n            error_info = {\n                \"event\": \"error\",\n                \"data\": {  # Ensure payload is under 'data' key for the new format_sse_event logic\n                    \"message\": str(e),\n                    \"traceback\": traceback.format_exc()\n                }\n            }\n            yield cls.format_sse_event(error_info)\n        finally:\n            # Always send end event\n            yield cls.format_sse_event({\"event\": \"stream_end\", \"data\": {\"id\": \"final\"}})\n\n            # Execute cleanup function if provided\n            if cleanup_func:\n                try:\n                    if inspect.iscoroutinefunction(cleanup_func):  # Check if it's an async def function\n                        await cleanup_func()\n                    elif inspect.isasyncgenfunction(cleanup_func) or inspect.isasyncgen(\n                        cleanup_func):  # Check if it's an async def generator function or already an async generator\n                        # If it's a function, call it to get the generator\n                        gen_to_exhaust = cleanup_func() if inspect.isasyncgenfunction(cleanup_func) else cleanup_func\n                        async for _ in gen_to_exhaust:\n                            pass  # Exhaust the generator to ensure cleanup completes\n                    else:\n                        # Synchronous function\n                        cleanup_func()\n                except Exception as e:\n                    # Log cleanup errors but don't propagate them to client\n                    error_info_cleanup = {\n                        \"event\": \"cleanup_error\",\n                        \"data\": {  # Ensure payload is under 'data' key\n                            \"message\": str(e),\n                            \"traceback\": traceback.format_exc()\n                        }\n                    }\n                    # We can't yield here as the stream is already closing/closed.\n                    # Instead, log the error.\n                    # In a real app, use a proper logger.\n                    print(f\"SSE cleanup error: {cls.format_sse_event(error_info_cleanup)}\", flush=True)\n</code></pre> <code>create_sse_stream(source, cleanup_func=None)</code> <code>async</code> <code>classmethod</code> \u00b6 <p>Convert any source to a properly formatted SSE stream.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Any</code> <p>Can be async generator, sync generator, iterable, or a single item.</p> required <code>cleanup_func</code> <code>Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None]</code> <p>Optional function to call when the stream ends or is cancelled.           Can be a synchronous function, async function, or async generator.</p> <code>None</code> <p>Yields:</p> Type Description <code>AsyncGenerator[str, None]</code> <p>Properly formatted SSE messages (strings).</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\nasync def create_sse_stream(\n    cls,\n    source: Any,  # Changed from positional arg to keyword for clarity in Result.stream\n    cleanup_func: Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None\n) -&gt; AsyncGenerator[str, None]:\n    \"\"\"\n    Convert any source to a properly formatted SSE stream.\n\n    Args:\n        source: Can be async generator, sync generator, iterable, or a single item.\n        cleanup_func: Optional function to call when the stream ends or is cancelled.\n                      Can be a synchronous function, async function, or async generator.\n\n    Yields:\n        Properly formatted SSE messages (strings).\n    \"\"\"\n    # Send stream start event\n    # This structure ensures data field contains {\"id\":\"0\"}\n    yield cls.format_sse_event({\"event\": \"stream_start\", \"data\": {\"id\": \"0\"}})\n\n    try:\n        # Handle different types of sources\n        if inspect.isasyncgen(source):\n            # Source is already an async generator\n            async for item in source:\n                yield cls.format_sse_event(item)\n        elif inspect.isgenerator(source) or (not isinstance(source, str) and hasattr(source, '__iter__')):\n            # Source is a sync generator or iterable (but not a string)\n            # Strings are iterable but should be treated as single items unless explicitly made a generator\n            async for item in cls.wrap_sync_generator(source):\n                yield cls.format_sse_event(item)\n        else:\n            # Single item (including strings)\n            yield cls.format_sse_event(source)\n    except asyncio.CancelledError:\n        # Client disconnected\n        yield cls.format_sse_event({\"event\": \"cancelled\", \"data\": {\"id\": \"cancelled\"}})\n        raise\n    except Exception as e:\n        # Error in stream\n        error_info = {\n            \"event\": \"error\",\n            \"data\": {  # Ensure payload is under 'data' key for the new format_sse_event logic\n                \"message\": str(e),\n                \"traceback\": traceback.format_exc()\n            }\n        }\n        yield cls.format_sse_event(error_info)\n    finally:\n        # Always send end event\n        yield cls.format_sse_event({\"event\": \"stream_end\", \"data\": {\"id\": \"final\"}})\n\n        # Execute cleanup function if provided\n        if cleanup_func:\n            try:\n                if inspect.iscoroutinefunction(cleanup_func):  # Check if it's an async def function\n                    await cleanup_func()\n                elif inspect.isasyncgenfunction(cleanup_func) or inspect.isasyncgen(\n                    cleanup_func):  # Check if it's an async def generator function or already an async generator\n                    # If it's a function, call it to get the generator\n                    gen_to_exhaust = cleanup_func() if inspect.isasyncgenfunction(cleanup_func) else cleanup_func\n                    async for _ in gen_to_exhaust:\n                        pass  # Exhaust the generator to ensure cleanup completes\n                else:\n                    # Synchronous function\n                    cleanup_func()\n            except Exception as e:\n                # Log cleanup errors but don't propagate them to client\n                error_info_cleanup = {\n                    \"event\": \"cleanup_error\",\n                    \"data\": {  # Ensure payload is under 'data' key\n                        \"message\": str(e),\n                        \"traceback\": traceback.format_exc()\n                    }\n                }\n                # We can't yield here as the stream is already closing/closed.\n                # Instead, log the error.\n                # In a real app, use a proper logger.\n                print(f\"SSE cleanup error: {cls.format_sse_event(error_info_cleanup)}\", flush=True)\n</code></pre> <code>format_sse_event(data)</code> <code>staticmethod</code> \u00b6 <p>Format any data as a proper SSE event message.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef format_sse_event(data: Any) -&gt; str:\n    \"\"\"Format any data as a proper SSE event message.\"\"\"\n    # Already formatted as SSE\n    if isinstance(data, str) and (data.startswith('data:') or data.startswith('event:')) and '\\n\\n' in data:\n        return data\n\n    # Handle bytes (binary data)\n    if isinstance(data, bytes):\n        try:\n            # Try to decode as UTF-8 first\n            decoded_data_str = data.decode('utf-8')\n            # If decoding works, treat it as a string for further processing\n            # This allows binary data that is valid UTF-8 JSON to be processed as JSON.\n            data = decoded_data_str\n        except UnicodeDecodeError:\n            # Binary data that is not UTF-8, encode as base64\n            b64_data = base64.b64encode(data).decode('utf-8')\n            return f\"event: binary\\ndata: {b64_data}\\n\\n\"\n\n    # Convert non-string objects (that are not already bytes) to JSON string\n    # If data was bytes and successfully decoded to UTF-8 string, it will be processed here.\n    original_data_type_was_complex = False\n    if not isinstance(data, str):\n        original_data_type_was_complex = True\n        try:\n            data_str = json.dumps(data)\n        except Exception:\n            data_str = str(data)  # Fallback to string representation\n    else:\n        data_str = data  # data is already a string\n\n    # Handle JSON data with special event formatting\n    # data_str now holds the string representation (either original string or JSON string)\n    if data_str.strip().startswith('{'):\n        try:\n            json_data = json.loads(data_str)\n            if isinstance(json_data, dict) and 'event' in json_data:\n                event_type = json_data['event']\n                event_id = json_data.get('id', None)  # Use None to distinguish from empty string\n\n                # Determine the actual data payload for the SSE 'data:' field\n                # If 'data' key exists in json_data, use its content.\n                # Otherwise, use the original data_str (which is the JSON of json_data).\n                if 'data' in json_data:\n                    payload_content = json_data['data']\n                    # If payload_content is complex, re-serialize it to JSON string\n                    if isinstance(payload_content, (dict, list)):\n                        sse_data_field = json.dumps(payload_content)\n                    else:  # Simple type (string, number, bool)\n                        sse_data_field = str(payload_content)\n                else:\n                    # If original data was complex (e.g. dict) and became json_data,\n                    # and no 'data' key in it, then use the full json_data as payload.\n                    # If original data was a simple string that happened to be JSON parsable\n                    # but without 'event' key, it would have been handled by \"Regular JSON without event\"\n                    # or \"Plain text\" later.\n                    # This path implies original data was a dict with 'event' key.\n                    sse_data_field = data_str\n\n                sse_lines = []\n                if event_type:  # Should always be true here\n                    sse_lines.append(f\"event: {event_type}\")\n                if event_id is not None:  # Check for None, allow empty string id\n                    sse_lines.append(f\"id: {event_id}\")\n\n                # Handle multi-line data for the data field\n                for line in sse_data_field.splitlines():\n                    sse_lines.append(f\"data: {line}\")\n\n                return \"\\n\".join(sse_lines) + \"\\n\\n\"\n            else:\n                # Regular JSON without special 'event' key\n                sse_lines = []\n                for line in data_str.splitlines():\n                    sse_lines.append(f\"data: {line}\")\n                return \"\\n\".join(sse_lines) + \"\\n\\n\"\n        except json.JSONDecodeError:\n            # Not valid JSON, treat as plain text\n            sse_lines = []\n            for line in data_str.splitlines():\n                sse_lines.append(f\"data: {line}\")\n            return \"\\n\".join(sse_lines) + \"\\n\\n\"\n    else:\n        # Plain text\n        sse_lines = []\n        for line in data_str.splitlines():\n            sse_lines.append(f\"data: {line}\")\n        return \"\\n\".join(sse_lines) + \"\\n\\n\"\n</code></pre> <code>wrap_sync_generator(generator)</code> <code>async</code> <code>classmethod</code> \u00b6 <p>Convert a synchronous generator to an async generator.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\nasync def wrap_sync_generator(cls, generator):\n    \"\"\"Convert a synchronous generator to an async generator.\"\"\"\n    for item in generator:\n        yield item\n        # Allow other tasks to run\n        await asyncio.sleep(0)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.Session","title":"<code>Session</code>  <code>dataclass</code>","text":"<p>Class representing a session.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@dataclass\nclass Session:\n    \"\"\"Class representing a session.\"\"\"\n    SiID: str\n    level: str\n    spec: str\n    user_name: str\n    # Allow for additional fields\n    extra_data: dict[str, Any] = field(default_factory=dict)\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any]) -&gt; 'Session':\n        \"\"\"Create a Session instance from a dictionary with default values.\"\"\"\n        known_fields = {\n            'SiID': data.get('SiID', '#0'),\n            'level': data.get('level', -1),\n            'spec': data.get('spec', 'app'),\n            'user_name': data.get('user_name', 'anonymous'),\n        }\n\n        extra_data = {k: v for k, v in data.items() if k not in known_fields}\n        return cls(**known_fields, extra_data=extra_data)\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert the Session object back to a dictionary.\"\"\"\n        result = {\n            'SiID': self.SiID,\n            'level': self.level,\n            'spec': self.spec,\n            'user_name': self.user_name,\n        }\n\n        # Add extra data\n        result.update(self.extra_data)\n\n        return result\n\n    @property\n    def valid(self):\n        return int(self.level) &gt; 0\n</code></pre> <code>from_dict(data)</code> <code>classmethod</code> \u00b6 <p>Create a Session instance from a dictionary with default values.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; 'Session':\n    \"\"\"Create a Session instance from a dictionary with default values.\"\"\"\n    known_fields = {\n        'SiID': data.get('SiID', '#0'),\n        'level': data.get('level', -1),\n        'spec': data.get('spec', 'app'),\n        'user_name': data.get('user_name', 'anonymous'),\n    }\n\n    extra_data = {k: v for k, v in data.items() if k not in known_fields}\n    return cls(**known_fields, extra_data=extra_data)\n</code></pre> <code>to_dict()</code> \u00b6 <p>Convert the Session object back to a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert the Session object back to a dictionary.\"\"\"\n    result = {\n        'SiID': self.SiID,\n        'level': self.level,\n        'spec': self.spec,\n        'user_name': self.user_name,\n    }\n\n    # Add extra data\n    result.update(self.extra_data)\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.parse_request_data","title":"<code>parse_request_data(data)</code>","text":"<p>Parse the incoming request data into a strongly typed structure.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def parse_request_data(data: dict[str, Any]) -&gt; RequestData:\n    \"\"\"Parse the incoming request data into a strongly typed structure.\"\"\"\n    return RequestData.from_dict(data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox","title":"<code>toolbox</code>","text":"<p>Main module.</p>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App","title":"<code>App</code>","text":"Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>class App(AppType, metaclass=Singleton):\n\n    def __init__(self, prefix: str = \"\", args=AppArgs().default()):\n        super().__init__(prefix, args)\n        self._web_context = None\n        t0 = time.perf_counter()\n        abspath = os.path.abspath(__file__)\n        self.system_flag = system()  # Linux: Linux Mac: Darwin Windows: Windows\n\n        self.appdata = os.getenv('APPDATA') if os.name == 'nt' else os.getenv('XDG_CONFIG_HOME') or os.path.expanduser(\n                '~/.config') if os.name == 'posix' else None\n\n        if self.system_flag == \"Darwin\" or self.system_flag == \"Linux\":\n            dir_name = os.path.dirname(abspath).replace(\"/utils\", \"\")\n        else:\n            dir_name = os.path.dirname(abspath).replace(\"\\\\utils\", \"\")\n\n        self.start_dir = str(dir_name)\n\n        self.bg_tasks = []\n\n        lapp = dir_name + '\\\\.data\\\\'\n\n        if not prefix:\n            if not os.path.exists(f\"{lapp}last-app-prefix.txt\"):\n                os.makedirs(lapp, exist_ok=True)\n                open(f\"{lapp}last-app-prefix.txt\", \"a\").close()\n            with open(f\"{lapp}last-app-prefix.txt\") as prefix_file:\n                cont = prefix_file.read()\n                if cont:\n                    prefix = cont.rstrip()\n        else:\n            if not os.path.exists(f\"{lapp}last-app-prefix.txt\"):\n                os.makedirs(lapp, exist_ok=True)\n                open(f\"{lapp}last-app-prefix.txt\", \"a\").close()\n            with open(f\"{lapp}last-app-prefix.txt\", \"w\") as prefix_file:\n                prefix_file.write(prefix)\n\n        self.prefix = prefix\n\n        node_ = node()\n\n        if 'localhost' in node_ and (host := os.getenv('HOSTNAME', 'localhost')) != 'localhost':\n            node_ = node_.replace('localhost', host)\n        self.id = prefix + '-' + node_\n        self.globals = {\n            \"root\": {**globals()},\n        }\n        self.locals = {\n            \"user\": {'app': self, **locals()},\n        }\n\n        identification = self.id\n\n        if \"test\" in prefix:\n            if self.system_flag == \"Darwin\" or self.system_flag == \"Linux\":\n                start_dir = self.start_dir.replace(\"ToolBoxV2/toolboxv2\", \"toolboxv2\")\n            else:\n                start_dir = self.start_dir.replace(\"ToolBoxV2\\\\toolboxv2\", \"toolboxv2\")\n            self.data_dir = start_dir + '\\\\.data\\\\' + \"test\"\n            self.config_dir = start_dir + '\\\\.config\\\\' + \"test\"\n            self.info_dir = start_dir + '\\\\.info\\\\' + \"test\"\n        elif identification.startswith('collective-'):\n            collective_identification = identification.split('-')[1]\n            self.data_dir = self.start_dir + '\\\\.data\\\\' + collective_identification\n            self.config_dir = self.start_dir + '\\\\.config\\\\' + collective_identification\n            self.info_dir = self.start_dir + '\\\\.info\\\\' + collective_identification\n        else:\n            self.data_dir = self.start_dir + '\\\\.data\\\\' + identification\n            self.config_dir = self.start_dir + '\\\\.config\\\\' + identification\n            self.info_dir = self.start_dir + '\\\\.info\\\\' + identification\n\n        if self.appdata is None:\n            self.appdata = self.data_dir\n        else:\n            self.appdata += \"/ToolBoxV2\"\n\n        if not os.path.exists(self.appdata):\n            os.makedirs(self.appdata, exist_ok=True)\n        if not os.path.exists(self.data_dir):\n            os.makedirs(self.data_dir, exist_ok=True)\n        if not os.path.exists(self.config_dir):\n            os.makedirs(self.config_dir, exist_ok=True)\n        if not os.path.exists(self.info_dir):\n            os.makedirs(self.info_dir, exist_ok=True)\n\n        print(f\"Starting ToolBox as {prefix} from :\", Style.Bold(Style.CYAN(f\"{os.getcwd()}\")))\n\n        logger_info_str, self.logger, self.logging_filename = self.set_logger(args.debug)\n\n        print(\"Logger \" + logger_info_str)\n        print(\"================================\")\n        self.logger.info(\"Logger initialized\")\n        get_logger().info(Style.GREEN(\"Starting Application instance\"))\n        if args.init and args.init is not None and self.start_dir not in sys.path:\n            sys.path.append(self.start_dir)\n\n\n        __version__ = get_version_from_pyproject()\n\n        self.version = __version__\n\n        self.keys = {\n            \"MACRO\": \"macro~~~~:\",\n            \"MACRO_C\": \"m_color~~:\",\n            \"HELPER\": \"helper~~~:\",\n            \"debug\": \"debug~~~~:\",\n            \"id\": \"name-spa~:\",\n            \"st-load\": \"mute~load:\",\n            \"comm-his\": \"comm-his~:\",\n            \"develop-mode\": \"dev~mode~:\",\n            \"provider::\": \"provider::\",\n        }\n\n        defaults = {\n            \"MACRO\": ['Exit'],\n            \"MACRO_C\": {},\n            \"HELPER\": {},\n            \"debug\": args.debug,\n            \"id\": self.id,\n            \"st-load\": False,\n            \"comm-his\": [[]],\n            \"develop-mode\": False,\n        }\n        self.config_fh = FileHandler(self.id + \".config\", keys=self.keys, defaults=defaults)\n        self.config_fh.load_file_handler()\n        self._debug = args.debug\n        self.flows = {}\n        self.dev_modi = self.config_fh.get_file_handler(self.keys[\"develop-mode\"])\n        if self.config_fh.get_file_handler(\"provider::\") is None:\n            self.config_fh.add_to_save_file_handler(\"provider::\", \"http://localhost:\" + str(\n                self.args_sto.port) if os.environ.get(\"HOSTNAME\",\n                                                                     \"localhost\") == \"localhost\" else \"https://simplecore.app\")\n        self.functions = {}\n        self.modules = {}\n\n        self.interface_type = ToolBoxInterfaces.native\n        self.PREFIX = Style.CYAN(f\"~{node()}@&gt;\")\n        self.alive = True\n        self.called_exit = False, time.time()\n\n        self.print(f\"Infos:\\n  {'Name':&lt;8} -&gt; {node()}\\n  {'ID':&lt;8} -&gt; {self.id}\\n  {'Version':&lt;8} -&gt; {self.version}\\n\")\n\n        self.logger.info(\n            Style.GREEN(\n                f\"Finish init up in {time.perf_counter() - t0:.2f}s\"\n            )\n        )\n\n        self.args_sto = args\n        self.loop = None\n\n        from .system.session import Session\n        self.session: Session = Session(self.get_username())\n        if len(sys.argv) &gt; 2 and \"db\" == sys.argv[1]:\n            return\n        from .system.db_cli_manager import ClusterManager, get_executable_path\n        self.cluster_manager = ClusterManager()\n        online_list, server_list = self.cluster_manager.status_all(silent=True)\n        if not server_list:\n            self.cluster_manager.start_all(get_executable_path(), self.version)\n            _, server_list = self.cluster_manager.status_all()\n        from .extras.blobs import BlobStorage\n        self.root_blob_storage = BlobStorage(servers=server_list, storage_directory=self.data_dir+ '\\\\blob_cache\\\\')\n\n    def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n        user_name = self.config_fh.get_file_handler(\"ac_user:::\")\n        if get_input and user_name is None:\n            user_name = input(\"Input your username: \")\n            self.config_fh.add_to_save_file_handler(\"ac_user:::\", user_name)\n        if user_name is None:\n            user_name = default\n            self.config_fh.add_to_save_file_handler(\"ac_user:::\", user_name)\n        return user_name\n\n    def set_username(self, username):\n        return self.config_fh.add_to_save_file_handler(\"ac_user:::\", username)\n\n    @staticmethod\n    def exit_main(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def hide_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def show_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def disconnect(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def set_logger(self, debug=False):\n        if \"test\" in self.prefix and not debug:\n            logger, logging_filename = setup_logging(logging.NOTSET, name=\"toolbox-test\", interminal=True,\n                                                     file_level=logging.NOTSET, app_name=self.id)\n            logger_info_str = \"in Test Mode\"\n        elif \"live\" in self.prefix and not debug:\n            logger, logging_filename = setup_logging(logging.DEBUG, name=\"toolbox-live\", interminal=False,\n                                                     file_level=logging.WARNING, app_name=self.id)\n            logger_info_str = \"in Live Mode\"\n            # setup_logging(logging.WARNING, name=\"toolbox-live\", is_online=True\n            #              , online_level=logging.WARNING).info(\"Logger initialized\")\n        elif \"debug\" in self.prefix or self.prefix.endswith(\"D\"):\n            self.prefix = self.prefix.replace(\"-debug\", '').replace(\"debug\", '')\n            logger, logging_filename = setup_logging(logging.DEBUG, name=\"toolbox-debug\", interminal=True,\n                                                     file_level=logging.WARNING, app_name=self.id)\n            logger_info_str = \"in debug Mode\"\n            self.debug = True\n        elif debug:\n            logger, logging_filename = setup_logging(logging.DEBUG, name=f\"toolbox-{self.prefix}-debug\",\n                                                     interminal=True,\n                                                     file_level=logging.DEBUG, app_name=self.id)\n            logger_info_str = \"in args debug Mode\"\n        else:\n            logger, logging_filename = setup_logging(logging.ERROR, name=f\"toolbox-{self.prefix}\", app_name=self.id)\n            logger_info_str = \"in Default\"\n\n        return logger_info_str, logger, logging_filename\n\n    @property\n    def debug(self):\n        return self._debug\n\n    @debug.setter\n    def debug(self, value):\n        if not isinstance(value, bool):\n            self.logger.debug(f\"Value must be an boolean. is : {value} type of {type(value)}\")\n            raise ValueError(\"Value must be an boolean.\")\n\n        # self.logger.info(f\"Setting debug {value}\")\n        self._debug = value\n\n    def debug_rains(self, e):\n        if self.debug:\n            import traceback\n            x = \"=\"*5\n            x += \" DEBUG \"\n            x += \"=\"*5\n            self.print(x)\n            self.print(traceback.format_exc())\n            self.print(x)\n            raise e\n        else:\n            self.logger.error(f\"Error: {e}\")\n            import traceback\n            x = \"=\"*5\n            x += \" DEBUG \"\n            x += \"=\"*5\n            self.print(x)\n            self.print(traceback.format_exc())\n            self.print(x)\n\n    def set_flows(self, r):\n        self.flows = r\n\n    async def run_flows(self, name, **kwargs):\n        from ..flows import flows_dict as flows_dict_func\n        if name not in self.flows:\n            self.flows = {**self.flows, **flows_dict_func(s=name, remote=True)}\n        if name in self.flows:\n            if asyncio.iscoroutinefunction(self.flows[name]):\n                return await self.flows[name](get_app(from_=\"runner\"), self.args_sto, **kwargs)\n            else:\n                return self.flows[name](get_app(from_=\"runner\"), self.args_sto, **kwargs)\n        else:\n            print(\"Flow not found, active flows:\", len(self.flows.keys()))\n\n    def _coppy_mod(self, content, new_mod_dir, mod_name, file_type='py'):\n\n        mode = 'xb'\n        self.logger.info(f\" coppy mod {mod_name} to {new_mod_dir} size : {sys.getsizeof(content) / 8388608:.3f} mb\")\n\n        if not os.path.exists(new_mod_dir):\n            os.makedirs(new_mod_dir)\n            with open(f\"{new_mod_dir}/__init__.py\", \"w\") as nmd:\n                nmd.write(f\"__version__ = '{self.version}'\")\n\n        if os.path.exists(f\"{new_mod_dir}/{mod_name}.{file_type}\"):\n            mode = False\n\n            with open(f\"{new_mod_dir}/{mod_name}.{file_type}\", 'rb') as d:\n                runtime_mod = d.read()  # Testing version but not efficient\n\n            if len(content) != len(runtime_mod):\n                mode = 'wb'\n\n        if mode:\n            with open(f\"{new_mod_dir}/{mod_name}.{file_type}\", mode) as f:\n                f.write(content)\n\n    def _pre_lib_mod(self, mod_name, path_to=\"./runtime\", file_type='py'):\n        working_dir = self.id.replace(\".\", \"_\")\n        lib_mod_dir = f\"toolboxv2.runtime.{working_dir}.mod_lib.\"\n\n        self.logger.info(f\"pre_lib_mod {mod_name} from {lib_mod_dir}\")\n\n        postfix = \"_dev\" if self.dev_modi else \"\"\n        mod_file_dir = f\"./mods{postfix}/{mod_name}.{file_type}\"\n        new_mod_dir = f\"{path_to}/{working_dir}/mod_lib\"\n        with open(mod_file_dir, \"rb\") as c:\n            content = c.read()\n        self._coppy_mod(content, new_mod_dir, mod_name, file_type=file_type)\n        return lib_mod_dir\n\n    def _copy_load(self, mod_name, file_type='py', **kwargs):\n        loc = self._pre_lib_mod(mod_name, file_type)\n        return self.inplace_load_instance(mod_name, loc=loc, **kwargs)\n\n    def helper_install_pip_module(self, module_name):\n        if 'main' in self.id:\n            return\n        self.print(f\"Installing {module_name} GREEDY\")\n        os.system(f\"{sys.executable} -m pip install {module_name}\")\n\n    def python_module_import_classifier(self, mod_name, error_message):\n\n        if error_message.startswith(\"No module named 'toolboxv2.utils\"):\n            return Result.default_internal_error(f\"404 {error_message.split('utils')[1]} not found\")\n        if error_message.startswith(\"No module named 'toolboxv2.mods\"):\n            if mod_name.startswith('.'):\n                return\n            return self.run_a_from_sync(self.a_run_any, (\"CloudM\", \"install\"), module_name=mod_name)\n        if error_message.startswith(\"No module named '\"):\n            pip_requ = error_message.split(\"'\")[1].replace(\"'\", \"\").strip()\n            # if 'y' in input(f\"\\t\\t\\tAuto install {pip_requ} Y/n\").lower:\n            return self.helper_install_pip_module(pip_requ)\n            # return Result.default_internal_error(f\"404 {pip_requ} not found\")\n\n    def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True, mfo=None):\n        if self.dev_modi and loc == \"toolboxv2.mods.\":\n            loc = \"toolboxv2.mods_dev.\"\n        if spec=='app' and self.mod_online(mod_name):\n            self.logger.info(f\"Reloading mod from : {loc + mod_name}\")\n            self.remove_mod(mod_name, spec=spec, delete=False)\n\n        if (os.path.exists(self.start_dir + '/mods/' + mod_name) or os.path.exists(\n            self.start_dir + '/mods/' + mod_name + '.py')) and (\n            os.path.isdir(self.start_dir + '/mods/' + mod_name) or os.path.isfile(\n            self.start_dir + '/mods/' + mod_name + '.py')):\n            try:\n                if mfo is None:\n                    modular_file_object = import_module(loc + mod_name)\n                else:\n                    modular_file_object = mfo\n                self.modules[mod_name] = modular_file_object\n            except ModuleNotFoundError as e:\n                self.logger.error(Style.RED(f\"module {loc + mod_name} not found is type sensitive {e}\"))\n                self.print(Style.RED(f\"module {loc + mod_name} not found is type sensitive {e}\"))\n                if self.debug or self.args_sto.sysPrint:\n                    self.python_module_import_classifier(mod_name, str(e))\n                self.debug_rains(e)\n                return None\n        else:\n            self.print(f\"module {loc + mod_name} is not valid\")\n            return None\n        if hasattr(modular_file_object, \"Tools\"):\n            tools_class = modular_file_object.Tools\n        else:\n            if hasattr(modular_file_object, \"name\"):\n                tools_class = modular_file_object\n                modular_file_object = import_module(loc + mod_name)\n            else:\n                tools_class = None\n\n        modular_id = None\n        instance = modular_file_object\n        app_instance_type = \"file/application\"\n\n        if tools_class is None:\n            modular_id = modular_file_object.Name if hasattr(modular_file_object, \"Name\") else mod_name\n\n        if tools_class is None and modular_id is None:\n            modular_id = str(modular_file_object.__name__)\n            self.logger.warning(f\"Unknown instance loaded {mod_name}\")\n            return modular_file_object\n\n        if tools_class is not None:\n            tools_class = self.save_initialized_module(tools_class, spec)\n            modular_id = tools_class.name\n            app_instance_type = \"functions/class\"\n        else:\n            instance.spec = spec\n        # if private:\n        #     self.functions[modular_id][f\"{spec}_private\"] = private\n\n        if not save:\n            return instance if tools_class is None else tools_class\n\n        return self.save_instance(instance, modular_id, spec, app_instance_type, tools_class=tools_class)\n\n    def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n\n        if modular_id in self.functions and tools_class is None:\n            if self.functions[modular_id].get(f\"{spec}_instance\", None) is None:\n                self.functions[modular_id][f\"{spec}_instance\"] = instance\n                self.functions[modular_id][f\"{spec}_instance_type\"] = instance_type\n            else:\n                self.print(\"ERROR OVERRIDE\")\n                raise ImportError(f\"Module already known {modular_id}\")\n\n        elif tools_class is not None:\n            if modular_id not in self.functions:\n                self.functions[modular_id] = {}\n            self.functions[modular_id][f\"{spec}_instance\"] = tools_class\n            self.functions[modular_id][f\"{spec}_instance_type\"] = instance_type\n\n            try:\n                if not hasattr(tools_class, 'tools'):\n                    tools_class.tools = {\"Version\": tools_class.get_version, 'name': tools_class.name}\n                for function_name in list(tools_class.tools.keys()):\n                    t_function_name = function_name.lower()\n                    if t_function_name != \"all\" and t_function_name != \"name\":\n                        self.tb(function_name, mod_name=modular_id)(tools_class.tools.get(function_name))\n                self.functions[modular_id][f\"{spec}_instance_type\"] += \"/BC\"\n                if hasattr(tools_class, 'on_exit'):\n                    if \"on_exit\" in self.functions[modular_id]:\n                        self.functions[modular_id][\"on_exit\"].append(tools_class.on_exit)\n                    else:\n                        self.functions[modular_id][\"on_exit\"] = [tools_class.on_exit]\n            except Exception as e:\n                self.logger.error(f\"Starting Module {modular_id} compatibility failed with : {e}\")\n                pass\n        elif modular_id not in self.functions and tools_class is None:\n            self.functions[modular_id] = {}\n            self.functions[modular_id][f\"{spec}_instance\"] = instance\n            self.functions[modular_id][f\"{spec}_instance_type\"] = instance_type\n\n        else:\n            raise ImportError(f\"Modular {modular_id} is not a valid mod\")\n        on_start = self.functions[modular_id].get(\"on_start\")\n        if on_start is not None:\n            i = 1\n            for f in on_start:\n                try:\n                    f_, e = self.get_function((modular_id, f), state=True, specification=spec)\n                    if e == 0:\n                        self.logger.info(Style.GREY(f\"Running On start {f} {i}/{len(on_start)}\"))\n                        if asyncio.iscoroutinefunction(f_):\n                            self.print(f\"Async on start is only in Tool claas supported for {modular_id}.{f}\" if tools_class is None else f\"initialization starting soon for {modular_id}.{f}\")\n                            self.run_bg_task_advanced(f_)\n                        else:\n                            o = f_()\n                            if o is not None:\n                                self.print(f\"Function {modular_id} On start result: {o}\")\n                    else:\n                        self.logger.warning(f\"starting function not found {e}\")\n                except Exception as e:\n                    self.logger.debug(Style.YELLOW(\n                        Style.Bold(f\"modular:{modular_id}.{f} on_start error {i}/{len(on_start)} -&gt; {e}\")))\n                    self.debug_rains(e)\n                finally:\n                    i += 1\n        return instance if tools_class is None else tools_class\n\n    def save_initialized_module(self, tools_class, spec):\n        tools_class.spec = spec\n        live_tools_class = tools_class(app=self)\n        return live_tools_class\n\n    def mod_online(self, mod_name, installed=False):\n        if installed and mod_name not in self.functions:\n            self.save_load(mod_name)\n        return mod_name in self.functions\n\n    def _get_function(self,\n                      name: Enum or None,\n                      state: bool = True,\n                      specification: str = \"app\",\n                      metadata=False, as_str: tuple or None = None, r=0, **kwargs):\n\n        if as_str is None and isinstance(name, Enum):\n            modular_id = str(name.NAME.value)\n            function_id = str(name.value)\n        elif as_str is None and isinstance(name, list):\n            modular_id, function_id = name[0], name[1]\n        else:\n            modular_id, function_id = as_str\n\n        self.logger.info(f\"getting function : {specification}.{modular_id}.{function_id}\")\n\n        if modular_id not in self.functions:\n            if r == 0:\n                self.save_load(modular_id, spec=specification)\n                return self.get_function(name=(modular_id, function_id),\n                                         state=state,\n                                         specification=specification,\n                                         metadata=metadata,\n                                         r=1)\n            self.logger.warning(f\"function modular not found {modular_id} 404\")\n            return \"404\", 404\n\n        if function_id not in self.functions[modular_id]:\n            self.logger.warning(f\"function data not found {modular_id}.{function_id} 404\")\n            return \"404\", 404\n\n        function_data = self.functions[modular_id][function_id]\n\n        if isinstance(function_data, list):\n            print(f\"functions {function_id} : {function_data}\")\n            function_data = self.functions[modular_id][function_data[kwargs.get('i', -1)]]\n            print(f\"functions {modular_id} : {function_data}\")\n        function = function_data.get(\"func\")\n        params = function_data.get(\"params\")\n\n        state_ = function_data.get(\"state\")\n        if state_ is not None and state != state_:\n            state = state_\n\n        if function is None:\n            self.logger.warning(\"No function found\")\n            return \"404\", 404\n\n        if params is None:\n            self.logger.warning(\"No function (params) found\")\n            return \"404\", 301\n\n        if metadata and not state:\n            self.logger.info(\"returning metadata stateless\")\n            return (function_data, function), 0\n\n        if not state:  # mens a stateless function\n            self.logger.info(\"returning stateless function\")\n            return function, 0\n\n        instance = self.functions[modular_id].get(f\"{specification}_instance\")\n\n        # instance_type = self.functions[modular_id].get(f\"{specification}_instance_type\", \"functions/class\")\n\n        if params[0] == 'app':\n            instance = get_app(from_=f\"fuction {specification}.{modular_id}.{function_id}\")\n\n        if instance is None and self.alive:\n            self.inplace_load_instance(modular_id, spec=specification)\n            instance = self.functions[modular_id].get(f\"{specification}_instance\")\n\n        if instance is None:\n            self.logger.warning(\"No live Instance found\")\n            return \"404\", 400\n\n        # if instance_type.endswith(\"/BC\"):  # for backwards compatibility  functions/class/BC old modules\n        #     # returning as stateless\n        #     # return \"422\", -1\n        #     self.logger.info(\n        #         f\"returning stateless function, cant find tools class for state handling found {instance_type}\")\n        #     if metadata:\n        #         self.logger.info(f\"returning metadata stateless\")\n        #         return (function_data, function), 0\n        #     return function, 0\n\n        self.logger.info(\"wrapping in higher_order_function\")\n\n        self.logger.info(f\"returned fuction {specification}.{modular_id}.{function_id}\")\n        higher_order_function = partial(function, instance)\n\n        if metadata:\n            self.logger.info(\"returning metadata stateful\")\n            return (function_data, higher_order_function), 0\n\n        self.logger.info(\"returning stateful function\")\n        return higher_order_function, 0\n\n    def save_exit(self):\n        self.logger.info(f\"save exiting saving data to {self.config_fh.file_handler_filename} states of {self.debug=}\")\n        self.config_fh.add_to_save_file_handler(self.keys[\"debug\"], str(self.debug))\n\n    def init_mod(self, mod_name, spec='app'):\n        if '.' in mod_name:\n            mod_name = mod_name.split('.')[0]\n        return self.loop_gard().run_until_complete(self.a_init_mod(mod_name, spec))\n\n    def run_bg_task(self, task: Callable, *args, **kwargs) -&gt; Optional[asyncio.Task]:\n        \"\"\"\n        Runs a coroutine in the background without blocking the caller.\n\n        This is the primary method for \"fire-and-forget\" async tasks. It schedules\n        the coroutine to run on the application's main event loop.\n\n        Args:\n            task: The coroutine function to run.\n            *args: Arguments to pass to the coroutine function.\n            **kwargs: Keyword arguments to pass to the coroutine function.\n\n        Returns:\n            An asyncio.Task object representing the scheduled task, or None if\n            the task could not be scheduled.\n        \"\"\"\n        if not callable(task):\n            self.logger.warning(\"Task passed to run_bg_task is not callable!\")\n            return None\n\n        if not asyncio.iscoroutinefunction(task) and not asyncio.iscoroutine(task):\n            self.logger.warning(f\"Task '{getattr(task, '__name__', 'unknown')}' is not a coroutine. \"\n                                f\"Use run_bg_task_advanced for synchronous functions.\")\n            # Fallback to advanced runner for convenience\n            self.run_bg_task_advanced(task, *args, **kwargs)\n            return None\n\n        try:\n            loop = self.loop_gard()\n            if not loop.is_running():\n                # If the main loop isn't running, we can't create a task on it.\n                # This scenario is handled by run_bg_task_advanced.\n                self.logger.info(\"Main event loop not running. Delegating to advanced background runner.\")\n                return self.run_bg_task_advanced(task, *args, **kwargs)\n\n            # Create the coroutine if it's a function\n            coro = task(*args, **kwargs) if asyncio.iscoroutinefunction(task) else task\n\n            # Create a task on the running event loop\n            bg_task = loop.create_task(coro)\n\n            # Add a callback to log exceptions from the background task\n            def _log_exception(the_task: asyncio.Task):\n                if not the_task.cancelled() and the_task.exception():\n                    self.logger.error(f\"Exception in background task '{the_task.get_name()}':\",\n                                      exc_info=the_task.exception())\n\n            bg_task.add_done_callback(_log_exception)\n            self.bg_tasks.append(bg_task)\n            return bg_task\n\n        except Exception as e:\n            self.logger.error(f\"Failed to schedule background task: {e}\", exc_info=True)\n            return None\n\n    def run_bg_task_advanced(self, task: Callable, *args, **kwargs) -&gt; threading.Thread:\n        \"\"\"\n        Runs a task in a separate, dedicated background thread with its own event loop.\n\n        This is ideal for:\n        1. Running an async task from a synchronous context.\n        2. Launching a long-running, independent operation that should not\n           interfere with the main application's event loop.\n\n        Args:\n            task: The function to run (can be sync or async).\n            *args: Arguments for the task.\n            **kwargs: Keyword arguments for the task.\n\n        Returns:\n            The threading.Thread object managing the background execution.\n        \"\"\"\n        if not callable(task):\n            self.logger.warning(\"Task for run_bg_task_advanced is not callable!\")\n            return None\n\n        def thread_target():\n            # Each thread gets its own event loop.\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n\n            try:\n                # Prepare the coroutine we need to run\n                if asyncio.iscoroutinefunction(task):\n                    coro = task(*args, **kwargs)\n                elif asyncio.iscoroutine(task):\n                    # It's already a coroutine object\n                    coro = task\n                else:\n                    # It's a synchronous function, run it in an executor\n                    # to avoid blocking the new event loop.\n                    coro = loop.run_in_executor(None, lambda: task(*args, **kwargs))\n\n                # Run the coroutine to completion\n                result = loop.run_until_complete(coro)\n                self.logger.debug(f\"Advanced background task '{getattr(task, '__name__', 'unknown')}' completed.\")\n                if result is not None:\n                    self.logger.debug(f\"Task result: {str(result)[:100]}\")\n\n            except Exception as e:\n                self.logger.error(f\"Error in advanced background task '{getattr(task, '__name__', 'unknown')}':\",\n                                  exc_info=e)\n            finally:\n                # Cleanly shut down the event loop in this thread.\n                try:\n                    all_tasks = asyncio.all_tasks(loop=loop)\n                    if all_tasks:\n                        for t in all_tasks:\n                            t.cancel()\n                        loop.run_until_complete(asyncio.gather(*all_tasks, return_exceptions=True))\n                finally:\n                    loop.close()\n                    asyncio.set_event_loop(None)\n\n        # Create, start, and return the thread.\n        # It's a daemon thread so it won't prevent the main app from exiting.\n        t = threading.Thread(target=thread_target, daemon=True, name=f\"BGTask-{getattr(task, '__name__', 'unknown')}\")\n        self.bg_tasks.append(t)\n        t.start()\n        return t\n\n    # Helper method to wait for background tasks to complete (optional)\n    def wait_for_bg_tasks(self, timeout=None):\n        \"\"\"\n        Wait for all background tasks to complete.\n\n        Args:\n            timeout: Maximum time to wait (in seconds) for all tasks to complete.\n                     None means wait indefinitely.\n\n        Returns:\n            bool: True if all tasks completed, False if timeout occurred\n        \"\"\"\n        active_tasks = [t for t in self.bg_tasks if t.is_alive()]\n\n        for task in active_tasks:\n            task.join(timeout=timeout)\n            if task.is_alive():\n                return False\n\n        return True\n\n    def __call__(self, *args, **kwargs):\n        return self.run(*args, **kwargs)\n\n    def run(self, *args, request=None, running_function_coro=None, **kwargs):\n        \"\"\"\n        Run a function with support for SSE streaming in both\n        threaded and non-threaded contexts.\n        \"\"\"\n        if running_function_coro is None:\n            mn, fn = args[0]\n            if self.functions.get(mn, {}).get(fn, {}).get('request_as_kwarg', False):\n                kwargs[\"request\"] = RequestData.from_dict(request)\n                if 'data' in kwargs and 'data' not in self.functions.get(mn, {}).get(fn, {}).get('params', []):\n                    kwargs[\"request\"].data = kwargs[\"request\"].body = kwargs['data']\n                    del kwargs['data']\n                if 'form_data' in kwargs and 'form_data' not in self.functions.get(mn, {}).get(fn, {}).get('params',\n                                                                                                           []):\n                    kwargs[\"request\"].form_data = kwargs[\"request\"].body = kwargs['form_data']\n                    del kwargs['form_data']\n\n        # Create the coroutine\n        coro = running_function_coro or self.a_run_any(*args, **kwargs)\n\n        # Get or create an event loop\n        try:\n            loop = asyncio.get_event_loop()\n            is_running = loop.is_running()\n        except RuntimeError:\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n            is_running = False\n\n        # If the loop is already running, run in a separate thread\n        if is_running:\n            # Create thread pool executor as needed\n            if not hasattr(self.__class__, '_executor'):\n                self.__class__._executor = ThreadPoolExecutor(max_workers=4)\n\n            def run_in_new_thread():\n                # Set up a new loop in this thread\n                new_loop = asyncio.new_event_loop()\n                asyncio.set_event_loop(new_loop)\n\n                try:\n                    # Run the coroutine\n                    return new_loop.run_until_complete(coro)\n                finally:\n                    new_loop.close()\n\n            # Run in thread and get result\n            thread_result = self.__class__._executor.submit(run_in_new_thread).result()\n\n            # Handle streaming results from thread\n            if isinstance(thread_result, dict) and thread_result.get(\"is_stream\"):\n                # Create a new SSE stream in the main thread\n                async def stream_from_function():\n                    # Re-run the function with direct async access\n                    stream_result = await self.a_run_any(*args, **kwargs)\n\n                    if (isinstance(stream_result, Result) and\n                        getattr(stream_result.result, 'data_type', None) == \"stream\"):\n                        # Get and forward data from the original generator\n                        original_gen = stream_result.result.data.get(\"generator\")\n                        if inspect.isasyncgen(original_gen):\n                            async for item in original_gen:\n                                yield item\n\n                # Return a new streaming Result\n                return Result.stream(\n                    stream_generator=stream_from_function(),\n                    headers=thread_result.get(\"headers\", {})\n                )\n\n            result = thread_result\n        else:\n            # Direct execution when loop is not running\n            result = loop.run_until_complete(coro)\n\n        # Process the final result\n        if isinstance(result, Result):\n            if 'debug' in self.id:\n                result.print()\n            if getattr(result.result, 'data_type', None) == \"stream\":\n                return result\n            return result.to_api_result().model_dump(mode='json')\n\n        return result\n\n    def loop_gard(self):\n        if self.loop is None:\n            self.loop = asyncio.get_event_loop()\n        if self.loop.is_closed():\n            self.loop = asyncio.get_event_loop()\n        return self.loop\n\n    async def a_init_mod(self, mod_name, spec='app'):\n        mod = self.save_load(mod_name, spec=spec)\n        if hasattr(mod, \"__initobj\") and not mod.async_initialized:\n            await mod\n        return mod\n\n\n    def load_mod(self, mod_name: str, mlm='I', **kwargs):\n\n        action_list_helper = ['I (inplace load dill on error python)',\n                              # 'C (coppy py file to runtime dir)',\n                              # 'S (save py file to dill)',\n                              # 'CS (coppy and save py file)',\n                              # 'D (development mode, inplace load py file)'\n                              ]\n        action_list = {\"I\": lambda: self.inplace_load_instance(mod_name, **kwargs),\n                       \"C\": lambda: self._copy_load(mod_name, **kwargs)\n                       }\n\n        try:\n            if mlm in action_list:\n\n                return action_list.get(mlm)()\n            else:\n                self.logger.critical(\n                    f\"config mlm must be {' or '.join(action_list_helper)} is {mlm=}\")\n                raise ValueError(f\"config mlm must be {' or '.join(action_list_helper)} is {mlm=}\")\n        except ValueError as e:\n            self.logger.warning(Style.YELLOW(f\"Error Loading Module '{mod_name}', with error :{e}\"))\n            self.debug_rains(e)\n        except ImportError as e:\n            self.logger.error(Style.YELLOW(f\"Error Loading Module '{mod_name}', with error :{e}\"))\n            self.debug_rains(e)\n        except Exception as e:\n            self.logger.critical(Style.RED(f\"Error Loading Module '{mod_name}', with critical error :{e}\"))\n            print(Style.RED(f\"Error Loading Module '{mod_name}'\"))\n            self.debug_rains(e)\n\n        return Result.default_internal_error(info=\"info's in logs.\")\n\n    async def load_all_mods_in_file(self, working_dir=\"mods\"):\n        print(f\"LOADING ALL MODS FROM FOLDER : {working_dir}\")\n        t0 = time.perf_counter()\n        # Get the list of all modules\n        module_list = self.get_all_mods(working_dir)\n        open_modules = self.functions.keys()\n        start_len = len(open_modules)\n\n        for om in open_modules:\n            if om in module_list:\n                module_list.remove(om)\n\n        tasks: set[Task] = set()\n\n        _ = {tasks.add(asyncio.create_task(asyncio.to_thread(self.save_load, mod, 'app'))) for mod in module_list}\n        for t in asyncio.as_completed(tasks):\n            try:\n                result = await t\n                if hasattr(result, 'Name'):\n                    print('Opened :', result.Name)\n                elif hasattr(result, 'name'):\n                    if hasattr(result, 'async_initialized'):\n                        if not result.async_initialized:\n                            async def _():\n                                try:\n                                    if asyncio.iscoroutine(result):\n                                        await result\n                                    if hasattr(result, 'Name'):\n                                        print('Opened :', result.Name)\n                                    elif hasattr(result, 'name'):\n                                        print('Opened :', result.name)\n                                except Exception as e:\n                                    self.debug_rains(e)\n                                    if hasattr(result, 'Name'):\n                                        print('Error opening :', result.Name)\n                                    elif hasattr(result, 'name'):\n                                        print('Error opening :', result.name)\n                            asyncio.create_task(_())\n                        else:\n                            print('Opened :', result.name)\n                else:\n                    print('Opened :', result)\n            except Exception as e:\n                self.logger.error(Style.RED(f\"An Error occurred while opening all modules error: {str(e)}\"))\n                self.debug_rains(e)\n        opened = len(self.functions.keys()) - start_len\n\n        self.logger.info(f\"Opened {opened} modules in {time.perf_counter() - t0:.2f}s\")\n        return f\"Opened {opened} modules in {time.perf_counter() - t0:.2f}s\"\n\n    def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\", use_wd=True):\n        self.logger.info(f\"collating all mods in working directory {working_dir}\")\n\n        pr = \"_dev\" if self.dev_modi else \"\"\n        if working_dir == \"mods\" and use_wd:\n            working_dir = f\"{self.start_dir}/mods{pr}\"\n        elif use_wd:\n            pass\n        else:\n            w_dir = self.id.replace(\".\", \"_\")\n            working_dir = f\"{path_to}/{w_dir}/mod_lib{pr}/\"\n        res = os.listdir(working_dir)\n\n        self.logger.info(f\"found : {len(res)} files\")\n\n        def do_helper(_mod):\n            if \"mainTool\" in _mod:\n                return False\n            # if not _mod.endswith(\".py\"):\n            #     return False\n            if _mod.startswith(\"__\"):\n                return False\n            if _mod.startswith(\".\"):\n                return False\n            return not _mod.startswith(\"test_\")\n\n        def r_endings(word: str):\n            if word.endswith(\".py\"):\n                return word[:-3]\n            return word\n\n        mods_list = list(map(r_endings, filter(do_helper, res)))\n\n        self.logger.info(f\"found : {len(mods_list)} Modules\")\n        return mods_list\n\n    def remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            self.remove_mod(mod, delete=delete)\n\n    def remove_mod(self, mod_name, spec='app', delete=True):\n        if mod_name not in self.functions:\n            self.logger.info(f\"mod not active {mod_name}\")\n            return\n\n        on_exit = self.functions[mod_name].get(\"on_exit\")\n        self.logger.info(f\"closing: {on_exit}\")\n        def helper():\n            if f\"{spec}_instance\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance\"]\n            if f\"{spec}_instance_type\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance_type\"]\n\n        if on_exit is None and self.functions[mod_name].get(f\"{spec}_instance_type\", \"\").endswith(\"/BC\"):\n            instance = self.functions[mod_name].get(f\"{spec}_instance\", None)\n            if instance is not None and hasattr(instance, 'on_exit'):\n                if asyncio.iscoroutinefunction(instance.on_exit):\n                    self.exit_tasks.append(instance.on_exit)\n                else:\n                    instance.on_exit()\n\n        if on_exit is None and delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n            return\n        if on_exit is None:\n            helper()\n            return\n\n        i = 1\n\n        for j, f in enumerate(on_exit):\n            try:\n                f_, e = self.get_function((mod_name, f), state=True, specification=spec, i=j)\n                if e == 0:\n                    self.logger.info(Style.GREY(f\"Running On exit {f} {i}/{len(on_exit)}\"))\n                    if asyncio.iscoroutinefunction(f_):\n                        self.exit_tasks.append(f_)\n                        o = None\n                    else:\n                        o = f_()\n                    if o is not None:\n                        self.print(f\"Function On Exit result: {o}\")\n                else:\n                    self.logger.warning(\"closing function not found\")\n            except Exception as e:\n                self.logger.debug(\n                    Style.YELLOW(Style.Bold(f\"modular:{mod_name}.{f} on_exit error {i}/{len(on_exit)} -&gt; {e}\")))\n\n                self.debug_rains(e)\n            finally:\n                i += 1\n\n        helper()\n\n        if delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n\n    async def a_remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            await self.a_remove_mod(mod, delete=delete)\n\n    async def a_remove_mod(self, mod_name, spec='app', delete=True):\n        if mod_name not in self.functions:\n            self.logger.info(f\"mod not active {mod_name}\")\n            return\n        on_exit = self.functions[mod_name].get(\"on_exit\")\n        self.logger.info(f\"closing: {on_exit}\")\n        def helper():\n            if f\"{spec}_instance\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance\"]\n            if f\"{spec}_instance_type\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance_type\"]\n\n        if on_exit is None and self.functions[mod_name].get(f\"{spec}_instance_type\", \"\").endswith(\"/BC\"):\n            instance = self.functions[mod_name].get(f\"{spec}_instance\", None)\n            if instance is not None and hasattr(instance, 'on_exit'):\n                if asyncio.iscoroutinefunction(instance.on_exit):\n                    await instance.on_exit()\n                else:\n                    instance.on_exit()\n\n        if on_exit is None and delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n            return\n        if on_exit is None:\n            helper()\n            return\n\n        i = 1\n        for f in on_exit:\n            try:\n                e = 1\n                if isinstance(f, str):\n                    f_, e = self.get_function((mod_name, f), state=True, specification=spec)\n                elif isinstance(f, Callable):\n                    f_, e, f  = f, 0, f.__name__\n                if e == 0:\n                    self.logger.info(Style.GREY(f\"Running On exit {f} {i}/{len(on_exit)}\"))\n                    if asyncio.iscoroutinefunction(f_):\n                        o = await f_()\n                    else:\n                        o = f_()\n                    if o is not None:\n                        self.print(f\"Function On Exit result: {o}\")\n                else:\n                    self.logger.warning(\"closing function not found\")\n            except Exception as e:\n                self.logger.debug(\n                    Style.YELLOW(Style.Bold(f\"modular:{mod_name}.{f} on_exit error {i}/{len(on_exit)} -&gt; {e}\")))\n                self.debug_rains(e)\n            finally:\n                i += 1\n\n        helper()\n\n        if delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n\n    def exit(self, remove_all=True):\n        if not self.alive:\n            return\n        if self.args_sto.debug:\n            self.hide_console()\n        self.disconnect()\n        if remove_all:\n            self.remove_all_modules()\n        self.logger.info(\"Exiting ToolBox interface\")\n        self.alive = False\n        self.called_exit = True, time.time()\n        self.save_exit()\n        if hasattr(self, 'root_blob_storage') and self.root_blob_storage:\n            self.root_blob_storage.exit()\n        try:\n            self.config_fh.save_file_handler()\n        except SystemExit:\n            print(\"If u ar testing this is fine else ...\")\n\n        if hasattr(self, 'daemon_app'):\n            import threading\n\n            for thread in threading.enumerate()[::-1]:\n                if thread.name == \"MainThread\":\n                    continue\n                try:\n                    with Spinner(f\"closing Thread {thread.name:^50}|\", symbols=\"s\", count_down=True,\n                                 time_in_s=0.751 if not self.debug else 0.6):\n                        thread.join(timeout=0.751 if not self.debug else 0.6)\n                except TimeoutError as e:\n                    self.logger.error(f\"Timeout error on exit {thread.name} {str(e)}\")\n                    print(str(e), f\"Timeout {thread.name}\")\n                except KeyboardInterrupt:\n                    print(\"Unsave Exit\")\n                    break\n        if hasattr(self, 'loop') and self.loop is not None:\n            with Spinner(\"closing Event loop:\", symbols=\"+\"):\n                self.loop.stop()\n\n    async def a_exit(self):\n        await self.a_remove_all_modules(delete=True)\n        results = await asyncio.gather(\n            *[asyncio.create_task(f()) for f in self.exit_tasks if asyncio.iscoroutinefunction(f)])\n        for result in results:\n            self.print(f\"Function On Exit result: {result}\")\n        self.exit(remove_all=False)\n\n    def save_load(self, modname, spec='app'):\n        self.logger.debug(f\"Save load module {modname}\")\n        if not modname:\n            self.logger.warning(\"no filename specified\")\n            return False\n        try:\n            return self.load_mod(modname, spec=spec)\n        except ModuleNotFoundError as e:\n            self.logger.error(Style.RED(f\"Module {modname} not found\"))\n            self.debug_rains(e)\n\n        return False\n\n    def get_function(self, name: Enum or tuple, **kwargs):\n        \"\"\"\n        Kwargs for _get_function\n            metadata:: return the registered function dictionary\n                stateless: (function_data, None), 0\n                stateful: (function_data, higher_order_function), 0\n            state::boolean\n                specification::str default app\n        \"\"\"\n        if isinstance(name, tuple):\n            return self._get_function(None, as_str=name, **kwargs)\n        else:\n            return self._get_function(name, **kwargs)\n\n    async def a_run_function(self, mod_function_name: Enum or tuple,\n                             tb_run_function_with_state=True,\n                             tb_run_with_specification='app',\n                             args_=None,\n                             kwargs_=None,\n                             *args,\n                             **kwargs) -&gt; Result:\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n        if isinstance(mod_function_name, tuple):\n            modular_name, function_name = mod_function_name\n        elif isinstance(mod_function_name, list):\n            modular_name, function_name = mod_function_name[0], mod_function_name[1]\n        elif isinstance(mod_function_name, Enum):\n            modular_name, function_name = mod_function_name.__class__.NAME.value, mod_function_name.value\n        else:\n            raise TypeError(\"Unknown function type\")\n\n        if not self.mod_online(modular_name, installed=True):\n            self.get_mod(modular_name)\n\n        function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                      metadata=True, specification=tb_run_with_specification)\n        self.logger.info(f\"Received fuction : {mod_function_name}, with execode: {error_code}\")\n        if error_code == 404:\n            mod = self.get_mod(modular_name)\n            if hasattr(mod, \"async_initialized\") and not mod.async_initialized:\n                await mod\n            function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                          metadata=True, specification=tb_run_with_specification)\n\n        if error_code == 404:\n            self.logger.warning(Style.RED(\"Function Not Found\"))\n            return (Result.default_user_error(interface=self.interface_type,\n                                              exec_code=404,\n                                              info=\"function not found function is not decorated\").\n                    set_origin(mod_function_name))\n\n        if error_code == 300:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 info=f\"module {modular_name}\"\n                                                      f\" has no state (instance)\").set_origin(mod_function_name)\n\n        if error_code != 0:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=error_code,\n                                                 info=f\"Internal error\"\n                                                      f\" {modular_name}.\"\n                                                      f\"{function_name}\").set_origin(mod_function_name)\n\n        if not tb_run_function_with_state:\n            function_data, _ = function_data\n            function = function_data.get('func')\n        else:\n            function_data, function = function_data\n\n        if not function:\n            self.logger.warning(Style.RED(f\"Function {function_name} not found\"))\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=404,\n                                                 info=\"function not found function\").set_origin(mod_function_name)\n\n        self.logger.info(\"Profiling function\")\n        t0 = time.perf_counter()\n        if asyncio.iscoroutinefunction(function):\n            return await self.a_fuction_runner(function, function_data, args, kwargs, t0)\n        else:\n            return self.fuction_runner(function, function_data, args, kwargs, t0)\n\n    def run_function(self, mod_function_name: Enum or tuple,\n                     tb_run_function_with_state=True,\n                     tb_run_with_specification='app',\n                     args_=None,\n                     kwargs_=None,\n                     *args,\n                     **kwargs) -&gt; Result:\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n        if isinstance(mod_function_name, tuple):\n            modular_name, function_name = mod_function_name\n        elif isinstance(mod_function_name, list):\n            modular_name, function_name = mod_function_name[0], mod_function_name[1]\n        elif isinstance(mod_function_name, Enum):\n            modular_name, function_name = mod_function_name.__class__.NAME.value, mod_function_name.value\n        else:\n            raise TypeError(\"Unknown function type\")\n\n        if not self.mod_online(modular_name, installed=True):\n            self.get_mod(modular_name)\n\n        function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                      metadata=True, specification=tb_run_with_specification)\n        self.logger.info(f\"Received fuction : {mod_function_name}, with execode: {error_code}\")\n        if error_code == 1 or error_code == 3 or error_code == 400:\n            self.get_mod(modular_name)\n            function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                          metadata=True, specification=tb_run_with_specification)\n\n        if error_code == 2:\n            self.logger.warning(Style.RED(\"Function Not Found\"))\n            return (Result.default_user_error(interface=self.interface_type,\n                                              exec_code=404,\n                                              info=\"function not found function is not decorated\").\n                    set_origin(mod_function_name))\n\n        if error_code == -1:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 info=f\"module {modular_name}\"\n                                                      f\" has no state (instance)\").set_origin(mod_function_name)\n\n        if error_code != 0:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=error_code,\n                                                 info=f\"Internal error\"\n                                                      f\" {modular_name}.\"\n                                                      f\"{function_name}\").set_origin(mod_function_name)\n\n        if not tb_run_function_with_state:\n            function_data, _ = function_data\n            function = function_data.get('func')\n        else:\n            function_data, function = function_data\n\n        if not function:\n            self.logger.warning(Style.RED(f\"Function {function_name} not found\"))\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=404,\n                                                 info=\"function not found function\").set_origin(mod_function_name)\n\n        self.logger.info(\"Profiling function\")\n        t0 = time.perf_counter()\n        if asyncio.iscoroutinefunction(function):\n            raise ValueError(f\"Fuction {function_name} is Async use a_run_any\")\n        else:\n            return self.fuction_runner(function, function_data, args, kwargs, t0)\n\n    def run_a_from_sync(self, function, *args, **kwargs):\n        # Initialize self.loop if not already set.\n        if self.loop is None:\n            try:\n                self.loop = asyncio.get_running_loop()\n            except RuntimeError:\n                self.loop = asyncio.new_event_loop()\n\n        # If the loop is running, offload the coroutine to a new thread.\n        if self.loop.is_running():\n            result_future = Future()\n\n            def run_in_new_loop():\n                new_loop = asyncio.new_event_loop()\n                asyncio.set_event_loop(new_loop)\n                try:\n                    result = new_loop.run_until_complete(function(*args, **kwargs))\n                    result_future.set_result(result)\n                except Exception as e:\n                    result_future.set_exception(e)\n                finally:\n                    new_loop.close()\n\n            thread = threading.Thread(target=run_in_new_loop)\n            thread.start()\n            thread.join()  # Block until the thread completes.\n            return result_future.result()\n        else:\n            # If the loop is not running, schedule and run the coroutine directly.\n            future = self.loop.create_task(function(*args, **kwargs))\n            return self.loop.run_until_complete(future)\n\n    def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        row = function_data.get('row')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        if_self_state = 1 if 'self' in parameters else 0\n\n        try:\n            if len(parameters) == 0:\n                res = function()\n            elif len(parameters) == len(args) + if_self_state:\n                res = function(*args)\n            elif len(parameters) == len(kwargs.keys()) + if_self_state:\n                res = function(**kwargs)\n            else:\n                res = function(*args, **kwargs)\n            self.logger.info(f\"Execution done in {time.perf_counter()-t0:.4f}\")\n            if isinstance(res, Result):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.set_origin(mod_function_name)\n            elif isinstance(res, ApiResult):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.as_result().set_origin(mod_function_name).to_api_result()\n            elif row:\n                formatted_result = res\n            else:\n                # Wrap the result in a Result object\n                formatted_result = Result.ok(\n                    interface=self.interface_type,\n                    data_info=\"Auto generated result\",\n                    data=res,\n                    info=\"Function executed successfully\"\n                ).set_origin(mod_function_name)\n            if not row:\n                self.logger.info(\n                    f\"Function Exec code: {formatted_result.info.exec_code} Info's: {formatted_result.info.help_text}\")\n            else:\n                self.logger.info(\n                    f\"Function Exec data: {formatted_result}\")\n        except Exception as e:\n            self.logger.error(\n                Style.YELLOW(Style.Bold(\n                    f\"! Function ERROR: in {modular_name}.{function_name}\")))\n            # Wrap the exception in a Result object\n            formatted_result = Result.default_internal_error(info=str(e)).set_origin(mod_function_name)\n            # res = formatted_result\n            self.logger.error(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed wit an error {str(e)}, {type(e)}\")\n            self.debug_rains(e)\n            self.print(f\"! Function ERROR: in {modular_name}.{function_name} \")\n\n\n\n        else:\n            self.print_ok()\n\n            self.logger.info(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed successfully\")\n\n        return formatted_result\n\n    async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        row = function_data.get('row')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        if_self_state = 1 if 'self' in parameters else 0\n\n        try:\n            if len(parameters) == 0:\n                res = await function()\n            elif len(parameters) == len(args) + if_self_state:\n                res = await function(*args)\n            elif len(parameters) == len(kwargs.keys()) + if_self_state:\n                res = await function(**kwargs)\n            else:\n                res = await function(*args, **kwargs)\n            self.logger.info(f\"Execution done in {time.perf_counter()-t0:.4f}\")\n            if isinstance(res, Result):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.set_origin(mod_function_name)\n            elif isinstance(res, ApiResult):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.as_result().set_origin(mod_function_name).to_api_result()\n            elif row:\n                formatted_result = res\n            else:\n                # Wrap the result in a Result object\n                formatted_result = Result.ok(\n                    interface=self.interface_type,\n                    data_info=\"Auto generated result\",\n                    data=res,\n                    info=\"Function executed successfully\"\n                ).set_origin(mod_function_name)\n            if not row:\n                self.logger.info(\n                    f\"Function Exec code: {formatted_result.info.exec_code} Info's: {formatted_result.info.help_text}\")\n            else:\n                self.logger.info(\n                    f\"Function Exec data: {formatted_result}\")\n        except Exception as e:\n            self.logger.error(\n                Style.YELLOW(Style.Bold(\n                    f\"! Function ERROR: in {modular_name}.{function_name}\")))\n            # Wrap the exception in a Result object\n            formatted_result = Result.default_internal_error(info=str(e)).set_origin(mod_function_name)\n            # res = formatted_result\n            self.logger.error(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed wit an error {str(e)}, {type(e)}\")\n            self.debug_rains(e)\n\n        else:\n            self.print_ok()\n\n            self.logger.info(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed successfully\")\n\n        return formatted_result\n\n    async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None,\n                       args_=None,\n                       kwargs_=None, method=\"GET\",\n                       *args, **kwargs):\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n\n        modular_name = mod_function_name\n        function_name = function_name\n\n        if isinstance(mod_function_name, str) and isinstance(function_name, str):\n            mod_function_name = (mod_function_name, function_name)\n\n        if isinstance(mod_function_name, tuple):\n            modular_name, function_name = mod_function_name\n        elif isinstance(mod_function_name, list):\n            modular_name, function_name = mod_function_name[0], mod_function_name[1]\n        elif isinstance(mod_function_name, Enum):\n            modular_name, function_name = mod_function_name.__class__.NAME.value, mod_function_name.value\n\n        r = await self.session.fetch(f\"/api/{modular_name}/{function_name}{'?' + args_ if args_ is not None else ''}\",\n                                     data=kwargs, method=method)\n        try:\n            if not r:\n                print(\"\u00a7 Session server Offline!\", self.session.base)\n                return Result.default_internal_error(info=\"Session fetch failed\").as_dict()\n\n            content_type = r.headers.get('Content-Type', '').lower()\n            raw = await r.read()\n            encoding = r.get_encoding() or 'utf-8'\n            text = raw.decode(encoding, errors='ignore')\n\n            # Attempt JSON\n            if 'application/json' in content_type:\n                try:\n                    return await r.json()\n                except Exception as e:\n                    print(\"\u26a0 JSON decode error:\", e)\n\n            # Attempt YAML\n            if 'yaml' in content_type or text.strip().startswith('---'):\n                try:\n                    import yaml\n                    return yaml.safe_load(text)\n                except Exception as e:\n                    print(\"\u26a0 YAML decode error:\", e)\n\n            # Attempt XML\n            if 'xml' in content_type or text.strip().startswith('&lt;?xml'):\n                try:\n                    import xmltodict\n                    return xmltodict.parse(text)\n                except Exception as e:\n                    print(\"\u26a0 XML decode error:\", e)\n\n            # Fallback: return plain text\n            return Result.default_internal_error(data={'raw_text': text, 'content_type': content_type}).as_dict()\n\n        except Exception as e:\n            print(\"\u274c Fatal error during API call:\", e)\n            return Result.default_internal_error(str(e)).as_dict()\n\n    def run_local(self, *args, **kwargs):\n        return self.run_any(*args, **kwargs)\n\n    async def a_run_local(self, *args, **kwargs):\n        return await self.a_run_any(*args, **kwargs)\n\n    def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n                get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                kwargs_=None,\n                *args, **kwargs):\n\n        # if self.debug:\n        #     self.logger.info(f'Called from: {getouterframes(currentframe(), 2)}')\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n\n        if isinstance(mod_function_name, str) and backwords_compability_variabel_string_holder is None:\n            backwords_compability_variabel_string_holder = mod_function_name.split('.')[-1]\n            mod_function_name = mod_function_name.replace(f\".{backwords_compability_variabel_string_holder}\", \"\")\n\n        if isinstance(mod_function_name, str) and isinstance(backwords_compability_variabel_string_holder, str):\n            mod_function_name = (mod_function_name, backwords_compability_variabel_string_holder)\n\n        res: Result = self.run_function(mod_function_name,\n                                        tb_run_function_with_state=tb_run_function_with_state,\n                                        tb_run_with_specification=tb_run_with_specification,\n                                        args_=args, kwargs_=kwargs).as_result()\n        if isinstance(res, ApiResult):\n            res = res.as_result()\n\n        if isinstance(res, Result) and res.bg_task is not None:\n            self.run_bg_task(res.bg_task)\n\n        if self.debug:\n            res.log(show_data=False)\n\n        if not get_results and isinstance(res, Result):\n            return res.get()\n\n        if get_results and not isinstance(res, Result):\n            return Result.ok(data=res)\n\n        return res\n\n    async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                        backwords_compability_variabel_string_holder=None,\n                        get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                        kwargs_=None,\n                        *args, **kwargs):\n\n        # if self.debug:\n        #     self.logger.info(f'Called from: {getouterframes(currentframe(), 2)}')\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n\n        if isinstance(mod_function_name, str) and backwords_compability_variabel_string_holder is None:\n            backwords_compability_variabel_string_holder = mod_function_name.split('.')[-1]\n            mod_function_name = mod_function_name.replace(f\".{backwords_compability_variabel_string_holder}\", \"\")\n\n        if isinstance(mod_function_name, str) and isinstance(backwords_compability_variabel_string_holder, str):\n            mod_function_name = (mod_function_name, backwords_compability_variabel_string_holder)\n\n        res: Result = await self.a_run_function(mod_function_name,\n                                                tb_run_function_with_state=tb_run_function_with_state,\n                                                tb_run_with_specification=tb_run_with_specification,\n                                                args_=args, kwargs_=kwargs)\n        if isinstance(res, ApiResult):\n            res = res.as_result()\n\n        if isinstance(res, Result) and res.bg_task is not None:\n            self.run_bg_task(res.bg_task)\n\n        if self.debug:\n            res.print()\n            res.log(show_data=False) if isinstance(res, Result) else self.logger.debug(res)\n        if not get_results and isinstance(res, Result):\n            return res.get()\n\n        if get_results and not isinstance(res, Result):\n            return Result.ok(data=res)\n\n        return res\n\n\n    def web_context(self):\n        if self._web_context is None:\n            try:\n                self._web_context = open(\"./dist/helper.html\", encoding=\"utf-8\").read()\n            except Exception as e:\n                self.logger.error(f\"Could not load web context: {e}\")\n                self._web_context = \"&lt;div&gt;&lt;h1&gt;Web Context not found&lt;/h1&gt;&lt;/div&gt;\"\n        return self._web_context\n\n    def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n        if spec != \"app\":\n            self.print(f\"Getting Module {name} spec: {spec}\")\n        if name not in self.functions:\n            mod = self.save_load(name, spec=spec)\n            if mod is False or (isinstance(mod, Result) and mod.is_error()):\n                self.logger.warning(f\"Could not find {name} in {list(self.functions.keys())}\")\n                raise ValueError(f\"Could not find {name} in {list(self.functions.keys())} pleas install the module, or its posibly broken use --debug for infos\")\n        # private = self.functions[name].get(f\"{spec}_private\")\n        # if private is not None:\n        #     if private and spec != 'app':\n        #         raise ValueError(\"Module is private\")\n        if name not in self.functions:\n            self.logger.warning(f\"Module '{name}' is not found\")\n            return None\n        instance = self.functions[name].get(f\"{spec}_instance\")\n        if instance is None:\n            return self.load_mod(name, spec=spec)\n        return self.functions[name].get(f\"{spec}_instance\")\n\n    def print(self, text, *args, **kwargs):\n        # self.logger.info(f\"Output : {text}\")\n        if 'live' in self.id:\n            return\n        if self.sprint(None):\n            print(Style.CYAN(f\"System${self.id}:\"), end=\" \")\n        print(text, *args, **kwargs)\n\n    def sprint(self, text, *args, **kwargs):\n        if text is None:\n            return True\n        if 'live' in self.id:\n            return\n        # self.logger.info(f\"Output : {text}\")\n        print(Style.CYAN(f\"System${self.id}:\"), end=\" \")\n        if isinstance(text, str) and kwargs == {} and text:\n            stram_print(text + ' '.join(args))\n            print()\n        else:\n            print(text, *args, **kwargs)\n\n    # ----------------------------------------------------------------\n    # Decorators for the toolbox\n\n    def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n        self.remove_mod(mod_name, delete=True)\n        if mod_name not in self.modules:\n            self.logger.warning(f\"Module '{mod_name}' is not found\")\n            return\n        if hasattr(self.modules[mod_name], 'reload_save') and self.modules[mod_name].reload_save:\n            def reexecute_module_code(x):\n                return x\n        else:\n            def reexecute_module_code(module_name):\n                if isinstance(module_name, str):\n                    module = import_module(module_name)\n                else:\n                    module = module_name\n                # Get the source code of the module\n                try:\n                    source = inspect.getsource(module)\n                except Exception:\n                    # print(f\"No source for {str(module_name).split('from')[0]}: {e}\")\n                    return module\n                # Compile the source code\n                try:\n                    code = compile(source, module.__file__, 'exec')\n                    # Execute the code in the module's namespace\n                    exec(code, module.__dict__)\n                except Exception:\n                    # print(f\"No source for {str(module_name).split('from')[0]}: {e}\")\n                    pass\n                return module\n\n        if not is_file:\n            mods = self.get_all_mods(\"./mods/\" + mod_name)\n            def recursive_reload(package_name):\n                package = import_module(package_name)\n\n                # First, reload all submodules\n                if hasattr(package, '__path__'):\n                    for _finder, name, _ispkg in pkgutil.walk_packages(package.__path__, package.__name__ + \".\"):\n                        try:\n                            mod = import_module(name)\n                            reexecute_module_code(mod)\n                            reload(mod)\n                        except Exception as e:\n                            print(f\"Error reloading module {name}: {e}\")\n                            break\n\n                # Finally, reload the package itself\n                reexecute_module_code(package)\n                reload(package)\n\n            for mod in mods:\n                if mod.endswith(\".txt\") or mod.endswith(\".yaml\"):\n                    continue\n                try:\n                    recursive_reload(loc + mod_name + '.' + mod)\n                    self.print(f\"Reloaded {mod_name}.{mod}\")\n                except ImportError:\n                    self.print(f\"Could not load {mod_name}.{mod}\")\n        reexecute_module_code(self.modules[mod_name])\n        if mod_name in self.functions:\n            if \"on_exit\" in self.functions[mod_name]:\n                self.functions[mod_name][\"on_exit\"] = []\n            if \"on_start\" in self.functions[mod_name]:\n                self.functions[mod_name][\"on_start\"] = []\n        self.inplace_load_instance(mod_name, spec=spec, mfo=reload(self.modules[mod_name]) if mod_name in self.modules else None)\n\n    def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None, on_reload=None):\n        if path_name is None:\n            path_name = mod_name\n        is_file = os.path.isfile(self.start_dir + '/mods/' + path_name + '.py')\n        import watchfiles\n        def helper():\n            paths = f'mods/{path_name}' + ('.py' if is_file else '')\n            self.print(f'Watching Path: {paths}')\n            for changes in watchfiles.watch(paths):\n                if not changes:\n                    continue\n                self.reload_mod(mod_name, spec, is_file, loc)\n                if on_reload:\n                    on_reload()\n\n        if not use_thread:\n            helper()\n        else:\n            threading.Thread(target=helper, daemon=True).start()\n\n    def _register_function(self, module_name, func_name, data):\n        if module_name not in self.functions:\n            self.functions[module_name] = {}\n        if func_name in self.functions[module_name]:\n            self.print(f\"Overriding function {func_name} from {module_name}\", end=\"\\r\")\n            self.functions[module_name][func_name] = data\n        else:\n            self.functions[module_name][func_name] = data\n\n    def _create_decorator(self, type_: str,\n                          name: str = \"\",\n                          mod_name: str = \"\",\n                          level: int = -1,\n                          restrict_in_virtual_mode: bool = False,\n                          api: bool = False,\n                          helper: str = \"\",\n                          version: str or None = None,\n                          initial: bool=False,\n                          exit_f: bool=False,\n                          test: bool=True,\n                          samples:list[dict[str, Any]] | None=None,\n                          state:bool | None=None,\n                          pre_compute:Callable | None=None,\n                          post_compute:Callable[[], Result] | None=None,\n                          api_methods:list[str] | None=None,\n                          memory_cache: bool=False,\n                          file_cache: bool=False,\n                          request_as_kwarg: bool=False,\n                          row: bool=False,\n                          memory_cache_max_size:int=100,\n                          memory_cache_ttl:int=300):\n\n        if isinstance(type_, Enum):\n            type_ = type_.value\n\n        if memory_cache and file_cache:\n            raise ValueError(\"Don't use both cash at the same time for the same fuction\")\n\n        use_cache = memory_cache or file_cache\n        cache = {}\n        if file_cache:\n            cache = FileCache(folder=self.data_dir + f'\\\\cache\\\\{mod_name}\\\\',\n                              filename=self.data_dir + f'\\\\cache\\\\{mod_name}\\\\{name}cache.db')\n        if memory_cache:\n            cache = MemoryCache(maxsize=memory_cache_max_size, ttl=memory_cache_ttl)\n\n        version = self.version if version is None else self.version + ':' + version\n\n        def a_additional_process(func):\n\n            async def executor(*args, **kwargs):\n\n                if pre_compute is not None:\n                    args, kwargs = await pre_compute(*args, **kwargs)\n                if asyncio.iscoroutinefunction(func):\n                    result = await func(*args, **kwargs)\n                else:\n                    result = func(*args, **kwargs)\n                if post_compute is not None:\n                    result = await post_compute(result)\n                if row:\n                    return result\n                if not isinstance(result, Result):\n                    result = Result.ok(data=result)\n                if result.origin is None:\n                    result.set_origin((mod_name if mod_name else func.__module__.split('.')[-1]\n                                       , name if name else func.__name__\n                                       , type_))\n                if result.result.data_to == ToolBoxInterfaces.native.name:\n                    result.result.data_to = ToolBoxInterfaces.remote if api else ToolBoxInterfaces.native\n                # Wenden Sie die to_api_result Methode auf das Ergebnis an, falls verf\u00fcgbar\n                if api and hasattr(result, 'to_api_result'):\n                    return result.to_api_result()\n                return result\n\n            @wraps(func)\n            async def wrapper(*args, **kwargs):\n\n                if not use_cache:\n                    return await executor(*args, **kwargs)\n\n                try:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{str(args)},{str(kwargs.items())}\")\n                except ValueError:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{bytes(args)},{str(kwargs.items())}\")\n\n                result = cache.get(cache_key)\n                if result is not None:\n                    return result\n\n                result = await executor(*args, **kwargs)\n\n                cache.set(cache_key, result)\n\n                return result\n\n            return wrapper\n\n        def additional_process(func):\n\n            def executor(*args, **kwargs):\n\n                if pre_compute is not None:\n                    args, kwargs = pre_compute(*args, **kwargs)\n                if asyncio.iscoroutinefunction(func):\n                    result = func(*args, **kwargs)\n                else:\n                    result = func(*args, **kwargs)\n                if post_compute is not None:\n                    result = post_compute(result)\n                if row:\n                    return result\n                if not isinstance(result, Result):\n                    result = Result.ok(data=result)\n                if result.origin is None:\n                    result.set_origin((mod_name if mod_name else func.__module__.split('.')[-1]\n                                       , name if name else func.__name__\n                                       , type_))\n                if result.result.data_to == ToolBoxInterfaces.native.name:\n                    result.result.data_to = ToolBoxInterfaces.remote if api else ToolBoxInterfaces.native\n                # Wenden Sie die to_api_result Methode auf das Ergebnis an, falls verf\u00fcgbar\n                if api and hasattr(result, 'to_api_result'):\n                    return result.to_api_result()\n                return result\n\n            @wraps(func)\n            def wrapper(*args, **kwargs):\n\n                if not use_cache:\n                    return executor(*args, **kwargs)\n\n                try:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{str(args)},{str(kwargs.items())}\")\n                except ValueError:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{bytes(args)},{str(kwargs.items())}\")\n\n                result = cache.get(cache_key)\n                if result is not None:\n                    return result\n\n                result = executor(*args, **kwargs)\n\n                cache.set(cache_key, result)\n\n                return result\n\n            return wrapper\n\n        def decorator(func):\n            sig = signature(func)\n            params = list(sig.parameters)\n            module_name = mod_name if mod_name else func.__module__.split('.')[-1]\n            func_name = name if name else func.__name__\n            if func_name == 'on_start':\n                func_name = 'on_startup'\n            if func_name == 'on_exit':\n                func_name = 'on_close'\n            if api or pre_compute is not None or post_compute is not None or memory_cache or file_cache:\n                if asyncio.iscoroutinefunction(func):\n                    func = a_additional_process(func)\n                else:\n                    func = additional_process(func)\n            if api and str(sig.return_annotation) == 'Result':\n                raise ValueError(f\"Fuction {module_name}.{func_name} registered as \"\n                                 f\"Api fuction but uses {str(sig.return_annotation)}\\n\"\n                                 f\"Please change the sig from ..)-&gt; Result to ..)-&gt; ApiResult\")\n            data = {\n                \"type\": type_,\n                \"module_name\": module_name,\n                \"func_name\": func_name,\n                \"level\": level,\n                \"restrict_in_virtual_mode\": restrict_in_virtual_mode,\n                \"func\": func,\n                \"api\": api,\n                \"helper\": helper,\n                \"version\": version,\n                \"initial\": initial,\n                \"exit_f\": exit_f,\n                \"api_methods\": api_methods if api_methods is not None else [\"AUTO\"],\n                \"__module__\": func.__module__,\n                \"signature\": sig,\n                \"params\": params,\n                \"row\": row,\n                \"state\": (\n                    False if len(params) == 0 else params[0] in ['self', 'state', 'app']) if state is None else state,\n                \"do_test\": test,\n                \"samples\": samples,\n                \"request_as_kwarg\": request_as_kwarg,\n\n            }\n            self._register_function(module_name, func_name, data)\n            if exit_f:\n                if \"on_exit\" not in self.functions[module_name]:\n                    self.functions[module_name][\"on_exit\"] = []\n                self.functions[module_name][\"on_exit\"].append(data)\n            if initial:\n                if \"on_start\" not in self.functions[module_name]:\n                    self.functions[module_name][\"on_start\"] = []\n                self.functions[module_name][\"on_start\"].append(func_name)\n\n            return func\n\n        decorator.tb_init = True\n\n        return decorator\n\n    def tb(self, name=None,\n           mod_name: str = \"\",\n           helper: str = \"\",\n           version: str | None = None,\n           test: bool = True,\n           restrict_in_virtual_mode: bool = False,\n           api: bool = False,\n           initial: bool = False,\n           exit_f: bool = False,\n           test_only: bool = False,\n           memory_cache: bool = False,\n           file_cache: bool = False,\n           request_as_kwarg: bool = False,\n           row: bool = False,\n           state: bool | None = None,\n           level: int = -1,\n           memory_cache_max_size: int = 100,\n           memory_cache_ttl: int = 300,\n           samples: list or dict or None = None,\n           interface: ToolBoxInterfaces or None or str = None,\n           pre_compute=None,\n           post_compute=None,\n           api_methods=None,\n           ):\n        \"\"\"\n    A decorator for registering and configuring functions within a module.\n\n    This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\n    Args:\n        name (str, optional): The name to register the function under. Defaults to the function's own name.\n        mod_name (str, optional): The name of the module the function belongs to.\n        helper (str, optional): A helper string providing additional information about the function.\n        version (str or None, optional): The version of the function or module.\n        test (bool, optional): Flag to indicate if the function is for testing purposes.\n        restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n        api (bool, optional): Flag to indicate if the function is part of an API.\n        initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n        exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n        test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n        memory_cache (bool, optional): Flag to enable memory caching for the function.\n        request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n        file_cache (bool, optional): Flag to enable file caching for the function.\n        row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n        state (bool or None, optional): Flag to indicate if the function maintains state.\n        level (int, optional): The level of the function, used for prioritization or categorization.\n        memory_cache_max_size (int, optional): Maximum size of the memory cache.\n        memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n        samples (list or dict or None, optional): Samples or examples of function usage.\n        interface (str, optional): The interface type for the function.\n        pre_compute (callable, optional): A function to be called before the main function.\n        post_compute (callable, optional): A function to be called after the main function.\n        api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\n    Returns:\n        function: The decorated function with additional processing and registration capabilities.\n    \"\"\"\n        if interface is None:\n            interface = \"tb\"\n        if test_only and 'test' not in self.id:\n            return lambda *args, **kwargs: args\n        return self._create_decorator(interface,\n                                      name,\n                                      mod_name,\n                                      level=level,\n                                      restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                      helper=helper,\n                                      api=api,\n                                      version=version,\n                                      initial=initial,\n                                      exit_f=exit_f,\n                                      test=test,\n                                      samples=samples,\n                                      state=state,\n                                      pre_compute=pre_compute,\n                                      post_compute=post_compute,\n                                      memory_cache=memory_cache,\n                                      file_cache=file_cache,\n                                      request_as_kwarg=request_as_kwarg,\n                                      row=row,\n                                      api_methods=api_methods,\n                                      memory_cache_max_size=memory_cache_max_size,\n                                      memory_cache_ttl=memory_cache_ttl)\n\n    def save_autocompletion_dict(self):\n        autocompletion_dict = {}\n        for module_name, _module in self.functions.items():\n            data = {}\n            for function_name, function_data in self.functions[module_name].items():\n                if not isinstance(function_data, dict):\n                    continue\n                data[function_name] = {arg: None for arg in\n                                       function_data.get(\"params\", [])}\n                if len(data[function_name].keys()) == 0:\n                    data[function_name] = None\n            autocompletion_dict[module_name] = data if len(data.keys()) &gt; 0 else None\n        self.config_fh.add_to_save_file_handler(\"auto~~~~~~\", str(autocompletion_dict))\n\n    def get_autocompletion_dict(self):\n        return self.config_fh.get_file_handler(\"auto~~~~~~\")\n\n    def save_registry_as_enums(self, directory: str, filename: str):\n        # Ordner erstellen, falls nicht vorhanden\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n        # Dateipfad vorbereiten\n        filepath = os.path.join(directory, filename)\n\n        # Enum-Klassen als Strings generieren\n        enum_classes = [f'\"\"\"Automatic generated by ToolBox v = {self.version}\"\"\"'\n                        f'\\nfrom enum import Enum\\nfrom dataclasses import dataclass'\n                        f'\\n\\n\\n']\n        for module, functions in self.functions.items():\n            if module.startswith(\"APP_INSTANCE\"):\n                continue\n            class_name = module\n            enum_members = \"\\n    \".join(\n                [\n                    f\"{func_name.upper().replace('-', '')}\"\n                    f\" = '{func_name}' \"\n                    f\"# Input: ({fuction_data['params'] if isinstance(fuction_data, dict) else ''}),\"\n                    f\" Output: {fuction_data['signature'].return_annotation if isinstance(fuction_data, dict) else 'None'}\"\n                    for func_name, fuction_data in functions.items()])\n            enum_class = (f'@dataclass\\nclass {class_name.upper().replace(\".\", \"_\").replace(\"-\", \"\")}(Enum):'\n                          f\"\\n    NAME = '{class_name}'\\n    {enum_members}\")\n            enum_classes.append(enum_class)\n\n        # Enums in die Datei schreiben\n        data = \"\\n\\n\\n\".join(enum_classes)\n        if len(data) &lt; 12:\n            raise ValueError(\n                \"Invalid Enums Loosing content pleas delete it ur self in the (utils/system/all_functions_enums.py) or add mor new stuff :}\")\n        with open(filepath, 'w') as file:\n            file.write(data)\n\n        print(Style.Bold(Style.BLUE(f\"Enums gespeichert in {filepath}\")))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.disconnect","title":"<code>disconnect(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef disconnect(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.exit_main","title":"<code>exit_main(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef exit_main(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.get_function","title":"<code>get_function(name, **kwargs)</code>","text":"<p>Kwargs for _get_function     metadata:: return the registered function dictionary         stateless: (function_data, None), 0         stateful: (function_data, higher_order_function), 0     state::boolean         specification::str default app</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def get_function(self, name: Enum or tuple, **kwargs):\n    \"\"\"\n    Kwargs for _get_function\n        metadata:: return the registered function dictionary\n            stateless: (function_data, None), 0\n            stateful: (function_data, higher_order_function), 0\n        state::boolean\n            specification::str default app\n    \"\"\"\n    if isinstance(name, tuple):\n        return self._get_function(None, as_str=name, **kwargs)\n    else:\n        return self._get_function(name, **kwargs)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.hide_console","title":"<code>hide_console(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef hide_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.run","title":"<code>run(*args, request=None, running_function_coro=None, **kwargs)</code>","text":"<p>Run a function with support for SSE streaming in both threaded and non-threaded contexts.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def run(self, *args, request=None, running_function_coro=None, **kwargs):\n    \"\"\"\n    Run a function with support for SSE streaming in both\n    threaded and non-threaded contexts.\n    \"\"\"\n    if running_function_coro is None:\n        mn, fn = args[0]\n        if self.functions.get(mn, {}).get(fn, {}).get('request_as_kwarg', False):\n            kwargs[\"request\"] = RequestData.from_dict(request)\n            if 'data' in kwargs and 'data' not in self.functions.get(mn, {}).get(fn, {}).get('params', []):\n                kwargs[\"request\"].data = kwargs[\"request\"].body = kwargs['data']\n                del kwargs['data']\n            if 'form_data' in kwargs and 'form_data' not in self.functions.get(mn, {}).get(fn, {}).get('params',\n                                                                                                       []):\n                kwargs[\"request\"].form_data = kwargs[\"request\"].body = kwargs['form_data']\n                del kwargs['form_data']\n\n    # Create the coroutine\n    coro = running_function_coro or self.a_run_any(*args, **kwargs)\n\n    # Get or create an event loop\n    try:\n        loop = asyncio.get_event_loop()\n        is_running = loop.is_running()\n    except RuntimeError:\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        is_running = False\n\n    # If the loop is already running, run in a separate thread\n    if is_running:\n        # Create thread pool executor as needed\n        if not hasattr(self.__class__, '_executor'):\n            self.__class__._executor = ThreadPoolExecutor(max_workers=4)\n\n        def run_in_new_thread():\n            # Set up a new loop in this thread\n            new_loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(new_loop)\n\n            try:\n                # Run the coroutine\n                return new_loop.run_until_complete(coro)\n            finally:\n                new_loop.close()\n\n        # Run in thread and get result\n        thread_result = self.__class__._executor.submit(run_in_new_thread).result()\n\n        # Handle streaming results from thread\n        if isinstance(thread_result, dict) and thread_result.get(\"is_stream\"):\n            # Create a new SSE stream in the main thread\n            async def stream_from_function():\n                # Re-run the function with direct async access\n                stream_result = await self.a_run_any(*args, **kwargs)\n\n                if (isinstance(stream_result, Result) and\n                    getattr(stream_result.result, 'data_type', None) == \"stream\"):\n                    # Get and forward data from the original generator\n                    original_gen = stream_result.result.data.get(\"generator\")\n                    if inspect.isasyncgen(original_gen):\n                        async for item in original_gen:\n                            yield item\n\n            # Return a new streaming Result\n            return Result.stream(\n                stream_generator=stream_from_function(),\n                headers=thread_result.get(\"headers\", {})\n            )\n\n        result = thread_result\n    else:\n        # Direct execution when loop is not running\n        result = loop.run_until_complete(coro)\n\n    # Process the final result\n    if isinstance(result, Result):\n        if 'debug' in self.id:\n            result.print()\n        if getattr(result.result, 'data_type', None) == \"stream\":\n            return result\n        return result.to_api_result().model_dump(mode='json')\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.run_bg_task","title":"<code>run_bg_task(task, *args, **kwargs)</code>","text":"<p>Runs a coroutine in the background without blocking the caller.</p> <p>This is the primary method for \"fire-and-forget\" async tasks. It schedules the coroutine to run on the application's main event loop.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Callable</code> <p>The coroutine function to run.</p> required <code>*args</code> <p>Arguments to pass to the coroutine function.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the coroutine function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Task]</code> <p>An asyncio.Task object representing the scheduled task, or None if</p> <code>Optional[Task]</code> <p>the task could not be scheduled.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def run_bg_task(self, task: Callable, *args, **kwargs) -&gt; Optional[asyncio.Task]:\n    \"\"\"\n    Runs a coroutine in the background without blocking the caller.\n\n    This is the primary method for \"fire-and-forget\" async tasks. It schedules\n    the coroutine to run on the application's main event loop.\n\n    Args:\n        task: The coroutine function to run.\n        *args: Arguments to pass to the coroutine function.\n        **kwargs: Keyword arguments to pass to the coroutine function.\n\n    Returns:\n        An asyncio.Task object representing the scheduled task, or None if\n        the task could not be scheduled.\n    \"\"\"\n    if not callable(task):\n        self.logger.warning(\"Task passed to run_bg_task is not callable!\")\n        return None\n\n    if not asyncio.iscoroutinefunction(task) and not asyncio.iscoroutine(task):\n        self.logger.warning(f\"Task '{getattr(task, '__name__', 'unknown')}' is not a coroutine. \"\n                            f\"Use run_bg_task_advanced for synchronous functions.\")\n        # Fallback to advanced runner for convenience\n        self.run_bg_task_advanced(task, *args, **kwargs)\n        return None\n\n    try:\n        loop = self.loop_gard()\n        if not loop.is_running():\n            # If the main loop isn't running, we can't create a task on it.\n            # This scenario is handled by run_bg_task_advanced.\n            self.logger.info(\"Main event loop not running. Delegating to advanced background runner.\")\n            return self.run_bg_task_advanced(task, *args, **kwargs)\n\n        # Create the coroutine if it's a function\n        coro = task(*args, **kwargs) if asyncio.iscoroutinefunction(task) else task\n\n        # Create a task on the running event loop\n        bg_task = loop.create_task(coro)\n\n        # Add a callback to log exceptions from the background task\n        def _log_exception(the_task: asyncio.Task):\n            if not the_task.cancelled() and the_task.exception():\n                self.logger.error(f\"Exception in background task '{the_task.get_name()}':\",\n                                  exc_info=the_task.exception())\n\n        bg_task.add_done_callback(_log_exception)\n        self.bg_tasks.append(bg_task)\n        return bg_task\n\n    except Exception as e:\n        self.logger.error(f\"Failed to schedule background task: {e}\", exc_info=True)\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.run_bg_task_advanced","title":"<code>run_bg_task_advanced(task, *args, **kwargs)</code>","text":"<p>Runs a task in a separate, dedicated background thread with its own event loop.</p> <p>This is ideal for: 1. Running an async task from a synchronous context. 2. Launching a long-running, independent operation that should not    interfere with the main application's event loop.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Callable</code> <p>The function to run (can be sync or async).</p> required <code>*args</code> <p>Arguments for the task.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments for the task.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Thread</code> <p>The threading.Thread object managing the background execution.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def run_bg_task_advanced(self, task: Callable, *args, **kwargs) -&gt; threading.Thread:\n    \"\"\"\n    Runs a task in a separate, dedicated background thread with its own event loop.\n\n    This is ideal for:\n    1. Running an async task from a synchronous context.\n    2. Launching a long-running, independent operation that should not\n       interfere with the main application's event loop.\n\n    Args:\n        task: The function to run (can be sync or async).\n        *args: Arguments for the task.\n        **kwargs: Keyword arguments for the task.\n\n    Returns:\n        The threading.Thread object managing the background execution.\n    \"\"\"\n    if not callable(task):\n        self.logger.warning(\"Task for run_bg_task_advanced is not callable!\")\n        return None\n\n    def thread_target():\n        # Each thread gets its own event loop.\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n\n        try:\n            # Prepare the coroutine we need to run\n            if asyncio.iscoroutinefunction(task):\n                coro = task(*args, **kwargs)\n            elif asyncio.iscoroutine(task):\n                # It's already a coroutine object\n                coro = task\n            else:\n                # It's a synchronous function, run it in an executor\n                # to avoid blocking the new event loop.\n                coro = loop.run_in_executor(None, lambda: task(*args, **kwargs))\n\n            # Run the coroutine to completion\n            result = loop.run_until_complete(coro)\n            self.logger.debug(f\"Advanced background task '{getattr(task, '__name__', 'unknown')}' completed.\")\n            if result is not None:\n                self.logger.debug(f\"Task result: {str(result)[:100]}\")\n\n        except Exception as e:\n            self.logger.error(f\"Error in advanced background task '{getattr(task, '__name__', 'unknown')}':\",\n                              exc_info=e)\n        finally:\n            # Cleanly shut down the event loop in this thread.\n            try:\n                all_tasks = asyncio.all_tasks(loop=loop)\n                if all_tasks:\n                    for t in all_tasks:\n                        t.cancel()\n                    loop.run_until_complete(asyncio.gather(*all_tasks, return_exceptions=True))\n            finally:\n                loop.close()\n                asyncio.set_event_loop(None)\n\n    # Create, start, and return the thread.\n    # It's a daemon thread so it won't prevent the main app from exiting.\n    t = threading.Thread(target=thread_target, daemon=True, name=f\"BGTask-{getattr(task, '__name__', 'unknown')}\")\n    self.bg_tasks.append(t)\n    t.start()\n    return t\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.show_console","title":"<code>show_console(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef show_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.tb","title":"<code>tb(name=None, mod_name='', helper='', version=None, test=True, restrict_in_virtual_mode=False, api=False, initial=False, exit_f=False, test_only=False, memory_cache=False, file_cache=False, request_as_kwarg=False, row=False, state=None, level=-1, memory_cache_max_size=100, memory_cache_ttl=300, samples=None, interface=None, pre_compute=None, post_compute=None, api_methods=None)</code>","text":"<p>A decorator for registering and configuring functions within a module.</p> <p>This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to register the function under. Defaults to the function's own name.</p> <code>None</code> <code>mod_name</code> <code>str</code> <p>The name of the module the function belongs to.</p> <code>''</code> <code>helper</code> <code>str</code> <p>A helper string providing additional information about the function.</p> <code>''</code> <code>version</code> <code>str or None</code> <p>The version of the function or module.</p> <code>None</code> <code>test</code> <code>bool</code> <p>Flag to indicate if the function is for testing purposes.</p> <code>True</code> <code>restrict_in_virtual_mode</code> <code>bool</code> <p>Flag to restrict the function in virtual mode.</p> <code>False</code> <code>api</code> <code>bool</code> <p>Flag to indicate if the function is part of an API.</p> <code>False</code> <code>initial</code> <code>bool</code> <p>Flag to indicate if the function should be executed at initialization.</p> <code>False</code> <code>exit_f</code> <code>bool</code> <p>Flag to indicate if the function should be executed at exit.</p> <code>False</code> <code>test_only</code> <code>bool</code> <p>Flag to indicate if the function should only be used for testing.</p> <code>False</code> <code>memory_cache</code> <code>bool</code> <p>Flag to enable memory caching for the function.</p> <code>False</code> <code>request_as_kwarg</code> <code>bool</code> <p>Flag to get request if the fuction is calld from api.</p> <code>False</code> <code>file_cache</code> <code>bool</code> <p>Flag to enable file caching for the function.</p> <code>False</code> <code>row</code> <code>bool</code> <p>rather to auto wrap the result in Result type default False means no row data aka result type</p> <code>False</code> <code>state</code> <code>bool or None</code> <p>Flag to indicate if the function maintains state.</p> <code>None</code> <code>level</code> <code>int</code> <p>The level of the function, used for prioritization or categorization.</p> <code>-1</code> <code>memory_cache_max_size</code> <code>int</code> <p>Maximum size of the memory cache.</p> <code>100</code> <code>memory_cache_ttl</code> <code>int</code> <p>Time-to-live for the memory cache entries.</p> <code>300</code> <code>samples</code> <code>list or dict or None</code> <p>Samples or examples of function usage.</p> <code>None</code> <code>interface</code> <code>str</code> <p>The interface type for the function.</p> <code>None</code> <code>pre_compute</code> <code>callable</code> <p>A function to be called before the main function.</p> <code>None</code> <code>post_compute</code> <code>callable</code> <p>A function to be called after the main function.</p> <code>None</code> <code>api_methods</code> <code>list[str]</code> <p>default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>function</code> <p>The decorated function with additional processing and registration capabilities.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def tb(self, name=None,\n       mod_name: str = \"\",\n       helper: str = \"\",\n       version: str | None = None,\n       test: bool = True,\n       restrict_in_virtual_mode: bool = False,\n       api: bool = False,\n       initial: bool = False,\n       exit_f: bool = False,\n       test_only: bool = False,\n       memory_cache: bool = False,\n       file_cache: bool = False,\n       request_as_kwarg: bool = False,\n       row: bool = False,\n       state: bool | None = None,\n       level: int = -1,\n       memory_cache_max_size: int = 100,\n       memory_cache_ttl: int = 300,\n       samples: list or dict or None = None,\n       interface: ToolBoxInterfaces or None or str = None,\n       pre_compute=None,\n       post_compute=None,\n       api_methods=None,\n       ):\n    \"\"\"\nA decorator for registering and configuring functions within a module.\n\nThis decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\nArgs:\n    name (str, optional): The name to register the function under. Defaults to the function's own name.\n    mod_name (str, optional): The name of the module the function belongs to.\n    helper (str, optional): A helper string providing additional information about the function.\n    version (str or None, optional): The version of the function or module.\n    test (bool, optional): Flag to indicate if the function is for testing purposes.\n    restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n    api (bool, optional): Flag to indicate if the function is part of an API.\n    initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n    exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n    test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n    memory_cache (bool, optional): Flag to enable memory caching for the function.\n    request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n    file_cache (bool, optional): Flag to enable file caching for the function.\n    row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n    state (bool or None, optional): Flag to indicate if the function maintains state.\n    level (int, optional): The level of the function, used for prioritization or categorization.\n    memory_cache_max_size (int, optional): Maximum size of the memory cache.\n    memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n    samples (list or dict or None, optional): Samples or examples of function usage.\n    interface (str, optional): The interface type for the function.\n    pre_compute (callable, optional): A function to be called before the main function.\n    post_compute (callable, optional): A function to be called after the main function.\n    api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\nReturns:\n    function: The decorated function with additional processing and registration capabilities.\n\"\"\"\n    if interface is None:\n        interface = \"tb\"\n    if test_only and 'test' not in self.id:\n        return lambda *args, **kwargs: args\n    return self._create_decorator(interface,\n                                  name,\n                                  mod_name,\n                                  level=level,\n                                  restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                  helper=helper,\n                                  api=api,\n                                  version=version,\n                                  initial=initial,\n                                  exit_f=exit_f,\n                                  test=test,\n                                  samples=samples,\n                                  state=state,\n                                  pre_compute=pre_compute,\n                                  post_compute=post_compute,\n                                  memory_cache=memory_cache,\n                                  file_cache=file_cache,\n                                  request_as_kwarg=request_as_kwarg,\n                                  row=row,\n                                  api_methods=api_methods,\n                                  memory_cache_max_size=memory_cache_max_size,\n                                  memory_cache_ttl=memory_cache_ttl)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.wait_for_bg_tasks","title":"<code>wait_for_bg_tasks(timeout=None)</code>","text":"<p>Wait for all background tasks to complete.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <p>Maximum time to wait (in seconds) for all tasks to complete.      None means wait indefinitely.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if all tasks completed, False if timeout occurred</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def wait_for_bg_tasks(self, timeout=None):\n    \"\"\"\n    Wait for all background tasks to complete.\n\n    Args:\n        timeout: Maximum time to wait (in seconds) for all tasks to complete.\n                 None means wait indefinitely.\n\n    Returns:\n        bool: True if all tasks completed, False if timeout occurred\n    \"\"\"\n    active_tasks = [t for t in self.bg_tasks if t.is_alive()]\n\n    for task in active_tasks:\n        task.join(timeout=timeout)\n        if task.is_alive():\n            return False\n\n    return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.show_console","title":"<code>toolboxv2.show_console(show=True)</code>","text":"Source code in <code>toolboxv2/utils/extras/show_and_hide_console.py</code> <pre><code>def show_console(show=True):\n    global TBRUNNER_console_viabel\n    \"\"\"Brings up the Console Window.\"\"\"\n    try:\n        if show and not TBRUNNER_console_viabel:\n            # Show console\n            ctypes.windll.user32.ShowWindow(ctypes.windll.kernel32.GetConsoleWindow(), 4)\n            TBRUNNER_console_viabel = True\n            return True\n        elif not show and TBRUNNER_console_viabel:\n            # Hide console\n            ctypes.windll.user32.ShowWindow(ctypes.windll.kernel32.GetConsoleWindow(), 0)\n            TBRUNNER_console_viabel = False\n            return True\n    except:\n        print(f\"Could not show_console {show=}\", )\n        return False\n    return False\n</code></pre>"},{"location":"toolboxv2/#logging","title":"Logging","text":""},{"location":"toolboxv2/#toolboxv2.get_logger","title":"<code>toolboxv2.get_logger()</code>","text":"Source code in <code>toolboxv2/utils/system/tb_logger.py</code> <pre><code>def get_logger() -&gt; logging.Logger:\n    return logging.getLogger(loggerNameOfToolboxv2)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.setup_logging","title":"<code>toolboxv2.setup_logging(level, name=loggerNameOfToolboxv2, online_level=None, is_online=False, file_level=None, interminal=False, logs_directory='../logs', app_name='main')</code>","text":"Source code in <code>toolboxv2/utils/system/tb_logger.py</code> <pre><code>def setup_logging(level: int, name=loggerNameOfToolboxv2, online_level=None, is_online=False, file_level=None,\n                  interminal=False, logs_directory=\"../logs\", app_name=\"main\"):\n    global loggerNameOfToolboxv2\n\n    if not online_level:\n        online_level = level\n\n    if not file_level:\n        file_level = level\n\n    if not os.path.exists(logs_directory):\n        os.makedirs(logs_directory, exist_ok=True)\n    if not os.path.exists(logs_directory + \"/Logs.info\"):\n        open(f\"{logs_directory}/Logs.info\", \"a\").close()\n\n    loggerNameOfToolboxv2 = name\n\n    available_log_levels = [logging.CRITICAL, logging.FATAL, logging.ERROR, logging.WARNING, logging.WARN, logging.INFO,\n                            logging.DEBUG, logging.NOTSET]\n\n    if level not in available_log_levels:\n        raise ValueError(f\"level must be one of {available_log_levels}, but logging level is {level}\")\n\n    if online_level not in available_log_levels:\n        raise ValueError(f\"online_level must be one of {available_log_levels}, but logging level is {online_level}\")\n\n    if file_level not in available_log_levels:\n        raise ValueError(f\"file_level must be one of {available_log_levels}, but logging level is {file_level}\")\n\n    log_date = datetime.datetime.today().strftime('%Y-%m-%d')\n    log_levels = [\"CRITICAL\", \"ERROR\", \"WARNING\", \"INFO\", \"DEBUG\", \"NOTSET\"]\n    log_level_index = log_levels.index(logging.getLevelName(level))\n\n    filename = f\"Logs-{name}-{log_date}-{log_levels[log_level_index]}\"\n    log_filename = f\"{logs_directory}/{filename}.log\"\n\n    log_info_data = {\n        filename: 0,\n        \"H\": \"localhost\",\n        \"P\": 62435\n    }\n\n    with open(f\"{logs_directory}/Logs.info\") as li:\n        log_info_data_str = li.read()\n        try:\n            log_info_data = eval(log_info_data_str)\n        except SyntaxError:\n            if log_info_data_str:\n                print(Style.RED(Style.Bold(\"Could not parse log info data\")))\n\n        if filename not in log_info_data:\n            log_info_data[filename] = 0\n\n        if not os.path.exists(log_filename):\n            log_info_data[filename] = 0\n            print(\"new log file\")\n\n        if os.path.exists(log_filename):\n            log_info_data[filename] += 1\n\n            while os.path.exists(f\"{logs_directory}/{filename}#{log_info_data[filename]}.log\"):\n                log_info_data[filename] += 1\n\n            try:\n                os.rename(log_filename,\n                          f\"{logs_directory}/{filename}#{log_info_data[filename]}.log\")\n            except PermissionError:\n                print(Style.YELLOW(Style.Bold(f\"Could not rename log file appending on {filename}\")))\n\n    with open(f\"{logs_directory}/Logs.info\", \"w\") as li:\n        if len(log_info_data.keys()) &gt;= 7:\n            log_info_data = {\n                filename: log_info_data[filename],\n                \"H\": log_info_data[\"H\"],\n                \"P\": log_info_data[\"P\"]\n            }\n        li.write(str(log_info_data))\n\n    try:\n        with open(log_filename, \"a\"):\n            pass\n    except OSError:\n        log_filename = f\"{logs_directory}/Logs-Test-{log_date}-{log_levels[log_level_index]}.log\"\n        with open(log_filename, \"a\"):\n            pass\n\n    logger = logging.getLogger(name)\n\n    logger.setLevel(level)\n    # Prevent logger from propagating to parent loggers\n    logger.propagate = False\n\n    terminal_format = f\"{app_name} %(asctime)s %(levelname)s %(name)s - %(message)s\"\n    file_format = f\"{app_name} %(asctime)s - %(name)s - %(levelname)s - %(filename)s - %(funcName)s:%(lineno)d - %(message)s\"\n\n    # Configure handlers\n    handlers = []\n\n    # File handler (always added)\n    file_handler = logging.FileHandler(log_filename)\n    file_handler.setFormatter(logging.Formatter(file_format))\n    file_handler.setLevel(file_level)\n    handlers.append(file_handler)\n\n    # Terminal handler (if requested)\n    if interminal:\n        terminal_handler = logging.StreamHandler()\n        terminal_handler.setFormatter(logging.Formatter(terminal_format))\n        terminal_handler.setLevel(level)\n        handlers.append(terminal_handler)\n\n    # Socket handler (if requested)\n    if is_online:\n        socket_handler = SocketHandler(log_info_data[\"H\"], log_info_data[\"P\"])\n        socket_handler.setFormatter(logging.Formatter(file_format))\n        socket_handler.setLevel(online_level)\n        handlers.append(socket_handler)\n\n    # Add all handlers to logger\n    for handler in handlers:\n        logger.addHandler(handler)\n\n    return logger, filename\n</code></pre>"},{"location":"toolboxv2/#styling-console-output","title":"Styling &amp; Console Output","text":""},{"location":"toolboxv2/#toolboxv2.Style","title":"<code>toolboxv2.Style</code>","text":"Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>class Style:\n    _END = '\\33[0m'\n    _BLACK = '\\33[30m'\n    _RED = '\\33[31m'\n    _GREEN = '\\33[32m'\n    _YELLOW = '\\33[33m'\n    _BLUE = '\\33[34m'\n    _MAGENTA = '\\33[35m'\n    _CYAN = '\\33[36m'\n    _WHITE = '\\33[37m'\n\n    _Bold = '\\33[1m'\n    _ITALIC = '\\33[3m'\n    _Underline = '\\33[4m'\n    _BLINK = '\\33[5m'\n    _BLINK2 = '\\33[6m'\n    _Reversed = '\\33[7m'\n\n    _BLACKBG = '\\33[40m'\n    _REDBG = '\\33[41m'\n    _GREENBG = '\\33[42m'\n    _YELLOWBG = '\\33[43m'\n    _BLUEBG = '\\33[44m'\n    _VIOLETBG = '\\33[45m'\n    _BEIGEBG = '\\33[46m'\n    _WHITEBG = '\\33[47m'\n\n    _GREY = '\\33[90m'\n    _RED2 = '\\33[91m'\n    _GREEN2 = '\\33[92m'\n    _YELLOW2 = '\\33[93m'\n    _BLUE2 = '\\33[94m'\n    _VIOLET2 = '\\33[95m'\n    _BEIGE2 = '\\33[96m'\n    _WHITE2 = '\\33[97m'\n\n    _GREYBG = '\\33[100m'\n    _REDBG2 = '\\33[101m'\n    _GREENBG2 = '\\33[102m'\n    _YELLOWBG2 = '\\33[103m'\n    _BLUEBG2 = '\\33[104m'\n    _VIOLETBG2 = '\\33[105m'\n    _BEIGEBG2 = '\\33[106m'\n    _WHITEBG2 = '\\33[107m'\n\n    style_dic = {\n        \"END\": _END,\n        \"BLACK\": _BLACK,\n        \"RED\": _RED,\n        \"GREEN\": _GREEN,\n        \"YELLOW\": _YELLOW,\n        \"BLUE\": _BLUE,\n        \"MAGENTA\": _MAGENTA,\n        \"CYAN\": _CYAN,\n        \"WHITE\": _WHITE,\n        \"Bold\": _Bold,\n        \"Underline\": _Underline,\n        \"Reversed\": _Reversed,\n\n        \"ITALIC\": _ITALIC,\n        \"BLINK\": _BLINK,\n        \"BLINK2\": _BLINK2,\n        \"BLACKBG\": _BLACKBG,\n        \"REDBG\": _REDBG,\n        \"GREENBG\": _GREENBG,\n        \"YELLOWBG\": _YELLOWBG,\n        \"BLUEBG\": _BLUEBG,\n        \"VIOLETBG\": _VIOLETBG,\n        \"BEIGEBG\": _BEIGEBG,\n        \"WHITEBG\": _WHITEBG,\n        \"GREY\": _GREY,\n        \"RED2\": _RED2,\n        \"GREEN2\": _GREEN2,\n        \"YELLOW2\": _YELLOW2,\n        \"BLUE2\": _BLUE2,\n        \"VIOLET2\": _VIOLET2,\n        \"BEIGE2\": _BEIGE2,\n        \"WHITE2\": _WHITE2,\n        \"GREYBG\": _GREYBG,\n        \"REDBG2\": _REDBG2,\n        \"GREENBG2\": _GREENBG2,\n        \"YELLOWBG2\": _YELLOWBG2,\n        \"BLUEBG2\": _BLUEBG2,\n        \"VIOLETBG2\": _VIOLETBG2,\n        \"BEIGEBG2\": _BEIGEBG2,\n        \"WHITEBG2\": _WHITEBG2,\n\n    }\n\n    @staticmethod\n    @text_save\n    def END_():\n        print(Style._END)\n\n    @staticmethod\n    @text_save\n    def GREEN_():\n        print(Style._GREEN)\n\n    @staticmethod\n    @text_save\n    def BLUE(text: str):\n        return Style._BLUE + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BLACK(text: str):\n        return Style._BLACK + text + Style._END\n\n    @staticmethod\n    @text_save\n    def RED(text: str):\n        return Style._RED + text + Style._END\n\n    @staticmethod\n    @text_save\n    def GREEN(text: str):\n        return Style._GREEN + text + Style._END\n\n    @staticmethod\n    @text_save\n    def YELLOW(text: str):\n        return Style._YELLOW + text + Style._END\n\n    @staticmethod\n    @text_save\n    def MAGENTA(text: str):\n        return Style._MAGENTA + text + Style._END\n\n    @staticmethod\n    @text_save\n    def CYAN(text: str):\n        return Style._CYAN + text + Style._END\n\n    @staticmethod\n    @text_save\n    def WHITE(text: str):\n        return Style._WHITE + text + Style._END\n\n    @staticmethod\n    @text_save\n    def Bold(text: str):\n        return Style._Bold + text + Style._END\n\n    @staticmethod\n    @text_save\n    def Underline(text: str):\n        return Style._Underline + text + Style._END\n\n    @staticmethod\n    @text_save\n    def Reversed(text: str):\n        return Style._Reversed + text + Style._END\n\n    @staticmethod\n    @text_save\n    def ITALIC(text: str):\n        return Style._ITALIC + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BLINK(text: str):\n        return Style._BLINK + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BLINK2(text: str):\n        return Style._BLINK2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BLACKBG(text: str):\n        return Style._BLACKBG + text + Style._END\n\n    @staticmethod\n    @text_save\n    def REDBG(text: str):\n        return Style._REDBG + text + Style._END\n\n    @staticmethod\n    @text_save\n    def GREENBG(text: str):\n        return Style._GREENBG + text + Style._END\n\n    @staticmethod\n    @text_save\n    def YELLOWBG(text: str):\n        return Style._YELLOWBG + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BLUEBG(text: str):\n        return Style._BLUEBG + text + Style._END\n\n    @staticmethod\n    @text_save\n    def VIOLETBG(text: str):\n        return Style._VIOLETBG + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BEIGEBG(text: str):\n        return Style._BEIGEBG + text + Style._END\n\n    @staticmethod\n    @text_save\n    def WHITEBG(text: str):\n        return Style._WHITEBG + text + Style._END\n\n    @staticmethod\n    @text_save\n    def GREY(text: str):\n        return Style._GREY + str(text) + Style._END\n\n    @staticmethod\n    @text_save\n    def RED2(text: str):\n        return Style._RED2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def GREEN2(text: str):\n        return Style._GREEN2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def YELLOW2(text: str):\n        return Style._YELLOW2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BLUE2(text: str):\n        return Style._BLUE2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def VIOLET2(text: str):\n        return Style._VIOLET2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BEIGE2(text: str):\n        return Style._BEIGE2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def WHITE2(text: str):\n        return Style._WHITE2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def GREYBG(text: str):\n        return Style._GREYBG + text + Style._END\n\n    @staticmethod\n    @text_save\n    def REDBG2(text: str):\n        return Style._REDBG2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def GREENBG2(text: str):\n        return Style._GREENBG2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def YELLOWBG2(text: str):\n        return Style._YELLOWBG2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BLUEBG2(text: str):\n        return Style._BLUEBG2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def VIOLETBG2(text: str):\n        return Style._VIOLETBG2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BEIGEBG2(text: str):\n        return Style._BEIGEBG2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def WHITEBG2(text: str):\n        return Style._WHITEBG2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def loading_al(text: str):\n        b = f\"{text} /\"\n        print(b)\n        sleep(0.05)\n        cls()\n        b = f\"{text} -\"\n        print(b)\n        sleep(0.05)\n        cls()\n        b = f\"{text} \\\\\"\n        print(b)\n        sleep(0.05)\n        cls()\n        b = f\"{text} |\"\n        print(b)\n        sleep(0.05)\n        cls()\n\n    @property\n    def END(self):\n        return self._END\n\n    def color_demo(self):\n        for color in self.style_dic:\n            print(f\"{color} -&gt; {self.style_dic[color]}Effect{self._END}\")\n\n    @property\n    def Underline2(self):\n        return self._Underline\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Spinner","title":"<code>toolboxv2.Spinner</code>","text":"<p>Enhanced Spinner with tqdm-like line rendering.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>class Spinner:\n    \"\"\"\n    Enhanced Spinner with tqdm-like line rendering.\n    \"\"\"\n    SYMBOL_SETS = {\n        \"c\": [\"\u25d0\", \"\u25d3\", \"\u25d1\", \"\u25d2\"],\n        \"b\": [\"\u2581\", \"\u2583\", \"\u2584\", \"\u2585\", \"\u2586\", \"\u2587\", \"\u2588\", \"\u2587\", \"\u2586\", \"\u2585\", \"\u2584\", \"\u2583\"],\n        \"d\": [\"\u28fe\", \"\u28fd\", \"\u28fb\", \"\u28bf\", \"\u287f\", \"\u28df\", \"\u28ef\", \"\u28f7\"],\n        \"w\": [\"\ud83c\udf0d\", \"\ud83c\udf0e\", \"\ud83c\udf0f\"],\n        \"s\": [\"\ud83c\udf00   \", \" \ud83c\udf00  \", \"  \ud83c\udf00 \", \"   \ud83c\udf00\", \"  \ud83c\udf00 \", \" \ud83c\udf00  \"],\n        \"+\": [\"+\", \"x\"],\n        \"t\": [\"\u2736\", \"\u2738\", \"\u2739\", \"\u273a\", \"\u2739\", \"\u2737\"]\n    }\n\n    def __init__(\n        self,\n        message: str = \"Loading...\",\n        delay: float = 0.1,\n        symbols=None,\n        count_down: bool = False,\n        time_in_s: float = 0\n    ):\n        \"\"\"Initialize spinner with flexible configuration.\"\"\"\n        # Resolve symbol set.\n        if isinstance(symbols, str):\n            symbols = self.SYMBOL_SETS.get(symbols, None)\n\n        # Default symbols if not provided.\n        if symbols is None:\n            symbols = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n\n        # Test mode symbol set.\n        if 'unittest' in sys.argv[0]:\n            symbols = ['#', '=', '-']\n\n        self.spinner = itertools.cycle(symbols)\n        self.delay = delay\n        self.message = message\n        self.running = False\n        self.spinner_thread = None\n        self.max_t = time_in_s\n        self.contd = count_down\n\n        # Rendering management.\n        self._is_primary = False\n        self._start_time = 0\n\n        # Central manager.\n        self.manager = SpinnerManager()\n\n    def _generate_render_line(self):\n        \"\"\"Generate the primary render line.\"\"\"\n        current_time = time.time()\n        if self.contd:\n            remaining = max(0, self.max_t - (current_time - self._start_time))\n            time_display = f\"{remaining:.2f}\"\n        else:\n            time_display = f\"{current_time - self._start_time:.2f}\"\n\n        symbol = next(self.spinner)\n        return f\"{symbol} {self.message} | {time_display}\"\n\n    def _generate_secondary_info(self):\n        \"\"\"Generate secondary spinner info for additional spinners.\"\"\"\n        return f\"{self.message}\"\n\n    def __enter__(self):\n        \"\"\"Start the spinner.\"\"\"\n        self.running = True\n        self._start_time = time.time()\n        self.manager.register_spinner(self)\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        \"\"\"Stop the spinner.\"\"\"\n        self.running = False\n        self.manager.unregister_spinner(self)\n        # Clear the spinner's line if it was the primary spinner.\n        if self._is_primary:\n            sys.stdout.write(\"\\r\\033[K\")\n            sys.stdout.flush()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Spinner.__enter__","title":"<code>__enter__()</code>","text":"<p>Start the spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __enter__(self):\n    \"\"\"Start the spinner.\"\"\"\n    self.running = True\n    self._start_time = time.time()\n    self.manager.register_spinner(self)\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Spinner.__exit__","title":"<code>__exit__(exc_type, exc_value, exc_traceback)</code>","text":"<p>Stop the spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __exit__(self, exc_type, exc_value, exc_traceback):\n    \"\"\"Stop the spinner.\"\"\"\n    self.running = False\n    self.manager.unregister_spinner(self)\n    # Clear the spinner's line if it was the primary spinner.\n    if self._is_primary:\n        sys.stdout.write(\"\\r\\033[K\")\n        sys.stdout.flush()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Spinner.__init__","title":"<code>__init__(message='Loading...', delay=0.1, symbols=None, count_down=False, time_in_s=0)</code>","text":"<p>Initialize spinner with flexible configuration.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __init__(\n    self,\n    message: str = \"Loading...\",\n    delay: float = 0.1,\n    symbols=None,\n    count_down: bool = False,\n    time_in_s: float = 0\n):\n    \"\"\"Initialize spinner with flexible configuration.\"\"\"\n    # Resolve symbol set.\n    if isinstance(symbols, str):\n        symbols = self.SYMBOL_SETS.get(symbols, None)\n\n    # Default symbols if not provided.\n    if symbols is None:\n        symbols = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n\n    # Test mode symbol set.\n    if 'unittest' in sys.argv[0]:\n        symbols = ['#', '=', '-']\n\n    self.spinner = itertools.cycle(symbols)\n    self.delay = delay\n    self.message = message\n    self.running = False\n    self.spinner_thread = None\n    self.max_t = time_in_s\n    self.contd = count_down\n\n    # Rendering management.\n    self._is_primary = False\n    self._start_time = 0\n\n    # Central manager.\n    self.manager = SpinnerManager()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.remove_styles","title":"<code>toolboxv2.remove_styles(text, infos=False)</code>","text":"Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def remove_styles(text: str, infos=False):\n    in_ = []\n    for key, style in Style.style_dic.items():\n        if style in text:\n            text = text.replace(style, '')\n            if infos:\n                in_.append([key for key, st in Style.style_dic.items() if style == st][0])\n    if infos:\n        if \"END\" in in_:\n            in_.remove('END')\n        return text, in_\n    return text\n</code></pre>"},{"location":"toolboxv2/#data-types-structures","title":"Data Types &amp; Structures","text":""},{"location":"toolboxv2/#toolboxv2.AppArgs","title":"<code>toolboxv2.AppArgs</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class AppArgs:\n    init = None\n    init_file = 'init.config'\n    get_version = False\n    mm = False\n    sm = False\n    lm = False\n    modi = 'cli'\n    kill = False\n    remote = False\n    remote_direct_key = None\n    background_application = False\n    background_application_runner = False\n    docker = False\n    build = False\n    install = None\n    remove = None\n    update = None\n    name = 'main'\n    port = 5000\n    host = '0.0.0.0'\n    load_all_mod_in_files = False\n    mods_folder = 'toolboxv2.mods.'\n    debug = None\n    test = None\n    profiler = None\n    hot_reload = False\n    live_application = True\n    sysPrint = False\n    kwargs = {}\n    session = None\n\n    def default(self):\n        return self\n\n    def set(self, name, value):\n        setattr(self, name, value)\n        return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result","title":"<code>toolboxv2.Result</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class Result:\n    _task = None\n    def __init__(self,\n                 error: ToolBoxError,\n                 result: ToolBoxResult,\n                 info: ToolBoxInfo,\n                 origin: Any | None = None,\n                 ):\n        self.error: ToolBoxError = error\n        self.result: ToolBoxResult = result\n        self.info: ToolBoxInfo = info\n        self.origin = origin\n\n    def as_result(self):\n        return self\n\n    def as_dict(self):\n        return {\n            \"error\":self.error.value if isinstance(self.error, Enum) else self.error,\n        \"result\" : {\n            \"data_to\":self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n            \"data_info\":self.result.data_info,\n            \"data\":self.result.data,\n            \"data_type\":self.result.data_type\n        } if self.result else None,\n        \"info\" : {\n            \"exec_code\" : self.info.exec_code,  # exec_code umwandel in http resposn codes\n        \"help_text\" : self.info.help_text\n        } if self.info else None,\n        \"origin\" : self.origin\n        }\n\n    def set_origin(self, origin):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = origin\n        return self\n\n    def set_dir_origin(self, name, extras=\"assets/\"):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = f\"mods/{name}/{extras}\"\n        return self\n\n    def is_error(self):\n        if _test_is_result(self.result.data):\n            return self.result.data.is_error()\n        if self.error == ToolBoxError.none:\n            return False\n        if self.info.exec_code == 0:\n            return False\n        if self.info.exec_code == 200:\n            return False\n        return True\n\n    def is_data(self):\n        return self.result.data is not None\n\n    def to_api_result(self):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=self.error.value if isinstance(self.error, Enum) else self.error,\n            result=ToolBoxResultBM(\n                data_to=self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n                data_info=self.result.data_info,\n                data=self.result.data,\n                data_type=self.result.data_type\n            ) if self.result else None,\n            info=ToolBoxInfoBM(\n                exec_code=self.info.exec_code,  # exec_code umwandel in http resposn codes\n                help_text=self.info.help_text\n            ) if self.info else None,\n            origin=self.origin\n        )\n\n    def task(self, task):\n        self._task = task\n        return self\n\n    @staticmethod\n    def result_from_dict(error: str, result: dict, info: dict, origin: list or None or str):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=error if isinstance(error, Enum) else error,\n            result=ToolBoxResultBM(\n                data_to=result.get('data_to') if isinstance(result.get('data_to'), Enum) else result.get('data_to'),\n                data_info=result.get('data_info', '404'),\n                data=result.get('data'),\n                data_type=result.get('data_type', '404'),\n            ) if result else ToolBoxResultBM(\n                data_to=ToolBoxInterfaces.cli.value,\n                data_info='',\n                data='404',\n                data_type='404',\n            ),\n            info=ToolBoxInfoBM(\n                exec_code=info.get('exec_code', 404),\n                help_text=info.get('help_text', '404')\n            ) if info else ToolBoxInfoBM(\n                exec_code=404,\n                help_text='404'\n            ),\n            origin=origin\n        ).as_result()\n\n    @classmethod\n    def stream(cls,\n               stream_generator: Any,  # Renamed from source for clarity\n               content_type: str = \"text/event-stream\",  # Default to SSE\n               headers: Union[dict, None] = None,\n               info: str = \"OK\",\n               interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n               cleanup_func: Union[\n                   Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None):\n        \"\"\"\n        Create a streaming response Result. Handles SSE and other stream types.\n\n        Args:\n            stream_generator: Any stream source (async generator, sync generator, iterable, or single item).\n            content_type: Content-Type header (default: text/event-stream for SSE).\n            headers: Additional HTTP headers for the response.\n            info: Help text for the result.\n            interface: Interface to send data to.\n            cleanup_func: Optional function for cleanup.\n\n        Returns:\n            A Result object configured for streaming.\n        \"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        final_generator: AsyncGenerator[str, None]\n\n        if content_type == \"text/event-stream\":\n            # For SSE, always use SSEGenerator.create_sse_stream to wrap the source.\n            # SSEGenerator.create_sse_stream handles various types of stream_generator internally.\n            final_generator = SSEGenerator.create_sse_stream(source=stream_generator, cleanup_func=cleanup_func)\n\n            # Standard SSE headers for the HTTP response itself\n            # These will be stored in the Result object. Rust side decides how to use them.\n            standard_sse_headers = {\n                \"Cache-Control\": \"no-cache\",  # SSE specific\n                \"Connection\": \"keep-alive\",  # SSE specific\n                \"X-Accel-Buffering\": \"no\",  # Useful for proxies with SSE\n                # Content-Type is implicitly text/event-stream, will be in streaming_data below\n            }\n            all_response_headers = standard_sse_headers.copy()\n            if headers:\n                all_response_headers.update(headers)\n        else:\n            # For non-SSE streams.\n            # If stream_generator is sync, wrap it to be async.\n            # If already async or single item, it will be handled.\n            # Rust's stream_generator in ToolboxClient seems to handle both sync/async Python generators.\n            # For consistency with how SSEGenerator does it, we can wrap sync ones.\n            if inspect.isgenerator(stream_generator) or \\\n                (not isinstance(stream_generator, str) and hasattr(stream_generator, '__iter__')):\n                final_generator = SSEGenerator.wrap_sync_generator(stream_generator)  # Simple async wrapper\n            elif inspect.isasyncgen(stream_generator):\n                final_generator = stream_generator\n            else:  # Single item or string\n                async def _single_item_gen():\n                    yield stream_generator\n\n                final_generator = _single_item_gen()\n            all_response_headers = headers if headers else {}\n\n        # Prepare streaming data to be stored in the Result object\n        streaming_data = {\n            \"type\": \"stream\",  # Indicator for Rust side\n            \"generator\": final_generator,\n            \"content_type\": content_type,  # Let Rust know the intended content type\n            \"headers\": all_response_headers  # Intended HTTP headers for the overall response\n        }\n\n        result_payload = ToolBoxResult(\n            data_to=interface,\n            data=streaming_data,\n            data_info=\"Streaming response\" if content_type != \"text/event-stream\" else \"SSE Event Stream\",\n            data_type=\"stream\"  # Generic type for Rust to identify it needs to stream from 'generator'\n        )\n\n        return cls(error=error, info=info_obj, result=result_payload)\n\n    @classmethod\n    def sse(cls,\n            stream_generator: Any,\n            info: str = \"OK\",\n            interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n            cleanup_func: Union[\n                Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None,\n            # http_headers: Optional[dict] = None # If we want to allow overriding default SSE HTTP headers\n            ):\n        \"\"\"\n        Create an Server-Sent Events (SSE) streaming response Result.\n\n        Args:\n            stream_generator: A source yielding individual data items. This can be an\n                              async generator, sync generator, iterable, or a single item.\n                              Each item will be formatted as an SSE event.\n            info: Optional help text for the Result.\n            interface: Optional ToolBoxInterface to target.\n            cleanup_func: Optional cleanup function to run when the stream ends or is cancelled.\n            #http_headers: Optional dictionary of custom HTTP headers for the SSE response.\n\n        Returns:\n            A Result object configured for SSE streaming.\n        \"\"\"\n        # Result.stream will handle calling SSEGenerator.create_sse_stream\n        # and setting appropriate default headers for SSE when content_type is \"text/event-stream\".\n        return cls.stream(\n            stream_generator=stream_generator,\n            content_type=\"text/event-stream\",\n            # headers=http_headers, # Pass if we add http_headers param\n            info=info,\n            interface=interface,\n            cleanup_func=cleanup_func\n        )\n\n    @classmethod\n    def default(cls, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=-1, help_text=\"\")\n        result = ToolBoxResult(data_to=interface)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None):\n        \"\"\"Create a JSON response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code or exec_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=data,\n            data_info=\"JSON response\",\n            data_type=\"json\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n        \"\"\"Create a text response Result with specific content type.\"\"\"\n        if headers is not None:\n            return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=text_data,\n            data_info=\"Text response\",\n            data_type=content_type\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n               interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a binary data response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        # Create a dictionary with binary data and metadata\n        binary_data = {\n            \"data\": data,\n            \"content_type\": content_type,\n            \"filename\": download_name\n        }\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=binary_data,\n            data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n            data_type=\"binary\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def file(cls, data, filename, content_type=None, info=\"OK\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a file download response Result.\n\n        Args:\n            data: File data as bytes or base64 string\n            filename: Name of the file for download\n            content_type: MIME type of the file (auto-detected if None)\n            info: Response info text\n            interface: Target interface\n\n        Returns:\n            Result object configured for file download\n        \"\"\"\n        import base64\n        import mimetypes\n\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=200, help_text=info)\n\n        # Auto-detect content type if not provided\n        if content_type is None:\n            content_type, _ = mimetypes.guess_type(filename)\n            if content_type is None:\n                content_type = \"application/octet-stream\"\n\n        # Ensure data is base64 encoded string (as expected by Rust server)\n        if isinstance(data, bytes):\n            base64_data = base64.b64encode(data).decode('utf-8')\n        elif isinstance(data, str):\n            # Assume it's already base64 encoded\n            base64_data = data\n        else:\n            raise ValueError(\"File data must be bytes or base64 string\")\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=base64_data,  # Rust expects base64 string for \"file\" type\n            data_info=f\"File download: {filename}\",\n            data_type=\"file\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a redirect response.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=url,\n            data_info=\"Redirect response\",\n            data_type=\"redirect\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def ok(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def html(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.remote, data_type=\"html\",status=200, headers=None, row=False):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=status, help_text=info)\n        from ...utils.system.getting_and_closing_app import get_app\n\n        if not row and not '\"&lt;div class=\"main-content\"\"' in data:\n            data = f'&lt;div class=\"main-content frosted-glass\"&gt;{data}&lt;div&gt;'\n        if not row and not get_app().web_context() in data:\n            data = get_app().web_context() + data\n\n        if isinstance(headers, dict):\n            result = ToolBoxResult(data_to=interface, data={'html':data,'headers':headers}, data_info=data_info,\n                                   data_type=\"special_html\")\n        else:\n            result = ToolBoxResult(data_to=interface, data=data, data_info=data_info,\n                                   data_type=data_type if data_type is not None else type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def future(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.future):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=\"future\")\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def custom_error(cls, data=None, data_info=\"\", info=\"\", exec_code=-1, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def error(cls, data=None, data_info=\"\", info=\"\", exec_code=450, interface=ToolBoxInterfaces.remote):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_user_error(cls, info=\"\", exec_code=-3, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.input_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_internal_error(cls, info=\"\", exec_code=-2, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.internal_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    def print(self, show=True, show_data=True, prifix=\"\"):\n        data = '\\n' + f\"{((prifix + 'Data: ' + str(self.result.data) if self.result.data is not None else 'NO Data') if not isinstance(self.result.data, Result) else self.result.data.print(show=False, show_data=show_data, prifix=prifix + '-')) if show_data else 'Data: private'}\"\n        origin = '\\n' + f\"{prifix + 'Origin: ' + str(self.origin) if self.origin is not None else 'NO Origin'}\"\n        text = (f\"Function Exec code: {self.info.exec_code}\"\n                f\"\\n{prifix}Info's:\"\n                f\" {self.info.help_text} {'&lt;|&gt; ' + str(self.result.data_info) if self.result.data_info is not None else ''}\"\n                f\"{origin}{data if not data.endswith('NO Data') else ''}\")\n        if not show:\n            return text\n        print(\"\\n======== Result ========\\n\" + text + \"\\n------- EndOfD -------\")\n        return self\n\n    def log(self, show_data=True, prifix=\"\"):\n        from toolboxv2 import get_logger\n        get_logger().debug(self.print(show=False, show_data=show_data, prifix=prifix).replace(\"\\n\", \" - \"))\n        return self\n\n    def __str__(self):\n        return self.print(show=False, show_data=True)\n\n    def get(self, key=None, default=None):\n        data = self.result.data\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    async def aget(self, key=None, default=None):\n        if asyncio.isfuture(self.result.data) or asyncio.iscoroutine(self.result.data) or (\n            isinstance(self.result.data_to, Enum) and self.result.data_to.name == ToolBoxInterfaces.future.name):\n            data = await self.result.data\n        else:\n            data = self.get(key=None, default=None)\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    def lazy_return(self, _=0, data=None, **kwargs):\n        flags = ['raise', 'logg', 'user', 'intern']\n        flag = flags[_] if isinstance(_, int) else _\n        if self.info.exec_code == 0:\n            return self if data is None else data if _test_is_result(data) else self.ok(data=data, **kwargs)\n        if flag == 'raise':\n            raise ValueError(self.print(show=False))\n        if flag == 'logg':\n            from .. import get_logger\n            get_logger().error(self.print(show=False))\n\n        if flag == 'user':\n            return self if data is None else data if _test_is_result(data) else self.default_user_error(data=data,\n                                                                                                        **kwargs)\n        if flag == 'intern':\n            return self if data is None else data if _test_is_result(data) else self.default_internal_error(data=data,\n                                                                                                            **kwargs)\n\n        return self if data is None else data if _test_is_result(data) else self.custom_error(data=data, **kwargs)\n\n    @property\n    def bg_task(self):\n        return self._task\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.binary","title":"<code>binary(data, content_type='application/octet-stream', download_name=None, info='OK', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a binary data response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n           interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a binary data response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    # Create a dictionary with binary data and metadata\n    binary_data = {\n        \"data\": data,\n        \"content_type\": content_type,\n        \"filename\": download_name\n    }\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=binary_data,\n        data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n        data_type=\"binary\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.file","title":"<code>file(data, filename, content_type=None, info='OK', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a file download response Result.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>File data as bytes or base64 string</p> required <code>filename</code> <p>Name of the file for download</p> required <code>content_type</code> <p>MIME type of the file (auto-detected if None)</p> <code>None</code> <code>info</code> <p>Response info text</p> <code>'OK'</code> <code>interface</code> <p>Target interface</p> <code>remote</code> <p>Returns:</p> Type Description <p>Result object configured for file download</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef file(cls, data, filename, content_type=None, info=\"OK\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a file download response Result.\n\n    Args:\n        data: File data as bytes or base64 string\n        filename: Name of the file for download\n        content_type: MIME type of the file (auto-detected if None)\n        info: Response info text\n        interface: Target interface\n\n    Returns:\n        Result object configured for file download\n    \"\"\"\n    import base64\n    import mimetypes\n\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=200, help_text=info)\n\n    # Auto-detect content type if not provided\n    if content_type is None:\n        content_type, _ = mimetypes.guess_type(filename)\n        if content_type is None:\n            content_type = \"application/octet-stream\"\n\n    # Ensure data is base64 encoded string (as expected by Rust server)\n    if isinstance(data, bytes):\n        base64_data = base64.b64encode(data).decode('utf-8')\n    elif isinstance(data, str):\n        # Assume it's already base64 encoded\n        base64_data = data\n    else:\n        raise ValueError(\"File data must be bytes or base64 string\")\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=base64_data,  # Rust expects base64 string for \"file\" type\n        data_info=f\"File download: {filename}\",\n        data_type=\"file\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.json","title":"<code>json(data, info='OK', interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None)</code>  <code>classmethod</code>","text":"<p>Create a JSON response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None):\n    \"\"\"Create a JSON response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code or exec_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=data,\n        data_info=\"JSON response\",\n        data_type=\"json\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.redirect","title":"<code>redirect(url, status_code=302, info='Redirect', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a redirect response.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a redirect response.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=url,\n        data_info=\"Redirect response\",\n        data_type=\"redirect\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.sse","title":"<code>sse(stream_generator, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code>  <code>classmethod</code>","text":"<p>Create an Server-Sent Events (SSE) streaming response Result.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <code>Any</code> <p>A source yielding individual data items. This can be an               async generator, sync generator, iterable, or a single item.               Each item will be formatted as an SSE event.</p> required <code>info</code> <code>str</code> <p>Optional help text for the Result.</p> <code>'OK'</code> <code>interface</code> <code>ToolBoxInterfaces</code> <p>Optional ToolBoxInterface to target.</p> <code>remote</code> <code>cleanup_func</code> <code>Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None]</code> <p>Optional cleanup function to run when the stream ends or is cancelled.</p> <code>None</code> <code>#http_headers</code> <p>Optional dictionary of custom HTTP headers for the SSE response.</p> required <p>Returns:</p> Type Description <p>A Result object configured for SSE streaming.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef sse(cls,\n        stream_generator: Any,\n        info: str = \"OK\",\n        interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n        cleanup_func: Union[\n            Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None,\n        # http_headers: Optional[dict] = None # If we want to allow overriding default SSE HTTP headers\n        ):\n    \"\"\"\n    Create an Server-Sent Events (SSE) streaming response Result.\n\n    Args:\n        stream_generator: A source yielding individual data items. This can be an\n                          async generator, sync generator, iterable, or a single item.\n                          Each item will be formatted as an SSE event.\n        info: Optional help text for the Result.\n        interface: Optional ToolBoxInterface to target.\n        cleanup_func: Optional cleanup function to run when the stream ends or is cancelled.\n        #http_headers: Optional dictionary of custom HTTP headers for the SSE response.\n\n    Returns:\n        A Result object configured for SSE streaming.\n    \"\"\"\n    # Result.stream will handle calling SSEGenerator.create_sse_stream\n    # and setting appropriate default headers for SSE when content_type is \"text/event-stream\".\n    return cls.stream(\n        stream_generator=stream_generator,\n        content_type=\"text/event-stream\",\n        # headers=http_headers, # Pass if we add http_headers param\n        info=info,\n        interface=interface,\n        cleanup_func=cleanup_func\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.stream","title":"<code>stream(stream_generator, content_type='text/event-stream', headers=None, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code>  <code>classmethod</code>","text":"<p>Create a streaming response Result. Handles SSE and other stream types.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <code>Any</code> <p>Any stream source (async generator, sync generator, iterable, or single item).</p> required <code>content_type</code> <code>str</code> <p>Content-Type header (default: text/event-stream for SSE).</p> <code>'text/event-stream'</code> <code>headers</code> <code>Union[dict, None]</code> <p>Additional HTTP headers for the response.</p> <code>None</code> <code>info</code> <code>str</code> <p>Help text for the result.</p> <code>'OK'</code> <code>interface</code> <code>ToolBoxInterfaces</code> <p>Interface to send data to.</p> <code>remote</code> <code>cleanup_func</code> <code>Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None]</code> <p>Optional function for cleanup.</p> <code>None</code> <p>Returns:</p> Type Description <p>A Result object configured for streaming.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef stream(cls,\n           stream_generator: Any,  # Renamed from source for clarity\n           content_type: str = \"text/event-stream\",  # Default to SSE\n           headers: Union[dict, None] = None,\n           info: str = \"OK\",\n           interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n           cleanup_func: Union[\n               Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None):\n    \"\"\"\n    Create a streaming response Result. Handles SSE and other stream types.\n\n    Args:\n        stream_generator: Any stream source (async generator, sync generator, iterable, or single item).\n        content_type: Content-Type header (default: text/event-stream for SSE).\n        headers: Additional HTTP headers for the response.\n        info: Help text for the result.\n        interface: Interface to send data to.\n        cleanup_func: Optional function for cleanup.\n\n    Returns:\n        A Result object configured for streaming.\n    \"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    final_generator: AsyncGenerator[str, None]\n\n    if content_type == \"text/event-stream\":\n        # For SSE, always use SSEGenerator.create_sse_stream to wrap the source.\n        # SSEGenerator.create_sse_stream handles various types of stream_generator internally.\n        final_generator = SSEGenerator.create_sse_stream(source=stream_generator, cleanup_func=cleanup_func)\n\n        # Standard SSE headers for the HTTP response itself\n        # These will be stored in the Result object. Rust side decides how to use them.\n        standard_sse_headers = {\n            \"Cache-Control\": \"no-cache\",  # SSE specific\n            \"Connection\": \"keep-alive\",  # SSE specific\n            \"X-Accel-Buffering\": \"no\",  # Useful for proxies with SSE\n            # Content-Type is implicitly text/event-stream, will be in streaming_data below\n        }\n        all_response_headers = standard_sse_headers.copy()\n        if headers:\n            all_response_headers.update(headers)\n    else:\n        # For non-SSE streams.\n        # If stream_generator is sync, wrap it to be async.\n        # If already async or single item, it will be handled.\n        # Rust's stream_generator in ToolboxClient seems to handle both sync/async Python generators.\n        # For consistency with how SSEGenerator does it, we can wrap sync ones.\n        if inspect.isgenerator(stream_generator) or \\\n            (not isinstance(stream_generator, str) and hasattr(stream_generator, '__iter__')):\n            final_generator = SSEGenerator.wrap_sync_generator(stream_generator)  # Simple async wrapper\n        elif inspect.isasyncgen(stream_generator):\n            final_generator = stream_generator\n        else:  # Single item or string\n            async def _single_item_gen():\n                yield stream_generator\n\n            final_generator = _single_item_gen()\n        all_response_headers = headers if headers else {}\n\n    # Prepare streaming data to be stored in the Result object\n    streaming_data = {\n        \"type\": \"stream\",  # Indicator for Rust side\n        \"generator\": final_generator,\n        \"content_type\": content_type,  # Let Rust know the intended content type\n        \"headers\": all_response_headers  # Intended HTTP headers for the overall response\n    }\n\n    result_payload = ToolBoxResult(\n        data_to=interface,\n        data=streaming_data,\n        data_info=\"Streaming response\" if content_type != \"text/event-stream\" else \"SSE Event Stream\",\n        data_type=\"stream\"  # Generic type for Rust to identify it needs to stream from 'generator'\n    )\n\n    return cls(error=error, info=info_obj, result=result_payload)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.text","title":"<code>text(text_data, content_type='text/plain', exec_code=None, status=200, info='OK', interface=ToolBoxInterfaces.remote, headers=None)</code>  <code>classmethod</code>","text":"<p>Create a text response Result with specific content type.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n    \"\"\"Create a text response Result with specific content type.\"\"\"\n    if headers is not None:\n        return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=text_data,\n        data_info=\"Text response\",\n        data_type=content_type\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.ApiResult","title":"<code>toolboxv2.ApiResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class ApiResult(BaseModel):\n    error: None | str= None\n    origin: Any | None\n    result: ToolBoxResultBM | None = None\n    info: ToolBoxInfoBM | None\n\n    def as_result(self):\n        return Result(\n            error=self.error.value if isinstance(self.error, Enum) else self.error,\n            result=ToolBoxResult(\n                data_to=self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n                data_info=self.result.data_info,\n                data=self.result.data,\n                data_type=self.result.data_type\n            ) if self.result else None,\n            info=ToolBoxInfo(\n                exec_code=self.info.exec_code,\n                help_text=self.info.help_text\n            ) if self.info else None,\n            origin=self.origin\n        )\n\n    def to_api_result(self):\n        return self\n\n    def print(self, *args, **kwargs):\n        res = self.as_result().print(*args, **kwargs)\n        if not isinstance(res, str):\n            res = res.to_api_result()\n        return res\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.RequestData","title":"<code>toolboxv2.RequestData</code>  <code>dataclass</code>","text":"<p>Main class representing the complete request data structure.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@dataclass\nclass RequestData:\n    \"\"\"Main class representing the complete request data structure.\"\"\"\n    request: Request\n    session: Session\n    session_id: str\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any]) -&gt; 'RequestData':\n        \"\"\"Create a RequestData instance from a dictionary.\"\"\"\n        return cls(\n            request=Request.from_dict(data.get('request', {})),\n            session=Session.from_dict(data.get('session', {})),\n            session_id=data.get('session_id', '')\n        )\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert the RequestData object back to a dictionary.\"\"\"\n        return {\n            'request': self.request.to_dict(),\n            'session': self.session.to_dict(),\n            'session_id': self.session_id\n        }\n\n    def __getattr__(self, name: str) -&gt; Any:\n        \"\"\"Delegate unknown attributes to the `request` object.\"\"\"\n        # Nur wenn das Attribut nicht direkt in RequestData existiert\n        # und auch nicht `session` oder `session_id` ist\n        if hasattr(self.request, name):\n            return getattr(self.request, name)\n        raise AttributeError(f\"'RequestData' object has no attribute '{name}'\")\n\n    @classmethod\n    def moc(cls):\n        return cls(\n            request=Request.from_dict({\n                'content_type': 'application/x-www-form-urlencoded',\n                'headers': {\n                    'accept': '*/*',\n                    'accept-encoding': 'gzip, deflate, br, zstd',\n                    'accept-language': 'de-DE,de;q=0.9,en-US;q=0.8,en;q=0.7',\n                    'connection': 'keep-alive',\n                    'content-length': '107',\n                    'content-type': 'application/x-www-form-urlencoded',\n                    'cookie': 'session=abc123',\n                    'host': 'localhost:8080',\n                    'hx-current-url': 'http://localhost:8080/api/TruthSeeker/get_main_ui',\n                    'hx-request': 'true',\n                    'hx-target': 'estimates-guest_1fc2c9',\n                    'hx-trigger': 'config-form-guest_1fc2c9',\n                    'origin': 'http://localhost:8080',\n                    'referer': 'http://localhost:8080/api/TruthSeeker/get_main_ui',\n                    'sec-ch-ua': '\"Chromium\";v=\"134\", \"Not:A-Brand\";v=\"24\", \"Google Chrome\";v=\"134\"',\n                    'sec-ch-ua-mobile': '?0',\n                    'sec-ch-ua-platform': '\"Windows\"',\n                    'sec-fetch-dest': 'empty',\n                    'sec-fetch-mode': 'cors',\n                    'sec-fetch-site': 'same-origin',\n                    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n                },\n                'method': 'POST',\n                'path': '/api/TruthSeeker/update_estimates',\n                'query_params': {},\n                'form_data': {\n                    'param1': 'value1',\n                    'param2': 'value2'\n                }\n            }),\n            session=Session.from_dict({\n                'SiID': '29a2e258e18252e2afd5ff943523f09c82f1bb9adfe382a6f33fc6a8381de898',\n                'level': '1',\n                'spec': '74eed1c8de06886842e235486c3c2fd6bcd60586998ac5beb87f13c0d1750e1d',\n                'user_name': 'root',\n                'custom_field': 'custom_value'\n            }),\n            session_id='0x29dd1ac0d1e30d3f'\n        )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.RequestData.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Delegate unknown attributes to the <code>request</code> object.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"Delegate unknown attributes to the `request` object.\"\"\"\n    # Nur wenn das Attribut nicht direkt in RequestData existiert\n    # und auch nicht `session` oder `session_id` ist\n    if hasattr(self.request, name):\n        return getattr(self.request, name)\n    raise AttributeError(f\"'RequestData' object has no attribute '{name}'\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.RequestData.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create a RequestData instance from a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; 'RequestData':\n    \"\"\"Create a RequestData instance from a dictionary.\"\"\"\n    return cls(\n        request=Request.from_dict(data.get('request', {})),\n        session=Session.from_dict(data.get('session', {})),\n        session_id=data.get('session_id', '')\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.RequestData.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert the RequestData object back to a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert the RequestData object back to a dictionary.\"\"\"\n    return {\n        'request': self.request.to_dict(),\n        'session': self.session.to_dict(),\n        'session_id': self.session_id\n    }\n</code></pre>"},{"location":"toolboxv2/#security","title":"Security","text":""},{"location":"toolboxv2/#toolboxv2.Code","title":"<code>toolboxv2.Code</code>","text":"Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>class Code:\n\n    @staticmethod\n    def DK():\n        return DEVICE_KEY\n\n    def decode_code(self, encrypted_data, key=None):\n\n        if not isinstance(encrypted_data, str):\n            encrypted_data = str(encrypted_data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.decrypt_symmetric(encrypted_data, key)\n\n    def encode_code(self, data, key=None):\n\n        if not isinstance(data, str):\n            data = str(data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.encrypt_symmetric(data, key)\n\n    @staticmethod\n    def generate_seed() -&gt; int:\n        \"\"\"\n        Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n        Returns:\n            int: Eine zuf\u00e4llige Zahl.\n        \"\"\"\n        return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n\n    @staticmethod\n    def one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n        \"\"\"\n        Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n        Args:\n            text (str): Der zu hashende Text.\n            salt (str): Der Salt-Wert.\n            pepper (str): Der Pepper-Wert.\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            str: Der resultierende Hash-Wert.\n        \"\"\"\n        return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n\n    @staticmethod\n    def generate_symmetric_key(as_str=True) -&gt; str or bytes:\n        \"\"\"\n        Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n        Returns:\n            str: Der generierte Schl\u00fcssel.\n        \"\"\"\n        key = Fernet.generate_key()\n        if as_str:\n            key = key.decode()\n        return key\n\n    @staticmethod\n    def encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        if isinstance(text, str):\n            text = text.encode()\n\n        try:\n            fernet = Fernet(key.encode())\n            return fernet.encrypt(text).decode()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n            return \"Error encrypt\"\n\n    @staticmethod\n    def decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n            to_str (bool): default true returns str if false returns bytes\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n\n        if isinstance(key, str):\n            key = key.encode()\n\n        #try:\n        fernet = Fernet(key)\n        text_b = fernet.decrypt(encrypted_text)\n        if not to_str:\n            return text_b\n        return text_b.decode()\n        # except Exception as e:\n        #     get_logger().error(f\"Error decrypt_symmetric {e}\")\n        #     if not mute:\n        #         raise e\n        #     if not to_str:\n        #         return f\"Error decoding\".encode()\n        #     return f\"Error decoding\"\n\n    @staticmethod\n    def generate_asymmetric_keys() -&gt; (str, str):\n        \"\"\"\n        Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n        Args:\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n        \"\"\"\n        private_key = rsa.generate_private_key(\n            public_exponent=65537,\n            key_size=2048 * 3,\n        )\n        public_key = private_key.public_key()\n\n        # Serialisieren der Schl\u00fcssel\n        pem_private_key = private_key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.NoEncryption()\n        ).decode()\n\n        pem_public_key = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        ).decode()\n\n        return pem_public_key, pem_private_key\n\n    @staticmethod\n    def save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n        \"\"\"\n        Speichert die generierten Schl\u00fcssel in separate Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n        Args:\n            public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n            private_key (str): Der private Schl\u00fcssel im PEM-Format\n            directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n        \"\"\"\n        # Erstelle das Verzeichnis, falls es nicht existiert\n        os.makedirs(directory, exist_ok=True)\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n        encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n        # Speichere den \u00f6ffentlichen Schl\u00fcssel\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        with open(public_key_path, \"w\") as f:\n            f.write(public_key)\n\n        # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n        with open(private_key_path, \"w\") as f:\n            f.write(encrypted_private_key)\n\n        print(\"Saved keys in \", public_key_path)\n\n    @staticmethod\n    def load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n        \"\"\"\n        L\u00e4dt die Schl\u00fcssel aus den Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n        Args:\n            directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n        Raises:\n            FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n        \"\"\"\n        # Pfade zu den Schl\u00fcsseldateien\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n\n        # Pr\u00fcfe ob die Dateien existieren\n        if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n            return \"\", \"\"\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Lade den \u00f6ffentlichen Schl\u00fcssel\n        with open(public_key_path) as f:\n            public_key = f.read()\n\n        # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n        with open(private_key_path) as f:\n            encrypted_private_key = f.read()\n            private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n        return public_key, private_key\n\n    @staticmethod\n    def encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        # try:\n        #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        #  except Exception as e:\n        #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            encrypted = public_key.encrypt(\n                text.encode(),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return encrypted.hex()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_asymmetric {e}\")\n            return \"Invalid\"\n\n    @staticmethod\n    def decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n        Args:\n            encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n            private_key_str (str): Der private Schl\u00fcssel als String.\n\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            decrypted = private_key.decrypt(\n                bytes.fromhex(encrypted_text_hex),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return decrypted.decode()\n\n        except Exception as e:\n            get_logger().error(f\"Error decrypt_asymmetric {e}\")\n        return \"Invalid\"\n\n    @staticmethod\n    def verify_signature(signature: str or bytes, message: str or bytes, public_key_str: str,\n                         salt_length=padding.PSS.MAX_LENGTH) -&gt; bool:\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                padding=padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                algorithm=hashes.SHA512()\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def verify_signature_web_algo(signature: str or bytes, message: str or bytes, public_key_str: str,\n                                  algo: int = -512) -&gt; bool:\n        signature_algorithm = ECDSA(hashes.SHA512())\n        if algo != -512:\n            signature_algorithm = ECDSA(hashes.SHA256())\n\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                # padding=padding.PSS(\n                #    mgf=padding.MGF1(hashes.SHA512()),\n                #    salt_length=padding.PSS.MAX_LENGTH\n                # ),\n                signature_algorithm=signature_algorithm\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def create_signature(message: str, private_key_str: str, salt_length=padding.PSS.MAX_LENGTH,\n                         row=False) -&gt; str or bytes:\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            signature = private_key.sign(\n                message.encode(),\n                padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                hashes.SHA512()\n            )\n            if row:\n                return signature\n            return base64.b64encode(signature).decode()\n        except Exception as e:\n            get_logger().error(f\"Error create_signature {e}\")\n            print(e)\n        return \"Invalid Key\"\n\n    @staticmethod\n    def pem_to_public_key(pem_key: str):\n        \"\"\"\n        Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n        Args:\n            pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n        Returns:\n            PublicKey: Das PublicKey-Objekt.\n        \"\"\"\n        public_key = serialization.load_pem_public_key(pem_key.encode())\n        return public_key\n\n    @staticmethod\n    def public_key_to_pem(public_key: RSAPublicKey):\n        \"\"\"\n        Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n        Args:\n            public_key (PublicKey): Das PublicKey-Objekt.\n\n        Returns:\n            str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n        \"\"\"\n        pem = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n        return pem.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.decrypt_asymmetric","title":"<code>decrypt_asymmetric(encrypted_text_hex, private_key_str)</code>  <code>staticmethod</code>","text":"<p>Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text_hex</code> <code>str</code> <p>Der verschl\u00fcsselte Text als Hex-String.</p> required <code>private_key_str</code> <code>str</code> <p>Der private Schl\u00fcssel als String.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n    Args:\n        encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n        private_key_str (str): Der private Schl\u00fcssel als String.\n\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n    try:\n        private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n        decrypted = private_key.decrypt(\n            bytes.fromhex(encrypted_text_hex),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return decrypted.decode()\n\n    except Exception as e:\n        get_logger().error(f\"Error decrypt_asymmetric {e}\")\n    return \"Invalid\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.decrypt_symmetric","title":"<code>decrypt_symmetric(encrypted_text, key, to_str=True, mute=False)</code>  <code>staticmethod</code>","text":"<p>Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text</code> <code>str</code> <p>Der zu entschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <code>to_str</code> <code>bool</code> <p>default true returns str if false returns bytes</p> <code>True</code> <p>Returns:     str: Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n        to_str (bool): default true returns str if false returns bytes\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n\n    if isinstance(key, str):\n        key = key.encode()\n\n    #try:\n    fernet = Fernet(key)\n    text_b = fernet.decrypt(encrypted_text)\n    if not to_str:\n        return text_b\n    return text_b.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.encrypt_asymmetric","title":"<code>encrypt_asymmetric(text, public_key_str)</code>  <code>staticmethod</code>","text":"<p>Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>public_key_str</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    # try:\n    #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n    #  except Exception as e:\n    #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n    try:\n        public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        encrypted = public_key.encrypt(\n            text.encode(),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return encrypted.hex()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        return \"Invalid\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.encrypt_symmetric","title":"<code>encrypt_symmetric(text, key)</code>  <code>staticmethod</code>","text":"<p>Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    if isinstance(text, str):\n        text = text.encode()\n\n    try:\n        fernet = Fernet(key.encode())\n        return fernet.encrypt(text).decode()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n        return \"Error encrypt\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.generate_asymmetric_keys","title":"<code>generate_asymmetric_keys()</code>  <code>staticmethod</code>","text":"<p>Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_asymmetric_keys() -&gt; (str, str):\n    \"\"\"\n    Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n    Args:\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n    \"\"\"\n    private_key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048 * 3,\n    )\n    public_key = private_key.public_key()\n\n    # Serialisieren der Schl\u00fcssel\n    pem_private_key = private_key.private_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PrivateFormat.PKCS8,\n        encryption_algorithm=serialization.NoEncryption()\n    ).decode()\n\n    pem_public_key = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    ).decode()\n\n    return pem_public_key, pem_private_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.generate_seed","title":"<code>generate_seed()</code>  <code>staticmethod</code>","text":"<p>Erzeugt eine zuf\u00e4llige Zahl als Seed.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Eine zuf\u00e4llige Zahl.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_seed() -&gt; int:\n    \"\"\"\n    Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n    Returns:\n        int: Eine zuf\u00e4llige Zahl.\n    \"\"\"\n    return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.generate_symmetric_key","title":"<code>generate_symmetric_key(as_str=True)</code>  <code>staticmethod</code>","text":"<p>Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str or bytes</code> <p>Der generierte Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_symmetric_key(as_str=True) -&gt; str or bytes:\n    \"\"\"\n    Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n    Returns:\n        str: Der generierte Schl\u00fcssel.\n    \"\"\"\n    key = Fernet.generate_key()\n    if as_str:\n        key = key.decode()\n    return key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.load_keys_from_files","title":"<code>load_keys_from_files(directory='keys')</code>  <code>staticmethod</code>","text":"<p>L\u00e4dt die Schl\u00fcssel aus den Dateien. Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen</p> <code>'keys'</code> <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n    \"\"\"\n    L\u00e4dt die Schl\u00fcssel aus den Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n    Args:\n        directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n    Raises:\n        FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n    \"\"\"\n    # Pfade zu den Schl\u00fcsseldateien\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n\n    # Pr\u00fcfe ob die Dateien existieren\n    if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n        return \"\", \"\"\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Lade den \u00f6ffentlichen Schl\u00fcssel\n    with open(public_key_path) as f:\n        public_key = f.read()\n\n    # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n    with open(private_key_path) as f:\n        encrypted_private_key = f.read()\n        private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n    return public_key, private_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.one_way_hash","title":"<code>one_way_hash(text, salt='', pepper='')</code>  <code>staticmethod</code>","text":"<p>Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu hashende Text.</p> required <code>salt</code> <code>str</code> <p>Der Salt-Wert.</p> <code>''</code> <code>pepper</code> <code>str</code> <p>Der Pepper-Wert.</p> <code>''</code> <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der resultierende Hash-Wert.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n    \"\"\"\n    Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n    Args:\n        text (str): Der zu hashende Text.\n        salt (str): Der Salt-Wert.\n        pepper (str): Der Pepper-Wert.\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        str: Der resultierende Hash-Wert.\n    \"\"\"\n    return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.pem_to_public_key","title":"<code>pem_to_public_key(pem_key)</code>  <code>staticmethod</code>","text":"<p>Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.</p> <p>Parameters:</p> Name Type Description Default <code>pem_key</code> <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef pem_to_public_key(pem_key: str):\n    \"\"\"\n    Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n    Args:\n        pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n    Returns:\n        PublicKey: Das PublicKey-Objekt.\n    \"\"\"\n    public_key = serialization.load_pem_public_key(pem_key.encode())\n    return public_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.public_key_to_pem","title":"<code>public_key_to_pem(public_key)</code>  <code>staticmethod</code>","text":"<p>Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef public_key_to_pem(public_key: RSAPublicKey):\n    \"\"\"\n    Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n    Args:\n        public_key (PublicKey): Das PublicKey-Objekt.\n\n    Returns:\n        str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n    \"\"\"\n    pem = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    )\n    return pem.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.save_keys_to_files","title":"<code>save_keys_to_files(public_key, private_key, directory='keys')</code>  <code>staticmethod</code>","text":"<p>Speichert die generierten Schl\u00fcssel in separate Dateien. Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel im PEM-Format</p> required <code>private_key</code> <code>str</code> <p>Der private Schl\u00fcssel im PEM-Format</p> required <code>directory</code> <code>str</code> <p>Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen</p> <code>'keys'</code> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n    \"\"\"\n    Speichert die generierten Schl\u00fcssel in separate Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n    Args:\n        public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n        private_key (str): Der private Schl\u00fcssel im PEM-Format\n        directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n    \"\"\"\n    # Erstelle das Verzeichnis, falls es nicht existiert\n    os.makedirs(directory, exist_ok=True)\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n    encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n    # Speichere den \u00f6ffentlichen Schl\u00fcssel\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    with open(public_key_path, \"w\") as f:\n        f.write(public_key)\n\n    # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n    with open(private_key_path, \"w\") as f:\n        f.write(encrypted_private_key)\n\n    print(\"Saved keys in \", public_key_path)\n</code></pre>"},{"location":"toolboxv2/#modules-flows","title":"Modules &amp; Flows","text":""},{"location":"toolboxv2/#toolboxv2.mods","title":"<code>toolboxv2.mods</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.Canvas","title":"<code>Canvas</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.Canvas.Tools","title":"<code>Tools</code>","text":"<p>               Bases: <code>MainTool</code></p> Source code in <code>toolboxv2/mods/Canvas.py</code> <pre><code>class Tools(MainTool):  # Removed EventManager for simplicity, as it was causing the issue. Direct SSE is better here.\n    def __init__(self, app: App):\n        self.name = MOD_NAME\n        self.version = VERSION\n        self.color = \"GREEN\"\n        self.tools_dict = {\"name\": MOD_NAME, \"Version\": self.show_version}\n\n        # Canvas specific state\n        self.live_canvas_sessions: Dict[str, List[asyncio.Queue]] = defaultdict(list)\n        self.active_user_previews: Dict[str, Dict[str, Any]] = defaultdict(dict)\n        self.previews_lock = asyncio.Lock()\n\n        MainTool.__init__(self, load=on_start, v=self.version, tool=self.tools_dict, name=self.name,\n                          color=self.color, app=app)\n        self.app.logger.info(f\"Canvas Tools (v{self.version}) initialized for app {self.app.id}.\")\n\n    @property\n    def db_mod(self):\n        db = self.app.get_mod(\"DB\", spec=Name)\n        if db.mode.value != \"CLUSTER_BLOB\":\n            db.edit_cli(\"CB\")\n        return db\n\n    def _broadcast_to_canvas_listeners(self, canvas_id: str, event_type: str, data: Dict[str, Any],\n                                       originator_user_id: Optional[str] = None):\n        \"\"\"\n        Creates a broadcast coroutine and submits it to the app's dedicated\n        async manager to be run in the background.\n        This is now a non-blocking fire-and-forget operation.\n        \"\"\"\n\n        async def broadcast_coro():\n            if canvas_id not in self.live_canvas_sessions:\n                return\n\n            message_obj = {\n                \"event\": event_type,\n                \"data\": json.dumps({\n                    \"canvas_id\": canvas_id,\n                    \"originator_user_id\": originator_user_id,\n                    **data\n                })\n            }\n\n            listeners = list(self.live_canvas_sessions.get(canvas_id, []))\n\n            for q in listeners:\n                try:\n                    # Non-blocking put. If the queue is full, the client is lagging,\n                    # and it's better to drop a message than to block the server.\n                    q.put_nowait(message_obj)\n                except asyncio.QueueFull:\n                    self.app.logger.warning(\n                        f\"SSE queue full for canvas {canvas_id}. Message '{event_type}' dropped for one client.\")\n                except Exception as e:\n                    self.app.logger.error(f\"Error putting message on SSE queue: {e}\")\n\n        # Use the app's robust background runner to execute immediately and not block the caller.\n        self.app.run_bg_task(broadcast_coro)\n\n    def show_version(self):\n        self.app.logger.info(f\"{self.name} Version: {self.version}\")\n        return self.version\n\n    async def _get_user_specific_db_key(self, request: RequestData, base_key: str) -&gt; Optional[str]:\n        # This logic is correct and can remain as is.\n\n        user = await get_user_from_request(self.app, request)\n        if user and user.uid:\n            return f\"{base_key}_{user.uid}\"\n        self.print(\"ok\")\n        # Fallback for public/guest access if you want to support it\n        return f\"{base_key}_public\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.Canvas.handle_send_canvas_action","title":"<code>handle_send_canvas_action(app, request, data)</code>  <code>async</code>","text":"<p>Handles incremental, real-time actions from clients (e.g., adding an element). It persists the change to the database and then broadcasts it to all live listeners.</p> Source code in <code>toolboxv2/mods/Canvas.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, name=\"send_canvas_action\", api_methods=['POST'],\n        request_as_kwarg=True)\nasync def handle_send_canvas_action(app: App, request: RequestData, data: Dict[str, Any]):\n    \"\"\"\n    Handles incremental, real-time actions from clients (e.g., adding an element).\n    It persists the change to the database and then broadcasts it to all live listeners.\n    \"\"\"\n    canvas_tool = app.get_mod(MOD_NAME)\n    if not canvas_tool or not canvas_tool.db_mod:\n        return Result.default_internal_error(\"Canvas module or DB not loaded.\")\n\n    canvas_id = data.get(\"canvas_id\")\n    action_type = data.get(\"action_type\")\n    action_payload = data.get(\"payload\")\n    user_id = data.get(\"user_id\")\n\n    if not all([canvas_id, action_type, user_id]) or action_payload is None:\n        return Result.default_user_error(\"Request missing required fields.\", 400)\n\n    # --- Flow 1: Ephemeral 'preview' actions that DO NOT get persisted ---\n    if action_type in [\"preview_update\", \"preview_clear\"]:\n        sse_event_type = \"user_preview_update\" if action_type == \"preview_update\" else \"clear_user_preview\"\n        sse_data = {\"user_id\": user_id}\n\n        async with canvas_tool.previews_lock:\n            if action_type == \"preview_update\":\n                canvas_tool.active_user_previews[canvas_id][user_id] = action_payload\n                sse_data[\"preview_data\"] = action_payload\n            elif user_id in canvas_tool.active_user_previews.get(canvas_id, {}):\n                del canvas_tool.active_user_previews[canvas_id][user_id]\n\n        # MODIFICATION: Call the non-blocking broadcast method. This returns immediately.\n        canvas_tool._broadcast_to_canvas_listeners(\n            canvas_id=canvas_id, event_type=sse_event_type,\n            data=sse_data, originator_user_id=user_id\n        )\n        return Result.ok(info=f\"'{action_type}' broadcasted.\")\n\n    # --- Flow 2: Persistent actions that modify the canvas state ---\n    if action_type not in [\"element_add\", \"element_update\", \"element_remove\"]:\n        return Result.default_user_error(f\"Unknown persistent action_type: {action_type}\", 400)\n\n    # Load the full, current session state from the database\n    user_db_key_base = await canvas_tool._get_user_specific_db_key(request, SESSION_DATA_PREFIX)\n    session_db_key = f\"{user_db_key_base}_{canvas_id}\"\n    try:\n        db_result = canvas_tool.db_mod.get(session_db_key)\n        if not db_result or db_result.is_error() or not db_result.get():\n            return Result.default_user_error(\"Canvas session not found in database.\", 404)\n\n        session_data_str = db_result.get()[0] if isinstance(db_result.get(), list) else db_result.get()\n        session_data = IdeaSessionData.model_validate_json(session_data_str)\n    except Exception as e:\n        app.logger.error(f\"DB Load/Parse failed for C:{canvas_id}. Error: {e}\", exc_info=True)\n        return Result.default_internal_error(\"Could not load canvas data to apply changes.\")\n\n    # Apply the action to the in-memory Pydantic object\n    if action_type == \"element_add\":\n        session_data.canvas_elements.append(CanvasElement(**action_payload))\n    elif action_type == \"element_update\":\n        element_id = action_payload.get(\"id\")\n        for i, el in enumerate(session_data.canvas_elements):\n            if el.id == element_id:\n                session_data.canvas_elements[i] = el.model_copy(update=action_payload)\n                break\n    elif action_type == \"element_remove\":\n        ids_to_remove = set(action_payload.get(\"ids\", [action_payload.get(\"id\")]))\n        session_data.canvas_elements = [el for el in session_data.canvas_elements if el.id not in ids_to_remove]\n\n    # Save the modified object back to the database\n    session_data.last_modified = datetime.now(timezone.utc).timestamp()\n    canvas_tool.db_mod.set(session_db_key, session_data.model_dump_json(exclude_none=True))\n\n    # Broadcast the successful, persisted action to all connected clients\n    # MODIFICATION: Call the non-blocking broadcast method.\n    canvas_tool._broadcast_to_canvas_listeners(\n        canvas_id=canvas_id,\n        event_type=\"canvas_elements_changed\",\n        data={\"action\": action_type, \"element\": action_payload},\n        originator_user_id=user_id\n    )\n\n    # Clear the temporary preview of the user who made the change\n    async with canvas_tool.previews_lock:\n        if user_id in canvas_tool.active_user_previews.get(canvas_id, {}):\n            del canvas_tool.active_user_previews[canvas_id][user_id]\n\n    # MODIFICATION: Call the non-blocking broadcast method.\n    canvas_tool._broadcast_to_canvas_listeners(\n        canvas_id=canvas_id, event_type=\"clear_user_preview\",\n        data={\"user_id\": user_id}, originator_user_id=user_id\n    )\n\n    return Result.ok(info=f\"Action '{action_type}' persisted and broadcast.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.Canvas.markdown_to_svg","title":"<code>markdown_to_svg(self, request, markdown_text='', width=400, font_family='sans-serif', font_size=14, bg_color='#ffffff', text_color='#000000')</code>  <code>async</code>","text":"<p>Converts a string of Markdown text into an SVG image. The SVG is returned as a base64 encoded data URL. This version uses a viewBox for better scalability and multi-line handling.</p> Source code in <code>toolboxv2/mods/Canvas.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, name=\"markdown_to_svg\", api_methods=['POST'],\n        request_as_kwarg=True)\nasync def markdown_to_svg(self, request: RequestData, markdown_text: str = \"\", width: int = 400,\n                          font_family: str = \"sans-serif\", font_size: int = 14,\n                          bg_color: str = \"#ffffff\", text_color: str = \"#000000\") -&gt; Result:\n    \"\"\"\n    Converts a string of Markdown text into an SVG image.\n    The SVG is returned as a base64 encoded data URL.\n    This version uses a viewBox for better scalability and multi-line handling.\n    \"\"\"\n    if not markdown_text:\n        markdown_text = request.data.get(\"markdown_text\", \"\")\n\n    if not markdown_text:\n        return Result.default_user_error(\"markdown_text cannot be empty.\")\n\n    try:\n        # Convert Markdown to HTML\n        html_content = markdown2.markdown(markdown_text, extras=[\"fenced-code-blocks\", \"tables\", \"strike\"])\n\n        # --- FIX for Multi-line text ---\n        # The key is to NOT set a fixed height on the SVG itself, but to use a viewBox.\n        # The client will determine the final rendered size.\n        # The width of the div inside the foreignObject controls the line wrapping.\n\n        # We still need a rough height for the viewBox.\n        # Estimate height: (number of lines * line-height) + padding\n        # A simple line-height estimate is font_size * 1.6\n        line_height_estimate = font_size * 1.6\n        num_lines_estimate = len(html_content.split('\\n')) + html_content.count('&lt;br') + html_content.count(\n            '&lt;p&gt;') + html_content.count('&lt;li&gt;')\n        estimated_height = (num_lines_estimate * line_height_estimate) + 40  # 20px top/bottom padding\n\n        svg_template = f\"\"\"\n        &lt;svg viewBox=\"0 0 {width} {int(estimated_height)}\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\n            &lt;foreignObject x=\"0\" y=\"0\" width=\"{width}\" height=\"{int(estimated_height)}\"&gt;\n                &lt;div xmlns=\"http://www.w3.org/1999/xhtml\"&gt;\n                    &lt;style&gt;\n                        div {{\n                            font-family: {font_family};\n                            font-size: {font_size}px;\n                            color: {text_color};\n                            background-color: {bg_color};\n                            padding: 10px;\n                            border-radius: 5px;\n                            line-height: 1.6;\n                            width: {width - 20}px; /* Width minus padding */\n                            word-wrap: break-word;\n                            height: 100%;\n                            overflow-y: auto; /* Allow scrolling if content overflows estimate */\n                        }}\n                        h1, h2, h3 {{ border-bottom: 1px solid #ccc; padding-bottom: 5px; margin-top: 1em; }}\n                        pre {{ background-color: #f0f0f0; padding: 10px; border-radius: 4px; overflow-x: auto; }}\n                        code {{ font-family: monospace; }}\n                        table {{ border-collapse: collapse; width: 100%; }}\n                        th, td {{ border: 1px solid #ddd; padding: 8px; }}\n                        th {{ background-color: #f2f2f2; }}\n                        blockquote {{ border-left: 4px solid #ccc; padding-left: 10px; color: #555; margin-left: 0; }}\n                    &lt;/style&gt;\n                    {html_content}\n                &lt;/div&gt;\n            &lt;/foreignObject&gt;\n        &lt;/svg&gt;\n        \"\"\"\n\n        svg_base64 = base64.b64encode(svg_template.encode('utf-8')).decode('utf-8')\n        data_url = f\"data:image/svg+xml;base64,{svg_base64}\"\n\n        # --- FIX for Editability ---\n        # Return the original markdown text along with the SVG\n        return Result.ok(data={\"svg_data_url\": data_url, \"original_markdown\": markdown_text})\n\n    except Exception as e:\n        self.app.logger.error(f\"Error converting Markdown to SVG: {e}\", exc_info=True)\n        return Result.default_internal_error(\"Failed to convert Markdown to SVG.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.Canvas.save_session","title":"<code>save_session(app, request, data)</code>  <code>async</code>","text":"<p>Saves the entire state of a canvas session to the database. This is typically triggered by a user's explicit \"Save\" action.</p> Source code in <code>toolboxv2/mods/Canvas.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, name=\"save_session\", api_methods=['POST'], request_as_kwarg=True)\nasync def save_session(app: App, request: RequestData, data: Union[Dict[str, Any], IdeaSessionData]) -&gt; Result:\n    \"\"\"\n    Saves the entire state of a canvas session to the database.\n    This is typically triggered by a user's explicit \"Save\" action.\n    \"\"\"\n    canvas_tool = app.get_mod(MOD_NAME)\n    if not canvas_tool or not canvas_tool.db_mod:\n        app.logger.error(\"Save failed: Canvas module or DB not available.\")\n        return Result.custom_error(info=\"Database module not available.\", exec_code=503)\n\n    user_db_key_base = await canvas_tool._get_user_specific_db_key(request, SESSION_DATA_PREFIX)\n    if not user_db_key_base:\n        return Result.default_user_error(info=\"User authentication required to save.\", exec_code=401)\n\n    try:\n        # Validate the incoming data against the Pydantic model\n        session_data_obj = IdeaSessionData(**data) if isinstance(data, dict) else data\n    except Exception as e:\n        app.logger.error(f\"Invalid session data for save: {e}. Data: {str(data)[:500]}\", exc_info=True)\n        return Result.default_user_error(info=f\"Invalid session data format: {e}\", exec_code=400)\n\n    # Update timestamp and construct the main session key\n    session_data_obj.last_modified = datetime.now(timezone.utc).timestamp()\n    session_db_key = f\"{user_db_key_base}_{session_data_obj.id}\"\n\n    # Save the full session object to the database\n    canvas_tool.db_mod.set(session_db_key, session_data_obj.model_dump_json(exclude_none=True))\n    app.logger.info(f\"Saved session data for C:{session_data_obj.id}\")\n\n    # --- Update the session list metadata ---\n    session_list_key = f\"{user_db_key_base}{SESSION_LIST_KEY_SUFFIX}\"\n    try:\n        list_res_obj = canvas_tool.db_mod.get(session_list_key)\n        user_sessions = []\n        if list_res_obj and not list_res_obj.is_error() and list_res_obj.get():\n            list_content = list_res_obj.get()[0] if isinstance(list_res_obj.get(), list) else list_res_obj.get()\n            user_sessions = json.loads(list_content)\n\n        # Find and update the existing entry, or add a new one\n        session_metadata = {\n            \"id\": session_data_obj.id,\n            \"name\": session_data_obj.name,\n            \"last_modified\": session_data_obj.last_modified\n        }\n        found_in_list = False\n        for i, sess_meta in enumerate(user_sessions):\n            if sess_meta.get(\"id\") == session_data_obj.id:\n                user_sessions[i] = session_metadata\n                found_in_list = True\n                break\n        if not found_in_list:\n            user_sessions.append(session_metadata)\n\n        canvas_tool.db_mod.set(session_list_key, json.dumps(user_sessions))\n        app.logger.info(f\"Updated session list for user key ending in ...{user_db_key_base[-12:]}\")\n\n    except Exception as e:\n        app.logger.error(f\"Failed to update session list for C:{session_data_obj.id}. Error: {e}\", exc_info=True)\n        # Non-fatal error; the main data was saved. We can continue.\n\n    return Result.ok(\n        info=\"Session saved successfully.\",\n        data={\"id\": session_data_obj.id, \"last_modified\": session_data_obj.last_modified}\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM","title":"<code>CloudM</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.check_multiple_processes","title":"<code>check_multiple_processes(pids)</code>","text":"<p>Checks the status of multiple processes in a single system call. Returns a dictionary mapping PIDs to their status (GREEN_CIRCLE, RED_CIRCLE, or YELLOW_CIRCLE).</p> Source code in <code>toolboxv2/mods/CloudM/mini.py</code> <pre><code>def check_multiple_processes(pids: list[int]) -&gt; dict[int, str]:\n    \"\"\"\n    Checks the status of multiple processes in a single system call.\n    Returns a dictionary mapping PIDs to their status (GREEN_CIRCLE, RED_CIRCLE, or YELLOW_CIRCLE).\n    \"\"\"\n    if not pids:\n        return {}\n\n    pid_status = {}\n\n    if os.name == 'nt':  # Windows\n        try:\n            # Windows tasklist requires separate /FI for each filter\n            command = 'tasklist'\n\n            # Add encoding handling for Windows\n            result = subprocess.run(\n                command,\n                capture_output=True,\n                text=True,\n                shell=True,\n                encoding='cp850'  # Use cp850 for Windows console output\n            )\n            # Create a set of running PIDs from the output\n            running_pids = set()\n            for line in result.stdout.lower().split('\\n'):\n                for pid in pids:\n                    if str(pid) in line:\n                        running_pids.add(pid)\n            # Assign status based on whether PID was found in output\n            for pid in pids:\n                if pid in running_pids:\n                    pid_status[pid] = GREEN_CIRCLE\n                else:\n                    pid_status[pid] = RED_CIRCLE\n\n        except subprocess.SubprocessError as e:\n            print(f\"SubprocessError: {e}\")  # For debugging\n            # Mark all as YELLOW_CIRCLE if there's an error running the command\n            for pid in pids:\n                pid_status[pid] = YELLOW_CIRCLE\n        except UnicodeDecodeError as e:\n            print(f\"UnicodeDecodeError: {e}\")  # For debugging\n            # Try alternate encoding if cp850 fails\n            try:\n                result = subprocess.run(\n                    command,\n                    capture_output=True,\n                    text=True,\n                    shell=True,\n                    encoding='utf-8'\n                )\n                running_pids = set()\n                for line in result.stdout.lower().split('\\n'):\n                    for pid in pids:\n                        if str(pid) in line:\n                            running_pids.add(pid)\n\n                for pid in pids:\n                    pid_status[pid] = GREEN_CIRCLE if pid in running_pids else RED_CIRCLE\n            except Exception as e:\n                print(f\"Failed with alternate encoding: {e}\")  # For debugging\n                for pid in pids:\n                    pid_status[pid] = YELLOW_CIRCLE\n\n    else:  # Unix/Linux/Mac\n        try:\n            pids_str = ','.join(str(pid) for pid in pids)\n            command = f'ps -p {pids_str} -o pid='\n\n            result = subprocess.run(\n                command,\n                capture_output=True,\n                text=True,\n                shell=True,\n                encoding='utf-8'\n            )\n            running_pids = set(int(pid) for pid in result.stdout.strip().split())\n\n            for pid in pids:\n                pid_status[pid] = GREEN_CIRCLE if pid in running_pids else RED_CIRCLE\n\n        except subprocess.SubprocessError as e:\n            print(f\"SubprocessError: {e}\")  # For debugging\n            for pid in pids:\n                pid_status[pid] = YELLOW_CIRCLE\n\n    return pid_status\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.get_service_pids","title":"<code>get_service_pids(info_dir)</code>","text":"<p>Extracts service names and PIDs from pid files.</p> Source code in <code>toolboxv2/mods/CloudM/mini.py</code> <pre><code>def get_service_pids(info_dir):\n    \"\"\"Extracts service names and PIDs from pid files.\"\"\"\n    services = {}\n    pid_files = [f for f in os.listdir(info_dir) if re.match(r'(.+)-(.+)\\.pid', f)]\n    for pid_file in pid_files:\n        match = re.match(r'(.+)-(.+)\\.pid', pid_file)\n        if match:\n            services_type, service_name = match.groups()\n            # Read the PID from the file\n            with open(os.path.join(info_dir, pid_file)) as file:\n                pid = file.read().strip()\n                # Store the PID using a formatted key\n                services[f\"{service_name} - {services_type}\"] = int(pid)\n    return services\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.get_service_status","title":"<code>get_service_status(dir)</code>","text":"<p>Displays the status of all services.</p> Source code in <code>toolboxv2/mods/CloudM/mini.py</code> <pre><code>def get_service_status(dir: str) -&gt; str:\n    \"\"\"Displays the status of all services.\"\"\"\n    if time.time()-services_data_sto_last_update_time[0] &gt; 30:\n        services = get_service_pids(dir)\n        services_data_sto[0] = services\n        services_data_sto_last_update_time[0] = time.time()\n    else:\n        services = services_data_sto[0]\n    if not services:\n        return \"No services found\"\n\n    # Get status for all PIDs in a single call\n    pid_statuses = check_multiple_processes(list(services.values()))\n\n    # Build the status string\n    res_s = \"Service(s):\" + (\"\\n\" if len(services) &gt; 1 else ' ')\n    for service_name, pid in services.items():\n        status = pid_statuses.get(pid, YELLOW_CIRCLE)\n        res_s += f\"{status} {service_name} (PID: {pid})\\n\"\n    services_data_display[0] = res_s.strip()\n    return res_s.rstrip()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager","title":"<code>ModManager</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.create_and_pack_module","title":"<code>create_and_pack_module(path, module_name='', version='-.-.-', additional_dirs=None, yaml_data=None)</code>","text":"<p>Erstellt ein Python-Modul und packt es in eine ZIP-Datei.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Pfad zum Ordner oder zur Datei, die in das Modul aufgenommen werden soll.</p> required <code>additional_dirs</code> <code>dict</code> <p>Zus\u00e4tzliche Verzeichnisse, die hinzugef\u00fcgt werden sollen.</p> <code>None</code> <code>version</code> <code>str</code> <p>Version des Moduls.</p> <code>'-.-.-'</code> <code>module_name</code> <code>str</code> <p>Name des Moduls.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <p>Pfad zur erstellten ZIP-Datei.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def create_and_pack_module(path, module_name='', version='-.-.-', additional_dirs=None, yaml_data=None):\n    \"\"\"\n    Erstellt ein Python-Modul und packt es in eine ZIP-Datei.\n\n    Args:\n        path (str): Pfad zum Ordner oder zur Datei, die in das Modul aufgenommen werden soll.\n        additional_dirs (dict): Zus\u00e4tzliche Verzeichnisse, die hinzugef\u00fcgt werden sollen.\n        version (str): Version des Moduls.\n        module_name (str): Name des Moduls.\n\n    Returns:\n        str: Pfad zur erstellten ZIP-Datei.\n    \"\"\"\n    if additional_dirs is None:\n        additional_dirs = {}\n    if yaml_data is None:\n        yaml_data = {}\n\n    os.makedirs(\"./mods_sto/temp/\", exist_ok=True)\n\n    module_path = os.path.join(path, module_name)\n    print(\"module_pathmodule_pathmodule_path\", module_path)\n    if not os.path.exists(module_path):\n        module_path += '.py'\n\n    temp_dir = tempfile.mkdtemp(dir=os.path.join(\"./mods_sto\", \"temp\"))\n    zip_file_name = f\"RST${module_name}&amp;{__version__}\u00a7{version}.zip\"\n    zip_path = f\"./mods_sto/{zip_file_name}\"\n\n    # Modulverzeichnis erstellen, falls es nicht existiert\n    if not os.path.exists(module_path):\n        return False\n\n    if os.path.isdir(module_path):\n        # tbConfig.yaml erstellen\n        config_path = os.path.join(module_path, \"tbConfig.yaml\")\n        with open(config_path, 'w') as config_file:\n            yaml.dump({\"version\": version, \"module_name\": module_name,\n                       \"dependencies_file\": f\"./mods/{module_name}/requirements.txt\",\n                       \"zip\": zip_file_name, **yaml_data}, config_file)\n\n        generate_requirements(module_path, os.path.join(module_path, \"requirements.txt\"))\n    # Datei oder Ordner in das Modulverzeichnis kopieren\n    if os.path.isdir(module_path):\n        shutil.copytree(module_path, os.path.join(temp_dir, os.path.basename(module_path)), dirs_exist_ok=True)\n    else:\n        shutil.copy2(module_path, temp_dir)\n        config_path = os.path.join(temp_dir, f\"{module_name}.yaml\")\n        with open(config_path, 'w') as config_file:\n            yaml.dump({\"version\": version, \"dependencies_file\": f\"./mods/{module_name}/requirements.txt\",\n                       \"module_name\": module_name, **yaml_data}, config_file)\n        generate_requirements(temp_dir, os.path.join(temp_dir, \"requirements.txt\"))\n    # Zus\u00e4tzliche Verzeichnisse hinzuf\u00fcgen\n    for dir_name, dir_paths in additional_dirs.items():\n        if isinstance(dir_paths, str):\n            dir_paths = [dir_paths]\n        for dir_path in dir_paths:\n            full_path = os.path.join(temp_dir, dir_name)\n            if os.path.isdir(dir_path):\n                shutil.copytree(dir_path, full_path, dirs_exist_ok=True)\n            elif os.path.isfile(dir_path):\n                # Stellen Sie sicher, dass das Zielverzeichnis existiert\n                os.makedirs(full_path, exist_ok=True)\n                # Kopieren Sie die Datei statt des Verzeichnisses\n                shutil.copy2(dir_path, full_path)\n            else:\n                print(f\"Der Pfad {dir_path} ist weder ein Verzeichnis noch eine Datei.\")\n\n    # Modul in eine ZIP-Datei packen\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, _dirs, files in os.walk(temp_dir):\n            for file in files:\n                file_path = os.path.join(root, file)\n                zipf.write(file_path, os.path.relpath(file_path, temp_dir))\n\n    # Temperatures Modulverzeichnis l\u00f6schen\n    shutil.rmtree(temp_dir)\n\n    return zip_path\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.download_files","title":"<code>download_files(urls, directory, desc, print_func, filename=None)</code>","text":"<p>Hilfsfunktion zum Herunterladen von Dateien.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def download_files(urls, directory, desc, print_func, filename=None):\n    \"\"\" Hilfsfunktion zum Herunterladen von Dateien. \"\"\"\n    for url in tqdm(urls, desc=desc):\n        if filename is None:\n            filename = os.path.basename(url)\n        print_func(f\"Download {filename}\")\n        print_func(f\"{url} -&gt; {directory}/{filename}\")\n        os.makedirs(directory, exist_ok=True)\n        urllib.request.urlretrieve(url, f\"{directory}/{filename}\")\n    return f\"{directory}/{filename}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.handle_requirements","title":"<code>handle_requirements(requirements_url, module_name, print_func)</code>","text":"<p>Verarbeitet und installiert Requirements.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def handle_requirements(requirements_url, module_name, print_func):\n    \"\"\" Verarbeitet und installiert Requirements. \"\"\"\n    if requirements_url:\n        requirements_filename = f\"{module_name}-requirements.txt\"\n        print_func(f\"Download requirements {requirements_filename}\")\n        urllib.request.urlretrieve(requirements_url, requirements_filename)\n\n        print_func(\"Install requirements\")\n        run_command(\n            [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", requirements_filename])\n\n        os.remove(requirements_filename)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.increment_version","title":"<code>increment_version(version_str, max_value=99)</code>","text":"<p>Inkrementiert eine Versionsnummer im Format \"vX.Y.Z\".</p> <p>Parameters:</p> Name Type Description Default <code>version_str</code> <code>str</code> <p>Die aktuelle Versionsnummer, z. B. \"v0.0.1\".</p> required <code>max_value</code> <code>int</code> <p>Die maximale Zahl pro Stelle (default: 99).</p> <code>99</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Die inkrementierte Versionsnummer.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def increment_version(version_str: str, max_value: int = 99) -&gt; str:\n    \"\"\"\n    Inkrementiert eine Versionsnummer im Format \"vX.Y.Z\".\n\n    Args:\n        version_str (str): Die aktuelle Versionsnummer, z. B. \"v0.0.1\".\n        max_value (int): Die maximale Zahl pro Stelle (default: 99).\n\n    Returns:\n        str: Die inkrementierte Versionsnummer.\n    \"\"\"\n    if not version_str.startswith(\"v\"):\n        raise ValueError(\"Die Versionsnummer muss mit 'v' beginnen, z. B. 'v0.0.1'.\")\n\n    # Entferne das f\u00fchrende 'v' und parse die Versionsnummer\n    version_core = version_str[1:]\n    try:\n        version = Version(version_core)\n    except ValueError as e:\n        raise ValueError(f\"Ung\u00fcltige Versionsnummer: {version_core}\") from e\n\n    # Extrahiere die Versionsteile und konvertiere sie zu einer Liste\n    parts = list(version.release)\n\n    # Inkrementiere die letzte Stelle\n    for i in range(len(parts) - 1, -1, -1):\n        if parts[i] &lt; max_value:\n            parts[i] += 1\n            break\n        else:\n            parts[i] = 0\n            # Schleife f\u00e4hrt fort, um die n\u00e4chsth\u00f6here Stelle zu inkrementieren\n    else:\n        # Wenn alle Stellen auf \"max_value\" sind, f\u00fcge eine neue Stelle hinzu\n        parts.insert(0, 1)\n\n    # Baue die neue Version\n    new_version = \"v\" + \".\".join(map(str, parts))\n    return new_version\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.installer","title":"<code>installer(app, module_name, build_state=True)</code>  <code>async</code>","text":"<p>Installiert oder aktualisiert ein Modul basierend auf der Remote-Version.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>@export(mod_name=Name, name=\"install\", test=False)\nasync def installer(app: App | None, module_name: str, build_state=True):\n    \"\"\"\n    Installiert oder aktualisiert ein Modul basierend auf der Remote-Version.\n    \"\"\"\n    if app is None:\n        app = get_app(f\"{Name}.installer\")\n\n    if not app.session.valid and not await app.session.login():\n        return Result.default_user_error(\"Please login with CloudM login\")\n\n    # Hole nur die h\u00f6chste verf\u00fcgbare Version vom Server\n    response = await app.session.fetch(f\"/installer/version?name={module_name}\", method=\"GET\")\n    remote_version : str = await response.text()\n    remote_version = remote_version.split('\"')[1]\n    if remote_version == \"None\":\n        remote_version = None\n    # Finde lokale Version\n    local_version = find_highest_zip_version(\n        module_name, version_only=True\n    )\n\n    if not local_version and not remote_version:\n        return Result.default_user_error(f\"404 mod {module_name} not found\")\n\n    # Vergleiche Versionen\n    local_ver = pv.parse(local_version) if local_version else pv.parse(\"0.0.0\")\n    remote_ver = pv.parse(remote_version)\n\n    app.print(f\"Mod versions - Local: {local_ver}, Remote: {remote_ver}\")\n\n    if remote_ver &gt; local_ver:\n        # Konstruiere die URL direkt aus Modulname und Version\n        mod_url = f\"/installer/mods_sto/RST${module_name}&amp;{app.version}\u00a7{remote_version}.zip\"\n        download_path = Path(app.start_dir) / 'mods_sto'\n\n        app.print(f\"Fetching Mod from {app.session.base+mod_url}\")\n        if not await app.session.download_file(mod_url, str(download_path)):\n            app.print(\"Failed to download mod\")\n            if 'y' not in input(\"Download manually and place in mods_sto folder. Done? (y/n) \").lower():\n                return Result.default_user_error(\"Installation cancelled\")\n\n        # Korrigiere Dateinamen\n        zip_name = mod_url.split('/')[-1]\n        clean_name = zip_name.replace(\"$\", '').replace(\"&amp;\", '').replace(\"\u00a7\", '')\n        with contextlib.suppress(FileExistsError):\n            os.rename(\n                str(download_path / clean_name),\n                str(download_path / zip_name)\n            )\n\n        with Spinner(\"Installing from zip\"):\n            report = install_from_zip(app, zip_name)\n\n        if not report:\n            return Result.default_user_error(\"Setup error occurred\")\n\n        if build_state:\n            get_state_from_app(app)\n\n        return report\n\n    app.print(\"Module is already up to date\")\n    return Result.ok()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.run_command","title":"<code>run_command(command, cwd=None)</code>","text":"<p>F\u00fchrt einen Befehl aus und gibt den Output zur\u00fcck.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def run_command(command, cwd=None):\n    \"\"\"F\u00fchrt einen Befehl aus und gibt den Output zur\u00fcck.\"\"\"\n    result = subprocess.run(command, cwd=cwd, capture_output=True, text=True, check=True,\n                            encoding='cp850')\n    return result.stdout\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.uninstall_module","title":"<code>uninstall_module(path, module_name='', version='-.-.-', additional_dirs=None, yaml_data=None)</code>","text":"<p>Deinstalliert ein Python-Modul, indem es das Modulverzeichnis oder die ZIP-Datei entfernt.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Pfad zum Ordner oder zur Datei, die in das Modul aufgenommen werden soll.</p> required <code>additional_dirs</code> <code>dict</code> <p>Zus\u00e4tzliche Verzeichnisse, die hinzugef\u00fcgt werden sollen.</p> <code>None</code> <code>version</code> <code>str</code> <p>Version des Moduls.</p> <code>'-.-.-'</code> <code>module_name</code> <code>str</code> <p>Name des Moduls.</p> <code>''</code> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def uninstall_module(path, module_name='', version='-.-.-', additional_dirs=None, yaml_data=None):\n    \"\"\"\n    Deinstalliert ein Python-Modul, indem es das Modulverzeichnis oder die ZIP-Datei entfernt.\n\n    Args:\n        path (str): Pfad zum Ordner oder zur Datei, die in das Modul aufgenommen werden soll.\n        additional_dirs (dict): Zus\u00e4tzliche Verzeichnisse, die hinzugef\u00fcgt werden sollen.\n        version (str): Version des Moduls.\n        module_name (str): Name des Moduls.\n\n    \"\"\"\n    if additional_dirs is None:\n        additional_dirs = {}\n    if yaml_data is None:\n        yaml_data = {}\n\n    os.makedirs(\"./mods_sto/temp/\", exist_ok=True)\n\n    base_path = os.path.dirname(path)\n    module_path = os.path.join(base_path, module_name)\n    zip_path = f\"./mods_sto/RST${module_name}&amp;{__version__}\u00a7{version}.zip\"\n\n    # Modulverzeichnis erstellen, falls es nicht existiert\n    if not os.path.exists(module_path):\n        print(\"Module %s already uninstalled\")\n        return False\n\n    # Datei oder Ordner in das Modulverzeichnis kopieren\n    shutil.rmtree(module_path)\n\n    # Zus\u00e4tzliche Verzeichnisse hinzuf\u00fcgen\n    for _dir_name, dir_paths in additional_dirs.items():\n        if isinstance(dir_paths, str):\n            dir_paths = [dir_paths]\n        for dir_path in dir_paths:\n            shutil.rmtree(dir_path)\n            print(f\"Der Pfad {dir_path} wurde entfernt\")\n\n    # Urspr\u00fcngliches Modulverzeichnis l\u00f6schen\n    shutil.rmtree(zip_path)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.unpack_and_move_module","title":"<code>unpack_and_move_module(zip_path, base_path='./mods', module_name='')</code>","text":"<p>Entpackt eine ZIP-Datei und verschiebt die Inhalte an die richtige Stelle. \u00dcberschreibt existierende Dateien f\u00fcr Update-Unterst\u00fctzung.</p> <p>Parameters:</p> Name Type Description Default <code>zip_path</code> <code>str</code> <p>Pfad zur ZIP-Datei, die entpackt werden soll</p> required <code>base_path</code> <code>str</code> <p>Basispfad, unter dem das Modul gespeichert werden soll</p> <code>'./mods'</code> <code>module_name</code> <code>str</code> <p>Name des Moduls (optional, wird sonst aus ZIP-Namen extrahiert)</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Name des installierten Moduls</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def unpack_and_move_module(zip_path: str, base_path: str = './mods', module_name: str = '') -&gt; str:\n    \"\"\"\n    Entpackt eine ZIP-Datei und verschiebt die Inhalte an die richtige Stelle.\n    \u00dcberschreibt existierende Dateien f\u00fcr Update-Unterst\u00fctzung.\n\n    Args:\n        zip_path (str): Pfad zur ZIP-Datei, die entpackt werden soll\n        base_path (str): Basispfad, unter dem das Modul gespeichert werden soll\n        module_name (str): Name des Moduls (optional, wird sonst aus ZIP-Namen extrahiert)\n\n    Returns:\n        str: Name des installierten Moduls\n    \"\"\"\n    # Konvertiere Pfade zu Path-Objekten f\u00fcr bessere Handhabung\n    zip_path = Path(zip_path)\n    base_path = Path(base_path)\n\n    # Extrahiere Modulnamen falls nicht angegeben\n    if not module_name:\n        module_name = zip_path.name.split('$')[1].split('&amp;')[0]\n\n    module_path = base_path / module_name\n    temp_base = Path('./mods_sto/temp')\n\n    try:\n        # Erstelle tempor\u00e4res Verzeichnis\n        temp_base.mkdir(parents=True, exist_ok=True)\n        with tempfile.TemporaryDirectory(dir=str(temp_base)) as temp_dir:\n            temp_dir = Path(temp_dir)\n\n            with Spinner(f\"Extracting {zip_path.name}\"):\n                # Entpacke ZIP-Datei\n                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                    zip_ref.extractall(temp_dir)\n\n            # Behandle Modul-Verzeichnis\n            source_module = temp_dir / module_name\n            if source_module.exists():\n                with Spinner(f\"Installing module to {module_path}\"):\n                    if module_path.exists():\n                        # L\u00f6sche existierendes Modul-Verzeichnis f\u00fcr sauberes Update\n                        shutil.rmtree(module_path)\n                    # Verschiebe neues Modul-Verzeichnis\n                    shutil.copytree(source_module, module_path, dirs_exist_ok=True)\n\n            # Behandle zus\u00e4tzliche Dateien im Root\n            with Spinner(\"Installing additional files\"):\n                for item in temp_dir.iterdir():\n                    if item.name == module_name:\n                        continue\n\n                    target = Path('./') / item.name\n                    if item.is_dir():\n                        with Spinner(f\"Installing directory {item.name}\"):\n                            if target.exists():\n                                shutil.rmtree(target)\n                            shutil.copytree(item, target, dirs_exist_ok=True)\n                    else:\n                        with Spinner(f\"Installing file {item.name}\"):\n                            shutil.copy2(item, target)\n\n            print(f\"Successfully installed/updated module {module_name} to {module_path}\")\n            return module_name\n\n    except Exception as e:\n        print(f\"Error during installation: {str(e)}\")\n        # Cleanup bei Fehler\n        if module_path.exists():\n            shutil.rmtree(module_path)\n        raise\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.UserAccountManager","title":"<code>UserAccountManager</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.UserAccountManager.get_current_user_from_request_api_wrapper","title":"<code>get_current_user_from_request_api_wrapper(app, request)</code>  <code>async</code>","text":"<p>API callable version of get_current_user_from_request for tbjs/admin panel</p> Source code in <code>toolboxv2/mods/CloudM/UserAccountManager.py</code> <pre><code>@export(mod_name=Name, api=True, version=version, request_as_kwarg=True, row=False)  # row=False to return JSON\nasync def get_current_user_from_request_api_wrapper(app: App, request: RequestData):\n    \"\"\" API callable version of get_current_user_from_request for tbjs/admin panel \"\"\"\n    user = await get_current_user_from_request(app, request)\n    if not user:\n        # Return error that tbjs can handle\n        return Result.default_user_error(info=\"User not authenticated or found.\", data=None, exec_code=401)\n    user_dict = asdict(user)\n    pub_user_data = {}\n    for key in ['name','pub_key','email','creation_time','is_persona','level','log_level','settings']:\n        pub_user_data[key] = user_dict.get(key, None)\n    return Result.ok(data=pub_user_data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.email_services","title":"<code>email_services</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.email_services.send_email_verification_email","title":"<code>send_email_verification_email(app, user_email, username, verification_url)</code>","text":"<p>Sends an email verification link to the user.</p> Source code in <code>toolboxv2/mods/CloudM/email_services.py</code> <pre><code>@s_export\ndef send_email_verification_email(app: App, user_email: str, username: str, verification_url: str):\n    \"\"\"Sends an email verification link to the user.\"\"\"\n    sender = EmailSender(app)\n    subject = f\"Verify Your Email for {APP_NAME}\"\n    preview_text = f\"Almost there, {username}! Just one more step to activate your account.\"\n\n    content_html = f\"\"\"\n        &lt;h2&gt;Hi {username},&lt;/h2&gt;\n        &lt;p&gt;Thanks for signing up for {APP_NAME}! To complete your registration, please verify your email address by clicking the button below.&lt;/p&gt;\n        &lt;a href=\"{verification_url}\" class=\"button\"&gt;Verify Email Address&lt;/a&gt;\n        &lt;p&gt;If you didn't create an account with {APP_NAME}, you can safely ignore this email.&lt;/p&gt;\n        &lt;p&gt;If the button doesn't work, copy and paste this link into your browser:&lt;br&gt;&lt;span class=\"link-in-text\"&gt;{verification_url}&lt;/span&gt;&lt;/p&gt;\n        &lt;p&gt;Sincerely,&lt;br&gt;The {APP_NAME} Team&lt;/p&gt;\n    \"\"\"\n    return sender.send_html_email(user_email, subject, content_html, preview_text)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.email_services.send_magic_link_email","title":"<code>send_magic_link_email(app, user_email, magic_link_url, username=None)</code>","text":"<p>Sends a magic link email for login.</p> Source code in <code>toolboxv2/mods/CloudM/email_services.py</code> <pre><code>@s_export\ndef send_magic_link_email(app: App, user_email: str, magic_link_url: str, username: str = None):\n    \"\"\"Sends a magic link email for login.\"\"\"\n    sender = EmailSender(app)\n    greeting_name = f\", {username}\" if username else \"\"\n    subject = f\"Your Magic Login Link for {APP_NAME}\"\n    preview_text = \"Securely access your account with this one-time link.\"\n\n    content_html = f\"\"\"\n        &lt;h2&gt;Hello{greeting_name}!&lt;/h2&gt;\n        &lt;p&gt;You requested a magic link to sign in to your {APP_NAME} account.&lt;/p&gt;\n        &lt;p&gt;Click the button below to log in. This link is temporary and will expire shortly.&lt;/p&gt;\n        &lt;a href=\"{magic_link_url}\" class=\"button\"&gt;Log In Securely&lt;/a&gt;\n        &lt;p&gt; Invitation key: {magic_link_url.split('?key=')[1].split('&amp;name=')[0].replace('%23', '#')}&lt;/p&gt;\n        &lt;p&gt;If you did not request this link, please ignore this email. Your account is safe.&lt;/p&gt;\n        &lt;p&gt;If the button doesn't work, copy and paste this link into your browser:&lt;br&gt;&lt;span class=\"link-in-text\"&gt;{magic_link_url}&lt;/span&gt;&lt;/p&gt;\n        &lt;p&gt;Thanks,&lt;br&gt;The {APP_NAME} Team&lt;/p&gt;\n    \"\"\"\n    return sender.send_html_email(user_email, subject, content_html, preview_text)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.email_services.send_signup_invitation_email","title":"<code>send_signup_invitation_email(app, invited_user_email, invited_username, inviter_username=None)</code>","text":"<p>Generates an invitation link and sends it via email.</p> Source code in <code>toolboxv2/mods/CloudM/email_services.py</code> <pre><code>@s_export\ndef send_signup_invitation_email(app: App, invited_user_email: str, invited_username: str,\n                                 inviter_username: str = None):\n    \"\"\"Generates an invitation link and sends it via email.\"\"\"\n    sender = EmailSender(app)\n\n    # Generate invitation code as specified in the prompt\n    # This uses the Code class, assuming TB_R_KEY is set in the environment\n    invitation_code = Code.one_way_hash(invited_username, \"00#\", os.getenv(\"TB_R_KEY\", \"pepper123\"))[:12] + str(\n        uuid.uuid4())[:6]\n\n    # Construct the signup link URL (adjust your frontend signup path as needed)\n    signup_link_url = f\"{APP_BASE_URL}/web/assets/signup.html?invitation={quote(invitation_code)}&amp;email={quote(invited_user_email)}&amp;username={quote(invited_username)}\"\n\n    subject = f\"You're Invited to Join {APP_NAME}!\"\n    preview_text = f\"{inviter_username or 'A friend'} has invited you to {APP_NAME}!\"\n    inviter_line = f\"&lt;p&gt;{inviter_username} has invited you to join.&lt;/p&gt;\" if inviter_username else \"&lt;p&gt;You've been invited to join.&lt;/p&gt;\"\n\n    content_html = f\"\"\"\n        &lt;h2&gt;Hello {invited_username},&lt;/h2&gt;\n        {inviter_line}\n        &lt;p&gt;{APP_NAME} is an exciting platform, and we'd love for you to be a part of it!&lt;/p&gt;\n        &lt;p&gt;Click the button below to accept the invitation and create your account:&lt;/p&gt;\n        &lt;a href=\"{signup_link_url}\" class=\"button\"&gt;Accept Invitation &amp; Sign Up&lt;/a&gt;\n        &lt;p&gt;This invitation is unique to you : {invitation_code}&lt;/p&gt;\n        &lt;p&gt;If the button doesn't work, copy and paste this link into your browser:&lt;br&gt;&lt;span class=\"link-in-text\"&gt;{signup_link_url}&lt;/span&gt;&lt;/p&gt;\n        &lt;p&gt;We look forward to seeing you there!&lt;br&gt;The {APP_NAME} Team&lt;/p&gt;\n    \"\"\"\n    return sender.send_html_email(invited_user_email, subject, content_html, preview_text)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.email_services.send_waiting_list_confirmation_email","title":"<code>send_waiting_list_confirmation_email(app, user_email)</code>","text":"<p>Sends a confirmation email for joining the waiting list.</p> Source code in <code>toolboxv2/mods/CloudM/email_services.py</code> <pre><code>@s_export\ndef send_waiting_list_confirmation_email(app: App, user_email: str):\n    \"\"\"Sends a confirmation email for joining the waiting list.\"\"\"\n    sender = EmailSender(app)\n    subject = f\"You're on the Waiting List for {APP_NAME}!\"\n    preview_text = \"Thanks for your interest! We'll keep you updated.\"\n\n    content_html = f\"\"\"\n        &lt;h2&gt;You're In!&lt;/h2&gt;\n        &lt;p&gt;Thank you for joining the waiting list for {APP_NAME}. We're working hard to get things ready and appreciate your interest.&lt;/p&gt;\n        &lt;p&gt;We'll notify you as soon as we have updates or when access becomes available.&lt;/p&gt;\n        &lt;p&gt;In the meantime, you can follow our progress or learn more at &lt;a href=\"{APP_BASE_URL}\" class=\"link-in-text\"&gt;{APP_BASE_URL}&lt;/a&gt;.&lt;/p&gt;\n        &lt;p&gt;Stay tuned,&lt;br&gt;The {APP_NAME} Team&lt;/p&gt;\n    \"\"\"\n    return sender.send_html_email(user_email, subject, content_html, preview_text,\n                                  recipient_email_for_unsubscribe=user_email, show_unsubscribe_link=True)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.email_services.send_welcome_email","title":"<code>send_welcome_email(app, user_email, username, welcome_action_url=None)</code>","text":"<p>Sends a welcome email to a new user.</p> Source code in <code>toolboxv2/mods/CloudM/email_services.py</code> <pre><code>@s_export  # Changed to native, api=False as it's a backend function\ndef send_welcome_email(app: App, user_email: str, username: str, welcome_action_url: str = None):\n    \"\"\"Sends a welcome email to a new user.\"\"\"\n    sender = EmailSender(app)\n    subject = f\"Welcome to {APP_NAME}, {username}!\"\n    preview_text = f\"We're thrilled to have you, {username}!\"\n    action_url = welcome_action_url or f\"{APP_BASE_URL}/dashboard\"  # Default to dashboard\n\n    content_html = f\"\"\"\n        &lt;h2&gt;Welcome Aboard, {username}!&lt;/h2&gt;\n        &lt;p&gt;Thank you for signing up for {APP_NAME}. We're excited to have you join our community!&lt;/p&gt;\n        &lt;p&gt;Here are a few things you might want to do next:&lt;/p&gt;\n        &lt;ul&gt;\n            &lt;li&gt;Explore your new account features.&lt;/li&gt;\n            &lt;li&gt;Customize your profile.&lt;/li&gt;\n        &lt;/ul&gt;\n        &lt;p&gt;Click the button below to get started:&lt;/p&gt;\n        &lt;a href=\"{action_url}\" class=\"button\"&gt;Go to Your Dashboard&lt;/a&gt;\n        &lt;p&gt;If the button doesn't work, copy and paste this link into your browser:&lt;br&gt;&lt;span class=\"link-in-text\"&gt;{action_url}&lt;/span&gt;&lt;/p&gt;\n        &lt;p&gt;Best regards,&lt;br&gt;The {APP_NAME} Team&lt;/p&gt;\n    \"\"\"\n    return sender.send_html_email(user_email, subject, content_html, preview_text,\n                                  recipient_email_for_unsubscribe=user_email, show_unsubscribe_link=True)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.mini","title":"<code>mini</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.mini.check_multiple_processes","title":"<code>check_multiple_processes(pids)</code>","text":"<p>Checks the status of multiple processes in a single system call. Returns a dictionary mapping PIDs to their status (GREEN_CIRCLE, RED_CIRCLE, or YELLOW_CIRCLE).</p> Source code in <code>toolboxv2/mods/CloudM/mini.py</code> <pre><code>def check_multiple_processes(pids: list[int]) -&gt; dict[int, str]:\n    \"\"\"\n    Checks the status of multiple processes in a single system call.\n    Returns a dictionary mapping PIDs to their status (GREEN_CIRCLE, RED_CIRCLE, or YELLOW_CIRCLE).\n    \"\"\"\n    if not pids:\n        return {}\n\n    pid_status = {}\n\n    if os.name == 'nt':  # Windows\n        try:\n            # Windows tasklist requires separate /FI for each filter\n            command = 'tasklist'\n\n            # Add encoding handling for Windows\n            result = subprocess.run(\n                command,\n                capture_output=True,\n                text=True,\n                shell=True,\n                encoding='cp850'  # Use cp850 for Windows console output\n            )\n            # Create a set of running PIDs from the output\n            running_pids = set()\n            for line in result.stdout.lower().split('\\n'):\n                for pid in pids:\n                    if str(pid) in line:\n                        running_pids.add(pid)\n            # Assign status based on whether PID was found in output\n            for pid in pids:\n                if pid in running_pids:\n                    pid_status[pid] = GREEN_CIRCLE\n                else:\n                    pid_status[pid] = RED_CIRCLE\n\n        except subprocess.SubprocessError as e:\n            print(f\"SubprocessError: {e}\")  # For debugging\n            # Mark all as YELLOW_CIRCLE if there's an error running the command\n            for pid in pids:\n                pid_status[pid] = YELLOW_CIRCLE\n        except UnicodeDecodeError as e:\n            print(f\"UnicodeDecodeError: {e}\")  # For debugging\n            # Try alternate encoding if cp850 fails\n            try:\n                result = subprocess.run(\n                    command,\n                    capture_output=True,\n                    text=True,\n                    shell=True,\n                    encoding='utf-8'\n                )\n                running_pids = set()\n                for line in result.stdout.lower().split('\\n'):\n                    for pid in pids:\n                        if str(pid) in line:\n                            running_pids.add(pid)\n\n                for pid in pids:\n                    pid_status[pid] = GREEN_CIRCLE if pid in running_pids else RED_CIRCLE\n            except Exception as e:\n                print(f\"Failed with alternate encoding: {e}\")  # For debugging\n                for pid in pids:\n                    pid_status[pid] = YELLOW_CIRCLE\n\n    else:  # Unix/Linux/Mac\n        try:\n            pids_str = ','.join(str(pid) for pid in pids)\n            command = f'ps -p {pids_str} -o pid='\n\n            result = subprocess.run(\n                command,\n                capture_output=True,\n                text=True,\n                shell=True,\n                encoding='utf-8'\n            )\n            running_pids = set(int(pid) for pid in result.stdout.strip().split())\n\n            for pid in pids:\n                pid_status[pid] = GREEN_CIRCLE if pid in running_pids else RED_CIRCLE\n\n        except subprocess.SubprocessError as e:\n            print(f\"SubprocessError: {e}\")  # For debugging\n            for pid in pids:\n                pid_status[pid] = YELLOW_CIRCLE\n\n    return pid_status\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.mini.get_service_pids","title":"<code>get_service_pids(info_dir)</code>","text":"<p>Extracts service names and PIDs from pid files.</p> Source code in <code>toolboxv2/mods/CloudM/mini.py</code> <pre><code>def get_service_pids(info_dir):\n    \"\"\"Extracts service names and PIDs from pid files.\"\"\"\n    services = {}\n    pid_files = [f for f in os.listdir(info_dir) if re.match(r'(.+)-(.+)\\.pid', f)]\n    for pid_file in pid_files:\n        match = re.match(r'(.+)-(.+)\\.pid', pid_file)\n        if match:\n            services_type, service_name = match.groups()\n            # Read the PID from the file\n            with open(os.path.join(info_dir, pid_file)) as file:\n                pid = file.read().strip()\n                # Store the PID using a formatted key\n                services[f\"{service_name} - {services_type}\"] = int(pid)\n    return services\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.mini.get_service_status","title":"<code>get_service_status(dir)</code>","text":"<p>Displays the status of all services.</p> Source code in <code>toolboxv2/mods/CloudM/mini.py</code> <pre><code>def get_service_status(dir: str) -&gt; str:\n    \"\"\"Displays the status of all services.\"\"\"\n    if time.time()-services_data_sto_last_update_time[0] &gt; 30:\n        services = get_service_pids(dir)\n        services_data_sto[0] = services\n        services_data_sto_last_update_time[0] = time.time()\n    else:\n        services = services_data_sto[0]\n    if not services:\n        return \"No services found\"\n\n    # Get status for all PIDs in a single call\n    pid_statuses = check_multiple_processes(list(services.values()))\n\n    # Build the status string\n    res_s = \"Service(s):\" + (\"\\n\" if len(services) &gt; 1 else ' ')\n    for service_name, pid in services.items():\n        status = pid_statuses.get(pid, YELLOW_CIRCLE)\n        res_s += f\"{status} {service_name} (PID: {pid})\\n\"\n    services_data_display[0] = res_s.strip()\n    return res_s.rstrip()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.module","title":"<code>module</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.module.hash_password","title":"<code>hash_password(password)</code>","text":"<p>Hash a password for storing.</p> Source code in <code>toolboxv2/mods/CloudM/module.py</code> <pre><code>def hash_password(password):\n    \"\"\"Hash a password for storing.\"\"\"\n    salt = hashlib.sha256(os.urandom(60)).hexdigest().encode('ascii')\n    pwdhash = hashlib.pbkdf2_hmac('sha512', password.encode('utf-8'), salt,\n                                  100000)\n    pwdhash = binascii.hexlify(pwdhash)\n    return (salt + pwdhash).decode('ascii')\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.module.verify_password","title":"<code>verify_password(stored_password, provided_password)</code>","text":"<p>Verify a stored password against one provided by user</p> Source code in <code>toolboxv2/mods/CloudM/module.py</code> <pre><code>def verify_password(stored_password, provided_password):\n    \"\"\"Verify a stored password against one provided by user\"\"\"\n    salt = stored_password[:64]\n    stored_password = stored_password[64:]\n    pwdhash = hashlib.pbkdf2_hmac('sha512', provided_password.encode('utf-8'),\n                                  salt.encode('ascii'), 100000)\n    pwdhash = binascii.hexlify(pwdhash).decode('ascii')\n    return pwdhash == stored_password\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification","title":"<code>CodeVerification</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification.VerificationSystem","title":"<code>VerificationSystem</code>","text":"Source code in <code>toolboxv2/mods/CodeVerification.py</code> <pre><code>class VerificationSystem:\n    def __init__(self, tools_db, scope=\"main\"):\n        \"\"\"\n        Initialize VerificationSystem with DB Tools integration\n\n        Args:\n            tools_db (Tools): Database tools from toolboxv2.mods.DB\n            scope (str, optional): Scope for templates and codes. Defaults to \"main\".\n        \"\"\"\n        self.tools_db = tools_db\n        self.scope = scope\n        self.tidmp = {}\n        self._ensure_scope_templates()\n\n    def get(self):\n        return self\n\n    def reset_scope_templates(self):\n        \"\"\"\n        Ensure a templates dictionary exists for the current scope in the database\n        \"\"\"\n        templates_key = f\"verification_templates_{self.scope}\"\n\n        self.tools_db.set(templates_key, json.dumps({}))\n\n    def _ensure_scope_templates(self):\n        \"\"\"\n        Ensure a templates dictionary exists for the current scope in the database\n        \"\"\"\n        templates_key = f\"verification_templates_{self.scope}\"\n\n        # Check if templates exist for this scope\n        templates_exist = self.tools_db.if_exist(templates_key)\n\n        if templates_exist.is_error() and not templates_exist.is_data():\n            # Initialize empty templates dictionary if not exists\n            self.tools_db.set(templates_key, json.dumps({}))\n        else:\n            allt = self.get_all_templates()\n\n            for k, v in allt.items():\n                if 'name' not in v:\n                    continue\n                self.tidmp[v['name']] = k\n\n    def add_config_template(self, template: ConfigTemplate) -&gt; str:\n        \"\"\"\n        Add a new configuration template to the database\n\n        Args:\n            template (ConfigTemplate): The configuration template\n\n        Returns:\n            str: Unique identifier of the template\n        \"\"\"\n        # Ensure template has the current scope\n        template.scope = self.scope\n\n        # Generate a unique template ID\n        template_id = secrets.token_urlsafe(8)\n\n        # Get existing templates for this scope\n        templates = self.get_all_templates()\n\n        # Add new template\n        self.tidmp[template.name] = template_id\n        templates[template_id] = asdict(template)\n\n        # Save updated templates back to database\n        templates_key = f\"verification_templates_{self.scope}\"\n        save_result = self.tools_db.set(templates_key, json.dumps(templates))\n\n        if save_result.is_error():\n            raise ValueError(\"Could not save template\")\n\n        return template_id\n\n    def get_all_templates(self):\n        templates_key = f\"verification_templates_{self.scope}\"\n        templates_result = self.tools_db.get(templates_key)\n\n        if not templates_result.is_error() and templates_result.is_data():\n            try:\n                templates_result.result.data = json.loads(templates_result.get())\n            except Exception as e:\n                templates_result.print()\n                print(f\"Errro loding template data curupted : {str(e)}\")\n                templates_result.result.data = {}\n        else:\n            templates_result.result.data = {}\n        if not isinstance(templates_result, dict):\n            templates_result = templates_result.result.data\n        return templates_result\n\n    def generate_code(self, template_id: str) -&gt; str:\n        \"\"\"\n        Generate a code based on the configuration template\n\n        Args:\n            template_id (str): ID of the configuration template\n\n        Returns:\n            str: Generated verification code\n        \"\"\"\n        # Get templates for this scope\n        templates = self.get_all_templates()\n        print(templates, self.tidmp, template_id)\n        if template_id not in templates:\n            template_id = self.tidmp.get(template_id, template_id)\n        if template_id not in templates:\n            raise ValueError(\"Invalid configuration template\")\n\n        template_dict = templates[template_id]\n        ConfigTemplate(**template_dict)\n\n        # Generate a random code with max 16 characters\n        code = secrets.token_urlsafe(10)[:16]\n\n        # Prepare code information\n        code_info = {\n            'template_id': template_id,\n            'created_at': time.time(),\n            'uses_count': 0,\n            'scope': self.scope\n        }\n\n        # Store code information in database\n        codes_key = f\"verification_codes_{self.scope}\"\n        existing_codes_result = self.tools_db.get(codes_key)\n\n        existing_codes = {}\n        if not existing_codes_result.is_error() and existing_codes_result.is_data():\n            d = existing_codes_result.get()\n            if isinstance(d, list):\n                d = d[0]\n            existing_codes = json.loads(d)\n\n        existing_codes[code] = code_info\n\n        save_result = self.tools_db.set(codes_key, json.dumps(existing_codes))\n\n        if save_result.is_error():\n            raise ValueError(\"Could not save generated code\")\n\n        return code\n\n    def validate_code(self, code: str) -&gt; dict[str, Any] | None:\n        \"\"\"\n        Validate a code and return template information\n\n        Args:\n            code (str): Code to validate\n\n        Returns:\n            Optional[Dict[str, Any]]: Template information for valid code, else None\n        \"\"\"\n        # Get codes for this scope\n        codes_key = f\"verification_codes_{self.scope}\"\n        codes_result = self.tools_db.get(codes_key)\n\n        if codes_result.is_error() or not codes_result.is_data():\n            return None\n\n        d = codes_result.get()\n        if isinstance(d, list):\n            d = d[0]\n        existing_codes = json.loads(d)\n\n        if code not in existing_codes:\n            return None\n\n        code_info = existing_codes[code]\n\n        # Check if code is from the same scope\n        if code_info.get('scope') != self.scope:\n            return None\n\n        # Get templates for this scope\n        templates = self.get_all_templates()\n        template_id = code_info['template_id']\n\n        if template_id not in templates:\n            return templates\n\n        template_dict = templates[template_id]\n        template = ConfigTemplate(**template_dict)\n\n        # Check usage count\n        if code_info['uses_count'] &gt;= template.max_uses:\n            del existing_codes[code]\n            self.tools_db.set(codes_key, json.dumps(existing_codes))\n            return None\n\n        # Check time validity for timed codes\n        if template.usage_type == 'timed':\n            current_time = time.time()\n            if template.valid_duration and (current_time - code_info['created_at']) &gt; template.valid_duration:\n                del existing_codes[code]\n                self.tools_db.set(codes_key, json.dumps(existing_codes))\n                return None\n\n        # Update uses count\n        existing_codes[code]['uses_count'] += 1\n        uses_count = existing_codes[code].get('uses_count', 1)\n        # Remove code if it's a one-time use\n        if template.usage_type == 'one_time':\n            del existing_codes[code]\n\n        # Save updated codes\n        self.tools_db.set(codes_key, json.dumps(existing_codes))\n\n        return {\n            'template_name': template.name,\n            'usage_type': template.usage_type,\n            'uses_count': uses_count\n        }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification.VerificationSystem.__init__","title":"<code>__init__(tools_db, scope='main')</code>","text":"<p>Initialize VerificationSystem with DB Tools integration</p> <p>Parameters:</p> Name Type Description Default <code>tools_db</code> <code>Tools</code> <p>Database tools from toolboxv2.mods.DB</p> required <code>scope</code> <code>str</code> <p>Scope for templates and codes. Defaults to \"main\".</p> <code>'main'</code> Source code in <code>toolboxv2/mods/CodeVerification.py</code> <pre><code>def __init__(self, tools_db, scope=\"main\"):\n    \"\"\"\n    Initialize VerificationSystem with DB Tools integration\n\n    Args:\n        tools_db (Tools): Database tools from toolboxv2.mods.DB\n        scope (str, optional): Scope for templates and codes. Defaults to \"main\".\n    \"\"\"\n    self.tools_db = tools_db\n    self.scope = scope\n    self.tidmp = {}\n    self._ensure_scope_templates()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification.VerificationSystem.add_config_template","title":"<code>add_config_template(template)</code>","text":"<p>Add a new configuration template to the database</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>ConfigTemplate</code> <p>The configuration template</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unique identifier of the template</p> Source code in <code>toolboxv2/mods/CodeVerification.py</code> <pre><code>def add_config_template(self, template: ConfigTemplate) -&gt; str:\n    \"\"\"\n    Add a new configuration template to the database\n\n    Args:\n        template (ConfigTemplate): The configuration template\n\n    Returns:\n        str: Unique identifier of the template\n    \"\"\"\n    # Ensure template has the current scope\n    template.scope = self.scope\n\n    # Generate a unique template ID\n    template_id = secrets.token_urlsafe(8)\n\n    # Get existing templates for this scope\n    templates = self.get_all_templates()\n\n    # Add new template\n    self.tidmp[template.name] = template_id\n    templates[template_id] = asdict(template)\n\n    # Save updated templates back to database\n    templates_key = f\"verification_templates_{self.scope}\"\n    save_result = self.tools_db.set(templates_key, json.dumps(templates))\n\n    if save_result.is_error():\n        raise ValueError(\"Could not save template\")\n\n    return template_id\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification.VerificationSystem.generate_code","title":"<code>generate_code(template_id)</code>","text":"<p>Generate a code based on the configuration template</p> <p>Parameters:</p> Name Type Description Default <code>template_id</code> <code>str</code> <p>ID of the configuration template</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated verification code</p> Source code in <code>toolboxv2/mods/CodeVerification.py</code> <pre><code>def generate_code(self, template_id: str) -&gt; str:\n    \"\"\"\n    Generate a code based on the configuration template\n\n    Args:\n        template_id (str): ID of the configuration template\n\n    Returns:\n        str: Generated verification code\n    \"\"\"\n    # Get templates for this scope\n    templates = self.get_all_templates()\n    print(templates, self.tidmp, template_id)\n    if template_id not in templates:\n        template_id = self.tidmp.get(template_id, template_id)\n    if template_id not in templates:\n        raise ValueError(\"Invalid configuration template\")\n\n    template_dict = templates[template_id]\n    ConfigTemplate(**template_dict)\n\n    # Generate a random code with max 16 characters\n    code = secrets.token_urlsafe(10)[:16]\n\n    # Prepare code information\n    code_info = {\n        'template_id': template_id,\n        'created_at': time.time(),\n        'uses_count': 0,\n        'scope': self.scope\n    }\n\n    # Store code information in database\n    codes_key = f\"verification_codes_{self.scope}\"\n    existing_codes_result = self.tools_db.get(codes_key)\n\n    existing_codes = {}\n    if not existing_codes_result.is_error() and existing_codes_result.is_data():\n        d = existing_codes_result.get()\n        if isinstance(d, list):\n            d = d[0]\n        existing_codes = json.loads(d)\n\n    existing_codes[code] = code_info\n\n    save_result = self.tools_db.set(codes_key, json.dumps(existing_codes))\n\n    if save_result.is_error():\n        raise ValueError(\"Could not save generated code\")\n\n    return code\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification.VerificationSystem.reset_scope_templates","title":"<code>reset_scope_templates()</code>","text":"<p>Ensure a templates dictionary exists for the current scope in the database</p> Source code in <code>toolboxv2/mods/CodeVerification.py</code> <pre><code>def reset_scope_templates(self):\n    \"\"\"\n    Ensure a templates dictionary exists for the current scope in the database\n    \"\"\"\n    templates_key = f\"verification_templates_{self.scope}\"\n\n    self.tools_db.set(templates_key, json.dumps({}))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification.VerificationSystem.validate_code","title":"<code>validate_code(code)</code>","text":"<p>Validate a code and return template information</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>Code to validate</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>Optional[Dict[str, Any]]: Template information for valid code, else None</p> Source code in <code>toolboxv2/mods/CodeVerification.py</code> <pre><code>def validate_code(self, code: str) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Validate a code and return template information\n\n    Args:\n        code (str): Code to validate\n\n    Returns:\n        Optional[Dict[str, Any]]: Template information for valid code, else None\n    \"\"\"\n    # Get codes for this scope\n    codes_key = f\"verification_codes_{self.scope}\"\n    codes_result = self.tools_db.get(codes_key)\n\n    if codes_result.is_error() or not codes_result.is_data():\n        return None\n\n    d = codes_result.get()\n    if isinstance(d, list):\n        d = d[0]\n    existing_codes = json.loads(d)\n\n    if code not in existing_codes:\n        return None\n\n    code_info = existing_codes[code]\n\n    # Check if code is from the same scope\n    if code_info.get('scope') != self.scope:\n        return None\n\n    # Get templates for this scope\n    templates = self.get_all_templates()\n    template_id = code_info['template_id']\n\n    if template_id not in templates:\n        return templates\n\n    template_dict = templates[template_id]\n    template = ConfigTemplate(**template_dict)\n\n    # Check usage count\n    if code_info['uses_count'] &gt;= template.max_uses:\n        del existing_codes[code]\n        self.tools_db.set(codes_key, json.dumps(existing_codes))\n        return None\n\n    # Check time validity for timed codes\n    if template.usage_type == 'timed':\n        current_time = time.time()\n        if template.valid_duration and (current_time - code_info['created_at']) &gt; template.valid_duration:\n            del existing_codes[code]\n            self.tools_db.set(codes_key, json.dumps(existing_codes))\n            return None\n\n    # Update uses count\n    existing_codes[code]['uses_count'] += 1\n    uses_count = existing_codes[code].get('uses_count', 1)\n    # Remove code if it's a one-time use\n    if template.usage_type == 'one_time':\n        del existing_codes[code]\n\n    # Save updated codes\n    self.tools_db.set(codes_key, json.dumps(existing_codes))\n\n    return {\n        'template_name': template.name,\n        'usage_type': template.usage_type,\n        'uses_count': uses_count\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB","title":"<code>DB</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.DB.blob_instance","title":"<code>blob_instance</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.DB.blob_instance.BlobDB","title":"<code>BlobDB</code>","text":"<p>A persistent, encrypted dictionary-like database that uses the BlobStorage system as its backend, making it networked and fault-tolerant.</p> Source code in <code>toolboxv2/mods/DB/blob_instance.py</code> <pre><code>class BlobDB:\n    \"\"\"\n    A persistent, encrypted dictionary-like database that uses the BlobStorage\n    system as its backend, making it networked and fault-tolerant.\n    \"\"\"\n    auth_type = AuthenticationTypes.location\n\n    def __init__(self):\n        self.data: dict = {}\n        self.key: str | None = None\n        self.db_path: str | None = None\n        self.storage_client: BlobStorage | None = None\n\n\n    def initialize(self, db_path: str, key: str, storage_client: BlobStorage) -&gt; Result:\n        \"\"\"\n        Initializes the database from a location within the blob storage.\n\n        Args:\n            db_path (str): The virtual path within the blob storage,\n                           e.g., \"my_database_blob/database.json\".\n            key (str): The encryption key for the database content.\n            storage_client (BlobStorage): An initialized BlobStorage client instance.\n\n        Returns:\n            Result: An OK result if successful.\n        \"\"\"\n        self.db_path = db_path\n        self.key = key\n        self.storage_client = storage_client\n\n        print(f\"Initializing BlobDB from blob path: '{self.db_path}'...\")\n\n        try:\n            # Use BlobFile for reading. It handles caching, networking, and decryption.\n            db_file = BlobFile(self.db_path, mode='r', storage=self.storage_client, key=self.key)\n            if not db_file.exists():\n                print(f\"Database file not found at '{self.db_path}'. Starting with an empty database.\")\n                db_file.create()\n                self.data = {}\n            else:\n                with db_file as f:\n                    # read_json safely loads the content.\n                    self.data = f.read_json()\n                    if not self.data:  # Handle case where file exists but is empty\n                        self.data = {}\n                print(\"Successfully initialized database.\")\n\n        except Exception as e:\n            print(f\"Warning: Could not initialize BlobDB from '{self.db_path}'. Error: {e}. Starting fresh.\")\n            self.data = {}\n\n        return Result.ok().set_origin(\"Blob Dict DB\")\n\n    def exit(self) -&gt; Result:\n        \"\"\"\n        Saves the current state of the database back to the blob storage.\n        \"\"\"\n        print(\"BLOB DB on exit \", not all([self.key, self.db_path, self.storage_client]))\n        if not all([self.key, self.db_path, self.storage_client]):\n            return Result.default_internal_error(\n                info=\"Database not initialized. Cannot exit.\"\n            ).set_origin(\"Blob Dict DB\")\n\n        print(f\"Saving database to blob path: '{self.db_path}'...\")\n        try:\n            # Use BlobFile for writing. It handles encryption, networking, and updates.\n            with BlobFile(self.db_path, mode='w', storage=self.storage_client, key=self.key) as f:\n                f.write_json(self.data)\n\n            print(\"Success: Database saved to blob storage.\")\n            return Result.ok().set_origin(\"Blob Dict DB\")\n\n        except Exception as e:\n            return Result.custom_error(\n                data=e,\n                info=f\"Error saving database to blob storage: {e}\"\n            ).set_origin(\"Blob Dict DB\")\n\n    # --- Data Manipulation Methods (Unchanged Logic) ---\n    # These methods operate on the in-memory `self.data` dictionary and do not\n    # need to be changed, as the loading/saving is handled by initialize/exit.\n\n    def get(self, key: str) -&gt; Result:\n        if not self.data:\n            return Result.default_internal_error(info=f\"No data found for key '{key}' (database is empty).\").set_origin(\n                \"Blob Dict DB\")\n\n        data = []\n        if key == 'all':\n            data_info = \"Returning all data available\"\n            data = list(self.data.items())\n        elif key == \"all-k\":\n            data_info = \"Returning all keys\"\n            data = list(self.data.keys())\n        else:\n            data_info = f\"Returning values for keys starting with '{key.replace('*', '')}'\"\n            data = [self.data[k] for k in self.scan_iter(key)]\n\n        if not data:\n            return Result.default_internal_error(info=f\"No data found for key '{key}'\").set_origin(\"Blob Dict DB\")\n\n        return Result.ok(data=data, data_info=data_info).set_origin(\"Blob Dict DB\")\n\n    def set(self, key: str, value) -&gt; Result:\n        if not isinstance(key, str) or not key:\n            return Result.default_user_error(info=\"Key must be a non-empty string.\").set_origin(\"Blob Dict DB\")\n\n        self.data[key] = value\n        return Result.ok().set_origin(\"Blob Dict DB\")\n\n    def scan_iter(self, search: str = ''):\n        if not self.data:\n            return []\n        prefix = search.replace('*', '')\n        return [key for key in self.data if key.startswith(prefix)]\n\n    def append_on_set(self, key: str, value: list) -&gt; Result:\n        if key not in self.data:\n            self.data[key] = []\n\n        if not isinstance(self.data[key], list):\n            return Result.default_user_error(info=f\"Existing value for key '{key}' is not a list.\").set_origin(\n                \"Blob Dict DB\")\n\n        # Use a set for efficient checking to avoid duplicates\n        existing_set = set(self.data[key])\n        new_items = [item for item in value if item not in existing_set]\n        self.data[key].extend(new_items)\n        return Result.ok().set_origin(\"Blob Dict DB\")\n\n    def if_exist(self, key: str) -&gt; int:\n        if key.endswith('*'):\n            return len(self.scan_iter(key))\n        return 1 if key in self.data else 0\n\n    def delete(self, key: str, matching: bool = False) -&gt; Result:\n        keys_to_delete = []\n        if matching:\n            keys_to_delete = self.scan_iter(key)\n        elif key in self.data:\n            keys_to_delete.append(key)\n\n        if not keys_to_delete:\n            return Result.default_internal_error(info=f\"No keys found to delete for pattern '{key}'\").set_origin(\n                \"Blob Dict DB\")\n\n        deleted_items = {k: self.data.pop(k) for k in keys_to_delete}\n        return Result.ok(\n            data=list(deleted_items.items()),\n            data_info=f\"Successfully removed {len(deleted_items)} item(s).\"\n        ).set_origin(\"Blob Dict DB\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.blob_instance.BlobDB.exit","title":"<code>exit()</code>","text":"<p>Saves the current state of the database back to the blob storage.</p> Source code in <code>toolboxv2/mods/DB/blob_instance.py</code> <pre><code>def exit(self) -&gt; Result:\n    \"\"\"\n    Saves the current state of the database back to the blob storage.\n    \"\"\"\n    print(\"BLOB DB on exit \", not all([self.key, self.db_path, self.storage_client]))\n    if not all([self.key, self.db_path, self.storage_client]):\n        return Result.default_internal_error(\n            info=\"Database not initialized. Cannot exit.\"\n        ).set_origin(\"Blob Dict DB\")\n\n    print(f\"Saving database to blob path: '{self.db_path}'...\")\n    try:\n        # Use BlobFile for writing. It handles encryption, networking, and updates.\n        with BlobFile(self.db_path, mode='w', storage=self.storage_client, key=self.key) as f:\n            f.write_json(self.data)\n\n        print(\"Success: Database saved to blob storage.\")\n        return Result.ok().set_origin(\"Blob Dict DB\")\n\n    except Exception as e:\n        return Result.custom_error(\n            data=e,\n            info=f\"Error saving database to blob storage: {e}\"\n        ).set_origin(\"Blob Dict DB\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.blob_instance.BlobDB.initialize","title":"<code>initialize(db_path, key, storage_client)</code>","text":"<p>Initializes the database from a location within the blob storage.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>The virtual path within the blob storage,            e.g., \"my_database_blob/database.json\".</p> required <code>key</code> <code>str</code> <p>The encryption key for the database content.</p> required <code>storage_client</code> <code>BlobStorage</code> <p>An initialized BlobStorage client instance.</p> required <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An OK result if successful.</p> Source code in <code>toolboxv2/mods/DB/blob_instance.py</code> <pre><code>def initialize(self, db_path: str, key: str, storage_client: BlobStorage) -&gt; Result:\n    \"\"\"\n    Initializes the database from a location within the blob storage.\n\n    Args:\n        db_path (str): The virtual path within the blob storage,\n                       e.g., \"my_database_blob/database.json\".\n        key (str): The encryption key for the database content.\n        storage_client (BlobStorage): An initialized BlobStorage client instance.\n\n    Returns:\n        Result: An OK result if successful.\n    \"\"\"\n    self.db_path = db_path\n    self.key = key\n    self.storage_client = storage_client\n\n    print(f\"Initializing BlobDB from blob path: '{self.db_path}'...\")\n\n    try:\n        # Use BlobFile for reading. It handles caching, networking, and decryption.\n        db_file = BlobFile(self.db_path, mode='r', storage=self.storage_client, key=self.key)\n        if not db_file.exists():\n            print(f\"Database file not found at '{self.db_path}'. Starting with an empty database.\")\n            db_file.create()\n            self.data = {}\n        else:\n            with db_file as f:\n                # read_json safely loads the content.\n                self.data = f.read_json()\n                if not self.data:  # Handle case where file exists but is empty\n                    self.data = {}\n            print(\"Successfully initialized database.\")\n\n    except Exception as e:\n        print(f\"Warning: Could not initialize BlobDB from '{self.db_path}'. Error: {e}. Starting fresh.\")\n        self.data = {}\n\n    return Result.ok().set_origin(\"Blob Dict DB\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.local_instance","title":"<code>local_instance</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.DB.local_instance.load_from_json","title":"<code>load_from_json(filename)</code>","text":"<p>L\u00e4dt Daten aus einer JSON-Datei.</p> <p>:param filename: Der Dateiname oder Pfad der zu ladenden Datei. :return: Die geladenen Daten.</p> Source code in <code>toolboxv2/mods/DB/local_instance.py</code> <pre><code>def load_from_json(filename):\n    \"\"\"\n    L\u00e4dt Daten aus einer JSON-Datei.\n\n    :param filename: Der Dateiname oder Pfad der zu ladenden Datei.\n    :return: Die geladenen Daten.\n    \"\"\"\n    if not os.path.exists(filename):\n        return {'data': ''}\n\n    with open(filename) as file:\n        return json.load(file)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.local_instance.save_to_json","title":"<code>save_to_json(data, filename)</code>","text":"<p>Speichert die \u00fcbergebenen Daten in einer JSON-Datei.</p> <p>:param data: Die zu speichernden Daten. :param filename: Der Dateiname oder Pfad, in dem die Daten gespeichert werden sollen.</p> Source code in <code>toolboxv2/mods/DB/local_instance.py</code> <pre><code>def save_to_json(data, filename):\n    \"\"\"\n    Speichert die \u00fcbergebenen Daten in einer JSON-Datei.\n\n    :param data: Die zu speichernden Daten.\n    :param filename: Der Dateiname oder Pfad, in dem die Daten gespeichert werden sollen.\n    \"\"\"\n    if not os.path.exists(filename):\n        open(filename, 'a').close()\n\n    with open(filename, 'w+') as file:\n        json.dump(data, file, indent=4)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.reddis_instance","title":"<code>reddis_instance</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.DB.reddis_instance.sync_redis_databases","title":"<code>sync_redis_databases(source_url, target_url)</code>","text":"<p>Synchronize keys from the source Redis database to the target Redis database. This function scans all keys in the source DB and uses DUMP/RESTORE to replicate data to the target.</p> <p>Parameters:</p> Name Type Description Default <code>source_url</code> <code>str</code> <p>The Redis URL of the source database.</p> required <code>target_url</code> <code>str</code> <p>The Redis URL of the target database.</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The number of keys successfully synchronized.</p> Source code in <code>toolboxv2/mods/DB/reddis_instance.py</code> <pre><code>def sync_redis_databases(source_url, target_url):\n    \"\"\"Synchronize keys from the source Redis database to the target Redis database.\n    This function scans all keys in the source DB and uses DUMP/RESTORE to replicate data to the target.\n\n    Args:\n        source_url (str): The Redis URL of the source database.\n        target_url (str): The Redis URL of the target database.\n\n    Returns:\n        int: The number of keys successfully synchronized.\n    \"\"\"\n    try:\n        src_client = redis.from_url(source_url)\n        tgt_client = redis.from_url(target_url)\n    except Exception as e:\n        print(f\"Error connecting to one of the Redis instances: {e}\")\n        return 0\n\n    total_synced = 0\n    cursor = 0\n    try:\n        while True:\n            cursor, keys = src_client.scan(cursor=cursor, count=100)\n            for key in keys:\n                try:\n                    serialized_value = src_client.dump(key)\n                    if serialized_value is None:\n                        continue\n                    # Restore key with TTL=0 and replace existing key\n                    tgt_client.restore(key, 0, serialized_value, replace=True)\n                    total_synced += 1\n                except Exception as e:\n                    print(f\"Error syncing key {key}: {e}\")\n            if cursor == 0:\n                break\n    except Exception as scan_error:\n        print(f\"Error during scanning keys: {scan_error}\")\n\n    print(f\"Synced {total_synced} keys from {source_url} to {target_url}\")\n    return total_synced\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter","title":"<code>tb_adapter</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB","title":"<code>DB</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>class DB(ABC):\n    @abc.abstractmethod\n    def get(self, query: str) -&gt; Result:\n        \"\"\"get data\"\"\"\n\n    @abc.abstractmethod\n    def set(self, query: str, value) -&gt; Result:\n        \"\"\"set data\"\"\"\n\n    @abc.abstractmethod\n    def append_on_set(self, query: str, value) -&gt; Result:\n        \"\"\"append set data\"\"\"\n\n    @abc.abstractmethod\n    def delete(self, query: str, matching=False) -&gt; Result:\n        \"\"\"delete data\"\"\"\n\n    @abc.abstractmethod\n    def if_exist(self, query: str) -&gt; bool:\n        \"\"\"return True if query exists\"\"\"\n\n    @abc.abstractmethod\n    def exit(self) -&gt; Result:\n        \"\"\"Close DB connection and optional save data\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB.append_on_set","title":"<code>append_on_set(query, value)</code>  <code>abstractmethod</code>","text":"<p>append set data</p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>@abc.abstractmethod\ndef append_on_set(self, query: str, value) -&gt; Result:\n    \"\"\"append set data\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB.delete","title":"<code>delete(query, matching=False)</code>  <code>abstractmethod</code>","text":"<p>delete data</p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>@abc.abstractmethod\ndef delete(self, query: str, matching=False) -&gt; Result:\n    \"\"\"delete data\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB.exit","title":"<code>exit()</code>  <code>abstractmethod</code>","text":"<p>Close DB connection and optional save data</p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>@abc.abstractmethod\ndef exit(self) -&gt; Result:\n    \"\"\"Close DB connection and optional save data\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB.get","title":"<code>get(query)</code>  <code>abstractmethod</code>","text":"<p>get data</p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>@abc.abstractmethod\ndef get(self, query: str) -&gt; Result:\n    \"\"\"get data\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB.if_exist","title":"<code>if_exist(query)</code>  <code>abstractmethod</code>","text":"<p>return True if query exists</p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>@abc.abstractmethod\ndef if_exist(self, query: str) -&gt; bool:\n    \"\"\"return True if query exists\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB.set","title":"<code>set(query, value)</code>  <code>abstractmethod</code>","text":"<p>set data</p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>@abc.abstractmethod\ndef set(self, query: str, value) -&gt; Result:\n    \"\"\"set data\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.ui","title":"<code>ui</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.DB.ui.api_change_mode","title":"<code>api_change_mode(self, request)</code>  <code>async</code>","text":"<p>Changes the database mode from a JSON POST body.</p> Source code in <code>toolboxv2/mods/DB/ui.py</code> <pre><code>@export(mod_name=Name, name=\"api_change_mode\", api=True, api_methods=['POST'], request_as_kwarg=True)\nasync def api_change_mode(self, request: RequestData):\n    \"\"\"Changes the database mode from a JSON POST body.\"\"\"\n    data = request.data\n    if not data or \"mode\" not in data:\n        return Result.default_user_error(\"Request body must contain 'mode'.\")\n    new_mode = data.get(\"mode\", \"LC\")\n    return self.edit_programmable(DatabaseModes.crate(new_mode))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.ui.api_delete_key","title":"<code>api_delete_key(self, request)</code>  <code>async</code>","text":"<p>Deletes a key from a JSON POST body.</p> Source code in <code>toolboxv2/mods/DB/ui.py</code> <pre><code>@export(mod_name=Name, name=\"api_delete_key\", api=True, api_methods=['POST'], request_as_kwarg=True)\nasync def api_delete_key(self, request: RequestData):\n    \"\"\"Deletes a key from a JSON POST body.\"\"\"\n    data = request.data\n    if not data or 'key' not in data:\n        return Result.default_user_error(\"Request body must contain 'key'.\")\n    key = data['key']\n    if not key:\n        return Result.default_user_error(\"Key parameter is required.\")\n    return self.delete(key)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.ui.api_get_all_keys","title":"<code>api_get_all_keys(self, request)</code>  <code>async</code>","text":"<p>Returns a list of all keys in the database.</p> Source code in <code>toolboxv2/mods/DB/ui.py</code> <pre><code>@export(mod_name=Name, name=\"api_get_all_keys\", api=True, request_as_kwarg=True)\nasync def api_get_all_keys(self, request: RequestData):\n    \"\"\"Returns a list of all keys in the database.\"\"\"\n    if self.data_base:\n        keys_result = self.data_base.get('all-k')\n        if keys_result.is_error():\n            return keys_result\n\n        unwrapped_keys = _unwrap_data(keys_result.get())\n        if not isinstance(unwrapped_keys, list):\n            self.app.logger.warning(f\"get_all_keys did not return a list. Got: {type(unwrapped_keys)}\")\n            return Result.json(data=[])\n\n        return Result.json(data=sorted(unwrapped_keys))\n    return Result.default_internal_error(\"DB not initialized\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.ui.api_get_status","title":"<code>api_get_status(self, request)</code>  <code>async</code>","text":"<p>Returns the current status of the DB manager.</p> Source code in <code>toolboxv2/mods/DB/ui.py</code> <pre><code>@export(mod_name=Name, name=\"api_get_status\", api=True, request_as_kwarg=True)\nasync def api_get_status(self, request: RequestData):\n    \"\"\"Returns the current status of the DB manager.\"\"\"\n    return Result.json(data={\"mode\": self.mode})\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.ui.api_get_value","title":"<code>api_get_value(self, request, key)</code>  <code>async</code>","text":"<p>Gets a value for a key and returns it as JSON-friendly text.</p> Source code in <code>toolboxv2/mods/DB/ui.py</code> <pre><code>@export(mod_name=Name, name=\"api_get_value\", api=True, request_as_kwarg=True)\nasync def api_get_value(self, request: RequestData, key: str):\n    \"\"\"Gets a value for a key and returns it as JSON-friendly text.\"\"\"\n    if not key:\n        return Result.default_user_error(\"Key parameter is required.\")\n    value_res = self.get(key)\n    if value_res.is_error():\n        return value_res\n\n    value_unwrapped = _unwrap_data(value_res.get())\n\n    if isinstance(value_unwrapped, bytes):\n        try:\n            value_str = value_unwrapped.decode('utf-8')\n        except UnicodeDecodeError:\n            value_str = str(value_unwrapped)\n    else:\n        value_str = str(value_unwrapped)\n\n    # Simplified for a JSON-focused UI. The client will handle formatting.\n    return Result.json(data={\"key\": key, \"value\": value_str})\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.ui.api_set_value","title":"<code>api_set_value(self, request)</code>  <code>async</code>","text":"<p>Sets a key-value pair from a JSON POST body.</p> Source code in <code>toolboxv2/mods/DB/ui.py</code> <pre><code>@export(mod_name=Name, name=\"api_set_value\", api=True, api_methods=['POST'], request_as_kwarg=True)\nasync def api_set_value(self, request: RequestData):\n    \"\"\"Sets a key-value pair from a JSON POST body.\"\"\"\n    data = request.data\n    if not data or 'key' not in data or 'value' not in data:\n        return Result.default_user_error(\"Request body must contain 'key' and 'value'.\")\n    key = data['key']\n    value = data['value']\n    if not key:\n        return Result.default_user_error(\"Key cannot be empty.\")\n    return self.set(key, value)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.ui.db_manager_ui","title":"<code>db_manager_ui(**kwargs)</code>","text":"<p>Serves the refactored, JSON-focused UI for the DB Manager.</p> Source code in <code>toolboxv2/mods/DB/ui.py</code> <pre><code>@export(mod_name=Name, name=\"ui\", api=True, state=False)\ndef db_manager_ui(**kwargs):\n    \"\"\"Serves the refactored, JSON-focused UI for the DB Manager.\"\"\"\n    html_content = \"\"\"\n    &lt;!DOCTYPE html&gt;\n    &lt;html lang=\"en\"&gt;\n    &lt;head&gt;\n        &lt;meta charset=\"UTF-8\"&gt;\n        &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n        &lt;title&gt;DB Manager&lt;/title&gt;\n        &lt;style&gt;\n            :root {\n                --font-family-sans: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif;\n                --font-family-mono: \"SF Mono\", \"Menlo\", \"Monaco\", \"Courier New\", Courier, monospace;\n                --color-bg: #f8f9fa;\n                --color-panel-bg: #ffffff;\n                --color-border: #dee2e6;\n                --color-text: #212529;\n                --color-text-muted: #6c757d;\n                --color-primary: #0d6efd;\n                --color-primary-hover: #0b5ed7;\n                --color-danger: #dc3545;\n                --color-danger-hover: #bb2d3b;\n                --color-key-folder-icon: #f7b731;\n                --color-key-file-icon: #adb5bd;\n                --color-key-hover-bg: #e9ecef;\n                --color-key-selected-bg: #0d6efd;\n                --color-key-selected-text: #ffffff;\n                --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\n                --radius: 0.375rem;\n            }\n\n            /* Basic styles */\n            * { box-sizing: border-box; }\n            html { font-size: 16px; }\n\n            body {\n                font-family: var(--font-family-sans);\n                background-color: var(--color-bg);\n                color: var(--color-text);\n                margin: 0;\n                padding: 1rem;\n                display: flex;\n                flex-direction: column;\n                height: 100vh;\n            }\n\n            /* Main layout */\n            .db-manager-container { display: flex; flex-direction: column; height: 100%; gap: 1rem; }\n            .db-header { display: flex; justify-content: space-between; align-items: center; padding-bottom: 1rem; border-bottom: 1px solid var(--color-border); flex-shrink: 0; }\n            .db-main-content { display: flex; gap: 1rem; flex: 1; min-height: 0; }\n\n            /* Panels */\n            .db-panel { background-color: var(--color-panel-bg); border: 1px solid var(--color-border); border-radius: var(--radius); box-shadow: var(--shadow-sm); display: flex; flex-direction: column; min-height: 0; }\n            .key-panel { width: 350px; min-width: 250px; max-width: 450px; }\n            .editor-panel, .placeholder-panel { flex-grow: 1; }\n            .panel-header { display: flex; justify-content: space-between; align-items: center; padding: 0.75rem 1rem; border-bottom: 1px solid var(--color-border); flex-shrink: 0; }\n            .panel-header h2 { font-size: 1.1rem; margin: 0; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; }\n\n            /* Controls */\n            select, input[type=\"text\"], textarea, button { font-size: 1rem; }\n            select, input[type=\"text\"] { background-color: var(--color-bg); color: var(--color-text); border: 1px solid var(--color-border); border-radius: var(--radius); padding: 0.5rem 0.75rem; }\n            select:focus, input[type=\"text\"]:focus, textarea:focus { outline: 2px solid var(--color-primary); outline-offset: -1px; }\n            button { border: none; border-radius: var(--radius); padding: 0.5rem 1rem; font-weight: 500; cursor: pointer; transition: background-color 0.2s; }\n            button.primary { background-color: var(--color-primary); color: white; }\n            button.primary:hover { background-color: var(--color-primary-hover); }\n            button.danger { background-color: var(--color-danger); color: white; }\n            button.danger:hover { background-color: var(--color-danger-hover); }\n            .header-actions { display: flex; gap: 0.5rem; }\n\n            /* Key Tree View */\n            #keySearchInput { width: calc(100% - 2rem); margin: 1rem; flex-shrink: 0; }\n            .key-tree-container { font-family: var(--font-family-mono); font-size: 0.9rem; padding: 0 0.5rem 1rem; overflow-y: auto; flex: 1; min-height: 0; }\n            .key-tree-container ul { list-style: none; padding-left: 0; margin: 0; }\n            .key-tree-container li { padding-left: 20px; position: relative; }\n            .node-label { display: flex; align-items: center; padding: 4px 8px; cursor: pointer; border-radius: 4px; word-break: break-all; user-select: none; }\n            .node-label:hover { background-color: var(--color-key-hover-bg); }\n            .node-label.selected { background-color: var(--color-key-selected-bg); color: var(--color-key-selected-text); }\n            .node-label.selected .node-icon { color: var(--color-key-selected-text) !important; }\n            .node-icon { width: 20px; text-align: center; margin-right: 5px; flex-shrink: 0; }\n            .tree-folder &gt; .node-label .node-icon { color: var(--color-key-folder-icon); font-style: normal; }\n            .tree-folder &gt; .node-label .node-icon::before { content: '\u25b8'; display: inline-block; transition: transform 0.15s ease-in-out; }\n            .tree-folder.open &gt; .node-label .node-icon::before { transform: rotate(90deg); }\n            .tree-leaf &gt; .node-label .node-icon { color: var(--color-key-file-icon); }\n            .tree-leaf &gt; .node-label .node-icon::before { content: '\u2022'; }\n            .tree-children { display: none; }\n            .tree-folder.open &gt; .tree-children { display: block; }\n\n            /* Editor Panel */\n            .editor-toolbar { display: flex; gap: 1rem; align-items: center; padding: 0.75rem 1rem; border-bottom: 1px solid var(--color-border); flex-shrink: 0; }\n            #valueEditor { flex: 1; width: 100%; min-height: 0; border: none; resize: none; font-family: var(--font-family-mono); font-size: 0.95rem; line-height: 1.5; padding: 1rem; background: transparent; color: var(--color-text); }\n            #valueEditor:focus { outline: none; }\n\n            /* Placeholder and Utility */\n            .placeholder-panel { display: flex; flex-direction: column; align-items: center; justify-content: center; color: var(--color-text-muted); text-align: center; }\n            .hidden { display: none !important; }\n            .key-tree-container p.status-message { padding: 1rem; margin: 0; color: var(--color-text-muted); text-align: center; }\n\n            /* Custom Scrollbars */\n            .key-tree-container::-webkit-scrollbar, #valueEditor::-webkit-scrollbar { width: 8px; height: 8px; }\n            .key-tree-container::-webkit-scrollbar-track, #valueEditor::-webkit-scrollbar-track { background: transparent; }\n            .key-tree-container::-webkit-scrollbar-thumb, #valueEditor::-webkit-scrollbar-thumb { background-color: var(--color-border); border-radius: 4px; }\n            .key-tree-container::-webkit-scrollbar-thumb:hover, #valueEditor::-webkit-scrollbar-thumb:hover { background-color: var(--color-text-muted); }\n            #valueEditor::-webkit-scrollbar-corner { background: transparent; }\n\n            /* Responsive */\n            @media (max-width: 768px) {\n                body { padding: 0.5rem; }\n                .db-main-content { flex-direction: column; }\n                .key-panel { width: 100%; max-height: 40vh; }\n            }\n        &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;div id=\"dbManagerContainer\" class=\"db-manager-container\"&gt;\n            &lt;header class=\"db-header\"&gt;\n                &lt;h1&gt;DB Manager&lt;/h1&gt;\n                &lt;div class=\"db-mode-selector\"&gt;\n                    &lt;label for=\"modeSelect\"&gt;Mode:&lt;/label&gt;\n                    &lt;select id=\"modeSelect\"&gt;\n                        &lt;option value=\"LC\"&gt;Local Dict&lt;/option&gt;\n                        &lt;option value=\"CB\"&gt;Cloud Blob&lt;/option&gt;\n                        &lt;option value=\"LR\"&gt;Local Redis&lt;/option&gt;\n                        &lt;option value=\"RR\"&gt;Remote Redis&lt;/option&gt;\n                    &lt;/select&gt;\n                &lt;/div&gt;\n            &lt;/header&gt;\n            &lt;main class=\"db-main-content\"&gt;\n                &lt;aside id=\"keyPanel\" class=\"db-panel key-panel\"&gt;\n                    &lt;div class=\"panel-header\"&gt;\n                        &lt;h2&gt;Keys&lt;/h2&gt;\n                        &lt;div class=\"header-actions\"&gt;\n                            &lt;button id=\"addKeyBtn\" title=\"Add New Key\" style=\"font-size: 1.2rem;\"&gt;+&lt;/button&gt;\n                            &lt;button id=\"refreshKeysBtn\" title=\"Refresh Keys\"&gt;\ud83d\udd04&lt;/button&gt;\n                        &lt;/div&gt;\n                    &lt;/div&gt;\n                    &lt;input type=\"text\" id=\"keySearchInput\" placeholder=\"Search keys...\"&gt;\n                    &lt;div id=\"keyTreeContainer\" class=\"key-tree-container\"&gt;&lt;/div&gt;\n                &lt;/aside&gt;\n                &lt;section id=\"editorPanel\" class=\"db-panel editor-panel hidden\"&gt;\n                    &lt;div class=\"panel-header\"&gt;\n                        &lt;h2 id=\"selectedKey\"&gt;&lt;/h2&gt;\n                        &lt;div class=\"header-actions\"&gt;\n                            &lt;button id=\"saveBtn\" class=\"primary\"&gt;Save&lt;/button&gt;\n                            &lt;button id=\"deleteBtn\" class=\"danger\"&gt;Delete&lt;/button&gt;\n                        &lt;/div&gt;\n                    &lt;/div&gt;\n                    &lt;div class=\"editor-toolbar\"&gt;\n                        &lt;button id=\"formatBtn\"&gt;Format JSON&lt;/button&gt;\n                    &lt;/div&gt;\n                    &lt;textarea id=\"valueEditor\" placeholder=\"Select a key to view its value...\"&gt;&lt;/textarea&gt;\n                &lt;/section&gt;\n                &lt;section id=\"placeholderPanel\" class=\"db-panel editor-panel placeholder-panel\"&gt;\n                    &lt;h3&gt;Select a key to get started&lt;/h3&gt;\n                    &lt;p&gt;Or click the '+' button to add a new one.&lt;/p&gt;\n                &lt;/section&gt;\n            &lt;/main&gt;\n        &lt;/div&gt;\n        &lt;script&gt;\n        (() =&gt; {\n            \"use strict\";\n            const API_NAME = \"DB\";\n\n            class DBManager {\n                constructor() {\n                    this.cache = {\n                        keys: [],\n                        selectedKey: null\n                    };\n                    this.dom = {\n                        modeSelect: document.getElementById('modeSelect'),\n                        keySearchInput: document.getElementById('keySearchInput'),\n                        keyTreeContainer: document.getElementById('keyTreeContainer'),\n                        editorPanel: document.getElementById('editorPanel'),\n                        placeholderPanel: document.getElementById('placeholderPanel'),\n                        selectedKey: document.getElementById('selectedKey'),\n                        valueEditor: document.getElementById('valueEditor'),\n                        addKeyBtn: document.getElementById('addKeyBtn'),\n                        refreshKeysBtn: document.getElementById('refreshKeysBtn'),\n                        saveBtn: document.getElementById('saveBtn'),\n                        deleteBtn: document.getElementById('deleteBtn'),\n                        formatBtn: document.getElementById('formatBtn'),\n                    };\n                    this.init();\n                }\n\n                async init() {\n                    this.setStatusMessage('Loading...');\n                    this.addEventListeners();\n                    await this.loadInitialStatus();\n                    await this.loadKeys();\n                }\n\n                addEventListeners() {\n                    this.dom.refreshKeysBtn.addEventListener('click', () =&gt; this.loadKeys());\n                    this.dom.addKeyBtn.addEventListener('click', () =&gt; this.showAddKeyModal());\n                    this.dom.saveBtn.addEventListener('click', () =&gt; this.saveValue());\n                    this.dom.deleteBtn.addEventListener('click', () =&gt; this.confirmDeleteKey());\n                    this.dom.formatBtn.addEventListener('click', () =&gt; this.formatJson());\n                    this.dom.keySearchInput.addEventListener('input', (e) =&gt; this.renderKeyTree(e.target.value));\n                    this.dom.modeSelect.addEventListener('change', (e) =&gt; this.changeMode(e.target.value));\n\n                    this.dom.keyTreeContainer.addEventListener('click', (e) =&gt; {\n                        const label = e.target.closest('.node-label');\n                        if (!label) return;\n                        const node = label.parentElement;\n                        if (node.classList.contains('tree-folder')) {\n                            node.classList.toggle('open');\n                        } else if (node.dataset.key) {\n                            this.selectKey(node.dataset.key);\n                        }\n                    });\n                }\n\n                async apiRequest(endpoint, payload = null, method = 'POST') {\n                    if (!window.TB?.api?.request) {\n                        console.error(\"TB.api not available!\");\n                        return { error: true, message: \"TB.api not available\" };\n                    }\n                    try {\n                        const url = (method === 'GET' &amp;&amp; payload) ? `${endpoint}?${new URLSearchParams(payload)}` : endpoint;\n                        const body = (method !== 'GET') ? payload : null;\n                        const response = await window.TB.api.request(API_NAME, url, body, method);\n\n                        if (response.error &amp;&amp; response.error !== 'none') {\n                            const errorMsg = response.info?.help_text || response.error;\n                            console.error(`API Error on ${endpoint}:`, errorMsg, response);\n                            if (window.TB?.ui?.Toast) TB.ui.Toast.showError(errorMsg, { duration: 5000 });\n                            return { error: true, message: errorMsg, data: response.get() };\n                        }\n                        return { error: false, data: response.get() };\n                    } catch (err) {\n                        console.error(\"Framework/Network Error:\", err);\n                        if (window.TB?.ui?.Toast) TB.ui.Toast.showError(\"Application or network error.\", { duration: 5000 });\n                        return { error: true, message: \"Network error\" };\n                    }\n                }\n\n                async loadInitialStatus() {\n                    const res = await this.apiRequest('api_get_status', null, 'GET');\n                    if (!res.error) this.dom.modeSelect.value = res.data.mode;\n                }\n\n                async loadKeys() {\n                    this.setStatusMessage('Loading keys...');\n                    const res = await this.apiRequest('api_get_all_keys', null, 'GET');\n                    if (!res.error) {\n                        this.cache.keys = res.data || [];\n                        this.renderKeyTree();\n                    } else {\n                        this.setStatusMessage('Failed to load keys.', true);\n                    }\n                }\n\n                renderKeyTree(filter = '') {\n                    const treeData = {};\n                    const filteredKeys = this.cache.keys.filter(k =&gt; k.toLowerCase().includes(filter.toLowerCase().trim()));\n\n                    for (const key of filteredKeys) {\n                        let currentLevel = treeData;\n                        const parts = key.split(':');\n                        for (let i = 0; i &lt; parts.length; i++) {\n                            const part = parts[i];\n                            if (!part) continue; // Skip empty parts from keys like \"a::b\"\n                            const isLeaf = i === parts.length - 1;\n\n                            if (!currentLevel[part]) {\n                                currentLevel[part] = { _children: {} };\n                            }\n                            if (isLeaf) {\n                                currentLevel[part]._fullKey = key;\n                            }\n                            currentLevel = currentLevel[part]._children;\n                        }\n                    }\n\n                    const treeHtml = this.buildTreeHtml(treeData);\n                    if (treeHtml) {\n                        this.dom.keyTreeContainer.innerHTML = `&lt;ul class=\"key-tree\"&gt;${treeHtml}&lt;/ul&gt;`;\n                        // Re-select the key if it's still visible\n                        if (this.cache.selectedKey) {\n                             const nodeEl = this.dom.keyTreeContainer.querySelector(`[data-key=\"${this.cache.selectedKey}\"] .node-label`);\n                             if(nodeEl) nodeEl.classList.add('selected');\n                        }\n                    } else {\n                         this.setStatusMessage(filter ? 'No keys match your search.' : 'No keys found.');\n                    }\n                }\n\n                buildTreeHtml(node) {\n                    return Object.keys(node).sort().map(key =&gt; {\n                        const childNode = node[key];\n                        const isFolder = Object.keys(childNode._children).length &gt; 0;\n\n                        if (isFolder) {\n                            return `&lt;li class=\"tree-folder\" ${childNode._fullKey ? `data-key=\"${childNode._fullKey}\"`: ''}&gt;\n                                        &lt;div class=\"node-label\"&gt;&lt;i class=\"node-icon\"&gt;&lt;/i&gt;${key}&lt;/div&gt;\n                                        &lt;ul class=\"tree-children\"&gt;${this.buildTreeHtml(childNode._children)}&lt;/ul&gt;\n                                    &lt;/li&gt;`;\n                        } else {\n                            return `&lt;li class=\"tree-leaf\" data-key=\"${childNode._fullKey}\"&gt;\n                                        &lt;div class=\"node-label\"&gt;&lt;i class=\"node-icon\"&gt;&lt;/i&gt;${key}&lt;/div&gt;\n                                    &lt;/li&gt;`;\n                        }\n                    }).join('');\n                }\n\n                async selectKey(key) {\n                    if (!key) return;\n                    this.showEditor(true);\n                    this.cache.selectedKey = key;\n\n                    document.querySelectorAll('.node-label.selected').forEach(el =&gt; el.classList.remove('selected'));\n                    const nodeEl = this.dom.keyTreeContainer.querySelector(`[data-key=\"${key}\"] &gt; .node-label`);\n                    if (nodeEl) nodeEl.classList.add('selected');\n\n                    this.dom.selectedKey.textContent = key;\n                    this.dom.selectedKey.title = key;\n                    this.dom.valueEditor.value = \"Loading...\";\n\n                    const res = await this.apiRequest('api_get_value', { key }, 'GET');\n                    this.dom.valueEditor.value = res.error ? `Error: ${res.message}` : res.data.value;\n                    if (!res.error) this.formatJson(false); // Auto-format if it's valid JSON, without showing an error\n                }\n\n                async saveValue() {\n                    if (!this.cache.selectedKey) return;\n                    if (window.TB?.ui?.Loader) TB.ui.Loader.show(\"Saving...\");\n                    const res = await this.apiRequest('api_set_value', {\n                        key: this.cache.selectedKey,\n                        value: this.dom.valueEditor.value\n                    });\n                    if (window.TB?.ui?.Loader) TB.ui.Loader.hide();\n                    if (!res.error &amp;&amp; window.TB?.ui?.Toast) TB.ui.Toast.showSuccess(\"Key saved successfully!\");\n                }\n\n                async confirmDeleteKey() {\n                    if (!this.cache.selectedKey) return;\n                    if (!window.TB?.ui?.Modal) {\n                        if(confirm(`Delete key \"${this.cache.selectedKey}\"?`)) this.deleteKey();\n                        return;\n                    }\n                    TB.ui.Modal.confirm({\n                        title: 'Delete Key?',\n                        content: `Are you sure you want to delete the key \"&lt;strong&gt;${this.cache.selectedKey}&lt;/strong&gt;\"?&lt;br/&gt;This action cannot be undone.`,\n                        confirmButtonText: 'Delete',\n                        confirmButtonVariant: 'danger',\n                        onConfirm: () =&gt; this.deleteKey()\n                    });\n                }\n\n                async deleteKey() {\n                    const keyToDelete = this.cache.selectedKey;\n                    if (!keyToDelete) return;\n                    if (window.TB?.ui?.Loader) TB.ui.Loader.show(\"Deleting...\");\n                    const res = await this.apiRequest('api_delete_key', { key: keyToDelete });\n                    if (window.TB?.ui?.Loader) TB.ui.Loader.hide();\n\n                    if (!res.error) {\n                        if (window.TB?.ui?.Toast) TB.ui.Toast.showSuccess(`Key \"${keyToDelete}\" deleted.`);\n                        this.cache.selectedKey = null;\n                        this.showEditor(false);\n                        this.loadKeys(); // Refresh the key list\n                    }\n                }\n\n                formatJson(showErrorToast = true) {\n                    try {\n                        const currentVal = this.dom.valueEditor.value.trim();\n                        if (!currentVal) return;\n                        const formatted = JSON.stringify(JSON.parse(currentVal), null, 2);\n                        this.dom.valueEditor.value = formatted;\n                    } catch (e) {\n                        if (showErrorToast &amp;&amp; window.TB?.ui?.Toast) {\n                            TB.ui.Toast.showWarning(\"Value is not valid JSON.\", { duration: 3000 });\n                        }\n                    }\n                }\n\n                showAddKeyModal() {\n                     if (!window.TB?.ui?.Modal) { alert(\"Add Key modal not available.\"); return; }\n                     TB.ui.Modal.show({\n                        title: 'Add New Key',\n                        content: `&lt;input type=\"text\" id=\"newKeyInput\" placeholder=\"Enter new key name (e.g., app:settings:user)\" style=\"width: 100%; margin-bottom: 1rem;\"/&gt;\n                                  &lt;textarea id=\"newValueInput\" placeholder='Enter value (e.g., {\"theme\": \"dark\"})' style=\"width: 100%; height: 150px; font-family: var(--font-family-mono);\"&gt;&lt;/textarea&gt;`,\n                        onOpen: (modal) =&gt; document.getElementById('newKeyInput').focus(),\n                        buttons: [{\n                            text: 'Save', variant: 'primary',\n                            action: async (modal) =&gt; {\n                                const newKey = document.getElementById('newKeyInput').value.trim();\n                                const newValue = document.getElementById('newValueInput').value;\n                                if (!newKey) { if (window.TB?.ui?.Toast) TB.ui.Toast.showError(\"Key name cannot be empty.\"); return; }\n                                modal.close();\n                                if (window.TB?.ui.Loader) TB.ui.Loader.show(\"Saving...\");\n                                const res = await this.apiRequest('api_set_value', { key: newKey, value: newValue });\n                                if (window.TB?.ui.Loader) TB.ui.Loader.hide();\n                                if (!res.error) {\n                                    if (window.TB?.ui?.Toast) TB.ui.Toast.showSuccess(\"New key created!\");\n                                    await this.loadKeys();\n                                    this.selectKey(newKey);\n                                }\n                            }\n                        }, { text: 'Cancel', action: (modal) =&gt; modal.close() }]\n                    });\n                }\n\n                async changeMode(newMode) {\n                    if (window.TB?.ui?.Loader) TB.ui.Loader.show(`Switching to ${newMode}...`);\n                    const res = await this.apiRequest('api_change_mode', { mode: newMode });\n                    if (!res.error) {\n                       this.cache.selectedKey = null;\n                       this.showEditor(false);\n                       await this.loadKeys();\n                       if (window.TB?.ui?.Toast) TB.ui.Toast.showSuccess(`Switched to ${newMode} mode.`);\n                    } else {\n                       if (window.TB?.ui?.Toast) TB.ui.Toast.showError(`Failed to switch mode.`);\n                       await this.loadInitialStatus(); // Revert dropdown to actual status\n                    }\n                    if (window.TB?.ui?.Loader) TB.ui.Loader.hide();\n                }\n\n                showEditor(show) {\n                    this.dom.editorPanel.classList.toggle('hidden', !show);\n                    this.dom.placeholderPanel.classList.toggle('hidden', show);\n                }\n\n                setStatusMessage(message, isError = false) {\n                    this.dom.keyTreeContainer.innerHTML = `&lt;p class=\"status-message\" style=\"${isError ? 'color: var(--color-danger);' : ''}\"&gt;${message}&lt;/p&gt;`;\n                }\n            }\n\n            // Defer initialization until the ToolboxV2 framework is ready\n\n             function onTbReady() { new DBManager(); }\n             if (window.TB?.events) {\n    if (window.TB.config?.get('appRootId')) { // A sign that TB.init might have run\n         onTbReady();\n    } else {\n        window.TB.events.on('tbjs:initialized', onTbReady, { once: true });\n    }\n} else {\n    // Fallback if TB is not even an object yet, very early load\n    document.addEventListener('tbjs:initialized', onTbReady, { once: true }); // Custom event dispatch from TB.init\n}\n\n        })();\n        &lt;/script&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \"\"\"\n    app = get_app(Name)\n    try:\n        # Prepend the web context to include necessary framework scripts (like TB.js)\n        web_context = app.web_context()\n        return Result.html(web_context + html_content)\n    except Exception as e:\n        # Fallback in case web_context is not available\n        return Result.html(html_content)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.EventManager","title":"<code>EventManager</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.EventManager.module","title":"<code>module</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.EventManager.module.EventManagerClass","title":"<code>EventManagerClass</code>","text":"Source code in <code>toolboxv2/mods/EventManager/module.py</code> <pre><code>class EventManagerClass:\n    events: set[Event] = set()\n    source_id: str\n    _name: str\n    _identification: str\n\n    routes_client: dict[str, ProxyRout] = {}\n    routers_servers: dict[str, DaemonRout] = {}\n    routers_servers_tasks: list[Any] = []\n    routers_servers_tasks_running_flag: bool = False\n\n    receiver_que: queue.Queue\n    response_que: queue.Queue\n\n    def add_c_route(self, name, route: ProxyRout):\n        self.routes_client[name] = route\n\n    async def receive_all_client_data(self):\n\n        close_connections = []\n        add_ev = []\n        for name, client in self.routes_client.items():\n            if client.client is None or not client.client.get('alive', False):\n                close_connections.append(name)\n                continue\n            data = client.r\n\n            if isinstance(data, str) and data == \"No data\":\n                continue\n            elif isinstance(data, EventID) and len(data.get_source()) != 0:\n                await self.trigger_event(data)\n            elif isinstance(data, EventID) and len(data.get_source()) == 0:\n                print(f\"Event returned {data.payload}\")\n                self.response_que.put(data)\n            elif isinstance(data,\n                            dict) and 'error' in data and 'origin' in data and 'result' in data and 'info' in data:\n\n                self.response_que.put(Result.result_from_dict(**data).print())\n            elif isinstance(data,\n                            dict) and 'source' in data and 'path' in data and 'ID' in data and 'identifier' in data:\n                del data['identifier']\n                ev_id = EventID(**data)\n                await self.trigger_event(ev_id)\n            elif isinstance(data, Event):\n                print(\"Event:\", str(data.event_id), data.name)\n                add_ev.append(data)\n            elif isinstance(data, Result):\n                self.response_que.put(data.print())\n            else:\n                print(f\"Unknown Data {data}\")\n\n        for ev in add_ev:\n            await self.register_event(ev)\n\n        for client_name in close_connections:\n            print(f\"Client {client_name} closing connection\")\n            self.remove_c_route(client_name)\n\n    def remove_c_route(self, name):\n        self.routes_client[name].close()\n        del self.routes_client[name]\n\n    def crate_rout(self, source, addr=None):\n        if addr is None:\n            addr = ('0.0.0.0', 6588)\n        host, port = addr\n        if isinstance(port, str):\n            port = int(port)\n        return Rout(\n            _from=self.source_id,\n            _to=source,\n            _from_port=int(os.getenv(\"TOOLBOXV2_BASE_PORT\", 6588)),\n            _from_host=os.getenv(\"TOOLBOXV2_BASE_HOST\"),\n            _to_port=port,\n            _to_host=host,\n            routing_function=self.routing_function_router,\n        )\n\n    def __init__(self, source_id, _identification=\"PN\"):\n        self.bo = False\n        self.running = False\n        self.source_id = source_id\n        self.receiver_que = queue.Queue()\n        self.response_que = queue.Queue()\n        self._identification = _identification\n        self._name = self._identification + '-' + str(uuid.uuid4()).split('-')[1]\n        self.routes = {}\n        self.logger = get_logger()\n\n    @property\n    def identification(self) -&gt; str:\n        return self._identification\n\n    @identification.setter\n    def identification(self, _identification: str):\n        self.stop()\n        self._identification = _identification\n        self._name = self._identification + '-' + str(uuid.uuid4()).split('-')[1]\n\n    async def identity_post_setter(self):\n\n        do_reconnect = len(list(self.routers_servers.keys())) &gt; 0\n        if self._identification == \"P0\":\n            await self.add_server_route(self._identification, ('0.0.0.0', 6568))\n        if self._identification == \"P0|S0\":\n            await self.add_server_route(self._identification, ('0.0.0.0', 6567))\n\n        await asyncio.sleep(0.1)\n        self.start()\n        await asyncio.sleep(0.1)\n        if do_reconnect:\n            self.reconnect(\"ALL\")\n\n    async def open_connection_server(self, port):\n        await self.add_server_route(self._identification, ('0.0.0.0', port))\n\n    def start(self):\n        self.running = True\n        threading.Thread(target=async_test(self.receiver), daemon=True).start()\n\n    def make_event_from_fuction(self, fuction, name, *args, source_types=SourceTypes.F,\n                                scope=Scope.local,\n                                exec_in=ExecIn.local,\n                                threaded=False, **kwargs):\n\n        return Event(source=fuction,\n                     name=name,\n                     event_id=EventID.crate_with_source(self.source_id), args=args,\n                     kwargs_=kwargs,\n                     source_types=source_types,\n                     scope=scope,\n                     exec_in=exec_in,\n                     threaded=threaded,\n                     )\n\n    async def add_client_route(self, source_id, addr):\n        if source_id in self.routes_client:\n            if self.routes_client[source_id].client is None or not self.routes_client[source_id].client.get('alive'):\n                await self.routes_client[source_id].reconnect()\n                return True\n            print(\"Already connected\")\n            return False\n        try:\n            pr = await ProxyRout.toProxy(rout=self.crate_rout(source_id, addr=addr), name=source_id)\n            await asyncio.sleep(0.1)\n            await pr.client.get('sender')({\"id\": self._identification,\n                                           \"continue\": False,\n                                           \"key\": os.getenv('TB_R_KEY', 'root@remote')})\n            await asyncio.sleep(0.1)\n            self.add_c_route(source_id, pr)\n            return True\n        except Exception as e:\n            print(f\"Check the port {addr} Sever likely not Online : {e}\")\n            return False\n\n    async def add_mini_client(self, name: str, addr: tuple[str, int]):\n\n        mini_proxy = await ProxyRout(class_instance=None, timeout=15, app=get_app(),\n                                     remote_functions=[\"\"], peer=False, name=name, do_connect=False)\n\n        async def _(x):\n            return await self.routers_servers[self._identification].send(x, addr)\n\n        mini_proxy.put_data = _\n        mini_proxy.connect = lambda *x, **_: None\n        mini_proxy.reconnect = lambda *x, **_: None\n        mini_proxy.close = lambda *x, **_: None\n        mini_proxy.client = {'alive': True}\n        mini_proxy.r = \"No data\"\n        self.routes_client[name] = mini_proxy\n\n    async def on_register(self, id_, data):\n        try:\n            if \"unknown\" not in self.routes:\n                self.routes[\"unknown\"] = {}\n\n            if id_ != \"new_con\" and 'id' in data:\n                id_data = data.get('id')\n                id_ = eval(id_)\n                c_host, c_pot = id_\n                print(f\"Registering: new client {id_data} : {c_host, c_pot}\")\n                if id_data not in self.routes_client:\n                    await self.add_mini_client(id_data, (c_host, c_pot))\n                    self.routes[str((c_host, c_pot))] = id_data\n\n            # print(\"self.routes:\", self.routes)\n        except Exception as e:\n            print(\"Error in on_register\", str(e))\n\n    def on_client_exit(self, id_):\n\n        if isinstance(id_, str):\n            id_ = eval(id_)\n\n        c_name = self.routes.get(id_)\n\n        if c_name is None:\n            return\n\n        if c_name in self.routes_client:\n            self.remove_c_route(c_name)\n            print(f\"Removed route to {c_name}\")\n\n    async def add_server_route(self, source_id, addr=None):\n        if addr is None:\n            addr = ('0.0.0.0', 6588)\n        try:\n            self.routers_servers[source_id] = await DaemonRout(rout=self.crate_rout(source_id, addr=addr),\n                                                               name=source_id,\n                                                               on_r=self.on_register)\n            self.routers_servers_tasks.append(self.routers_servers[source_id].online)\n        except Exception as e:\n            print(f\"Sever already Online : {e}\")\n\n        if not self.routers_servers_tasks_running_flag:\n            self.routers_servers_tasks_running_flag = True\n            threading.Thread(target=self.server_route_runner, daemon=True).start()\n\n    def server_route_runner(self):\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n\n        # Sammle alle Ergebnisse zusammen\n        results = loop.run_until_complete(asyncio.gather(*self.routers_servers_tasks))\n\n        for result in results:\n            print(result)\n\n        loop.close()\n        self.routers_servers_tasks_running_flag = False\n\n    async def add_js_route(self, source_id=\"js:web\"):\n        await self.add_server_route(source_id, (\"./web/scripts/tb_socket.sock\", 0))\n\n    async def register_event(self, event: Event):\n\n        if event in self.events:\n            return Result.default_user_error(\"Event registration failed Event already registered\")\n\n        print(f\"Registration new Event : {event.name}, {str(event.event_id)}\")\n        self.events.add(event)\n\n        if event.scope.name == Scope.instance.name:\n            return\n\n        if event.scope.name == Scope.local.name:\n            if not self.bo and \"P0\" not in self.routes_client and os.getenv(\"TOOLBOXV2_BASE_HOST\",\n                                                                            \"localhost\") != \"localhost\":\n                await self.add_client_route(\"P0\", (os.getenv(\"TOOLBOXV2_BASE_HOST\", \"localhost\"),\n                                                   os.getenv(\"TOOLBOXV2_BASE_PORT\", 6568)))\n                self.bo = True\n            return\n\n        if event.scope.name == Scope.local_network.name:\n            if self.identification == \"P0\" and not self.bo:\n                t0 = threading.Thread(target=self.start_brodcast_router_local_network, daemon=True)\n                t0.start()\n            elif not self.bo and \"P0\" not in self.routes_client and os.getenv(\"TOOLBOXV2_BASE_HOST\",\n                                                                              \"localhost\") == \"localhost\":\n                self.bo = True\n                # self.add_server_route(self.identification, (\"127.0.0.1\", 44667))\n                with Spinner(message=\"Sercheing for Rooter instance\", count_down=True, time_in_s=6):\n                    with ThreadPoolExecutor(max_workers=1) as executor:\n                        t0 = executor.submit(make_known, self.identification)\n                        try:\n                            data = t0.result(timeout=6)\n                        except TimeoutError:\n                            print(\"No P0 found in network or on device\")\n                            return\n                    print(f\"Found P0 on {type(data)} {data.get('host')}\")\n                    await self.add_client_route(\"P0\", (data.get(\"host\"), os.getenv(\"TOOLBOXV2_BASE_PORT\", 6568)))\n            elif not self.bo and \"P0\" not in self.routes_client and os.getenv(\"TOOLBOXV2_BASE_HOST\",\n                                                                              \"localhost\") != \"localhost\":\n                do = await self.add_client_route(\"P0\", (\n                    os.getenv(\"TOOLBOXV2_BASE_HOST\", \"localhost\"), os.getenv(\"TOOLBOXV2_BASE_PORT\", 6568)))\n                self.bo = do\n                if not do:\n                    print(\"Connection failed\")\n                    os.environ[\"TOOLBOXV2_BASE_HOST\"] = \"localhost\"\n\n        if event.scope.name == Scope.global_network.name:\n            await self.add_server_route(self.source_id, ('0.0.0.0', os.getenv(\"TOOLBOXV2_REMOTE_PORT\", 6587)))\n\n    async def connect_to_remote(self, host=os.getenv(\"TOOLBOXV2_REMOTE_IP\"),\n                                port=os.getenv(\"TOOLBOXV2_REMOTE_PORT\", 6587)):\n        await self.add_client_route(\"S0\", (host, port))\n\n    def start_brodcast_router_local_network(self):\n        self.bo = True\n\n        # print(\"Starting brodcast router 0\")\n        router = start_client(get_local_ip())\n        # print(\"Starting brodcast router 1\")\n        # next(router)\n        # print(\"Starting brodcast router\")\n        while self.running:\n            source_id, connection = next(router)\n            print(f\"Infos :{source_id}, connection :{connection}\")\n            self.routes[source_id] = connection[0]\n            router.send(self.running)\n\n        router.send(\"e\")\n        router.close()\n\n    def _get_event_by_id_or_name(self, event_id: str or EventID):\n        if isinstance(event_id, str):\n            events = [e for e in self.events if e.name == event_id]\n            if len(events) &lt; 1:\n                return Result.default_user_error(\"Event not registered\")\n            event = events[0]\n\n        elif isinstance(event_id, EventID):\n            events = [e for e in self.events if e.event_id.ID == event_id.ID]\n            if len(events) &lt; 1:\n                events = [e for e in self.events if e.name == event_id.ID]\n            if len(events) &lt; 1:\n                return Result.default_user_error(\"Event not registered\")\n            event = events[0]\n\n        elif isinstance(event_id, Event):\n            if event_id not in self.events:\n                return Result.default_user_error(\"Event not registered\")\n            event = event_id\n\n        else:\n            event = Result.default_user_error(\"Event not registered\")\n\n        return event\n\n    def remove_event(self, event: Event or EventID or str):\n\n        event = self._get_event_by_id_or_name(event)\n        if isinstance(event, Event):\n            self.events.remove(event)\n        else:\n            return event\n\n    async def _trigger_local(self, event_id: EventID):\n        \"\"\"\n        Exec source based on\n\n        source_types\n            F -&gt; call directly\n            R -&gt; use get_app(str(event_id)).run_any(*args, **kwargs)\n            S -&gt; evaluate string\n        scope\n            instance -&gt; _trigger_local\n            local -&gt; if you ar proxy app run the event through get_app(str(event_id)).run_any(TBEF.EventManager._trigger_local, args=args, kwargs=kwargs, get_result=True)\n            local_network -&gt; use proxy0 app to communicate withe Daemon0 then local\n            global_network -&gt;\n        exec_in\n        event_id\n        threaded\n\n                       \"\"\"\n        event = self._get_event_by_id_or_name(event_id)\n\n        if isinstance(event, Result):\n            event.print()\n            if self.identification == \"P0\":\n                return event\n            print(f\"Routing to P0 {self.events}\")\n            if self.source_id not in self.routes_client:\n                # self.routers[self.source_id] = DaemonRout(rout=self.crate_rout(self.source_id))\n                await self.add_client_route(\"P0\", ('127.0.0.1', 6568))\n            return await self.route_event_id(event_id)\n\n        # if event.threaded:\n        #    threading.Thread(target=self.runner, args=(event, event_id), daemon=True).start()\n        #    return \"Event running In Thread\"\n        # else:\n\n        return await self.runner(event, event_id)\n\n    async def runner(self, event, event_id: EventID):\n\n        if event.kwargs_ is None:\n            event.kwargs_ = {}\n        if event.args is None:\n            event.args = []\n\n        if event.source_types.name is SourceTypes.P.name:\n            return event.source(*event.args, payload=event_id, **event.kwargs_)\n\n        if event.source_types.name is SourceTypes.F.name:\n            return event.source(*event.args, **event.kwargs_)\n\n        if event.source_types.name is SourceTypes.R.name:\n            return get_app(str(event_id)).run_any(mod_function_name=event.source, get_results=True, args_=event.args,\n                                                  kwargs_=event.kwargs_)\n\n        if event.source_types.name is SourceTypes.AP.name:\n            if 'payload' in event.kwargs_:\n                if event_id.payload != event.kwargs_['payload']:\n                    event_id.payload = event.kwargs_['payload']\n                del event.kwargs_['payload']\n            print(event.args, event.kwargs_, \"TODO: remove\")\n            return await event.source(*event.args, payload=event_id, **event.kwargs_)\n\n        if event.source_types.name is SourceTypes.AF.name:\n            return await event.source(*event.args, **event.kwargs_)\n\n        if event.source_types.name is SourceTypes.AR.name:\n            return await get_app(str(event_id)).run_any(mod_function_name=event.source, get_results=True,\n                                                        args_=event.args,\n                                                        kwargs_=event.kwargs_)\n\n        if event.source_types.name is SourceTypes.S.name:\n            return eval(event.source, __locals={'app': get_app(str(event_id)), 'event': event, 'eventManagerC': self})\n\n    async def routing_function_router(self, event_id: EventID):\n\n        result = await self.trigger_event(event_id)\n\n        if result is None:\n            result = Result.default_user_error(\"Invalid Event ID\")\n\n        if isinstance(result, bytes | dict):\n            pass\n        elif isinstance(result, Result):\n            result.result.data_info = str(event_id)\n        elif isinstance(result, EventID):\n            result = Result.default_internal_error(\"Event not found\", data=result)\n        else:\n            result = Result.ok(data=result, data_info=\"&lt;automatic&gt;\", info=str(event_id.path))\n\n        if isinstance(result, str):\n            result = result.encode()\n\n        return result\n\n    async def trigger_evnet_by_name(self, name: str):\n        await self.trigger_event(EventID.crate_name_as_id(name=name))\n\n    async def trigger_event(self, event_id: EventID):\n        \"\"\"\n        Exec source based on\n\n        source_types\n            F -&gt; call directly\n            R -&gt; use get_app(str(event_id)).run_any(*args, **kwargs)\n            S -&gt; evaluate string\n        scope\n            instance -&gt; _trigger_local\n            local -&gt; if you ar proxy app run the event through get_app(str(event_id)).run_any(TBEF.EventManager._trigger_local, args=args, kwargs=kwargs, get_result=True)\n            local_network -&gt; use proxy0 app to communicate withe Daemon0 then local\n            global_network -&gt;\n        exec_in\n        event_id\n        threaded\n\n                       \"\"\"\n        # print(f\"event-id Ptah : {event_id.get_path()}\")\n        # print(f\"testing trigger_event for {event_id.get_source()} {event_id.get_source()[-1] == self.source_id} \")\n        print(str(event_id))\n        if event_id.get_source()[-1] == self.source_id:\n            payload = await self._trigger_local(event_id)\n            event_id.set_payload(payload)\n            if len(event_id.path) &gt; 1:\n                event_id.source = ':'.join([e.split(':')[0] for e in event_id.get_path() if e != \"E\"])\n                res = await self.route_event_id(event_id)\n                if isinstance(res, Result):\n                    res.print()\n                else:\n                    print(res)\n            return payload\n        return await self.route_event_id(event_id)\n\n    async def route_event_id(self, event_id: EventID):\n\n        # print(f\"testing route_event_id for {event_id.get_source()[-1]}\")\n        if event_id.get_source()[-1] == '*':  # self.identification == \"P0\" and\n            responses = []\n            event_id.source = ':'.join(event_id.get_source()[:-1])\n            event_id.add_path(f\"{self._name}({self.source_id})\")\n            data = asdict(event_id)\n            for name, rout_ in self.routes_client.items():\n                if name in event_id.path:\n                    continue\n                ret = await rout_.put_data(data)\n                responses.append(ret)\n            return responses\n        route = self.routes_client.get(event_id.get_source()[-1])\n        # print(\"route:\", route)\n        if route is None:\n            route = self.routes_client.get(event_id.get_path()[-1])\n        if route is None:\n            return event_id.add_path((\"\" if len(event_id.get_source()) == 1 else \"404#\")+self.identification)\n        time.sleep(0.25)\n        event_id.source = ':'.join(event_id.get_source()[:-1])\n        event_id.add_path(f\"{self._name}({self.source_id})\")\n        return await route.put_data(asdict(event_id))\n\n    async def receiver(self):\n\n        t0 = time.time()\n\n        while self.running:\n            time.sleep(0.25)\n            if not self.receiver_que.empty():\n                event_id = self.receiver_que.get()\n                print(\"Receiver Event\", str(event_id))\n                await self.trigger_event(event_id)\n\n            if time.time() - t0 &gt; 5:\n                await self.receive_all_client_data()\n                t0 = time.time()\n\n    def info(self):\n        return {\"source\": self.source_id, \"known_routs:\": self.routers_servers, \"_router\": self.routes_client,\n                \"events\": self.events}\n\n    def stop(self):\n        self.running = False\n        list(map(lambda x: x.disconnect(), self.routes_client.values()))\n        list(map(lambda x: x.stop(), self.routers_servers.values()))\n\n    def reconnect(self, name):\n        if name is None:\n            pass\n        elif name in self.routes_client:\n            self.routes_client[name].reconnect()\n            return\n        list(map(lambda x: x.reconnect(), self.routes_client.values()))\n\n    async def verify(self, name):\n        if name is None:\n            pass\n        elif name in self.routes_client:\n            await self.routes_client[name].verify()\n            return\n        for x in self.routes_client.values():\n            await x.verify()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.EventManager.module.EventManagerClass.trigger_event","title":"<code>trigger_event(event_id)</code>  <code>async</code>","text":"<p>Exec source based on</p> <p>source_types     F -&gt; call directly     R -&gt; use get_app(str(event_id)).run_any(args, *kwargs)     S -&gt; evaluate string scope     instance -&gt; _trigger_local     local -&gt; if you ar proxy app run the event through get_app(str(event_id)).run_any(TBEF.EventManager._trigger_local, args=args, kwargs=kwargs, get_result=True)     local_network -&gt; use proxy0 app to communicate withe Daemon0 then local     global_network -&gt; exec_in event_id threaded</p> Source code in <code>toolboxv2/mods/EventManager/module.py</code> <pre><code>async def trigger_event(self, event_id: EventID):\n    \"\"\"\n    Exec source based on\n\n    source_types\n        F -&gt; call directly\n        R -&gt; use get_app(str(event_id)).run_any(*args, **kwargs)\n        S -&gt; evaluate string\n    scope\n        instance -&gt; _trigger_local\n        local -&gt; if you ar proxy app run the event through get_app(str(event_id)).run_any(TBEF.EventManager._trigger_local, args=args, kwargs=kwargs, get_result=True)\n        local_network -&gt; use proxy0 app to communicate withe Daemon0 then local\n        global_network -&gt;\n    exec_in\n    event_id\n    threaded\n\n                   \"\"\"\n    # print(f\"event-id Ptah : {event_id.get_path()}\")\n    # print(f\"testing trigger_event for {event_id.get_source()} {event_id.get_source()[-1] == self.source_id} \")\n    print(str(event_id))\n    if event_id.get_source()[-1] == self.source_id:\n        payload = await self._trigger_local(event_id)\n        event_id.set_payload(payload)\n        if len(event_id.path) &gt; 1:\n            event_id.source = ':'.join([e.split(':')[0] for e in event_id.get_path() if e != \"E\"])\n            res = await self.route_event_id(event_id)\n            if isinstance(res, Result):\n                res.print()\n            else:\n                print(res)\n        return payload\n    return await self.route_event_id(event_id)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.EventManager.module.Rout","title":"<code>Rout</code>  <code>dataclass</code>","text":"Source code in <code>toolboxv2/mods/EventManager/module.py</code> <pre><code>@dataclass\nclass Rout:\n    _from: str\n    _to: str\n\n    _from_port: int\n    _from_host: str\n\n    _to_port: int\n    _to_host: str\n\n    routing_function: Callable\n\n    @property\n    def to_host(self):\n        return self._to_host\n\n    @property\n    def to_port(self):\n        return self._to_port\n\n    async def put_data(self, event_id_data: dict[str, str]):\n        event_id: EventID = EventID(**event_id_data)\n        return await self.routing_function(event_id)\n\n    def close(self):\n        \"\"\" Close \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.EventManager.module.Rout.close","title":"<code>close()</code>","text":"<p>Close</p> Source code in <code>toolboxv2/mods/EventManager/module.py</code> <pre><code>def close(self):\n    \"\"\" Close \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi","title":"<code>FastApi</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_api_install","title":"<code>fast_api_install</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_api_install.FileBrowser","title":"<code>FileBrowser</code>","text":"Source code in <code>toolboxv2/mods/FastApi/fast_api_install.py</code> <pre><code>class FileBrowser:\n    ALLOWED_DIRECTORIES: set[str] = {\"mods_sto\", \"flows\", \"static\", \"apps\"}\n\n    def __init__(self, start_dir: str):\n        self.static_dir = pathlib.Path(start_dir).resolve()\n        self.current_container = None\n\n    def is_path_allowed(self, file_path: pathlib.Path) -&gt; bool:\n        \"\"\"Check if the path is within allowed directories.\"\"\"\n        if not file_path.is_relative_to(self.static_dir):\n            return False\n\n        relative_parts = file_path.parts[len(self.static_dir.parts):]\n        return any(part in self.ALLOWED_DIRECTORIES for part in relative_parts)\n\n    async def download_file(self, file_path: pathlib.Path) -&gt; None:\n        \"\"\"Handle file download.\"\"\"\n        if not file_path.is_file() or not self.is_path_allowed(file_path):\n            ui.notify('Access denied or file not found', type='negative')\n            return\n\n        # Use NiceGUI's download function\n        await ui.download(str(file_path))\n\n    def refresh_view(self, path: pathlib.Path) -&gt; None:\n        \"\"\"Refresh the file browser view.\"\"\"\n        if self.current_container:\n            self.current_container.clear()\n\n        with self.current_container:\n            # Add header with current path\n            ui.label(f'Current directory: {path.relative_to(self.static_dir)}').classes('text-h6')\n\n            # Add parent directory link if not at root\n            if path != self.static_dir and path.parent.is_relative_to(self.static_dir):\n                with ui.row().classes('w-full items-center'):\n                    ui.button('..', on_click=lambda p=path.parent: self.refresh_view(p)) \\\n                        .classes('bg-blue-100 px-4 py-2 rounded')\n\n            # List directories first\n            for item in sorted(path.iterdir()):\n                if not self.is_path_allowed(item):\n                    continue\n\n                with ui.row().classes('w-full items-center gap-2'):\n                    if item.is_dir():\n                        ui.button(f'\ud83d\udcc1 {item.name}/',\n                                  on_click=lambda p=item: self.refresh_view(p)) \\\n                            .classes('bg-blue-100 px-4 py-2 rounded')\n                    else:\n                        ui.label(f'\ud83d\udcc4 {item.name}').classes('flex-grow')\n                        ui.button('Download',\n                                  on_click=lambda p=item: self.download_file(p)) \\\n                            .classes('bg-green-100 px-4 py-2 rounded')\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_api_install.FileBrowser.download_file","title":"<code>download_file(file_path)</code>  <code>async</code>","text":"<p>Handle file download.</p> Source code in <code>toolboxv2/mods/FastApi/fast_api_install.py</code> <pre><code>async def download_file(self, file_path: pathlib.Path) -&gt; None:\n    \"\"\"Handle file download.\"\"\"\n    if not file_path.is_file() or not self.is_path_allowed(file_path):\n        ui.notify('Access denied or file not found', type='negative')\n        return\n\n    # Use NiceGUI's download function\n    await ui.download(str(file_path))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_api_install.FileBrowser.is_path_allowed","title":"<code>is_path_allowed(file_path)</code>","text":"<p>Check if the path is within allowed directories.</p> Source code in <code>toolboxv2/mods/FastApi/fast_api_install.py</code> <pre><code>def is_path_allowed(self, file_path: pathlib.Path) -&gt; bool:\n    \"\"\"Check if the path is within allowed directories.\"\"\"\n    if not file_path.is_relative_to(self.static_dir):\n        return False\n\n    relative_parts = file_path.parts[len(self.static_dir.parts):]\n    return any(part in self.ALLOWED_DIRECTORIES for part in relative_parts)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_api_install.FileBrowser.refresh_view","title":"<code>refresh_view(path)</code>","text":"<p>Refresh the file browser view.</p> Source code in <code>toolboxv2/mods/FastApi/fast_api_install.py</code> <pre><code>def refresh_view(self, path: pathlib.Path) -&gt; None:\n    \"\"\"Refresh the file browser view.\"\"\"\n    if self.current_container:\n        self.current_container.clear()\n\n    with self.current_container:\n        # Add header with current path\n        ui.label(f'Current directory: {path.relative_to(self.static_dir)}').classes('text-h6')\n\n        # Add parent directory link if not at root\n        if path != self.static_dir and path.parent.is_relative_to(self.static_dir):\n            with ui.row().classes('w-full items-center'):\n                ui.button('..', on_click=lambda p=path.parent: self.refresh_view(p)) \\\n                    .classes('bg-blue-100 px-4 py-2 rounded')\n\n        # List directories first\n        for item in sorted(path.iterdir()):\n            if not self.is_path_allowed(item):\n                continue\n\n            with ui.row().classes('w-full items-center gap-2'):\n                if item.is_dir():\n                    ui.button(f'\ud83d\udcc1 {item.name}/',\n                              on_click=lambda p=item: self.refresh_view(p)) \\\n                        .classes('bg-blue-100 px-4 py-2 rounded')\n                else:\n                    ui.label(f'\ud83d\udcc4 {item.name}').classes('flex-grow')\n                    ui.button('Download',\n                              on_click=lambda p=item: self.download_file(p)) \\\n                        .classes('bg-green-100 px-4 py-2 rounded')\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit","title":"<code>fast_lit</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit.APIRequestHelper","title":"<code>APIRequestHelper</code>","text":"Source code in <code>toolboxv2/mods/FastApi/fast_lit.py</code> <pre><code>class APIRequestHelper:\n    def __init__(self, token_secret: str):\n        self.token_secret = token_secret\n\n    async def make_api_request(self, endpoint: str, method: str, data: dict | None = None,\n                               headers: dict | None = None, session_token: str | None = None) -&gt; Any:\n        \"\"\"\n        Make API requests while maintaining session context\n        \"\"\"\n        import httpx\n\n        if headers is None:\n            headers = {}\n\n        if session_token:\n            try:\n                session_data = jwt.decode(session_token, self.token_secret, algorithms=[\"HS256\"])\n                headers['X-Session-ID'] = session_data.get('session_id')\n                headers['Authorization'] = f'Bearer {session_token}'\n            except jwt.InvalidTokenError:\n                raise ValueError(\"Invalid session token\")\n\n        async with httpx.AsyncClient() as client:\n            response = await client.request(\n                method=method,\n                url=endpoint,\n                json=data,\n                headers=headers\n            )\n\n            return response.json()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit.APIRequestHelper.make_api_request","title":"<code>make_api_request(endpoint, method, data=None, headers=None, session_token=None)</code>  <code>async</code>","text":"<p>Make API requests while maintaining session context</p> Source code in <code>toolboxv2/mods/FastApi/fast_lit.py</code> <pre><code>async def make_api_request(self, endpoint: str, method: str, data: dict | None = None,\n                           headers: dict | None = None, session_token: str | None = None) -&gt; Any:\n    \"\"\"\n    Make API requests while maintaining session context\n    \"\"\"\n    import httpx\n\n    if headers is None:\n        headers = {}\n\n    if session_token:\n        try:\n            session_data = jwt.decode(session_token, self.token_secret, algorithms=[\"HS256\"])\n            headers['X-Session-ID'] = session_data.get('session_id')\n            headers['Authorization'] = f'Bearer {session_token}'\n        except jwt.InvalidTokenError:\n            raise ValueError(\"Invalid session token\")\n\n    async with httpx.AsyncClient() as client:\n        response = await client.request(\n            method=method,\n            url=endpoint,\n            json=data,\n            headers=headers\n        )\n\n        return response.json()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit.BidirectionalStreamlitAppManager","title":"<code>BidirectionalStreamlitAppManager</code>","text":"<p>               Bases: <code>BaseHTTPMiddleware</code></p> Source code in <code>toolboxv2/mods/FastApi/fast_lit.py</code> <pre><code>class BidirectionalStreamlitAppManager(BaseHTTPMiddleware, metaclass=Singleton):\n    def __init__(self, app: FastAPI, streamlit_apps_dir: str = \"./apps\"):\n        super().__init__(app)\n        self.streamlit_manager = StreamlitAppManager()\n        self.streamlit_apps_dir = streamlit_apps_dir\n        self.token_secret = os.getenv(\"TOKEN_SECRET\", \"your-secret-key\")\n        self.api_helper = APIRequestHelper(self.token_secret)\n\n        # Run cleanup task\n        asyncio.create_task(self.periodic_cleanup())\n\n    #def add_ws(self, fast_app):\n        # Register WebSocket routes\n     #   fast_app.add_api_websocket_route(\"/ws/{session_id}/{app_id}\", self.websocket_endpoint, \"StWebSocket\")\n\n    async def periodic_cleanup(self):\n        while True:\n            self.streamlit_manager.cleanup_inactive_apps()\n            await asyncio.sleep(3600)\n\n    def create_streamlit_token(self, session_data: dict, app_name: str) -&gt; str:\n        payload = {\n            \"app_name\": app_name,\n            \"session_id\": session_data.get(\"ID\"),\n            \"user_data\": session_data.get(\"live_data\"),\n            \"exp\": datetime.utcnow() + timedelta(hours=1)\n        }\n        return jwt.encode(payload, self.token_secret, algorithm=\"HS256\")\n\n    #async def websocket_endpoint(self, websocket: WebSocket, session_id: str, app_id: str):\n    #    await self.streamlit_manager.ws_manager.connect(websocket, session_id, app_id)\n    #    try:\n    #        while True:\n    #            message = await websocket.receive_json()\n    #            await self.streamlit_manager.ws_manager.handle_message(session_id, message)\n    #    except WebSocketDisconnect:\n    #        await self.streamlit_manager.ws_manager.disconnect(session_id, app_id)\n\n    async def resolve_session_token(self, request: Request) -&gt; str | None:\n        \"\"\"\n        Extract and validate session token from request\n        \"\"\"\n        token = request.headers.get('Authorization', '').replace('Bearer ', '')\n        if not token:\n            token = request.query_params.get('token')\n\n        if token:\n            try:\n                jwt.decode(token, self.token_secret, algorithms=[\"HS256\"])\n                return token\n            except jwt.InvalidTokenError:\n                return None\n        return None\n\n    async def dispatch(self, request: Request, call_next) -&gt; Response:\n        # Handle API routes with session token resolution\n        if request.url.path.startswith(\"/api/\"):\n            session_token = await self.resolve_session_token(request)\n            if session_token:\n                # Inject session data into request state\n                request.state.session_token = session_token\n                request.state.api_helper = self.api_helper\n\n        # Handle Streamlit routes\n        elif request.url.path.startswith(\"/apps/\"):\n            app_name = request.url.path.split(\"/\")[-1]\n            app_path = os.path.join(self.streamlit_apps_dir, f\"{app_name}.py\")\n\n            # Verify session is valid\n            if 'public' not in app_name and not request.session.get(\"valid\", False):\n                return JSONResponse(\n                    status_code=401,\n                    content={\"message\": \"Invalid session\"}\n                )\n\n            if not os.path.exists(app_path):\n                return JSONResponse(\n                    status_code=401,\n                    content={\"message\": \"no app found\"}\n                )\n\n            streamlit_token = self.create_streamlit_token(request.session, app_name)\n            port = await self.streamlit_manager.start_app(app_path, request.session.get(\"ID\")+app_name)\n            streamlit_url = f\"http://{host}:{port}?token={streamlit_token}\"\n            return RedirectResponse(url=streamlit_url)\n\n        resposee = await call_next(request)\n        return resposee\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit.BidirectionalStreamlitAppManager.resolve_session_token","title":"<code>resolve_session_token(request)</code>  <code>async</code>","text":"<p>Extract and validate session token from request</p> Source code in <code>toolboxv2/mods/FastApi/fast_lit.py</code> <pre><code>async def resolve_session_token(self, request: Request) -&gt; str | None:\n    \"\"\"\n    Extract and validate session token from request\n    \"\"\"\n    token = request.headers.get('Authorization', '').replace('Bearer ', '')\n    if not token:\n        token = request.query_params.get('token')\n\n    if token:\n        try:\n            jwt.decode(token, self.token_secret, algorithms=[\"HS256\"])\n            return token\n        except jwt.InvalidTokenError:\n            return None\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit.inject_custom_css","title":"<code>inject_custom_css(css_file_path='./web/assets/styles.css')</code>","text":"<p>Liest eine CSS-Datei ein und injiziert sie in die Streamlit-App.</p> Source code in <code>toolboxv2/mods/FastApi/fast_lit.py</code> <pre><code>def inject_custom_css(css_file_path=\"./web/assets/styles.css\"):\n    \"\"\"\n    Liest eine CSS-Datei ein und injiziert sie in die Streamlit-App.\n    \"\"\"\n    import streamlit as st\n    try:\n        with open(css_file_path) as f:\n            css_content = f.read()\n\n        # CSS in einen &lt;style&gt;-Tag einbetten\n        css_injection = f\"&lt;style&gt;{css_content}&lt;/style&gt;\"\n\n        # CSS in Streamlit injizieren\n        st.markdown(css_injection, unsafe_allow_html=True)\n    except Exception as e:\n        st.error(f\"Fehler beim Laden des CSS: {e}\")\n\n    st.markdown(\"\"\"\n        &lt;style&gt;\n            .reportview-container {\n                margin-top: -2em;\n            }\n            #MainMenu {visibility: hidden;}\n            .stDeployButton {display:none;}\n            footer {visibility: hidden;}\n            #stDecoration {display:none;}\n        &lt;/style&gt;\n    \"\"\", unsafe_allow_html=True)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit.make_api_request","title":"<code>make_api_request(endpoint, method='GET', data=None)</code>  <code>async</code>","text":"<p>Helper function for making API requests from Streamlit apps</p> Source code in <code>toolboxv2/mods/FastApi/fast_lit.py</code> <pre><code>async def make_api_request(endpoint: str, method: str = \"GET\", data: dict | None = None):\n    \"\"\"Helper function for making API requests from Streamlit apps\"\"\"\n    import streamlit as st\n\n    if not hasattr(st.session_state, 'token'):\n        st.error(\"No valid session token found\")\n        st.stop()\n\n    headers = {\n        'Authorization': f'Bearer {st.session_state.token}',\n        'Content-Type': 'application/json'\n    }\n\n    try:\n        api_helper = APIRequestHelper(os.getenv(\"TOKEN_SECRET\", \"your-secret-key\"))\n        response = await api_helper.make_api_request(\n            endpoint=endpoint,\n            method=method,\n            data=data,\n            headers=headers,\n            session_token=st.session_state.token\n        )\n        return response\n    except Exception as e:\n        st.error(f\"API request failed: {str(e)}\")\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice","title":"<code>fast_nice</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager","title":"<code>NiceGUIManager</code>","text":"Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>class NiceGUIManager(metaclass=Singleton):\n    init = False\n    def __init__(self, fastapi_app: FastAPI = None, styles_path: str = \"./web/assets/styles.css\"):\n\n        if fastapi_app is None:\n            return None\n        self.admin_password = os.getenv(\"TB_R_KEY\", \"root@admin\")\n        self.app = fastapi_app\n        self.styles_path = styles_path\n        self.registered_guis: dict[str, dict[str, Any]] = {}\n        self.ws_connections: dict[str, dict[str, WebSocket]] = {}\n        self.mount_path = \"/gui\"\n        self.endpoints: list[UIEndpoint] = []\n\n        self.helper_contex = open(\"./dist/helper.html\", encoding=\"utf-8\").read()\n\n        self.app.add_middleware(BaseHTTPMiddleware, dispatch=self.middleware_dispatch)\n\n        # Add WebSocket endpoint\n        self.app.websocket(\"/ws/{session_id}/{gui_id}\")(self.websocket_endpoint)\n        self._setup_admin_gui()\n        self._setup_endpoints_api()\n\n    def _setup_endpoints_api(self):\n        @self.app.get(\"/api/CloudM/openui\")\n        def get_ui_endpoints(request: Request) -&gt; list[dict]:\n            def _(endpoint):\n                add_true = True\n                if endpoint.only_valid:\n                    add_true = request.session['valid']\n\n                if add_true and endpoint.only_root:\n                    add_true = request.session.get('live_data', {}).get('user_name') == 'root'\n                return add_true\n            return [{\"path\": endpoint.path,\n    \"title\": endpoint.title,\n    \"description\": endpoint.description} for endpoint in self.endpoints if endpoint.show and _(endpoint)]\n\n    def _setup_admin_gui(self):\n        \"\"\"Setup the admin GUI interface\"\"\"\n\n        @ui.page('/admin')\n        def admin_gui(user=None):\n            print(\"admin_gui;\", user)\n            if user is None or user.name != \"root\":\n                return\n\n            with ui.card().style(\"background-color: var(--background-color) !important\").classes('w-full'):\n                ui.label('NiceGUI Manager Admin Interface').classes('text-2xl font-bold mb-4')\n\n                # GUI Management Section\n                with ui.tabs().style(\"background-color: var(--background-color) !important\") as tabs:\n                    ui.tab('Registered GUIs')\n                    ui.tab('Add New GUI')\n                    ui.tab('System Status')\n\n                with ui.tab_panels(tabs, value='Registered GUIs').style(\n                    \"background-color: var(--background-color) !important\"):\n                    with ui.tab_panel('Registered GUIs'):\n                        self._show_registered_guis()\n\n                    with ui.tab_panel('Add New GUI'):\n                        self._show_add_gui_form()\n\n                    with ui.tab_panel('System Status'):\n                        self._show_system_status()\n\n        self.register_gui(\"admin\", admin_gui, \"/admin\", only_root=True)\n\n    def _show_registered_guis(self):\n        \"\"\"Show list of registered GUIs with management options\"\"\"\n        with ui.column().classes('w-full gap-4'):\n            for gui_id, gui_info in self.registered_guis.items():\n                with ui.card().classes('w-full').style(\"background-color: var(--background-color) !important\"):\n                    with ui.row().classes('w-full items-center justify-between').style(\n                        \"background-color: var(--background-color) !important\"):\n                        ui.label(f'GUI ID: {gui_id}').classes('font-bold')\n                        ui.label(f'Path: {gui_info[\"path\"]}')\n\n                        created_at = gui_info['created_at'].strftime('%Y-%m-%d %H:%M:%S')\n                        ui.label(f'Created: {created_at}')\n\n                        with ui.row().classes('gap-2').style(\"background-color: var(--background-color) !important\"):\n                            ui.button('View', on_click=lambda g=gui_info['path']: ui.navigate.to(g))\n                            ui.button('Remove', on_click=lambda g=gui_id: self._handle_gui_removal(g))\n                            ui.button('Restart', on_click=lambda g=gui_id: self._handle_gui_restart(g))\n\n                    # Show connection status\n                    active_connections = sum(\n                        1 for connections in self.ws_connections.values()\n                        if gui_id in connections\n                    )\n                    ui.label(f'Active Connections: {active_connections}')\n\n    def _show_add_gui_form(self):\n        \"\"\"Show form for adding new GUI\"\"\"\n        with ui.card().classes('w-full').style(\"background-color: var(--background-color) !important\"):\n            gui_id = ui.input('GUI ID').classes('w-full')\n            mount_path = ui.input('Mount Path (optional)').classes('w-full')\n\n            # Code editor for GUI setup\n            code_editor = ui.editor(\n                value='def setup_gui():\\n    ui.label(\"New GUI\")\\n',\n            ).classes('w-full h-64')\n\n            def add_new_gui():\n                try:\n                    # Create setup function from code\n                    setup_code = code_editor.value\n                    setup_namespace = {}\n                    exec(setup_code, {'ui': ui}, setup_namespace)\n                    setup_func = setup_namespace['setup_gui']\n\n                    # Register the new GUI\n                    self.register_gui(\n                        gui_id.value,\n                        setup_func,\n                        mount_path.value if mount_path.value else None\n                    )\n\n                    ui.notify('GUI added successfully')\n                    ui.navigate.to('admin')  # Refresh page\n                except Exception as e:\n                    ui.notify(f'Error adding GUI: {str(e)}', color='negative')\n\n            ui.button('Add GUI', on_click=add_new_gui).classes('w-full mt-4')\n\n    def _show_system_status(self):\n        \"\"\"Show system status information\"\"\"\n        with ui.card().classes('w-full').style(\"background-color: var(--background-color) !important\"):\n            ui.label('System Status').classes('text-xl font-bold mb-4')\n\n            # System stats\n            ui.label(f'Total GUIs: {len(self.registered_guis)}')\n            ui.label(f'Total WebSocket Connections: {sum(len(conns) for conns in self.ws_connections.values())}')\n\n            # Memory usage\n            import psutil\n            process = psutil.Process()\n            memory_usage = process.memory_info().rss / 1024 / 1024  # MB\n            ui.label(f'Memory Usage: {memory_usage:.2f} MB')\n\n            # Add refresh button\n            ui.button('Refresh Stats', on_click=lambda: ui.navigate.to('/admin'))\n\n    def _handle_gui_removal(self, gui_id: str):\n        \"\"\"Handle GUI removal with confirmation\"\"\"\n\n        def confirm_remove():\n            if self.remove_gui(gui_id):\n                ui.notify(f'GUI {gui_id} removed successfully')\n                ui.navigate.to('/admin')  # Refresh page\n            else:\n                ui.notify('Error removing GUI', color='negative')\n\n        ui.notify('Are you sure?',\n                  actions=[{'label': 'Yes', 'on_click': confirm_remove},\n                           {'label': 'No'}])\n\n    def _handle_gui_restart(self, gui_id: str):\n        \"\"\"Handle GUI restart\"\"\"\n        try:\n            if gui_id in self.registered_guis:\n                gui_info = self.registered_guis[gui_id]\n                # Re-register the GUI with the same setup\n                self.register_gui(gui_id, gui_info['setup'], gui_info['path'])\n                ui.notify(f'GUI {gui_id} restarted successfully')\n            else:\n                ui.notify('GUI not found', color='negative')\n        except Exception as e:\n            ui.notify(f'Error restarting GUI: {str(e)}', color='negative')\n\n    def _load_styles(self) -&gt; str:\n        \"\"\"Load custom styles from CSS file\"\"\"\n        try:\n            with open(self.styles_path) as f:\n                return f.read()\n        except Exception as e:\n            print(f\"Error loading styles: {e}\")\n            return \"\"\n\n    def register_gui(self, gui_id: str, setup_func: Callable, mount_path: str | None = None, additional: str | None = None, title: str | None = None , description: str | None = None, **kwargs) -&gt; None:\n        \"\"\"Register a new NiceGUI application\"\"\"\n        path = mount_path or f\"/{gui_id}\"\n        self.endpoints.append(UIEndpoint(path=self.mount_path+path, title=title if title is not None else path.replace('/', '') , description=description if description is not None else '', **kwargs))\n        if additional is None:\n            additional = \"\"\n\n        def has_parameters(func, *params):\n            \"\"\"\n            \u00dcberpr\u00fcft, ob die Funktion bestimmte Parameter hat.\n\n            :param func: Die zu analysierende Funktion.\n            :param params: Eine Liste der zu suchenden Parameter.\n            :return: Ein Dictionary mit den Parametern und einem booleschen Wert.\n            \"\"\"\n            signature = inspect.signature(func)\n            func_params = signature.parameters.keys()\n            return {param: param in func_params for param in params}\n\n        async def request_to_request_session(request):\n            jk = request.json()\n            if asyncio.iscoroutine(jk):\n                with contextlib.suppress(Exception):\n                    jk = await jk\n            def js():\n                return jk\n            return RequestSession(\n                session=request.session,\n                body=request.body,\n                json=js,\n                row=request,\n            )\n\n        get_app()\n\n        @ui.page(path)\n        async def wrapped_gui(request: Request):\n            # Inject custom styles\n            ui.add_body_html(self.helper_contex + additional)\n            # ui.switch('Dark').bind_value(ui, 'dark_mode')\n            # ui.add_css(\"q-card {background-color: var(--background-color)} !important\")\n            # ui.add_body_html('&lt;script src=\"../index.js\" type=\"module\" defer&gt;&lt;/script&gt;')\n\n            # Initialize the GUI\n            params_ = {}\n            params = has_parameters(setup_func, 'request', 'user', 'session', 'id', 'sid')\n\n            if params.get('request'):\n                params_['request'] = await request_to_request_session(request)\n            if params.get('user'):\n                params_['user'] = await get_user_from_request(get_app(), request)\n            if params.get('session'):\n                params_['session'] = request.session\n            if params.get('spec'):\n                params_['spec'] = get_spec(request)\n            if params.get('sid'):\n                params_['sid'] = get_s_id(request)\n\n            async def task():\n                if asyncio.iscoroutine(setup_func):\n\n                    # Event Listener f\u00fcr Button hinzuf\u00fcgen\n                    await ui.run_javascript('''\n                            Quasar.Dark.set(\"auto\");\n                            tailwind.config.darkMode = \"media\";\n                        ''')\n\n                    await ui.run_javascript(\"\"\"\n                    document.getElementById('darkModeToggle').addEventListener('click', function () {\n                    const labelToggel = document.getElementById('toggleLabel')\n                    if (labelToggel.innerHTML == `&lt;span class=\"material-symbols-outlined\"&gt;\ndark_mode\n&lt;/span&gt;`){\n                            Quasar.Dark.set(true);\n                            tailwind.config.darkMode = \"class\";\n                            document.body.classList.add(\"dark\");\n                        }else{\n                            Quasar.Dark.set(false);\n                            tailwind.config.darkMode = \"class\"\n                            document.body.classList.remove(\"dark\");\n                        }\n                    });\n                    \"\"\")\n\n                    if not params_:\n                        await setup_func()\n                    else:\n                        await setup_func(**params_)\n                else:\n                    if not params_:\n                        setup_func()\n                    else:\n                        setup_func(**params_)\n\n\n\n\n            await task()\n            # return result\n\n        self.registered_guis[gui_id] = {\n            'path': path,\n            'setup': setup_func,\n            'created_at': datetime.now()\n        }\n\n        print(\"Registered GUI:\", self.registered_guis[gui_id])\n        return True\n\n    def remove_gui(self, gui_id: str) -&gt; bool:\n        \"\"\"Remove a registered GUI application\"\"\"\n        if gui_id in self.registered_guis:\n            # Remove from registry\n            del self.registered_guis[gui_id]\n\n            # Clean up any WebSocket connections\n            for session_id in self.ws_connections:\n                if gui_id in self.ws_connections[session_id]:\n                    del self.ws_connections[session_id][gui_id]\n\n            return True\n        return False\n\n    async def websocket_endpoint(self, websocket: WebSocket, session_id: str, gui_id: str):\n        \"\"\"Handle WebSocket connections for real-time updates\"\"\"\n        await websocket.accept()\n\n        if session_id not in self.ws_connections:\n            self.ws_connections[session_id] = {}\n        self.ws_connections[session_id][gui_id] = websocket\n\n        try:\n            while True:\n                data = await websocket.receive_json()\n                # Handle incoming WebSocket messages\n                await self.handle_ws_message(session_id, gui_id, data)\n        except WebSocketDisconnect:\n            if session_id in self.ws_connections:\n                if gui_id in self.ws_connections[session_id]:\n                    del self.ws_connections[session_id][gui_id]\n\n    async def handle_ws_message(self, session_id: str, gui_id: str, message: dict):\n        \"\"\"Handle incoming WebSocket messages\"\"\"\n        # Implement custom WebSocket message handling\n        if message.get('type') == 'update':\n            # Broadcast updates to all connected clients for this GUI\n            await self.broadcast_to_gui(gui_id, {\n                'type': 'update',\n                'data': message.get('data')\n            })\n\n    async def broadcast_to_gui(self, gui_id: str, message: dict):\n        \"\"\"Broadcast a message to all sessions connected to a specific GUI\"\"\"\n        for session_connections in self.ws_connections.values():\n            if gui_id in session_connections:\n                await session_connections[gui_id].send_json(message)\n\n    async def middleware_dispatch(self, request: Request, call_next) -&gt; Response:\n        \"\"\"Custom middleware for session handling and authentication\"\"\"\n        async def callN():\n            response = await call_next(request)\n            return response\n\n        if not request.url.path.startswith(self.mount_path):\n            return await callN()\n\n        if request.url.path.endswith(\"/favicon.ico\"):\n            return await callN()\n        if \"_nicegui\" in request.url.path and \"static\" in request.url.path:\n            return await callN()\n        if \"_nicegui\" in request.url.path and \"components\" in request.url.path:\n            return await callN()\n        if \"_nicegui\" in request.url.path and \"codehilite\" in request.url.path:\n            return await callN()\n        if \"_nicegui\" in request.url.path and \"libraries\" in request.url.path:\n            return await callN()\n\n        if \"open\" in request.url.path:\n            return await callN()\n\n        # Verify session if needed\n        if not request.session.get(\"valid\", False):\n            return RedirectResponse(f\"/web/login?next={request.url.path}\")\n\n        response = await call_next(request)\n        return response\n\n    def init_app(self) -&gt; None:\n        \"\"\"Initialize the FastAPI application with NiceGUI integration\"\"\"\n        self.init = True\n        ui.run_with(\n            self.app,\n            mount_path=self.mount_path,\n            favicon=os.getenv(\"FAVI\"), # \"/root/Toolboxv2/toolboxv2/favicon.ico\"\n            show_welcome_message=False,\n            # prod_js=False,\n        )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.broadcast_to_gui","title":"<code>broadcast_to_gui(gui_id, message)</code>  <code>async</code>","text":"<p>Broadcast a message to all sessions connected to a specific GUI</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>async def broadcast_to_gui(self, gui_id: str, message: dict):\n    \"\"\"Broadcast a message to all sessions connected to a specific GUI\"\"\"\n    for session_connections in self.ws_connections.values():\n        if gui_id in session_connections:\n            await session_connections[gui_id].send_json(message)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.handle_ws_message","title":"<code>handle_ws_message(session_id, gui_id, message)</code>  <code>async</code>","text":"<p>Handle incoming WebSocket messages</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>async def handle_ws_message(self, session_id: str, gui_id: str, message: dict):\n    \"\"\"Handle incoming WebSocket messages\"\"\"\n    # Implement custom WebSocket message handling\n    if message.get('type') == 'update':\n        # Broadcast updates to all connected clients for this GUI\n        await self.broadcast_to_gui(gui_id, {\n            'type': 'update',\n            'data': message.get('data')\n        })\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.init_app","title":"<code>init_app()</code>","text":"<p>Initialize the FastAPI application with NiceGUI integration</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>def init_app(self) -&gt; None:\n    \"\"\"Initialize the FastAPI application with NiceGUI integration\"\"\"\n    self.init = True\n    ui.run_with(\n        self.app,\n        mount_path=self.mount_path,\n        favicon=os.getenv(\"FAVI\"), # \"/root/Toolboxv2/toolboxv2/favicon.ico\"\n        show_welcome_message=False,\n        # prod_js=False,\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.middleware_dispatch","title":"<code>middleware_dispatch(request, call_next)</code>  <code>async</code>","text":"<p>Custom middleware for session handling and authentication</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>async def middleware_dispatch(self, request: Request, call_next) -&gt; Response:\n    \"\"\"Custom middleware for session handling and authentication\"\"\"\n    async def callN():\n        response = await call_next(request)\n        return response\n\n    if not request.url.path.startswith(self.mount_path):\n        return await callN()\n\n    if request.url.path.endswith(\"/favicon.ico\"):\n        return await callN()\n    if \"_nicegui\" in request.url.path and \"static\" in request.url.path:\n        return await callN()\n    if \"_nicegui\" in request.url.path and \"components\" in request.url.path:\n        return await callN()\n    if \"_nicegui\" in request.url.path and \"codehilite\" in request.url.path:\n        return await callN()\n    if \"_nicegui\" in request.url.path and \"libraries\" in request.url.path:\n        return await callN()\n\n    if \"open\" in request.url.path:\n        return await callN()\n\n    # Verify session if needed\n    if not request.session.get(\"valid\", False):\n        return RedirectResponse(f\"/web/login?next={request.url.path}\")\n\n    response = await call_next(request)\n    return response\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.register_gui","title":"<code>register_gui(gui_id, setup_func, mount_path=None, additional=None, title=None, description=None, **kwargs)</code>","text":"<p>Register a new NiceGUI application</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>    def register_gui(self, gui_id: str, setup_func: Callable, mount_path: str | None = None, additional: str | None = None, title: str | None = None , description: str | None = None, **kwargs) -&gt; None:\n        \"\"\"Register a new NiceGUI application\"\"\"\n        path = mount_path or f\"/{gui_id}\"\n        self.endpoints.append(UIEndpoint(path=self.mount_path+path, title=title if title is not None else path.replace('/', '') , description=description if description is not None else '', **kwargs))\n        if additional is None:\n            additional = \"\"\n\n        def has_parameters(func, *params):\n            \"\"\"\n            \u00dcberpr\u00fcft, ob die Funktion bestimmte Parameter hat.\n\n            :param func: Die zu analysierende Funktion.\n            :param params: Eine Liste der zu suchenden Parameter.\n            :return: Ein Dictionary mit den Parametern und einem booleschen Wert.\n            \"\"\"\n            signature = inspect.signature(func)\n            func_params = signature.parameters.keys()\n            return {param: param in func_params for param in params}\n\n        async def request_to_request_session(request):\n            jk = request.json()\n            if asyncio.iscoroutine(jk):\n                with contextlib.suppress(Exception):\n                    jk = await jk\n            def js():\n                return jk\n            return RequestSession(\n                session=request.session,\n                body=request.body,\n                json=js,\n                row=request,\n            )\n\n        get_app()\n\n        @ui.page(path)\n        async def wrapped_gui(request: Request):\n            # Inject custom styles\n            ui.add_body_html(self.helper_contex + additional)\n            # ui.switch('Dark').bind_value(ui, 'dark_mode')\n            # ui.add_css(\"q-card {background-color: var(--background-color)} !important\")\n            # ui.add_body_html('&lt;script src=\"../index.js\" type=\"module\" defer&gt;&lt;/script&gt;')\n\n            # Initialize the GUI\n            params_ = {}\n            params = has_parameters(setup_func, 'request', 'user', 'session', 'id', 'sid')\n\n            if params.get('request'):\n                params_['request'] = await request_to_request_session(request)\n            if params.get('user'):\n                params_['user'] = await get_user_from_request(get_app(), request)\n            if params.get('session'):\n                params_['session'] = request.session\n            if params.get('spec'):\n                params_['spec'] = get_spec(request)\n            if params.get('sid'):\n                params_['sid'] = get_s_id(request)\n\n            async def task():\n                if asyncio.iscoroutine(setup_func):\n\n                    # Event Listener f\u00fcr Button hinzuf\u00fcgen\n                    await ui.run_javascript('''\n                            Quasar.Dark.set(\"auto\");\n                            tailwind.config.darkMode = \"media\";\n                        ''')\n\n                    await ui.run_javascript(\"\"\"\n                    document.getElementById('darkModeToggle').addEventListener('click', function () {\n                    const labelToggel = document.getElementById('toggleLabel')\n                    if (labelToggel.innerHTML == `&lt;span class=\"material-symbols-outlined\"&gt;\ndark_mode\n&lt;/span&gt;`){\n                            Quasar.Dark.set(true);\n                            tailwind.config.darkMode = \"class\";\n                            document.body.classList.add(\"dark\");\n                        }else{\n                            Quasar.Dark.set(false);\n                            tailwind.config.darkMode = \"class\"\n                            document.body.classList.remove(\"dark\");\n                        }\n                    });\n                    \"\"\")\n\n                    if not params_:\n                        await setup_func()\n                    else:\n                        await setup_func(**params_)\n                else:\n                    if not params_:\n                        setup_func()\n                    else:\n                        setup_func(**params_)\n\n\n\n\n            await task()\n            # return result\n\n        self.registered_guis[gui_id] = {\n            'path': path,\n            'setup': setup_func,\n            'created_at': datetime.now()\n        }\n\n        print(\"Registered GUI:\", self.registered_guis[gui_id])\n        return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.remove_gui","title":"<code>remove_gui(gui_id)</code>","text":"<p>Remove a registered GUI application</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>def remove_gui(self, gui_id: str) -&gt; bool:\n    \"\"\"Remove a registered GUI application\"\"\"\n    if gui_id in self.registered_guis:\n        # Remove from registry\n        del self.registered_guis[gui_id]\n\n        # Clean up any WebSocket connections\n        for session_id in self.ws_connections:\n            if gui_id in self.ws_connections[session_id]:\n                del self.ws_connections[session_id][gui_id]\n\n        return True\n    return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.websocket_endpoint","title":"<code>websocket_endpoint(websocket, session_id, gui_id)</code>  <code>async</code>","text":"<p>Handle WebSocket connections for real-time updates</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>async def websocket_endpoint(self, websocket: WebSocket, session_id: str, gui_id: str):\n    \"\"\"Handle WebSocket connections for real-time updates\"\"\"\n    await websocket.accept()\n\n    if session_id not in self.ws_connections:\n        self.ws_connections[session_id] = {}\n    self.ws_connections[session_id][gui_id] = websocket\n\n    try:\n        while True:\n            data = await websocket.receive_json()\n            # Handle incoming WebSocket messages\n            await self.handle_ws_message(session_id, gui_id, data)\n    except WebSocketDisconnect:\n        if session_id in self.ws_connections:\n            if gui_id in self.ws_connections[session_id]:\n                del self.ws_connections[session_id][gui_id]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.create_nicegui_manager","title":"<code>create_nicegui_manager(app, token_secret=None)</code>","text":"<p>Create and initialize a NiceGUI manager instance</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>def create_nicegui_manager(app: FastAPI, token_secret: str | None = None) -&gt; NiceGUIManager:\n    \"\"\"Create and initialize a NiceGUI manager instance\"\"\"\n    manager = NiceGUIManager(app, token_secret)\n    manager.init_app()\n    manager_online[0] = True\n    return manager\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager","title":"<code>manager</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools","title":"<code>Tools</code>","text":"<p>               Bases: <code>MainTool</code>, <code>FileHandler</code></p> <p>A production-ready API Manager for running, monitoring, and managing FastAPI instances.</p> This class allows you to <ul> <li>Start API instances (live, development, debug)</li> <li>Stop and restart running APIs</li> <li>Update configuration for APIs</li> <li>Get live diagnostic info about running APIs</li> </ul> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>class Tools(MainTool, FileHandler):\n    \"\"\"\n    A production-ready API Manager for running, monitoring, and managing FastAPI instances.\n\n    This class allows you to:\n      - Start API instances (live, development, debug)\n      - Stop and restart running APIs\n      - Update configuration for APIs\n      - Get live diagnostic info about running APIs\n    \"\"\"\n\n    def __init__(self, app: Any | None = None) -&gt; None:\n        # Running APIs will be stored as a mapping from api_name to subprocess.Popen\n        self.running_apis: dict[str, multiprocessing.Process] = {}\n        self.api_config: dict[str, dict[str, str | int]] = {}\n        self.version: str = VERSION\n        self.name: str = NAME\n        self.logger: logging.Logger = app.logger if app else logging.getLogger(__name__)\n        self.color: str = \"WHITE\"\n        self.keys: dict[str, str] = {\"Apis\": \"api~config\"}\n        # In case app is not passed in, ensure that we have a dummy object with required properties\n\n        # Define available tool commands\n        self.tools: dict[str, Any] = {\n            \"all\": [\n                [\"Version\", \"Shows current Version\"],\n                [\"edit-api\", \"Set default API for name, host and port\"],\n                [\"start-api\", \"Start an API instance\"],\n                [\"stop-api\", \"Stop a running API instance\"],\n                [\"restart-api\", \"Restart an API instance\"],\n                [\"info\", \"Show API configurations and running APIs\"],\n            ],\n            \"name\": \"api_manager\",\n            \"Version\": self.show_version,\n            \"edit-api\": self.conf_api,\n            \"stop-api\": self.stop_api,\n            \"start\": self.start_live,\n            \"startE\": self._start_api,\n            \"startDev\": self.start_dev,\n            \"startDUG\": self.start_debug,\n            \"info\": self.show_running,\n            \"restart-api\": self.restart_api,\n        }\n\n        # Initialize FileHandler with default configuration data\n        default_config = {\n            \"Apis\": {\n                'main': {\n                    \"Name\": 'main',\n                    \"version\": self.version,\n                    \"port\": 5000,\n                    \"host\": '127.0.0.1'\n                }\n            }\n        }\n        FileHandler.__init__(self, \"apis.config\", self.app.id, self.keys, default_config)\n        MainTool.__init__(\n            self,\n            load=self.on_start,\n            v=self.version,\n            tool=self.tools,\n            name=self.name,\n            logs=self.logger,\n            color=self.color,\n            on_exit=self.on_exit,\n        )\n        os.makedirs(\"./.data\", exist_ok=True)\n\n    @staticmethod\n    def _get_pid_file_path(api_name: str) -&gt; str:\n        \"\"\"Get the path to the PID file for an API.\"\"\"\n        return os.path.join(\"./.data\", f\"api_pid_{api_name}\")\n\n\n    def show_version(self) -&gt; str:\n        \"\"\"Display and return the current version.\"\"\"\n        self.logger.info(\"Version: %s\", self.version)\n        return self.version\n\n    def info(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Return diagnostic information about API configurations and currently running APIs.\n        \"\"\"\n        config_info = {name: cfg for name, cfg in self.api_config.items()}\n        running_info = {name: proc.pid for name, proc in self.running_apis.items() if proc.is_alive()}\n        self.logger.info(\"API Configurations: %s\", config_info)\n        self.logger.info(\"Running APIs: %s\", running_info)\n        # Optionally, print to console as well\n        for api_name, cfg in config_info.items():\n            print(f\"Configured API - Name: {api_name}, Config: {cfg}\")\n        print(\"Running APIs:\")\n        for api_name, pid in running_info.items():\n            print(f\"API: {api_name}, Process ID: {pid}\")\n        return {\"configurations\": config_info, \"running\": running_info}\n\n    def conf_api(self, api_name: str, host: str = \"localhost\", port: int = 5000) -&gt; None:\n        \"\"\"\n        Update or create an API configuration.\n\n        Args:\n            api_name (str): The name of the API.\n            host (str): The host address (default \"localhost\"). Use \"lh\" for \"127.0.0.1\" or \"0\" for \"0.0.0.0\".\n            port (int): The port number (default 5000; use \"0\" for port 8000).\n        \"\"\"\n        if host.lower() == \"lh\":\n            host = \"127.0.0.1\"\n        if host == \"0\":\n            host = \"0.0.0.0\"\n        if str(port) == \"0\":\n            port = 8000\n\n        self.api_config[api_name] = {\n            \"Name\": api_name,\n            \"version\": self.version,\n            \"port\": int(port),\n            \"host\": host,\n        }\n        self.logger.info(\"Updated API configuration for '%s': %s\", api_name, self.api_config[api_name])\n        print(f\"API configuration updated: {self.api_config[api_name]}\")\n\n    def start_dev(self, api_name: str, *modules: str, **kwargs: Any) -&gt; str | None:\n        \"\"\"\n        Start an API in development mode.\n\n        If additional modules are provided, they are stored in a BlobFile for later use.\n\n        Args:\n            api_name (str): The API name.\n            *modules (str): Additional modules for the API.\n\n        Returns:\n            Optional[str]: Status message.\n        \"\"\"\n        if modules:\n            api_name_dev = f\"{api_name}_D\"\n            with BlobFile(f\"FastApi/{api_name_dev}/dev\", mode='w') as f:\n                f.write_json({'modules': modules})\n            api_name = api_name_dev\n\n        return self._start_api(api_name, live=False, reload=False, test_override=False, host=\"localhost\")\n\n    def start_live(self, api_name: str) -&gt; str | None:\n        \"\"\"\n        Start an API in live mode.\n        \"\"\"\n        return self._start_api(api_name, live=True, reload=False, test_override=False)\n\n    def start_debug(self, api_name: str) -&gt; str | None:\n        \"\"\"\n        Start an API in debug mode.\n        \"\"\"\n        return self._start_api(api_name, live=False, reload=True, test_override=True, host=\"localhost\")\n\n    def _start_api(\n        self,\n        api_name: str,\n        live: bool = False,\n        reload: bool = False,\n        test_override: bool = False,\n        host: str = \"localhost\"\n    ) -&gt; str | None:\n        \"\"\"\n        Start an API process with the given configuration.\n\n        Args:\n            api_name (str): The API name.\n            live (bool): Whether to run in live mode.\n            reload (bool): Whether to enable auto-reload.\n            test_override (bool): If True, allow start even if running in a test environment.\n            host (str): Host to bind the API on.\n\n        Returns:\n            Optional[str]: A status message or error message.\n        \"\"\"\n        # Prevent starting an API if in test mode unless explicitly overridden.\n        if 'test' in self.app.id and not test_override:\n            msg = \"No API allowed in test mode\"\n            self.logger.warning(msg)\n            return msg\n\n        if not api_name:\n            self.logger.error(\"No API name provided.\")\n            return None\n\n        # Check if API is already running.\n        if api_name in self.running_apis and self.running_apis[api_name].is_alive():\n            msg = f\"API '{api_name}' is already running.\"\n            self.logger.info(msg)\n            return msg\n\n        # Ensure that live and reload are not both enabled.\n        if live and reload:\n            raise ValueError(\"Live mode and reload mode cannot be enabled simultaneously.\")\n\n        # If configuration does not exist, add it automatically.\n        if api_name not in self.api_config:\n            self.api_config[api_name] = {\n                \"Name\": api_name,\n                \"version\": self.version,\n                \"port\": self.app.args_sto.port,\n                \"host\": host if host and isinstance(host, str) else \"localhost\",\n            }\n            if live:\n                self.api_config[api_name]['host'] = \"0.0.0.0\"\n            self.logger.info(\"Auto-added API configuration for '%s': %s\", api_name, self.api_config[api_name])\n\n        # For live mode, always bind to all interfaces.\n        if live:\n            self.api_config[api_name]['host'] = \"0.0.0.0\"\n\n        api_data = self.api_config[api_name]\n\n        # Check for required frontend dependencies.\n        node_modules_path = os.path.join(self.app.start_dir, \"web\", \"node_modules\")\n        if not os.path.exists(node_modules_path):\n            self.logger.info(\"Node modules folder not found. Installing dependencies in '%s'\", node_modules_path)\n            os.system(\"npm install --prefix ./web ./web\")\n\n        # Build the uvicorn command.\n        cmd_parts: list[str] = [\n            # sys.executable,\n            # \"-m\",\n            \"uvicorn\",\n            \"toolboxv2.mods.FastApi.fast_api_main:app\",\n            f\"--host {api_data['host']}\",\n            f\"--port {api_data['port']}\",\n            f\"--header data:{self.app.debug}:{api_name}\"\n        ]\n        if reload:\n            # Reload directories can be adjusted as needed.\n            cmd_parts.append(\"--reload\")\n            cmd_parts.append(\"--reload-dir ./utils\")\n            cmd_parts.append(\"--reload-dir ./mods/FastApi\")\n        command: str = \" \".join(cmd_parts)\n        self.logger.info(\"Starting API '%s' with command: %s\", api_name, command)\n\n        print(command)\n\n        # Print QR codes for local and public IPs for convenience.\n        protocol = \"http\"  # Adjust if SSL is configured\n        local_url = f\"{protocol}://{get_local_ip()}:{api_data['port']}\"\n        public_url = f\"{protocol}://{get_public_ip()}:{api_data['port']}\"\n        print_qrcode_to_console(local_url)\n        print_qrcode_to_console(public_url)\n\n        try:\n\n            process = multiprocessing.Process(\n                target=os.system,\n                args=(command,),\n                # daemon=True\n            )\n            process.start()\n\n            # Store the process\n            self.running_apis[api_name] = process\n\n            # Save PID to file\n            with open(self._get_pid_file_path(api_name), \"w\") as f:\n                f.write(str(process.pid))\n\n            # Store process info in file handler\n            self.add_to_save_file_handler(\n                key=f\"pr{api_name}\",\n                value=json.dumps({\n                    \"pid\": process.pid,\n                    \"start_time\": datetime.now().isoformat(),\n                    \"host\": api_data['host'],\n                    \"port\": api_data['port']\n                })\n            )\n\n            msg = f\"Starting API '{api_name}' at {api_data['host']}:{api_data['port']} (PID: {process.pid})\"\n            self.logger.info(msg)\n            return msg\n        except Exception as e:\n            self.logger.exception(\"Failed to start API '%s': %s\", api_name, e)\n            return f\"Failed to start API '{api_name}': {e}\"\n\n    async def stop_api(self, api_name: str, delete: bool = True) -&gt; str:\n        \"\"\"\n        Stop a running API and clean up resources.\n        \"\"\"\n        if api_name not in self.api_config:\n            msg = f\"API with the name '{api_name}' is not configured.\"\n            self.logger.warning(msg)\n            return msg\n\n        pid_file = self._get_pid_file_path(api_name)\n        if not os.path.exists(pid_file):\n            self.logger.warning(\"No pid file found for API '%s'\", api_name)\n            return f\"No pid file found for API '{api_name}'.\"\n\n        try:\n            # Read PID from file\n            with open(pid_file) as f:\n                api_pid = int(f.read().strip())\n\n            # Try graceful shutdown first\n            if 'core' in self.app.id:\n                if not await self.app.session.login():\n                    self.logger.warning(\"Could not login with username '%s'\", self.app.get_username())\n                try:\n                    response = await self.app.session.fetch(f\"/api/exit/{api_pid}\", method=\"POST\")\n                    self.logger.info(\"Exit response for API '%s': %s\", api_name, response)\n                except Exception as e:\n                    self.logger.warning(\"Failed to stop API gracefully: %s\", e)\n\n            # Force kill if process still exists\n            process = self.running_apis.get(api_name)\n            if process and process.is_alive():\n                process.terminate()\n                process.join(timeout=5)\n                if process.is_alive():\n                    process.kill()\n\n            # Fallback to system commands if needed\n            try:\n                if system() == \"Windows\":\n                    os.system(f\"taskkill /pid {api_pid} /F\")\n                else:\n                    os.kill(api_pid, signal.SIGKILL)\n            except ProcessLookupError:\n                pass  # Process already terminated\n\n            # Cleanup\n            if os.path.exists(pid_file):\n                os.remove(pid_file)\n            if delete and api_name in self.running_apis:\n                del self.running_apis[api_name]\n\n            # Update file handler\n            self.add_to_save_file_handler(\n                key=f\"pr{api_name}\",\n                value=json.dumps({\n                    \"stop_time\": datetime.now().isoformat(),\n                    \"status\": \"stopped\"\n                })\n            )\n            self.save_file_handler()\n\n            msg = f\"Stopped API '{api_name}'.\"\n            self.logger.info(msg)\n            return msg\n\n        except Exception as e:\n            self.logger.exception(\"Error stopping API '%s': %s\", api_name, e)\n            return f\"Error stopping API '{api_name}': {e}\"\n\n    def nf(self, name):\n        if len(name) &gt; 10:\n            return name[:10]\n        elif len(name) &lt; 10:\n            return name + '~' * (len(name)-10)\n        else:\n            return name\n\n    def show_running(self) -&gt; list[str]:\n        \"\"\"\n        Display and return the list of currently running APIs with their status.\n        \"\"\"\n        self.on_start()\n        running_list = []\n        print(self.api_config)\n        for api_name in self.api_config:\n\n            # Get stored process info\n            process_info = self.get_file_handler(f\"pr{api_name}\")\n            print('#',api_name, '#',process_info)\n            if process_info is None:\n                process_info = {}\n            status = {\n                \"name\": api_name,\n                \"online\": api_name in self.running_apis,\n                \"start_time\": process_info.get(\"start_time\", \"offline\"),\n                \"pid\": process_info.get(\"pid\", ''),\n                \"host\": process_info.get(\"host\", ''),\n                \"port\": process_info.get(\"port\", '')\n            }\n            running_list.append(status)\n\n        # Log and print current status\n        self.logger.info(\"APIs: %s\", running_list)\n        print(\"\\nAPIs:\")\n        for api in running_list:\n            print(f\"- {api['name']}: at {api['host']}:{api['port']}\")\n            print(f\"  Started: {api['start_time']}\")\n\n        return [api[\"name\"] for api in running_list]\n\n    async def restart_api(self, api_name: str) -&gt; str:\n        \"\"\"\n        Restart the given API by stopping it and starting it again.\n\n        Args:\n            api_name (str): The name of the API to restart.\n\n        Returns:\n            str: A status message.\n        \"\"\"\n        stop_message = await self.stop_api(api_name)\n        self.logger.info(\"Restart: %s\", stop_message)\n        # Allow some time for the process to fully terminate.\n        time.sleep(4)\n        start_message = self._start_api(api_name)\n        return f\"Restarting API '{api_name}': {start_message}\"\n\n    def on_start(self) -&gt; None:\n        \"\"\"\n        Load API configuration from file when the tool starts.\n        \"\"\"\n        self.load_file_handler()\n        data = self.get_file_handler(self.keys[\"Apis\"])\n        try:\n            if isinstance(data, str):\n                self.api_config = json.loads(data)\n            else:\n                self.api_config = data\n            self.logger.info(\"Loaded API configuration: %s\", self.api_config)\n        except Exception as e:\n            self.logger.exception(\"Error loading API configuration: %s\", e)\n            self.api_config = {}\n\n    async def on_exit(self) -&gt; None:\n        \"\"\"\n        Gracefully stop all running APIs and save configuration upon exit.\n        \"\"\"\n        # Save configuration data.\n        if len(self.api_config) != 0:\n            self.add_to_save_file_handler(self.keys[\"Apis\"], json.dumps(self.api_config))\n        # Attempt to stop all running APIs.\n        # for api_name in list(self.running_apis.keys()):\n        #     await self.stop_api(api_name, delete=False)\n        self.running_apis = {}\n        self.save_file_handler()\n        self.logger.info(\"Exiting API Manager. All running APIs stopped and configuration saved.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.conf_api","title":"<code>conf_api(api_name, host='localhost', port=5000)</code>","text":"<p>Update or create an API configuration.</p> <p>Parameters:</p> Name Type Description Default <code>api_name</code> <code>str</code> <p>The name of the API.</p> required <code>host</code> <code>str</code> <p>The host address (default \"localhost\"). Use \"lh\" for \"127.0.0.1\" or \"0\" for \"0.0.0.0\".</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>The port number (default 5000; use \"0\" for port 8000).</p> <code>5000</code> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def conf_api(self, api_name: str, host: str = \"localhost\", port: int = 5000) -&gt; None:\n    \"\"\"\n    Update or create an API configuration.\n\n    Args:\n        api_name (str): The name of the API.\n        host (str): The host address (default \"localhost\"). Use \"lh\" for \"127.0.0.1\" or \"0\" for \"0.0.0.0\".\n        port (int): The port number (default 5000; use \"0\" for port 8000).\n    \"\"\"\n    if host.lower() == \"lh\":\n        host = \"127.0.0.1\"\n    if host == \"0\":\n        host = \"0.0.0.0\"\n    if str(port) == \"0\":\n        port = 8000\n\n    self.api_config[api_name] = {\n        \"Name\": api_name,\n        \"version\": self.version,\n        \"port\": int(port),\n        \"host\": host,\n    }\n    self.logger.info(\"Updated API configuration for '%s': %s\", api_name, self.api_config[api_name])\n    print(f\"API configuration updated: {self.api_config[api_name]}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.info","title":"<code>info()</code>","text":"<p>Return diagnostic information about API configurations and currently running APIs.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def info(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Return diagnostic information about API configurations and currently running APIs.\n    \"\"\"\n    config_info = {name: cfg for name, cfg in self.api_config.items()}\n    running_info = {name: proc.pid for name, proc in self.running_apis.items() if proc.is_alive()}\n    self.logger.info(\"API Configurations: %s\", config_info)\n    self.logger.info(\"Running APIs: %s\", running_info)\n    # Optionally, print to console as well\n    for api_name, cfg in config_info.items():\n        print(f\"Configured API - Name: {api_name}, Config: {cfg}\")\n    print(\"Running APIs:\")\n    for api_name, pid in running_info.items():\n        print(f\"API: {api_name}, Process ID: {pid}\")\n    return {\"configurations\": config_info, \"running\": running_info}\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.on_exit","title":"<code>on_exit()</code>  <code>async</code>","text":"<p>Gracefully stop all running APIs and save configuration upon exit.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>async def on_exit(self) -&gt; None:\n    \"\"\"\n    Gracefully stop all running APIs and save configuration upon exit.\n    \"\"\"\n    # Save configuration data.\n    if len(self.api_config) != 0:\n        self.add_to_save_file_handler(self.keys[\"Apis\"], json.dumps(self.api_config))\n    # Attempt to stop all running APIs.\n    # for api_name in list(self.running_apis.keys()):\n    #     await self.stop_api(api_name, delete=False)\n    self.running_apis = {}\n    self.save_file_handler()\n    self.logger.info(\"Exiting API Manager. All running APIs stopped and configuration saved.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.on_start","title":"<code>on_start()</code>","text":"<p>Load API configuration from file when the tool starts.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def on_start(self) -&gt; None:\n    \"\"\"\n    Load API configuration from file when the tool starts.\n    \"\"\"\n    self.load_file_handler()\n    data = self.get_file_handler(self.keys[\"Apis\"])\n    try:\n        if isinstance(data, str):\n            self.api_config = json.loads(data)\n        else:\n            self.api_config = data\n        self.logger.info(\"Loaded API configuration: %s\", self.api_config)\n    except Exception as e:\n        self.logger.exception(\"Error loading API configuration: %s\", e)\n        self.api_config = {}\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.restart_api","title":"<code>restart_api(api_name)</code>  <code>async</code>","text":"<p>Restart the given API by stopping it and starting it again.</p> <p>Parameters:</p> Name Type Description Default <code>api_name</code> <code>str</code> <p>The name of the API to restart.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A status message.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>async def restart_api(self, api_name: str) -&gt; str:\n    \"\"\"\n    Restart the given API by stopping it and starting it again.\n\n    Args:\n        api_name (str): The name of the API to restart.\n\n    Returns:\n        str: A status message.\n    \"\"\"\n    stop_message = await self.stop_api(api_name)\n    self.logger.info(\"Restart: %s\", stop_message)\n    # Allow some time for the process to fully terminate.\n    time.sleep(4)\n    start_message = self._start_api(api_name)\n    return f\"Restarting API '{api_name}': {start_message}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.show_running","title":"<code>show_running()</code>","text":"<p>Display and return the list of currently running APIs with their status.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def show_running(self) -&gt; list[str]:\n    \"\"\"\n    Display and return the list of currently running APIs with their status.\n    \"\"\"\n    self.on_start()\n    running_list = []\n    print(self.api_config)\n    for api_name in self.api_config:\n\n        # Get stored process info\n        process_info = self.get_file_handler(f\"pr{api_name}\")\n        print('#',api_name, '#',process_info)\n        if process_info is None:\n            process_info = {}\n        status = {\n            \"name\": api_name,\n            \"online\": api_name in self.running_apis,\n            \"start_time\": process_info.get(\"start_time\", \"offline\"),\n            \"pid\": process_info.get(\"pid\", ''),\n            \"host\": process_info.get(\"host\", ''),\n            \"port\": process_info.get(\"port\", '')\n        }\n        running_list.append(status)\n\n    # Log and print current status\n    self.logger.info(\"APIs: %s\", running_list)\n    print(\"\\nAPIs:\")\n    for api in running_list:\n        print(f\"- {api['name']}: at {api['host']}:{api['port']}\")\n        print(f\"  Started: {api['start_time']}\")\n\n    return [api[\"name\"] for api in running_list]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.show_version","title":"<code>show_version()</code>","text":"<p>Display and return the current version.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def show_version(self) -&gt; str:\n    \"\"\"Display and return the current version.\"\"\"\n    self.logger.info(\"Version: %s\", self.version)\n    return self.version\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.start_debug","title":"<code>start_debug(api_name)</code>","text":"<p>Start an API in debug mode.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def start_debug(self, api_name: str) -&gt; str | None:\n    \"\"\"\n    Start an API in debug mode.\n    \"\"\"\n    return self._start_api(api_name, live=False, reload=True, test_override=True, host=\"localhost\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.start_dev","title":"<code>start_dev(api_name, *modules, **kwargs)</code>","text":"<p>Start an API in development mode.</p> <p>If additional modules are provided, they are stored in a BlobFile for later use.</p> <p>Parameters:</p> Name Type Description Default <code>api_name</code> <code>str</code> <p>The API name.</p> required <code>*modules</code> <code>str</code> <p>Additional modules for the API.</p> <code>()</code> <p>Returns:</p> Type Description <code>str | None</code> <p>Optional[str]: Status message.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def start_dev(self, api_name: str, *modules: str, **kwargs: Any) -&gt; str | None:\n    \"\"\"\n    Start an API in development mode.\n\n    If additional modules are provided, they are stored in a BlobFile for later use.\n\n    Args:\n        api_name (str): The API name.\n        *modules (str): Additional modules for the API.\n\n    Returns:\n        Optional[str]: Status message.\n    \"\"\"\n    if modules:\n        api_name_dev = f\"{api_name}_D\"\n        with BlobFile(f\"FastApi/{api_name_dev}/dev\", mode='w') as f:\n            f.write_json({'modules': modules})\n        api_name = api_name_dev\n\n    return self._start_api(api_name, live=False, reload=False, test_override=False, host=\"localhost\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.start_live","title":"<code>start_live(api_name)</code>","text":"<p>Start an API in live mode.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def start_live(self, api_name: str) -&gt; str | None:\n    \"\"\"\n    Start an API in live mode.\n    \"\"\"\n    return self._start_api(api_name, live=True, reload=False, test_override=False)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.stop_api","title":"<code>stop_api(api_name, delete=True)</code>  <code>async</code>","text":"<p>Stop a running API and clean up resources.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>async def stop_api(self, api_name: str, delete: bool = True) -&gt; str:\n    \"\"\"\n    Stop a running API and clean up resources.\n    \"\"\"\n    if api_name not in self.api_config:\n        msg = f\"API with the name '{api_name}' is not configured.\"\n        self.logger.warning(msg)\n        return msg\n\n    pid_file = self._get_pid_file_path(api_name)\n    if not os.path.exists(pid_file):\n        self.logger.warning(\"No pid file found for API '%s'\", api_name)\n        return f\"No pid file found for API '{api_name}'.\"\n\n    try:\n        # Read PID from file\n        with open(pid_file) as f:\n            api_pid = int(f.read().strip())\n\n        # Try graceful shutdown first\n        if 'core' in self.app.id:\n            if not await self.app.session.login():\n                self.logger.warning(\"Could not login with username '%s'\", self.app.get_username())\n            try:\n                response = await self.app.session.fetch(f\"/api/exit/{api_pid}\", method=\"POST\")\n                self.logger.info(\"Exit response for API '%s': %s\", api_name, response)\n            except Exception as e:\n                self.logger.warning(\"Failed to stop API gracefully: %s\", e)\n\n        # Force kill if process still exists\n        process = self.running_apis.get(api_name)\n        if process and process.is_alive():\n            process.terminate()\n            process.join(timeout=5)\n            if process.is_alive():\n                process.kill()\n\n        # Fallback to system commands if needed\n        try:\n            if system() == \"Windows\":\n                os.system(f\"taskkill /pid {api_pid} /F\")\n            else:\n                os.kill(api_pid, signal.SIGKILL)\n        except ProcessLookupError:\n            pass  # Process already terminated\n\n        # Cleanup\n        if os.path.exists(pid_file):\n            os.remove(pid_file)\n        if delete and api_name in self.running_apis:\n            del self.running_apis[api_name]\n\n        # Update file handler\n        self.add_to_save_file_handler(\n            key=f\"pr{api_name}\",\n            value=json.dumps({\n                \"stop_time\": datetime.now().isoformat(),\n                \"status\": \"stopped\"\n            })\n        )\n        self.save_file_handler()\n\n        msg = f\"Stopped API '{api_name}'.\"\n        self.logger.info(msg)\n        return msg\n\n    except Exception as e:\n        self.logger.exception(\"Error stopping API '%s': %s\", api_name, e)\n        return f\"Error stopping API '{api_name}': {e}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FileWidget","title":"<code>FileWidget</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.FileWidget.FileUploadHandler","title":"<code>FileUploadHandler</code>","text":"Source code in <code>toolboxv2/mods/FileWidget.py</code> <pre><code>class FileUploadHandler:\n    def __init__(self, upload_dir: str = 'uploads'):\n        self.upload_dir = Path(upload_dir)\n        self.upload_dir.mkdir(parents=True, exist_ok=True)\n        # self.app = get_app().app # If logger is needed here\n\n    def save_file(self, chunk_info: ChunkInfo, storage: BlobStorage) -&gt; str:\n        \"\"\"Speichert die Datei oder Chunk. Chunks werden lokal gespeichert, dann zu BlobStorage gemerged.\"\"\"\n        final_blob_path = Path(chunk_info.filename).name  # Use only filename part for security within blob storage\n\n        if chunk_info.total_chunks == 1:\n            # Komplette Datei direkt in BlobStorage speichern\n            # print(f\"Saving single part file: {final_blob_path} to BlobStorage directly.\") # Debug\n            with BlobFile(final_blob_path, 'w', storage=storage) as bf:\n                bf.write(chunk_info.content)\n        else:\n            # Chunk lokal speichern\n            # Sanitize filename for local path (original chunk_info.filename might contain path parts client-side)\n            safe_base_filename = \"\".join(\n                c if c.isalnum() or c in ('.', '_', '-') else '_' for c in Path(chunk_info.filename).name)\n            chunk_path = self.upload_dir / f\"{safe_base_filename}.part{chunk_info.chunk_index}\"\n            # print(f\"Saving chunk: {chunk_path} locally. Total chunks: {chunk_info.total_chunks}\") # Debug\n\n            with open(chunk_path, 'wb') as f:\n                f.write(chunk_info.content)\n\n            if self._all_chunks_received(safe_base_filename, chunk_info.total_chunks):\n                # print(f\"All chunks received for {safe_base_filename}. Merging to BlobStorage path: {final_blob_path}\") # Debug\n                self._merge_chunks_to_blob(safe_base_filename, chunk_info.total_chunks, final_blob_path, storage)\n                self._cleanup_chunks(safe_base_filename, chunk_info.total_chunks)\n            # else:\n            # print(f\"Still waiting for more chunks for {safe_base_filename}.\") # Debug\n\n        return final_blob_path  # Path within BlobStorage\n\n    def _all_chunks_received(self, safe_base_filename: str, total_chunks: int) -&gt; bool:\n        for i in range(total_chunks):\n            chunk_path = self.upload_dir / f\"{safe_base_filename}.part{i}\"\n            if not chunk_path.exists():\n                # print(f\"Chunk {i} for {safe_base_filename} not found. Path: {chunk_path}\") # Debug\n                return False\n        # print(f\"All {total_chunks} chunks found for {safe_base_filename}.\") # Debug\n        return True\n\n    def _merge_chunks_to_blob(self, safe_base_filename: str, total_chunks: int, final_blob_path: str,\n                              storage: BlobStorage):\n        # print(f\"Merging {total_chunks} chunks for {safe_base_filename} into Blob: {final_blob_path}\") # Debug\n        with BlobFile(final_blob_path, 'w', storage=storage) as outfile:\n            for i in range(total_chunks):\n                chunk_path = self.upload_dir / f\"{safe_base_filename}.part{i}\"\n                # print(f\"Appending chunk {i} ({chunk_path}) to Blob.\") # Debug\n                with open(chunk_path, 'rb') as chunk_file:\n                    outfile.write(chunk_file.read())\n        # print(f\"Finished merging chunks for {safe_base_filename} to Blob: {final_blob_path}\") # Debug\n\n    def _cleanup_chunks(self, safe_base_filename: str, total_chunks: int):\n        # print(f\"Cleaning up {total_chunks} chunks for {safe_base_filename}.\") # Debug\n        for i in range(total_chunks):\n            chunk_path = self.upload_dir / f\"{safe_base_filename}.part{i}\"\n            if chunk_path.exists():\n                # print(f\"Removing chunk: {chunk_path}\") # Debug\n                try:\n                    os.remove(chunk_path)\n                except OSError as e:\n                    # self.app.logger.error(f\"Error removing chunk {chunk_path}: {e}\") # If logger available\n                    print(f\"Error removing chunk {chunk_path}: {e}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FileWidget.FileUploadHandler.save_file","title":"<code>save_file(chunk_info, storage)</code>","text":"<p>Speichert die Datei oder Chunk. Chunks werden lokal gespeichert, dann zu BlobStorage gemerged.</p> Source code in <code>toolboxv2/mods/FileWidget.py</code> <pre><code>def save_file(self, chunk_info: ChunkInfo, storage: BlobStorage) -&gt; str:\n    \"\"\"Speichert die Datei oder Chunk. Chunks werden lokal gespeichert, dann zu BlobStorage gemerged.\"\"\"\n    final_blob_path = Path(chunk_info.filename).name  # Use only filename part for security within blob storage\n\n    if chunk_info.total_chunks == 1:\n        # Komplette Datei direkt in BlobStorage speichern\n        # print(f\"Saving single part file: {final_blob_path} to BlobStorage directly.\") # Debug\n        with BlobFile(final_blob_path, 'w', storage=storage) as bf:\n            bf.write(chunk_info.content)\n    else:\n        # Chunk lokal speichern\n        # Sanitize filename for local path (original chunk_info.filename might contain path parts client-side)\n        safe_base_filename = \"\".join(\n            c if c.isalnum() or c in ('.', '_', '-') else '_' for c in Path(chunk_info.filename).name)\n        chunk_path = self.upload_dir / f\"{safe_base_filename}.part{chunk_info.chunk_index}\"\n        # print(f\"Saving chunk: {chunk_path} locally. Total chunks: {chunk_info.total_chunks}\") # Debug\n\n        with open(chunk_path, 'wb') as f:\n            f.write(chunk_info.content)\n\n        if self._all_chunks_received(safe_base_filename, chunk_info.total_chunks):\n            # print(f\"All chunks received for {safe_base_filename}. Merging to BlobStorage path: {final_blob_path}\") # Debug\n            self._merge_chunks_to_blob(safe_base_filename, chunk_info.total_chunks, final_blob_path, storage)\n            self._cleanup_chunks(safe_base_filename, chunk_info.total_chunks)\n        # else:\n        # print(f\"Still waiting for more chunks for {safe_base_filename}.\") # Debug\n\n    return final_blob_path  # Path within BlobStorage\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FileWidget.access_shared_file","title":"<code>access_shared_file(self, request, share_id, filename=None, row=None)</code>  <code>async</code>","text":"<p>Accesses a shared file via its share_id. The URL for this would be like /api/FileWidget/shared/{share_id_value} The 'share_id: str' in signature implies ToolBoxV2 extracts it from path.</p> Source code in <code>toolboxv2/mods/FileWidget.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, name=\"open_shared\", api_methods=['GET'],\n        request_as_kwarg=True, level=-1, row=True)\nasync def access_shared_file(self, request: RequestData, share_id: str, filename: str = None, row=None) -&gt; Result:  # share_id from query params\n    \"\"\"\n    Accesses a shared file via its share_id.\n    The URL for this would be like /api/FileWidget/shared/{share_id_value}\n    The 'share_id: str' in signature implies ToolBoxV2 extracts it from path.\n    \"\"\"\n    if not share_id:\n        return Result.html(data=\"Share ID is missing in path.\", status=302)\n\n    share_info = self.shares.get(share_id)\n    if not share_info:\n        return Result.html(data=\"Share link is invalid or has expired.\", status=404)\n\n    owner_uid = share_info[\"owner_uid\"]\n    file_path_in_owner_storage = share_info[\"file_path\"]\n\n    try:\n        # Get BlobStorage for the owner, not the current request's user (if any)\n        owner_storage = await self.get_blob_storage(\n            owner_uid_override=owner_uid)  # Crucially, pass request=None if not needed\n        self.app.logger.info(\n            f\"Accessing shared file via link {share_id}: owner {owner_uid}, path {file_path_in_owner_storage}\")\n        result = await _prepare_file_response(self, owner_storage, file_path_in_owner_storage, row=row is not None)\n        if result.is_error():\n            self.app.logger.error(f\"Error preparing shared file response for {share_id}: {result.info.help_text}\")\n            return Result.html(data=f\"Failed to prepare shared file for download. {result.info.help_text} {result.result.data_info}\")\n        return result\n    except ValueError as e:  # From get_blob_storage if owner_uid is invalid for some reason\n        self.app.logger.error(f\"Error getting owner's storage for shared file {share_id} (owner {owner_uid}): {e}\",\n                              exc_info=True)\n        return Result.html(data=\"Could not access owner's storage for shared file.\")\n    except Exception as e:\n        self.app.logger.error(\n            f\"Error accessing shared file {share_id} (owner {owner_uid}, path {file_path_in_owner_storage}): {e}\",\n            exc_info=True)\n        return Result.html(data=\"Could not retrieve shared file.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FileWidget.get_main_ui","title":"<code>get_main_ui(self)</code>  <code>async</code>","text":"<p>Serves the main HTML UI for the FileWidget.</p> Source code in <code>toolboxv2/mods/FileWidget.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, name=\"ui\", api_methods=['GET'])\nasync def get_main_ui(self) -&gt; Result:\n    \"\"\"Serves the main HTML UI for the FileWidget.\"\"\"\n    html_content = get_template_content()\n    return Result.html(data=html_content)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FileWidget.handle_upload","title":"<code>handle_upload(self, request, form_data=None)</code>  <code>async</code>","text":"<p>Handles file uploads. Expects chunked data via form_data kwarg from Rust server. 'form_data' structure (from Rust's parsing of multipart) after client sends FormData with fields: 'file' (the blob), 'fileName', 'chunkIndex', 'totalChunks'.</p> <p>Expected <code>form_data</code> in this Python function: {     \"file\": {  // This 'file' key is the NAME of the form field that held the file blob         \"filename\": \"original_file_name_for_this_chunk.txt\", // from Content-Disposition of the 'file' field part         \"content_type\": \"mime/type_of_chunk\",         \"content_base64\": \"BASE64_ENCODED_CHUNK_CONTENT\"     },     \"fileName\": \"overall_final_filename.txt\", // From a separate form field named 'fileName'     \"chunkIndex\": \"0\",                        // From a separate form field named 'chunkIndex'     \"totalChunks\": \"5\"                        // From a separate form field named 'totalChunks' }</p> Source code in <code>toolboxv2/mods/FileWidget.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, name=\"upload\", api_methods=['POST'], request_as_kwarg=True)\nasync def handle_upload(self, request: RequestData, form_data: Optional[Dict[str, Any]] = None) -&gt; Result:\n    \"\"\"\n    Handles file uploads. Expects chunked data via form_data kwarg from Rust server.\n    'form_data' structure (from Rust's parsing of multipart) after client sends FormData with fields:\n    'file' (the blob), 'fileName', 'chunkIndex', 'totalChunks'.\n\n    Expected `form_data` in this Python function:\n    {\n        \"file\": {  // This 'file' key is the NAME of the form field that held the file blob\n            \"filename\": \"original_file_name_for_this_chunk.txt\", // from Content-Disposition of the 'file' field part\n            \"content_type\": \"mime/type_of_chunk\",\n            \"content_base64\": \"BASE64_ENCODED_CHUNK_CONTENT\"\n        },\n        \"fileName\": \"overall_final_filename.txt\", // From a separate form field named 'fileName'\n        \"chunkIndex\": \"0\",                        // From a separate form field named 'chunkIndex'\n        \"totalChunks\": \"5\"                        // From a separate form field named 'totalChunks'\n    }\n    \"\"\"\n    self.app.logger.debug(\n        f\"FileWidget: handle_upload called. Received form_data keys: {list(form_data.keys()) if form_data else 'None'}\"\n    )\n    self.app.logger.debug(f\"FileWidget: handle_upload called. Received form_data: {request.to_dict()}\")\n    # self.app.logger.debug(f\"Full form_data: {form_data}\") # For deeper debugging if needed\n\n    if not form_data:\n        return Result.default_user_error(info=\"No form data received for upload.\", exec_code=400)\n\n    try:\n        storage = await self.get_blob_storage(request)\n\n        # Extract data from form_data (populated by Rust server from multipart)\n        file_field_data = form_data.get('file')  # This is the dict from UploadedFile struct\n        # The 'file_field_data.get('filename')' is the name of the chunk part,\n        # which the JS client sets to be the same as the original file's name.\n        # This is fine for FileUploadHandler.save_file's chunk_info.filename if total_chunks &gt; 1,\n        # as it will be used to create temporary part files like \"original_file_name.txt.part0\".\n\n        overall_filename_from_form = form_data.get('fileName') # This is the target filename for the assembled file.\n        chunk_index_str = form_data.get('chunkIndex')\n        total_chunks_str = form_data.get('totalChunks')\n\n        if not all([\n            file_field_data, isinstance(file_field_data, dict),\n            overall_filename_from_form,\n            chunk_index_str is not None, # Check for presence, not just truthiness (0 is valid)\n            total_chunks_str is not None # Check for presence\n        ]):\n            missing = []\n            if not file_field_data or not isinstance(file_field_data, dict): missing.append(\"'file' object field\")\n            if not overall_filename_from_form: missing.append(\"'fileName' field\")\n            if chunk_index_str is None: missing.append(\"'chunkIndex' field\")\n            if total_chunks_str is None: missing.append(\"'totalChunks' field\")\n\n            self.app.logger.error(\n                f\"Missing critical form data fields for upload: {missing}. Received form_data: {form_data}\")\n            return Result.default_user_error(info=f\"Incomplete upload data. Missing: {', '.join(missing)}\",\n                                             exec_code=400)\n\n        content_base64 = file_field_data.get('content_base64')\n        if not content_base64:\n            return Result.default_user_error(info=\"File content (base64) not found in 'file' field data.\",\n                                             exec_code=400)\n\n        try:\n            content_bytes = base64.b64decode(content_base64)\n        except base64.binascii.Error as b64_error:\n            self.app.logger.error(f\"Base64 decoding failed for upload: {b64_error}\")\n            return Result.default_user_error(info=\"Invalid file content encoding.\", exec_code=400)\n\n        try:\n            chunk_index = int(chunk_index_str)\n            total_chunks = int(total_chunks_str)\n        except ValueError:\n            return Result.default_user_error(info=\"Invalid chunk index or total chunks value. Must be integers.\", exec_code=400)\n\n        # Use the 'overall_filename_from_form' for the ChunkInfo.filename,\n        # as this is the intended final name in blob storage.\n        # FileUploadHandler will use Path(this_name).name to ensure it's just a filename.\n        chunk_info_to_save = ChunkInfo(\n            filename=overall_filename_from_form, # THIS IS THE KEY CHANGE FOR CONSISTENCY\n            chunk_index=chunk_index,\n            total_chunks=total_chunks,\n            content=content_bytes\n        )\n\n        self.app.logger.info(\n            f\"Processing chunk {chunk_index + 1}/{total_chunks} for final file '{overall_filename_from_form}'. \" # Log the intended final name\n            f\"Size: {len(content_bytes)} bytes.\"\n        )\n\n        saved_blob_path = self.upload_handler.save_file(chunk_info_to_save, storage) # saved_blob_path will be Path(overall_filename_from_form).name\n\n        msg = f\"Chunk {chunk_index + 1}/{total_chunks} for '{saved_blob_path}' saved.\"\n        if chunk_info_to_save.chunk_index == chunk_info_to_save.total_chunks - 1:\n            # Check if fully assembled\n            # The 'safe_base_filename' in FileUploadHandler is derived from ChunkInfo.filename,\n            # which we've now set to 'overall_filename_from_form'.\n            # So, this check should work correctly.\n            safe_base_filename_for_check = \"\".join(\n                c if c.isalnum() or c in ('.', '_', '-') else '_' for c in Path(overall_filename_from_form).name)\n\n            # A slight delay might be needed if file system operations are not instantly consistent across threads/processes\n            # For now, assume direct check is okay.\n            # await asyncio.sleep(0.1) # Optional small delay if race conditions are suspected with file system\n\n            if self.upload_handler._all_chunks_received(safe_base_filename_for_check, total_chunks):\n                msg = f\"File '{saved_blob_path}' upload complete and assembled.\"\n                self.app.logger.info(msg)\n            else:\n                msg = f\"Final chunk for '{saved_blob_path}' saved, but assembly check failed or is pending.\"\n                self.app.logger.warning(msg + f\" (Could not verify all chunks for '{safe_base_filename_for_check}' immediately after final one)\")\n\n\n        return Result.ok(data={\"message\": msg, \"path\": saved_blob_path}) # Return the blob-relative path\n\n    except ValueError as e:\n        self.app.logger.error(f\"Upload processing error: {e}\", exc_info=True)\n        return Result.default_user_error(info=f\"Upload error: {str(e)}\",\n                                         exec_code=400 if \"authentication\" in str(e).lower() else 400)\n    except Exception as e:\n        self.app.logger.error(f\"Unexpected error during file upload: {e}\", exc_info=True)\n        return Result.default_internal_error(info=\"An unexpected error occurred during upload.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.POA","title":"<code>POA</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.POA.module","title":"<code>module</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.POA.module.ActionManagerEnhanced","title":"<code>ActionManagerEnhanced</code>","text":"Source code in <code>toolboxv2/mods/POA/module.py</code> <pre><code>class ActionManagerEnhanced:\n    DB_ITEMS_PREFIX = \"donext_items\"\n    DB_HISTORY_PREFIX = \"donext_history\"\n    DB_CURRENT_ITEM_PREFIX = \"donext_current_item\"\n    DB_UNDO_LOG_PREFIX = \"donext_undo_log\"\n    DB_SETTINGS_PREFIX = \"donext_settings\"  # Added for user settings\n\n    def __init__(self, app: App, user_id: str):\n        self.app = app\n        self.user_id = user_id\n        self.db = app.get_mod(\"DB\")\n        self.isaa = app.get_mod(\"isaa\")\n\n        self.settings: UserSettings = UserSettings(user_id=user_id)  # Initialize with defaults\n        self.items: List[ActionItem] = []\n        self.history: List[HistoryEntry] = []\n        self.current_item: Optional[ActionItem] = None\n        self.undo_log: List[UndoLogEntry] = []\n\n        self._load_settings()  # Load settings first as they might affect item loading\n        self._load_data()\n\n    def _get_db_key(self, prefix: str) -&gt; str:\n        return f\"{prefix}_{self.user_id}\"\n\n    def get_user_timezone(self) -&gt; pytz.BaseTzInfo:\n        try:\n            return pytz.timezone(self.settings.timezone)\n        except pytz.UnknownTimeZoneError:\n            return pytz.utc\n\n    def _load_settings(self):\n        settings_key = self._get_db_key(self.DB_SETTINGS_PREFIX)\n        try:\n            settings_data = self.db.get(settings_key)\n            if settings_data.is_data() and settings_data.get():\n                loaded_settings = json.loads(settings_data.get()[0]) if isinstance(settings_data.get(),\n                                                                                   list) else json.loads(\n                    settings_data.get())\n                self.settings = UserSettings.model_validate_json_safe(loaded_settings)\n            else:  # Save default settings if not found\n                self._save_settings()\n        except Exception as e:\n            self.app.logger.error(f\"Error loading settings for user {self.user_id}: {e}. Using defaults.\")\n            self.settings = UserSettings(user_id=self.user_id)  # Fallback to defaults\n            self._save_settings()  # Attempt to save defaults\n\n    def _save_settings(self):\n        try:\n            self.db.set(self._get_db_key(self.DB_SETTINGS_PREFIX), json.dumps(self.settings.model_dump_json_safe()))\n        except Exception as e:\n            self.app.logger.error(f\"Error saving settings for user {self.user_id}: {e}\")\n\n    def update_user_settings(self, settings_data: Dict[str, Any]) -&gt; UserSettings:\n        # Ensure user_id is not changed by malicious input\n        current_user_id = self.settings.user_id\n        updated_settings = UserSettings.model_validate(\n            {**self.settings.model_dump(), **settings_data, \"user_id\": current_user_id})\n        self.settings = updated_settings\n        self._save_settings()\n        # Potentially re-process items if timezone change affects interpretations, though this is complex.\n        # For now, new items will use the new timezone. Existing UTC times remain.\n        self.app.logger.info(f\"User {self.user_id} settings updated: Timezone {self.settings.timezone}\")\n        return self.settings\n\n    def _load_data(self):\n        items_key = self._get_db_key(self.DB_ITEMS_PREFIX)\n        history_key = self._get_db_key(self.DB_HISTORY_PREFIX)\n        current_item_key = self._get_db_key(self.DB_CURRENT_ITEM_PREFIX)\n        undo_log_key = self._get_db_key(self.DB_UNDO_LOG_PREFIX)\n        user_tz_str = self.settings.timezone  # For model_validate_json_safe context\n\n        try:\n            items_data = self.db.get(items_key)\n            if items_data.is_data() and items_data.get():\n                loaded_items_raw = json.loads(items_data.get()[0]) if isinstance(items_data.get(),\n                                                                                 list) else json.loads(items_data.get())\n                self.items = [ActionItem.model_validate_json_safe(item_dict, user_timezone_str=user_tz_str) for\n                              item_dict in loaded_items_raw]\n\n            history_data = self.db.get(history_key)\n            if history_data.is_data() and history_data.get():\n                loaded_history_raw = json.loads(history_data.get()[0]) if isinstance(history_data.get(),\n                                                                                     list) else json.loads(\n                    history_data.get())\n                self.history = [HistoryEntry.model_validate_json_safe(entry_dict) for entry_dict in loaded_history_raw]\n\n            current_item_data = self.db.get(current_item_key)\n            if current_item_data.is_data() and current_item_data.get():\n                current_item_dict = json.loads(current_item_data.get()[0]) if isinstance(current_item_data.get(),\n                                                                                         list) else json.loads(\n                    current_item_data.get())\n                if current_item_dict:\n                    self.current_item = ActionItem.model_validate_json_safe(current_item_dict,\n                                                                            user_timezone_str=user_tz_str)\n\n            undo_log_data = self.db.get(undo_log_key)\n            if undo_log_data.is_data() and undo_log_data.get():\n                loaded_undo_raw = json.loads(undo_log_data.get()[0]) if isinstance(undo_log_data.get(),\n                                                                                   list) else json.loads(\n                    undo_log_data.get())\n                self.undo_log = [UndoLogEntry.model_validate_json_safe(entry_dict) for entry_dict in loaded_undo_raw]\n\n        except Exception as e:\n            self.app.logger.error(f\"Error loading data for user {self.user_id}: {e}\")\n            self.items, self.history, self.current_item, self.undo_log = [], [], None, []\n        self._recalculate_next_due_for_all()\n\n    def _save_data(self):\n        try:\n            self.db.set(self._get_db_key(self.DB_ITEMS_PREFIX),\n                        json.dumps([item.model_dump_json_safe() for item in self.items]))\n            self.db.set(self._get_db_key(self.DB_HISTORY_PREFIX),\n                        json.dumps([entry.model_dump_json_safe() for entry in self.history]))\n            self.db.set(self._get_db_key(self.DB_CURRENT_ITEM_PREFIX),\n                        json.dumps(self.current_item.model_dump_json_safe() if self.current_item else None))\n            self.db.set(self._get_db_key(self.DB_UNDO_LOG_PREFIX),\n                        json.dumps([entry.model_dump_json_safe() for entry in self.undo_log]))\n        except Exception as e:\n            self.app.logger.error(f\"Error saving data for user {self.user_id}: {e}\")\n\n    def _add_history_entry(self, item: ActionItem, status_override: Optional[ActionStatus] = None,\n                           notes: Optional[str] = None):\n        entry = HistoryEntry(\n            item_id=item.id, item_title=item.title, item_type=item.item_type,\n            status_changed_to=status_override or item.status,\n            parent_id=item.parent_id, notes=notes\n        )\n        self.history.append(entry)\n\n    def _datetime_to_user_tz(self, dt_utc: Optional[datetime]) -&gt; Optional[datetime]:\n        if not dt_utc: return None\n        if dt_utc.tzinfo is None: dt_utc = pytz.utc.localize(dt_utc)  # Should already be UTC\n        return dt_utc.astimezone(self.get_user_timezone())\n\n    def _datetime_from_user_input_str(self, dt_str: Optional[str]) -&gt; Optional[datetime]:\n        if not dt_str: return None\n        try:\n            dt = isoparse(dt_str)\n            if dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None:  # Naive\n                return self.get_user_timezone().localize(dt).astimezone(pytz.utc)\n            return dt.astimezone(pytz.utc)  # Aware, convert to UTC\n        except ValueError:\n            self.app.logger.warning(f\"Could not parse datetime string: {dt_str}\")\n            return None\n\n    def _recalculate_next_due(self, item: ActionItem):\n        now_utc = datetime.now(pytz.utc)\n        user_tz = self.get_user_timezone()\n\n        if item.status == ActionStatus.COMPLETED and item.item_type == ItemType.TASK:\n            if item.frequency and item.frequency != Frequency.ONE_TIME:\n                base_time_utc = item.last_completed or now_utc  # last_completed is already UTC\n\n                # If item had a fixed_time, align next_due to that time of day in user's timezone\n                if item.fixed_time:\n                    original_fixed_time_user_tz = item.fixed_time.astimezone(user_tz)\n                    # Start from last_completed (or now if missing) in user's timezone for calculation\n                    base_time_user_tz = base_time_utc.astimezone(user_tz)\n\n                    # Ensure base_time_user_tz is at least original_fixed_time_user_tz for alignment\n                    # but calculations should project from last completion.\n                    # For example, if daily task due 9am was completed at 11am, next one is tomorrow 9am.\n                    # If completed at 8am, next one is today 9am (if fixed_time was today 9am) or tomorrow 9am.\n\n                    # Let's use last_completed as the primary anchor for when the *next* cycle starts.\n                    # The original fixed_time's time component is used for the *time of day* of the next due.\n\n                    current_anchor_user_tz = base_time_user_tz\n\n                    # Calculate next occurrence based on frequency\n                    if item.frequency == Frequency.DAILY:\n                        next_due_user_tz_date = (current_anchor_user_tz + timedelta(days=1)).date()\n                    elif item.frequency == Frequency.WEEKLY:\n                        next_due_user_tz_date = (current_anchor_user_tz + timedelta(weeks=1)).date()\n                    elif item.frequency == Frequency.MONTHLY:  # Simplified\n                        next_due_user_tz_date = (current_anchor_user_tz + timedelta(days=30)).date()\n                    elif item.frequency == Frequency.ANNUALLY:\n                        next_due_user_tz_date = (current_anchor_user_tz + timedelta(days=365)).date()\n                    else:  # Should not happen for recurring\n                        item.next_due = None\n                        return\n\n                    # Combine with original time of day\n                    next_due_user_tz = datetime.combine(next_due_user_tz_date, original_fixed_time_user_tz.time(),\n                                                        tzinfo=user_tz)\n                    item.next_due = next_due_user_tz.astimezone(pytz.utc)\n\n                else:  # No original fixed_time, so recur based on current time of completion\n                    if item.frequency == Frequency.DAILY:\n                        item.next_due = base_time_utc + timedelta(days=1)\n                    elif item.frequency == Frequency.WEEKLY:\n                        item.next_due = base_time_utc + timedelta(weeks=1)\n                    elif item.frequency == Frequency.MONTHLY:\n                        item.next_due = base_time_utc + timedelta(days=30)\n                    elif item.frequency == Frequency.ANNUALLY:\n                        item.next_due = base_time_utc + timedelta(days=365)\n\n                # Advance until future if needed (e.g., completing an overdue recurring task)\n                # This loop must operate on user's local time perception of \"next day\"\n                while item.next_due and item.next_due &lt; now_utc:\n                    next_due_user = item.next_due.astimezone(user_tz)\n                    original_time_comp = next_due_user.time()  # Preserve time of day\n\n                    if item.frequency == Frequency.DAILY:\n                        next_due_user_adv = next_due_user + timedelta(days=1)\n                    elif item.frequency == Frequency.WEEKLY:\n                        next_due_user_adv = next_due_user + timedelta(weeks=1)\n                    # For monthly/annually, simple timedelta might shift day of month. Using replace for date part.\n                    elif item.frequency == Frequency.MONTHLY:\n                        # This simplified logic might need dateutil.relativedelta for accuracy\n                        year, month = (next_due_user.year, next_due_user.month + 1) if next_due_user.month &lt; 12 else (\n                            next_due_user.year + 1, 1)\n                        try:\n                            next_due_user_adv = next_due_user.replace(year=year, month=month)\n                        except ValueError:  # Handle e.g. trying to set Feb 30\n                            import calendar\n                            last_day = calendar.monthrange(year, month)[1]\n                            next_due_user_adv = next_due_user.replace(year=year, month=month, day=last_day)\n\n                    elif item.frequency == Frequency.ANNUALLY:\n                        try:\n                            next_due_user_adv = next_due_user.replace(year=next_due_user.year + 1)\n                        except ValueError:  # Handle leap day if original was Feb 29\n                            next_due_user_adv = next_due_user.replace(year=next_due_user.year + 1,\n                                                                      day=28)  # Or March 1st\n                    else:\n                        break\n\n                    item.next_due = user_tz.localize(\n                        datetime.combine(next_due_user_adv.date(), original_time_comp)).astimezone(pytz.utc)\n\n                item.status = ActionStatus.NOT_STARTED  # Reset for next occurrence\n            else:  # One-time task\n                item.next_due = None\n        elif item.status == ActionStatus.NOT_STARTED and item.fixed_time and not item.next_due:\n            item.next_due = item.fixed_time  # fixed_time is already UTC\n\n        # If task is not completed, not started, and has a next_due in the past, but also a fixed_time in the future\n        # (e.g. recurring task whose current instance was missed, but fixed_time points to a specific time for all instances)\n        # ensure next_due is not before fixed_time if fixed_time is relevant for setting.\n        # This logic is complex. Current setup: fixed_time is the \"template\", next_due is the \"instance\".\n\n    def _recalculate_next_due_for_all(self):\n        for item in self.items:\n            self._recalculate_next_due(item)\n\n    def add_item(self, item_data: Dict[str, Any], by_ai: bool = False, imported: bool = False) -&gt; ActionItem:\n        item_data['_user_timezone_str'] = self.settings.timezone  # For validation context\n        item = ActionItem.model_validate(\n            item_data)  # Pydantic handles string-&gt;datetime, then model_validator converts to UTC\n        item.created_by_ai = by_ai\n        item.updated_at = datetime.now(pytz.utc)  # Ensure update\n\n        # Initial next_due for new items if not already set by iCal import logic\n        if not item.next_due and item.fixed_time and item.status == ActionStatus.NOT_STARTED:\n            item.next_due = item.fixed_time\n\n        self.items.append(item)\n        self._add_history_entry(item, status_override=ActionStatus.NOT_STARTED,\n                                notes=\"Item created\" + (\" by AI\" if by_ai else \"\") + (\n                                    \" via import\" if imported else \"\"))\n        if by_ai:\n            self._log_ai_action(\"ai_create_item\", [item.id])\n\n        self._save_data()\n        return item\n\n    def get_item_by_id(self, item_id: str) -&gt; Optional[ActionItem]:\n        return next((item for item in self.items if item.id == item_id), None)\n\n    def update_item(self, item_id: str, update_data: Dict[str, Any], by_ai: bool = False) -&gt; Optional[ActionItem]:\n        item = self.get_item_by_id(item_id)\n        if not item: return None\n\n        previous_data_json = item.model_dump_json() if by_ai else None\n\n        # Pass user timezone for validation context if datetime strings are present\n        update_data_with_tz_context = {**update_data, '_user_timezone_str': self.settings.timezone}\n\n        updated_item_dict = item.model_dump()\n        updated_item_dict.update(update_data_with_tz_context)\n\n        try:\n            # Re-validate the whole model to ensure consistency and proper conversions\n            new_item_state = ActionItem.model_validate(updated_item_dict)\n            # Preserve original ID and created_at, apply new state\n            new_item_state.id = item.id\n            new_item_state.created_at = item.created_at\n            self.items[self.items.index(item)] = new_item_state\n            item = new_item_state\n        except Exception as e:\n            self.app.logger.error(f\"Error validating updated item data: {e}. Update aborted for item {item_id}.\")\n            return None  # Or raise error\n\n        item.updated_at = datetime.now(pytz.utc)\n        item.created_by_ai = by_ai\n\n        self._recalculate_next_due(item)\n        self._add_history_entry(item, notes=\"Item updated\" + (\" by AI\" if by_ai else \"\"))\n\n        if by_ai:\n            self._log_ai_action(\"ai_modify_item\", [item.id],\n                                {item.id: previous_data_json} if previous_data_json else None)\n\n        self._save_data()\n        return item\n\n    def remove_item(self, item_id: str, record_history: bool = True) -&gt; bool:\n        item = self.get_item_by_id(item_id)\n        if not item: return False\n\n        children_ids = [child.id for child in self.items if child.parent_id == item_id]\n        for child_id in children_ids:\n            self.remove_item(child_id, record_history=record_history)\n\n        self.items = [i for i in self.items if i.id != item_id]\n        if self.current_item and self.current_item.id == item_id:\n            self.current_item = None\n\n        if record_history:\n            self._add_history_entry(item, status_override=ActionStatus.CANCELLED, notes=\"Item removed\")\n        self._save_data()\n        return True\n\n    def set_current_item(self, item_id: str) -&gt; Optional[ActionItem]:\n        item = self.get_item_by_id(item_id)\n        if not item: return None\n        if item.status == ActionStatus.COMPLETED and item.item_type == ItemType.TASK and item.frequency == Frequency.ONE_TIME:\n            return None\n\n        self.current_item = item\n        if item.status == ActionStatus.NOT_STARTED:\n            item.status = ActionStatus.IN_PROGRESS\n            item.updated_at = datetime.now(pytz.utc)\n            self._add_history_entry(item, notes=\"Set as current, status to In Progress\")\n        else:\n            self._add_history_entry(item, notes=\"Set as current\")\n        self._save_data()\n        return item\n\n    def complete_current_item(self) -&gt; Optional[ActionItem]:\n        if not self.current_item: return None\n\n        item_to_complete = self.current_item\n        item_to_complete.status = ActionStatus.COMPLETED\n        item_to_complete.last_completed = datetime.now(pytz.utc)\n        item_to_complete.updated_at = datetime.now(pytz.utc)\n\n        self._recalculate_next_due(item_to_complete)\n        self._add_history_entry(item_to_complete, status_override=ActionStatus.COMPLETED, notes=\"Marked as completed\")\n\n        self.current_item = None  # Clear current item after completion\n        self._save_data()\n        return item_to_complete\n\n    def get_suggestions(self, count: int = 2) -&gt; List[ActionItem]:\n        # Prioritize AI suggestions if ISAA is available\n        if self.isaa:\n            active_items_for_ai = []\n            for item in self.items:\n                if item.status != ActionStatus.COMPLETED and item.status != ActionStatus.CANCELLED:\n                    # Convert datetimes to user's local timezone string for AI context\n                    item_dump = item.model_dump_json_safe()  # This is already UTC ISO\n                    # Optionally, convert to user's timezone string if AI is better with local times\n                    # For now, UTC ISO is fine.\n                    active_items_for_ai.append(item_dump)\n\n            MAX_ITEMS_FOR_CONTEXT = 20\n            if len(active_items_for_ai) &gt; MAX_ITEMS_FOR_CONTEXT:\n                active_items_for_ai.sort(\n                    key=lambda x: (x.get('priority', 3), x.get('next_due') or '9999-12-31T23:59:59Z'))\n                active_items_for_ai = active_items_for_ai[:MAX_ITEMS_FOR_CONTEXT]\n\n            now_user_tz_str = datetime.now(self.get_user_timezone()).isoformat()\n\n            prompt = (\n                f\"User's current time: {now_user_tz_str} (Timezone: {self.settings.timezone}). \"\n                f\"Active items (tasks/notes) are provided below (datetimes are in UTC ISO format). \"\n                f\"Suggest the top {count} item IDs to focus on. Consider priority, due dates (next_due), \"\n                f\"and if a current item is set (current_item_id), its sub-items might be relevant. \"\n                f\"Tasks are generally more actionable. Focus on 'not_started' or 'in_progress'.\\n\\n\"\n                f\"Active Items (JSON):\\n{json.dumps(active_items_for_ai, indent=2)}\\n\\n\"\n                f\"Current Item ID: {self.current_item.id if self.current_item else 'None'}\\n\\n\"\n                f\"Return JSON: {{ \\\"suggested_item_ids\\\": [\\\"id1\\\", \\\"id2\\\"] }}.\"\n            )\n\n            class SuggestedIds(BaseModel):\n                suggested_item_ids: List[str]\n\n            try:\n                structured_response = asyncio.run(\n                    self.isaa.format_class(SuggestedIds, prompt, agent_name=\"TaskCompletion\"))\n                if structured_response and isinstance(structured_response, dict):\n                    suggested_ids_model = SuggestedIds(**structured_response)\n                    ai_suggestions = [self.get_item_by_id(id_str) for id_str in suggested_ids_model.suggested_item_ids\n                                      if self.get_item_by_id(id_str)]\n                    if ai_suggestions: return ai_suggestions[:count]\n            except Exception as e:\n                self.app.logger.error(f\"Error getting AI suggestions: {e}\")\n\n        # Fallback to basic suggestions\n        return self._get_basic_suggestions(count)\n\n    def _get_basic_suggestions(self, count: int = 2) -&gt; List[ActionItem]:\n        now_utc = datetime.now(pytz.utc)\n        available_items = [\n            item for item in self.items\n            if item.status in [ActionStatus.NOT_STARTED, ActionStatus.IN_PROGRESS]\n        ]\n\n        if self.current_item:\n            sub_items = [item for item in available_items if item.parent_id == self.current_item.id]\n            # If current item has actionable sub-items, prioritize them\n            if any(s.next_due and s.next_due &lt; (now_utc + timedelta(hours=2)) for s in sub_items) or \\\n                any(s.priority &lt;= 2 for s in sub_items):  # Urgent sub-items (due soon or high priority)\n                available_items = sub_items  # Focus on sub-items\n            # If no urgent sub-items, consider other items too, but maybe give slight preference to other sub-items.\n            # For simplicity now, if current_item is set, and it has sub-items, suggestions come from sub-items.\n            # If no sub-items, or current_item is not set, consider all available_items.\n            elif sub_items:  # Has sub-items, but none are \"urgent\" by above criteria\n                available_items = sub_items\n            # If current_item has no sub_items, then general pool is used.\n\n        def sort_key(item: ActionItem):\n            # Sort by: 1. Due Date (earlier is better, None is last) 2. Priority (lower num is higher)\n            due_date_utc = item.next_due if item.next_due else datetime.max.replace(tzinfo=pytz.utc)\n            return (due_date_utc, item.priority)\n\n        available_items.sort(key=sort_key)\n        return available_items[:count]\n\n    def get_history(self, limit: int = 50) -&gt; List[HistoryEntry]:\n        return sorted(self.history, key=lambda x: x.timestamp, reverse=True)[:limit]\n\n    def get_all_items_hierarchy(self) -&gt; Dict[str, List[Dict[str, Any]]]:\n        # This method remains largely the same, just ensure model_dump_json_safe is used.\n        # Datetimes will be ISO UTC strings. Client JS needs to handle display in user's local time.\n        hierarchy = {\"root\": []}\n        item_map = {item.id: item.model_dump_json_safe() for item in self.items}  # Uses UTC ISO dates\n\n        # This part seems fine, it builds hierarchy based on parent_id\n        processed_ids = set()\n        root_items_temp = []\n\n        for item_id, item_dict in item_map.items():\n            parent_id = item_dict.get(\"parent_id\")\n            if parent_id and parent_id in item_map:\n                if \"children\" not in item_map[parent_id]:\n                    item_map[parent_id][\"children\"] = []\n                item_map[parent_id][\"children\"].append(item_dict)\n            else:\n                root_items_temp.append(item_dict)\n        hierarchy[\"root\"] = root_items_temp\n\n        def sort_children_recursive(node_list):\n            for node_dict in node_list:\n                if \"children\" in node_dict:\n                    # Sort children by priority, then creation date\n                    node_dict[\"children\"].sort(key=lambda x: (x.get('priority', 3), isoparse(x.get('created_at'))))\n                    sort_children_recursive(node_dict[\"children\"])\n\n        # Sort root items\n        hierarchy[\"root\"].sort(key=lambda x: (x.get('priority', 3), isoparse(x.get('created_at'))))\n        sort_children_recursive(hierarchy[\"root\"])\n        return hierarchy\n\n    # --- AI Specific Methods ---\n    async def ai_create_item_from_text(self, text: str) -&gt; Optional[ActionItem]:\n        if not self.isaa:\n            self.app.logger.warning(\"ISAA module not available for AI item creation.\")\n            return None\n\n        class ParsedItemFromText(BaseModel):\n            item_type: Literal[\"task\", \"note\"] = \"task\"\n            title: str\n            description: Optional[str] = None\n            priority: Optional[int] = Field(default=3, ge=1, le=5)\n            due_date_str: Optional[str] = None  # e.g., \"tomorrow\", \"next monday at 5pm\", \"2024-12-25 17:00\"\n            frequency_str: Optional[str] = Field(default=\"one_time\",\n                                                 description=\"e.g. 'daily', 'weekly', 'one_time', 'every friday'\")\n\n        user_tz = self.get_user_timezone()\n        current_time_user_tz_str = datetime.now(user_tz).strftime('%Y-%m-%d %H:%M:%S %Z%z')\n        prompt = (\n            f\"User's current time is {current_time_user_tz_str}. Parse the input into a structured item. \"\n            f\"For due_date_str, interpret relative dates/times based on this current time and output \"\n            f\"a specific date string like 'YYYY-MM-DD HH:MM:SS'. If time is omitted, assume a default like 9 AM. \"\n            f\"If date is omitted but time is given (e.g. 'at 5pm'), assume today if 5pm is future, else tomorrow. \"\n            f\"User input: \\\"{text}\\\"\\n\\n\"\n            f\"Format as JSON for ParsedItemFromText.\"\n        )\n        try:\n            raw_response = await self.isaa.mini_task_completion(prompt, agent_name=\"TaskCompletion\")\n            if not raw_response: self.app.logger.error(\"AI parsing returned empty.\"); return None\n\n            json_str = raw_response\n            if \"```json\" in json_str: json_str = json_str.split(\"```json\")[1].split(\"```\")[0].strip()\n            parsed_dict = json.loads(json_str)\n            parsed_data_model = ParsedItemFromText(**parsed_dict)\n\n            item_constructor_data = {\n                \"item_type\": ItemType(parsed_data_model.item_type),\n                \"title\": parsed_data_model.title,\n                \"description\": parsed_data_model.description,\n                \"priority\": parsed_data_model.priority or 3,\n            }\n\n            if parsed_data_model.due_date_str:\n                # ISAA is prompted to return YYYY-MM-DD HH:MM:SS.\n                # This string is assumed to be in the user's local timezone.\n                # The ActionItem model_validator will convert this to UTC.\n                item_constructor_data[\"fixed_time\"] = parsed_data_model.due_date_str  # Pass as string\n\n            # Frequency parsing (simplified)\n            if parsed_data_model.frequency_str:\n                freq_str_lower = parsed_data_model.frequency_str.lower()\n                if \"daily\" in freq_str_lower:\n                    item_constructor_data[\"frequency\"] = Frequency.DAILY\n                elif \"weekly\" in freq_str_lower:\n                    item_constructor_data[\"frequency\"] = Frequency.WEEKLY\n                elif \"monthly\" in freq_str_lower:\n                    item_constructor_data[\"frequency\"] = Frequency.MONTHLY\n                elif \"annually\" in freq_str_lower or \"yearly\" in freq_str_lower:\n                    item_constructor_data[\"frequency\"] = Frequency.ANNUALLY\n                else:\n                    item_constructor_data[\"frequency\"] = Frequency.ONE_TIME\n\n            return self.add_item(item_constructor_data, by_ai=True)\n        except Exception as e:\n            self.app.logger.error(\n                f\"Error creating item with AI: {e}. Raw: {raw_response if 'raw_response' in locals() else 'N/A'}\")\n            return None\n\n    def _log_ai_action(self, action_type: Literal[\"ai_create_item\", \"ai_modify_item\", \"ical_import\"],\n                       item_ids: List[str], previous_data_map: Optional[Dict[str, str]] = None):\n        entry = UndoLogEntry(action_type=action_type, item_ids=item_ids, previous_data_json_map=previous_data_map)\n        self.undo_log.append(entry)\n        if len(self.undo_log) &gt; 20: self.undo_log = self.undo_log[-20:]\n        # _save_data called by caller\n\n    async def undo_last_ai_action(self) -&gt; bool:  # Also handles iCal import undo\n        if not self.undo_log: return False\n        last_action = self.undo_log.pop()\n        action_undone_count = 0\n\n        if last_action.action_type in [\"ai_create_item\", \"ical_import\"]:\n            for item_id in last_action.item_ids:\n                if self.remove_item(item_id, record_history=False):  # Don't double-log removal for undo\n                    action_undone_count += 1\n        elif last_action.action_type == \"ai_modify_item\":\n            if last_action.previous_data_json_map:\n                for item_id, prev_data_json in last_action.previous_data_json_map.items():\n                    try:\n                        prev_data = ActionItem.model_validate_json_safe(json.loads(prev_data_json),\n                                                                        user_timezone_str=self.settings.timezone)\n                        # Replace item\n                        found = False\n                        for i, item_in_list in enumerate(self.items):\n                            if item_in_list.id == item_id:\n                                self.items[i] = prev_data\n                                if self.current_item and self.current_item.id == item_id:\n                                    self.current_item = prev_data\n                                found = True\n                                break\n                        if found:\n                            action_undone_count += 1\n                        else:\n                            self.app.logger.warning(f\"Could not find item {item_id} to restore during AI undo.\")\n                    except Exception as e:\n                        self.app.logger.error(f\"Error restoring item {item_id} during undo: {e}\")\n            else:  # Should not happen for modify\n                self.app.logger.warning(\n                    f\"Undo for AI modify action on item(s) {last_action.item_ids} had no previous_data_json_map.\")\n\n        if action_undone_count &gt; 0:\n            # Create a generic history entry for the undo action\n            generic_undo_item_title = f\"Related to {len(last_action.item_ids)} item(s)\"\n            if len(last_action.item_ids) == 1:\n                item_for_title = self.get_item_by_id(last_action.item_ids[0])  # Might be None if it was a create undo\n                generic_undo_item_title = item_for_title.title if item_for_title else \"N/A (Undone Action)\"\n\n            self.history.append(HistoryEntry(\n                item_id=last_action.item_ids[0],  # Representative item\n                item_title=generic_undo_item_title,\n                item_type=ItemType.TASK,  # Generic\n                status_changed_to=ActionStatus.CANCELLED,  # Generic status for undo\n                notes=f\"Undid action: {last_action.action_type} for {len(last_action.item_ids)} item(s).\"\n            ))\n            self._save_data()\n            return True\n\n        # If nothing was undone, put action back to log\n        self.undo_log.append(last_action)\n        return False\n\n    # --- iCalendar Methods ---\n    def _parse_ical_dt(self, dt_ical: Union[vDatetime, vDate], user_tz: pytz.BaseTzInfo) -&gt; Optional[datetime]:\n        \"\"\"Converts icalendar vDatetime or vDate to UTC datetime.\"\"\"\n        if not dt_ical: return None\n        dt_val = dt_ical.dt\n\n        if isinstance(dt_val, datetime):\n            if dt_val.tzinfo is None:  # Naive datetime, assume user's local timezone as per iCal spec for floating\n                return user_tz.localize(dt_val).astimezone(pytz.utc)\n            return dt_val.astimezone(pytz.utc)  # Aware datetime\n        elif isinstance(dt_val, date):  # All-day event, represent as start of day in user's TZ, then UTC\n            return user_tz.localize(datetime.combine(dt_val, datetime.min.time())).astimezone(pytz.utc)\n        return None\n\n    def _map_ical_priority_to_app(self, ical_priority: Optional[int]) -&gt; int:\n        if ical_priority is None: return 3  # Default\n        if 1 &lt;= ical_priority &lt;= 4: return 1  # High\n        if ical_priority == 5: return 3  # Medium\n        if 6 &lt;= ical_priority &lt;= 9: return 5  # Low\n        return 3  # Default for 0 or other values\n\n    def _map_app_priority_to_ical(self, app_priority: int) -&gt; int:\n        if app_priority == 1: return 1  # High\n        if app_priority == 2: return 3\n        if app_priority == 3: return 5  # Medium\n        if app_priority == 4: return 7\n        if app_priority == 5: return 9  # Low\n        return 0  # No priority\n\n    def _map_rrule_to_frequency(self, rrule_prop: Optional[vRecur]) -&gt; Tuple[Frequency, Optional[str]]:\n        if not rrule_prop:\n            return Frequency.ONE_TIME, None\n\n        rrule_dict = rrule_prop.to_dict()\n        freq = rrule_dict.get('FREQ')\n        original_rrule_str = vRecur.from_dict(rrule_dict).to_ical().decode('utf-8')\n\n        if freq == 'DAILY': return Frequency.DAILY, original_rrule_str\n        if freq == 'WEEKLY': return Frequency.WEEKLY, original_rrule_str\n        if freq == 'MONTHLY': return Frequency.MONTHLY, original_rrule_str\n        if freq == 'YEARLY': return Frequency.ANNUALLY, original_rrule_str\n\n        # If RRULE is complex or not a direct match, import as ONE_TIME for each instance\n        # but store the original RRULE string for reference or future advanced handling.\n        return Frequency.ONE_TIME, original_rrule_str\n\n    def import_ical_events(self, ical_string: str) -&gt; List[ActionItem]:\n        imported_items: List[ActionItem] = []\n        try:\n            cal = iCalCalendar.from_ical(ical_string)\n            user_tz = self.get_user_timezone()\n            now_utc = datetime.now(pytz.utc)\n            import_limit_date_utc = now_utc + timedelta(days=RECURRING_IMPORT_WINDOW_DAYS)\n\n            processed_uids_for_session = set()  # To avoid processing same base recurring event multiple times in one import\n\n            for component in cal.walk():\n                if component.name == \"VEVENT\":\n                    uid = component.get('uid')\n                    if not uid:\n                        uid = str(uuid.uuid4())  # Generate a UID if missing\n                    else:\n                        uid = uid.to_ical().decode('utf-8')\n\n                    summary = component.get('summary', 'Untitled Event').to_ical().decode('utf-8')\n                    description = component.get('description', '').to_ical().decode('utf-8')\n                    location = component.get('location', '').to_ical().decode('utf-8')\n                    dtstart_ical = component.get('dtstart')\n                    dtend_ical = component.get('dtend')  # Can be used for duration if needed\n                    ical_priority_val = component.get('priority')\n                    ical_priority = int(ical_priority_val.to_ical().decode('utf-8')) if ical_priority_val else None\n\n                    rrule_prop = component.get('rrule')  # This is a vRecur object or None\n\n                    start_time_utc = self._parse_ical_dt(dtstart_ical, user_tz)\n                    if not start_time_utc:\n                        self.app.logger.warning(f\"Skipping event '{summary}' due to missing/invalid DTSTART.\")\n                        continue\n\n                    app_priority = self._map_ical_priority_to_app(ical_priority)\n\n                    # Check for existing item with this iCal UID to potentially update (simplistic check)\n                    # A more robust update would involve comparing sequence numbers, etc.\n                    # For now, if UID exists, we might skip or update. Let's try to update.\n                    # To keep it simpler for now, we will create new items for occurrences.\n                    # UID management needs to be precise for updates.\n                    # If an item is an instance of a recurring event, its UID in our system might be base_uid + occurrence_date.\n\n                    if rrule_prop:\n                        if uid in processed_uids_for_session:  # Already processed this recurring event's base\n                            continue\n                        processed_uids_for_session.add(uid)\n\n                        # Handle recurring event\n                        rrule_str = rrule_prop.to_ical().decode('utf-8')\n                        # Ensure DTSTART is part of the rrule context if not explicitly in rrulestr\n                        if 'DTSTART' not in rrule_str.upper() and start_time_utc:\n                            # dateutil.rrule needs start time; icalendar often bakes it in.\n                            # If start_time_utc is naive, use user_tz to make it aware.\n                            dtstart_for_rrule = start_time_utc.astimezone(\n                                user_tz) if start_time_utc.tzinfo else user_tz.localize(start_time_utc)\n                            # rrule_obj = rrulestr(rrule_str, dtstart=dtstart_for_rrule) # This is complex due to TZ handling in rrulestr\n                            # The icalendar library's component should be timezone aware from DTSTART\n                            # So, let's assume dtstart_ical.dt is the correct starting point.\n                            try:\n                                rrule_obj = rrulestr(rrule_str, dtstart=dtstart_ical.dt)\n                            except Exception as e_rr:\n                                self.app.logger.error(\n                                    f\"Could not parse RRULE '{rrule_str}' for event '{summary}': {e_rr}\")\n                                continue\n\n                        occurrences_imported = 0\n                        # Generate occurrences starting from now (in user's timezone, aligned to event's time)\n                        # or from event's start_time_utc if it's in the future.\n\n                        # The rrule iteration should be in the event's original timezone context if possible,\n                        # or consistently in user's timezone for 'now'.\n                        # Let's use UTC for iteration and then convert.\n\n                        # Iterate from the event's actual start time or now, whichever is later for relevant future instances.\n                        iteration_start_utc = max(now_utc, start_time_utc)\n\n                        for occ_dt_aware in rrule_obj.between(iteration_start_utc, import_limit_date_utc, inc=True):\n                            if occurrences_imported &gt;= MAX_RECURRING_INSTANCES_TO_IMPORT:\n                                break\n\n                            # occ_dt_aware is usually from dateutil.rrule, may need tzinfo set or conversion.\n                            # If rrulestr was given an aware dtstart, occurrences should be aware.\n                            # Ensure it's UTC for our system.\n                            occ_utc = occ_dt_aware.astimezone(pytz.utc) if occ_dt_aware.tzinfo else pytz.utc.localize(\n                                occ_dt_aware)\n\n                            instance_uid = f\"{uid}-{occ_utc.strftime('%Y%m%dT%H%M%S%Z')}\"\n\n                            # Check if this specific instance already exists\n                            existing_instance = next((item for item in self.items if item.ical_uid == instance_uid),\n                                                     None)\n                            if existing_instance:\n                                self.app.logger.info(\n                                    f\"Instance {instance_uid} for '{summary}' already exists. Skipping.\")\n                                continue\n\n                            item_data = {\n                                \"title\": summary, \"description\": description, \"location\": location,\n                                \"item_type\": ItemType.TASK, \"fixed_time\": occ_utc,\n                                \"frequency\": Frequency.ONE_TIME,  # Each imported instance is one-time in our system\n                                \"priority\": app_priority, \"ical_uid\": instance_uid,  # Instance-specific UID\n                                \"status\": ActionStatus.NOT_STARTED,\n                                \"ical_rrule_original\": rrule_str  # Store original rule for reference\n                            }\n                            new_item = self.add_item(item_data, imported=True)\n                            imported_items.append(new_item)\n                            occurrences_imported += 1\n\n                        if occurrences_imported == 0 and start_time_utc &gt; now_utc and start_time_utc &lt;= import_limit_date_utc:\n                            # If it's a future non-recurring event (or rrule didn't yield instances in window but start is in window)\n                            # This case is for when rrule_prop exists but yields no instances in the .between() range,\n                            # but the initial DTSTART itself is valid and upcoming.\n                            # However, rrule.between should include dtstart if inc=True and it's within range.\n                            # This path might be redundant if .between is inclusive and dtstart is in range.\n                            pass\n\n\n                    else:  # Non-recurring event\n                        # Only import if it's upcoming or started recently and not completed (e.g. within last day)\n                        if start_time_utc &lt; (\n                            now_utc - timedelta(days=1)) and not dtend_ical:  # Too old, and no end time to check\n                            self.app.logger.info(f\"Skipping old non-recurring event '{summary}' (UID: {uid})\")\n                            continue\n                        if dtend_ical:\n                            end_time_utc = self._parse_ical_dt(dtend_ical, user_tz)\n                            if end_time_utc and end_time_utc &lt; now_utc:  # Event has already ended\n                                self.app.logger.info(f\"Skipping past event '{summary}' (UID: {uid}) that has ended.\")\n                                continue\n\n                        existing_item = next((item for item in self.items if item.ical_uid == uid), None)\n                        if existing_item:  # Simplistic update: remove old, add new. Better: update in place.\n                            self.app.logger.info(\n                                f\"Event with UID {uid} ('{summary}') already exists. Re-importing (simple replace).\")\n                            self.remove_item(existing_item.id, record_history=False)\n\n                        item_data = {\n                            \"title\": summary, \"description\": description, \"location\": location,\n                            \"item_type\": ItemType.TASK, \"fixed_time\": start_time_utc,\n                            \"frequency\": Frequency.ONE_TIME, \"priority\": app_priority,\n                            \"ical_uid\": uid, \"status\": ActionStatus.NOT_STARTED\n                        }\n                        new_item = self.add_item(item_data, imported=True)\n                        imported_items.append(new_item)\n\n            if imported_items:\n                self._log_ai_action(\"ical_import\", [item.id for item in imported_items])\n            self._save_data()  # Ensure all changes are saved\n            self.app.logger.info(f\"Imported {len(imported_items)} items from iCalendar data.\")\n\n        except Exception as e:\n            self.app.logger.error(f\"Failed to parse iCalendar string: {e}\", exc_info=True)\n            # Potentially re-raise or return empty list with error status\n        return imported_items\n\n    def import_ical_from_url(self, url: str) -&gt; List[ActionItem]:\n        try:\n            headers = {'User-Agent': 'POA_App/1.0 (+https://yourdomain.com/poa_app_info)'}  # Be a good internet citizen\n            response = requests.get(url, timeout=10, headers=headers)\n            response.raise_for_status()  # Raises HTTPError for bad responses (4XX or 5XX)\n            return self.import_ical_events(response.text)\n        except requests.exceptions.RequestException as e:\n            self.app.logger.error(f\"Error fetching iCalendar from URL {url}: {e}\")\n            return []\n        except Exception as e:  # Catch other errors like parsing\n            self.app.logger.error(f\"Error processing iCalendar from URL {url}: {e}\")\n            return []\n\n    def import_ical_from_file_content(self, file_content: bytes) -&gt; List[ActionItem]:\n        try:\n            # Try to decode as UTF-8, but iCal can have other encodings.\n            # Standard is UTF-8. `icalendar` lib handles encoding detection mostly.\n            ical_string = file_content.decode('utf-8', errors='replace')\n            return self.import_ical_events(ical_string)\n        except UnicodeDecodeError as e:\n            self.app.logger.error(f\"Encoding error reading iCalendar file: {e}. Try ensuring UTF-8 encoding.\")\n            # Try with 'latin-1' as a common fallback for some older files\n            try:\n                ical_string = file_content.decode('latin-1', errors='replace')\n                return self.import_ical_events(ical_string)\n            except Exception as e_fallback:\n                self.app.logger.error(f\"Fallback decoding also failed for iCalendar file: {e_fallback}\")\n                return []\n        except Exception as e:\n            self.app.logger.error(f\"Error processing iCalendar file content: {e}\")\n            return []\n\n    def export_to_ical_string(self) -&gt; str:\n        cal = iCalCalendar()\n        cal.add('prodid', '-//POA App//yourdomain.com//')\n        cal.add('version', '2.0')\n        user_tz = self.get_user_timezone()\n\n        for item in self.items:\n            if item.item_type == ItemType.TASK and item.fixed_time:\n                event = iCalEvent()\n                event.add('summary', item.title)\n\n                # Ensure fixed_time is UTC for iCal standard practice\n                dtstart_utc = item.fixed_time\n                if dtstart_utc.tzinfo is None:  # Should not happen if stored correctly\n                    dtstart_utc = pytz.utc.localize(dtstart_utc)\n                else:\n                    dtstart_utc = dtstart_utc.astimezone(pytz.utc)\n                event.add('dtstart', dtstart_utc)  # vDatetime handles UTC conversion for .to_ical()\n\n                # Add DTEND (e.g., 1 hour duration for tasks, or based on item if available)\n                # For simplicity, let's assume 1 hour duration if not specified\n                event.add('dtend', dtstart_utc + timedelta(hours=1))\n\n                event.add('dtstamp', datetime.now(pytz.utc))  # Time the event was created in iCal\n                event.add('uid', item.ical_uid or item.id)  # Use original iCal UID if present, else our ID\n\n                if item.description:\n                    event.add('description', item.description)\n                if item.location:\n                    event.add('location', item.location)\n\n                event.add('priority', self._map_app_priority_to_ical(item.priority))\n\n                # Handle recurrence\n                if item.frequency != Frequency.ONE_TIME:\n                    if item.ical_rrule_original:  # If we have the original complex rule, use it\n                        try:\n                            # vRecur.from_ical requires bytes\n                            event.add('rrule', vRecur.from_ical(item.ical_rrule_original.encode()))\n                        except Exception as e_rrule:\n                            self.app.logger.warning(\n                                f\"Could not parse stored original RRULE '{item.ical_rrule_original}' for item {item.id}: {e_rrule}. Exporting as simple recurrence.\")\n                            # Fallback to simple mapping\n                            self._add_simple_rrule(event, item.frequency)\n                    else:  # Map simple frequency\n                        self._add_simple_rrule(event, item.frequency)\n\n                cal.add_component(event)\n        return cal.to_ical().decode('utf-8')\n\n    def _add_simple_rrule(self, event: iCalEvent, frequency: Frequency):\n        rrule_params = {}\n        if frequency == Frequency.DAILY:\n            rrule_params['freq'] = 'DAILY'\n        elif frequency == Frequency.WEEKLY:\n            rrule_params['freq'] = 'WEEKLY'\n        elif frequency == Frequency.MONTHLY:\n            rrule_params['freq'] = 'MONTHLY'\n        elif frequency == Frequency.ANNUALLY:\n            rrule_params['freq'] = 'YEARLY'\n\n        if rrule_params:\n            event.add('rrule', vRecur(rrule_params))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager","title":"<code>SchedulerManager</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.SchedulerManagerClass","title":"<code>SchedulerManagerClass</code>","text":"Source code in <code>toolboxv2/mods/SchedulerManager.py</code> <pre><code>class SchedulerManagerClass:\n    def __init__(self):\n        self.jobs = {}\n        self.thread = None\n        self.running = False\n        self.last_successful_jobs = deque(maxlen=3)  # Stores last 3 successful job names\n        self.job_errors = {}  # Stores job names as keys and error messages as values\n\n    def _run(self):\n        while self.running:\n            schedule.run_pending()\n            time.sleep(1)\n\n    def start(self):\n        if not self.running:\n            self.running = True\n            self.thread = threading.Thread(target=self._run, daemon=True)\n            self.thread.start()\n\n    def stop(self):\n        self.running = False\n        if self.thread is not None:\n            self.thread.join()\n\n    def job_wrapper(self, job_name: str, job_function: callable):\n        \"\"\"\n        Wrap a job function to track success and errors.\n        \"\"\"\n        def wrapped_job(*args, **kwargs):\n            try:\n                job_function(*args, **kwargs)\n                # If the job ran successfully, store it in the success queue\n                self.last_successful_jobs.append(job_name)\n                if job_name in self.job_errors:\n                    del self.job_errors[job_name]  # Remove error record if job succeeded after failing\n            except Exception as e:\n                # Capture any exceptions and store them\n                self.job_errors[job_name] = str(e)\n\n        return wrapped_job\n\n\n    def register_job(self,\n                     job_id: str,\n                     second: int = -1,\n                     func: (Callable or str) | None = None,\n                     job: schedule.Job | None = None,\n                     time_passer: schedule.Job | None = None,\n                     object_name: str | None = None,\n                     receive_job: bool = False,\n                     save: bool = False,\n                     max_live: bool = False,\n                     serializer=serializer_default,\n                     args=None, kwargs=None):\n        \"\"\"\n            Parameters\n            ----------\n                job_id : str\n                    id for the job for management\n                second : int\n                    The time interval in seconds between each call of the job.\n                func : Callable or str\n                    The function to be executed as the job.\n                job : schedule.Job\n                    An existing job object from the schedule library.\n                time_passer : schedule.Job\n                    A job without a function, used to specify the time interval.\n                object_name : str\n                    The name of the object containing in the 'func' var to be executed.\n                receive_job : bool\n                    A flag indicating whether the job should be received from an object from 'func' var.\n                save : bool\n                    A flag indicating whether the job should be saved.\n                max_live : bool\n                    A flag indicating whether the job should have a maximum live time.\n                serializer : dill\n                    json pickel or dill must have a dumps fuction\n                *args, **kwargs : Any serializable and deserializable\n                    Additional arguments to be passed to the job function.\n\n            Returns\n            -------\n           \"\"\"\n\n        if job is None and func is None:\n            return Result.default_internal_error(\"Both job and func are not specified.\"\n                                                 \" Please specify either job or func.\")\n        if job is not None and func is not None:\n            return Result.default_internal_error(\"Both job and func are specified. Please specify either job or func.\")\n\n        if job is not None:\n            def func(x):\n                return x\n            return self._save_job(job_id=job_id,\n                                  job=job,\n                                  save=save,\n                                  func=func,\n                                  args=args,\n                                  kwargs=kwargs,\n                                  serializer=serializer)\n\n        parsed_attr = self._parse_function(func=func, object_name=object_name)\n\n        if parsed_attr.is_error():\n            parsed_attr.result.data_info = f\"Error parsing function for job : {job_id}\"\n            return parsed_attr\n\n        if receive_job:\n            job = parsed_attr.get()\n        else:\n            func = parsed_attr.get()\n\n        time_passer = self._prepare_time_passer(time_passer=time_passer,\n                                                second=second)\n\n        job_func = self._prepare_job_func(func=func,\n                                          max_live=max_live,\n                                          second=second,\n                                          args=args,\n                                          kwargs=kwargs,\n                                          job_id=job_id)\n\n        job = self._get_final_job(job=job,\n                                  func=self.job_wrapper(job_id, job_func),\n                                  time_passer=time_passer,\n                                  job_func=job_func,\n                                  args=args,\n                                  kwargs=kwargs)\n        if job.is_error():\n            return job\n\n        job = job.get()\n\n        return self._save_job(job_id=job_id,\n                              job=job,\n                              save=save,\n                              func=func,\n                              args=args,\n                              kwargs=kwargs,\n                              serializer=serializer)\n\n    @staticmethod\n    def _parse_function(func: str or Callable, object_name):\n        if isinstance(func, str) and func.endswith('.py'):\n            with open(func) as file:\n                func_code = file.read()\n                exec(func_code)\n                func = locals()[object_name]\n        elif isinstance(func, str) and func.endswith('.dill') and safety_mode == 'open':\n            try:\n                with open(func, 'rb') as file:\n                    func = dill.load(file)\n            except FileNotFoundError:\n                return Result.default_internal_error(f\"Function file {func} not found or dill not installed\")\n        elif isinstance(func, str):\n            local_vars = {'app': get_app(from_=Name + f\".pasing.{object_name}\")}\n            try:\n                exec(func.strip(), {}, local_vars)\n            except Exception as e:\n                return Result.default_internal_error(f\"Function parsing failed withe {e}\")\n            func = local_vars[object_name]\n        elif isinstance(func, Callable):\n            pass\n        else:\n            return Result.default_internal_error(\"Could not parse object scheduler_manager.parse_function\")\n        return Result.ok(func)\n\n    @staticmethod\n    def _prepare_time_passer(time_passer, second):\n        if time_passer is None and second &gt; 0:\n            return schedule.every(second).seconds\n        elif time_passer is None and second &lt;= 0:\n            raise ValueError(\"second must be greater than 0\")\n        return time_passer\n\n    def _prepare_job_func(self, func: Callable, max_live: bool, second: float, job_id: str, *args, **kwargs):\n        if max_live:\n            end_time = datetime.now() + timedelta(seconds=second)\n\n            def job_func():\n                if datetime.now() &lt; end_time:\n                    func(*args, **kwargs)\n                else:\n                    job = self.jobs.get(job_id, {}).get('job')\n                    if job is not None:\n                        schedule.cancel_job(job)\n                    else:\n                        print(\"Error Canceling job\")\n\n            return job_func\n        return func\n\n    @staticmethod\n    def _get_final_job(job, func, time_passer, job_func, args, kwargs):\n        if job is None and isinstance(func, Callable):\n            job = time_passer.do(job_func, *args, **kwargs)\n        elif job is not None:\n            pass\n        else:\n            return Result.default_internal_error(\"No Final job found for register\")\n        return Result.ok(job)\n\n    def _save_job(self, job_id, job, save, args=None, **kwargs):\n        if job is not None:\n            self.jobs[job_id] = {'id': job_id, 'job': job, 'save': save, 'func': job_id, 'args': args,\n                                 'kwargs': kwargs}\n            f = (f\"Added Job {job_id} :{' - saved' if save else ''}\"\n                  f\"{' - args ' + str(len(args)) if args else ''}\"\n                  f\"{' - kwargs ' + str(len(kwargs.keys())) if kwargs else ''}\")\n            return Result.ok(f)\n        else:\n            return Result.default_internal_error(job_id)\n\n    def cancel_job(self, job_id):\n        if job_id not in self.jobs:\n            print(\"Job not found\")\n            return\n        schedule.cancel_job(self.jobs[job_id].get('job'))\n        self.jobs[job_id][\"cancelled\"] = True\n        self.jobs[job_id][\"save\"] = False\n        print(\"Job cancelled\")\n\n    def del_job(self, job_id):\n        if job_id not in self.jobs:\n            print(\"Job not found\")\n            return\n        if not self.jobs[job_id].get(\"cancelled\", False):\n            print(\"Job not cancelled canceling job\")\n            self.cancel_job(job_id)\n        del self.jobs[job_id]\n        print(\"Job deleted\")\n\n    def save_jobs(self, file_path, serializer=serializer_default):\n        with open(file_path, 'wb') as file:\n            save_jobs = [job for job in self.jobs.values() if job['save']]\n            serializer.dump(save_jobs, file)\n\n    def load_jobs(self, file_path, deserializer=deserializer_default):\n        with open(file_path, 'rb') as file:\n            jobs = deserializer.load(file)\n            for job_info in jobs:\n                del job_info['job']\n                func = deserializer.loads(job_info['func'])\n                self.register_job(job_info['id'], func=func, **job_info)\n\n    def get_tasks_table(self):\n        if not self.jobs:\n            return \"No tasks registered.\"\n\n        # Calculate the maximum width for each column\n        id_width = max(len(\"Task ID\"), max(len(job_id) for job_id in self.jobs))\n        next_run_width = len(\"Next Execution\")\n        interval_width = len(\"Interval\")\n\n        # Create the header\n        header = f\"| {'Task ID':&lt;{id_width}} | {'Next Execution':&lt;{next_run_width}} | {'Interval':&lt;{interval_width}} |\"\n        separator = f\"|{'-' * (id_width + 2)}|{'-' * (next_run_width + 2)}|{'-' * (interval_width + 2)}|\"\n\n        # Create the table rows\n        rows = []\n        for job_id, job_info in self.jobs.items():\n            job = job_info['job']\n            next_run = job.next_run.strftime(\"%Y-%m-%d %H:%M:%S\") if job.next_run else \"N/A\"\n            interval = self._get_interval_str(job)\n            row = f\"| {job_id:&lt;{id_width}} | {next_run:&lt;{next_run_width}} | {interval:&lt;{interval_width}} |\"\n            rows.append(row)\n\n        # Combine all parts of the table\n        table = \"\\n\".join([header, separator] + rows)\n        return table\n\n    def _get_interval_str(self, job):\n        if job.interval == 0:\n            return \"Once\"\n\n        units = [\n            (86400, \"day\"),\n            (3600, \"hour\"),\n            (60, \"minute\"),\n            (1, \"second\")\n        ]\n\n        for seconds, unit in units:\n            if job.interval % seconds == 0:\n                count = job.interval // seconds\n                return f\"Every {count} {unit}{'s' if count &gt; 1 else ''}\"\n\n        return f\"Every {job.interval} seconds\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.SchedulerManagerClass.job_wrapper","title":"<code>job_wrapper(job_name, job_function)</code>","text":"<p>Wrap a job function to track success and errors.</p> Source code in <code>toolboxv2/mods/SchedulerManager.py</code> <pre><code>def job_wrapper(self, job_name: str, job_function: callable):\n    \"\"\"\n    Wrap a job function to track success and errors.\n    \"\"\"\n    def wrapped_job(*args, **kwargs):\n        try:\n            job_function(*args, **kwargs)\n            # If the job ran successfully, store it in the success queue\n            self.last_successful_jobs.append(job_name)\n            if job_name in self.job_errors:\n                del self.job_errors[job_name]  # Remove error record if job succeeded after failing\n        except Exception as e:\n            # Capture any exceptions and store them\n            self.job_errors[job_name] = str(e)\n\n    return wrapped_job\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.SchedulerManagerClass.register_job","title":"<code>register_job(job_id, second=-1, func=None, job=None, time_passer=None, object_name=None, receive_job=False, save=False, max_live=False, serializer=serializer_default, args=None, kwargs=None)</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.SchedulerManagerClass.register_job--parameters","title":"Parameters","text":"<pre><code>job_id : str\n    id for the job for management\nsecond : int\n    The time interval in seconds between each call of the job.\nfunc : Callable or str\n    The function to be executed as the job.\njob : schedule.Job\n    An existing job object from the schedule library.\ntime_passer : schedule.Job\n    A job without a function, used to specify the time interval.\nobject_name : str\n    The name of the object containing in the 'func' var to be executed.\nreceive_job : bool\n    A flag indicating whether the job should be received from an object from 'func' var.\nsave : bool\n    A flag indicating whether the job should be saved.\nmax_live : bool\n    A flag indicating whether the job should have a maximum live time.\nserializer : dill\n    json pickel or dill must have a dumps fuction\n*args, **kwargs : Any serializable and deserializable\n    Additional arguments to be passed to the job function.\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.SchedulerManagerClass.register_job--returns","title":"Returns","text":"Source code in <code>toolboxv2/mods/SchedulerManager.py</code> <pre><code>def register_job(self,\n                 job_id: str,\n                 second: int = -1,\n                 func: (Callable or str) | None = None,\n                 job: schedule.Job | None = None,\n                 time_passer: schedule.Job | None = None,\n                 object_name: str | None = None,\n                 receive_job: bool = False,\n                 save: bool = False,\n                 max_live: bool = False,\n                 serializer=serializer_default,\n                 args=None, kwargs=None):\n    \"\"\"\n        Parameters\n        ----------\n            job_id : str\n                id for the job for management\n            second : int\n                The time interval in seconds between each call of the job.\n            func : Callable or str\n                The function to be executed as the job.\n            job : schedule.Job\n                An existing job object from the schedule library.\n            time_passer : schedule.Job\n                A job without a function, used to specify the time interval.\n            object_name : str\n                The name of the object containing in the 'func' var to be executed.\n            receive_job : bool\n                A flag indicating whether the job should be received from an object from 'func' var.\n            save : bool\n                A flag indicating whether the job should be saved.\n            max_live : bool\n                A flag indicating whether the job should have a maximum live time.\n            serializer : dill\n                json pickel or dill must have a dumps fuction\n            *args, **kwargs : Any serializable and deserializable\n                Additional arguments to be passed to the job function.\n\n        Returns\n        -------\n       \"\"\"\n\n    if job is None and func is None:\n        return Result.default_internal_error(\"Both job and func are not specified.\"\n                                             \" Please specify either job or func.\")\n    if job is not None and func is not None:\n        return Result.default_internal_error(\"Both job and func are specified. Please specify either job or func.\")\n\n    if job is not None:\n        def func(x):\n            return x\n        return self._save_job(job_id=job_id,\n                              job=job,\n                              save=save,\n                              func=func,\n                              args=args,\n                              kwargs=kwargs,\n                              serializer=serializer)\n\n    parsed_attr = self._parse_function(func=func, object_name=object_name)\n\n    if parsed_attr.is_error():\n        parsed_attr.result.data_info = f\"Error parsing function for job : {job_id}\"\n        return parsed_attr\n\n    if receive_job:\n        job = parsed_attr.get()\n    else:\n        func = parsed_attr.get()\n\n    time_passer = self._prepare_time_passer(time_passer=time_passer,\n                                            second=second)\n\n    job_func = self._prepare_job_func(func=func,\n                                      max_live=max_live,\n                                      second=second,\n                                      args=args,\n                                      kwargs=kwargs,\n                                      job_id=job_id)\n\n    job = self._get_final_job(job=job,\n                              func=self.job_wrapper(job_id, job_func),\n                              time_passer=time_passer,\n                              job_func=job_func,\n                              args=args,\n                              kwargs=kwargs)\n    if job.is_error():\n        return job\n\n    job = job.get()\n\n    return self._save_job(job_id=job_id,\n                          job=job,\n                          save=save,\n                          func=func,\n                          args=args,\n                          kwargs=kwargs,\n                          serializer=serializer)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.Tools","title":"<code>Tools</code>","text":"<p>               Bases: <code>MainTool</code>, <code>SchedulerManagerClass</code></p> Source code in <code>toolboxv2/mods/SchedulerManager.py</code> <pre><code>class Tools(MainTool, SchedulerManagerClass):\n    version = version\n\n    def __init__(self, app=None):\n        self.name = Name\n        self.color = \"VIOLET2\"\n\n        self.keys = {\"mode\": \"db~mode~~:\"}\n        self.encoding = 'utf-8'\n        self.tools = {'name': Name}\n\n        SchedulerManagerClass.__init__(self)\n        MainTool.__init__(self,\n                          load=self.init_sm,\n                          v=self.version,\n                          name=self.name,\n                          color=self.color,\n                          on_exit=self.on_exit)\n\n\n    @export(\n        mod_name=Name,\n        name=\"Version\",\n        version=version,\n    )\n    def get_version(self):\n        return self.version\n\n    # Exportieren der Scheduler-Instanz f\u00fcr die Nutzung in anderen Modulen\n    @export(mod_name=Name, name='init', version=version, initial=True)\n    def init_sm(self):\n        if os.path.exists(self.app.data_dir + '/jobs.compact'):\n            print(\"SchedulerManager try loading from file\")\n            self.load_jobs(\n                self.app.data_dir + '/jobs.compact'\n            )\n            print(\"SchedulerManager Successfully loaded\")\n        print(\"STARTING SchedulerManager\")\n        self.start()\n\n    @export(mod_name=Name, name='clos_manager', version=version, exit_f=True)\n    def on_exit(self):\n        self.stop()\n        self.save_jobs(self.app.data_dir + '/jobs.compact')\n        return f\"saved {len(self.jobs.keys())} jobs in {self.app.data_dir + '/jobs.compact'}\"\n\n    @export(mod_name=Name, name='instance', version=version)\n    def get_instance(self):\n        return self\n\n    @export(mod_name=Name, name='start', version=version)\n    def start_instance(self):\n        return self.start()\n\n    @export(mod_name=Name, name='stop', version=version)\n    def stop_instance(self):\n        return self.stop()\n\n    @export(mod_name=Name, name='cancel', version=version)\n    def cancel_instance(self, job_id):\n        return self.cancel_job(job_id)\n\n    @export(mod_name=Name, name='dealt', version=version)\n    def dealt_instance(self, job_id):\n        return self.del_job(job_id)\n\n    @export(mod_name=Name, name='add', version=version)\n    def register_instance(self, job_data: dict):\n        \"\"\"\n        example dicts :\n            -----------\n            {\n                \"job_id\": \"job0\",\n                \"second\": 0,\n                \"func\": None,\n                \"job\": None,\n                \"time_passer\": None,\n                \"object_name\": \"tb_job_fuction\",\n                \"receive_job\": False,\n                \"save\": False,\n                \"max_live\": True,\n                # just lev it out \"serializer\": serializer_default,\n                \"args\": [],\n                \"kwargs\": {},\n            }\n\n            job_id : str\n                id for the job for management\n            second (optional): int\n                The time interval in seconds between each call of the job.\n            func (optional): Callable or str\n                The function to be executed as the job.\n            job (optional):  schedule.Job\n                An existing job object from the schedule library.\n            time_passer (optional):  schedule.Job\n                A job without a function, used to specify the time interval.\n            object_name (optional): str\n                The name of the object containing in the 'func' var to be executed.\n            receive_job (optional): bool\n                A flag indicating whether the job should be received from an object from 'func' var.\n            save (optional): bool\n                A flag indicating whether the job should be saved.\n            max_live (optional): bool\n                A flag indicating whether the job should have a maximum live time.\n            serializer (optional): bool\n                json pickel or dill must have a dumps fuction\n            *args, **kwargs (optional):\n                Additional arguments to be passed to the job function.\n\n\n        Parameters\n            ----------\n           job_data : dict\n\n        example usage\n            ----------\n            `python\n\n            `\n\n    \"\"\"\n        if job_data is None:\n            self.app.logger.error(\"No job data provided\")\n            return None\n        job_id = job_data[\"job_id\"]\n        second = job_data.get(\"second\", 0)\n        func = job_data.get(\"func\")\n        job = job_data.get(\"job\")\n        time_passer = job_data.get(\"time_passer\")\n        object_name = job_data.get(\"object_name\", \"tb_job_fuction\")\n        receive_job = job_data.get(\"receive_job\", False)\n        save = job_data.get(\"save\", False)\n        max_live = job_data.get(\"max_live\", True)\n        serializer = job_data.get(\"serializer\", serializer_default)\n        args = job_data.get(\"args\", ())\n        kwargs = job_data.get(\"kwargs\", {})\n\n        return self.register_job(\n            job_id=job_id,\n            second=second,\n            func=func,\n            job=job,\n            time_passer=time_passer,\n            object_name=object_name,\n            receive_job=receive_job,\n            save=save,\n            max_live=max_live,\n            serializer=serializer,\n            args=args,\n            kwargs=kwargs\n        )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.Tools.register_instance","title":"<code>register_instance(job_data)</code>","text":"example dicts <p>{     \"job_id\": \"job0\",     \"second\": 0,     \"func\": None,     \"job\": None,     \"time_passer\": None,     \"object_name\": \"tb_job_fuction\",     \"receive_job\": False,     \"save\": False,     \"max_live\": True,     # just lev it out \"serializer\": serializer_default,     \"args\": [],     \"kwargs\": {}, }</p> <p>job_id : str     id for the job for management second (optional): int     The time interval in seconds between each call of the job. func (optional): Callable or str     The function to be executed as the job. job (optional):  schedule.Job     An existing job object from the schedule library. time_passer (optional):  schedule.Job     A job without a function, used to specify the time interval. object_name (optional): str     The name of the object containing in the 'func' var to be executed. receive_job (optional): bool     A flag indicating whether the job should be received from an object from 'func' var. save (optional): bool     A flag indicating whether the job should be saved. max_live (optional): bool     A flag indicating whether the job should have a maximum live time. serializer (optional): bool     json pickel or dill must have a dumps fuction args, *kwargs (optional):     Additional arguments to be passed to the job function.</p> <p>Parameters     ----------    job_data : dict</p> <p>example usage     ----------     `python</p> <pre><code>`\n</code></pre> Source code in <code>toolboxv2/mods/SchedulerManager.py</code> <pre><code>@export(mod_name=Name, name='add', version=version)\ndef register_instance(self, job_data: dict):\n    \"\"\"\n    example dicts :\n        -----------\n        {\n            \"job_id\": \"job0\",\n            \"second\": 0,\n            \"func\": None,\n            \"job\": None,\n            \"time_passer\": None,\n            \"object_name\": \"tb_job_fuction\",\n            \"receive_job\": False,\n            \"save\": False,\n            \"max_live\": True,\n            # just lev it out \"serializer\": serializer_default,\n            \"args\": [],\n            \"kwargs\": {},\n        }\n\n        job_id : str\n            id for the job for management\n        second (optional): int\n            The time interval in seconds between each call of the job.\n        func (optional): Callable or str\n            The function to be executed as the job.\n        job (optional):  schedule.Job\n            An existing job object from the schedule library.\n        time_passer (optional):  schedule.Job\n            A job without a function, used to specify the time interval.\n        object_name (optional): str\n            The name of the object containing in the 'func' var to be executed.\n        receive_job (optional): bool\n            A flag indicating whether the job should be received from an object from 'func' var.\n        save (optional): bool\n            A flag indicating whether the job should be saved.\n        max_live (optional): bool\n            A flag indicating whether the job should have a maximum live time.\n        serializer (optional): bool\n            json pickel or dill must have a dumps fuction\n        *args, **kwargs (optional):\n            Additional arguments to be passed to the job function.\n\n\n    Parameters\n        ----------\n       job_data : dict\n\n    example usage\n        ----------\n        `python\n\n        `\n\n\"\"\"\n    if job_data is None:\n        self.app.logger.error(\"No job data provided\")\n        return None\n    job_id = job_data[\"job_id\"]\n    second = job_data.get(\"second\", 0)\n    func = job_data.get(\"func\")\n    job = job_data.get(\"job\")\n    time_passer = job_data.get(\"time_passer\")\n    object_name = job_data.get(\"object_name\", \"tb_job_fuction\")\n    receive_job = job_data.get(\"receive_job\", False)\n    save = job_data.get(\"save\", False)\n    max_live = job_data.get(\"max_live\", True)\n    serializer = job_data.get(\"serializer\", serializer_default)\n    args = job_data.get(\"args\", ())\n    kwargs = job_data.get(\"kwargs\", {})\n\n    return self.register_job(\n        job_id=job_id,\n        second=second,\n        func=func,\n        job=job,\n        time_passer=time_passer,\n        object_name=object_name,\n        receive_job=receive_job,\n        save=save,\n        max_live=max_live,\n        serializer=serializer,\n        args=args,\n        kwargs=kwargs\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SocketManager","title":"<code>SocketManager</code>","text":"<p>The SocketManager Supports 2 types of connections 1. Client Server 2. Peer to Peer</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker","title":"<code>TruthSeeker</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler","title":"<code>arXivCrawler</code>","text":"<p>ArXiv Crawler for TruthSeeker. Main module for processing research queries.</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler.ArXivPDFProcessor","title":"<code>ArXivPDFProcessor</code>","text":"<p>Main processor for research queries. This is a wrapper around the new ResearchProcessor for backward compatibility.</p> Source code in <code>toolboxv2/mods/TruthSeeker/arXivCrawler.py</code> <pre><code>class ArXivPDFProcessor:\n    \"\"\"\n    Main processor for research queries.\n    This is a wrapper around the new ResearchProcessor for backward compatibility.\n    \"\"\"\n    def __init__(self,\n                 query: str,\n                 tools,\n                 chunk_size: int = 1_000_000,\n                 overlap: int = 2_000,\n                 max_workers=None,\n                 num_search_result_per_query=6,\n                 max_search=6,\n                 download_dir=\"pdfs\",\n                 callback=None,\n                 num_workers=None):\n        \"\"\"Initialize the ArXiv PDF processor.\n\n        Args:\n            query: Research query\n            tools: Tools module\n            chunk_size: Size of text chunks for processing\n            overlap: Overlap between chunks\n            max_workers: Maximum number of worker threads\n            num_search_result_per_query: Number of search results per query\n            max_search: Maximum number of search queries\n            download_dir: Directory to save downloaded files\n            callback: Callback function for status updates\n            num_workers: Number of worker threads\n        \"\"\"\n        # Create the new research processor\n        self.processor = ResearchProcessor(\n            query=query,\n            tools=tools,\n            chunk_size=chunk_size,\n            overlap=overlap,\n            max_workers=max_workers,\n            num_search_result_per_query=num_search_result_per_query,\n            max_search=max_search,\n            download_dir=download_dir,\n            callback=callback,\n            num_workers=num_workers\n        )\n\n        # Copy attributes for backward compatibility\n        self.insights_generated = False\n        self.queries_generated = False\n        self.query = query\n        self.tools = tools\n        self.mem = tools.get_memory()\n        self.chunk_size = chunk_size\n        self.overlap = overlap\n        self.max_workers = max_workers\n        self.nsrpq = num_search_result_per_query\n        self.max_search = max_search\n        self.download_dir = download_dir\n        self.parser = RobustPDFDownloader(download_dir=download_dir)\n        self.callback = callback if callback is not None else lambda status: None\n        self.mem_name = None\n        self.current_session = None\n        self.all_ref_papers = 0\n        self.last_insights_list = None\n        self.all_texts_len = 0\n        self.f_texts_len = 0\n        self.s_id = str(uuid.uuid4())\n        self.semantic_model = self.processor.semantic_model\n        self._query_progress = {}\n        self._progress_lock = threading.Lock()\n        self.num_workers = self.processor.num_workers\n\n    def _update_global_progress(self) -&gt; float:\n        \"\"\"Calculate overall progress considering all processing phases.\"\"\"\n        return self.processor._update_global_progress()\n\n    async def search_and_process_papers(self, queries: list[str]) -&gt; list[Paper]:\n        \"\"\"Search for and process papers based on queries.\n\n        Args:\n            queries: List of search queries\n\n        Returns:\n            List of processed papers\n        \"\"\"\n        # Use the new processor to search and process papers\n        unified_papers = await self.processor.search_and_process_papers(queries)\n\n        # Convert UnifiedPaper objects to Paper objects for backward compatibility\n        papers = []\n        for paper in unified_papers:\n            if paper.source == \"arxiv\":\n                # Convert to the old Paper format\n                arxiv_paper = Paper(\n                    title=paper.title,\n                    authors=paper.authors,\n                    summary=paper.summary,\n                    url=paper.url,\n                    pdf_url=paper.pdf_url,\n                    published=paper.published,\n                    updated=paper.source_specific_data.get(\"updated\", \"\"),\n                    categories=paper.source_specific_data.get(\"categories\", []),\n                    paper_id=paper.paper_id\n                )\n                papers.append(arxiv_paper)\n\n        # Update attributes for backward compatibility\n        self.all_ref_papers = self.processor.all_ref_papers\n        self.all_texts_len = self.processor.all_texts_len\n        self.f_texts_len = self.processor.f_texts_len\n\n        return papers\n\n    def send_status(self, step: str, progress: float = None, additional_info: str = \"\"):\n        \"\"\"Send status update via callback.\"\"\"\n        if progress is None:\n            progress = self._update_global_progress()\n        self.callback({\n            \"step\": step,\n            \"progress\": progress,\n            \"info\": additional_info\n        })\n\n    def generate_queries(self) -&gt; list[str]:\n        self.send_status(\"Generating search queries\")\n        self.queries_generated = False\n\n        class ArXivQueries(BaseModel):\n            queries: list[str] = Field(..., description=\"List of ArXiv search queries (en)\")\n\n        try:\n            query_generator: ArXivQueries = self.tools.format_class(\n                ArXivQueries,\n                f\"Generate a list of precise ArXiv search queries to comprehensively address: {self.query}\"\n            )\n            queries = [self.query] + query_generator[\"queries\"]\n        except Exception:\n            self.send_status(\"Error generating queries\", additional_info=\"Using default query.\")\n            queries = [self.query]\n\n        if len(queries[:self.max_search]) &gt; 0:\n            self.queries_generated = True\n        return queries[:self.max_search]\n\n    def init_process_papers(self):\n        self.mem.create_memory(self.mem_name, model_config={\"model_name\": \"anthropic/claude-3-5-haiku-20241022\"})\n        self.send_status(\"Memory initialized\")\n\n\n    async def generate_insights(self, queries) -&gt; dict:\n        self.send_status(\"Generating insights\")\n        query = self.query\n        # max_it = 0\n        results = await self.mem.query(query=query, memory_names=self.mem_name, unified_retrieve=True, query_params={\n            \"max_sentences\": 25})\n        #query = queries[min(len(queries)-1, max_it)]\n\n        self.insights_generated = True\n        self.send_status(\"Insights generated\", progress=1.0)\n        return results\n\n    async def extra_query(self, query, query_params=None, unified_retrieve=True):\n        self.send_status(\"Processing follow-up query\", progress=0.5)\n        results = await self.mem.query(query=query, memory_names=self.mem_name,\n                                                      query_params=query_params, unified_retrieve=unified_retrieve)\n        self.send_status(\"Processing follow-up query Done\", progress=1)\n        return results\n\n    def generate_mem_name(self):\n        class UniqueMemoryName(BaseModel):\n            \"\"\"unique memory name based on the user query\"\"\"\n            name: str\n        return self.tools.get_agent(\"thinkm\").format_class(UniqueMemoryName, self.query).get('name', '_'.join(self.query.split(\" \")[:3]))\n\n    def initialize(self, session_id, second=False):\n        self.current_session = session_id\n        self.insights_generated = False\n        self.queries_generated = False\n        if second:\n            return\n        self.mem_name = self.generate_mem_name().strip().replace(\"\\n\", '') + '_' + session_id\n        self.init_process_papers()\n\n    async def process(self, query=None) -&gt; tuple[list[Paper], dict]:\n        if query is not None:\n            self.query = query\n        self.send_status(\"Starting research process\")\n        t0 = time.perf_counter()\n        self.initialize(self.s_id, query is not None)\n\n        queries = self.generate_queries()\n\n        papers = await self.search_and_process_papers(queries)\n\n        if len(papers) == 0:\n            class UserQuery(BaseModel):\n                \"\"\"Fix all typos and clear the original user query\"\"\"\n                new_query: str\n            self.query= self.tools.format_class(\n                UserQuery,\n                self.query\n            )[\"new_query\"]\n            queries = self.generate_queries()\n            papers = await self.search_and_process_papers(queries)\n\n        insights = await self.generate_insights(queries)\n\n        elapsed_time = time.perf_counter() - t0\n        self.send_status(\"Process complete\", progress=1.0,\n                         additional_info=f\"Total time: {elapsed_time:.2f}s, Papers analyzed: {len(papers)}/{self.all_ref_papers}\")\n\n        return papers, insights\n\n    @staticmethod\n    def estimate_processing_metrics(query_length: int, **config) -&gt; (float, float):\n        \"\"\"Return estimated time (seconds) and price for processing.\"\"\"\n        total_papers = config['max_search'] * config['num_search_result_per_query']\n        median_text_length = 100000  # 10 pages * 10000 characters\n\n        # Estimated chunks to process\n        total_chunks = total_papers * (median_text_length / config['chunk_size']) + 1 / config['overlap']\n        processed_chunks = total_chunks * 0.45\n        total_chars = TextSplitter(config['chunk_size'],\n                     config['overlap']\n                     ).approximate(config['chunk_size'] * processed_chunks)\n        # Time estimation (seconds)\n        .75 / config['chunk_size']  # Hypothetical time per chunk in seconds\n        w = (config.get('num_workers', 16) if config.get('num_workers', 16) is not None else 16 / 10)\n        # Processing_ time - Insights Genration - Insights Query   -   Indexing Time     -    Download Time     -       workers   -   Query Genration time - Ui - Init Db\n        estimated_time = ((8+total_papers*0.012)+(total_chunks/20000) * .005 + (total_chunks/2) * .0003 + total_papers * 2.8 ) / w + (0.25 * config['max_search']) + 6 + 4\n\n        price_per_char = 0.0000012525\n        price_per_t_chunk =  total_chars * price_per_char\n        estimated_price = price_per_t_chunk ** 1.7\n\n        # estimated_price = 0 if query_length &lt; 420 and estimated_price &lt; 5 else estimated_price\n        if estimated_time &lt; 10:\n            estimated_time = 10\n        if estimated_price &lt; .04:\n            estimated_price = .04\n        return round(estimated_time, 2), round(estimated_price, 4)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler.ArXivPDFProcessor.__init__","title":"<code>__init__(query, tools, chunk_size=1000000, overlap=2000, max_workers=None, num_search_result_per_query=6, max_search=6, download_dir='pdfs', callback=None, num_workers=None)</code>","text":"<p>Initialize the ArXiv PDF processor.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Research query</p> required <code>tools</code> <p>Tools module</p> required <code>chunk_size</code> <code>int</code> <p>Size of text chunks for processing</p> <code>1000000</code> <code>overlap</code> <code>int</code> <p>Overlap between chunks</p> <code>2000</code> <code>max_workers</code> <p>Maximum number of worker threads</p> <code>None</code> <code>num_search_result_per_query</code> <p>Number of search results per query</p> <code>6</code> <code>max_search</code> <p>Maximum number of search queries</p> <code>6</code> <code>download_dir</code> <p>Directory to save downloaded files</p> <code>'pdfs'</code> <code>callback</code> <p>Callback function for status updates</p> <code>None</code> <code>num_workers</code> <p>Number of worker threads</p> <code>None</code> Source code in <code>toolboxv2/mods/TruthSeeker/arXivCrawler.py</code> <pre><code>def __init__(self,\n             query: str,\n             tools,\n             chunk_size: int = 1_000_000,\n             overlap: int = 2_000,\n             max_workers=None,\n             num_search_result_per_query=6,\n             max_search=6,\n             download_dir=\"pdfs\",\n             callback=None,\n             num_workers=None):\n    \"\"\"Initialize the ArXiv PDF processor.\n\n    Args:\n        query: Research query\n        tools: Tools module\n        chunk_size: Size of text chunks for processing\n        overlap: Overlap between chunks\n        max_workers: Maximum number of worker threads\n        num_search_result_per_query: Number of search results per query\n        max_search: Maximum number of search queries\n        download_dir: Directory to save downloaded files\n        callback: Callback function for status updates\n        num_workers: Number of worker threads\n    \"\"\"\n    # Create the new research processor\n    self.processor = ResearchProcessor(\n        query=query,\n        tools=tools,\n        chunk_size=chunk_size,\n        overlap=overlap,\n        max_workers=max_workers,\n        num_search_result_per_query=num_search_result_per_query,\n        max_search=max_search,\n        download_dir=download_dir,\n        callback=callback,\n        num_workers=num_workers\n    )\n\n    # Copy attributes for backward compatibility\n    self.insights_generated = False\n    self.queries_generated = False\n    self.query = query\n    self.tools = tools\n    self.mem = tools.get_memory()\n    self.chunk_size = chunk_size\n    self.overlap = overlap\n    self.max_workers = max_workers\n    self.nsrpq = num_search_result_per_query\n    self.max_search = max_search\n    self.download_dir = download_dir\n    self.parser = RobustPDFDownloader(download_dir=download_dir)\n    self.callback = callback if callback is not None else lambda status: None\n    self.mem_name = None\n    self.current_session = None\n    self.all_ref_papers = 0\n    self.last_insights_list = None\n    self.all_texts_len = 0\n    self.f_texts_len = 0\n    self.s_id = str(uuid.uuid4())\n    self.semantic_model = self.processor.semantic_model\n    self._query_progress = {}\n    self._progress_lock = threading.Lock()\n    self.num_workers = self.processor.num_workers\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler.ArXivPDFProcessor.estimate_processing_metrics","title":"<code>estimate_processing_metrics(query_length, **config)</code>  <code>staticmethod</code>","text":"<p>Return estimated time (seconds) and price for processing.</p> Source code in <code>toolboxv2/mods/TruthSeeker/arXivCrawler.py</code> <pre><code>@staticmethod\ndef estimate_processing_metrics(query_length: int, **config) -&gt; (float, float):\n    \"\"\"Return estimated time (seconds) and price for processing.\"\"\"\n    total_papers = config['max_search'] * config['num_search_result_per_query']\n    median_text_length = 100000  # 10 pages * 10000 characters\n\n    # Estimated chunks to process\n    total_chunks = total_papers * (median_text_length / config['chunk_size']) + 1 / config['overlap']\n    processed_chunks = total_chunks * 0.45\n    total_chars = TextSplitter(config['chunk_size'],\n                 config['overlap']\n                 ).approximate(config['chunk_size'] * processed_chunks)\n    # Time estimation (seconds)\n    .75 / config['chunk_size']  # Hypothetical time per chunk in seconds\n    w = (config.get('num_workers', 16) if config.get('num_workers', 16) is not None else 16 / 10)\n    # Processing_ time - Insights Genration - Insights Query   -   Indexing Time     -    Download Time     -       workers   -   Query Genration time - Ui - Init Db\n    estimated_time = ((8+total_papers*0.012)+(total_chunks/20000) * .005 + (total_chunks/2) * .0003 + total_papers * 2.8 ) / w + (0.25 * config['max_search']) + 6 + 4\n\n    price_per_char = 0.0000012525\n    price_per_t_chunk =  total_chars * price_per_char\n    estimated_price = price_per_t_chunk ** 1.7\n\n    # estimated_price = 0 if query_length &lt; 420 and estimated_price &lt; 5 else estimated_price\n    if estimated_time &lt; 10:\n        estimated_time = 10\n    if estimated_price &lt; .04:\n        estimated_price = .04\n    return round(estimated_time, 2), round(estimated_price, 4)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler.ArXivPDFProcessor.search_and_process_papers","title":"<code>search_and_process_papers(queries)</code>  <code>async</code>","text":"<p>Search for and process papers based on queries.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>list[str]</code> <p>List of search queries</p> required <p>Returns:</p> Type Description <code>list[Paper]</code> <p>List of processed papers</p> Source code in <code>toolboxv2/mods/TruthSeeker/arXivCrawler.py</code> <pre><code>async def search_and_process_papers(self, queries: list[str]) -&gt; list[Paper]:\n    \"\"\"Search for and process papers based on queries.\n\n    Args:\n        queries: List of search queries\n\n    Returns:\n        List of processed papers\n    \"\"\"\n    # Use the new processor to search and process papers\n    unified_papers = await self.processor.search_and_process_papers(queries)\n\n    # Convert UnifiedPaper objects to Paper objects for backward compatibility\n    papers = []\n    for paper in unified_papers:\n        if paper.source == \"arxiv\":\n            # Convert to the old Paper format\n            arxiv_paper = Paper(\n                title=paper.title,\n                authors=paper.authors,\n                summary=paper.summary,\n                url=paper.url,\n                pdf_url=paper.pdf_url,\n                published=paper.published,\n                updated=paper.source_specific_data.get(\"updated\", \"\"),\n                categories=paper.source_specific_data.get(\"categories\", []),\n                paper_id=paper.paper_id\n            )\n            papers.append(arxiv_paper)\n\n    # Update attributes for backward compatibility\n    self.all_ref_papers = self.processor.all_ref_papers\n    self.all_texts_len = self.processor.all_texts_len\n    self.f_texts_len = self.processor.f_texts_len\n\n    return papers\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler.ArXivPDFProcessor.send_status","title":"<code>send_status(step, progress=None, additional_info='')</code>","text":"<p>Send status update via callback.</p> Source code in <code>toolboxv2/mods/TruthSeeker/arXivCrawler.py</code> <pre><code>def send_status(self, step: str, progress: float = None, additional_info: str = \"\"):\n    \"\"\"Send status update via callback.\"\"\"\n    if progress is None:\n        progress = self._update_global_progress()\n    self.callback({\n        \"step\": step,\n        \"progress\": progress,\n        \"info\": additional_info\n    })\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler.main","title":"<code>main(query='Beste strategien in bretspielen sitler von katar')</code>  <code>async</code>","text":"<p>Main execution function</p> Source code in <code>toolboxv2/mods/TruthSeeker/arXivCrawler.py</code> <pre><code>async def main(query: str = \"Beste strategien in bretspielen sitler von katar\"):\n    \"\"\"Main execution function\"\"\"\n    with Spinner(\"Init Isaa\"):\n        tools = get_app(\"ArXivPDFProcessor\", name=None).get_mod(\"isaa\")\n        tools.init_isaa(build=True)\n    processor = ArXivPDFProcessor(query, tools=tools)\n    papers, insights = await processor.process()\n\n    print(\"Generated Insights:\", insights)\n    print(\"Generated Insights_list:\", processor.last_insights_list)\n    kb = tools.get_memory(processor.mem_name)\n    print(await kb.query_concepts(\"AI\"))\n    print(await kb.retrieve(\"Evaluation metrics for assessing AI Agent performance\"))\n    print(kb.concept_extractor.concept_graph.concepts.keys())\n    kb.vis(output_file=\"insights_graph.html\")\n    kb.save(\"mem.plk\")\n    # await get_app(\"ArXivPDFProcessor\", name=None).a_idle()\n    return insights\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui","title":"<code>nGui</code>","text":"<p>import colorsys import json import time from datetime import datetime, timedelta from queue import Queue from typing import Dict, Union, List, Any</p> <p>from fastapi import Request import os import random from threading import Thread, Event</p> <p>import networkx as nx from dataclasses import asdict</p> <p>from toolboxv2 import get_app from toolboxv2.mods.FastApi.fast_nice import register_nicegui</p> <p>import asyncio</p> <p>from nicegui import ui</p> <p>from pathlib import Path import stripe</p> <p>from toolboxv2.mods.TruthSeeker.arXivCrawler import Paper from toolboxv2.mods.isaa.base.AgentUtils import anything_from_str_to_dict</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--set-your-secret-key-use-environment-variables-in-production","title":"Set your secret key (use environment variables in production!)","text":"<p>stripe.api_key = os.getenv('STRIPE_SECRET_KEY', 'sk_test_YourSecretKey')</p> <p>def create_landing_page():     # Set up dynamic background     ui.query(\"body\").style(\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%)\")</p> <pre><code># Main container with enhanced responsive design\nwith ui.column().classes(\n\"w-full max-w-md p-8 rounded-3xl shadow-2xl \"\n\"items-center self-center mx-auto my-8\"\n):\n    # Advanced styling for glass-morphism effect\n    ui.query(\".nicegui-column\").style(\"\"\"\n    background: rgba(255, 255, 255, 0.05);\n    backdrop-filter: blur(12px);\n    border: 1px solid rgba(255, 255, 255, 0.1);\n    transition: all 0.3s ease-in-out;\n    \"\"\")\n\n    # Animated logo/brand icon\n    with ui.element(\"div\").classes(\"animate-fadeIn\"):\n        ui.icon(\"science\").classes(\n        \"text-7xl mb-6 text-primary \"\n        \"transform hover:scale-110 transition-transform\"\n        )\n\n    # Enhanced typography for title\n    ui.label(\"TruthSeeker\").classes(\n    \"text-5xl font-black text-center \"\n    \"text-primary mb-2 animate-slideDown\"\n    )\n\n    # Stylized subtitle with brand message\n    ui.label(\"Precision. Discovery. Insights.\").classes(\n    \"text-xl font-medium text-center \"\n    \"mb-10 animate-fadeIn\"\n    )\n\n    # Button container for consistent spacing\n    ui.button(\n    \"Start Research\",\n    on_click=lambda: ui.navigate.to(\"/open-Seeker.seek\")\n    ).classes(\n    \"w-full px-6 py-4 text-lg font-bold \"\n    \"bg-primary hover:bg-primary-dark \"\n    \"transform hover:-translate-y-0.5 \"\n    \"transition-all duration-300 ease-in-out \"\n    \"rounded-xl shadow-lg animate-slideUp\"\n    )\n\n    # Navigation links container\n    with ui.element(\"div\").classes(\"mt-8 space-y-3 text-center\"):\n        ui.link(\n        \"Demo video\",\n        ).classes(\n        \"block text-lg text-gray-200 hover:text-primary \"\n        \"transition-colors duration-300 animate-fadeIn\"\n        ).on(\"click\", lambda: ui.navigate.to(\"/open-Seeker.demo\"))\n\n        ui.link(\n        \"About Us\",\n        ).classes(\n        \"block text-lg text-gray-400 hover:text-primary \"\n        \"transition-colors duration-300 animate-fadeIn\"\n        ).on(\"click\", lambda: ui.navigate.to(\"/open-Seeker.about\"))\n</code></pre> <p>def create_video_demo():     with ui.card().classes('w-full max-w-3xl mx-auto').style(         'background: var(--background-color); color: var(--text-color)'):         # Video container with responsive aspect ratio         with ui.element('div').classes('relative w-full aspect-video'):             video = ui.video('../api/TruthSeeker/video').classes('w-full h-full object-cover')</p> <pre><code>        # Custom controls overlay\n        with ui.element('div').classes('absolute bottom-0 left-0 right-0 bg-black/50 p-2'):\n            with ui.row().classes('items-center gap-2'):\n                #play_btn = ui.button(icon='play_arrow', on_click=lambda: video.props('playing=true'))\n                #pause_btn = ui.button(icon='pause', on_click=lambda: video.props('playing=false'))\n                ui.slider(min=0, max=100, value=0).classes('w-full').bind_value(video, 'time')\n                #mute_btn = ui.button(icon='volume_up', on_click=lambda: video.props('muted=!muted'))\n                #fullscreen_btn = ui.button(icon='fullscreen', on_click=lambda: video.props('fullscreen=true'))\n\n\n    # Video description\n    ui.markdown('Walkthrough of TruthSeeker features and capabilities.')\n    # Back to Home Button\n    ui.button('Back to Home', on_click=lambda: ui.navigate.to('/open-Seeker')).classes(\n        'mt-6 w-full bg-primary text-white hover:opacity-90'\n    )\n\nreturn video\n</code></pre> <p>def create_about_page():     \"\"\"Create a comprehensive About page for TruthSeeker\"\"\"     with ui.column().classes('w-full max-w-4xl mx-auto p-6'):         # Page Header         ui.label('About TruthSeeker').classes('text-4xl font-bold text-primary mb-6')</p> <pre><code>    # Mission Statement\n    with ui.card().classes('w-full mb-6').style(\n        'background: var(--background-color); color: var(--text-color); padding: 20px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);'\n    ):\n        ui.label('Our Mission').classes('text-2xl font-semibold text-primary mb-4')\n        ui.markdown(\"\"\"\n            TruthSeeker aims to democratize access to scientific knowledge,\n            transforming complex academic research into comprehensible insights.\n            We bridge the gap between raw data and meaningful understanding.\n        \"\"\").classes('text-lg').style('color: var(--text-color);')\n\n    # Core Technologies\n    with ui.card().classes('w-full mb-6').style(\n        'background: var(--background-color); color: var(--text-color); padding: 20px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);'\n    ):\n        ui.label('Core Technologies').classes('text-2xl font-semibold text-primary mb-4')\n        with ui.row().classes('gap-4 w-full'):\n            with ui.column().classes('flex-1 text-center'):\n                ui.icon('search').classes('text-4xl text-primary mb-2')\n                ui.label('Advanced Query Processing').classes('font-bold')\n                ui.markdown('Intelligent algorithms that extract nuanced research insights.').style(\n                    'color: var(--text-color);')\n            with ui.column().classes('flex-1 text-center'):\n                ui.icon('analytics').classes('text-4xl text-primary mb-2')\n                ui.label('Semantic Analysis').classes('font-bold')\n                ui.markdown('Deep learning models for comprehensive research verification.').style(\n                    'color: var(--text-color);')\n            with ui.column().classes('flex-1 text-center'):\n                ui.icon('verified').classes('text-4xl text-primary mb-2')\n                ui.label('Research Validation').classes('font-bold')\n                ui.markdown('Multi-layered verification of academic sources.').style('color: var(--text-color);')\n    # Research Process\n    with ui.card().classes('w-full').style('background: var(--background-color);color: var(--text-color);'):\n        ui.label('Research Discovery Process').classes('text-2xl font-semibold text-primary mb-4')\n        with ui.card().classes('q-pa-md q-mx-auto').style(\n            'max-width: 800px; background: var(--background-color); border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);'\n        ) as card:\n            ui.markdown(\"# Research Workflow\").style(\n                \"color: var(--primary-color); text-align: center; margin-bottom: 20px;\")\n            ui.markdown(\n                \"\"\"\n                Welcome to TruthSeeker\u2019s interactive research assistant. Follow the steps below to transform your initial inquiry into a refined, actionable insight.\n                \"\"\"\n            ).style(\"color: var(--text-color); text-align: center; margin-bottom: 30px;\")\n\n            # The stepper component\n            with ui.stepper().style('background: var(--background-color); color: var(--text-color);') as stepper:\n                # Step 1: Query Initialization\n                with ui.step('Query Initialization'):\n                    ui.markdown(\"### Step 1: Query Initialization\").style(\"color: var(--primary-color);\")\n                    ui.markdown(\n                        \"\"\"\n                        Begin by entering your research question or selecting from popular academic domains.\n                        This sets the direction for our semantic analysis engine.\n                        \"\"\"\n                    ).style(\"color: var(--text-color); margin-bottom: 20px;\")\n                    with ui.stepper_navigation():\n                        ui.button('Next', on_click=stepper.next).props('rounded color=primary')\n\n                # Step 2: Semantic Search\n                with ui.step('Semantic Search'):\n                    ui.markdown(\"### Step 2: Semantic Search\").style(\"color: var(--primary-color);\")\n                    ui.markdown(\n                        \"\"\"\n                        Our advanced algorithms now process your input to generate context-rich queries.\n                        This stage refines the search context by understanding the deeper intent behind your question.\n                        \"\"\"\n                    ).style(\"color: var(--text-color); margin-bottom: 20px;\")\n                    with ui.stepper_navigation():\n                        ui.button('Back', on_click=stepper.previous).props('flat')\n                        ui.button('Next', on_click=stepper.next).props('rounded color=primary')\n\n                # Step 3: Document Analysis\n                with ui.step('Document Analysis'):\n                    ui.markdown(\"### Step 3: Document Analysis\").style(\"color: var(--primary-color);\")\n                    ui.markdown(\n                        \"\"\"\n                        The system then dives into a detailed analysis of academic papers, parsing content to extract key insights and connections.\n                        This ensures that even subtle but crucial information is captured.\n                        \"\"\"\n                    ).style(\"color: var(--text-color); margin-bottom: 20px;\")\n                    with ui.stepper_navigation():\n                        ui.button('Back', on_click=stepper.previous).props('flat')\n                        ui.button('Next', on_click=stepper.next).props('rounded color=primary')\n\n                # Step 4: Insight Generation\n                with ui.step('Insight Generation'):\n                    ui.markdown(\"### Step 4: Insight Generation\").style(\"color: var(--primary-color);\")\n                    ui.markdown(\n                        \"\"\"\n                        Finally, we synthesize the analyzed data into clear, actionable research summaries.\n                        These insights empower you with concise guidance to drive further inquiry or practical application.\n                        \"\"\"\n                    ).style(\"color: var(--text-color); margin-bottom: 20px;\")\n                    with ui.stepper_navigation():\n                        ui.button('Back', on_click=stepper.previous).props('flat')\n\n    # Back to Home Button\n    ui.button('Back to Home', on_click=lambda: ui.navigate.to('/open-Seeker')).classes(\n        'mt-6 w-full bg-primary text-white hover:opacity-90'\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--dummy-implementierung-fur-get_tools","title":"Dummy-Implementierung f\u00fcr get_tools()","text":"<p>def get_tools():     \"\"\"     Hier solltest du dein richtiges Werkzeug-Objekt zur\u00fcckliefern.     In diesem Beispiel gehen wir davon aus, dass du \u00fcber eine Funktion wie get_app verf\u00fcgst.     \"\"\"     return get_app(\"ArXivPDFProcessor\", name=None).get_mod(\"isaa\")</p> <p>def create_graph_tab(processor_instance: Dict, graph_ui: ui.element, main_ui: ui.element):     \"\"\"Create and update the graph visualization\"\"\"</p> <pre><code># Get HTML graph from processor\n_html_content = processor_instance[\"instance\"].tools.get_memory(processor_instance[\"instance\"].mem_name)\nhtml_content = \"\" if isinstance(_html_content, list) else _html_content.vis(get_output_html=True)\n\n# Ensure static directory exists\nstatic_dir = Path('dist/static')\nstatic_dir.mkdir(exist_ok=True)\n\n# Save HTML to static file\ngraph_file = static_dir / f'graph{processor_instance[\"instance\"].mem_name}.html'\n# Save HTML to static file with added fullscreen functionality\n\n# Add fullscreen JavaScript\ngraph_file.write_text(html_content, encoding='utf-8')\n\nwith main_ui:\n    # Clear existing content except fullscreen button\n    graph_ui.clear()\n\n    with graph_ui:\n        ui.html(f\"\"\"\n\n            &lt;iframe\n                 src=\"/static/graph{processor_instance[\"instance\"].mem_name}.html\"\n                style=\"width: 100%; height: 800px; border: none; background: #1a1a1a;\"\n                &gt;\n            &lt;/iframe&gt;\n        \"\"\").classes('w-full h-full')\n</code></pre> <p>is_init = [False]</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui---database-setup-","title":"--- Database Setup ---","text":"<p>def get_db():     db = get_app().get_mod(\"DB\")     if not is_init[0]:         is_init[0] = True         db.edit_cli(\"LD\")         db.initialize_database()     return db</p> <p>import pickle</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui---session-state-management-","title":"--- Session State Management ---","text":"<p>def get_user_state(session_id: str, is_new=False) -&gt; dict:     db = get_db()     state_ = {         'balance': .5,         'last_reset': datetime.utcnow().isoformat(),         'research_history': [],         'payment_id': '',     }     if session_id is None:         state_['balance'] *= -1         if is_new:             return state_, True         return state_     state = db.get(f\"TruthSeeker::session:{session_id}\")     if state.get() is None:         state = state_         if is_new:             return state_, True     else:         try:             state = pickle.loads(state.get())         except Exception as e:             print(e)             state = {         'balance': 0.04,         'last_reset': datetime.utcnow().isoformat(),         'research_history': [\"Sorry we had an error recreating your state\"],         'payment_id': '',             }             if is_new:                 return state, True     if is_new:         return state, False     return state</p> <p>def save_user_state(session_id: str, state: dict):     db = get_db()     print(\"Saving state\")     db.set(f\"TruthSeeker::session:{session_id}\", pickle.dumps(state)).print()</p> <p>def delete_user_state(session_id: str):     db = get_db()     print(\"Saving state\")     db.delete(f\"TruthSeeker::session:{session_id}\").print()</p> <p>def reset_daily_balance(state: dict, valid=False) -&gt; dict:     now = datetime.utcnow()     last_reset = datetime.fromisoformat(state.get('last_reset', now.isoformat()))     if now - last_reset &gt; timedelta(hours=24):         state['balance'] = max(state.get('balance', 1.6 if valid else 0.5), 1.6 if valid else 0.5)         state['last_reset'] = now.isoformat()     return state</p> class MemoryResultsDisplay <p>def init(self, results: List[Dict[str, Any]], main_ui: ui.element):     self.results = results     self.main_ui = main_ui     self.setup_ui()</p> <p>def setup_ui(self):     \"\"\"Set up the main UI for displaying memory results\"\"\"     with self.main_ui:         self.main_ui.clear()         with ui.column().classes('w-full'):             for mem_result in self.results:                 self.create_memory_card(mem_result)</p> <p>def create_memory_card(self, mem_result: Dict[str, Any]):     \"\"\"Create a card for each memory result\"\"\"     result = mem_result.get(\"result\", {})     with self.main_ui:         if isinstance(result, dict):             self.display_dict_result(result)         elif hasattr(result, 'overview'):  # Assuming RetrievalResult type             self.display_retrieval_result(result)         else:             ui.label(\"Unsupported result type\").classes('--text-color:error')</p> <p>def display_dict_result(self, result: Dict[str, Any]):     \"\"\"Display dictionary-based result with collapsible sections\"\"\"     # Summary Section     summary = result.get(\"summary\", {})     if isinstance(summary, str):         try:             summary = json.loads(summary[:-1])         except json.JSONDecodeError:             summary = {\"error\": \"Could not parse summary\"}</p> <pre><code># Raw Results Section\nraw_results = result.get(\"raw_results\", {})\nif isinstance(raw_results, str):\n    try:\n        raw_results = json.loads(raw_results[:-1])\n    except json.JSONDecodeError:\n        raw_results = {\"error\": \"Could not parse raw results\"}\n\n# Metadata Section\nmetadata = result.get(\"metadata\", {})\nwith self.main_ui:\n    # Collapsible Sections\n    with ui.column().classes('w-full space-y-2').style(\"max-width: 100%;\"):\n        # Summary Section\n        with ui.expansion('Summary', icon='description').classes('w-full') as se:\n            self.display_nested_data(summary, main_ui=se)\n\n        # Raw Results Section\n        with ui.expansion('Raw Results', icon='work').classes('w-full') as re:\n            self.display_nested_data(raw_results, main_ui=re)\n\n        # Metadata Section\n        if metadata:\n            with ui.expansion('Metadata', icon='info').classes('w-full'):\n                ui.markdown(f\"```json\n</code></pre> <p>{json.dumps(metadata, indent=2)} ```\").style(\"max-width: 100%;\")</p> <pre><code>def display_retrieval_result(self, result):\n    \"\"\"Display retrieval result with detailed sections\"\"\"\n    with self.main_ui:\n        with ui.column().classes('w-full space-y-4').style(\"max-width: 100%;\"):\n            # Overview Section\n            with ui.expansion('Overview', icon='visibility').classes('w-full') as ov:\n                for overview_item in result.overview:\n                    if isinstance(overview_item, str):\n                        overview_item = json.loads(overview_item)\n                    self.display_nested_data(overview_item, main_ui=ov)\n\n            # Details Section\n            with ui.expansion('Details', icon='article').classes('w-full'):\n                for chunk in result.details:\n                    with ui.card().classes('w-full p-3 mb-2').style(\"background: var(--background-color)\"):\n                        ui.label(chunk.text).classes('font-medium mb-2 --text-color:secondary')\n\n                        with ui.row().classes('w-full justify-between').style(\"background: var(--background-color)\"):\n                            ui.label(f\"Embedding Shape: {chunk.embedding.shape}\").classes('text-sm')\n                            ui.label(f\"Content Hash: {chunk.content_hash}\").classes('text-sm')\n\n                        if chunk.cluster_id is not None:\n                            ui.label(f\"Cluster ID: {chunk.cluster_id}\").classes('text-sm')\n\n            # Cross References Section\n            with ui.expansion('Cross References', icon='link').classes('w-full'):\n                for topic, chunks in result.cross_references.items():\n                    with ui.card().classes('w-full p-3 mb-2').style(\"background: var(--background-color)\"):\n                        ui.label(topic).classes('font-semibold mb-2 --text-color:secondary')\n                        for chunk in chunks:\n                            ui.label(chunk.text).classes('text-sm mb-1')\n\ndef display_nested_data(self, data: Union[Dict, List], indent: int = 0, main_ui=None):\n    \"\"\"Recursively display nested dictionary or list data\"\"\"\n    with (self.main_ui if main_ui is None else main_ui):\n        if isinstance(data, dict):\n            with ui.column().classes(f'ml-{indent * 2}').style(\"max-width: 100%;\"):\n                for key, value in data.items():\n                    with ui.row().classes('items-center'):\n                        ui.label(f\"{key}:\").classes('font-bold mr-2 --text-color:primary')\n                        if isinstance(value, list):\n                            if key == \"main_chunks\":\n                                continue\n                            self.display_nested_data(value, indent + 1, main_ui=main_ui)\n                        if isinstance(value, dict):\n                            ui.markdown(f\"```json\n</code></pre> <p>{json.dumps(value, indent=2)} <code>\").classes(\"break-words w-full\").style(\"max-width: 100%;\")                             else:                                 ui.label(str(value)).classes('--text-color:secondary')             elif isinstance(data, list):                 with ui.column().classes(f'ml-{indent * 2}').style(\"max-width: 100%;\"):                     for item in data:                         if isinstance(item, str):                             item = json.loads(item)                         if isinstance(item, list):                             self.display_nested_data(item, indent + 1, main_ui=main_ui)                         if isinstance(item, dict):                             ui.markdown(f\"</code>json {json.dumps(item, indent=2)} ```\").classes(\"break-words w-full\").style(\"max-width: 100%;\")                         else:                             ui.label(str(item)).classes('--text-color:secondary')</p> <p>def create_followup_section(processor_instance: Dict, main_ui: ui.element, session_id, balance):     main_ui.clear()     with main_ui:         ui.label(\"Query Interface  (1ct)\").classes(\"text-xl font-semibold mb-4\")</p> <pre><code>    # Container for query inputs\n    query_container = ui.column().classes(\"w-full gap-4\")\n    query = \"\"  # Store references to query inputs\n    # Query parameters section\n    with ui.expansion(\"Query Parameters\", icon=\"settings\").classes(\"w-full\") as query_e:\n        with ui.grid(columns=2).classes(\"w-full gap-4\"):\n            k_input = ui.number(\"Results Count (k)\", value=2, min=1, max=20)\n            min_sim = ui.number(\"Min Similarity\", value=.3, min=0, max=1, step=0.1)\n            cross_depth = ui.number(\"Cross Reference Depth\", value=2, min=1, max=5)\n            max_cross = ui.number(\"Max Cross References\", value=10, min=1, max=20)\n            max_sent = ui.number(\"Max Sentences\", value=10, min=1, max=50)\n            unified = ui.switch(\"Unified Retrieve (+3ct)\", value=True)\n\n    # Results display\n    with ui.element(\"div\").classes(\"w-full mt-4\") as results_display:\n        pass\n    results_display = results_display\n    with query_container:\n        query_input = ui.input(\"Query\", placeholder=\"Enter your query...\")                 .classes(\"w-full\")\n    # Control buttons\n    with ui.row().classes(\"w-full gap-4 mt-4\"):\n        ui.button(\"Execute Query\", on_click=lambda: asyncio.create_task(execute_query()))                 .classes(\"bg-green-600 hover:bg-green-700\")\n        ui.button(\"Clear Results\", on_click=lambda: results_display.clear())                 .classes(\"bg-red-600 hover:bg-red-700\")\nquery_input = query_input\n\nasync def execute_query():\n    \"\"\"Execute a single query with parameters\"\"\"\n    nonlocal query_input, results_display, main_ui\n    try:\n        query_text = query_input.value\n        if not query_text.strip():\n            with main_ui:\n                ui.notify(\"No Input\", type=\"warning\")\n            return \"\"\n\n        if not processor_instance.get(\"instance\"):\n            with main_ui:\n                ui.notify(\"No active processor instance\", type=\"warning\")\n            return\n        # Collect parameters\n        params = {\n            \"k\": int(k_input.value),\n            \"min_similarity\": min_sim.value,\n            \"cross_ref_depth\": int(cross_depth.value),\n            \"max_cross_refs\": int(max_cross.value),\n            \"max_sentences\": int(max_sent.value),\n            \"unified\": unified.value\n        }\n        # Construct query parameters\n        query_params = {\n            \"k\": params[\"k\"],\n            \"min_similarity\": params[\"min_similarity\"],\n            \"cross_ref_depth\": params[\"cross_ref_depth\"],\n            \"max_cross_refs\": params[\"max_cross_refs\"],\n            \"max_sentences\": params[\"max_sentences\"]\n        }\n\n        # Execute query\n        results = await processor_instance[\"instance\"].extra_query(\n            query=query_text,\n            query_params=query_params,\n            unified_retrieve=params[\"unified\"]\n        )\n        print(\"results\",results)\n        s = get_user_state(session_id)\n        s['balance'] -= .04 if unified.value else .01\n        save_user_state(session_id, s)\n        with main_ui:\n            balance.set_text(f\"Balance: {s['balance']:.2f}\u20ac\")\n        # Format results\n        with main_ui:\n            with results_display:\n                MemoryResultsDisplay(results, results_display)\n\n    except Exception as e:\n        return f\"Error executing query: {str(e)}\n</code></pre> <p>\"</p> <pre><code># Add initial query input\n</code></pre> <p>online_states = [0] def create_research_interface(Processor):</p> <pre><code>def helpr(request, session: dict):\n\n    state = {'balance':0, 'research_history': []}\n    main_ui = None\n    with ui.column().classes(\"w-full max-w-6xl mx-auto p-6 space-y-6\") as loading:\n        ui.spinner(size='lg')\n        ui.label('Initializing...').classes('ml-2')\n\n    # Container for main content (initially hidden)\n    content = ui.column().classes('hidden')\n\n    # Extract session data before spawning thread\n    session_id = session.get('ID')\n    session_id_h = session.get('IDh')\n    session_rid = request.row.query_params.get('session_id') if hasattr(request, 'row') else request.query_params.get('session_id')\n    session_valid = session.get('valid')\n\n    # Thread communication\n    result_queue = Queue()\n    ready_event = Event()\n\n    def init_background():\n        nonlocal session_id, session_id_h, session_rid, session_valid\n        try:\n            # Original initialization logic\n            _state, is_new = get_user_state(session_id, is_new=True)\n\n            if is_new and session_id_h != \"#0\":\n                _state = get_user_state(session_id_h)\n                save_user_state(session_id, _state)\n                delete_user_state(session_id_h)\n            if session_rid:\n                state_: dict\n                state_, is_new_ = get_user_state(session_rid, is_new=True)\n                if not is_new_:\n                    _state = state_.copy()\n                    state_['payment_id'] = ''\n                    state_['last_reset'] = datetime.utcnow().isoformat()\n                    state_['research_history'] = state_['research_history'][:3]\n                    state_['balance'] = 0\n                    save_user_state(session_id, _state)\n            _state = reset_daily_balance(_state, session_valid)\n            save_user_state(session_id, _state)\n\n            # Send result back to main thread\n            result_queue.put(_state)\n            ready_event.set()\n        except Exception as e:\n            result_queue.put(e)\n            ready_event.set()\n\n        # Start background initialization\n\n    Thread(target=init_background).start()\n\n    def check_ready():\n        nonlocal state\n        if ready_event.is_set():\n            result = result_queue.get()\n\n            # Check if initialization failed\n            if isinstance(result, Exception):\n                loading.clear()\n                with loading:\n                    ui.label(f\"Error during initialization: {str(result)}\").classes('text-red-500')\n                return\n\n            # Get state and build main UI\n            state = result\n            loading.classes('hidden')\n            content.classes(remove='hidden')\n            main_ui.visible = True\n            with main_ui:\n                balance.set_text(f\"Balance: {state['balance']:.2f}\u20ac\")\n                show_history()\n            return  # Stop the timer\n\n        # Check again in 100ms\n        ui.timer(0.1, check_ready, once=True)\n\n    # Start checking for completion\n    check_ready()\n\n    # Wir speichern die aktive Instanz, damit Follow-Up Fragen gestellt werden k\u00f6nnen\n    processor_instance = {\"instance\": None}\n\n    # UI-Elemente als Platzhalter; wir definieren sie sp\u00e4ter in der UI und machen sie so\n    # in den Callback-Funktionen \u00fcber \"nonlocal\" verf\u00fcgbar.\n    overall_progress = None\n    status_label = None\n    results_card = None\n    summary_content = None\n    analysis_content = None\n    references_content = None\n    followup_card = None\n    research_card = None\n    config_cart = None\n    progress_card = None\n    balance = None\n    graph_ui = None\n\n    sr_button = None\n    r_button = None\n    r_text = None\n\n\n    # Global config storage with default values\n    config = {\n        'chunk_size': 21000,\n        'overlap': 600,\n        'num_search_result_per_query': 3,\n        'max_search': 3,\n        'num_workers': None\n    }\n\n    def update_estimates():\n        \"\"\"\n        Dummy estimation based on query length and configuration.\n        (Replace with your own non-linear formula if needed.)\n        \"\"\"\n        query_text = query.value or \"\"\n        query_length = len(query_text)\n        # For example: estimated time scales with chunk size and query length.\n        estimated_time ,estimated_price = Processor.estimate_processing_metrics(query_length, **config)\n        estimated_time *= max(1, online_states[0] * 6)\n        if processor_instance[\"instance\"] is not None:\n            estimated_price += .25\n        if estimated_time &lt; 60:\n            time_str = f\"~{int(estimated_time)}s\"\n        elif estimated_time &lt; 3600:\n            minutes = estimated_time // 60\n            seconds = estimated_time % 60\n            time_str = f\"~{int(minutes)}m {int(seconds)}s\"\n        else:\n            hours = estimated_time // 3600\n            minutes = (estimated_time % 3600) // 60\n            time_str = f\"~{int(hours)}h {int(minutes)}m\"\n        with main_ui:\n            query_length_label.set_text(f\"Total Papers: {config['max_search']*config['num_search_result_per_query']}\")\n            time_label.set_text(f\"Processing Time: {time_str}\")\n            price_label.set_text(f\"Price: {estimated_price:.2f}\u20ac\")\n\n        return estimated_price\n\n    def on_config_change(event):\n        \"\"\"\n        Update the global config based on input changes and recalc estimates.\n        \"\"\"\n        try:\n            config['chunk_size'] = int(chunk_size_input.value)\n        except ValueError:\n            pass\n        try:\n            config['overlap'] = int(overlap_input.value)\n            if config['overlap'] &gt; config['chunk_size'] / 4:\n                config['overlap'] = int(config['chunk_size'] / 4)\n                with main_ui:\n                    overlap_input.value = config['overlap']\n        except ValueError:\n            pass\n        try:\n            config['num_search_result_per_query'] = int(num_search_result_input.value)\n        except ValueError:\n            pass\n        try:\n            config['max_search'] = int(max_search_input.value)\n        except ValueError:\n            pass\n        try:\n            config['num_workers'] = int(num_workers_input.value) if num_workers_input.value != 0 else None\n        except ValueError:\n            config['num_workers'] = None\n\n        update_estimates()\n\n    def on_query_change():\n        update_estimates()\n\n    # Callback, der vom Processor (\u00fcber processor_instance.callback) aufgerufen wird.\n    def update_status(data: dict):\n        nonlocal overall_progress, status_label\n        if not data:\n            return\n        # Aktualisiere den Fortschrittsbalken und den aktuellen Schritt (wenn vorhanden)\n        with main_ui:\n            if isinstance(data, dict):\n                progress = data.get(\"progress\", 0)\n                step = data.get(\"step\", \"Processing...\")\n                overall_progress.value =round( progress ,2) # nicegui.linear_progress erwartet einen Wert zwischen 0 und 1\n                status_label.set_text(f\"{step} {data.get('info','')}\")\n            else:\n                status_label.set_text(f\"{data}\")\n\n    def start_search():\n        nonlocal balance\n\n        async def helper():\n            nonlocal processor_instance, overall_progress, status_label, results_card,                     summary_content, analysis_content,config, references_content, followup_card,sr_button,r_button,r_text\n\n            try:\n                if not validate_inputs():\n                    with main_ui:\n                        state['balance'] += est_price\n                        save_user_state(session_id, state)\n                        balance.set_text(f\"Balance: {state['balance']:.2f}\u20ac\")\n                    return\n                reset_interface()\n                show_progress_indicators()\n\n                query_text = query.value.strip()\n                # Erzeuge das \"tools\"-Objekt (abh\u00e4ngig von deiner konkreten Implementation)\n                tools = get_tools()\n                with main_ui:\n                    research_card.visible = False\n                    config_cart.visible = False\n                    config_section.visible = False\n                    query.set_value(\"\")\n                # Direkt instanziieren: Eine neue ArXivPDFProcessor-Instanz\n                if processor_instance[\"instance\"] is not None:\n                    processor = processor_instance[\"instance\"]\n                    processor.chunk_size = config['chunk_size']\n                    processor.overlap = config['overlap']\n                    processor.num_search_result_per_query = config['num_search_result_per_query']\n                    processor.max_search = config['max_search']\n                    processor.num_workers = config['num_workers']\n                    papers, insights = await processor.process(query_text)\n                else:\n                    processor = Processor(query_text, tools=tools, **config)\n                # Setze den Callback so, dass Updates in der GUI angezeigt werden\n                    processor.callback = update_status\n                    processor_instance[\"instance\"] = processor\n                    papers, insights = await processor.process()\n\n                update_results({\n                    \"papers\": papers,\n                    \"insights\": insights\n                })\n                with main_ui:\n                    research_card.visible = True\n                    config_cart.visible = True\n                    show_history()\n\n            except Exception as e:\n                import traceback\n\n                with main_ui:\n                    update_status({\"progress\": 0, \"step\": \"Error\", \"info\": str(e)})\n                    state['balance'] += est_price\n                    save_user_state(session_id, state)\n                    balance.set_text(f\"Balance: {state['balance']:.2f}\u20ac\")\n                    ui.notify(f\"Error {str(e)})\", type=\"negative\")\n                    research_card.visible = True\n                    config_cart.visible = True\n                    config_section.visible = True\n                print(traceback.format_exc())\n\n        def target():\n            get_app().run_a_from_sync(helper, )\n\n        est_price = update_estimates()\n        if est_price &gt; state['balance']:\n            with main_ui:\n                ui.notify(f\"Insufficient balance. Need \u20ac{est_price:.2f}\", type='negative')\n        else:\n            state['balance'] -= est_price\n            save_user_state(session_id, state)\n            with main_ui:\n                online_states[0] += 1\n                balance.set_text(f\"Balance: {state['balance']:.2f}\u20ac Running Queries: {online_states[0]}\")\n\n            Thread(target=target, daemon=True).start()\n            with main_ui:\n                online_states[0] -= 1\n                balance.set_text(f\"Balance: {get_user_state(session_id)['balance']:.2f}\u20ac\")\n\n\n    def show_history():\n        with config_cart:\n            for idx, entry in enumerate(state['research_history']):\n                with ui.card().classes(\"w-full backdrop-blur-lg bg-white/10 p-4\"):\n                    ui.label(entry['query']).classes('text-sm')\n                    ui.button(\"Open\").on_click(lambda _, i=idx: load_history(i))\n\n    def reset():\n        nonlocal processor_instance, results_card, followup_card, sr_button, r_button, r_text\n        processor_instance[\"instance\"] = None\n        show_progress_indicators()\n        with main_ui:\n            config_cart.visible = False\n            config_section.visible = False\n            followup_card.visible = False\n            results_card.visible = False\n            r_button.visible = False\n            r_text.set_text(\"Research Interface\")\n            sr_button.set_text(\"Start Research\")\n        start_search()\n    # UI-Aufbau\n\n    with ui.column().classes(\"w-full max-w-6xl mx-auto p-6 space-y-6\") as main_ui:\n        balance = ui.label(f\"Balance: {state['balance']:.2f}\u20ac\").classes(\"text-s font-semibold\")\n\n        config_cart = config_cart\n\n        # --- Research Input UI Card ---\n        with ui.card().classes(\"w-full backdrop-blur-lg bg-white/10 p-4\") as research_card:\n            r_text = ui.label(\"Research Interface\").classes(\"text-3xl font-bold mb-4\")\n\n            # Query input section with auto-updating estimates\n            query = ui.input(\"Research Query\",\n                                placeholder=\"Gib hier deine Forschungsfrage ein...\",\n                                value=\"\")                     .classes(\"w-full min-h-[100px]\")                     .on('change', lambda e: on_query_change()).style(\"color: var(--text-color)\")\n\n            # --- Action Buttons ---\n            with ui.row().classes(\"mt-4\"):\n                sr_button =ui.button(\"Start Research\", on_click=start_search)                         .classes(\"bg-blue-600 hover:bg-blue-700 py-3 rounded-lg\")\n                ui.button(\"toggle config\",\n                          on_click=lambda: setattr(config_section, 'visible', not config_section.visible) or show_progress_indicators()).style(\n                    \"color: var(--text-color)\")\n                r_button = ui.button(\"Start new Research\",\n                          on_click=reset).style(\n                    \"color: var(--text-color)\")\n        sr_button = sr_button\n        r_button = r_button\n        r_button.visible = False\n        research_card = research_card\n\n        # --- Options Cart / Configurations ---\n        with ui.card_section().classes(\"w-full backdrop-blur-lg bg-white/10 hidden\") as config_section:\n            ui.separator()\n            ui.label(\"Configuration Options\").classes(\"text-xl font-semibold mt-4 mb-2\")\n            with ui.row():\n                chunk_size_input = ui.number(label=\"Chunk Size\",\n                                             value=config['chunk_size'], format='%.0f', max=64_000, min=1000,\n                                             step=100)                         .on('change', on_config_change).style(\"color: var(--text-color)\")\n                overlap_input = ui.number(label=\"Overlap\",\n                                          value=config['overlap'], format='%.0f', max=6400, min=100, step=50)                         .on('change', on_config_change).style(\"color: var(--text-color)\")\n\n            with ui.row():\n                num_search_result_input = ui.number(label=\"Results per Query\",\n                                                    value=config['num_search_result_per_query'], format='%.0f',\n                                                    min=1, max=100, step=1)                         .on('change', on_config_change).style(\"color: var(--text-color)\")\n                max_search_input = ui.number(label=\"Max Search Queries\",\n                                             value=config['max_search'], format='%.0f', min=1, max=100, step=1)                         .on('change', on_config_change).style(\"color: var(--text-color)\")\n                num_workers_input = ui.number(label=\"Number of Workers (leave empty for default)\",\n                                              value=0, format='%.0f', min=0, max=32, step=1)                         .on('change', on_config_change).style(\"color: var(--text-color)\")\n        config_section = config_section\n        config_section.visible = False\n        # --- Ergebnisse anzeigen ---\n        with ui.card().classes(\"w-full backdrop-blur-lg p-4 bg-white/10\") as results_card:\n            ui.label(\"Research Results\").classes(\"text-xl font-semibold mb-4\")\n            with ui.tabs() as tabs:\n                ui.tab(\"Summary\")\n                ui.tab(\"References\")\n                ui.tab(\"SystemStates\")\n            with ui.tab_panels(tabs, value=\"Summary\").classes(\"w-full\").style(\"background-color: var(--background-color)\"):\n                with ui.tab_panel(\"Summary\"):\n                    summary_content = ui.markdown(\"\").style(\"color : var(--text-color)\")\n                with ui.tab_panel(\"References\"):\n                    references_content = ui.markdown(\"\").style(\"color : var(--text-color)\")\n                with ui.tab_panel(\"SystemStates\"):\n                    analysis_content = ui.markdown(\"\").style(\"color : var(--text-color)\")\n\n\n        # Ergebnisse sichtbar machen, sobald sie vorliegen.\n        results_card = results_card\n        results_card.visible = False\n\n        # --- Follow-Up Bereich mit mehrfachen Folgefragen und Suchparametern ---\n        with ui.card().classes(\"w-full backdrop-blur-lg bg-white/10 p-4 hidden\") as followup_card:\n            pass\n\n        # Zugriff auf followup_card (falls sp\u00e4ter ben\u00f6tigt)\n        followup_card = followup_card\n        followup_card.visible = False\n\n        # --- Fortschrittsanzeige ---\n        with ui.card().classes(\"w-full backdrop-blur-lg bg-white/10 p-4\") as progress_card:\n            with ui.row():\n                ui.label(\"Research Progress\").classes(\"text-xl font-semibold mb-4\")\n                query_length_label = ui.label(\"\").classes(\"mt-6 hover:text-primary transition-colors duration-300\")\n                time_label = ui.label(\"Time: ...\").classes(\"mt-6 hover:text-primary transition-colors duration-300\")\n                price_label = ui.label(\"Price: ...\").classes(\n                    \"mt-6 hover:text-primary transition-colors duration-300\")\n\n            overall_progress = ui.linear_progress(0).classes(\"w-full mb-4\")\n            status_label = ui.label(\"Warte auf Start...\").classes(\"text-base\")\n        # Wir merken uns progress_card, falls wir ihn zur\u00fccksetzen wollen.\n        progress_card = progress_card\n\n        query_length_label = query_length_label\n        time_label = time_label\n        price_label = price_label\n\n        with ui.card().classes(\"w-full backdrop-blur-lg bg-white/10 p-4\") as config_cart:\n            # --- Process Code Section ---\n            # --- Estimated Time and Price ---\n            # ui.label(\"History\").classes(\"text-xl font-semibold mt-4 mb-2\")\n            ui.label('Research History').classes('text-xl p-4')\n            show_history()\n\n        ui.button('Add Credits', on_click=lambda: balance_overlay(session_id)).props('icon=paid')\n        ui.label('About TruthSeeker').classes(\n            'mt-6 text-gray-500 hover:text-primary '\n            'transition-colors duration-300'\n        ).on('click', lambda: ui.navigate.to('/open-Seeker.about', new_tab=True))\n\n        with ui.element('div').classes(\"w-full\").style(\"white:100%; height:100%\") as graph_ui:\n            pass\n\n        with ui.card().classes(\"w-full p-4\").style(\"background-color: var(--background-color)\"):\n            ui.label(\"Private Session link (restore the session on a different device)\")\n            base_url = f'https://{os.getenv(\"HOSTNAME\")}/gui/open-Seeker.seek' if not 'localhost' in os.getenv(\"HOSTNAME\") else 'http://localhost:5000/gui/open-Seeker.seek'\n            ui.label(f\"{base_url}?session_id={session_id}\").style(\"white:100%\")\n            ui.label(\"Changes each time!\")\n\n        graph_ui = graph_ui\n        graph_ui.visible = False\n    main_ui = main_ui\n    main_ui.visible = False\n\n    # --- Hilfsfunktionen ---\n    def validate_inputs() -&gt; bool:\n        if not query.value.strip():\n            with main_ui:\n                ui.notify(\"Bitte gib eine Forschungsfrage ein.\", type=\"warning\")\n            return False\n        return True\n\n    def reset_interface():\n        nonlocal overall_progress, status_label, results_card, followup_card\n        overall_progress.value = 0\n        with main_ui:\n            status_label.set_text(\"Research startet...\")\n        # Ergebnisse und Follow-Up Bereich verstecken\n        results_card.visible = False\n        followup_card.visible = False\n        graph_ui.visible = False\n\n    def show_progress_indicators():\n        nonlocal progress_card\n        progress_card.visible = True\n\n    def update_results(data: dict, save=True):\n        nonlocal summary_content, analysis_content, references_content, results_card,                followup_card,graph_ui, r_button, r_text, sr_button\n        with main_ui:\n            r_button.visible = True\n            r_text.set_text(\"Add to current Results or press 'Start new Research'\")\n            sr_button.set_text(\"Add to current Results\")\n        # Handle papers (1-to-1 case)\n        papers = data.get(\"papers\", [])\n        if not isinstance(papers, list):\n            papers = [papers]\n\n        # Get insights\n        insights = data.get(\"insights\", [])\n\n        if save:\n            history_entry = data.copy()\n            history_entry['papers'] = [paper.model_dump_json() for paper in papers]\n            if processor_instance is not None and processor_instance['instance'] is not None:\n                history_entry[\"mam_name\"] = processor_instance['instance'].mem_name\n                history_entry[\"query\"] = processor_instance['instance'].query\n\n                history_entry[\"processor_memory\"] = processor_instance['instance'].tools.get_memory(\n\n                ).save_memory(history_entry[\"mam_name\"], None)\n            state['research_history'].append(history_entry)\n            save_user_state(session_id, state)\n        else:\n            papers = [Paper(**json.loads(paper)) for paper in papers]\n        create_followup_section(processor_instance, followup_card, session_id, balance)\n        with main_ui:\n            progress_card.visible = False\n            # Build summary from insights\n            summaries = []\n            for insight in insights:\n                if 'result' in insight and 'summary' in insight['result']:\n                    if isinstance(insight['result']['summary'], str):\n                        # print(insight['result']['summary'], \"NEXT\", json.loads(insight['result']['summary'][:-1]),\"NEXT22\",  type(json.loads(insight['result']['summary'][:-1])))\n                        insight['result']['summary'] = json.loads(insight['result']['summary'][:-1])\n                    main_summary = insight['result']['summary'].get('main_summary', '')\n                    if main_summary:\n                        summaries.append(main_summary)\n            summary_text = \"\n</code></pre> <p>\".join(summaries) if summaries else \"No summary available.\"                 summary_content.set_content(f\"# Research Summary</p> <p>{summary_text}\")</p> <pre><code>            # Analysis section (unchanged if processor details haven't changed)\n            if processor_instance[\"instance\"] is not None:\n                inst = processor_instance[\"instance\"]\n                analysis_md = (\n                    f\"# Analysis\n</code></pre> <p>\"                         f\"- query: {inst.query} \"                         f\"- chunk_size: {inst.chunk_size} \"                         f\"- overlap: {inst.overlap} \"                         f\"- max_workers: {inst.max_workers} \"                         f\"- num_search_result_per_query: {inst.nsrpq} \"                         f\"- max_search: {inst.max_search} \"                         f\"- download_dir: {inst.download_dir} \"                         f\"- mem_name: {inst.mem_name} \"                         f\"- current_session: {inst.current_session} \"                         f\"- all_ref_papers: {inst.all_ref_papers} \"                         f\"- all_texts_len: {inst.all_texts_len} \"                         f\"- final_texts_len: {inst.f_texts_len} \"                         f\"- num_workers: {inst.num_workers}\"                     )                     analysis_content.set_content(analysis_md)</p> <pre><code>            # References and Insights section\n            references_md = \"# References\n</code></pre> <p>\"                 # Add papers                 references_md += \" \".join(                     f\"- ({i}) {getattr(paper, 'title', 'Unknown Title')}})\"                     for i, paper in enumerate(papers)                 )</p> <pre><code>            # Add detailed insights\n            references_md += \"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--insights","title":"Insights","text":"<p>\"                 for i, insight in enumerate(insights):                     print(insight)                     result = insight.get('result', {})                     summary = result.get('summary', {})</p> <pre><code>                if isinstance(summary, str):\n                    summary = json.loads(summary)\n\n                # Main summary\n                references_md += f\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--insight","title":"Insight","text":"<p>\"                     references_md += f\"### Main Summary {summary.get('main_summary', 'No summary available.')} \"</p> <pre><code>                # Concept Analysis\n                concept_analysis = summary.get('concept_analysis', {})\n                if concept_analysis:\n                    references_md += \"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--concept-analysis","title":"Concept Analysis","text":"<p>\"                         references_md += \"#### Key Concepts - \" + \" - \".join(                             concept_analysis.get('key_concepts', [])) + \" \"                         references_md += \"</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--relationships","title":"Relationships","text":"<ul> <li>\" + \"</li> <li>\".join(                             concept_analysis.get('relationships', [])) + \" \"                         references_md += \"</li> </ul>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--importance-hierarchy","title":"Importance Hierarchy","text":"<ul> <li>\" + \"</li> <li> <p>\".join(                             concept_analysis.get('importance_hierarchy', [])) + \" \"</p> <pre><code>            # Topic Insights\n            topic_insights = summary.get('topic_insights', {})\n            if topic_insights:\n                references_md += \"\n</code></pre> </li> </ul>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--topic-insights","title":"Topic Insights","text":"<p>\"                     references_md += \"#### Primary Topics - \" + \" - \".join(                         topic_insights.get('primary_topics', [])) + \" \"                     references_md += \"</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--cross-references","title":"Cross References","text":"<ul> <li>\" + \"</li> <li>\".join(                         topic_insights.get('cross_references', [])) + \" \"                     references_md += \"</li> </ul>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--knowledge-gaps","title":"Knowledge Gaps","text":"<ul> <li>\" + \"</li> <li> <p>\".join(                         topic_insights.get('knowledge_gaps', [])) + \" \"</p> <pre><code>        # Relevance Assessment\n        relevance = summary.get('relevance_assessment', {})\n        if relevance:\n            references_md += \"\n</code></pre> </li> </ul> <p>return helpr</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--relevance-assessment","title":"Relevance Assessment","text":"<p>\"                 references_md += f\"- Query Alignment: {relevance.get('query_alignment', 'N/A')} \"                 references_md += f\"- Confidence Score: {relevance.get('confidence_score', 'N/A')} \"                 references_md += f\"- Coverage Analysis: {relevance.get('coverage_analysis', 'N/A')} \"</p> <pre><code>    references_content.set_content(references_md)\n\n    # nx concpts graph\n    if processor_instance[\"instance\"] is not None:\n        create_graph_tab(\n            processor_instance,\n            graph_ui,main_ui\n        )\n\n    # Show results and followup cards\n    results_card.visible = True\n    followup_card.visible = True\n    graph_ui.visible = True\n</code></pre> <p>def load_history(index: int):     entry = state['research_history'][index]     if processor_instance is not None and processor_instance['instance'] is not None:</p> <pre><code>    processor_instance[\"instance\"].mem_name = entry[\"mam_name\"]\n    processor_instance['instance'].query = entry[\"query\"]\n\n    pass\nelse:\n    processor = Processor(entry[\"query\"], tools=get_tools(), **config)\n    # Setze den Callback so, dass Updates in der GUI angezeigt werden\n    processor.callback = update_status\n    processor.mem_name = entry[\"mam_name\"]\n    processor_instance[\"instance\"] = processor\n\nprocessor_instance[\"instance\"].tools.get_memory().load_memory(entry[\"mam_name\"], entry[\"processor_memory\"])\nprocessor_instance[\"instance\"].mem_name = entry[\"mam_name\"]\nupdate_results(entry, save=False)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui---stripe-integration-","title":"--- Stripe Integration ---","text":"<p>def regiser_stripe_integration(is_scc=True):     def stripe_callback(request: Request):</p> <pre><code>    sid = request.row.query_params.get('session_id') if hasattr(request, 'row') else request.query_params.get('session_id')\n    state = get_user_state(sid)\n\n    if state['payment_id'] == '':\n        with ui.card().classes(\"w-full items-center\").style(\"background-color: var(--background-color)\"):\n            ui.label(f\"No payment id!\").classes(\"text-lg font-bold\")\n            ui.button(\n                \"Start Research\",\n                on_click=lambda: ui.navigate.to(\"/open-Seeker.seek?session_id=\"+sid)\n            ).classes(\n                \"w-full px-6 py-4 text-lg font-bold \"\n                \"bg-primary hover:bg-primary-dark \"\n                \"transform hover:-translate-y-0.5 \"\n                \"transition-all duration-300 ease-in-out \"\n                \"rounded-xl shadow-lg animate-slideUp\"\n            )\n        return\n\n    try:\n        session_data = stripe.checkout.Session.retrieve(state['payment_id'])\n    except Exception as e:\n        with ui.card().classes(\"w-full items-center\").style(\"background-color: var(--background-color)\"):\n            ui.label(f\"No Transactions Details !{e}\").classes(\"text-lg font-bold\")\n            ui.button(\n                \"Start Research\",\n                on_click=lambda: ui.navigate.to(\"/open-Seeker.seek\")\n            ).classes(\n                \"w-full px-6 py-4 text-lg font-bold \"\n                \"bg-primary hover:bg-primary-dark \"\n                \"transform hover:-translate-y-0.5 \"\n                \"transition-all duration-300 ease-in-out \"\n                \"rounded-xl shadow-lg animate-slideUp\"\n            )\n            return\n    with ui.card().classes(\"w-full items-center\").style(\"background-color: var(--background-color)\"):\n        if is_scc and state['payment_id'] != '' and session_data.payment_status == 'paid':\n            state = get_user_state(sid)\n            amount = session_data.amount_total / 100  # Convert cents to euros\n            state['balance'] += amount\n            state['payment_id'] = ''\n            save_user_state(sid, state)\n\n        # ui.navigate.to(f'/session?session={session}')\n            ui.label(f\"Transaction Complete - New balance :{state['balance']}\").classes(\"text-lg font-bold\")\n            with ui.card().classes(\"w-full p-4\").style(\"background-color: var(--background-color)\"):\n                ui.label(\"Private Session link (restore the session on a different device)\")\n                base_url = f'https://{os.getenv(\"HOSTNAME\")}/gui/open-Seeker.seek' if not 'localhost' in os.getenv(\"HOSTNAME\")else 'http://localhost:5000/gui/open-Seeker.seek'\n                ui.label(f\"{base_url}?session_id={sid}\").style(\"white:100%\")\n                ui.label(\"Changes each time!\")\n        else:\n            ui.label(f\"Transaction Error! {session_data}, {dir(session_data)}\").classes(\"text-lg font-bold\")\n        ui.button(\n            \"Start Research\",\n            on_click=lambda: ui.navigate.to(\"/open-Seeker.seek\")\n        ).classes(\n            \"w-full px-6 py-4 text-lg font-bold \"\n            \"bg-primary hover:bg-primary-dark \"\n            \"transform hover:-translate-y-0.5 \"\n            \"transition-all duration-300 ease-in-out \"\n            \"rounded-xl shadow-lg animate-slideUp\"\n        )\n\n\nreturn stripe_callback\n</code></pre> <p>def handle_stripe_payment(amount: float, session_id):     base_url = f'https://{os.getenv(\"HOSTNAME\")}/gui/open-Seeker.stripe' if not 'localhost' in os.getenv(\"HOSTNAME\") else 'http://localhost:5000/gui/open-Seeker.stripe'     session = stripe.checkout.Session.create(         payment_method_types=['card',                               \"link\",                               ],         line_items=[{             'price_data': {                 'currency': 'eur',                 'product_data': {'name': 'Research Credits'},                 'unit_amount': int(amount * 100),             },             'quantity': 1,         }],         automatic_tax={\"enabled\": True},         mode='payment',         success_url=f'{base_url}?session_id={session_id}',         cancel_url=f'{base_url}.error'     )     state = get_user_state(session_id)     state['payment_id'] = session.id     save_user_state(session_id, state)     ui.navigate.to(session.url, new_tab=True)</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui---ui-components-","title":"--- UI Components ---","text":"<p>def balance_overlay(session_id):     with ui.dialog().classes('w-full max-w-md bg-white/20 backdrop-blur-lg rounded-xl') as dialog:         with ui.card().classes('w-full p-6 space-y-4').style(\"background-color: var(--background-color)\"):             ui.label('Add Research Credits').classes('text-2xl font-bold')             amount = ui.number('Amount (\u20ac) min 2', value=5, format='%.2f', min=2, max=9999, step=1).classes('w-full')             with ui.row().classes('w-full justify-between'):                 ui.button('Cancel', on_click=dialog.close).props('flat')                 ui.button('Purchase', on_click=lambda: handle_stripe_payment(amount.value, session_id))     return dialog</p> <p>def create_ui(processor):     # ui_instance =     register_nicegui(\"open-Seeker\", create_landing_page                      , additional=\"\"\"     \"\"\", show=False)     register_nicegui(\"open-Seeker.demo\", create_video_demo, additional=\"\"\"          \"\"\", show=False)</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui","title":"<code>newui</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.cleanup_module","title":"<code>cleanup_module(app)</code>","text":"<p>Cleanup resources when the module is unloaded</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, version=version, exit_f=True)\ndef cleanup_module(app: App):\n    \"\"\"Cleanup resources when the module is unloaded\"\"\"\n    # Clean up any temp files or resources\n    import glob\n    import shutil\n\n    # Remove temporary PDF directories\n    for pdf_dir in glob.glob(\"pdfs_*\"):\n        try:\n            shutil.rmtree(pdf_dir)\n        except Exception as e:\n            print(f\"Error removing directory {pdf_dir}: {str(e)}\")\n\n    # Clear any SSE queues\n    if hasattr(app, 'sse_queues'):\n        app.sse_queues = {}\n\n    if hasattr(app, 'payment_queues'):\n        app.payment_queues = {}\n\n    return Result.ok(info=\"ArXivPDFProcessor UI cleaned up\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.create_payment","title":"<code>create_payment(app, data)</code>  <code>async</code>","text":"<p>Create a Stripe payment session</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def create_payment(app: App, data):\n    \"\"\"Create a Stripe payment session\"\"\"\n    amount = data.get(\"amount\")\n    session_id = data.get(\"session_id\")\n\n    if amount &lt; 2:\n        return Result.default_user_error(info=\"Minimum donation amount is \u20ac2\")\n\n    try:\n        # Create a Stripe Checkout Session\n        base_url = f\"https://{os.getenv('HOSTNAME', 'localhost:5000')}\"\n        success_url = f\"{base_url}/api/{MOD_NAME}/payment_success?session_id={session_id}\"\n        cancel_url = f\"{base_url}/api/{MOD_NAME}/payment_cancel?session_id={session_id}\"\n\n        stripe_session = stripe.checkout.Session.create(\n            payment_method_types=['card', 'link'],\n            line_items=[{\n                'price_data': {\n                    'currency': 'eur',\n                    'product_data': {'name': 'Research Credits'},\n                    'unit_amount': int(amount * 100),\n                },\n                'quantity': 1,\n            }],\n            automatic_tax={\"enabled\": True},\n            mode='payment',\n            success_url=success_url,\n            cancel_url=cancel_url\n        )\n\n        # Store the payment info\n        if not hasattr(app, 'payment_info'):\n            app.payment_info = {}\n\n        # Initialize payment_queues if not already done\n        if not hasattr(app, 'payment_queues'):\n            app.payment_queues = {}\n\n        # Create a queue for this payment\n        app.payment_queues[session_id] = asyncio.Queue()\n\n        app.payment_info[session_id] = {\n            'payment_id': stripe_session.id,\n            'amount': amount,\n            'status': 'pending'\n        }\n\n        return Result.ok(data={\"url\": stripe_session.url})\n    except Exception as e:\n        return Result.default_sys_error(info=f\"Error creating payment: {str(e)}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.estimate_processing","title":"<code>estimate_processing(data)</code>  <code>async</code>","text":"<p>Estimate processing time and cost for a given query</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def estimate_processing(data):\n    \"\"\"Estimate processing time and cost for a given query\"\"\"\n    # Use the static method to estimate metrics\n    query, max_search, num_search_result_per_query= data.get(\"query\", \"\"), data.get(\"max_search\",4), data.get(\"num_search_result_per_query\",6)\n    estimated_time, estimated_price = ArXivPDFProcessor.estimate_processing_metrics(\n        query_length=len(query),\n        max_search=max_search,\n        num_search_result_per_query=num_search_result_per_query,\n        chunk_size=1_000_000,\n        overlap=2_000,\n        num_workers=None\n    )\n\n    return Result.ok(data={\n        \"time\": estimated_time,\n        \"price\": estimated_price\n    })\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.follow_up_query","title":"<code>follow_up_query(app, data)</code>  <code>async</code>","text":"<p>Ask a follow-up question about the research</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def follow_up_query(app: App, data):\n    \"\"\"Ask a follow-up question about the research\"\"\"\n    research_id = data.get(\"research_id\")\n    query = data.get(\"query\")\n\n    if not hasattr(app, 'research_processes') or research_id not in app.research_processes:\n        return Result.default_user_error(info=\"Research process not found\")\n\n    research_process = app.research_processes[research_id]\n\n    if research_process['status'] != 'complete':\n        return Result.default_user_error(info=\"Research is not complete\")\n\n    processor = research_process['processor']\n    if not processor:\n        return Result.default_user_error(info=\"Processor not available\")\n\n    try:\n        # Use the extra_query method to ask follow-up questions\n        result = await processor.extra_query(query)\n\n        return Result.ok(data={\"answer\": result['response'] if result and 'response' in result else \"No response\"})\n    except Exception as e:\n        return Result.default_sys_error(info=f\"Error processing follow-up query: {str(e)}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.initialize_module","title":"<code>initialize_module(app)</code>","text":"<p>Initialize the module and register UI with CloudM</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, version=version, initial=True)\ndef initialize_module(app: App):\n    \"\"\"Initialize the module and register UI with CloudM\"\"\"\n    # Register the UI with CloudM\n    app.run_any((\"CloudM\", \"add_ui\"),\n                name=\"TruthSeeker\",\n                title=\"TruthSeeker Research\",\n                path=f\"/api/{MOD_NAME}/get_main_ui\",\n                description=\"AI Research Assistant\"\n                )\n\n    # Initialize SSE message queues\n    if not hasattr(app, 'sse_queues'):\n        app.sse_queues = {}\n    print(\"TruthSeeker online\")\n    return Result.ok(info=\"ArXivPDFProcessor UI initialized\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.payment_cancel","title":"<code>payment_cancel(app, session_id, request_as_kwarg=True, request=None)</code>  <code>async</code>","text":"<p>Handle cancelled payment</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def payment_cancel(app: App, session_id: str, request_as_kwarg=True, request=None):\n    \"\"\"Handle cancelled payment\"\"\"\n    if hasattr(app, 'payment_info') and session_id in app.payment_info:\n        app.payment_info[session_id]['status'] = 'cancelled'\n\n        # Notify SSE clients about payment cancellation\n        if hasattr(app, 'payment_queues') and session_id in app.payment_queues:\n            await app.payment_queues[session_id].put({\n                \"status\": \"cancelled\"\n            })\n\n    return Result.html(app.web_context() + \"\"\"\n    &lt;div style=\"text-align: center; padding: 50px;\"&gt;\n        &lt;h2&gt;Payment Cancelled&lt;/h2&gt;\n        &lt;p&gt;Your payment was cancelled.&lt;/p&gt;\n        &lt;script&gt;\n            setTimeout(function() {\n                window.close();\n            }, 3000);\n        &lt;/script&gt;\n    &lt;/div&gt;\n    \"\"\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.payment_stream","title":"<code>payment_stream(app, session_id)</code>  <code>async</code>","text":"<p>SSE stream endpoint for payment status updates</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def payment_stream(app: App, session_id: str):\n    \"\"\"SSE stream endpoint for payment status updates\"\"\"\n    if not hasattr(app, 'payment_queues'):\n        app.payment_queues = {}\n\n    # Create a message queue for this session_id if it doesn't exist\n    if session_id not in app.payment_queues:\n        app.payment_queues[session_id] = asyncio.Queue()\n\n    async def generate():\n        try:\n            # Stream payment updates\n            while True:\n                try:\n                    # Wait for a payment update with a timeout\n                    payment_data = await asyncio.wait_for(app.payment_queues[session_id].get(), timeout=30)\n                    yield f\"event: payment_update\\ndata: {json.dumps(payment_data)}\\n\\n\"\n\n                    # If the payment is complete or cancelled, exit the loop\n                    if payment_data.get('status') in ['completed', 'cancelled']:\n                        break\n                except TimeoutError:\n                    # Send a keep-alive comment to prevent connection timeout\n                    yield \":\\n\\n\"\n        finally:\n            # Clean up resources when the client disconnects\n            if session_id in app.payment_queues:\n                # Keep the queue for other potential clients\n                pass\n\n    return Result.stream(generate())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.payment_success","title":"<code>payment_success(app, session_id, request_as_kwarg=True, request=None)</code>  <code>async</code>","text":"<p>Handle successful payment</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def payment_success(app: App, session_id: str, request_as_kwarg=True, request=None):\n    \"\"\"Handle successful payment\"\"\"\n    if not hasattr(app, 'payment_info') or session_id not in app.payment_info:\n        return Result.html(app.web_context() + \"\"\"\n        &lt;div style=\"text-align: center; padding: 50px;\"&gt;\n            &lt;h2&gt;Payment Session Not Found&lt;/h2&gt;\n            &lt;p&gt;Return to the main page to continue.&lt;/p&gt;\n            &lt;a href=\"/\" style=\"display: inline-block; margin-top: 20px; padding: 10px 20px; background-color: #4F46E5; color: white; text-decoration: none; border-radius: 5px;\"&gt;Return to Home&lt;/a&gt;\n        &lt;/div&gt;\n        \"\"\")\n\n    payment_info = app.payment_info[session_id]\n\n    try:\n        # Verify the payment with Stripe\n        stripe_session = stripe.checkout.Session.retrieve(payment_info['payment_id'])\n\n        if stripe_session.payment_status == 'paid':\n            payment_info['status'] = 'completed'\n\n            # Notify SSE clients about payment completion\n            if hasattr(app, 'payment_queues') and session_id in app.payment_queues:\n                await app.payment_queues[session_id].put({\n                    \"status\": \"completed\",\n                    \"amount\": payment_info['amount']\n                })\n\n            return Result.html(app.web_context() + \"\"\"\n            &lt;div style=\"text-align: center; padding: 50px;\"&gt;\n                &lt;h2&gt;Thank You for Your Support!&lt;/h2&gt;\n                &lt;p&gt;Your payment was successful. You can now close this window and continue with your research.&lt;/p&gt;\n                &lt;script&gt;\n                    setTimeout(function() {\n                        window.close();\n                    }, 5000);\n                &lt;/script&gt;\n            &lt;/div&gt;\n            \"\"\")\n        else:\n            return Result.html(app.web_context() + \"\"\"\n            &lt;div style=\"text-align: center; padding: 50px;\"&gt;\n                &lt;h2&gt;Payment Not Completed&lt;/h2&gt;\n                &lt;p&gt;Your payment has not been completed. Please try again.&lt;/p&gt;\n                &lt;button onclick=\"window.close()\"&gt;Close Window&lt;/button&gt;\n            &lt;/div&gt;\n            \"\"\")\n    except Exception as e:\n        return Result.html(app.web_context() + f\"\"\"\n        &lt;div style=\"text-align: center; padding: 50px;\"&gt;\n            &lt;h2&gt;Error Processing Payment&lt;/h2&gt;\n            &lt;p&gt;There was an error processing your payment: {str(e)}&lt;/p&gt;\n            &lt;button onclick=\"window.close()\"&gt;Close Window&lt;/button&gt;\n        &lt;/div&gt;\n        \"\"\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.research_results","title":"<code>research_results(app, research_id)</code>  <code>async</code>","text":"<p>Get the results of a completed research process</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def research_results(app: App, research_id: str):\n    \"\"\"Get the results of a completed research process\"\"\"\n    if not hasattr(app, 'research_processes') or research_id not in app.research_processes:\n        return Result.default_user_error(info=\"Research process not found\")\n\n    research_process = app.research_processes[research_id]\n\n    if research_process['status'] != 'complete':\n        return Result.default_user_error(info=\"Research is not complete\")\n\n    return Result.ok(data=research_process['results'])\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.research_status","title":"<code>research_status(app, research_id)</code>  <code>async</code>","text":"<p>Get the status of a research process</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def research_status(app: App, research_id: str):\n    \"\"\"Get the status of a research process\"\"\"\n    if not hasattr(app, 'research_processes') or research_id not in app.research_processes:\n        return Result.default_user_error(info=\"Research process not found\")\n\n    research_process = app.research_processes[research_id]\n\n    return Result.ok(data={\n        \"status\": research_process['status'],\n        \"progress\": research_process['progress'],\n        \"step\": research_process['step'],\n        \"info\": research_process['info']\n    })\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.start_research","title":"<code>start_research(app, data)</code>  <code>async</code>","text":"<p>Start a new research process</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def start_research(app: App, data):\n    \"\"\"Start a new research process\"\"\"\n    # Get data from the request\n    query = data.get(\"query\")\n    session_id = data.get(\"session_id\")\n    max_search = data.get(\"max_search\", 4)\n    num_search_result_per_query = data.get(\"num_search_result_per_query\", 4)\n\n    # Get the tools module\n    tools = get_app(\"ArXivPDFProcessor\").get_mod(\"isaa\")\n    if not hasattr(tools, 'initialized') or not tools.initialized:\n        tools.init_isaa(build=True)\n\n    # Generate a unique research_id\n    research_id = str(uuid.uuid4())\n\n    # Store the research information in a global dictionary\n    if not hasattr(app, 'research_processes'):\n        app.research_processes = {}\n\n    # Initialize SSE queues if not already done\n    if not hasattr(app, 'sse_queues'):\n        app.sse_queues = {}\n\n    # Create a queue for this research process\n    app.sse_queues[research_id] = asyncio.Queue()\n\n    # Create a processor with callback for status updates\n    app.research_processes[research_id] = {\n        'status': 'initializing',\n        'progress': 0.0,\n        'step': 'Initializing',\n        'info': '',\n        'query': query,\n        'session_id': session_id,\n        'processor': None,\n        'results': None,\n        'stop_requested': False\n    }\n\n    # Define the callback function that sends updates to the SSE queue\n    def status_callback(status_data):\n        if research_id in app.research_processes:\n            process = app.research_processes[research_id]\n            process['status'] = 'processing'\n            process['progress'] = status_data.get('progress', 0.0)\n            process['step'] = status_data.get('step', '')\n            process['info'] = status_data.get('info', '')\n\n            # Put the status update in the SSE queue\n            status_update = {\n                \"status\": process['status'],\n                \"progress\": process['progress'],\n                \"step\": process['step'],\n                \"info\": process['info']\n            }\n\n            if research_id in app.sse_queues:\n                asyncio.create_task(app.sse_queues[research_id].put(status_update))\n\n    # Create the processor\n    processor = ArXivPDFProcessor(\n        query=query,\n        tools=tools,\n        chunk_size=1_000_000,\n        overlap=2_000,\n        max_search=max_search,\n        num_search_result_per_query=num_search_result_per_query,\n        download_dir=f\"pdfs_{research_id}\",\n        callback=status_callback\n    )\n\n    app.research_processes[research_id]['processor'] = processor\n\n    # Process in the background\n    async def process_in_background():\n        try:\n            # Check if stop was requested before starting\n            if app.research_processes[research_id]['stop_requested']:\n                app.research_processes[research_id]['status'] = 'stopped'\n                if research_id in app.sse_queues:\n                    await app.sse_queues[research_id].put({\n                        \"status\": \"stopped\",\n                        \"progress\": 0,\n                        \"step\": \"Research stopped\",\n                        \"info\": \"\"\n                    })\n                return\n\n            # Start processing\n            papers, insights = await processor.process()\n\n            # Check if stop was requested during processing\n            if app.research_processes[research_id]['stop_requested']:\n                app.research_processes[research_id]['status'] = 'stopped'\n                if research_id in app.sse_queues:\n                    await app.sse_queues[research_id].put({\n                        \"status\": \"stopped\",\n                        \"progress\": 1,\n                        \"step\": \"Research stopped\",\n                        \"info\": \"\"\n                    })\n                return\n\n            # Store results\n            app.research_processes[research_id]['results'] = {\n                'papers': papers,\n                'insights': insights['response'] if insights and 'response' in insights else None\n            }\n            app.research_processes[research_id]['status'] = 'complete'\n\n            # Send final status update\n            if research_id in app.sse_queues:\n                await app.sse_queues[research_id].put({\n                    \"status\": \"complete\",\n                    \"progress\": 1,\n                    \"step\": \"Research complete\",\n                    \"info\": f\"Found {len(papers)} papers\"\n                })\n\n        except Exception as e:\n            app.research_processes[research_id]['status'] = 'error'\n            app.research_processes[research_id]['info'] = str(e)\n\n            # Send error status\n            if research_id in app.sse_queues:\n                await app.sse_queues[research_id].put({\n                    \"status\": \"error\",\n                    \"progress\": 0,\n                    \"step\": \"Error\",\n                    \"info\": str(e)\n                })\n\n            print(f\"Error in research process {research_id}: {str(e)}\")\n\n    # Start the background task\n    asyncio.create_task(process_in_background())\n\n    return Result.ok(data={\"research_id\": research_id})\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.status_stream","title":"<code>status_stream(app, research_id)</code>  <code>async</code>","text":"<p>SSE stream endpoint for research status updates</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def status_stream(app: App, research_id: str):\n    \"\"\"SSE stream endpoint for research status updates\"\"\"\n    if not hasattr(app, 'sse_queues'):\n        app.sse_queues = {}\n\n    # Create a message queue for this research_id if it doesn't exist\n    if research_id not in app.sse_queues:\n        app.sse_queues[research_id] = asyncio.Queue()\n\n    async def generate():\n        # Send initial status\n        if hasattr(app, 'research_processes') and research_id in app.research_processes:\n            process = app.research_processes[research_id]\n            initial_status = {\n                \"status\": process['status'],\n                \"progress\": process['progress'],\n                \"step\": process['step'],\n                \"info\": process['info']\n            }\n            yield f\"event: status_update\\ndata: {json.dumps(initial_status)}\\n\\n\"\n\n        try:\n            # Stream status updates\n            while True:\n                try:\n                    # Wait for a new status update with a timeout\n                    status_data = await asyncio.wait_for(app.sse_queues[research_id].get(), timeout=30)\n                    yield f\"event: status_update\\ndata: {json.dumps(status_data)}\\n\\n\"\n\n                    # If the research is complete or there was an error, exit the loop\n                    if status_data.get('status') in ['complete', 'error', 'stopped']:\n                        break\n                except TimeoutError:\n                    # Send a keep-alive comment to prevent connection timeout\n                    yield \":\\n\\n\"\n        finally:\n            # Clean up resources when the client disconnects\n            if research_id in app.sse_queues:\n                # Keep the queue for other potential clients\n                pass\n\n    return Result.stream(generate())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.stop_research","title":"<code>stop_research(app, data)</code>  <code>async</code>","text":"<p>Stop a research process</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def stop_research(app: App, data):\n    \"\"\"Stop a research process\"\"\"\n    research_id = data.get(\"research_id\")\n    if not hasattr(app, 'research_processes') or research_id not in app.research_processes:\n        return Result.default_user_error(info=\"Research process not found\")\n\n    app.research_processes[research_id]['stop_requested'] = True\n\n    # Send stopped status to SSE clients\n    if hasattr(app, 'sse_queues') and research_id in app.sse_queues:\n        await app.sse_queues[research_id].put({\n            \"status\": \"stopped\",\n            \"progress\": app.research_processes[research_id]['progress'],\n            \"step\": \"Stopping research\",\n            \"info\": \"\"\n        })\n\n    return Result.ok(data={\"status\": \"stop_requested\"})\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.one","title":"<code>one</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.one.IntelligenceRingEmbeddings","title":"<code>IntelligenceRingEmbeddings</code>","text":"Source code in <code>toolboxv2/mods/TruthSeeker/one.py</code> <pre><code>class IntelligenceRingEmbeddings:\n    name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n    clip_name: str = \"openai/clip-vit-base-patch32\"\n    wav2vec_name: str = \"facebook/wav2vec2-base-960h\"\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    vector_size: int = 768\n    tokenizer: Any | None = None\n    text_model: Any | None = None\n\n    clip_processor: Any | None = None\n    clip_model: Any | None = None\n\n    audio_processor: Any | None = None\n    audio_model: Any | None = None\n\n    text_projection: Any | None = None\n    image_projection: Any | None = None\n    audio_projection: Any | None = None\n\n    def __init__(self, **kwargs):\n\n        super().__init__(**kwargs)\n        self._ndims = self.vector_size\n\n        # Text embedding model\n        self.tokenizer = AutoTokenizer.from_pretrained(self.name)\n        self.text_model = AutoModel.from_pretrained(self.name).to(self.device)\n\n        # Image embedding model (CLIP)\n        self.clip_processor = CLIPProcessor.from_pretrained(self.clip_name)\n        self.clip_model = CLIPModel.from_pretrained(self.clip_name).to(self.device)\n\n        # Audio embedding model (Wav2Vec2)\n        self.audio_processor = Wav2Vec2Processor.from_pretrained(self.wav2vec_name)\n        self.audio_model = Wav2Vec2Model.from_pretrained(self.wav2vec_name).to(self.device)\n\n        # Projection layers to align dimensions\n        self.text_projection = torch.nn.Linear(\n            self.text_model.config.hidden_size,\n            self.vector_size\n        ).to(self.device)\n        self.image_projection = torch.nn.Linear(\n            self.clip_model.config.vision_config.hidden_size,\n            self.vector_size\n        ).to(self.device)\n        self.audio_projection = torch.nn.Linear(\n            self.audio_model.config.hidden_size,\n            self.vector_size\n        ).to(self.device)\n\n    def _process_text(self, text: str) -&gt; torch.Tensor:\n        encoded_input = self.tokenizer(\n            text,\n            padding=True,\n            truncation=True,\n            max_length=self.vector_size,\n            return_tensors='pt'\n        ).to(self.device)\n\n        with torch.no_grad():\n            outputs = self.text_model(**encoded_input)\n            embeddings = self._mean_pooling(outputs, encoded_input['attention_mask'])\n            projected = self.text_projection(embeddings)\n            return torch.nn.functional.normalize(projected, p=2, dim=1)\n\n    def _process_image(self, image_data: bytes | str) -&gt; torch.Tensor:\n        # Handle different image input types\n        if isinstance(image_data, str):\n            if image_data.startswith('data:image'):\n                # Handle base64 encoded images\n                image_data = base64.b64decode(image_data.split(',')[1])\n            else:\n                # Handle file paths\n                with open(image_data, 'rb') as f:\n                    image_data = f.read()\n\n        # Convert bytes to PIL Image\n        image = Image.open(io.BytesIO(image_data))\n\n        # Process image with CLIP\n        inputs = self.clip_processor(images=image, return_tensors=\"pt\").to(self.device)\n\n        with torch.no_grad():\n            outputs = self.clip_model.get_image_features(**inputs)\n            projected = self.image_projection(outputs)\n            return torch.nn.functional.normalize(projected, p=2, dim=1)\n\n    def _process_audio(self, audio_data: bytes | str | np.ndarray) -&gt; torch.Tensor:\n        try:\n            import torchaudio\n        except ImportError:\n            raise ValueError(\"Couldn't load audio install torchaudio'\")\n        # Handle different audio input types\n        if isinstance(audio_data, str):\n            if audio_data.startswith('data:audio'):\n                # Handle base64 encoded audio\n                audio_data = base64.b64decode(audio_data.split(',')[1])\n                waveform, sample_rate = torchaudio.load(io.BytesIO(audio_data))\n            else:\n                # Handle file paths\n                waveform, sample_rate = torchaudio.load(audio_data)\n        elif isinstance(audio_data, bytes):\n            waveform, sample_rate = torchaudio.load(io.BytesIO(audio_data))\n        else:\n            # Assume numpy array with sample rate in metadata\n            waveform = torch.from_numpy(audio_data)\n            sample_rate = 16000  # Default sample rate\n\n        # Resample if necessary\n        if sample_rate != 16000:\n            resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n            waveform = resampler(waveform)\n\n        # Process audio with Wav2Vec2\n        inputs = self.audio_processor(waveform, sampling_rate=16000, return_tensors=\"pt\").to(self.device)\n\n        with torch.no_grad():\n            outputs = self.audio_model(**inputs)\n            # Mean pooling over time dimension\n            embeddings = outputs.last_hidden_state.mean(dim=1)\n            projected = self.audio_projection(embeddings)\n            return torch.nn.functional.normalize(projected, p=2, dim=1)\n\n    def _mean_pooling(self, model_output: torch.Tensor, attention_mask: torch.Tensor) -&gt; torch.Tensor:\n        token_embeddings = model_output[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n    def process_input(self, input_data: InputData) -&gt; np.ndarray:\n        if input_data.modality == \"text\":\n            embeddings = self._process_text(input_data.content)\n        elif input_data.modality == \"image\":\n            embeddings = self._process_image(input_data.content)\n        elif input_data.modality == \"audio\":\n            embeddings = self._process_audio(input_data.content)\n        else:\n            raise ValueError(f\"Unsupported modality: {input_data.modality}\")\n\n        return embeddings.cpu().numpy()\n\n    def compute_query_embeddings(self, query: str | bytes | np.ndarray, modality: str = \"text\") -&gt; list[\n        np.ndarray]:\n        \"\"\"Compute embeddings for query input\"\"\"\n        input_data = InputData(query, modality)\n        embedding = self.process_input(input_data)\n        return [embedding.squeeze()]\n\n    def compute_source_embeddings(self, sources: list[str | bytes | np.ndarray], modalities: list[str]) -&gt; list[\n        np.ndarray]:\n        \"\"\"Compute embeddings for source inputs\"\"\"\n        embeddings = []\n        for source, modality in zip(sources, modalities, strict=False):\n            input_data = InputData(source, modality)\n            embedding = self.process_input(input_data)\n            embeddings.append(embedding.squeeze())\n        return embeddings\n\n    def ndims(self) -&gt; int:\n        return self._ndims\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.one.IntelligenceRingEmbeddings.compute_query_embeddings","title":"<code>compute_query_embeddings(query, modality='text')</code>","text":"<p>Compute embeddings for query input</p> Source code in <code>toolboxv2/mods/TruthSeeker/one.py</code> <pre><code>def compute_query_embeddings(self, query: str | bytes | np.ndarray, modality: str = \"text\") -&gt; list[\n    np.ndarray]:\n    \"\"\"Compute embeddings for query input\"\"\"\n    input_data = InputData(query, modality)\n    embedding = self.process_input(input_data)\n    return [embedding.squeeze()]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.one.IntelligenceRingEmbeddings.compute_source_embeddings","title":"<code>compute_source_embeddings(sources, modalities)</code>","text":"<p>Compute embeddings for source inputs</p> Source code in <code>toolboxv2/mods/TruthSeeker/one.py</code> <pre><code>def compute_source_embeddings(self, sources: list[str | bytes | np.ndarray], modalities: list[str]) -&gt; list[\n    np.ndarray]:\n    \"\"\"Compute embeddings for source inputs\"\"\"\n    embeddings = []\n    for source, modality in zip(sources, modalities, strict=False):\n        input_data = InputData(source, modality)\n        embedding = self.process_input(input_data)\n        embeddings.append(embedding.squeeze())\n    return embeddings\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests","title":"<code>tests</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker","title":"<code>TestTruthSeeker</code>","text":"<p>               Bases: <code>TestCase</code></p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>class TestTruthSeeker(unittest.TestCase):\n    def setUp(self):\n        # Mock the App class\n        self.mock_app = Mock()\n        self.mock_app.get_mod.return_value = Mock()\n\n        # Setup mock for run_any that returns iterable dict\n        self.mock_app.run_any.return_value = {\n            \"1\": {\"name\": \"template1\"},\n            \"2\": {\"name\": \"template2\"}\n        }\n\n        # Mock RequestSession\n        self.mock_request = Mock()\n        self.mock_request.json = AsyncMock()\n\n    @patch('os.path.join')\n    @patch('builtins.open', create=True)\n    def test_start_initialization(self, mock_open, mock_join):\n        \"\"\"Test the start function initializes correctly\"\"\"\n        # Setup mock file handling\n        mock_file = Mock()\n        mock_file.read.return_value = \"test content\"\n        mock_open.return_value.__enter__.return_value = mock_file\n\n        # Call start function\n        start(self.mock_app)\n\n        # Verify app initialization calls\n        self.mock_app.get_mod.assert_called_with(\"CodeVerification\")\n        self.mock_app.run_any.assert_any_call((\"CodeVerification\", \"init_scope\"), scope=\"TruthSeeker\")\n        self.mock_app.run_any.assert_any_call((\"CodeVerification\", \"init_scope\"), scope=\"TruthSeeker-promo\")\n\n    @async_test\n    async def test_codes_valid_request(self):\n        \"\"\"Test the codes function with valid input\"\"\"\n        # Mock request data\n        test_data = {\n            \"query\": \"test query\",\n            \"depth\": \"Q\",\n            \"promoCode\": \"PROMO15\",\n            \"ontimeCode\": \"TEST123\"\n        }\n        self.mock_request.json.return_value = test_data\n\n        # Mock code verification\n        self.mock_app.run_any.return_value = {\n            \"template_name\": \"Promo15\",\n            \"usage_type\": \"one_time\"\n        }\n\n        result = await codes(self.mock_app, self.mock_request)\n\n        self.assertTrue(result['valid'])\n        self.assertIn('ontimeKey', result)\n        self.assertIn('ppc', result)\n\n    @async_test\n    async def test_codes_invalid_promo(self):\n        \"\"\"Test the codes function with invalid promo code\"\"\"\n        test_data = {\n            \"query\": \"test query\",\n            \"depth\": \"I\",\n            \"promoCode\": \"INVALID\",\n            \"ontimeCode\": \"TEST123\"\n        }\n        self.mock_request.json.return_value = test_data\n\n        # Mock invalid promo code verification\n        self.mock_app.run_any.return_value = None\n\n        result = await codes(self.mock_app, self.mock_request)\n\n        self.assertIn('ppc', result)\n        self.assertTrue(result['ppc']['price'] &gt; 0)\n\n    @async_test\n    async def test_process_valid_request(self):\n        \"\"\"Test the process function with valid input\"\"\"\n        test_data = {\n            \"query\": \"test query\",\n            \"depth\": \"Q\",\n            \"ontimeKey\": \"VALID_KEY\",\n            \"email\": \"test@example.com\"\n        }\n        self.mock_request.json.return_value = test_data\n\n        # Mock valid key verification\n        self.mock_app.run_any.return_value = {\n            \"template_name\": \"PROCESS\",\n            \"usage_type\": \"timed\",\n            \"uses_count\": 1\n        }\n\n        # Mock ArXivPDFProcessor\n        with patch('toolboxv2.mods.TruthSeeker.module.ArXivPDFProcessor') as mock_processor:\n            mock_insights = MagicMock()\n            mock_insights.is_true = \"True\"\n            mock_insights.summary = \"Test summary\"\n            mock_insights.key_point = \"Point1&gt;\\n\\n&lt;Point2\"\n\n            mock_processor.return_value.process.return_value = ([], mock_insights)\n\n            result = await process(self.mock_app, self.mock_request)\n\n            self.assertEqual(result['is_true'], \"True\")\n            self.assertEqual(result['summary'], \"Test summary\")\n\n    @async_test\n    async def test_process_invalid_key(self):\n        \"\"\"Test the process function with invalid key\"\"\"\n        test_data = {\n            \"query\": \"test query\",\n            \"depth\": \"Q\",\n            \"ontimeKey\": \"INVALID_KEY\",\n            \"email\": \"test@example.com\"\n        }\n        self.mock_request.json.return_value = test_data\n\n        # Mock invalid key verification\n        self.mock_app.run_any.return_value = None\n\n        result = await process(self.mock_app, self.mock_request)\n\n        self.assertEqual(result['summary'], \"INVALID QUERY\")\n        self.assertEqual(result['insights'], [])\n        self.assertEqual(result['papers'], [])\n\n    def test_byCode_functionality(self):\n        \"\"\"Test the byCode function\"\"\"\n        test_request = Mock()\n        test_request.json.return_value = [\"payKey\", \"codeClass\", \"ontimeKey\"]\n\n        result = byCode(self.mock_app, test_request)\n\n        self.assertEqual(result, {'code': 'code'})\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker.test_byCode_functionality","title":"<code>test_byCode_functionality()</code>","text":"<p>Test the byCode function</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>def test_byCode_functionality(self):\n    \"\"\"Test the byCode function\"\"\"\n    test_request = Mock()\n    test_request.json.return_value = [\"payKey\", \"codeClass\", \"ontimeKey\"]\n\n    result = byCode(self.mock_app, test_request)\n\n    self.assertEqual(result, {'code': 'code'})\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker.test_codes_invalid_promo","title":"<code>test_codes_invalid_promo()</code>  <code>async</code>","text":"<p>Test the codes function with invalid promo code</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@async_test\nasync def test_codes_invalid_promo(self):\n    \"\"\"Test the codes function with invalid promo code\"\"\"\n    test_data = {\n        \"query\": \"test query\",\n        \"depth\": \"I\",\n        \"promoCode\": \"INVALID\",\n        \"ontimeCode\": \"TEST123\"\n    }\n    self.mock_request.json.return_value = test_data\n\n    # Mock invalid promo code verification\n    self.mock_app.run_any.return_value = None\n\n    result = await codes(self.mock_app, self.mock_request)\n\n    self.assertIn('ppc', result)\n    self.assertTrue(result['ppc']['price'] &gt; 0)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker.test_codes_valid_request","title":"<code>test_codes_valid_request()</code>  <code>async</code>","text":"<p>Test the codes function with valid input</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@async_test\nasync def test_codes_valid_request(self):\n    \"\"\"Test the codes function with valid input\"\"\"\n    # Mock request data\n    test_data = {\n        \"query\": \"test query\",\n        \"depth\": \"Q\",\n        \"promoCode\": \"PROMO15\",\n        \"ontimeCode\": \"TEST123\"\n    }\n    self.mock_request.json.return_value = test_data\n\n    # Mock code verification\n    self.mock_app.run_any.return_value = {\n        \"template_name\": \"Promo15\",\n        \"usage_type\": \"one_time\"\n    }\n\n    result = await codes(self.mock_app, self.mock_request)\n\n    self.assertTrue(result['valid'])\n    self.assertIn('ontimeKey', result)\n    self.assertIn('ppc', result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker.test_process_invalid_key","title":"<code>test_process_invalid_key()</code>  <code>async</code>","text":"<p>Test the process function with invalid key</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@async_test\nasync def test_process_invalid_key(self):\n    \"\"\"Test the process function with invalid key\"\"\"\n    test_data = {\n        \"query\": \"test query\",\n        \"depth\": \"Q\",\n        \"ontimeKey\": \"INVALID_KEY\",\n        \"email\": \"test@example.com\"\n    }\n    self.mock_request.json.return_value = test_data\n\n    # Mock invalid key verification\n    self.mock_app.run_any.return_value = None\n\n    result = await process(self.mock_app, self.mock_request)\n\n    self.assertEqual(result['summary'], \"INVALID QUERY\")\n    self.assertEqual(result['insights'], [])\n    self.assertEqual(result['papers'], [])\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker.test_process_valid_request","title":"<code>test_process_valid_request()</code>  <code>async</code>","text":"<p>Test the process function with valid input</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@async_test\nasync def test_process_valid_request(self):\n    \"\"\"Test the process function with valid input\"\"\"\n    test_data = {\n        \"query\": \"test query\",\n        \"depth\": \"Q\",\n        \"ontimeKey\": \"VALID_KEY\",\n        \"email\": \"test@example.com\"\n    }\n    self.mock_request.json.return_value = test_data\n\n    # Mock valid key verification\n    self.mock_app.run_any.return_value = {\n        \"template_name\": \"PROCESS\",\n        \"usage_type\": \"timed\",\n        \"uses_count\": 1\n    }\n\n    # Mock ArXivPDFProcessor\n    with patch('toolboxv2.mods.TruthSeeker.module.ArXivPDFProcessor') as mock_processor:\n        mock_insights = MagicMock()\n        mock_insights.is_true = \"True\"\n        mock_insights.summary = \"Test summary\"\n        mock_insights.key_point = \"Point1&gt;\\n\\n&lt;Point2\"\n\n        mock_processor.return_value.process.return_value = ([], mock_insights)\n\n        result = await process(self.mock_app, self.mock_request)\n\n        self.assertEqual(result['is_true'], \"True\")\n        self.assertEqual(result['summary'], \"Test summary\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker.test_start_initialization","title":"<code>test_start_initialization(mock_open, mock_join)</code>","text":"<p>Test the start function initializes correctly</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@patch('os.path.join')\n@patch('builtins.open', create=True)\ndef test_start_initialization(self, mock_open, mock_join):\n    \"\"\"Test the start function initializes correctly\"\"\"\n    # Setup mock file handling\n    mock_file = Mock()\n    mock_file.read.return_value = \"test content\"\n    mock_open.return_value.__enter__.return_value = mock_file\n\n    # Call start function\n    start(self.mock_app)\n\n    # Verify app initialization calls\n    self.mock_app.get_mod.assert_called_with(\"CodeVerification\")\n    self.mock_app.run_any.assert_any_call((\"CodeVerification\", \"init_scope\"), scope=\"TruthSeeker\")\n    self.mock_app.run_any.assert_any_call((\"CodeVerification\", \"init_scope\"), scope=\"TruthSeeker-promo\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.run_all_tests","title":"<code>run_all_tests()</code>","text":"<p>Run all test classes</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@default_test\ndef run_all_tests():\n    \"\"\"Run all test classes\"\"\"\n    return run_test_suite()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.run_arxiv_processor_tests","title":"<code>run_arxiv_processor_tests(test_name=None)</code>","text":"<p>Run TestArXivPDFProcessor tests</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>def run_arxiv_processor_tests(test_name=None):\n    \"\"\"Run TestArXivPDFProcessor tests\"\"\"\n    return run_test_suite(TestArXivPDFProcessor, test_name)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.run_pdf_downloader_tests","title":"<code>run_pdf_downloader_tests(test_name=None)</code>","text":"<p>Run TestRobustPDFDownloader tests</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>def run_pdf_downloader_tests(test_name=None):\n    \"\"\"Run TestRobustPDFDownloader tests\"\"\"\n    return run_test_suite(TestRobustPDFDownloader, test_name)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.run_specific_test","title":"<code>run_specific_test(test_class, test_name)</code>","text":"<p>Run a specific test from a test class</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>def run_specific_test(test_class, test_name):\n    \"\"\"Run a specific test from a test class\"\"\"\n    return run_test_suite(test_class, test_name)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.run_test_suite","title":"<code>run_test_suite(test_class=None, test_name=None, verbosity=2)</code>","text":"<p>Run specific test class or test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_class</code> <p>The test class to run (optional)</p> <code>None</code> <code>test_name</code> <p>Specific test method name to run (optional)</p> <code>None</code> <code>verbosity</code> <p>Output detail level (default=2)</p> <code>2</code> <p>Returns:</p> Type Description <p>TestResult object</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>def run_test_suite(test_class=None, test_name=None, verbosity=2):\n    \"\"\"\n    Run specific test class or test case.\n\n    Args:\n        test_class: The test class to run (optional)\n        test_name: Specific test method name to run (optional)\n        verbosity: Output detail level (default=2)\n\n    Returns:\n        TestResult object\n    \"\"\"\n    loader = unittest.TestLoader()\n    suite = unittest.TestSuite()\n\n    if test_class and test_name:\n        # Run specific test method\n        suite.addTest(test_class(test_name))\n    elif test_class:\n        # Run all tests in the class\n        suite.addTests(loader.loadTestsFromTestCase(test_class))\n    else:\n        # Run all tests\n        suite.addTests(loader.loadTestsFromModule(sys.modules[__name__]))\n\n    runner = unittest.TextTestRunner(verbosity=verbosity)\n    return runner.run(suite)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.run_truth_seeker_tests","title":"<code>run_truth_seeker_tests(test_name=None)</code>","text":"<p>Run TestTruthSeeker tests</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>def run_truth_seeker_tests(test_name=None):\n    \"\"\"Run TestTruthSeeker tests\"\"\"\n    return run_test_suite(TestTruthSeeker, test_name)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.test_arxiv_search","title":"<code>test_arxiv_search()</code>","text":"<p>Run only ArXiv search tests</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@default_test\ndef test_arxiv_search():\n    \"\"\"Run only ArXiv search tests\"\"\"\n    return run_specific_test(\n        TestArXivPDFProcessor,\n        'test_search_and_process_papers'\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.test_pdf_download","title":"<code>test_pdf_download()</code>","text":"<p>Run only PDF download tests</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@default_test\ndef test_pdf_download():\n    \"\"\"Run only PDF download tests\"\"\"\n    return run_specific_test(\n        TestRobustPDFDownloader,\n        'test_download_pdf_success'\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.test_truth_seeker","title":"<code>test_truth_seeker()</code>","text":"<p>Run only PDF download tests</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@default_test\ndef test_truth_seeker():\n    \"\"\"Run only PDF download tests\"\"\"\n    return run_specific_test(\n        TestTruthSeeker,\n        'test_truth_seeker_success'\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.UltimateTTT","title":"<code>UltimateTTT</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.UltimateTTT.UltimateTTTGameEngine","title":"<code>UltimateTTTGameEngine</code>","text":"Source code in <code>toolboxv2/mods/UltimateTTT.py</code> <pre><code>class UltimateTTTGameEngine:  # Renamed for clarity\n    def __init__(self, game_state: GameState):\n        self.gs = game_state\n        self.size = game_state.config.grid_size\n\n    def _check_line_for_win(self, line: List[Union[CellState, BoardWinner]],\n                            symbol_to_check: Union[CellState, BoardWinner]) -&gt; bool:\n        if not line or line[0] == CellState.EMPTY or line[0] == BoardWinner.NONE:\n            return False\n        return all(cell == symbol_to_check for cell in line)\n\n    def _get_board_winner_symbol(self, board: List[List[Union[CellState, BoardWinner]]],\n                                 symbol_class: Union[type[CellState], type[BoardWinner]]) -&gt; Optional[\n        Union[CellState, BoardWinner]]:\n        symbols_to_try = [symbol_class.X, symbol_class.O]\n        for symbol in symbols_to_try:\n            # Rows\n            for r in range(self.size):\n                if self._check_line_for_win([board[r][c] for c in range(self.size)], symbol): return symbol\n            # Columns\n            for c in range(self.size):\n                if self._check_line_for_win([board[r][c] for r in range(self.size)], symbol): return symbol\n            # Diagonals\n            if self._check_line_for_win([board[i][i] for i in range(self.size)], symbol): return symbol\n            if self._check_line_for_win([board[i][self.size - 1 - i] for i in range(self.size)], symbol): return symbol\n        return None  # No winner\n\n    def _is_board_full(self, board: List[List[Union[CellState, BoardWinner]]],\n                       empty_value: Union[CellState, BoardWinner]) -&gt; bool:\n        return all(cell != empty_value for row in board for cell in row)\n\n    def _determine_local_board_result(self, global_r: int, global_c: int) -&gt; BoardWinner:\n        if self.gs.global_board_winners[global_r][global_c] != BoardWinner.NONE:\n            return self.gs.global_board_winners[global_r][global_c]\n\n        local_board_cells = self.gs.local_boards_state[global_r][global_c]\n        winner_symbol = self._get_board_winner_symbol(local_board_cells, CellState)\n        if winner_symbol:\n            return BoardWinner(winner_symbol.value)  # Convert CellState.X to BoardWinner.X\n        if self._is_board_full(local_board_cells, CellState.EMPTY):\n            return BoardWinner.DRAW\n        return BoardWinner.NONE\n\n    def _update_local_winner_and_check_global(self, global_r: int, global_c: int):\n        new_local_winner = self._determine_local_board_result(global_r, global_c)\n        if new_local_winner != BoardWinner.NONE and self.gs.global_board_winners[global_r][\n            global_c] == BoardWinner.NONE:\n            self.gs.global_board_winners[global_r][global_c] = new_local_winner\n            self._check_for_overall_game_end()\n\n    def _check_for_overall_game_end(self):\n        if self.gs.status == GameStatus.FINISHED: return\n\n        winner_board_symbol = self._get_board_winner_symbol(self.gs.global_board_winners, BoardWinner)\n        if winner_board_symbol:  # This is BoardWinner.X or BoardWinner.O\n            self.gs.overall_winner_symbol = PlayerSymbol(winner_board_symbol.value)  # Convert to PlayerSymbol\n            self.gs.status = GameStatus.FINISHED\n            return\n\n        if self._is_board_full(self.gs.global_board_winners, BoardWinner.NONE):\n            self.gs.is_draw = True\n            self.gs.status = GameStatus.FINISHED\n\n    def _determine_next_forced_board(self, last_move_local_r: int, last_move_local_c: int) -&gt; Optional[Tuple[int, int]]:\n        target_gr, target_gc = last_move_local_r, last_move_local_c\n\n        if self.gs.global_board_winners[target_gr][target_gc] == BoardWinner.NONE and \\\n            not self._is_local_board_full(self.gs.local_boards_state[target_gr][target_gc], CellState.EMPTY):\n            return (target_gr, target_gc)\n        return None  # Play anywhere valid\n\n    def _is_local_board_full(self, local_board_cells: List[List[CellState]], cell_type=CellState.EMPTY) -&gt; bool:\n        \"\"\"Checks if a specific local board (passed as a 2D list of CellState) is full.\"\"\"\n        for r in range(self.size):\n            for c in range(self.size):\n                if local_board_cells[r][c] == cell_type:\n                    return False\n        return True\n\n    def add_player(self, player_id: str, player_name: str,\n                   is_npc: bool = False, npc_difficulty: Optional[NPCDifficulty] = None) -&gt; bool:\n        if len(self.gs.players) &gt;= 2:\n            self.gs.last_error_message = \"Game is already full (2 players max).\"\n            return False\n\n        # Reconnect logic for existing player (human or NPC if that makes sense)\n        existing_player = self.gs.get_player_info(player_id)\n        if existing_player:\n            if not existing_player.is_connected:\n                existing_player.is_connected = True\n                # If NPC \"reconnects\", ensure its properties are correct (though unlikely scenario for NPC)\n                if is_npc:\n                    existing_player.is_npc = True\n                    existing_player.npc_difficulty = npc_difficulty\n                    existing_player.name = player_name  # Update name if it changed for NPC\n\n                self.gs.last_error_message = None\n                self.gs.updated_at = datetime.now(timezone.utc)\n\n                if len(self.gs.players) == 2 and all(p.is_connected for p in self.gs.players) and \\\n                    self.gs.status == GameStatus.WAITING_FOR_OPPONENT:  # Should not be waiting if NPC is P2\n                    self.gs.status = GameStatus.IN_PROGRESS\n                    player_x_info = next(p for p in self.gs.players if p.symbol == PlayerSymbol.X)\n                    self.gs.current_player_id = player_x_info.id\n                    self.gs.waiting_since = None\n                return True\n            else:  # Player ID exists and is already connected\n                self.gs.last_error_message = f\"Player with ID {player_id} is already in the game and connected.\"\n                return False\n\n        # Adding a new player\n        symbol = PlayerSymbol.X if not self.gs.players else PlayerSymbol.O\n\n        # Construct PlayerInfo with NPC details if applicable\n        player_info_data = {\n            \"id\": player_id,\n            \"symbol\": symbol,\n            \"name\": player_name,\n            \"is_connected\": True,  # NPCs are always \"connected\"\n            \"is_npc\": is_npc\n        }\n        if is_npc and npc_difficulty:\n            player_info_data[\"npc_difficulty\"] = npc_difficulty\n\n        new_player = PlayerInfo(**player_info_data)\n        self.gs.players.append(new_player)\n        self.gs.last_error_message = None\n\n        if len(self.gs.players) == 1:  # First player added\n            if self.gs.mode == GameMode.ONLINE:\n                self.gs.status = GameStatus.WAITING_FOR_OPPONENT\n                self.gs.current_player_id = player_id\n                self.gs.waiting_since = datetime.now(timezone.utc)\n            # For local mode with P1, we wait for P2 (human or NPC) to be added\n            # No status change yet, current_player_id not set until P2 joins\n\n        elif len(self.gs.players) == 2:  # Both players now present\n            self.gs.status = GameStatus.IN_PROGRESS\n            player_x_info = next(p for p in self.gs.players if p.symbol == PlayerSymbol.X)\n            self.gs.current_player_id = player_x_info.id  # X always starts\n            self.gs.next_forced_global_board = None\n            self.gs.waiting_since = None\n\n            # If the second player added is an NPC and it's their turn (e.g. P1 is human, P2 is NPC, P1 made a move)\n            # This specific logic is more for when make_move hands over to an NPC.\n            # Here, we just set up the game. X (P1) will make the first move.\n\n        self.gs.updated_at = datetime.now(timezone.utc)\n        return True\n\n    def make_move(self, move: Move) -&gt; bool:\n        self.gs.last_error_message = None\n\n        if self.gs.status != GameStatus.IN_PROGRESS:\n            self.gs.last_error_message = \"Game is not in progress.\"\n            return False\n\n        player_info = self.gs.get_player_info(move.player_id)\n        if not player_info or move.player_id != self.gs.current_player_id:\n            self.gs.last_error_message = \"Not your turn or invalid player.\"\n            return False\n\n        s = self.size\n        if not (0 &lt;= move.global_row &lt; s and 0 &lt;= move.global_col &lt; s and \\\n                0 &lt;= move.local_row &lt; s and 0 &lt;= move.local_col &lt; s):\n            self.gs.last_error_message = f\"Coordinates out of bounds for {s}x{s} grid.\"\n            return False\n\n        gr, gc, lr, lc = move.global_row, move.global_col, move.local_row, move.local_col\n\n        if self.gs.next_forced_global_board and (gr, gc) != self.gs.next_forced_global_board:\n            self.gs.last_error_message = f\"Must play in global board {self.gs.next_forced_global_board}.\"\n            return False\n\n        if self.gs.global_board_winners[gr][gc] != BoardWinner.NONE:\n            self.gs.last_error_message = f\"Local board ({gr},{gc}) is already decided.\"\n            return False\n        if self.gs.local_boards_state[gr][gc][lr][lc] != CellState.EMPTY:\n            self.gs.last_error_message = f\"Cell ({gr},{gc})-({lr},{lc}) is already empty.\"  # Should be 'not empty' or 'occupied'\n            # Correction:\n            self.gs.last_error_message = f\"Cell ({gr},{gc})-({lr},{lc}) is already occupied.\"\n            return False\n\n        self.gs.local_boards_state[gr][gc][lr][lc] = CellState(player_info.symbol.value)\n        self.gs.moves_history.append(move)\n\n        self._update_local_winner_and_check_global(gr, gc)\n\n        if self.gs.status == GameStatus.FINISHED:\n            self.gs.next_forced_global_board = None\n        else:\n            opponent_info = self.gs.get_opponent_info(self.gs.current_player_id)\n            self.gs.current_player_id = opponent_info.id\n            self.gs.next_forced_global_board = self._determine_next_forced_board(lr, lc)\n\n            if self.gs.next_forced_global_board is None:\n                is_any_move_possible = any(\n                    self.gs.global_board_winners[r_idx][c_idx] == BoardWinner.NONE and \\\n                    not self._is_local_board_full(self.gs.local_boards_state[r_idx][c_idx], CellState.EMPTY)\n                    for r_idx in range(s) for c_idx in range(s)\n                )\n                if not is_any_move_possible:\n                    self._check_for_overall_game_end()\n                    if self.gs.status != GameStatus.FINISHED:\n                        self.gs.is_draw = True\n                        self.gs.status = GameStatus.FINISHED\n\n        self.gs.updated_at = datetime.now(timezone.utc)\n        self.gs.last_made_move_coords = (move.global_row, move.global_col, move.local_row, move.local_col)\n\n        return True\n\n    def handle_player_disconnect(self, player_id: str):\n        player = self.gs.get_player_info(player_id)\n        app = get_app(GAME_NAME)  # Hol dir die App-Instanz\n        if player:\n            if not player.is_connected:  # Already marked as disconnected\n                app.logger.info(f\"Player {player_id} was already marked as disconnected from game {self.gs.game_id}.\")\n                return\n\n            player.is_connected = False\n            self.gs.updated_at = datetime.now(timezone.utc)\n            app.logger.info(f\"Player {player_id} disconnected from game {self.gs.game_id}. Name: {player.name}\")\n\n            if self.gs.mode == GameMode.ONLINE:\n                if self.gs.status == GameStatus.IN_PROGRESS:\n                    opponent = self.gs.get_opponent_info(player_id)\n                    if opponent and opponent.is_connected:\n                        self.gs.status = GameStatus.ABORTED  # Use ABORTED as \"paused\"\n                        self.gs.player_who_paused = player_id  # Store who disconnected\n                        # This message is for the game state, will be seen by the other player via SSE\n                        self.gs.last_error_message = f\"Player {player.name} disconnected. Waiting for them to rejoin.\"\n                        app.logger.info(\n                            f\"Game {self.gs.game_id} PAUSED, waiting for {player.name} ({player_id}) to reconnect.\")\n                    else:\n                        # Opponent also disconnected or was already gone\n                        self.gs.status = GameStatus.ABORTED\n                        self.gs.last_error_message = \"Both players disconnected. Game aborted.\"\n                        self.gs.player_who_paused = None  # No specific player to wait for\n                        app.logger.info(\n                            f\"Game {self.gs.game_id} ABORTED, both players (or last active player) disconnected.\")\n                elif self.gs.status == GameStatus.WAITING_FOR_OPPONENT:\n                    # If the creator (P1) disconnects while waiting for P2\n                    if len(self.gs.players) == 1 and self.gs.players[0].id == player_id:\n                        self.gs.status = GameStatus.ABORTED\n                        self.gs.last_error_message = \"Game creator disconnected before opponent joined. Game aborted.\"\n                        self.gs.player_who_paused = None\n                        app.logger.info(\n                            f\"Game {self.gs.game_id} ABORTED, creator {player.name} ({player_id}) disconnected while WAITING_FOR_OPPONENT.\")\n                elif self.gs.status == GameStatus.ABORTED and self.gs.player_who_paused:\n                    # Game was already paused (e.g. P1 disconnected), and now P2 (the waiting one) disconnects\n                    if self.gs.player_who_paused != player_id:  # Ensure it's the other player\n                        self.gs.last_error_message = \"Other player also disconnected during pause. Game aborted.\"\n                        self.gs.player_who_paused = None  # No one specific to wait for now\n                        app.logger.info(\n                            f\"Game {self.gs.game_id} ABORTED, waiting player {player.name} ({player_id}) disconnected.\")\n\n    def handle_player_reconnect(self, player_id: str) -&gt; bool:\n        player = self.gs.get_player_info(player_id)\n        app = get_app(GAME_NAME)\n        if not player:\n            app.logger.warning(f\"Reconnect attempt for unknown player {player_id} in game {self.gs.game_id}.\")\n            return False\n\n        if player.is_connected:\n            app.logger.info(\n                f\"Player {player.name} ({player_id}) attempted reconnect but was already marked as connected to game {self.gs.game_id}.\")\n            if self.gs.status == GameStatus.ABORTED and self.gs.player_who_paused == player_id:\n                opponent = self.gs.get_opponent_info(player_id)\n                if opponent and opponent.is_connected:\n                    self.gs.status = GameStatus.IN_PROGRESS\n                    self.gs.last_error_message = f\"Connection for {player.name} re-established. Game resumed.\"\n                    self.gs.player_who_paused = None\n                    self.gs.updated_at = datetime.now(timezone.utc)\n                    app.logger.info(\n                        f\"Game {self.gs.game_id} resumed as already-connected pauser {player.name} re-interacted.\")\n                else:\n                    self.gs.last_error_message = f\"Welcome back, {player.name}! Your opponent is still not connected.\"\n            return True\n\n        player.is_connected = True\n        self.gs.updated_at = datetime.now(timezone.utc)\n        app.logger.info(\n            f\"Player {player.name} ({player_id}) reconnected to game {self.gs.game_id}. Previous status: {self.gs.status}, Paused by: {self.gs.player_who_paused}\")\n\n        if self.gs.status == GameStatus.ABORTED:\n            if self.gs.player_who_paused == player_id:  # The player who caused the pause has reconnected\n                opponent = self.gs.get_opponent_info(player_id)\n                if opponent and opponent.is_connected:\n                    self.gs.status = GameStatus.IN_PROGRESS\n                    self.gs.last_error_message = f\"Player {player.name} reconnected. Game resumed!\"\n                    self.gs.player_who_paused = None\n                    app.logger.info(\n                        f\"Game {self.gs.game_id} RESUMED. Pauser {player.name} reconnected, opponent {opponent.name} is present.\")\n                else:  # Pauser reconnected, opponent (still) gone or never joined (if P1 disconnected from WAITING)\n                    if not opponent and len(\n                        self.gs.players) == 1:  # P1 reconnected to a game they created but no P2 yet\n                        self.gs.status = GameStatus.WAITING_FOR_OPPONENT\n                        self.gs.player_who_paused = None\n                        self.gs.current_player_id = player_id\n                        self.gs.last_error_message = f\"Creator {player.name} reconnected. Waiting for opponent.\"\n                        self.gs.waiting_since = datetime.now(timezone.utc)  # Reset waiting timer\n                    elif opponent:  # Opponent was there but is now disconnected\n                        self.gs.player_who_paused = opponent.id  # Now waiting for the other person\n                        self.gs.last_error_message = f\"Welcome back, {player.name}! Your opponent ({opponent.name}) is not connected. Game remains paused.\"\n                        app.logger.info(\n                            f\"Game {self.gs.game_id} still PAUSED. {player.name} reconnected, but opponent {opponent.name} is NOT. Waiting for {opponent.name}.\")\n                    else:  # Should be rare: 2 players in list, but opponent object not found for P1\n                        self.gs.last_error_message = f\"Welcome back, {player.name}! Opponent details unclear. Game remains paused.\"\n\n\n            elif self.gs.player_who_paused and self.gs.player_who_paused != player_id:\n                # The *other* player reconnected, while game was paused for initial pauser.\n                initial_pauser_info = self.gs.get_player_info(self.gs.player_who_paused)\n                if initial_pauser_info and initial_pauser_info.is_connected:  # This implies both are now connected.\n                    self.gs.status = GameStatus.IN_PROGRESS\n                    self.gs.last_error_message = f\"Both players are now connected. Game resumed!\"\n                    self.gs.player_who_paused = None\n                    app.logger.info(\n                        f\"Game {self.gs.game_id} RESUMED. Waiting player {player.name} reconnected, initial pauser {initial_pauser_info.name} also present.\")\n                else:\n                    self.gs.last_error_message = f\"Welcome back, {player.name}! Still waiting for {initial_pauser_info.name if initial_pauser_info else 'the other player'} to reconnect.\"\n                    app.logger.info(\n                        f\"Game {self.gs.game_id} still PAUSED. Player {player.name} reconnected, but still waiting for original pauser {self.gs.player_who_paused}.\")\n\n            else:  # game is ABORTED but no specific player_who_paused (hard abort by timeout or both disconnected)\n                if len(self.gs.players) == 2:  # Was a two-player game\n                    opponent = self.gs.get_opponent_info(player_id)\n                    if opponent:\n                        # Revive the game to a paused state, waiting for the other player\n                        self.gs.player_who_paused = opponent.id\n                        self.gs.status = GameStatus.ABORTED  # Remains aborted, but now specifically for opponent\n                        self.gs.last_error_message = f\"Welcome back, {player.name}! Game was fully aborted. Now waiting for {opponent.name} to rejoin.\"\n                        app.logger.info(\n                            f\"Game {self.gs.game_id} REVIVED from HARD ABORT by {player.name}. Now paused, waiting for {opponent.name} ({opponent.id}).\")\n                    else:  # Should not happen if two players were in game and player_id is one of them\n                        self.gs.last_error_message = f\"Player {player.name} reconnected, but game state is inconsistent (opponent not found).\"\n                        app.logger.warning(\n                            f\"Game {self.gs.game_id} HARD ABORT revival by {player.name} failed, opponent info missing.\")\n                elif len(self.gs.players) == 1 and self.gs.players[0].id == player_id:\n                    # P1 created, P1 disconnected, game WAITING_FOR_OPPONENT timed out &amp; hard aborted. P1 tries to rejoin.\n                    self.gs.status = GameStatus.WAITING_FOR_OPPONENT\n                    self.gs.player_who_paused = None\n                    self.gs.current_player_id = player_id\n                    self.gs.last_error_message = f\"Creator {player.name} reconnected. Waiting for opponent.\"\n                    self.gs.waiting_since = datetime.now(timezone.utc)  # Reset waiting timer\n                    app.logger.info(\n                        f\"Game {self.gs.game_id} (previously hard aborted while waiting) revived by creator {player.name}. Now WAITING_FOR_OPPONENT.\")\n                else:\n                    self.gs.last_error_message = f\"Player {player.name} reconnected, but the game was aborted and cannot be revived in its current player configuration.\"\n                    app.logger.info(\n                        f\"Game {self.gs.game_id} HARD ABORTED. Player {player.name} reconnected, but game cannot resume in current configuration.\")\n\n\n        elif self.gs.status == GameStatus.IN_PROGRESS:\n            opponent = self.gs.get_opponent_info(player_id)\n            if not opponent or not opponent.is_connected:\n                self.gs.status = GameStatus.ABORTED\n                self.gs.player_who_paused = opponent.id if opponent else None\n                self.gs.last_error_message = f\"Welcome back, {player.name}! Your opponent disconnected while you were away. Waiting for them.\"\n                app.logger.info(\n                    f\"Game {self.gs.game_id} transitions to PAUSED. {player.name} reconnected to IN_PROGRESS, but opponent {opponent.id if opponent else 'N/A'} is gone.\")\n            else:\n                self.gs.last_error_message = f\"Player {player.name} re-established connection during active game.\"\n                app.logger.info(\n                    f\"Player {player.name} ({player_id}) re-established connection to IN_PROGRESS game {self.gs.game_id}.\")\n\n        elif self.gs.status == GameStatus.WAITING_FOR_OPPONENT:\n            if len(self.gs.players) == 1 and self.gs.players[0].id == player_id:\n                self.gs.last_error_message = f\"Creator {player.name} reconnected. Still waiting for opponent.\"\n                self.gs.current_player_id = player_id\n                self.gs.waiting_since = datetime.now(timezone.utc)  # Reset waiting timer\n                app.logger.info(\n                    f\"Creator {player.name} ({player_id}) reconnected to WAITING_FOR_OPPONENT game {self.gs.game_id}.\")\n            else:\n                app.logger.warning(\n                    f\"Non-creator {player.name} or unexpected player count for reconnect to WAITING_FOR_OPPONENT game {self.gs.game_id}.\")\n\n        return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager","title":"<code>WebSocketManager</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager","title":"<code>WebSocketPoolManager</code>","text":"Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>class WebSocketPoolManager:\n    def __init__(self):\n        self.pools: dict[str, dict[str, Any]] = {}\n        self.logger = logging.getLogger(__name__)\n\n    async def create_pool(self, pool_id: str) -&gt; None:\n        \"\"\"Create a new WebSocket pool.\"\"\"\n        if pool_id not in self.pools:\n            self.pools[pool_id] = {\n                'connections': {},\n                'actions': {},\n                'global_actions': {}\n            }\n            self.logger.info(f\"Created new pool: {pool_id}\")\n        else:\n            self.logger.warning(f\"Pool {pool_id} already exists\")\n\n    async def add_connection(self, pool_id: str, connection_id: str, websocket) -&gt; None:\n        \"\"\"Add a WebSocket connection to a pool.\"\"\"\n        if pool_id not in self.pools:\n            await self.create_pool(pool_id)\n\n        self.pools[pool_id]['connections'][connection_id] = websocket\n        self.logger.info(f\"Added connection {connection_id} to pool {pool_id}\")\n\n    async def remove_connection(self, pool_id: str, connection_id: str) -&gt; None:\n        \"\"\"Remove a WebSocket connection from a pool.\"\"\"\n        if pool_id in self.pools and connection_id in self.pools[pool_id]['connections']:\n            del self.pools[pool_id]['connections'][connection_id]\n            self.logger.info(f\"Removed connection {connection_id} from pool {pool_id}\")\n        else:\n            self.logger.warning(f\"Connection {connection_id} not found in pool {pool_id}\")\n\n    def register_action(self, pool_id: str, action_name: str, handler: Callable,\n                        connection_ids: list[str] = None) -&gt; None:\n        \"\"\"Register an action for specific connections or the entire pool.\"\"\"\n        if pool_id not in self.pools:\n            self.logger.error(f\"Pool {pool_id} does not exist\")\n            return\n\n        if connection_ids is None:\n            self.pools[pool_id]['global_actions'][action_name] = handler\n            self.logger.info(f\"Registered global action {action_name} for pool {pool_id}\")\n        else:\n            for conn_id in connection_ids:\n                if conn_id not in self.pools[pool_id]['actions']:\n                    self.pools[pool_id]['actions'][conn_id] = {}\n                self.pools[pool_id]['actions'][conn_id][action_name] = handler\n            self.logger.info(f\"Registered action {action_name} for connections {connection_ids} in pool {pool_id}\")\n\n    async def handle_message(self, pool_id: str, connection_id: str, message: str) -&gt; None:\n        \"\"\"Handle incoming messages and route them to the appropriate action handler.\"\"\"\n        if pool_id not in self.pools or connection_id not in self.pools[pool_id]['connections']:\n            self.logger.error(f\"Invalid pool_id or connection_id: {pool_id}, {connection_id}\")\n            return\n\n        try:\n            data = json.loads(message)\n            action = data.get('action')\n\n            if action:\n                if action in self.pools[pool_id]['global_actions']:\n                    await self.pools[pool_id]['global_actions'][action](pool_id, connection_id, data)\n                elif connection_id in self.pools[pool_id]['actions'] and action in self.pools[pool_id]['actions'][\n                    connection_id]:\n                    await self.pools[pool_id]['actions'][connection_id][action](pool_id, connection_id, data)\n                else:\n                    self.logger.warning(f\"No handler found for action {action} in pool {pool_id}\")\n            else:\n                self.logger.warning(f\"No action specified in message from {connection_id} in pool {pool_id}\")\n        except json.JSONDecodeError:\n            self.logger.error(f\"Invalid JSON received from {connection_id} in pool {pool_id}\")\n\n    async def broadcast(self, pool_id: str, message: str, exclude_connection_id: str = None) -&gt; None:\n        \"\"\"Broadcast a message to all connections in a pool, optionally excluding one connection.\"\"\"\n        if pool_id not in self.pools:\n            self.logger.error(f\"Pool {pool_id} does not exist\")\n            return\n\n        for conn_id, websocket in self.pools[pool_id]['connections'].items():\n            if conn_id != exclude_connection_id:\n                try:\n                    await websocket.send_text(message)\n                except Exception as e:\n                    self.logger.error(f\"Error sending message to {conn_id} in pool {pool_id}: {str(e)}\")\n\n    async def send_to_connection(self, pool_id: str, connection_id: str, message: str) -&gt; None:\n        \"\"\"Send a message to a specific connection in a pool.\"\"\"\n        if pool_id in self.pools and connection_id in self.pools[pool_id]['connections']:\n            try:\n                await self.pools[pool_id]['connections'][connection_id].send_text(message)\n            except Exception as e:\n                self.logger.error(f\"Error sending message to {connection_id} in pool {pool_id}: {str(e)}\")\n        else:\n            self.logger.error(f\"Connection {connection_id} not found in pool {pool_id}\")\n\n    def get_pool_connections(self, pool_id: str) -&gt; list[str]:\n        \"\"\"Get a list of all connection IDs in a pool.\"\"\"\n        if pool_id in self.pools:\n            return list(self.pools[pool_id]['connections'].keys())\n        else:\n            self.logger.error(f\"Pool {pool_id} does not exist\")\n            return []\n\n    def get_all_pools(self) -&gt; list[str]:\n        \"\"\"Get a list of all pool IDs.\"\"\"\n        return list(self.pools.keys())\n\n    async def close_pool(self, pool_id: str) -&gt; None:\n        \"\"\"Close all connections in a pool and remove the pool.\"\"\"\n        if pool_id in self.pools:\n            for websocket in self.pools[pool_id]['connections'].values():\n                await websocket.close()\n            del self.pools[pool_id]\n            self.logger.info(f\"Closed and removed pool {pool_id}\")\n        else:\n            self.logger.warning(f\"Pool {pool_id} does not exist\")\n\n    async def close_all_pools(self) -&gt; None:\n        \"\"\"Close all connections in all pools and remove all pools.\"\"\"\n        for pool_id in list(self.pools.keys()):\n            await self.close_pool(pool_id)\n        self.logger.info(\"Closed all pools\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.add_connection","title":"<code>add_connection(pool_id, connection_id, websocket)</code>  <code>async</code>","text":"<p>Add a WebSocket connection to a pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def add_connection(self, pool_id: str, connection_id: str, websocket) -&gt; None:\n    \"\"\"Add a WebSocket connection to a pool.\"\"\"\n    if pool_id not in self.pools:\n        await self.create_pool(pool_id)\n\n    self.pools[pool_id]['connections'][connection_id] = websocket\n    self.logger.info(f\"Added connection {connection_id} to pool {pool_id}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.broadcast","title":"<code>broadcast(pool_id, message, exclude_connection_id=None)</code>  <code>async</code>","text":"<p>Broadcast a message to all connections in a pool, optionally excluding one connection.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def broadcast(self, pool_id: str, message: str, exclude_connection_id: str = None) -&gt; None:\n    \"\"\"Broadcast a message to all connections in a pool, optionally excluding one connection.\"\"\"\n    if pool_id not in self.pools:\n        self.logger.error(f\"Pool {pool_id} does not exist\")\n        return\n\n    for conn_id, websocket in self.pools[pool_id]['connections'].items():\n        if conn_id != exclude_connection_id:\n            try:\n                await websocket.send_text(message)\n            except Exception as e:\n                self.logger.error(f\"Error sending message to {conn_id} in pool {pool_id}: {str(e)}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.close_all_pools","title":"<code>close_all_pools()</code>  <code>async</code>","text":"<p>Close all connections in all pools and remove all pools.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def close_all_pools(self) -&gt; None:\n    \"\"\"Close all connections in all pools and remove all pools.\"\"\"\n    for pool_id in list(self.pools.keys()):\n        await self.close_pool(pool_id)\n    self.logger.info(\"Closed all pools\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.close_pool","title":"<code>close_pool(pool_id)</code>  <code>async</code>","text":"<p>Close all connections in a pool and remove the pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def close_pool(self, pool_id: str) -&gt; None:\n    \"\"\"Close all connections in a pool and remove the pool.\"\"\"\n    if pool_id in self.pools:\n        for websocket in self.pools[pool_id]['connections'].values():\n            await websocket.close()\n        del self.pools[pool_id]\n        self.logger.info(f\"Closed and removed pool {pool_id}\")\n    else:\n        self.logger.warning(f\"Pool {pool_id} does not exist\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.create_pool","title":"<code>create_pool(pool_id)</code>  <code>async</code>","text":"<p>Create a new WebSocket pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def create_pool(self, pool_id: str) -&gt; None:\n    \"\"\"Create a new WebSocket pool.\"\"\"\n    if pool_id not in self.pools:\n        self.pools[pool_id] = {\n            'connections': {},\n            'actions': {},\n            'global_actions': {}\n        }\n        self.logger.info(f\"Created new pool: {pool_id}\")\n    else:\n        self.logger.warning(f\"Pool {pool_id} already exists\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.get_all_pools","title":"<code>get_all_pools()</code>","text":"<p>Get a list of all pool IDs.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>def get_all_pools(self) -&gt; list[str]:\n    \"\"\"Get a list of all pool IDs.\"\"\"\n    return list(self.pools.keys())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.get_pool_connections","title":"<code>get_pool_connections(pool_id)</code>","text":"<p>Get a list of all connection IDs in a pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>def get_pool_connections(self, pool_id: str) -&gt; list[str]:\n    \"\"\"Get a list of all connection IDs in a pool.\"\"\"\n    if pool_id in self.pools:\n        return list(self.pools[pool_id]['connections'].keys())\n    else:\n        self.logger.error(f\"Pool {pool_id} does not exist\")\n        return []\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.handle_message","title":"<code>handle_message(pool_id, connection_id, message)</code>  <code>async</code>","text":"<p>Handle incoming messages and route them to the appropriate action handler.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def handle_message(self, pool_id: str, connection_id: str, message: str) -&gt; None:\n    \"\"\"Handle incoming messages and route them to the appropriate action handler.\"\"\"\n    if pool_id not in self.pools or connection_id not in self.pools[pool_id]['connections']:\n        self.logger.error(f\"Invalid pool_id or connection_id: {pool_id}, {connection_id}\")\n        return\n\n    try:\n        data = json.loads(message)\n        action = data.get('action')\n\n        if action:\n            if action in self.pools[pool_id]['global_actions']:\n                await self.pools[pool_id]['global_actions'][action](pool_id, connection_id, data)\n            elif connection_id in self.pools[pool_id]['actions'] and action in self.pools[pool_id]['actions'][\n                connection_id]:\n                await self.pools[pool_id]['actions'][connection_id][action](pool_id, connection_id, data)\n            else:\n                self.logger.warning(f\"No handler found for action {action} in pool {pool_id}\")\n        else:\n            self.logger.warning(f\"No action specified in message from {connection_id} in pool {pool_id}\")\n    except json.JSONDecodeError:\n        self.logger.error(f\"Invalid JSON received from {connection_id} in pool {pool_id}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.register_action","title":"<code>register_action(pool_id, action_name, handler, connection_ids=None)</code>","text":"<p>Register an action for specific connections or the entire pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>def register_action(self, pool_id: str, action_name: str, handler: Callable,\n                    connection_ids: list[str] = None) -&gt; None:\n    \"\"\"Register an action for specific connections or the entire pool.\"\"\"\n    if pool_id not in self.pools:\n        self.logger.error(f\"Pool {pool_id} does not exist\")\n        return\n\n    if connection_ids is None:\n        self.pools[pool_id]['global_actions'][action_name] = handler\n        self.logger.info(f\"Registered global action {action_name} for pool {pool_id}\")\n    else:\n        for conn_id in connection_ids:\n            if conn_id not in self.pools[pool_id]['actions']:\n                self.pools[pool_id]['actions'][conn_id] = {}\n            self.pools[pool_id]['actions'][conn_id][action_name] = handler\n        self.logger.info(f\"Registered action {action_name} for connections {connection_ids} in pool {pool_id}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.remove_connection","title":"<code>remove_connection(pool_id, connection_id)</code>  <code>async</code>","text":"<p>Remove a WebSocket connection from a pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def remove_connection(self, pool_id: str, connection_id: str) -&gt; None:\n    \"\"\"Remove a WebSocket connection from a pool.\"\"\"\n    if pool_id in self.pools and connection_id in self.pools[pool_id]['connections']:\n        del self.pools[pool_id]['connections'][connection_id]\n        self.logger.info(f\"Removed connection {connection_id} from pool {pool_id}\")\n    else:\n        self.logger.warning(f\"Connection {connection_id} not found in pool {pool_id}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.send_to_connection","title":"<code>send_to_connection(pool_id, connection_id, message)</code>  <code>async</code>","text":"<p>Send a message to a specific connection in a pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def send_to_connection(self, pool_id: str, connection_id: str, message: str) -&gt; None:\n    \"\"\"Send a message to a specific connection in a pool.\"\"\"\n    if pool_id in self.pools and connection_id in self.pools[pool_id]['connections']:\n        try:\n            await self.pools[pool_id]['connections'][connection_id].send_text(message)\n        except Exception as e:\n            self.logger.error(f\"Error sending message to {connection_id} in pool {pool_id}: {str(e)}\")\n    else:\n        self.logger.error(f\"Connection {connection_id} not found in pool {pool_id}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb","title":"<code>WhatsAppTb</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client","title":"<code>client</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.DocumentSystem","title":"<code>DocumentSystem</code>","text":"Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>class DocumentSystem:\n    def __init__(self, storage: BlobStorage):\n        self.storage = storage\n        self.media_types = {\n            'document': ['pdf', 'doc', 'docx', 'txt'],\n            'image': ['jpg', 'jpeg', 'png', 'gif'],\n            'video': ['mp4', 'mov', 'avi']\n        }\n\n    def list_documents(self, filter_type: str = None) -&gt; list[dict]:\n        \"\"\"List all documents with metadata\"\"\"\n        docs = []\n        for blob_id in self.storage._get_all_blob_ids():\n            with BlobFile(blob_id, 'r', self.storage) as f:\n                metadata = f.read_json()\n                if metadata:\n                    docs.append({\n                        'id': blob_id,\n                        'name': metadata.get('filename', blob_id),\n                        'type': metadata.get('type', 'document'),\n                        'size': metadata.get('size', 0),\n                        'modified': metadata.get('timestamp', ''),\n                        'preview': metadata.get('preview', '')\n                    })\n        if filter_type:\n            return [d for d in docs if d['type'] == filter_type]\n        return docs\n\n    def save_document(self, file_data: bytes, filename: str, file_type: str) -&gt; str:\n        \"\"\"Save a document with metadata\"\"\"\n        blob_id = self.storage._generate_blob_id()\n        metadata = {\n            'filename': filename,\n            'type': file_type,\n            'size': len(file_data),\n            'timestamp': datetime.now().isoformat(),\n            'preview': self._generate_preview(file_data, file_type)\n        }\n\n        with BlobFile(blob_id, 'w', self.storage) as f:\n            f.write_json(metadata)\n            f.write(file_data)\n        return blob_id\n\n    def delete_document(self, blob_id: str) -&gt; bool:\n        \"\"\"Delete a document\"\"\"\n        try:\n            self.storage.delete_blob(blob_id)\n            return True\n        except Exception as e:\n            logging.error(f\"Delete failed: {str(e)}\")\n            return False\n\n    def search_documents(self, query: str) -&gt; list[dict]:\n        \"\"\"Search documents by filename or content\"\"\"\n        results = []\n        for doc in self.list_documents():\n            if query.lower() in doc['name'].lower() or self._search_in_content(doc['id'], query):\n                results.append(doc)\n        return results\n\n    def _generate_preview(self, data: bytes, file_type: str) -&gt; str:\n        \"\"\"Generate preview based on file type\"\"\"\n        if file_type in self.media_types['image']:\n            return f\"Image preview: {data[:100].hex()}\"\n        elif file_type in self.media_types['video']:\n            return \"Video preview unavailable\"\n        return data[:100].decode('utf-8', errors='ignore')\n\n    def _search_in_content(self, blob_id: str, query: str) -&gt; bool:\n        \"\"\"Search content within documents\"\"\"\n        try:\n            with BlobFile(blob_id, 'r', self.storage) as f:\n                content = f.read().decode('utf-8', errors='ignore')\n                return query.lower() in content.lower()\n        except:\n            return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.DocumentSystem.delete_document","title":"<code>delete_document(blob_id)</code>","text":"<p>Delete a document</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def delete_document(self, blob_id: str) -&gt; bool:\n    \"\"\"Delete a document\"\"\"\n    try:\n        self.storage.delete_blob(blob_id)\n        return True\n    except Exception as e:\n        logging.error(f\"Delete failed: {str(e)}\")\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.DocumentSystem.list_documents","title":"<code>list_documents(filter_type=None)</code>","text":"<p>List all documents with metadata</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def list_documents(self, filter_type: str = None) -&gt; list[dict]:\n    \"\"\"List all documents with metadata\"\"\"\n    docs = []\n    for blob_id in self.storage._get_all_blob_ids():\n        with BlobFile(blob_id, 'r', self.storage) as f:\n            metadata = f.read_json()\n            if metadata:\n                docs.append({\n                    'id': blob_id,\n                    'name': metadata.get('filename', blob_id),\n                    'type': metadata.get('type', 'document'),\n                    'size': metadata.get('size', 0),\n                    'modified': metadata.get('timestamp', ''),\n                    'preview': metadata.get('preview', '')\n                })\n    if filter_type:\n        return [d for d in docs if d['type'] == filter_type]\n    return docs\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.DocumentSystem.save_document","title":"<code>save_document(file_data, filename, file_type)</code>","text":"<p>Save a document with metadata</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def save_document(self, file_data: bytes, filename: str, file_type: str) -&gt; str:\n    \"\"\"Save a document with metadata\"\"\"\n    blob_id = self.storage._generate_blob_id()\n    metadata = {\n        'filename': filename,\n        'type': file_type,\n        'size': len(file_data),\n        'timestamp': datetime.now().isoformat(),\n        'preview': self._generate_preview(file_data, file_type)\n    }\n\n    with BlobFile(blob_id, 'w', self.storage) as f:\n        f.write_json(metadata)\n        f.write(file_data)\n    return blob_id\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.DocumentSystem.search_documents","title":"<code>search_documents(query)</code>","text":"<p>Search documents by filename or content</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def search_documents(self, query: str) -&gt; list[dict]:\n    \"\"\"Search documents by filename or content\"\"\"\n    results = []\n    for doc in self.list_documents():\n        if query.lower() in doc['name'].lower() or self._search_in_content(doc['id'], query):\n            results.append(doc)\n    return results\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant","title":"<code>WhatsAppAssistant</code>  <code>dataclass</code>","text":"Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>@dataclass\nclass WhatsAppAssistant:\n    whc: WhClient\n    isaa: 'Tools'\n    agent: Optional['Agent'] = None\n    credentials: Credentials | None = None\n    state: AssistantState = AssistantState.OFFLINE\n\n    # Service clients\n    gmail_service: Any = None\n    calendar_service: Any = None\n\n    start_time: Any = None\n\n    blob_docs_system: Any = None\n    duration_minutes: int = 20\n    credentials_path: str = \"/root/Toolboxv2/credentials.json\"\n    # Progress messengers\n    progress_messengers: dict[str, 'ProgressMessenger'] = field(default_factory=dict)\n    buttons: dict[str, dict] = field(default_factory=dict)\n    history: FileCache = field(default_factory=FileCache)\n\n    pending_actions: dict[str, dict] = field(default_factory=dict)\n\n\n    def __post_init__(self):\n\n        self.start_time = datetime.now()\n        self.processed_messages = set()\n        self.message_lock = threading.Lock()\n        self.audio_processor = None\n        self.blob_docs_system = DocumentSystem(BlobStorage())\n        self.stt = get_app().run_any(TBEF.AUDIO.STT_GENERATE,\n                                     model=\"openai/whisper-small\",\n                                     row=False, device=1)\n\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n\n        self.load_credentials()\n        self.setup_progress_messengers()\n        self.setup_interaction_buttons()\n        self.history = FileCache(folder=\".data/WhatsAppAssistant\")\n        self.state = AssistantState.ONLINE\n\n    async def generate_authorization_url(self, *a):\n        \"\"\"\n        Generate an authorization URL for user consent\n\n        :return: Authorization URL for the user to click and authorize access\n        \"\"\"\n        from google_auth_oauthlib.flow import Flow\n        # Define the scopes required for Gmail and Calendar\n        SCOPES = [\n            'https://www.googleapis.com/auth/gmail.modify',\n            'https://www.googleapis.com/auth/calendar'\n        ]\n\n        # Create a flow instance to manage the OAuth 2.0 authorization process\n        flow = Flow.from_client_secrets_file(\n            self.credentials_path,\n            scopes=SCOPES,\n            redirect_uri='urn:ietf:wg:oauth:2.0:oob'  # Use 'urn:ietf:wg:oauth:2.0:oob' for desktop apps\n        )\n\n        # Generate the authorization URL\n        authorization_url, _ = flow.authorization_url(\n            access_type='offline',  # Allows obtaining refresh token\n            prompt='consent'  # Ensures user is always prompted for consent\n        )\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {'type': 'auth',\n                                                                              'step': 'awaiting_key'}\n        return {\n            'type': 'quick_reply',\n            'text': f'Url to log in {authorization_url}',\n            'options': {'cancel': '\u274c Cancel Upload'}\n        }\n\n    def complete_authorization(self, message: Message):\n        \"\"\"\n        Complete the authorization process using the authorization code\n\n        :param authorization_code: Authorization code received from Google\n        \"\"\"\n        from google_auth_oauthlib.flow import Flow\n        authorization_code = message.content\n        # Define the scopes required for Gmail and Calendar\n        SCOPES = [\n            'https://www.googleapis.com/auth/gmail.modify',\n            'https://www.googleapis.com/auth/calendar'\n        ]\n\n        # Create a flow instance to manage the OAuth 2.0 authorization process\n        flow = Flow.from_client_secrets_file(\n            self.credentials_path,\n            scopes=SCOPES,\n            redirect_uri='urn:ietf:wg:oauth:2.0:oob'\n        )\n\n        # Exchange the authorization code for credentials\n        flow.fetch_token(code=authorization_code)\n        self.credentials = flow.credentials\n\n        # Save the credentials for future use\n        self.save_credentials()\n\n        # Initialize services\n        self.init_services()\n        return \"Done\"\n\n\n    def save_credentials(self):\n        \"\"\"\n        Save the obtained credentials to a file for future use\n        \"\"\"\n        if not os.path.exists('token'):\n            os.makedirs('token')\n\n        with open('token/google_token.json', 'w') as token_file:\n            token_file.write(self.credentials.to_json())\n\n\n    def load_credentials(self):\n        \"\"\"\n        Load previously saved credentials if available\n\n        :return: Whether credentials were successfully loaded\n        \"\"\"\n        try:\n            self.credentials = Credentials.from_authorized_user_file('token/google_token.json')\n            self.init_services()\n            return True\n        except FileNotFoundError:\n            return False\n\n\n    def init_services(self):\n        \"\"\"\n        Initialize Gmail and Calendar services\n        \"\"\"\n        from googleapiclient.discovery import build\n\n        self.gmail_service = build('gmail', 'v1', credentials=self.credentials)\n        self.calendar_service = build('calendar', 'v3', credentials=self.credentials)\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n\n    def setup_progress_messengers(self):\n        \"\"\"Initialize progress messengers for different types of tasks\"\"\"\n        self.progress_messengers = {\n            'task': self.whc.progress_messenger0,\n            'email': self.whc.progress_messenger1,\n            'calendar': self.whc.progress_messenger2\n        }\n\n    def setup_interaction_buttons(self):\n        \"\"\"Define WhatsApp interaction buttons for different functionalities\"\"\"\n        self.buttons = {\n            'menu': {\n                'header': 'Digital Assistant',\n                'body': 'Please select an option:',\n                'footer': '-- + --',\n                'action': {\n                    'button': 'Menu',\n                    'sections': [\n                        {\n                            'title': 'Main Functions',\n                            'rows': [\n                                {'id': 'agent', 'title': 'Agent Controls', 'description': 'Manage your AI assistant'},\n                                {'id': 'email', 'title': 'Email Management', 'description': 'Handle your emails'},\n                                {'id': 'calendar', 'title': 'Calendar', 'description': 'Manage your schedule'},\n                                {'id': 'docs', 'title': 'Documents', 'description': 'Handle documents'},\n                                {'id': 'system', 'title': 'System', 'description': 'System controls and metrics'}\n                            ]\n                        }\n                    ]\n                }\n            },\n            'agent': self._create_agent_controls_buttons(),\n            'email': self._create_email_controls_buttons(),\n            'calendar': self._create_calendar_controls_buttons(),\n            'docs': self._create_docs_controls_buttons(),\n            'system': self._create_system_controls_buttons()\n        }\n\n    @staticmethod\n    def _create_agent_controls_buttons():\n        return {\n            'header': 'Agent Controls',\n            'body': 'Manage your AI assistant:',\n            'action': {\n                'button': 'Select',\n                'sections': [\n                    {\n                        'title': 'Basic Actions',\n                        'rows': [\n                            {'id': 'agent-task', 'title': 'Agent Task', 'description': 'Run the agent'},\n                            {'id': 'start', 'title': 'Start Agent', 'description': 'Run taskstack in background'},\n                            {'id': 'stop', 'title': 'Stop Agent', 'description': 'Stop taskstack execution'}\n                        ]\n                    },\n                    {\n                        'title': 'Advanced Actions',\n                        'rows': [\n                            {'id': 'system-task', 'title': 'System Task',\n                             'description': 'Run the Isaa Reasoning Agent system'},\n                            {'id': 'tasks', 'title': 'Task Stack', 'description': 'View and manage tasks'},\n                            {'id': 'memory', 'title': 'Clear Memory', 'description': 'Reset agent memory'}\n                        ]\n                    }\n                ]\n            }\n        }\n\n    @staticmethod\n    def _create_email_controls_buttons():\n        return {\n            'header': 'Email Management',\n            'body': 'Handle your emails:',\n            'action': {\n                'button': 'Select',\n                'sections': [\n                    {\n                        'title': 'Basic Actions',\n                        'rows': [\n                            {'id': 'check', 'title': 'Check Emails', 'description': 'View recent emails'},\n                            {'id': 'send', 'title': 'Send Email', 'description': 'Compose new email'},\n                            {'id': 'summary', 'title': 'Get Summary', 'description': 'Summarize emails'}\n                        ]\n                    },\n                    {\n                        'title': 'Advanced Actions',\n                        'rows': [\n                            {'id': 'search', 'title': 'Search', 'description': 'Search emails'}\n                        ]\n                    }\n                ]\n            }\n        }\n\n    @staticmethod\n    def _create_calendar_controls_buttons():\n        return {\n            'header': 'Calendar Management',\n            'body': 'Manage your schedule:',\n            'action': {\n                'button': 'Select',\n                'sections': [\n                    {\n                        'title': 'Basic Actions',\n                        'rows': [\n                            {'id': 'today', 'title': 'Today\\'s Events', 'description': 'View today\\'s schedule'},\n                            {'id': 'add', 'title': 'Add Event', 'description': 'Create new event'},\n                            {'id': 'upcoming', 'title': 'Upcoming', 'description': 'View upcoming events'}\n                        ]\n                    },\n                    {\n                        'title': 'Advanced Actions',\n                        'rows': [\n                            {'id': 'find_slot', 'title': 'Find Time Slot', 'description': 'Find available time'}\n                        ]\n                    }\n                ]\n            }\n        }\n\n    @staticmethod\n    def _create_docs_controls_buttons():\n        return {\n            'header': 'Document Management',\n            'body': 'Handle your documents:',\n            'action': {\n                'button': 'Select',\n                'sections': [\n                    {\n                        'title': 'Basic Actions',\n                        'rows': [\n                            {'id': 'upload', 'title': 'Upload', 'description': 'Add new document'},\n                            {'id': 'list', 'title': 'List Documents', 'description': 'View all documents'},\n                            {'id': 'search', 'title': 'Search', 'description': 'Search documents'}\n                        ]\n                    },\n                    {\n                        'title': 'Advanced Actions',\n                        'rows': [\n                            {'id': 'delete', 'title': 'Delete', 'description': 'Remove document'}\n                        ]\n                    }\n                ]\n            }\n        }\n\n    @staticmethod\n    def _create_system_controls_buttons():\n        return {\n            'header': 'System Controls',\n            'body': 'System management:',\n            'action': {\n                'button': 'Select',\n                'sections': [\n                    {\n                        'title': 'Basic Actions',\n                        'rows': [\n                            {'id': 'status', 'title': 'System Status', 'description': 'View current status'},\n                            {'id': 'restart', 'title': 'Restart', 'description': 'Restart system'},\n                            {'id': 'connect', 'title': 'Connect', 'description': 'Connect to Google Calendar and Email'}\n                        ]\n                    }\n                ]\n            }\n        }\n\n    async def handle_message(self, message: 'Message'):\n        \"\"\"Main message handler for incoming WhatsApp messages\"\"\"\n\n        # Deduplication check\n        with self.message_lock:\n            if message.id in self.processed_messages:\n                return\n            last_ts = time.time()\n            print(last_ts)\n            if len(self.processed_messages) &gt; 0:\n                m_id, last_ts = self.processed_messages.pop()\n                self.processed_messages.add((m_id, last_ts))\n\n            print(\"DUPLICATION P\", message.data.get('entry', [{}])[0].get('changes', [{}])[0].get('value', {}).get('messages', [{}])[0].get('timestamp', 0) , last_ts)\n            if float(message.data.get('entry', [{}])[0].get('changes', [{}])[0].get('value', {}).get('messages', [{}])[0].get('timestamp', 0)) &lt; last_ts - 120:\n                return\n            self.processed_messages.add((message.id, time.perf_counter()))\n\n        # Mark message as read\n        message.mark_as_read()\n\n        # Extract content and type\n        content_type = message.type\n        content = message.content\n\n        print(f\"message.content {content=} {content_type=} {message.data=}\")\n\n        try:\n            if content_type == 'interactive':\n                await self.handle_interactive(message)\n            elif content_type == 'audio':\n                await self.handle_audio_message(message)\n            elif content_type in ['document', 'image', 'video']:\n                response = await self.handle_media_message(message)\n                self.save_reply(message, response)\n            elif content_type == 'text':\n                if content.lower() == \"menu\":\n                    self.whc.messenger.send_button(\n                        recipient_id=self.whc.progress_messenger0.recipient_phone,\n                        button=self.buttons[content.lower()]\n                    )\n                else:\n                    await self.helper_text(message)\n            else:\n                message.reply(\"Unsupported message type\")\n        #except Exception as e:\n        #    logging.error(f\"Message handling error: {str(e)}\")\n        #   message.reply(\"\u274c Error processing request\")\n        finally:\n            # Cleanup old messages (keep 1 hour history)\n            with self.message_lock:\n                self._clean_processed_messages()\n\n    async def helper_text(self, message: 'Message', return_text=False):\n        if not isinstance(message.content, str) and not len(message.content) &gt; 0:\n            content = self.whc.messenger.get_message(message.data)\n            print(f\"contents {content=}, {message.content=}\")\n            message.content = content\n        self.history.set(message.id, message.content)\n        if len(self.pending_actions[self.whc.progress_messenger0.recipient_phone].keys()) != 0:\n            message.reply(\n                f\"Open Interaction : {json.dumps(self.pending_actions[self.whc.progress_messenger0.recipient_phone], indent=2)}\")\n            if self.pending_actions[self.whc.progress_messenger0.recipient_phone].get('type') == 'auth':\n                res = self.complete_authorization(message)\n                self.save_reply(message, res)\n            res = await self.handle_calendar_actions(message)\n            if res:\n                self.save_reply(message, res)\n                return\n            res2 = await self.handle_email_actions(message)\n            if res2:\n                self.save_reply(message, res2)\n                return\n            await self.handle_agent_actions(message)\n            return\n        await self.handle_agent_actions(message)\n\n    async def handle_interactive(self, message: Message):\n        \"\"\"Handle all interactive messages\"\"\"\n        content = self.whc.messenger.get_interactive_response(message.data)\n        if content.get(\"type\") == \"list_reply\":\n            await self.handle_button_interaction(content.get(\"list_reply\"), message)\n        elif content.get(\"type\") == \"button_reply\":\n            print(content)\n\n    async def handle_audio_message(self, message: 'Message'):\n        \"\"\"Process audio messages with STT and TTS\"\"\"\n        # Download audio\n        progress = self.progress_messengers['task']\n        stop_flag = threading.Event()\n        # message_id = progress.send_initial_message(mode=\"loading\")\n        progress.message_id = message.id\n        progress.start_loading_in_background(stop_flag)\n\n        content = self.whc.messenger.get_audio(message.data)\n        audio_file_name = self.whc.messenger.download_media(media_url=self.whc.messenger.query_media_url(media_id=content.get('id')), mime_type='audio/opus', file_path=\".data/temp\")\n        print(f\"audio_file_name {audio_file_name}\")\n        if audio_file_name is None:\n            message.reply(\"Could not process audio file\")\n            stop_flag.set()\n            return\n\n        text = self.stt(audio_file_name)['text']\n        if not text:\n            message.reply(\"Could not process audio\")\n            stop_flag.set()\n            return\n\n        message.reply(\"Transcription :\\n \"+ text)\n        message.content = text\n        agent_res = await self.helper_text(message, return_text=True)\n\n        if agent_res is not None:\n            pass\n\n        stop_flag.set()\n        # Process text and get response\n        # response = await self.process_input(text, message)\n\n        # Convert response to audio\n        #audio_file = self.audio_processor.tts(response)\n        #audio_file = None # TODO\n        #self.whc.messenger.send_audio(\n        #    audio=audio_file,\n        #    recipient_id=self.whc.progress_messenger0.recipient_phone,\n        #)\n\n    async def confirm(self, message: Message):\n        status = self.pending_actions[self.whc.progress_messenger0.recipient_phone]\n        if status.get('type') == \"create_event\":\n            if status.get('step') == \"confirm_envet\":\n                event = self._create_calendar_event(status.get('event_data'))\n                self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n                return f\"\u2705 Event created!\\n{event.get('htmlLink')}\"\n            return \"\u274c\"\n        elif status.get('type') == \"compose_email\":\n            if status.get('step') == \"confirm_email\":\n                # Send email\n                result = self.gmail_service.users().messages().send(\n                    userId='me',\n                    body=self._build_email_draft(status['draft'])\n                ).execute()\n                self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n                return f\"\u2705 Email sent! Message ID: {result['id']}\"\n            return \"\u274c\"\n        return \"\u274c Done\"\n\n    async def cancel(self, *a):\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n        return \"\u2705 cancel Done\"\n\n    async def handle_button_interaction(self, content: dict, message: Message):\n        \"\"\"Handle button click interactions\"\"\"\n        button_id = content['id']\n\n        # First check if it's a main menu button\n        if button_id in self.buttons:\n            self.whc.messenger.send_button(\n                recipient_id=self.whc.progress_messenger0.recipient_phone,\n                button=self.buttons[button_id]\n            )\n            return\n\n        # Handle action buttons\n        action_handlers = {\n            # Agent controls\n            'start': self.start_agent,\n            'stop': self.stop_agent,\n            'tasks': self.show_task_stack,\n            'memory': self.clear_memory,\n            'system-task': self.system_task,\n            'agent-task': self.agent_task,\n\n            # Email controls\n            'check': self.check_emails,\n            'send': self.start_email_compose,\n            'summary': self.email_summary,\n            'search': self.email_search,\n\n            # Calendar controls\n            'today': self.show_today_events,\n            'add': self.start_event_create,\n            'upcoming': self.show_upcoming_events,\n            'find_slot': self.find_time_slot,\n\n            # Document controls\n            'upload': self.start_document_upload,\n            'list': self.list_documents,\n            'search_docs': self.search_documents,\n            'delete': self.delete_document,\n\n            # System controls\n            'status': self.system_status,\n            'restart': self.restart_system,\n            'connect': self.generate_authorization_url,\n\n            'cancel': self.cancel,\n            'confirm': self.confirm,\n        }\n        if button_id in action_handlers:\n            try:\n                # Start progress indicator\n                progress = self.progress_messengers['task']\n                stop_flag = threading.Event()\n                # message_id = progress.send_initial_message(mode=\"loading\")\n                progress.message_id = message.id\n                progress.start_loading_in_background(stop_flag)\n\n                # Execute handler\n\n                result = await action_handlers[button_id](message)\n\n\n                # Send result\n                if isinstance(result, str):\n                    self.save_reply(message, result)\n                elif isinstance(result, dict):  # For structured responses\n                    self.send_structured_response(result)\n\n                stop_flag.set()\n            finally:\n                #except Exception as e:\n                stop_flag.set()\n            #    message.reply(f\"\u274c Error processing {button_id}: {str(e)}\")\n        elif 'event_' in button_id:\n            res = await self.get_event_details(button_id.replace(\"event_\", ''))\n            if isinstance(res, str):\n                self.save_reply(message, res)\n                return\n            for r in res:\n                if isinstance(r, str):\n                    self.save_reply(message, r)\n                else:\n                    self.whc.messenger.send_location(**r)\n\n        elif 'email_' in button_id:\n            res = await self.get_email_details(button_id.replace(\"email_\", ''))\n            self.save_reply(message, res)\n        else:\n            message.reply(\"\u26a0\ufe0f Unknown command\")\n\n    def send_structured_response(self, result: dict):\n        \"\"\"Send complex responses using appropriate WhatsApp features\"\"\"\n        if result['type'] == 'list':\n            self.whc.messenger.send_button(\n                recipient_id=self.whc.progress_messenger0.recipient_phone,\n                button={\n                    'header': result.get('header', ''),\n                    'body': result.get('body', ''),\n                    'footer': result.get('footer', ''),\n                    'action': {\n                        'button': 'Action',\n                        'sections': result['sections']\n                    }\n                }\n            )\n        elif result['type'] == 'quick_reply':\n            self.whc.messenger.send_button(\n                recipient_id=self.whc.progress_messenger0.recipient_phone,\n                button={\n                    'header': \"Quick reply\",\n                    'body': result['text'],\n                    'footer': '',\n                    'action': {'button': 'Action', 'sections': [{\n                        'title': 'View',\n                        'rows': [{'id': k, 'title': v[:23]} for k, v in result['options'].items()]\n                    }]}\n                }\n            )\n\n        elif result['type'] == 'media':\n            if result['media_type'] == 'image':\n                self.whc.messenger.send_image(\n                    image=result['url'],\n                    recipient_id=self.whc.progress_messenger0.recipient_phone,\n                    caption=result.get('caption', '')\n                )\n            elif result['media_type'] == 'document':\n                self.whc.messenger.send_document(\n                    document=result['url'],\n                    recipient_id=self.whc.progress_messenger0.recipient_phone,\n                    caption=result.get('caption', '')\n                )\n\n    async def clear_memory(self, message):\n        self.agent.reset_context()\n        self.agent.taskstack.tasks = []\n        return \"\ud83e\udde0 Memory cleared successfully\"\n\n    async def system_task(self, message):\n        \"\"\"Initiate email search workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n            'type': 'system',\n            'step': 'await_query'\n        }\n        return {\n            'type': 'quick_reply',\n            'text': \"Now prompt the \ud83e\udde0ISAA-System \ud83d\udcdd\",\n            'options': {'cancel': '\u274c Cancel Search'}\n        }\n\n    async def agent_task(self, message):\n        \"\"\"Initiate email search workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n            'type': 'self-agent',\n            'step': 'await_query'\n        }\n        return {\n            'type': 'quick_reply',\n            'text': \"Now prompt the self-agent \ud83d\udcdd\",\n            'options': {'cancel': '\u274c Cancel Search'}\n        }\n\n    async def check_emails(self, message, query=\"\"):\n        \"\"\"Improved email checking with WhatsApp API formatting\"\"\"\n        if not self.gmail_service:\n            return \"\u26a0\ufe0f Gmail service not configured\"\n\n        try:\n            results = self.gmail_service.users().messages().list(\n                userId='me',\n                maxResults=10,\n                labelIds=['INBOX'],\n                q=query\n            ).execute()\n\n            emails = []\n            for msg in results.get('messages', [])[:10]:\n                email_data = self.gmail_service.users().messages().get(\n                    userId='me',\n                    id=msg['id'],\n                    format='metadata'\n                ).execute()\n\n                headers = {h['name']: h['value'] for h in email_data['payload']['headers']}\n                emails.append({\n                    'id': msg['id'],\n                    'from': headers.get('From', 'Unknown'),\n                    'subject': headers.get('Subject', 'No Subject'),\n                    'date': headers.get('Date', 'Unknown'),\n                    'snippet': email_data.get('snippet', ''),\n                    'unread': 'UNREAD' in email_data.get('labelIds', [])\n                })\n\n            return {\n                'type': 'list',\n                'header': '\ud83d\udce8 Recent Emails',\n                'body': 'Tap to view full email',\n                'footer': 'Email Manager',\n                'sections': [{\n                    'title': f\"Inbox ({len(emails)} emails)\",\n                    'rows': [{\n                        'id': f\"email_{email['id']}\",\n                        'title': f\"{'\ud83d\udcec' if email['unread'] else '\ud83d\udced'} {email['subject']}\"[:23],\n                        'description': f\"From: {email['from']}\\n{email['snippet']}\"[:45]\n                    } for email in emails]\n                }]\n            }\n        except Exception as e:\n            return f\"\u26a0\ufe0f Error fetching emails: {str(e)}\"\n\n    async def get_email_details(self, email_id):\n        \"\"\"Retrieve and format full email details\"\"\"\n        if not self.gmail_service:\n            return \"\u26a0\ufe0f Gmail service not configured\"\n\n        try:\n            email_data = self.gmail_service.users().messages().get(\n                userId='me',\n                id=email_id,\n                format='full'\n            ).execute()\n\n            headers = {h['name']: h['value'] for h in email_data['payload']['headers']}\n            body = \"\"\n            for part in email_data.get('payload', {}).get('parts', []):\n                if part['mimeType'] == 'text/plain':\n                    body = base64.urlsafe_b64decode(part['body']['data']).decode('utf-8')\n                    break\n\n            formatted_text = (\n                f\"\ud83d\udce7 *Email Details*\\n\\n\"\n                f\"From: {headers.get('From', 'Unknown')}\\n\"\n                f\"Subject: {headers.get('Subject', 'No Subject')}\\n\"\n                f\"Date: {headers.get('Date', 'Unknown')}\\n\\n\"\n                f\"{body[:15000]}{'...' if len(body) &gt; 15000 else ''}\"\n            )\n            return  self.agent.mini_task(\n                formatted_text , \"system\", \"Summarize the email in bullet points with key details\"\n            )\n        except Exception as e:\n            return f\"\u26a0\ufe0f Error fetching email: {str(e)}\"\n\n    async def email_summary(self, message):\n        \"\"\"Generate AI-powered email summaries\"\"\"\n        try:\n            messages = self.gmail_service.users().messages().list(\n                userId='me',\n                maxResults=3,\n                labelIds=['INBOX']\n            ).execute().get('messages', [])\n\n            email_contents = []\n            for msg in messages[:3]:\n                email_data = self.gmail_service.users().messages().get(\n                    userId='me',\n                    id=msg['id'],\n                    format='full'\n                ).execute()\n                email_contents.append(self._parse_email_content(email_data))\n\n            summary = self.agent.mini_task(\n                \"\\n\\n\".join(email_contents) , \"system\", \"Summarize these emails in bullet points with key details:\"\n            )\n\n            return f\"\ud83d\udccb Email Summary:\\n{summary}\\n\\n*Powered by AI*\"\n        except Exception as e:\n            logging.error(f\"Summary failed: {str(e)}\")\n            return f\"\u274c Could not generate summary: {str(e)}\"\n\n    async def email_search(self, message):\n        \"\"\"Initiate email search workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n            'type': 'email_search',\n            'step': 'await_query'\n        }\n        return {\n            'type': 'quick_reply',\n            'text': \"\ud83d\udd0d What would you like to search for?\",\n            'options': {'cancel': '\u274c Cancel Search'}\n        }\n\n    async def start_email_compose(self, message):\n        \"\"\"Enhanced email composition workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n            'type': 'compose_email',\n            'step': 'subject',\n            'draft': {'attachments': []}\n        }\n        return {\n            'type': 'quick_reply',\n            'text': \"\ud83d\udcdd Let's compose an email\\n\\nSubject:\",\n            'options': {'cancel': '\u274c Cancel Composition'}\n        }\n\n    async def handle_email_actions(self, message):\n        \"\"\"Handle multi-step email workflows\"\"\"\n        user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n\n        if user_state.get('type') == 'compose_email':\n            return await self._handle_email_composition(message, user_state)\n        if user_state.get('type') == 'email_search':\n            return await self.check_emails(message, self.agent.mini_task(\"\"\"Conventire Pezise zu einer googel str only query using : Gmail Suchoperatoren!\n\nBasis-Operatoren:\n- from: Absender\n- to: Empf\u00e4nger\n- subject: Betreff\n- label: Gmail Label\n- has:attachment Anh\u00e4nge\n- newer_than:7d Zeitfilter\n- before: Datum vor\n- after: Datum nach\n\nErweiterte Operatoren:\n- in:inbox\n- in:sent\n- in:spam\n- cc: Kopie\n- bcc: Blindkopie\n- is:unread\n- is:read\n- larger:10M Gr\u00f6\u00dfenfilter\n- smaller:5M\n- filename:pdf Dateityp\n\nProfi-Tipps:\n- Kombinierbar mit UND/ODER\n- Anf\u00fchrungszeichen f\u00fcr exakte Suche\n- Negation mit -\n beispeile : 'Ungelesene Mails letzte Woche': -&gt; 'is:unread newer_than:7d'\n\n\"\"\", \"user\",message.content))\n\n\n        return None\n\n    async def _handle_email_composition(self, message, state):\n        if state['step'] == 'subject':\n            state['draft']['subject'] = message.content\n            state['step'] = 'body'\n            return {\n                'type': 'quick_reply',\n                'text': \"\u270d\ufe0f Email body:\",\n                'options': {'attach': '\ud83d\udcce Add Attachment', 'send': '\ud83d\udce4 Send Now'}\n            }\n\n        elif state['step'] == 'body':\n            if message.content == 'attach':\n                state['step'] = 'attachment'\n                return \"\ud83d\udcce Please send the file you want to attach\"\n\n            state['draft']['body'] = message.content\n            state['step'] = 'confirm_email'\n            return {\n                'type': 'quick_reply',\n                'text': f\"\ud83d\udce7 Ready to send?\\n\\nSubject: {state['draft']['subject']}\\n\\n{state['draft']['body']}\",\n                'options': {'confirm': '\u2705 Send', 'cancel': '\u274c cancel'}\n            }\n\n        elif state['step'] == 'attachment':\n            # Handle attachment upload\n            file_type = message.type\n            if file_type not in ['document', 'image']:\n                return \"\u274c Unsupported file type\"\n\n            media_url = getattr(message, file_type).id\n            media_data = self.whc.messenger.download_media(media_url=self.whc.messenger.query_media_url(media_id=media_url), mime_type=media_url.type, file_path=\".data/temp\")\n            state['draft']['attachments'].append(media_data)\n            state['step'] = 'body'\n            return \"\ud83d\udcce Attachment added! Add more or send the email\"\n\n\n    def _parse_email_content(self, email_data):\n        \"\"\"Extract readable content from email payload\"\"\"\n        parts = email_data.get('payload', {}).get('parts', [])\n        body = \"\"\n        for part in parts:\n            if part['mimeType'] == 'text/plain':\n                body += base64.urlsafe_b64decode(part['body']['data']).decode('utf-8')\n        return f\"Subject: {email_data.get('subject', '')}\\nFrom: {email_data.get('from', '')}\\n\\n{body}\"\n\n    def _build_email_draft(self, draft):\n        \"\"\"Create MIME message from draft data\"\"\"\n        message = MIMEMultipart()\n        message['to'] = draft.get('to', '')\n        message['subject'] = draft['subject']\n        message.attach(MIMEText(draft['body']))\n\n        for attachment in draft['attachments']:\n            part = MIMEBase('application', 'octet-stream')\n            part.set_payload(attachment)\n            encoders.encode_base64(part)\n            part.add_header('Content-Disposition', 'attachment')\n            message.attach(part)\n\n        return {'raw': base64.urlsafe_b64encode(message.as_bytes()).decode()}\n\n    def _get_email_subject(self, msg):\n        headers = msg.get('payload', {}).get('headers', [])\n        return next((h['value'] for h in headers if h['name'] == 'Subject'), 'No Subject')\n\n    def _get_email_sender(self, msg):\n        headers = msg.get('payload', {}).get('headers', [])\n        return next((h['value'] for h in headers if h['name'] == 'From'), 'Unknown Sender')\n\n    def _get_email_snippet(self, msg):\n        return msg.get('snippet', '')[:100] + '...'\n    # Calendar Handlers\n\n    # Calendar Functions\n    def _format_event_time(self, event):\n        \"\"\"Improved time formatting for calendar events\"\"\"\n        start = event['start'].get('dateTime', event['start'].get('date'))\n        end = event['end'].get('dateTime', event['end'].get('date'))\n\n        try:\n            start_dt = parser.parse(start)\n            end_dt = parser.parse(end)\n            if 'T' in start:\n                return f\"{start_dt.strftime('%a %d %b %H:%M')} - {end_dt.strftime('%H:%M')}\"\n            return f\"{start_dt.strftime('%d %b %Y')} (All Day)\"\n        except:\n            return \"Time not specified\"\n\n    async def get_event_details(self, event_id):\n        \"\"\"Retrieve and format calendar event details with location support\"\"\"\n        if not self.calendar_service:\n            return \"\u26a0\ufe0f Calendar service not configured\"\n\n        try:\n            event = self.calendar_service.events().get(\n                calendarId='primary',\n                eventId=event_id\n            ).execute()\n\n            response = [ (\n                    f\"\ud83d\udcc5 *Event Details*\\n\\n\"\n                    f\"Title: {event.get('summary', 'No title')}\\n\"\n                    f\"Time: {self._format_event_time(event)}\\n\"\n                    f\"Location: {event.get('location', 'Not specified')}\\n\\n\"\n                    f\"{event.get('description', 'No description')[:1000]}\"\n                )]\n\n            if 'geo' in event:\n                response.append({\n                    'lat': float(event['geo']['latitude']),\n                    'long': float(event['geo']['longitude']),\n                    'name': event.get('location', 'Event Location'),\n                    'address': event.get('location', ''),\n                    'recipient_id': self.whc.progress_messenger0.recipient_phone\n                })\n            return response\n        except Exception as e:\n            return f\"\u26a0\ufe0f Error fetching event: {str(e)}\"\n\n    async def show_today_events(self, message):\n        \"\"\"Show today's calendar events\"\"\"\n        if not self.calendar_service:\n            message.replay(\"service not online\")\n\n        now = datetime.utcnow().isoformat() + 'Z'\n        end_of_day = (datetime.now() + timedelta(days=1)).replace(\n            hour=0, minute=0, second=0).isoformat() + 'Z'\n\n        events_result = self.calendar_service.events().list(\n            calendarId='primary',\n            timeMin=now,\n            timeMax=end_of_day,\n            singleEvents=True,\n            orderBy='startTime'\n        ).execute()\n\n        events = events_result.get('items', [])\n        return self._format_calendar_response(events, \"Today's Events\")\n\n    # Updated Calendar List Handlers\n    async def show_upcoming_events(self, message):\n        \"\"\"Show upcoming events with interactive support\"\"\"\n        if not self.calendar_service:\n            return \"\u26a0\ufe0f Calendar service not configured\"\n\n        try:\n            now = datetime.utcnow().isoformat() + 'Z'\n            next_week = (datetime.now() + timedelta(days=7)).isoformat() + 'Z'\n\n            events_result = self.calendar_service.events().list(\n                calendarId='primary',\n                timeMin=now,\n                timeMax=next_week,\n                singleEvents=True,\n                orderBy='startTime',\n                maxResults=10\n            ).execute()\n\n            events = events_result.get('items', [])\n            return self._format_calendar_response(events, \"Upcoming Events\")\n        except Exception as e:\n            return f\"\u26a0\ufe0f Error fetching events: {str(e)}\"\n\n    async def start_event_create(self, message):\n        \"\"\"Initiate event creation workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n            'type': 'create_event',\n            'step': 'title',\n            'event_data': {}\n        }\n        return {\n            'type': 'quick_reply',\n            'text': \"Let's create an event! What's the title?\",\n            'options': {'cancel': '\u274c Cancel'}\n        }\n\n    async def find_time_slot(self, message):\n        \"\"\"Find and display the next 5 available time slots with dynamic durations\"\"\"\n        if not self.calendar_service:\n            return \"\u26a0\ufe0f Calendar service not configured\"\n\n        try:\n            # Define the time range for the search (next 24 hours)\n            now = datetime.now(UTC)\n            end_time = now + timedelta(days=1)\n\n            # FreeBusy Request\n            freebusy_request = {\n                \"timeMin\": now.isoformat(),\n                \"timeMax\": end_time.isoformat(),\n                \"items\": [{\"id\": 'primary'}]\n            }\n\n            freebusy_response = self.calendar_service.freebusy().query(body=freebusy_request).execute()\n            busy_slots = freebusy_response['calendars']['primary']['busy']\n\n            # Slot-Berechnung\n            available_slots = self._calculate_efficient_slots(\n                busy_slots,\n                self.duration_minutes\n            )\n\n            # Format the response for WhatsApp\n            return {\n                'type': 'list',\n                'header': \"\u23f0 Available Time Slots\",\n                'body': \"Tap to select a time slot\",\n                'footer': \"Time Slot Finder\",\n                'sections': [{\n                    'title': \"Next 5 Available Slots\",\n                    'rows': [{\n                        'id': f\"slot_{slot['start'].timestamp()}\",\n                        'title': f\"\ud83d\udd52 {slot['start'].strftime('%H:%M')} - {slot['end'].strftime('%H:%M')}\",\n                        'description': f\"Duration: {slot['duration']}\"\n                    } for slot in available_slots[:5]]\n                }]\n            }\n        except Exception as e:\n            return f\"\u26a0\ufe0f Error finding time slots: {str(e)}\"\n\n    def _calculate_efficient_slots(self, busy_slots, duration_minutes):\n        \"\"\"Effiziente Slot-Berechnung\"\"\"\n        available_slots = []\n        current = datetime.now(UTC)\n        end_time = current + timedelta(days=1)\n\n        while current &lt; end_time:\n            slot_end = current + timedelta(minutes=duration_minutes)\n\n            if slot_end &gt; end_time:\n                break\n\n            is_available = all(\n                slot_end &lt;= parser.parse(busy['start']) or\n                current &gt;= parser.parse(busy['end'])\n                for busy in busy_slots\n            )\n\n            if is_available:\n                available_slots.append({\n                    'start': current,\n                    'end': slot_end,\n                    'duration': f\"{duration_minutes} min\"\n                })\n                current = slot_end\n            else:\n                current += timedelta(minutes=15)\n\n        return available_slots\n\n    async def handle_calendar_actions(self, message):\n        \"\"\"Handle calendar-related pending actions\"\"\"\n        user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n\n        if user_state.get('type') == 'create_event':\n            return await self._handle_event_creation(message, user_state)\n\n        return None\n\n    async def _handle_event_creation(self, message, state):\n        step = state['step']\n        event_data = state['event_data']\n\n        if step == 'title':\n            event_data['summary'] = message.content\n            state['step'] = 'start_time'\n            return \"\ud83d\udcc5 When should it start? (e.g., 'tomorrow 2pm' or '2024-03-20 14:30')\"\n\n        elif step == 'start_time':\n            event_data['start'] = self._parse_time(message.content)\n            state['step'] = 'end_time'\n            return \"\u23f0 When should it end? (e.g., '3pm' or '2024-03-20 15:30')\"\n\n        elif step == 'end_time':\n            event_data['end'] = self._parse_time(message.content, reference=event_data['start'])\n            state['step'] = 'description'\n            return \"\ud83d\udcdd Add a description (or type 'skip')\"\n\n        elif step == 'description':\n            if message.content.lower() != 'skip':\n                event_data['description'] = message.content\n            state['step'] = 'confirm_envet'\n            return self._create_confirmation_message(event_data)\n\n    def _format_calendar_response(self, events, title):\n        \"\"\"Enhanced calendar formatting with interactive support\"\"\"\n        if not events:\n            return f\"\ud83d\udcc5 No {title.lower()} found\"\n\n        return {\n            'type': 'list',\n            'header': title,\n            'body': \"Tap to view event details\",\n            \"footer\": \"-- Calendar --\",\n            'sections': [{\n                'title': f\"{len(events)} Events\",\n                'rows': [{\n                    'id': f\"event_{event['id']}\",\n                    'title': f\"\ud83d\udcc5 {event['summary']}\"[:23],\n                    'description': self._format_event_time(event)[:45]\n                } for event in events[:5]]\n            }]\n        }\n\n    def _parse_iso_to_readable(self, iso_str):\n        \"\"\"Convert ISO datetime to readable format\"\"\"\n        dt = datetime.fromisoformat(iso_str.replace('Z', '+00:00'))\n        return dt.strftime(\"%a %d %b %Y %H:%M\")\n\n    def _parse_time(self, time_str, reference=None):\n        \"\"\"\n        Konvertiert nat\u00fcrliche Sprache zu pr\u00e4ziser Datetime\n\n        Unterst\u00fctzt:\n        - 'heute'\n        - 'morgen'\n        - 'in einer woche'\n        - '10 uhr'\n        - '10pm'\n        - 'n\u00e4chsten montag'\n        \"\"\"\n        if reference is None:\n            reference = datetime.now()\n\n        try:\n            import dateparser\n\n            # Dateparser f\u00fcr flexibel Zeitparsing\n            parsed_time = dateparser.parse(\n                time_str,\n                settings={\n                    'PREFER_DATES_FROM': 'future',\n                    'RELATIVE_BASE': reference,\n                    'TIMEZONE': 'Europe/Berlin'\n                }\n            )\n\n            if parsed_time is None:\n                # Fallback auf dateutil wenn dateparser scheitert\n                parsed_time = parser .parse(time_str, fuzzy=True, default=reference)\n\n            return parsed_time\n\n        except Exception as e:\n            print(f\"Zeitparsing-Fehler: {e}\")\n            return reference\n\n    def _calculate_free_slots(self, start, end, busy_slots):\n        \"\"\"Calculate free time slots between busy periods\"\"\"\n        # Implementation would calculate available windows\n        return [{\n            'start': \"09:00\",\n            'end': \"11:00\",\n            'duration': \"2 hours\"\n        }]\n\n    def _create_confirmation_message(self, event_data):\n        \"\"\"Create event confirmation message\"\"\"\n        details = [\n            f\"\ud83d\udccc Title: {event_data['summary']}\",\n            f\"\ud83d\udd52 Start: {self._parse_iso_to_readable(event_data['start'])}\",\n            f\"\u23f0 End: {self._parse_iso_to_readable(event_data['end'])}\",\n            f\"\ud83d\udcdd Description: {event_data.get('description', 'None')}\"\n        ]\n        return {\n            'type': 'quick_reply',\n            'text': \"\\n\".join(details),\n            'options': {'confirm': '\u2705 Confirm', 'cancel': '\u274c Cancel'}\n        }\n\n    def _create_calendar_event(self, event_data):\n        \"\"\"Create event through Calendar API\"\"\"\n        event = {\n            'summary': event_data['summary'],\n            'start': {'dateTime': event_data['start']},\n            'end': {'dateTime': event_data['end']},\n        }\n        if 'description' in event_data:\n            event['description'] = event_data['description']\n\n        return self.calendar_service.events().insert(\n            calendarId='primary',\n            body=event\n        ).execute()\n\n    async def system_status(self, message):\n        o = (datetime.now() - self.start_time)\n        o.microseconds = 0\n        status = {\n            \"\ud83e\udd16 Agent\": \"Online\" if self.agent else \"Offline\",\n            \"\ud83d\udce7 Email\": \"Connected\" if self.gmail_service else \"Disconnected\",\n            \"\ud83d\udcc5 Calendar\": \"Connected\" if self.calendar_service else \"Disconnected\",\n            \"\ud83d\udcc4 Documents\": \"Connected\" if self.blob_docs_system else \"Disconnected\",\n            \"\u23f3 Uptime\": f\"{str(o.isoformat())}\"\n        }\n        return \"\\n\".join([f\"{k}: {v}\" for k, v in status.items()])\n\n    async def restart_system(self, message):\n        message.reply(\"\ud83d\udd04 System restart initiated...\")\n        time.sleep(1)\n        await self.clear_memory(message)\n        time.sleep(1)\n        return  \"\u2705 System restarted\"\n\n    # Updated document handlers\n    async def list_documents(self, message, filter_type=None):\n        docs = self.blob_docs_system.list_documents(filter_type)\n        if len(docs) == 0:\n            return \"No docs found\"\n        else:\n            return str(docs)\n        return {\n            'type': 'list',\n            'body': 'Stored Documents',\n            'action': {\n                'sections': [{\n                    'title': 'Your Documents',\n                    'rows': [{\n                        'id': doc['id'],\n                        'title': f\"{self._get_icon(doc['type'])} {doc['name']}\"[:23],\n                        'description': f\"{doc['type'].title()} | {self._format_size(doc['size'])} | {doc['modified']}\"[:29]\n                    } for doc in docs[:10]]\n                }]}\n        }\n\n    async def start_document_upload(self, message):\n        \"\"\"Initiate document upload workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {'type': 'document', 'step': 'awaiting_file'}\n        return {\n            'type': 'quick_reply',\n            'text': '\ud83d\udce4 Send me the file you want to upload',\n            'options': {'cancel': '\u274c Cancel Upload'}\n        }\n\n    async def search_documents(self, message):\n        \"\"\"Initiate document search workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {'type': 'search', 'step': 'awaiting_query'}\n        return {\n            'type': 'quick_reply',\n            'text': '\ud83d\udd0d What are you looking for?',\n            'options': {'cancel': '\u274c Cancel Search'}\n        }\n\n    async def handle_media_message(self, message: 'Message'):\n        \"\"\"Handle document/image/video uploads\"\"\"\n        user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n\n        if user_state.get('step') == 'awaiting_file':\n            file_type = message.type\n            if file_type not in ['document', 'image', 'video']:\n                return \"Unsupported file type\"\n\n            try:\n                # Download media\n                #media_url = message.document.url if hasattr(message, 'document') else \\\n                #    message.image.url if hasattr(message, 'image') else \\\n                #        message.video.url\n                if file_type =='video':\n                    content = self.whc.messenger.get_video(message.data)\n                if file_type =='image':\n                    content = self.whc.messenger.get_image(message.data)\n                if file_type =='document':\n                    content = self.whc.messenger.get_document(message.data)\n                print(\"Media content:\", content)\n                media_data = self.whc.messenger.download_media(media_url=self.whc.messenger.query_media_url(media_id=content.get('id')),  mime_type=content.get('mime_type'), file_path='.data/temp')\n                print(\"Media media_data:\", media_data)\n                # Save to blob storage\n                filename = f\"file_{file_type}_{datetime.now().isoformat()}_{content.get('sha256', '')}\"\n                blob_id = self.blob_docs_system.save_document(\n                    open(media_data, 'rb').read(),\n                    filename=filename,\n                    file_type=file_type\n                )\n\n                self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n                return f\"\u2705 File uploaded successfully!\\nID: {blob_id}\"\n\n            except Exception as e:\n                logging.error(f\"Upload failed: {str(e)}\")\n                return f\"\u274c Failed to upload file Error : {str(e)}\"\n\n        return \"No pending uploads\"\n\n    async def delete_document(self, message):\n        \"\"\"Delete document workflow\"\"\"\n        docs = self.blob_docs_system.list_documents()\n        return {\n            'type': 'quick_reply',\n            'text': 'Select document to delete:',\n            'options': {doc['id']: doc['name'] for doc in docs[:5]},\n            'handler': self._confirm_delete\n        }\n\n    async def _confirm_delete(self, doc_id, message):\n        \"\"\"Confirm deletion workflow\"\"\"\n        doc = next((d for d in self.blob_docs_system.list_documents() if d['id'] == doc_id), None)\n        if not doc:\n            return \"Document not found\"\n\n        if self.blob_docs_system.delete_document(doc_id):\n            return f\"\u2705 {doc['name']} deleted successfully\"\n        return \"\u274c Failed to delete document\"\n\n    # Helper methods\n    def _get_icon(self, file_type: str) -&gt; str:\n        icons = {\n            'document': '\ud83d\udcc4',\n            'image': '\ud83d\uddbc\ufe0f',\n            'video': '\ud83c\udfa5'\n        }\n        return icons.get(file_type, '\ud83d\udcc1')\n\n    def _format_size(self, size: int) -&gt; str:\n        if size &lt; 1024:\n            return f\"{size}B\"\n        elif size &lt; 1024 ** 2:\n            return f\"{size / 1024:.1f}KB\"\n        elif size &lt; 1024 ** 3:\n            return f\"{size / (1024 ** 2):.1f}MB\"\n        return f\"{size / (1024 ** 3):.1f}GB\"\n\n    # Utility Methods\n\n    def _clean_processed_messages(self):\n        \"\"\"Clean old messages from processed cache\"\"\"\n        now = time.time()\n        self.processed_messages = {\n            msg_id for msg_id, timestamp in self.processed_messages\n            if now - timestamp &lt; 3600  # 1 hour retention\n        }\n\n    def send_email(self, to, subject, body):\n        \"\"\"Actual email sending function to be called by agent\"\"\"\n        if not self.gmail_service:\n            return False\n\n        message = MIMEText(body)\n        message['to'] = to\n        message['subject'] = subject\n\n        encoded_message = base64.urlsafe_b64encode(message.as_bytes()).decode()\n        self.gmail_service.users().messages().send(\n            userId='me',\n            body={'raw': encoded_message}\n        ).execute()\n        return True\n\n    async def start_agent(self, *a):\n        \"\"\"Start the agent in background mode\"\"\"\n        if self.agent:\n            self.agent.run_in_background()\n            return True\n        return False\n\n    async def stop_agent(self, *b):\n        \"\"\"Stop the currently running agent\"\"\"\n        if self.agent:\n            self.agent.stop()\n            return True\n        return False\n\n    async def show_task_stack(self, *a):\n        \"\"\"Display current task stack\"\"\"\n        if self.agent and len(self.agent.taskstack.tasks) &gt; 0:\n            tasks = self.agent.taskstack.tasks\n            return self.agent.mini_task(\"\\n\".join([f\"Task {t.id}: {t.description}\" for t in tasks]), \"system\", \"Format to nice and clean whatsapp format\")\n        return \"No tasks in stack\"\n\n    def run(self):\n        \"\"\"Start the WhatsApp assistant\"\"\"\n        try:\n            self.state = AssistantState.ONLINE\n            # Send welcome message\n\n            mas = self.whc.messenger.create_message(\n                content=\"Digital Assistant is online! Send /help for available commands.\",to=self.whc.progress_messenger0.recipient_phone,\n            ).send(sender=0)\n            mas_id = mas.get(\"messages\", [{}])[0].get(\"id\")\n            print(mas_id)\n\n        except Exception as e:\n            logging.error(f\"Assistant error: {str(e)}\")\n            self.state = AssistantState.OFFLINE\n            raise\n\n    async def handle_agent_actions(self, message):\n        user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n        def helper():\n\n            stop_flag = threading.Event()\n            try:\n                progress = self.progress_messengers['task']\n                # message_id = progress.send_initial_message(mode=\"loading\")\n                progress.message_id = message.id\n                progress.start_loading_in_background(stop_flag)\n                res = message.content\n                print(message.data.get('entry', [{}])[0].get('changes', [{}])[0].get('value', {}).get('messages', [{}])[0].get(\n                    'context'))\n                if context := message.data.get('entry', [{}])[0].get('changes', [{}])[0].get('value', {}).get('messages', [{}])[0].get(\n                    'context'):\n                    context_str = f\"Context : source {'USER' if context.get('from') in self.whc.progress_messenger0.recipient_phone else 'AGENT'}\"\n                    cd = self.history.get(context.get('id'))\n                    context_str += \"\\n\" + (cd if cd is not None else \"The ref Message is not in the history\")\n                    res += \"\\n\" + context_str\n                if user_state.get('type') == 'system':\n                    res = self.isaa.run(res)\n                    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n                elif user_state.get('type') == 'self-agent':\n                    res = self.agent.run(res)\n                    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n                self.agent.mode = LLMMode(\n                    name=\"Chatter\",\n                    description=\"whatsapp Chat LLM\",\n                    system_msg=\"Response precise and short style using whatsapp syntax!\",\n                    post_msg=None\n                )\n                response = self.agent.mini_task(res, \"user\", persist=True)\n                self.save_reply(message, response)\n            except Exception as e:\n                stop_flag.set()\n                message.reply(\"\u274c Error in agent \"+str(e))\n            finally:\n                self.agent.mode = None\n                stop_flag.set()\n        threading.Thread(target=helper, daemon=True).start()\n\n    def save_reply(self, message, content):\n        res = message.reply(content)\n        res_id = res.get(\"messages\", [{}])[0].get(\"id\")\n        if res_id is not None:\n            self.history.set(res_id, content)\n        else:\n            print(f\"No ID to add to history: {res}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.agent_task","title":"<code>agent_task(message)</code>  <code>async</code>","text":"<p>Initiate email search workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def agent_task(self, message):\n    \"\"\"Initiate email search workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n        'type': 'self-agent',\n        'step': 'await_query'\n    }\n    return {\n        'type': 'quick_reply',\n        'text': \"Now prompt the self-agent \ud83d\udcdd\",\n        'options': {'cancel': '\u274c Cancel Search'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.check_emails","title":"<code>check_emails(message, query='')</code>  <code>async</code>","text":"<p>Improved email checking with WhatsApp API formatting</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def check_emails(self, message, query=\"\"):\n    \"\"\"Improved email checking with WhatsApp API formatting\"\"\"\n    if not self.gmail_service:\n        return \"\u26a0\ufe0f Gmail service not configured\"\n\n    try:\n        results = self.gmail_service.users().messages().list(\n            userId='me',\n            maxResults=10,\n            labelIds=['INBOX'],\n            q=query\n        ).execute()\n\n        emails = []\n        for msg in results.get('messages', [])[:10]:\n            email_data = self.gmail_service.users().messages().get(\n                userId='me',\n                id=msg['id'],\n                format='metadata'\n            ).execute()\n\n            headers = {h['name']: h['value'] for h in email_data['payload']['headers']}\n            emails.append({\n                'id': msg['id'],\n                'from': headers.get('From', 'Unknown'),\n                'subject': headers.get('Subject', 'No Subject'),\n                'date': headers.get('Date', 'Unknown'),\n                'snippet': email_data.get('snippet', ''),\n                'unread': 'UNREAD' in email_data.get('labelIds', [])\n            })\n\n        return {\n            'type': 'list',\n            'header': '\ud83d\udce8 Recent Emails',\n            'body': 'Tap to view full email',\n            'footer': 'Email Manager',\n            'sections': [{\n                'title': f\"Inbox ({len(emails)} emails)\",\n                'rows': [{\n                    'id': f\"email_{email['id']}\",\n                    'title': f\"{'\ud83d\udcec' if email['unread'] else '\ud83d\udced'} {email['subject']}\"[:23],\n                    'description': f\"From: {email['from']}\\n{email['snippet']}\"[:45]\n                } for email in emails]\n            }]\n        }\n    except Exception as e:\n        return f\"\u26a0\ufe0f Error fetching emails: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.complete_authorization","title":"<code>complete_authorization(message)</code>","text":"<p>Complete the authorization process using the authorization code</p> <p>:param authorization_code: Authorization code received from Google</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def complete_authorization(self, message: Message):\n    \"\"\"\n    Complete the authorization process using the authorization code\n\n    :param authorization_code: Authorization code received from Google\n    \"\"\"\n    from google_auth_oauthlib.flow import Flow\n    authorization_code = message.content\n    # Define the scopes required for Gmail and Calendar\n    SCOPES = [\n        'https://www.googleapis.com/auth/gmail.modify',\n        'https://www.googleapis.com/auth/calendar'\n    ]\n\n    # Create a flow instance to manage the OAuth 2.0 authorization process\n    flow = Flow.from_client_secrets_file(\n        self.credentials_path,\n        scopes=SCOPES,\n        redirect_uri='urn:ietf:wg:oauth:2.0:oob'\n    )\n\n    # Exchange the authorization code for credentials\n    flow.fetch_token(code=authorization_code)\n    self.credentials = flow.credentials\n\n    # Save the credentials for future use\n    self.save_credentials()\n\n    # Initialize services\n    self.init_services()\n    return \"Done\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.delete_document","title":"<code>delete_document(message)</code>  <code>async</code>","text":"<p>Delete document workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def delete_document(self, message):\n    \"\"\"Delete document workflow\"\"\"\n    docs = self.blob_docs_system.list_documents()\n    return {\n        'type': 'quick_reply',\n        'text': 'Select document to delete:',\n        'options': {doc['id']: doc['name'] for doc in docs[:5]},\n        'handler': self._confirm_delete\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.email_search","title":"<code>email_search(message)</code>  <code>async</code>","text":"<p>Initiate email search workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def email_search(self, message):\n    \"\"\"Initiate email search workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n        'type': 'email_search',\n        'step': 'await_query'\n    }\n    return {\n        'type': 'quick_reply',\n        'text': \"\ud83d\udd0d What would you like to search for?\",\n        'options': {'cancel': '\u274c Cancel Search'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.email_summary","title":"<code>email_summary(message)</code>  <code>async</code>","text":"<p>Generate AI-powered email summaries</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def email_summary(self, message):\n    \"\"\"Generate AI-powered email summaries\"\"\"\n    try:\n        messages = self.gmail_service.users().messages().list(\n            userId='me',\n            maxResults=3,\n            labelIds=['INBOX']\n        ).execute().get('messages', [])\n\n        email_contents = []\n        for msg in messages[:3]:\n            email_data = self.gmail_service.users().messages().get(\n                userId='me',\n                id=msg['id'],\n                format='full'\n            ).execute()\n            email_contents.append(self._parse_email_content(email_data))\n\n        summary = self.agent.mini_task(\n            \"\\n\\n\".join(email_contents) , \"system\", \"Summarize these emails in bullet points with key details:\"\n        )\n\n        return f\"\ud83d\udccb Email Summary:\\n{summary}\\n\\n*Powered by AI*\"\n    except Exception as e:\n        logging.error(f\"Summary failed: {str(e)}\")\n        return f\"\u274c Could not generate summary: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.find_time_slot","title":"<code>find_time_slot(message)</code>  <code>async</code>","text":"<p>Find and display the next 5 available time slots with dynamic durations</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def find_time_slot(self, message):\n    \"\"\"Find and display the next 5 available time slots with dynamic durations\"\"\"\n    if not self.calendar_service:\n        return \"\u26a0\ufe0f Calendar service not configured\"\n\n    try:\n        # Define the time range for the search (next 24 hours)\n        now = datetime.now(UTC)\n        end_time = now + timedelta(days=1)\n\n        # FreeBusy Request\n        freebusy_request = {\n            \"timeMin\": now.isoformat(),\n            \"timeMax\": end_time.isoformat(),\n            \"items\": [{\"id\": 'primary'}]\n        }\n\n        freebusy_response = self.calendar_service.freebusy().query(body=freebusy_request).execute()\n        busy_slots = freebusy_response['calendars']['primary']['busy']\n\n        # Slot-Berechnung\n        available_slots = self._calculate_efficient_slots(\n            busy_slots,\n            self.duration_minutes\n        )\n\n        # Format the response for WhatsApp\n        return {\n            'type': 'list',\n            'header': \"\u23f0 Available Time Slots\",\n            'body': \"Tap to select a time slot\",\n            'footer': \"Time Slot Finder\",\n            'sections': [{\n                'title': \"Next 5 Available Slots\",\n                'rows': [{\n                    'id': f\"slot_{slot['start'].timestamp()}\",\n                    'title': f\"\ud83d\udd52 {slot['start'].strftime('%H:%M')} - {slot['end'].strftime('%H:%M')}\",\n                    'description': f\"Duration: {slot['duration']}\"\n                } for slot in available_slots[:5]]\n            }]\n        }\n    except Exception as e:\n        return f\"\u26a0\ufe0f Error finding time slots: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.generate_authorization_url","title":"<code>generate_authorization_url(*a)</code>  <code>async</code>","text":"<p>Generate an authorization URL for user consent</p> <p>:return: Authorization URL for the user to click and authorize access</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def generate_authorization_url(self, *a):\n    \"\"\"\n    Generate an authorization URL for user consent\n\n    :return: Authorization URL for the user to click and authorize access\n    \"\"\"\n    from google_auth_oauthlib.flow import Flow\n    # Define the scopes required for Gmail and Calendar\n    SCOPES = [\n        'https://www.googleapis.com/auth/gmail.modify',\n        'https://www.googleapis.com/auth/calendar'\n    ]\n\n    # Create a flow instance to manage the OAuth 2.0 authorization process\n    flow = Flow.from_client_secrets_file(\n        self.credentials_path,\n        scopes=SCOPES,\n        redirect_uri='urn:ietf:wg:oauth:2.0:oob'  # Use 'urn:ietf:wg:oauth:2.0:oob' for desktop apps\n    )\n\n    # Generate the authorization URL\n    authorization_url, _ = flow.authorization_url(\n        access_type='offline',  # Allows obtaining refresh token\n        prompt='consent'  # Ensures user is always prompted for consent\n    )\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {'type': 'auth',\n                                                                          'step': 'awaiting_key'}\n    return {\n        'type': 'quick_reply',\n        'text': f'Url to log in {authorization_url}',\n        'options': {'cancel': '\u274c Cancel Upload'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.get_email_details","title":"<code>get_email_details(email_id)</code>  <code>async</code>","text":"<p>Retrieve and format full email details</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def get_email_details(self, email_id):\n    \"\"\"Retrieve and format full email details\"\"\"\n    if not self.gmail_service:\n        return \"\u26a0\ufe0f Gmail service not configured\"\n\n    try:\n        email_data = self.gmail_service.users().messages().get(\n            userId='me',\n            id=email_id,\n            format='full'\n        ).execute()\n\n        headers = {h['name']: h['value'] for h in email_data['payload']['headers']}\n        body = \"\"\n        for part in email_data.get('payload', {}).get('parts', []):\n            if part['mimeType'] == 'text/plain':\n                body = base64.urlsafe_b64decode(part['body']['data']).decode('utf-8')\n                break\n\n        formatted_text = (\n            f\"\ud83d\udce7 *Email Details*\\n\\n\"\n            f\"From: {headers.get('From', 'Unknown')}\\n\"\n            f\"Subject: {headers.get('Subject', 'No Subject')}\\n\"\n            f\"Date: {headers.get('Date', 'Unknown')}\\n\\n\"\n            f\"{body[:15000]}{'...' if len(body) &gt; 15000 else ''}\"\n        )\n        return  self.agent.mini_task(\n            formatted_text , \"system\", \"Summarize the email in bullet points with key details\"\n        )\n    except Exception as e:\n        return f\"\u26a0\ufe0f Error fetching email: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.get_event_details","title":"<code>get_event_details(event_id)</code>  <code>async</code>","text":"<p>Retrieve and format calendar event details with location support</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def get_event_details(self, event_id):\n    \"\"\"Retrieve and format calendar event details with location support\"\"\"\n    if not self.calendar_service:\n        return \"\u26a0\ufe0f Calendar service not configured\"\n\n    try:\n        event = self.calendar_service.events().get(\n            calendarId='primary',\n            eventId=event_id\n        ).execute()\n\n        response = [ (\n                f\"\ud83d\udcc5 *Event Details*\\n\\n\"\n                f\"Title: {event.get('summary', 'No title')}\\n\"\n                f\"Time: {self._format_event_time(event)}\\n\"\n                f\"Location: {event.get('location', 'Not specified')}\\n\\n\"\n                f\"{event.get('description', 'No description')[:1000]}\"\n            )]\n\n        if 'geo' in event:\n            response.append({\n                'lat': float(event['geo']['latitude']),\n                'long': float(event['geo']['longitude']),\n                'name': event.get('location', 'Event Location'),\n                'address': event.get('location', ''),\n                'recipient_id': self.whc.progress_messenger0.recipient_phone\n            })\n        return response\n    except Exception as e:\n        return f\"\u26a0\ufe0f Error fetching event: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_audio_message","title":"<code>handle_audio_message(message)</code>  <code>async</code>","text":"<p>Process audio messages with STT and TTS</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def handle_audio_message(self, message: 'Message'):\n    \"\"\"Process audio messages with STT and TTS\"\"\"\n    # Download audio\n    progress = self.progress_messengers['task']\n    stop_flag = threading.Event()\n    # message_id = progress.send_initial_message(mode=\"loading\")\n    progress.message_id = message.id\n    progress.start_loading_in_background(stop_flag)\n\n    content = self.whc.messenger.get_audio(message.data)\n    audio_file_name = self.whc.messenger.download_media(media_url=self.whc.messenger.query_media_url(media_id=content.get('id')), mime_type='audio/opus', file_path=\".data/temp\")\n    print(f\"audio_file_name {audio_file_name}\")\n    if audio_file_name is None:\n        message.reply(\"Could not process audio file\")\n        stop_flag.set()\n        return\n\n    text = self.stt(audio_file_name)['text']\n    if not text:\n        message.reply(\"Could not process audio\")\n        stop_flag.set()\n        return\n\n    message.reply(\"Transcription :\\n \"+ text)\n    message.content = text\n    agent_res = await self.helper_text(message, return_text=True)\n\n    if agent_res is not None:\n        pass\n\n    stop_flag.set()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_button_interaction","title":"<code>handle_button_interaction(content, message)</code>  <code>async</code>","text":"<p>Handle button click interactions</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def handle_button_interaction(self, content: dict, message: Message):\n    \"\"\"Handle button click interactions\"\"\"\n    button_id = content['id']\n\n    # First check if it's a main menu button\n    if button_id in self.buttons:\n        self.whc.messenger.send_button(\n            recipient_id=self.whc.progress_messenger0.recipient_phone,\n            button=self.buttons[button_id]\n        )\n        return\n\n    # Handle action buttons\n    action_handlers = {\n        # Agent controls\n        'start': self.start_agent,\n        'stop': self.stop_agent,\n        'tasks': self.show_task_stack,\n        'memory': self.clear_memory,\n        'system-task': self.system_task,\n        'agent-task': self.agent_task,\n\n        # Email controls\n        'check': self.check_emails,\n        'send': self.start_email_compose,\n        'summary': self.email_summary,\n        'search': self.email_search,\n\n        # Calendar controls\n        'today': self.show_today_events,\n        'add': self.start_event_create,\n        'upcoming': self.show_upcoming_events,\n        'find_slot': self.find_time_slot,\n\n        # Document controls\n        'upload': self.start_document_upload,\n        'list': self.list_documents,\n        'search_docs': self.search_documents,\n        'delete': self.delete_document,\n\n        # System controls\n        'status': self.system_status,\n        'restart': self.restart_system,\n        'connect': self.generate_authorization_url,\n\n        'cancel': self.cancel,\n        'confirm': self.confirm,\n    }\n    if button_id in action_handlers:\n        try:\n            # Start progress indicator\n            progress = self.progress_messengers['task']\n            stop_flag = threading.Event()\n            # message_id = progress.send_initial_message(mode=\"loading\")\n            progress.message_id = message.id\n            progress.start_loading_in_background(stop_flag)\n\n            # Execute handler\n\n            result = await action_handlers[button_id](message)\n\n\n            # Send result\n            if isinstance(result, str):\n                self.save_reply(message, result)\n            elif isinstance(result, dict):  # For structured responses\n                self.send_structured_response(result)\n\n            stop_flag.set()\n        finally:\n            #except Exception as e:\n            stop_flag.set()\n        #    message.reply(f\"\u274c Error processing {button_id}: {str(e)}\")\n    elif 'event_' in button_id:\n        res = await self.get_event_details(button_id.replace(\"event_\", ''))\n        if isinstance(res, str):\n            self.save_reply(message, res)\n            return\n        for r in res:\n            if isinstance(r, str):\n                self.save_reply(message, r)\n            else:\n                self.whc.messenger.send_location(**r)\n\n    elif 'email_' in button_id:\n        res = await self.get_email_details(button_id.replace(\"email_\", ''))\n        self.save_reply(message, res)\n    else:\n        message.reply(\"\u26a0\ufe0f Unknown command\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_calendar_actions","title":"<code>handle_calendar_actions(message)</code>  <code>async</code>","text":"<p>Handle calendar-related pending actions</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def handle_calendar_actions(self, message):\n    \"\"\"Handle calendar-related pending actions\"\"\"\n    user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n\n    if user_state.get('type') == 'create_event':\n        return await self._handle_event_creation(message, user_state)\n\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_email_actions","title":"<code>handle_email_actions(message)</code>  <code>async</code>","text":"<p>Handle multi-step email workflows</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>    async def handle_email_actions(self, message):\n        \"\"\"Handle multi-step email workflows\"\"\"\n        user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n\n        if user_state.get('type') == 'compose_email':\n            return await self._handle_email_composition(message, user_state)\n        if user_state.get('type') == 'email_search':\n            return await self.check_emails(message, self.agent.mini_task(\"\"\"Conventire Pezise zu einer googel str only query using : Gmail Suchoperatoren!\n\nBasis-Operatoren:\n- from: Absender\n- to: Empf\u00e4nger\n- subject: Betreff\n- label: Gmail Label\n- has:attachment Anh\u00e4nge\n- newer_than:7d Zeitfilter\n- before: Datum vor\n- after: Datum nach\n\nErweiterte Operatoren:\n- in:inbox\n- in:sent\n- in:spam\n- cc: Kopie\n- bcc: Blindkopie\n- is:unread\n- is:read\n- larger:10M Gr\u00f6\u00dfenfilter\n- smaller:5M\n- filename:pdf Dateityp\n\nProfi-Tipps:\n- Kombinierbar mit UND/ODER\n- Anf\u00fchrungszeichen f\u00fcr exakte Suche\n- Negation mit -\n beispeile : 'Ungelesene Mails letzte Woche': -&gt; 'is:unread newer_than:7d'\n\n\"\"\", \"user\",message.content))\n\n\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_interactive","title":"<code>handle_interactive(message)</code>  <code>async</code>","text":"<p>Handle all interactive messages</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def handle_interactive(self, message: Message):\n    \"\"\"Handle all interactive messages\"\"\"\n    content = self.whc.messenger.get_interactive_response(message.data)\n    if content.get(\"type\") == \"list_reply\":\n        await self.handle_button_interaction(content.get(\"list_reply\"), message)\n    elif content.get(\"type\") == \"button_reply\":\n        print(content)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_media_message","title":"<code>handle_media_message(message)</code>  <code>async</code>","text":"<p>Handle document/image/video uploads</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def handle_media_message(self, message: 'Message'):\n    \"\"\"Handle document/image/video uploads\"\"\"\n    user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n\n    if user_state.get('step') == 'awaiting_file':\n        file_type = message.type\n        if file_type not in ['document', 'image', 'video']:\n            return \"Unsupported file type\"\n\n        try:\n            # Download media\n            #media_url = message.document.url if hasattr(message, 'document') else \\\n            #    message.image.url if hasattr(message, 'image') else \\\n            #        message.video.url\n            if file_type =='video':\n                content = self.whc.messenger.get_video(message.data)\n            if file_type =='image':\n                content = self.whc.messenger.get_image(message.data)\n            if file_type =='document':\n                content = self.whc.messenger.get_document(message.data)\n            print(\"Media content:\", content)\n            media_data = self.whc.messenger.download_media(media_url=self.whc.messenger.query_media_url(media_id=content.get('id')),  mime_type=content.get('mime_type'), file_path='.data/temp')\n            print(\"Media media_data:\", media_data)\n            # Save to blob storage\n            filename = f\"file_{file_type}_{datetime.now().isoformat()}_{content.get('sha256', '')}\"\n            blob_id = self.blob_docs_system.save_document(\n                open(media_data, 'rb').read(),\n                filename=filename,\n                file_type=file_type\n            )\n\n            self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n            return f\"\u2705 File uploaded successfully!\\nID: {blob_id}\"\n\n        except Exception as e:\n            logging.error(f\"Upload failed: {str(e)}\")\n            return f\"\u274c Failed to upload file Error : {str(e)}\"\n\n    return \"No pending uploads\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_message","title":"<code>handle_message(message)</code>  <code>async</code>","text":"<p>Main message handler for incoming WhatsApp messages</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def handle_message(self, message: 'Message'):\n    \"\"\"Main message handler for incoming WhatsApp messages\"\"\"\n\n    # Deduplication check\n    with self.message_lock:\n        if message.id in self.processed_messages:\n            return\n        last_ts = time.time()\n        print(last_ts)\n        if len(self.processed_messages) &gt; 0:\n            m_id, last_ts = self.processed_messages.pop()\n            self.processed_messages.add((m_id, last_ts))\n\n        print(\"DUPLICATION P\", message.data.get('entry', [{}])[0].get('changes', [{}])[0].get('value', {}).get('messages', [{}])[0].get('timestamp', 0) , last_ts)\n        if float(message.data.get('entry', [{}])[0].get('changes', [{}])[0].get('value', {}).get('messages', [{}])[0].get('timestamp', 0)) &lt; last_ts - 120:\n            return\n        self.processed_messages.add((message.id, time.perf_counter()))\n\n    # Mark message as read\n    message.mark_as_read()\n\n    # Extract content and type\n    content_type = message.type\n    content = message.content\n\n    print(f\"message.content {content=} {content_type=} {message.data=}\")\n\n    try:\n        if content_type == 'interactive':\n            await self.handle_interactive(message)\n        elif content_type == 'audio':\n            await self.handle_audio_message(message)\n        elif content_type in ['document', 'image', 'video']:\n            response = await self.handle_media_message(message)\n            self.save_reply(message, response)\n        elif content_type == 'text':\n            if content.lower() == \"menu\":\n                self.whc.messenger.send_button(\n                    recipient_id=self.whc.progress_messenger0.recipient_phone,\n                    button=self.buttons[content.lower()]\n                )\n            else:\n                await self.helper_text(message)\n        else:\n            message.reply(\"Unsupported message type\")\n    #except Exception as e:\n    #    logging.error(f\"Message handling error: {str(e)}\")\n    #   message.reply(\"\u274c Error processing request\")\n    finally:\n        # Cleanup old messages (keep 1 hour history)\n        with self.message_lock:\n            self._clean_processed_messages()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.init_services","title":"<code>init_services()</code>","text":"<p>Initialize Gmail and Calendar services</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def init_services(self):\n    \"\"\"\n    Initialize Gmail and Calendar services\n    \"\"\"\n    from googleapiclient.discovery import build\n\n    self.gmail_service = build('gmail', 'v1', credentials=self.credentials)\n    self.calendar_service = build('calendar', 'v3', credentials=self.credentials)\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.load_credentials","title":"<code>load_credentials()</code>","text":"<p>Load previously saved credentials if available</p> <p>:return: Whether credentials were successfully loaded</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def load_credentials(self):\n    \"\"\"\n    Load previously saved credentials if available\n\n    :return: Whether credentials were successfully loaded\n    \"\"\"\n    try:\n        self.credentials = Credentials.from_authorized_user_file('token/google_token.json')\n        self.init_services()\n        return True\n    except FileNotFoundError:\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.run","title":"<code>run()</code>","text":"<p>Start the WhatsApp assistant</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def run(self):\n    \"\"\"Start the WhatsApp assistant\"\"\"\n    try:\n        self.state = AssistantState.ONLINE\n        # Send welcome message\n\n        mas = self.whc.messenger.create_message(\n            content=\"Digital Assistant is online! Send /help for available commands.\",to=self.whc.progress_messenger0.recipient_phone,\n        ).send(sender=0)\n        mas_id = mas.get(\"messages\", [{}])[0].get(\"id\")\n        print(mas_id)\n\n    except Exception as e:\n        logging.error(f\"Assistant error: {str(e)}\")\n        self.state = AssistantState.OFFLINE\n        raise\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.save_credentials","title":"<code>save_credentials()</code>","text":"<p>Save the obtained credentials to a file for future use</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def save_credentials(self):\n    \"\"\"\n    Save the obtained credentials to a file for future use\n    \"\"\"\n    if not os.path.exists('token'):\n        os.makedirs('token')\n\n    with open('token/google_token.json', 'w') as token_file:\n        token_file.write(self.credentials.to_json())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.search_documents","title":"<code>search_documents(message)</code>  <code>async</code>","text":"<p>Initiate document search workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def search_documents(self, message):\n    \"\"\"Initiate document search workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {'type': 'search', 'step': 'awaiting_query'}\n    return {\n        'type': 'quick_reply',\n        'text': '\ud83d\udd0d What are you looking for?',\n        'options': {'cancel': '\u274c Cancel Search'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.send_email","title":"<code>send_email(to, subject, body)</code>","text":"<p>Actual email sending function to be called by agent</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def send_email(self, to, subject, body):\n    \"\"\"Actual email sending function to be called by agent\"\"\"\n    if not self.gmail_service:\n        return False\n\n    message = MIMEText(body)\n    message['to'] = to\n    message['subject'] = subject\n\n    encoded_message = base64.urlsafe_b64encode(message.as_bytes()).decode()\n    self.gmail_service.users().messages().send(\n        userId='me',\n        body={'raw': encoded_message}\n    ).execute()\n    return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.send_structured_response","title":"<code>send_structured_response(result)</code>","text":"<p>Send complex responses using appropriate WhatsApp features</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def send_structured_response(self, result: dict):\n    \"\"\"Send complex responses using appropriate WhatsApp features\"\"\"\n    if result['type'] == 'list':\n        self.whc.messenger.send_button(\n            recipient_id=self.whc.progress_messenger0.recipient_phone,\n            button={\n                'header': result.get('header', ''),\n                'body': result.get('body', ''),\n                'footer': result.get('footer', ''),\n                'action': {\n                    'button': 'Action',\n                    'sections': result['sections']\n                }\n            }\n        )\n    elif result['type'] == 'quick_reply':\n        self.whc.messenger.send_button(\n            recipient_id=self.whc.progress_messenger0.recipient_phone,\n            button={\n                'header': \"Quick reply\",\n                'body': result['text'],\n                'footer': '',\n                'action': {'button': 'Action', 'sections': [{\n                    'title': 'View',\n                    'rows': [{'id': k, 'title': v[:23]} for k, v in result['options'].items()]\n                }]}\n            }\n        )\n\n    elif result['type'] == 'media':\n        if result['media_type'] == 'image':\n            self.whc.messenger.send_image(\n                image=result['url'],\n                recipient_id=self.whc.progress_messenger0.recipient_phone,\n                caption=result.get('caption', '')\n            )\n        elif result['media_type'] == 'document':\n            self.whc.messenger.send_document(\n                document=result['url'],\n                recipient_id=self.whc.progress_messenger0.recipient_phone,\n                caption=result.get('caption', '')\n            )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.setup_interaction_buttons","title":"<code>setup_interaction_buttons()</code>","text":"<p>Define WhatsApp interaction buttons for different functionalities</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def setup_interaction_buttons(self):\n    \"\"\"Define WhatsApp interaction buttons for different functionalities\"\"\"\n    self.buttons = {\n        'menu': {\n            'header': 'Digital Assistant',\n            'body': 'Please select an option:',\n            'footer': '-- + --',\n            'action': {\n                'button': 'Menu',\n                'sections': [\n                    {\n                        'title': 'Main Functions',\n                        'rows': [\n                            {'id': 'agent', 'title': 'Agent Controls', 'description': 'Manage your AI assistant'},\n                            {'id': 'email', 'title': 'Email Management', 'description': 'Handle your emails'},\n                            {'id': 'calendar', 'title': 'Calendar', 'description': 'Manage your schedule'},\n                            {'id': 'docs', 'title': 'Documents', 'description': 'Handle documents'},\n                            {'id': 'system', 'title': 'System', 'description': 'System controls and metrics'}\n                        ]\n                    }\n                ]\n            }\n        },\n        'agent': self._create_agent_controls_buttons(),\n        'email': self._create_email_controls_buttons(),\n        'calendar': self._create_calendar_controls_buttons(),\n        'docs': self._create_docs_controls_buttons(),\n        'system': self._create_system_controls_buttons()\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.setup_progress_messengers","title":"<code>setup_progress_messengers()</code>","text":"<p>Initialize progress messengers for different types of tasks</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def setup_progress_messengers(self):\n    \"\"\"Initialize progress messengers for different types of tasks\"\"\"\n    self.progress_messengers = {\n        'task': self.whc.progress_messenger0,\n        'email': self.whc.progress_messenger1,\n        'calendar': self.whc.progress_messenger2\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.show_task_stack","title":"<code>show_task_stack(*a)</code>  <code>async</code>","text":"<p>Display current task stack</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def show_task_stack(self, *a):\n    \"\"\"Display current task stack\"\"\"\n    if self.agent and len(self.agent.taskstack.tasks) &gt; 0:\n        tasks = self.agent.taskstack.tasks\n        return self.agent.mini_task(\"\\n\".join([f\"Task {t.id}: {t.description}\" for t in tasks]), \"system\", \"Format to nice and clean whatsapp format\")\n    return \"No tasks in stack\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.show_today_events","title":"<code>show_today_events(message)</code>  <code>async</code>","text":"<p>Show today's calendar events</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def show_today_events(self, message):\n    \"\"\"Show today's calendar events\"\"\"\n    if not self.calendar_service:\n        message.replay(\"service not online\")\n\n    now = datetime.utcnow().isoformat() + 'Z'\n    end_of_day = (datetime.now() + timedelta(days=1)).replace(\n        hour=0, minute=0, second=0).isoformat() + 'Z'\n\n    events_result = self.calendar_service.events().list(\n        calendarId='primary',\n        timeMin=now,\n        timeMax=end_of_day,\n        singleEvents=True,\n        orderBy='startTime'\n    ).execute()\n\n    events = events_result.get('items', [])\n    return self._format_calendar_response(events, \"Today's Events\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.show_upcoming_events","title":"<code>show_upcoming_events(message)</code>  <code>async</code>","text":"<p>Show upcoming events with interactive support</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def show_upcoming_events(self, message):\n    \"\"\"Show upcoming events with interactive support\"\"\"\n    if not self.calendar_service:\n        return \"\u26a0\ufe0f Calendar service not configured\"\n\n    try:\n        now = datetime.utcnow().isoformat() + 'Z'\n        next_week = (datetime.now() + timedelta(days=7)).isoformat() + 'Z'\n\n        events_result = self.calendar_service.events().list(\n            calendarId='primary',\n            timeMin=now,\n            timeMax=next_week,\n            singleEvents=True,\n            orderBy='startTime',\n            maxResults=10\n        ).execute()\n\n        events = events_result.get('items', [])\n        return self._format_calendar_response(events, \"Upcoming Events\")\n    except Exception as e:\n        return f\"\u26a0\ufe0f Error fetching events: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.start_agent","title":"<code>start_agent(*a)</code>  <code>async</code>","text":"<p>Start the agent in background mode</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def start_agent(self, *a):\n    \"\"\"Start the agent in background mode\"\"\"\n    if self.agent:\n        self.agent.run_in_background()\n        return True\n    return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.start_document_upload","title":"<code>start_document_upload(message)</code>  <code>async</code>","text":"<p>Initiate document upload workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def start_document_upload(self, message):\n    \"\"\"Initiate document upload workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {'type': 'document', 'step': 'awaiting_file'}\n    return {\n        'type': 'quick_reply',\n        'text': '\ud83d\udce4 Send me the file you want to upload',\n        'options': {'cancel': '\u274c Cancel Upload'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.start_email_compose","title":"<code>start_email_compose(message)</code>  <code>async</code>","text":"<p>Enhanced email composition workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def start_email_compose(self, message):\n    \"\"\"Enhanced email composition workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n        'type': 'compose_email',\n        'step': 'subject',\n        'draft': {'attachments': []}\n    }\n    return {\n        'type': 'quick_reply',\n        'text': \"\ud83d\udcdd Let's compose an email\\n\\nSubject:\",\n        'options': {'cancel': '\u274c Cancel Composition'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.start_event_create","title":"<code>start_event_create(message)</code>  <code>async</code>","text":"<p>Initiate event creation workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def start_event_create(self, message):\n    \"\"\"Initiate event creation workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n        'type': 'create_event',\n        'step': 'title',\n        'event_data': {}\n    }\n    return {\n        'type': 'quick_reply',\n        'text': \"Let's create an event! What's the title?\",\n        'options': {'cancel': '\u274c Cancel'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.stop_agent","title":"<code>stop_agent(*b)</code>  <code>async</code>","text":"<p>Stop the currently running agent</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def stop_agent(self, *b):\n    \"\"\"Stop the currently running agent\"\"\"\n    if self.agent:\n        self.agent.stop()\n        return True\n    return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.system_task","title":"<code>system_task(message)</code>  <code>async</code>","text":"<p>Initiate email search workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def system_task(self, message):\n    \"\"\"Initiate email search workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n        'type': 'system',\n        'step': 'await_query'\n    }\n    return {\n        'type': 'quick_reply',\n        'text': \"Now prompt the \ud83e\udde0ISAA-System \ud83d\udcdd\",\n        'options': {'cancel': '\u274c Cancel Search'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server","title":"<code>server</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager","title":"<code>AppManager</code>","text":"Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>class AppManager(metaclass=Singleton):\n    pepper = \"pepper0\"\n\n    def __init__(self, start_port: int = 8000, port_range: int = 10, em=None):\n        self.instances: dict[str, dict] = {}\n        self.start_port = start_port\n        self.port_range = port_range\n        self.threads: dict[str, Thread] = {}\n        self.stop_events: dict[str, Event] = {}\n        self.message_queue: asyncio.Queue = asyncio.Queue()\n        self.last_messages: dict[str, datetime] = {}\n        self.keys: dict[str, str] = {}\n        self.forwarders: dict[str, dict] = {}\n        self.runner = lambda :None\n\n        if em is None:\n            from toolboxv2 import get_app\n            em = get_app().get_mod(\"EventManager\")\n        from toolboxv2.mods import EventManager\n        self.event_manager: EventManager = em.get_manager()\n\n        # Set up signal handlers for graceful shutdown\n        try:\n            if threading.current_thread() is threading.main_thread():\n                signal.signal(signal.SIGINT, self.signal_handler)\n                signal.signal(signal.SIGTERM, self.signal_handler)\n        except Exception:\n            pass\n\n    def offline(self, instance_id):\n\n        def mark_as_offline():\n            self.forwarders[instance_id]['send'] = None\n            return 'done'\n\n        return mark_as_offline\n\n    def online(self, instance_id):\n\n        def mark_as_online():\n            return self.instances[instance_id]['app']\n\n        def set_callbacks(callback, e_callback=None):\n            if callback is not None:\n                self.forwarders[instance_id]['send'] = callback\n            if e_callback is not None:\n                self.forwarders[instance_id]['sende'] = e_callback\n\n        return mark_as_online(), set_callbacks\n\n    def get_next_available_port(self) -&gt; int:\n        \"\"\"Find the next available port in the range.\"\"\"\n        used_ports = {instance['port'] for instance in self.instances.values()}\n        for port in range(self.start_port, self.start_port + self.port_range):\n            if port not in used_ports:\n                return port\n        raise RuntimeError(\"No available ports in range\")\n\n    def add_instance(self, instance_id: str, **kwargs):\n        \"\"\"\n        Add a new app instance to the manager with automatic port assignment.\n        \"\"\"\n        if instance_id in self.instances:\n            raise ValueError(f\"Instance {instance_id} already exists\")\n\n        port = self.get_next_available_port()\n        app_instance = WhatsApp(**kwargs)\n\n        self.instances[instance_id] = {\n            'app': app_instance,\n            'port': port,\n            'kwargs': kwargs,\n            'phone_number_id': kwargs.get(\"phone_number_id\", {}),\n            'retry_count': 0,\n            'max_retries': 3,\n            'retry_delay': 5\n        }\n        self.keys[instance_id] = Code.one_way_hash(kwargs.get(\"phone_number_id\", {}).get(\"key\"), \"WhatsappAppManager\",\n                                                   self.pepper)\n        self.forwarders[instance_id] = {}\n\n        # Set up message handlers\n        @app_instance.on_message\n        async def message_handler(message):\n            await self.on_message(instance_id, message)\n\n        @app_instance.on_event\n        async def event_handler(event):\n            await self.on_event(instance_id, event)\n\n        @app_instance.on_verification\n        async def verification_handler(verification):\n            await self.on_verification(instance_id, verification)\n\n        # Create stop event for this instance Error parsing message1:\n        self.stop_events[instance_id] = Event()\n\n    def run_instance(self, instance_id: str):\n        \"\"\"Run a single instance in a separate thread with error handling and automatic restart.\"\"\"\n        instance_data = self.instances[instance_id]\n        stop_event = self.stop_events[instance_id]\n\n        while not stop_event.is_set():\n            try:\n                logger.info(f\"Starting instance {instance_id} on port {instance_data['port']}\")\n                instance_data['app'].run(host='0.0.0.0', port=instance_data['port'])\n\n            except Exception as e:\n                logger.error(f\"Error in instance {instance_id}: {str(e)}\")\n                instance_data['retry_count'] += 1\n\n                if instance_data['retry_count'] &gt; instance_data['max_retries']:\n                    logger.error(f\"Max retries exceeded for instance {instance_id}\")\n                    break\n\n                logger.info(f\"Restarting instance {instance_id} in {instance_data['retry_delay']} seconds...\")\n                time.sleep(instance_data['retry_delay'])\n\n                # Recreate the instance\n                instance_data['app'] = WhatsApp(**instance_data['kwargs'])\n                continue\n\n    async def on_message(self, instance_id: str, message: Message):\n        \"\"\"Handle and forward incoming messages.\"\"\"\n        logger.info(f\"Message from instance {instance_id}: {message}\")\n        if instance_id in self.forwarders and 'send' in self.forwarders[instance_id]:\n            await self.forwarders[instance_id]['send'](message)\n\n    async def on_event(self, instance_id: str, event):\n        \"\"\"Handle events.\"\"\"\n        logger.info(f\"Event from instance {instance_id}: {event}\")\n        if instance_id in self.forwarders and 'sende' in self.forwarders[instance_id] and self.forwarders[instance_id]['sende'] is not None:\n            self.forwarders[instance_id]['sende'](event)\n\n    async def on_verification(self, instance_id: str, verification):\n        \"\"\"Handle verification events.\"\"\"\n        logger.info(f\"Verification from instance {instance_id}: {verification}\")\n\n    def run_all_instances(self):\n        \"\"\"Start all instances in separate daemon threads.\"\"\"\n        # Start message forwarder\n\n        # Start all instances\n        for instance_id in self.instances:\n            thread = Thread(\n                target=self.run_instance,\n                args=(instance_id,),\n                daemon=True,\n                name=f\"WhatsApp-{instance_id}\"\n            )\n            self.threads[instance_id] = thread\n            thread.start()\n\n    def signal_handler(self, signum, frame):\n        \"\"\"Handle shutdown signals gracefully.\"\"\"\n        logger.info(\"Shutdown signal received, stopping all instances...\")\n        self.stop_all_instances()\n        sys.exit(0)\n\n    def stop_all_instances(self):\n        \"\"\"Stop all running instances gracefully.\"\"\"\n        for instance_id in self.stop_events:\n            self.stop_events[instance_id].set()\n\n        for thread in self.threads.values():\n            thread.join(timeout=5)\n\n    def create_manager_ui(self, start_assistant):\n        \"\"\"Enhanced WhatsApp Manager UI with instance configuration controls\"\"\"\n        self.runner = start_assistant\n        def ui_manager():\n            # Track instance states and messages\n            original_on_message = self.on_message\n\n            async def enhanced_on_message(instance_id: str, message):\n                self.last_messages[instance_id] = datetime.now()\n                await original_on_message(instance_id, message)\n\n            self.on_message = enhanced_on_message\n\n            def create_instance_card(instance_id: str):\n                \"\"\"Interactive instance control card\"\"\"\n                config = self.instances[instance_id]\n                with ui.card().classes('w-full p-4 mb-4 bg-gray-50 dark:bg-gray-800').style(\"background-color: var(--background-color) !important\"):\n                    # Header Section\n                    with ui.row().classes('w-full justify-between items-center'):\n                        ui.label(f'\ud83d\udcf1 {instance_id}').classes('text-xl font-bold')\n\n                        # Status Indicator\n                        ui.label().bind_text_from(\n                            self.threads, instance_id,\n                            lambda x: 'Running' if x and x.is_alive() else 'Stopped'\n                        )\n\n                    # Configuration Display\n                    with ui.grid(columns=2).classes('w-full mt-4 gap-2'):\n\n                        ui.label('port:').classes('font-bold')\n                        ui.label(config['port'])\n\n                        ui.label('Last Activity:').classes('font-bold')\n                        ui.label().bind_text_from(\n                            self.last_messages, instance_id,\n                            lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\") if x else 'Never'\n                        )\n\n                    # Action Controls\n                    with ui.row().classes('w-full mt-4 gap-2'):\n                        with ui.button(icon='settings', on_click=lambda: edit_dialog.open()).props('flat'):\n                            ui.tooltip('Configure')\n\n                        with ui.button(icon='refresh', color='orange',\n                                       on_click=lambda: self.restart_instance(instance_id)):\n                            ui.tooltip('Restart')\n\n                        with ui.button(icon='stop', color='red',\n                                       on_click=lambda: self.stop_instance(instance_id)):\n                            ui.tooltip('Stop')\n\n                    # Edit Configuration Dialog\n                    with ui.dialog() as edit_dialog, ui.card().classes('p-4 gap-4'):\n                        new_key = ui.input('API Key', value=config['phone_number_id'].get('key', ''))\n                        new_number = ui.input('Phone Number', value=config['phone_number_id'].get('number', ''))\n\n                        with ui.row().classes('w-full justify-end'):\n                            ui.button('Cancel', on_click=edit_dialog.close)\n                            ui.button('Save', color='primary', on_click=lambda: (\n                                self.update_instance_config(\n                                    instance_id,\n                                    new_key.value,\n                                    new_number.value\n                                ),\n                                edit_dialog.close()\n                            ))\n\n            # Main UI Layout\n            with ui.column().classes('w-full max-w-4xl mx-auto p-4'):\n                ui.label('WhatsApp Instance Manager').classes('text-2xl font-bold mb-6')\n\n                # Add Instance Section\n                with ui.expansion('\u2795 Add New Instance', icon='add').classes('w-full'):\n                    with ui.card().classes('w-full p-4 mt-2'):\n                        instance_id = ui.input('Instance ID').classes('w-full')\n                        token = ui.input('API Token').classes('w-full')\n                        phone_key = ui.input('Phone Number Key').classes('w-full')\n                        phone_number = ui.input('Phone Number').classes('w-full')\n\n                        with ui.row().classes('w-full justify-end gap-2'):\n                            ui.button('Clear', on_click=lambda: (\n                                instance_id.set_value(''),\n                                token.set_value(''),\n                                phone_key.set_value(''),\n                                phone_number.set_value('')\n                            ))\n                            ui.button('Create', color='positive', on_click=lambda: (\n                                self.add_update_instance(\n                                    instance_id.value,\n                                    token.value,\n                                    phone_key.value,\n                                    phone_number.value\n                                ),\n                                instances_container.refresh()\n                            ))\n\n                # Instances Display\n                instances_container = ui.column().classes('w-full')\n                with instances_container:\n                    for instance_id in self.instances:\n                        create_instance_card(instance_id)\n\n        return ui_manager\n\n    # Add to manager class\n    def add_update_instance(self, instance_id, token, phone_key, phone_number):\n        \"\"\"Add or update instance configuration\"\"\"\n        if instance_id in self.instances:\n            self.stop_instance(instance_id)\n            del self.instances[instance_id]\n\n        self.add_instance(\n            instance_id,\n            token=token,\n            phone_number_id={\n                'key': phone_key,\n                'number': phone_number\n            },\n            verify_token=os.getenv(\"WHATSAPP_VERIFY_TOKEN\")\n        )\n        self.start_instance(instance_id)\n\n    def update_instance_config(self, instance_id, new_key, new_number):\n        \"\"\"Update existing instance configuration\"\"\"\n        if instance_id in self.instances:\n            self.instances[instance_id]['phone_number_id'] = {\n                'key': new_key,\n                'number': new_number\n            }\n            self.restart_instance(instance_id)\n\n    def restart_instance(self, instance_id):\n        \"\"\"Safe restart of instance\"\"\"\n        self.stop_instance(instance_id)\n        self.start_instance(instance_id)\n\n    def stop_instance(self, instance_id):\n        \"\"\"Graceful stop of instance\"\"\"\n        if instance_id in self.threads:\n            self.stop_events[instance_id].set()\n            self.threads[instance_id].join(timeout=5)\n            del self.threads[instance_id]\n\n    def start_instance(self, instance_id):\n        \"\"\"Start instance thread\"\"\"\n        print(\"Starting Istance\")\n\n        self.stop_events[instance_id] = threading.Event()\n        self.threads[instance_id] = threading.Thread(\n            target=self.run_instance,\n            args=(instance_id,),\n            daemon=True\n        )\n        self.threads[instance_id].start()\n        print(\"Running starter\", self.runner())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.add_instance","title":"<code>add_instance(instance_id, **kwargs)</code>","text":"<p>Add a new app instance to the manager with automatic port assignment.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def add_instance(self, instance_id: str, **kwargs):\n    \"\"\"\n    Add a new app instance to the manager with automatic port assignment.\n    \"\"\"\n    if instance_id in self.instances:\n        raise ValueError(f\"Instance {instance_id} already exists\")\n\n    port = self.get_next_available_port()\n    app_instance = WhatsApp(**kwargs)\n\n    self.instances[instance_id] = {\n        'app': app_instance,\n        'port': port,\n        'kwargs': kwargs,\n        'phone_number_id': kwargs.get(\"phone_number_id\", {}),\n        'retry_count': 0,\n        'max_retries': 3,\n        'retry_delay': 5\n    }\n    self.keys[instance_id] = Code.one_way_hash(kwargs.get(\"phone_number_id\", {}).get(\"key\"), \"WhatsappAppManager\",\n                                               self.pepper)\n    self.forwarders[instance_id] = {}\n\n    # Set up message handlers\n    @app_instance.on_message\n    async def message_handler(message):\n        await self.on_message(instance_id, message)\n\n    @app_instance.on_event\n    async def event_handler(event):\n        await self.on_event(instance_id, event)\n\n    @app_instance.on_verification\n    async def verification_handler(verification):\n        await self.on_verification(instance_id, verification)\n\n    # Create stop event for this instance Error parsing message1:\n    self.stop_events[instance_id] = Event()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.add_update_instance","title":"<code>add_update_instance(instance_id, token, phone_key, phone_number)</code>","text":"<p>Add or update instance configuration</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def add_update_instance(self, instance_id, token, phone_key, phone_number):\n    \"\"\"Add or update instance configuration\"\"\"\n    if instance_id in self.instances:\n        self.stop_instance(instance_id)\n        del self.instances[instance_id]\n\n    self.add_instance(\n        instance_id,\n        token=token,\n        phone_number_id={\n            'key': phone_key,\n            'number': phone_number\n        },\n        verify_token=os.getenv(\"WHATSAPP_VERIFY_TOKEN\")\n    )\n    self.start_instance(instance_id)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.create_manager_ui","title":"<code>create_manager_ui(start_assistant)</code>","text":"<p>Enhanced WhatsApp Manager UI with instance configuration controls</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def create_manager_ui(self, start_assistant):\n    \"\"\"Enhanced WhatsApp Manager UI with instance configuration controls\"\"\"\n    self.runner = start_assistant\n    def ui_manager():\n        # Track instance states and messages\n        original_on_message = self.on_message\n\n        async def enhanced_on_message(instance_id: str, message):\n            self.last_messages[instance_id] = datetime.now()\n            await original_on_message(instance_id, message)\n\n        self.on_message = enhanced_on_message\n\n        def create_instance_card(instance_id: str):\n            \"\"\"Interactive instance control card\"\"\"\n            config = self.instances[instance_id]\n            with ui.card().classes('w-full p-4 mb-4 bg-gray-50 dark:bg-gray-800').style(\"background-color: var(--background-color) !important\"):\n                # Header Section\n                with ui.row().classes('w-full justify-between items-center'):\n                    ui.label(f'\ud83d\udcf1 {instance_id}').classes('text-xl font-bold')\n\n                    # Status Indicator\n                    ui.label().bind_text_from(\n                        self.threads, instance_id,\n                        lambda x: 'Running' if x and x.is_alive() else 'Stopped'\n                    )\n\n                # Configuration Display\n                with ui.grid(columns=2).classes('w-full mt-4 gap-2'):\n\n                    ui.label('port:').classes('font-bold')\n                    ui.label(config['port'])\n\n                    ui.label('Last Activity:').classes('font-bold')\n                    ui.label().bind_text_from(\n                        self.last_messages, instance_id,\n                        lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\") if x else 'Never'\n                    )\n\n                # Action Controls\n                with ui.row().classes('w-full mt-4 gap-2'):\n                    with ui.button(icon='settings', on_click=lambda: edit_dialog.open()).props('flat'):\n                        ui.tooltip('Configure')\n\n                    with ui.button(icon='refresh', color='orange',\n                                   on_click=lambda: self.restart_instance(instance_id)):\n                        ui.tooltip('Restart')\n\n                    with ui.button(icon='stop', color='red',\n                                   on_click=lambda: self.stop_instance(instance_id)):\n                        ui.tooltip('Stop')\n\n                # Edit Configuration Dialog\n                with ui.dialog() as edit_dialog, ui.card().classes('p-4 gap-4'):\n                    new_key = ui.input('API Key', value=config['phone_number_id'].get('key', ''))\n                    new_number = ui.input('Phone Number', value=config['phone_number_id'].get('number', ''))\n\n                    with ui.row().classes('w-full justify-end'):\n                        ui.button('Cancel', on_click=edit_dialog.close)\n                        ui.button('Save', color='primary', on_click=lambda: (\n                            self.update_instance_config(\n                                instance_id,\n                                new_key.value,\n                                new_number.value\n                            ),\n                            edit_dialog.close()\n                        ))\n\n        # Main UI Layout\n        with ui.column().classes('w-full max-w-4xl mx-auto p-4'):\n            ui.label('WhatsApp Instance Manager').classes('text-2xl font-bold mb-6')\n\n            # Add Instance Section\n            with ui.expansion('\u2795 Add New Instance', icon='add').classes('w-full'):\n                with ui.card().classes('w-full p-4 mt-2'):\n                    instance_id = ui.input('Instance ID').classes('w-full')\n                    token = ui.input('API Token').classes('w-full')\n                    phone_key = ui.input('Phone Number Key').classes('w-full')\n                    phone_number = ui.input('Phone Number').classes('w-full')\n\n                    with ui.row().classes('w-full justify-end gap-2'):\n                        ui.button('Clear', on_click=lambda: (\n                            instance_id.set_value(''),\n                            token.set_value(''),\n                            phone_key.set_value(''),\n                            phone_number.set_value('')\n                        ))\n                        ui.button('Create', color='positive', on_click=lambda: (\n                            self.add_update_instance(\n                                instance_id.value,\n                                token.value,\n                                phone_key.value,\n                                phone_number.value\n                            ),\n                            instances_container.refresh()\n                        ))\n\n            # Instances Display\n            instances_container = ui.column().classes('w-full')\n            with instances_container:\n                for instance_id in self.instances:\n                    create_instance_card(instance_id)\n\n    return ui_manager\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.get_next_available_port","title":"<code>get_next_available_port()</code>","text":"<p>Find the next available port in the range.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def get_next_available_port(self) -&gt; int:\n    \"\"\"Find the next available port in the range.\"\"\"\n    used_ports = {instance['port'] for instance in self.instances.values()}\n    for port in range(self.start_port, self.start_port + self.port_range):\n        if port not in used_ports:\n            return port\n    raise RuntimeError(\"No available ports in range\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.on_event","title":"<code>on_event(instance_id, event)</code>  <code>async</code>","text":"<p>Handle events.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>async def on_event(self, instance_id: str, event):\n    \"\"\"Handle events.\"\"\"\n    logger.info(f\"Event from instance {instance_id}: {event}\")\n    if instance_id in self.forwarders and 'sende' in self.forwarders[instance_id] and self.forwarders[instance_id]['sende'] is not None:\n        self.forwarders[instance_id]['sende'](event)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.on_message","title":"<code>on_message(instance_id, message)</code>  <code>async</code>","text":"<p>Handle and forward incoming messages.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>async def on_message(self, instance_id: str, message: Message):\n    \"\"\"Handle and forward incoming messages.\"\"\"\n    logger.info(f\"Message from instance {instance_id}: {message}\")\n    if instance_id in self.forwarders and 'send' in self.forwarders[instance_id]:\n        await self.forwarders[instance_id]['send'](message)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.on_verification","title":"<code>on_verification(instance_id, verification)</code>  <code>async</code>","text":"<p>Handle verification events.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>async def on_verification(self, instance_id: str, verification):\n    \"\"\"Handle verification events.\"\"\"\n    logger.info(f\"Verification from instance {instance_id}: {verification}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.restart_instance","title":"<code>restart_instance(instance_id)</code>","text":"<p>Safe restart of instance</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def restart_instance(self, instance_id):\n    \"\"\"Safe restart of instance\"\"\"\n    self.stop_instance(instance_id)\n    self.start_instance(instance_id)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.run_all_instances","title":"<code>run_all_instances()</code>","text":"<p>Start all instances in separate daemon threads.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def run_all_instances(self):\n    \"\"\"Start all instances in separate daemon threads.\"\"\"\n    # Start message forwarder\n\n    # Start all instances\n    for instance_id in self.instances:\n        thread = Thread(\n            target=self.run_instance,\n            args=(instance_id,),\n            daemon=True,\n            name=f\"WhatsApp-{instance_id}\"\n        )\n        self.threads[instance_id] = thread\n        thread.start()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.run_instance","title":"<code>run_instance(instance_id)</code>","text":"<p>Run a single instance in a separate thread with error handling and automatic restart.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def run_instance(self, instance_id: str):\n    \"\"\"Run a single instance in a separate thread with error handling and automatic restart.\"\"\"\n    instance_data = self.instances[instance_id]\n    stop_event = self.stop_events[instance_id]\n\n    while not stop_event.is_set():\n        try:\n            logger.info(f\"Starting instance {instance_id} on port {instance_data['port']}\")\n            instance_data['app'].run(host='0.0.0.0', port=instance_data['port'])\n\n        except Exception as e:\n            logger.error(f\"Error in instance {instance_id}: {str(e)}\")\n            instance_data['retry_count'] += 1\n\n            if instance_data['retry_count'] &gt; instance_data['max_retries']:\n                logger.error(f\"Max retries exceeded for instance {instance_id}\")\n                break\n\n            logger.info(f\"Restarting instance {instance_id} in {instance_data['retry_delay']} seconds...\")\n            time.sleep(instance_data['retry_delay'])\n\n            # Recreate the instance\n            instance_data['app'] = WhatsApp(**instance_data['kwargs'])\n            continue\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.signal_handler","title":"<code>signal_handler(signum, frame)</code>","text":"<p>Handle shutdown signals gracefully.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def signal_handler(self, signum, frame):\n    \"\"\"Handle shutdown signals gracefully.\"\"\"\n    logger.info(\"Shutdown signal received, stopping all instances...\")\n    self.stop_all_instances()\n    sys.exit(0)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.start_instance","title":"<code>start_instance(instance_id)</code>","text":"<p>Start instance thread</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def start_instance(self, instance_id):\n    \"\"\"Start instance thread\"\"\"\n    print(\"Starting Istance\")\n\n    self.stop_events[instance_id] = threading.Event()\n    self.threads[instance_id] = threading.Thread(\n        target=self.run_instance,\n        args=(instance_id,),\n        daemon=True\n    )\n    self.threads[instance_id].start()\n    print(\"Running starter\", self.runner())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.stop_all_instances","title":"<code>stop_all_instances()</code>","text":"<p>Stop all running instances gracefully.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def stop_all_instances(self):\n    \"\"\"Stop all running instances gracefully.\"\"\"\n    for instance_id in self.stop_events:\n        self.stop_events[instance_id].set()\n\n    for thread in self.threads.values():\n        thread.join(timeout=5)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.stop_instance","title":"<code>stop_instance(instance_id)</code>","text":"<p>Graceful stop of instance</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def stop_instance(self, instance_id):\n    \"\"\"Graceful stop of instance\"\"\"\n    if instance_id in self.threads:\n        self.stop_events[instance_id].set()\n        self.threads[instance_id].join(timeout=5)\n        del self.threads[instance_id]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.update_instance_config","title":"<code>update_instance_config(instance_id, new_key, new_number)</code>","text":"<p>Update existing instance configuration</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def update_instance_config(self, instance_id, new_key, new_number):\n    \"\"\"Update existing instance configuration\"\"\"\n    if instance_id in self.instances:\n        self.instances[instance_id]['phone_number_id'] = {\n            'key': new_key,\n            'number': new_number\n        }\n        self.restart_instance(instance_id)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils","title":"<code>utils</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils.ProgressMessenger","title":"<code>ProgressMessenger</code>","text":"Source code in <code>toolboxv2/mods/WhatsAppTb/utils.py</code> <pre><code>class ProgressMessenger:\n    def __init__(self, messenger, recipient_phone: str, max_steps: int = 5, emoji_set: list[str] = None, content=None):\n        self.messenger = messenger\n        self.recipient_phone = recipient_phone\n        self.max_steps = max_steps\n        self.emoji_set = emoji_set or [\"\u2b1c\", \"\u2b1b\", \"\ud83d\udfe9\", \"\ud83d\udfe8\", \"\ud83d\udfe6\"]\n        self.message_id = None\n        self.content = content\n\n    def send_initial_message(self, mode: str = \"progress\"):\n        \"\"\"\n        Sends the initial message. Modes can be 'progress' or 'loading'.\n        \"\"\"\n        if mode == \"progress\":\n            emoji_legend = \"\\n\".join(\n                f\"{emoji} - Step {i + 1}\" for i, emoji in enumerate(self.emoji_set)\n            )\n            content = (\n                \"Progress is being updated in real-time!\\n\\n\"\n                \"Legend:\\n\"\n                f\"{emoji_legend}\\n\\n\"\n                \"Stay tuned for updates!\"\n            )\n        elif mode == \"loading\":\n            content = (\n                \"Loading in progress! \ud83c\udf00\\n\"\n                \"The indicator will loop until work is done.\"\n            )\n        else:\n            raise ValueError(\"Invalid mode. Use 'progress' or 'loading'.\")\n\n        if self.content is not None:\n            content += '\\n'+self.content\n        message = self.messenger.create_message(content=content, to=self.recipient_phone)\n        response = message.send(sender=0)\n        self.message_id = response.get(\"messages\", [{}])[0].get(\"id\")\n        logging.info(f\"Initial message sent: {content}\")\n        return self.message_id\n\n    def update_progress(self, step_flag: threading.Event):\n        \"\"\"\n        Updates the reaction on the message to represent progress.\n        \"\"\"\n        if not self.message_id:\n            raise ValueError(\"Message ID not found. Ensure the initial message is sent first.\")\n        message = self.messenger.create_message(id=self.message_id, to=self.recipient_phone)\n        for step in range(self.max_steps):\n            emoji = self.emoji_set[step % len(self.emoji_set)]\n            message.react(emoji)\n            logging.info(f\"Progress updated: Step {step + 1}/{self.max_steps} with emoji {emoji}\")\n            while not step_flag.is_set():\n                time.sleep(0.5)\n            step_flag.clear()\n        # Final acknowledgment\n        message.react(\"\ud83d\udc4d\")\n        logging.info(\"Progress completed with final acknowledgment.\")\n\n    def update_loading(self, stop_flag: threading.Event):\n        \"\"\"\n        Continuously updates the reaction to represent a looping 'loading' indicator.\n        \"\"\"\n        if not self.message_id:\n            raise ValueError(\"Message ID not found. Ensure the initial message is sent first.\")\n        message = self.messenger.create_message(id=self.message_id, to=self.recipient_phone)\n        step = 0\n        while not stop_flag.is_set():\n            emoji = self.emoji_set[step % len(self.emoji_set)]\n            message.react(emoji)\n            logging.info(f\"Loading update: {emoji}\")\n            time.sleep(1)  # Faster updates for loading\n            step += 1\n        # Final acknowledgment\n        message.react(\"\u2705\")\n        logging.info(\"Loading completed with final acknowledgment.\")\n        message.reply(\"\u2705Done\u2705\")\n\n    def start_progress_in_background(self, step_flag):\n        \"\"\"\n        Starts the progress update in a separate thread.\n        \"\"\"\n        threading.Thread(target=self.update_progress, args=(step_flag, ), daemon=True).start()\n\n    def start_loading_in_background(self, stop_flag: threading.Event):\n        \"\"\"\n        Starts the loading update in a separate thread.\n        \"\"\"\n        threading.Thread(target=self.update_loading, args=(stop_flag,), daemon=True).start()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils.ProgressMessenger.send_initial_message","title":"<code>send_initial_message(mode='progress')</code>","text":"<p>Sends the initial message. Modes can be 'progress' or 'loading'.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/utils.py</code> <pre><code>def send_initial_message(self, mode: str = \"progress\"):\n    \"\"\"\n    Sends the initial message. Modes can be 'progress' or 'loading'.\n    \"\"\"\n    if mode == \"progress\":\n        emoji_legend = \"\\n\".join(\n            f\"{emoji} - Step {i + 1}\" for i, emoji in enumerate(self.emoji_set)\n        )\n        content = (\n            \"Progress is being updated in real-time!\\n\\n\"\n            \"Legend:\\n\"\n            f\"{emoji_legend}\\n\\n\"\n            \"Stay tuned for updates!\"\n        )\n    elif mode == \"loading\":\n        content = (\n            \"Loading in progress! \ud83c\udf00\\n\"\n            \"The indicator will loop until work is done.\"\n        )\n    else:\n        raise ValueError(\"Invalid mode. Use 'progress' or 'loading'.\")\n\n    if self.content is not None:\n        content += '\\n'+self.content\n    message = self.messenger.create_message(content=content, to=self.recipient_phone)\n    response = message.send(sender=0)\n    self.message_id = response.get(\"messages\", [{}])[0].get(\"id\")\n    logging.info(f\"Initial message sent: {content}\")\n    return self.message_id\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils.ProgressMessenger.start_loading_in_background","title":"<code>start_loading_in_background(stop_flag)</code>","text":"<p>Starts the loading update in a separate thread.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/utils.py</code> <pre><code>def start_loading_in_background(self, stop_flag: threading.Event):\n    \"\"\"\n    Starts the loading update in a separate thread.\n    \"\"\"\n    threading.Thread(target=self.update_loading, args=(stop_flag,), daemon=True).start()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils.ProgressMessenger.start_progress_in_background","title":"<code>start_progress_in_background(step_flag)</code>","text":"<p>Starts the progress update in a separate thread.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/utils.py</code> <pre><code>def start_progress_in_background(self, step_flag):\n    \"\"\"\n    Starts the progress update in a separate thread.\n    \"\"\"\n    threading.Thread(target=self.update_progress, args=(step_flag, ), daemon=True).start()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils.ProgressMessenger.update_loading","title":"<code>update_loading(stop_flag)</code>","text":"<p>Continuously updates the reaction to represent a looping 'loading' indicator.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/utils.py</code> <pre><code>def update_loading(self, stop_flag: threading.Event):\n    \"\"\"\n    Continuously updates the reaction to represent a looping 'loading' indicator.\n    \"\"\"\n    if not self.message_id:\n        raise ValueError(\"Message ID not found. Ensure the initial message is sent first.\")\n    message = self.messenger.create_message(id=self.message_id, to=self.recipient_phone)\n    step = 0\n    while not stop_flag.is_set():\n        emoji = self.emoji_set[step % len(self.emoji_set)]\n        message.react(emoji)\n        logging.info(f\"Loading update: {emoji}\")\n        time.sleep(1)  # Faster updates for loading\n        step += 1\n    # Final acknowledgment\n    message.react(\"\u2705\")\n    logging.info(\"Loading completed with final acknowledgment.\")\n    message.reply(\"\u2705Done\u2705\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils.ProgressMessenger.update_progress","title":"<code>update_progress(step_flag)</code>","text":"<p>Updates the reaction on the message to represent progress.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/utils.py</code> <pre><code>def update_progress(self, step_flag: threading.Event):\n    \"\"\"\n    Updates the reaction on the message to represent progress.\n    \"\"\"\n    if not self.message_id:\n        raise ValueError(\"Message ID not found. Ensure the initial message is sent first.\")\n    message = self.messenger.create_message(id=self.message_id, to=self.recipient_phone)\n    for step in range(self.max_steps):\n        emoji = self.emoji_set[step % len(self.emoji_set)]\n        message.react(emoji)\n        logging.info(f\"Progress updated: Step {step + 1}/{self.max_steps} with emoji {emoji}\")\n        while not step_flag.is_set():\n            time.sleep(0.5)\n        step_flag.clear()\n    # Final acknowledgment\n    message.react(\"\ud83d\udc4d\")\n    logging.info(\"Progress completed with final acknowledgment.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.cli_functions","title":"<code>cli_functions</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.cli_functions.replace_bracketed_content","title":"<code>replace_bracketed_content(text, replacements, inlist=False)</code>","text":"<p>Ersetzt Inhalte in eckigen Klammern mit entsprechenden Werten aus einem W\u00f6rterbuch.</p> <p>:param text: Der zu verarbeitende Text als String. :param replacements: Ein W\u00f6rterbuch mit Schl\u00fcssel-Wert-Paaren f\u00fcr die Ersetzung. :return: Den modifizierten Text.</p> Source code in <code>toolboxv2/mods/cli_functions.py</code> <pre><code>def replace_bracketed_content(text, replacements, inlist=False):\n    \"\"\"\n    Ersetzt Inhalte in eckigen Klammern mit entsprechenden Werten aus einem W\u00f6rterbuch.\n\n    :param text: Der zu verarbeitende Text als String.\n    :param replacements: Ein W\u00f6rterbuch mit Schl\u00fcssel-Wert-Paaren f\u00fcr die Ersetzung.\n    :return: Den modifizierten Text.\n    \"\"\"\n    # Finde alle Vorkommen von Texten in eckigen Klammern\n    matches = re.findall(r'\\[([^\\]]+)\\]', text)\n\n    # Ersetze jeden gefundenen Text durch den entsprechenden Wert aus dem W\u00f6rterbuch\n    as_list = text.split(' ')\n    i = 0\n    for key in matches:\n        if key in replacements:\n            if not inlist:\n                text = text.replace(f'[{key}]', str(replacements[key]))\n            else:\n                as_list[i] = replacements[key]\n        i += 1\n    if not inlist:\n        return text\n    return as_list\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa","title":"<code>isaa</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent","title":"<code>CodingAgent</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live","title":"<code>live</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.AsyncCodeDetector","title":"<code>AsyncCodeDetector</code>","text":"<p>               Bases: <code>NodeVisitor</code></p> <p>Detect async code and top-level await</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class AsyncCodeDetector(ast.NodeVisitor):\n    \"\"\"Detect async code and top-level await\"\"\"\n    def __init__(self):\n        self.has_async = False\n        self.has_top_level_await = False\n        self.await_nodes = []\n\n    def visit_AsyncFunctionDef(self, node):\n        self.has_async = True\n        self.generic_visit(node)\n\n    def visit_Await(self, node):\n        self.has_async = True\n        # Track all await nodes\n        self.await_nodes.append(node)\n        # Check if this await is at top level\n        parent = node\n        while hasattr(parent, 'parent'):\n            parent = parent.parent\n            if isinstance(parent, ast.AsyncFunctionDef | ast.FunctionDef):\n                break\n        else:\n            self.has_top_level_await = True\n        self.generic_visit(node)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.BrowserWrapper","title":"<code>BrowserWrapper</code>","text":"<p>A wrapper for browser agent functionality that allows seamless interaction with web browsers.</p> <p>This class provides a system-agnostic interface to control browsers through the browser_use library, supporting both local and remote browser connections.</p> <p>Attributes:</p> Name Type Description <code>browser</code> <p>The Browser instance for web automation</p> <code>agent</code> <p>The BrowserAgent instance for intelligent browsing</p> <code>is_initialized</code> <code>bool</code> <p>Whether the browser has been initialized</p> <code>config</code> <code>Dict</code> <p>Configuration for the browser</p> <code>remote_url</code> <code>Optional[str]</code> <p>URL for remote browser connection if applicable</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class BrowserWrapper:\n    \"\"\"\n    A wrapper for browser agent functionality that allows seamless interaction with web browsers.\n\n    This class provides a system-agnostic interface to control browsers through the browser_use\n    library, supporting both local and remote browser connections.\n\n    Attributes:\n        browser: The Browser instance for web automation\n        agent: The BrowserAgent instance for intelligent browsing\n        is_initialized (bool): Whether the browser has been initialized\n        config (Dict): Configuration for the browser\n        remote_url (Optional[str]): URL for remote browser connection if applicable\n    \"\"\"\n\n    def __init__(self,\n                 llm: Any = None,\n                 headless: bool = False,\n                 chrome_path: str | None = None,\n                 remote_url: str | None = None,\n                 api_key: str | None=None,\n                 config: dict[str, Any] | None = None):\n        \"\"\"\n        Initialize the browser wrapper.\n\n        Args:\n            llm: Language model to use for the browser agent\n            headless: Whether to run the browser in headless mode\n            chrome_path: Path to local Chrome executable\n            remote_url: URL for remote browser connection (wss or cdp)\n            config: Additional browser configuration\n        \"\"\"\n        self.is_initialized = False\n        self.agent = None\n        self.browser = None\n        self.context = None\n        import os\n\n        from pydantic import SecretStr\n        def pars(x):\n            return x.split('/')[-1] if '/' in x else x\n        if llm is None:\n            llm = 'google/gemini-2.0-flash-exp'\n        if not isinstance(llm, str):\n            llm = llm\n        elif 'deepseek' in llm:\n            from langchain_openai import ChatOpenAI\n            llm = ChatOpenAI(base_url='https://api.deepseek.com/v1', model=pars(llm), api_key=SecretStr(api_key or os.getenv('DEEPSEEK_API_KEY')))\n        elif 'google' in llm:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n            llm = ChatGoogleGenerativeAI(model=pars(llm), api_key=SecretStr(api_key or os.getenv('GEMINI_API_KEY')))\n        elif 'claude' in llm:\n            from langchain_anthropic import ChatAnthropic\n            llm = ChatAnthropic(\n                model_name=pars(llm),\n                temperature=0.0,\n                timeout=400,  # Increase for complex tasks\n                api_key=SecretStr(api_key or os.getenv('ANTHROPIC_API_KEY')))\n        elif isinstance(llm, str):\n            from langchain_openai import ChatOpenAI\n            llm = ChatOpenAI(\n                model=pars(llm),\n                temperature=0.0,api_key=SecretStr(api_key or os.getenv('OPENAI_API_KEY'))\n            )\n\n\n\n        self.llm = ChatLiteLLM(model=llm) if isinstance(llm,str) else llm\n        self.parser = None\n\n        browser_config = {\n            'headless': headless,\n            'disable_security': True\n        }\n\n        if config:\n            browser_config.update(config)\n\n        self.config = browser_config\n\n        # Set up remote connection if specified\n        if remote_url:\n            if remote_url.startswith('wss://'):\n                self.config['wss_url'] = remote_url\n            elif remote_url.startswith('http'):\n                self.config['cdp_url'] = remote_url\n            self.remote_url = remote_url\n        else:\n            self.remote_url = None\n\n        # Set up local Chrome path if specified\n        if not headless and remote_url is None and chrome_path is None:\n            import os\n            import platform\n\n            def get_chrome_path():\n                \"\"\"\n                Returns the correct path to the Chrome executable based on the OS.\n                If Chrome is not found, returns None.\n                \"\"\"\n                chrome_paths = {\n                    \"Darwin\": \"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\",  # macOS\n                    \"Windows\": \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\",  # Windows\n                    \"Linux\": \"/usr/bin/google-chrome\"  # Linux\n                }\n\n                system = platform.system()\n                chrome_path_ = chrome_paths.get(system)\n\n                if chrome_path_ and os.path.isfile(chrome_path_):\n                    return chrome_path_\n\n                return None\n\n            chrome_path = get_chrome_path()\n        if chrome_path:\n            self.config['chrome_instance_path'] = chrome_path\n\n\n    async def initialize(self):\n        \"\"\"Initialize the browser and context\"\"\"\n        if self.is_initialized:\n            return\n\n        try:\n            # Create browser instance\n            self.browser = Browser(\n                config=BrowserConfig(**self.config)\n            )\n\n            # Create context configuration with better settings for scraping\n            context_config = BrowserContextConfig(\n                wait_for_network_idle_page_load_time=3.0,\n                highlight_elements=True,\n                viewport_expansion=500,\n                wait_between_actions=0.5  # Add a small delay between actions\n            )\n\n            # Initialize context\n            self.context = await self.browser.new_context(config=context_config)\n\n            # Create an initial page\n            browser_state = await self.context.get_state()\n            if not browser_state or not browser_state.tabs:\n                # If no tabs exist, create a new page\n                await self.browser.get_playwright_browser()\n                browser_context = await self.context.get_playwright_context()\n                self.page = await browser_context.new_page()\n            else:\n                # Use the existing active tab\n                self.page = await self.context.get_current_page()\n\n            self.is_initialized = True\n\n        except Exception as e:\n            # Clean up resources in case of initialization error\n            if self.context:\n                await self.context.close()\n            if self.browser:\n                await self.browser.close()\n            raise Exception(f\"Failed to initialize browser: {str(e)}\")\n\n    async def create_agent(self, task: str, initial_actions=None):\n        \"\"\"Create a browser agent with the specified task\"\"\"\n        #if not self.is_initialized:\n        #    await self.initialize()\n\n        self.agent = BrowserAgent(\n            task=task,\n            llm=self.llm,\n            #browser_context=self.context,\n            initial_actions=initial_actions,\n            #browser=self.browser,\n        )\n        return self.agent\n\n    async def run(self, task: str):\n        \"\"\"Run the browser agent with the specified task\"\"\"\n        agent = await self.create_agent(task)\n        result = await agent.run()\n        return result\n\n    async def navigate(self, url: str):\n        \"\"\"Navigate to a URL\"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        # Get the current active page or create a new one if needed\n        try:\n            page = await self.context.get_current_page()\n            if not page:\n                browser_context = await self.context.get_playwright_context()\n                page = await browser_context.new_page()\n\n            # Navigate to the URL\n            await page.goto(url)\n            self.page = page\n            return page\n        except Exception as e:\n            raise Exception(f\"Failed to navigate to {url}: {str(e)}\")\n\n    async def get_tabs(self):\n        \"\"\"Get all open tabs/pages\"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        browser_state = await self.context.get_state()\n        return browser_state.tabs if browser_state else []\n\n    async def switch_to_tab(self, tab_index: int):\n        \"\"\"Switch to a specific tab by index\"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        browser_state = await self.context.get_state()\n        if not browser_state or not browser_state.tabs or tab_index &gt;= len(browser_state.tabs):\n            raise ValueError(f\"Tab index {tab_index} is out of range\")\n\n        tab_id = browser_state.tabs[tab_index].id\n        await self.context.switch_to_tab(tab_id)\n        self.page = await self.context.get_current_page()\n        return self.page\n\n    async def create_new_tab(self):\n        \"\"\"Create a new tab/page\"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        browser_context = await self.context.get_playwright_context()\n        new_page = await browser_context.new_page()\n        self.page = new_page\n        return new_page\n\n    async def close_current_tab(self):\n        \"\"\"Close the current tab/page\"\"\"\n        if not self.is_initialized:\n            return\n\n        page = await self.context.get_current_page()\n        if page:\n            await page.close()\n\n        # Update the current page reference\n        browser_state = await self.context.get_state()\n        if browser_state and browser_state.tabs:\n            await self.switch_to_tab(0)\n\n    async def execute_js(self, code: str, page=None):\n        \"\"\"Execute JavaScript code in the browser context\"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        if page is None:\n            pages = await self.context.pages()\n            if not pages:\n                page = await self.context.new_page()\n            else:\n                page = pages[0]\n\n        result = await page.evaluate(code)\n        return result\n\n    async def save_context(self):\n        \"\"\"Save browser context state\"\"\"\n        if not self.is_initialized:\n            return None\n\n        return await self.browser.export_context(self.context)\n\n    async def restore_context(self, context_data):\n        \"\"\"Restore browser context from saved state\"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        await self.browser.import_context(context_data)\n\n    async def close(self):\n        \"\"\"Close the browser\"\"\"\n        if self.is_initialized and self.browser:\n            await self.browser.close()\n            self.is_initialized = False\n\n    # Add these methods to the BrowserWrapper class\n\n    def get_parser(self):\n        \"\"\"Get a content parser for the browser\"\"\"\n        if self.parser is None:\n            self.parser = WebContentParser(self)\n        return self.parser\n\n    async def extract_markdown(self, page=None, selector=\"body\", include_images=True):\n        \"\"\"\n        Extract content from a webpage and convert it to markdown.\n        \"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        if page is None:\n            pages = await self.context.pages()\n            if not pages:\n                page = await self.context.new_page()\n            else:\n                page = pages[0]\n\n        # JavaScript to convert HTML to markdown\n        script = \"\"\"\n        (selector, includeImages) =&gt; {\n            const element = document.querySelector(selector);\n            if (!element) return '';\n\n            // Simple HTML to Markdown conversion function\n            const htmlToMarkdown = (node) =&gt; {\n                let result = '';\n\n                // Process text nodes\n                if (node.nodeType === Node.TEXT_NODE) {\n                    return node.textContent;\n                }\n\n                // Process element nodes\n                if (node.nodeType === Node.ELEMENT_NODE) {\n                    const tagName = node.tagName.toLowerCase();\n\n                    // Process by tag type\n                    switch(tagName) {\n                        case 'h1': return '# ' + getInnerText(node) + '\\\\n\\\\n';\n                        case 'h2': return '## ' + getInnerText(node) + '\\\\n\\\\n';\n                        case 'h3': return '### ' + getInnerText(node) + '\\\\n\\\\n';\n                        case 'h4': return '#### ' + getInnerText(node) + '\\\\n\\\\n';\n                        case 'h5': return '##### ' + getInnerText(node) + '\\\\n\\\\n';\n                        case 'h6': return '###### ' + getInnerText(node) + '\\\\n\\\\n';\n                        case 'p': return getInnerText(node) + '\\\\n\\\\n';\n                        case 'br': return '\\\\n';\n                        case 'hr': return '---\\\\n\\\\n';\n                        case 'b':\n                        case 'strong': return '**' + getInnerText(node) + '**';\n                        case 'i':\n                        case 'em': return '*' + getInnerText(node) + '*';\n                        case 'a': {\n                            const href = node.getAttribute('href');\n                            return '[' + getInnerText(node) + '](' + href + ')';\n                        }\n                        case 'img': {\n                            if (!includeImages) return '';\n                            const src = node.getAttribute('src');\n                            const alt = node.getAttribute('alt') || 'image';\n                            return '![' + alt + '](' + src + ')\\\\n\\\\n';\n                        }\n                        case 'code':\n                        case 'pre': return '`' + getInnerText(node) + '`';\n                        case 'ul': {\n                            let listResult = '\\\\n';\n                            Array.from(node.children).forEach(li =&gt; {\n                                if (li.tagName.toLowerCase() === 'li') {\n                                    listResult += '- ' + getInnerText(li) + '\\\\n';\n                                }\n                            });\n                            return listResult + '\\\\n';\n                        }\n                        case 'ol': {\n                            let listResult = '\\\\n';\n                            Array.from(node.children).forEach((li, index) =&gt; {\n                                if (li.tagName.toLowerCase() === 'li') {\n                                    listResult += (index + 1) + '. ' + getInnerText(li) + '\\\\n';\n                                }\n                            });\n                            return listResult + '\\\\n';\n                        }\n                        case 'blockquote': return '&gt; ' + getInnerText(node) + '\\\\n\\\\n';\n                        default: {\n                            // Process child nodes for other elements\n                            for (const child of node.childNodes) {\n                                result += htmlToMarkdown(child);\n                            }\n                            return result;\n                        }\n                    }\n                }\n\n                return '';\n            };\n\n            // Helper function to get inner text with special handling\n            const getInnerText = (node) =&gt; {\n                let text = '';\n                for (const child of node.childNodes) {\n                    text += htmlToMarkdown(child);\n                }\n                return text;\n            };\n\n            return htmlToMarkdown(element);\n        }\n        \"\"\"\n\n        try:\n            # Try to convert to markdown using our script\n            markdown = await page.evaluate(script, selector, include_images)\n\n            # Add a title if we have one\n            title = await page.title()\n            if title and not markdown.startswith(\"# \"):\n                markdown = f\"# {title}\\n\\n{markdown}\"\n\n            return markdown\n        except Exception:\n            # Fallback to basic extraction if script fails\n            content = await self.extract_text(page, selector)\n            title = await page.title()\n            return f\"# {title}\\n\\n{content}\"\n\n    async def take_scrolling_screenshot(self, page=None, full_page=True, path=None,\n                                        initial_delay=1000, scroll_delay=500, format='png'):\n        \"\"\"\n        Take a screenshot with scrolling functionality and delay.\n        \"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        if page is None:\n            pages = await self.context.pages()\n            if not pages:\n                page = await self.context.new_page()\n            else:\n                page = pages[0]\n\n        # Wait for the initial delay to let content load\n        if initial_delay &gt; 0:\n            await page.wait_for_timeout(initial_delay)\n\n        if full_page and scroll_delay &gt; 0:\n            # Get page dimensions\n            dimensions = await page.evaluate(\"\"\"\n                () =&gt; {\n                    return {\n                        width: document.documentElement.scrollWidth,\n                        height: document.documentElement.scrollHeight,\n                        windowHeight: window.innerHeight\n                    }\n                }\n            \"\"\")\n\n            # Scroll down the page gradually to trigger lazy loading\n            current_position = 0\n            while current_position &lt; dimensions['height']:\n                await page.evaluate(f\"window.scrollTo(0, {current_position})\")\n                await page.wait_for_timeout(scroll_delay)\n                current_position += dimensions['windowHeight'] // 2  # Scroll by half viewport\n\n        # Reset scroll position to top\n        await page.evaluate(\"window.scrollTo(0, 0)\")\n\n        # Take the screenshot\n        screenshot_params = {\n            'full_page': full_page,\n            'type': format\n        }\n\n        if path:\n            screenshot_params['path'] = path\n\n        return await page.screenshot(**screenshot_params)\n\n    async def extract_text(self, page=None, selector=\"body\"):\n        \"\"\"\n        Extract plain text from a webpage.\n        \"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        if page is None:\n            pages = await self.context.pages()\n            if not pages:\n                page = await self.context.new_page()\n            else:\n                page = pages[0]\n\n        text = await page.evaluate(\"\"\"\n            (selector) =&gt; {\n                const element = document.querySelector(selector);\n                return element ? element.innerText : '';\n            }\n        \"\"\", selector)\n\n        return text\n\n    async def extract_structured_content(self, page=None, config=None):\n        \"\"\"\n        Extract structured content from a webpage based on a configuration.\n        \"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        if page is None:\n            pages = await self.context.pages()\n            if not pages:\n                page = await self.context.new_page()\n            else:\n                page = pages[0]\n\n        if not config:\n            # Default configuration if none provided\n            config = {\n                'title': 'h1',\n                'headings': 'h2, h3, h4, h5, h6',\n                'paragraphs': 'p',\n                'links': 'a',\n                'images': 'img'\n            }\n\n        result = {}\n\n        for key, selector in config.items():\n            if key == 'links':\n                # Extract links with their href and text\n                result[key] = await page.evaluate(\"\"\"\n                    (selector) =&gt; {\n                        return Array.from(document.querySelectorAll(selector))\n                            .map(el =&gt; ({\n                                text: el.innerText.trim(),\n                                href: el.href\n                            }))\n                            .filter(item =&gt; item.text &amp;&amp; item.href);\n                    }\n                \"\"\", selector)\n            elif key == 'images':\n                # Extract images with their src and alt\n                result[key] = await page.evaluate(\"\"\"\n                    (selector) =&gt; {\n                        return Array.from(document.querySelectorAll(selector))\n                            .map(el =&gt; ({\n                                src: el.src,\n                                alt: el.alt || ''\n                            }))\n                            .filter(item =&gt; item.src);\n                    }\n                \"\"\", selector)\n            else:\n                # Extract text content for other elements\n                result[key] = await page.evaluate(\"\"\"\n                    (selector) =&gt; {\n                        return Array.from(document.querySelectorAll(selector))\n                            .map(el =&gt; el.innerText.trim())\n                            .filter(text =&gt; text);\n                    }\n                \"\"\", selector)\n\n        return result\n</code></pre> <code>__init__(llm=None, headless=False, chrome_path=None, remote_url=None, api_key=None, config=None)</code> \u00b6 <p>Initialize the browser wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>Any</code> <p>Language model to use for the browser agent</p> <code>None</code> <code>headless</code> <code>bool</code> <p>Whether to run the browser in headless mode</p> <code>False</code> <code>chrome_path</code> <code>str | None</code> <p>Path to local Chrome executable</p> <code>None</code> <code>remote_url</code> <code>str | None</code> <p>URL for remote browser connection (wss or cdp)</p> <code>None</code> <code>config</code> <code>dict[str, Any] | None</code> <p>Additional browser configuration</p> <code>None</code> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __init__(self,\n             llm: Any = None,\n             headless: bool = False,\n             chrome_path: str | None = None,\n             remote_url: str | None = None,\n             api_key: str | None=None,\n             config: dict[str, Any] | None = None):\n    \"\"\"\n    Initialize the browser wrapper.\n\n    Args:\n        llm: Language model to use for the browser agent\n        headless: Whether to run the browser in headless mode\n        chrome_path: Path to local Chrome executable\n        remote_url: URL for remote browser connection (wss or cdp)\n        config: Additional browser configuration\n    \"\"\"\n    self.is_initialized = False\n    self.agent = None\n    self.browser = None\n    self.context = None\n    import os\n\n    from pydantic import SecretStr\n    def pars(x):\n        return x.split('/')[-1] if '/' in x else x\n    if llm is None:\n        llm = 'google/gemini-2.0-flash-exp'\n    if not isinstance(llm, str):\n        llm = llm\n    elif 'deepseek' in llm:\n        from langchain_openai import ChatOpenAI\n        llm = ChatOpenAI(base_url='https://api.deepseek.com/v1', model=pars(llm), api_key=SecretStr(api_key or os.getenv('DEEPSEEK_API_KEY')))\n    elif 'google' in llm:\n        from langchain_google_genai import ChatGoogleGenerativeAI\n        llm = ChatGoogleGenerativeAI(model=pars(llm), api_key=SecretStr(api_key or os.getenv('GEMINI_API_KEY')))\n    elif 'claude' in llm:\n        from langchain_anthropic import ChatAnthropic\n        llm = ChatAnthropic(\n            model_name=pars(llm),\n            temperature=0.0,\n            timeout=400,  # Increase for complex tasks\n            api_key=SecretStr(api_key or os.getenv('ANTHROPIC_API_KEY')))\n    elif isinstance(llm, str):\n        from langchain_openai import ChatOpenAI\n        llm = ChatOpenAI(\n            model=pars(llm),\n            temperature=0.0,api_key=SecretStr(api_key or os.getenv('OPENAI_API_KEY'))\n        )\n\n\n\n    self.llm = ChatLiteLLM(model=llm) if isinstance(llm,str) else llm\n    self.parser = None\n\n    browser_config = {\n        'headless': headless,\n        'disable_security': True\n    }\n\n    if config:\n        browser_config.update(config)\n\n    self.config = browser_config\n\n    # Set up remote connection if specified\n    if remote_url:\n        if remote_url.startswith('wss://'):\n            self.config['wss_url'] = remote_url\n        elif remote_url.startswith('http'):\n            self.config['cdp_url'] = remote_url\n        self.remote_url = remote_url\n    else:\n        self.remote_url = None\n\n    # Set up local Chrome path if specified\n    if not headless and remote_url is None and chrome_path is None:\n        import os\n        import platform\n\n        def get_chrome_path():\n            \"\"\"\n            Returns the correct path to the Chrome executable based on the OS.\n            If Chrome is not found, returns None.\n            \"\"\"\n            chrome_paths = {\n                \"Darwin\": \"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\",  # macOS\n                \"Windows\": \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\",  # Windows\n                \"Linux\": \"/usr/bin/google-chrome\"  # Linux\n            }\n\n            system = platform.system()\n            chrome_path_ = chrome_paths.get(system)\n\n            if chrome_path_ and os.path.isfile(chrome_path_):\n                return chrome_path_\n\n            return None\n\n        chrome_path = get_chrome_path()\n    if chrome_path:\n        self.config['chrome_instance_path'] = chrome_path\n</code></pre> <code>close()</code> <code>async</code> \u00b6 <p>Close the browser</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def close(self):\n    \"\"\"Close the browser\"\"\"\n    if self.is_initialized and self.browser:\n        await self.browser.close()\n        self.is_initialized = False\n</code></pre> <code>close_current_tab()</code> <code>async</code> \u00b6 <p>Close the current tab/page</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def close_current_tab(self):\n    \"\"\"Close the current tab/page\"\"\"\n    if not self.is_initialized:\n        return\n\n    page = await self.context.get_current_page()\n    if page:\n        await page.close()\n\n    # Update the current page reference\n    browser_state = await self.context.get_state()\n    if browser_state and browser_state.tabs:\n        await self.switch_to_tab(0)\n</code></pre> <code>create_agent(task, initial_actions=None)</code> <code>async</code> \u00b6 <p>Create a browser agent with the specified task</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def create_agent(self, task: str, initial_actions=None):\n    \"\"\"Create a browser agent with the specified task\"\"\"\n    #if not self.is_initialized:\n    #    await self.initialize()\n\n    self.agent = BrowserAgent(\n        task=task,\n        llm=self.llm,\n        #browser_context=self.context,\n        initial_actions=initial_actions,\n        #browser=self.browser,\n    )\n    return self.agent\n</code></pre> <code>create_new_tab()</code> <code>async</code> \u00b6 <p>Create a new tab/page</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def create_new_tab(self):\n    \"\"\"Create a new tab/page\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    browser_context = await self.context.get_playwright_context()\n    new_page = await browser_context.new_page()\n    self.page = new_page\n    return new_page\n</code></pre> <code>execute_js(code, page=None)</code> <code>async</code> \u00b6 <p>Execute JavaScript code in the browser context</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def execute_js(self, code: str, page=None):\n    \"\"\"Execute JavaScript code in the browser context\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    if page is None:\n        pages = await self.context.pages()\n        if not pages:\n            page = await self.context.new_page()\n        else:\n            page = pages[0]\n\n    result = await page.evaluate(code)\n    return result\n</code></pre> <code>extract_markdown(page=None, selector='body', include_images=True)</code> <code>async</code> \u00b6 <p>Extract content from a webpage and convert it to markdown.</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def extract_markdown(self, page=None, selector=\"body\", include_images=True):\n    \"\"\"\n    Extract content from a webpage and convert it to markdown.\n    \"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    if page is None:\n        pages = await self.context.pages()\n        if not pages:\n            page = await self.context.new_page()\n        else:\n            page = pages[0]\n\n    # JavaScript to convert HTML to markdown\n    script = \"\"\"\n    (selector, includeImages) =&gt; {\n        const element = document.querySelector(selector);\n        if (!element) return '';\n\n        // Simple HTML to Markdown conversion function\n        const htmlToMarkdown = (node) =&gt; {\n            let result = '';\n\n            // Process text nodes\n            if (node.nodeType === Node.TEXT_NODE) {\n                return node.textContent;\n            }\n\n            // Process element nodes\n            if (node.nodeType === Node.ELEMENT_NODE) {\n                const tagName = node.tagName.toLowerCase();\n\n                // Process by tag type\n                switch(tagName) {\n                    case 'h1': return '# ' + getInnerText(node) + '\\\\n\\\\n';\n                    case 'h2': return '## ' + getInnerText(node) + '\\\\n\\\\n';\n                    case 'h3': return '### ' + getInnerText(node) + '\\\\n\\\\n';\n                    case 'h4': return '#### ' + getInnerText(node) + '\\\\n\\\\n';\n                    case 'h5': return '##### ' + getInnerText(node) + '\\\\n\\\\n';\n                    case 'h6': return '###### ' + getInnerText(node) + '\\\\n\\\\n';\n                    case 'p': return getInnerText(node) + '\\\\n\\\\n';\n                    case 'br': return '\\\\n';\n                    case 'hr': return '---\\\\n\\\\n';\n                    case 'b':\n                    case 'strong': return '**' + getInnerText(node) + '**';\n                    case 'i':\n                    case 'em': return '*' + getInnerText(node) + '*';\n                    case 'a': {\n                        const href = node.getAttribute('href');\n                        return '[' + getInnerText(node) + '](' + href + ')';\n                    }\n                    case 'img': {\n                        if (!includeImages) return '';\n                        const src = node.getAttribute('src');\n                        const alt = node.getAttribute('alt') || 'image';\n                        return '![' + alt + '](' + src + ')\\\\n\\\\n';\n                    }\n                    case 'code':\n                    case 'pre': return '`' + getInnerText(node) + '`';\n                    case 'ul': {\n                        let listResult = '\\\\n';\n                        Array.from(node.children).forEach(li =&gt; {\n                            if (li.tagName.toLowerCase() === 'li') {\n                                listResult += '- ' + getInnerText(li) + '\\\\n';\n                            }\n                        });\n                        return listResult + '\\\\n';\n                    }\n                    case 'ol': {\n                        let listResult = '\\\\n';\n                        Array.from(node.children).forEach((li, index) =&gt; {\n                            if (li.tagName.toLowerCase() === 'li') {\n                                listResult += (index + 1) + '. ' + getInnerText(li) + '\\\\n';\n                            }\n                        });\n                        return listResult + '\\\\n';\n                    }\n                    case 'blockquote': return '&gt; ' + getInnerText(node) + '\\\\n\\\\n';\n                    default: {\n                        // Process child nodes for other elements\n                        for (const child of node.childNodes) {\n                            result += htmlToMarkdown(child);\n                        }\n                        return result;\n                    }\n                }\n            }\n\n            return '';\n        };\n\n        // Helper function to get inner text with special handling\n        const getInnerText = (node) =&gt; {\n            let text = '';\n            for (const child of node.childNodes) {\n                text += htmlToMarkdown(child);\n            }\n            return text;\n        };\n\n        return htmlToMarkdown(element);\n    }\n    \"\"\"\n\n    try:\n        # Try to convert to markdown using our script\n        markdown = await page.evaluate(script, selector, include_images)\n\n        # Add a title if we have one\n        title = await page.title()\n        if title and not markdown.startswith(\"# \"):\n            markdown = f\"# {title}\\n\\n{markdown}\"\n\n        return markdown\n    except Exception:\n        # Fallback to basic extraction if script fails\n        content = await self.extract_text(page, selector)\n        title = await page.title()\n        return f\"# {title}\\n\\n{content}\"\n</code></pre> <code>extract_structured_content(page=None, config=None)</code> <code>async</code> \u00b6 <p>Extract structured content from a webpage based on a configuration.</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def extract_structured_content(self, page=None, config=None):\n    \"\"\"\n    Extract structured content from a webpage based on a configuration.\n    \"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    if page is None:\n        pages = await self.context.pages()\n        if not pages:\n            page = await self.context.new_page()\n        else:\n            page = pages[0]\n\n    if not config:\n        # Default configuration if none provided\n        config = {\n            'title': 'h1',\n            'headings': 'h2, h3, h4, h5, h6',\n            'paragraphs': 'p',\n            'links': 'a',\n            'images': 'img'\n        }\n\n    result = {}\n\n    for key, selector in config.items():\n        if key == 'links':\n            # Extract links with their href and text\n            result[key] = await page.evaluate(\"\"\"\n                (selector) =&gt; {\n                    return Array.from(document.querySelectorAll(selector))\n                        .map(el =&gt; ({\n                            text: el.innerText.trim(),\n                            href: el.href\n                        }))\n                        .filter(item =&gt; item.text &amp;&amp; item.href);\n                }\n            \"\"\", selector)\n        elif key == 'images':\n            # Extract images with their src and alt\n            result[key] = await page.evaluate(\"\"\"\n                (selector) =&gt; {\n                    return Array.from(document.querySelectorAll(selector))\n                        .map(el =&gt; ({\n                            src: el.src,\n                            alt: el.alt || ''\n                        }))\n                        .filter(item =&gt; item.src);\n                }\n            \"\"\", selector)\n        else:\n            # Extract text content for other elements\n            result[key] = await page.evaluate(\"\"\"\n                (selector) =&gt; {\n                    return Array.from(document.querySelectorAll(selector))\n                        .map(el =&gt; el.innerText.trim())\n                        .filter(text =&gt; text);\n                }\n            \"\"\", selector)\n\n    return result\n</code></pre> <code>extract_text(page=None, selector='body')</code> <code>async</code> \u00b6 <p>Extract plain text from a webpage.</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def extract_text(self, page=None, selector=\"body\"):\n    \"\"\"\n    Extract plain text from a webpage.\n    \"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    if page is None:\n        pages = await self.context.pages()\n        if not pages:\n            page = await self.context.new_page()\n        else:\n            page = pages[0]\n\n    text = await page.evaluate(\"\"\"\n        (selector) =&gt; {\n            const element = document.querySelector(selector);\n            return element ? element.innerText : '';\n        }\n    \"\"\", selector)\n\n    return text\n</code></pre> <code>get_parser()</code> \u00b6 <p>Get a content parser for the browser</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def get_parser(self):\n    \"\"\"Get a content parser for the browser\"\"\"\n    if self.parser is None:\n        self.parser = WebContentParser(self)\n    return self.parser\n</code></pre> <code>get_tabs()</code> <code>async</code> \u00b6 <p>Get all open tabs/pages</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def get_tabs(self):\n    \"\"\"Get all open tabs/pages\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    browser_state = await self.context.get_state()\n    return browser_state.tabs if browser_state else []\n</code></pre> <code>initialize()</code> <code>async</code> \u00b6 <p>Initialize the browser and context</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def initialize(self):\n    \"\"\"Initialize the browser and context\"\"\"\n    if self.is_initialized:\n        return\n\n    try:\n        # Create browser instance\n        self.browser = Browser(\n            config=BrowserConfig(**self.config)\n        )\n\n        # Create context configuration with better settings for scraping\n        context_config = BrowserContextConfig(\n            wait_for_network_idle_page_load_time=3.0,\n            highlight_elements=True,\n            viewport_expansion=500,\n            wait_between_actions=0.5  # Add a small delay between actions\n        )\n\n        # Initialize context\n        self.context = await self.browser.new_context(config=context_config)\n\n        # Create an initial page\n        browser_state = await self.context.get_state()\n        if not browser_state or not browser_state.tabs:\n            # If no tabs exist, create a new page\n            await self.browser.get_playwright_browser()\n            browser_context = await self.context.get_playwright_context()\n            self.page = await browser_context.new_page()\n        else:\n            # Use the existing active tab\n            self.page = await self.context.get_current_page()\n\n        self.is_initialized = True\n\n    except Exception as e:\n        # Clean up resources in case of initialization error\n        if self.context:\n            await self.context.close()\n        if self.browser:\n            await self.browser.close()\n        raise Exception(f\"Failed to initialize browser: {str(e)}\")\n</code></pre> <code>navigate(url)</code> <code>async</code> \u00b6 <p>Navigate to a URL</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def navigate(self, url: str):\n    \"\"\"Navigate to a URL\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    # Get the current active page or create a new one if needed\n    try:\n        page = await self.context.get_current_page()\n        if not page:\n            browser_context = await self.context.get_playwright_context()\n            page = await browser_context.new_page()\n\n        # Navigate to the URL\n        await page.goto(url)\n        self.page = page\n        return page\n    except Exception as e:\n        raise Exception(f\"Failed to navigate to {url}: {str(e)}\")\n</code></pre> <code>restore_context(context_data)</code> <code>async</code> \u00b6 <p>Restore browser context from saved state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def restore_context(self, context_data):\n    \"\"\"Restore browser context from saved state\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    await self.browser.import_context(context_data)\n</code></pre> <code>run(task)</code> <code>async</code> \u00b6 <p>Run the browser agent with the specified task</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def run(self, task: str):\n    \"\"\"Run the browser agent with the specified task\"\"\"\n    agent = await self.create_agent(task)\n    result = await agent.run()\n    return result\n</code></pre> <code>save_context()</code> <code>async</code> \u00b6 <p>Save browser context state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def save_context(self):\n    \"\"\"Save browser context state\"\"\"\n    if not self.is_initialized:\n        return None\n\n    return await self.browser.export_context(self.context)\n</code></pre> <code>switch_to_tab(tab_index)</code> <code>async</code> \u00b6 <p>Switch to a specific tab by index</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def switch_to_tab(self, tab_index: int):\n    \"\"\"Switch to a specific tab by index\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    browser_state = await self.context.get_state()\n    if not browser_state or not browser_state.tabs or tab_index &gt;= len(browser_state.tabs):\n        raise ValueError(f\"Tab index {tab_index} is out of range\")\n\n    tab_id = browser_state.tabs[tab_index].id\n    await self.context.switch_to_tab(tab_id)\n    self.page = await self.context.get_current_page()\n    return self.page\n</code></pre> <code>take_scrolling_screenshot(page=None, full_page=True, path=None, initial_delay=1000, scroll_delay=500, format='png')</code> <code>async</code> \u00b6 <p>Take a screenshot with scrolling functionality and delay.</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def take_scrolling_screenshot(self, page=None, full_page=True, path=None,\n                                    initial_delay=1000, scroll_delay=500, format='png'):\n    \"\"\"\n    Take a screenshot with scrolling functionality and delay.\n    \"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    if page is None:\n        pages = await self.context.pages()\n        if not pages:\n            page = await self.context.new_page()\n        else:\n            page = pages[0]\n\n    # Wait for the initial delay to let content load\n    if initial_delay &gt; 0:\n        await page.wait_for_timeout(initial_delay)\n\n    if full_page and scroll_delay &gt; 0:\n        # Get page dimensions\n        dimensions = await page.evaluate(\"\"\"\n            () =&gt; {\n                return {\n                    width: document.documentElement.scrollWidth,\n                    height: document.documentElement.scrollHeight,\n                    windowHeight: window.innerHeight\n                }\n            }\n        \"\"\")\n\n        # Scroll down the page gradually to trigger lazy loading\n        current_position = 0\n        while current_position &lt; dimensions['height']:\n            await page.evaluate(f\"window.scrollTo(0, {current_position})\")\n            await page.wait_for_timeout(scroll_delay)\n            current_position += dimensions['windowHeight'] // 2  # Scroll by half viewport\n\n    # Reset scroll position to top\n    await page.evaluate(\"window.scrollTo(0, 0)\")\n\n    # Take the screenshot\n    screenshot_params = {\n        'full_page': full_page,\n        'type': format\n    }\n\n    if path:\n        screenshot_params['path'] = path\n\n    return await page.screenshot(**screenshot_params)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface","title":"<code>CargoRustInterface</code>","text":"<p>Usage :</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface--create-interface","title":"Create interface","text":"<p>cargo_interface = CargoRustInterface()</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface--set-up-new-project","title":"Set up new project","text":"<p>await cargo_interface.setup_project(\"hello_rust\")</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface--add-a-dependency","title":"Add a dependency","text":"<p>await cargo_interface.add_dependency(\"serde\", \"1.0\")</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface--write-and-run-some-code","title":"Write and run some code","text":"<p>code = \"\"\" fn main() {     println!(\"Hello, Rust!\"); } \"\"\" result = await cargo_interface.run_code(code)</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface--modify-code","title":"Modify code","text":"<p>new_function = \"\"\" fn main() {     println!(\"Modified Hello, Rust!\"); } \"\"\" await cargo_interface.modify_code(new_function, \"main()\")</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface--build-and-test","title":"Build and test","text":"<p>await cargo_interface.build() await cargo_interface.test()</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class CargoRustInterface:\n    '''Usage :\n# Create interface\ncargo_interface = CargoRustInterface()\n\n# Set up new project\nawait cargo_interface.setup_project(\"hello_rust\")\n\n# Add a dependency\nawait cargo_interface.add_dependency(\"serde\", \"1.0\")\n\n# Write and run some code\ncode = \"\"\"\nfn main() {\n    println!(\"Hello, Rust!\");\n}\n\"\"\"\nresult = await cargo_interface.run_code(code)\n\n# Modify code\nnew_function = \"\"\"\nfn main() {\n    println!(\"Modified Hello, Rust!\");\n}\n\"\"\"\nawait cargo_interface.modify_code(new_function, \"main()\")\n\n# Build and test\nawait cargo_interface.build()\nawait cargo_interface.test()\n\n    '''\n    def __init__(self, session_dir=None, auto_remove=True):\n        \"\"\"Initialize the Rust/Cargo interface\"\"\"\n        self.auto_remove = auto_remove\n        self._session_dir = session_dir or Path.home() / '.cargo_sessions'\n        self._session_dir.mkdir(exist_ok=True)\n        self.vfs = VirtualFileSystem(self._session_dir / 'virtual_fs')\n        self.output_history = {}\n        self._execution_count = 0\n        self.current_project = None\n\n    def reset(self):\n        \"\"\"Reset the interface state\"\"\"\n        if self.auto_remove and self.current_project:\n            shutil.rmtree(self.current_project, ignore_errors=True)\n        self.output_history.clear()\n        self._execution_count = 0\n        self.current_project = None\n\n    async def setup_project(self, name: str) -&gt; str:\n        \"\"\"Set up a new Cargo project\"\"\"\n        try:\n            project_path = self.vfs.base_dir / name\n            if project_path.exists():\n                shutil.rmtree(project_path)\n\n            result = subprocess.run(\n                ['cargo', 'new', str(project_path)],\n                capture_output=True,\n                text=True, check=True\n            )\n\n            if result.returncode != 0:\n                return f\"Error creating project: {result.stderr}\"\n\n            self.current_project = project_path\n            return f\"Created new project at {project_path}\"\n\n        except Exception as e:\n            return f\"Failed to create project: {str(e)}\"\n\n    async def add_dependency(self, name: str, version: str | None = None) -&gt; str:\n        \"\"\"Add a dependency to Cargo.toml\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            cargo_toml = self.current_project / \"Cargo.toml\"\n            if not cargo_toml.exists():\n                return \"Cargo.toml not found\"\n\n            cmd = ['cargo', 'add', name]\n            if version:\n                cmd.extend(['--vers', version])\n\n            result = subprocess.run(\n                cmd,\n                cwd=self.current_project,\n                capture_output=True,\n                text=True,check=True\n            )\n\n            return result.stdout if result.returncode == 0 else f\"Error: {result.stderr}\"\n\n        except Exception as e:\n            return f\"Failed to add dependency: {str(e)}\"\n\n    async def build(self, release: bool = False) -&gt; str:\n        \"\"\"Build the project\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            cmd = ['cargo', 'build']\n            if release:\n                cmd.append('--release')\n\n            result = subprocess.run(\n                cmd,\n                cwd=self.current_project,\n                capture_output=True,\n                text=True\n            )\n\n            return result.stdout if result.returncode == 0 else f\"Build error: {result.stderr}\"\n\n        except Exception as e:\n            return f\"Build failed: {str(e)}\"\n\n    async def test(self) -&gt; str:\n        \"\"\"Run project tests\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            result = subprocess.run(\n                ['cargo', 'test'],\n                cwd=self.current_project,\n                capture_output=True,\n                text=True, check=True\n            )\n\n            return result.stdout if result.returncode == 0 else f\"Test error: {result.stderr}\"\n\n        except Exception as e:\n            return f\"Tests failed: {str(e)}\"\n\n    async def run_code(self, code: str) -&gt; str:\n        \"\"\"Run Rust code\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            # Write code to main.rs\n            main_rs = self.current_project / \"src\" / \"main.rs\"\n            with open(main_rs, 'w') as f:\n                f.write(code)\n\n            # Build and run\n            build_result = subprocess.run(\n                ['cargo', 'build'],\n                cwd=self.current_project,\n                capture_output=True,\n                text=True\n            )\n\n            if build_result.returncode != 0:\n                return f\"Compilation error: {build_result.stderr}\"\n\n            run_result = subprocess.run(\n                ['cargo', 'run'],\n                cwd=self.current_project,\n                capture_output=True,\n                text=True\n            )\n\n            self._execution_count += 1\n            output = {\n                'code': code,\n                'stdout': run_result.stdout,\n                'stderr': run_result.stderr,\n                'result': run_result.returncode == 0\n            }\n            self.output_history[self._execution_count] = output\n\n            return run_result.stdout if run_result.returncode == 0 else f\"Runtime error: {run_result.stderr}\"\n\n        except Exception as e:\n            return f\"Execution failed: {str(e)}\"\n\n    async def modify_code(self, code: str, object_name: str, file: str = \"src/main.rs\") -&gt; str:\n        \"\"\"Modify existing Rust code\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            file_path = self.current_project / file\n            if not file_path.exists():\n                return f\"File {file} not found\"\n\n            with open(file_path) as f:\n                content = f.read()\n\n            # Handle function modification\n            if object_name.endswith(\"()\"):\n                func_name = object_name[:-2]\n                # Find and replace function definition\n                pattern = f\"fn {func_name}.*?}}(?=\\n|$)\"\n                updated_content = re.sub(pattern, code.strip(), content, flags=re.DOTALL)\n            else:\n                # Handle other modifications (structs, constants, etc.)\n                pattern = f\"{object_name}.*?(?=\\n|$)\"\n                updated_content = re.sub(pattern, code.strip(), content)\n\n            with open(file_path, 'w') as f:\n                f.write(updated_content)\n\n            return f\"Modified {object_name} in {file}\"\n\n        except Exception as e:\n            return f\"Modification failed: {str(e)}\"\n\n    def save_session(self, name: str):\n        \"\"\"Save current session state\"\"\"\n        session_file = self._session_dir / f\"{name}.json\"\n        state = {\n            'output_history': self.output_history,\n            'current_project': str(self.current_project) if self.current_project else None\n        }\n\n        with open(session_file, 'w') as f:\n            json.dump(state, f)\n\n    def load_session(self, name: str):\n        \"\"\"Load saved session state\"\"\"\n        session_file = self._session_dir / f\"{name}.json\"\n        if session_file.exists():\n            with open(session_file) as f:\n                state = json.load(f)\n                self.output_history = state['output_history']\n                self.current_project = Path(state['current_project']) if state['current_project'] else None\n</code></pre> <code>__init__(session_dir=None, auto_remove=True)</code> \u00b6 <p>Initialize the Rust/Cargo interface</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __init__(self, session_dir=None, auto_remove=True):\n    \"\"\"Initialize the Rust/Cargo interface\"\"\"\n    self.auto_remove = auto_remove\n    self._session_dir = session_dir or Path.home() / '.cargo_sessions'\n    self._session_dir.mkdir(exist_ok=True)\n    self.vfs = VirtualFileSystem(self._session_dir / 'virtual_fs')\n    self.output_history = {}\n    self._execution_count = 0\n    self.current_project = None\n</code></pre> <code>add_dependency(name, version=None)</code> <code>async</code> \u00b6 <p>Add a dependency to Cargo.toml</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def add_dependency(self, name: str, version: str | None = None) -&gt; str:\n    \"\"\"Add a dependency to Cargo.toml\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        cargo_toml = self.current_project / \"Cargo.toml\"\n        if not cargo_toml.exists():\n            return \"Cargo.toml not found\"\n\n        cmd = ['cargo', 'add', name]\n        if version:\n            cmd.extend(['--vers', version])\n\n        result = subprocess.run(\n            cmd,\n            cwd=self.current_project,\n            capture_output=True,\n            text=True,check=True\n        )\n\n        return result.stdout if result.returncode == 0 else f\"Error: {result.stderr}\"\n\n    except Exception as e:\n        return f\"Failed to add dependency: {str(e)}\"\n</code></pre> <code>build(release=False)</code> <code>async</code> \u00b6 <p>Build the project</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def build(self, release: bool = False) -&gt; str:\n    \"\"\"Build the project\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        cmd = ['cargo', 'build']\n        if release:\n            cmd.append('--release')\n\n        result = subprocess.run(\n            cmd,\n            cwd=self.current_project,\n            capture_output=True,\n            text=True\n        )\n\n        return result.stdout if result.returncode == 0 else f\"Build error: {result.stderr}\"\n\n    except Exception as e:\n        return f\"Build failed: {str(e)}\"\n</code></pre> <code>load_session(name)</code> \u00b6 <p>Load saved session state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def load_session(self, name: str):\n    \"\"\"Load saved session state\"\"\"\n    session_file = self._session_dir / f\"{name}.json\"\n    if session_file.exists():\n        with open(session_file) as f:\n            state = json.load(f)\n            self.output_history = state['output_history']\n            self.current_project = Path(state['current_project']) if state['current_project'] else None\n</code></pre> <code>modify_code(code, object_name, file='src/main.rs')</code> <code>async</code> \u00b6 <p>Modify existing Rust code</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def modify_code(self, code: str, object_name: str, file: str = \"src/main.rs\") -&gt; str:\n    \"\"\"Modify existing Rust code\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        file_path = self.current_project / file\n        if not file_path.exists():\n            return f\"File {file} not found\"\n\n        with open(file_path) as f:\n            content = f.read()\n\n        # Handle function modification\n        if object_name.endswith(\"()\"):\n            func_name = object_name[:-2]\n            # Find and replace function definition\n            pattern = f\"fn {func_name}.*?}}(?=\\n|$)\"\n            updated_content = re.sub(pattern, code.strip(), content, flags=re.DOTALL)\n        else:\n            # Handle other modifications (structs, constants, etc.)\n            pattern = f\"{object_name}.*?(?=\\n|$)\"\n            updated_content = re.sub(pattern, code.strip(), content)\n\n        with open(file_path, 'w') as f:\n            f.write(updated_content)\n\n        return f\"Modified {object_name} in {file}\"\n\n    except Exception as e:\n        return f\"Modification failed: {str(e)}\"\n</code></pre> <code>reset()</code> \u00b6 <p>Reset the interface state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def reset(self):\n    \"\"\"Reset the interface state\"\"\"\n    if self.auto_remove and self.current_project:\n        shutil.rmtree(self.current_project, ignore_errors=True)\n    self.output_history.clear()\n    self._execution_count = 0\n    self.current_project = None\n</code></pre> <code>run_code(code)</code> <code>async</code> \u00b6 <p>Run Rust code</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def run_code(self, code: str) -&gt; str:\n    \"\"\"Run Rust code\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        # Write code to main.rs\n        main_rs = self.current_project / \"src\" / \"main.rs\"\n        with open(main_rs, 'w') as f:\n            f.write(code)\n\n        # Build and run\n        build_result = subprocess.run(\n            ['cargo', 'build'],\n            cwd=self.current_project,\n            capture_output=True,\n            text=True\n        )\n\n        if build_result.returncode != 0:\n            return f\"Compilation error: {build_result.stderr}\"\n\n        run_result = subprocess.run(\n            ['cargo', 'run'],\n            cwd=self.current_project,\n            capture_output=True,\n            text=True\n        )\n\n        self._execution_count += 1\n        output = {\n            'code': code,\n            'stdout': run_result.stdout,\n            'stderr': run_result.stderr,\n            'result': run_result.returncode == 0\n        }\n        self.output_history[self._execution_count] = output\n\n        return run_result.stdout if run_result.returncode == 0 else f\"Runtime error: {run_result.stderr}\"\n\n    except Exception as e:\n        return f\"Execution failed: {str(e)}\"\n</code></pre> <code>save_session(name)</code> \u00b6 <p>Save current session state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def save_session(self, name: str):\n    \"\"\"Save current session state\"\"\"\n    session_file = self._session_dir / f\"{name}.json\"\n    state = {\n        'output_history': self.output_history,\n        'current_project': str(self.current_project) if self.current_project else None\n    }\n\n    with open(session_file, 'w') as f:\n        json.dump(state, f)\n</code></pre> <code>setup_project(name)</code> <code>async</code> \u00b6 <p>Set up a new Cargo project</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def setup_project(self, name: str) -&gt; str:\n    \"\"\"Set up a new Cargo project\"\"\"\n    try:\n        project_path = self.vfs.base_dir / name\n        if project_path.exists():\n            shutil.rmtree(project_path)\n\n        result = subprocess.run(\n            ['cargo', 'new', str(project_path)],\n            capture_output=True,\n            text=True, check=True\n        )\n\n        if result.returncode != 0:\n            return f\"Error creating project: {result.stderr}\"\n\n        self.current_project = project_path\n        return f\"Created new project at {project_path}\"\n\n    except Exception as e:\n        return f\"Failed to create project: {str(e)}\"\n</code></pre> <code>test()</code> <code>async</code> \u00b6 <p>Run project tests</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def test(self) -&gt; str:\n    \"\"\"Run project tests\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        result = subprocess.run(\n            ['cargo', 'test'],\n            cwd=self.current_project,\n            capture_output=True,\n            text=True, check=True\n        )\n\n        return result.stdout if result.returncode == 0 else f\"Test error: {result.stderr}\"\n\n    except Exception as e:\n        return f\"Tests failed: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.EnhancedVerboseOutput","title":"<code>EnhancedVerboseOutput</code>","text":"Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class EnhancedVerboseOutput:\n    def __init__(self, verbose: bool = True,print_f=None):\n        self.verbose = verbose\n        self.print = print_f or print\n        self.formatter = VerboseFormatter(self.print)\n\n\n    async def log_message(self, role: str, content: str):\n        \"\"\"Log chat messages with role-based formatting\"\"\"\n        if not self.verbose:\n            return\n\n        role_formats = {\n            'user': (self.formatter.style.GREEN, \"\ud83d\udc64\"),\n            'assistant': (self.formatter.style.BLUE, \"\ud83e\udd16\"),\n            'system': (self.formatter.style.YELLOW, \"\u2699\ufe0f\")\n        }\n\n        color_func, icon = role_formats.get(role, (self.formatter.style.WHITE, \"\u2022\"))\n        self.print(f\"\\n{icon} {color_func(f'[{role}]')}\")\n        self.print(f\"{self.formatter.style.GREY('\u2514\u2500')} {content}\\n\")\n\n    async def log_think_result(self, result: dict[str, Any]):\n        \"\"\"Log thinking results with structured formatting\"\"\"\n        if not self.verbose:\n            return\n\n        self.formatter.print_section(\n            \"Action Result\",\n            f\"Action: {result.get('action', 'N/A')}\\n\"\n            f\"context: {result.get('context', 'N/A')}\\n\"\n            f\"Content:\\n{result.get('content', '')}\"\n        )\n\n    async def log_process_result(self, result: dict[str, Any]):\n        \"\"\"Log processing results with structured formatting\"\"\"\n        if not self.verbose:\n            return\n\n        self.formatter.print_section(\n            \"Process Result\",\n            f\"Completed: {result.get('is_completed', False)}\\n\"\n            f\"Effectiveness: {result.get('effectiveness', 'N/A')}\\n\"\n            f\"Recommendations: \\n{result.get('recommendations', 'None')}\\n\"\n            f\"workflow: \\n{result.get('workflow', 'None')}\\n\"\n            f\"errors: {result.get('errors', 'None')}\\n\"\n            f\"text: {result.get('text', 'None')}\"\n        )\n\n    def log_header(self, text: str):\n        \"\"\"Log method update with structured formatting\"\"\"\n        if not self.verbose:\n            return\n\n        self.formatter.print_header(text)\n\n    def log_state(self, state: str, user_ns:dict, override=False):\n        \"\"\"Log method update with structured formatting\"\"\"\n        if not self.verbose and override:\n            return\n\n        return self.formatter.print_state(state, user_ns)\n\n    async def process(self, message: str, coroutine):\n        if not self.verbose:\n            return await coroutine\n        if message == \"code\":\n            return await coroutine\n        return await self.formatter.process_with_spinner(message, coroutine)\n</code></pre> <code>log_header(text)</code> \u00b6 <p>Log method update with structured formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def log_header(self, text: str):\n    \"\"\"Log method update with structured formatting\"\"\"\n    if not self.verbose:\n        return\n\n    self.formatter.print_header(text)\n</code></pre> <code>log_message(role, content)</code> <code>async</code> \u00b6 <p>Log chat messages with role-based formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def log_message(self, role: str, content: str):\n    \"\"\"Log chat messages with role-based formatting\"\"\"\n    if not self.verbose:\n        return\n\n    role_formats = {\n        'user': (self.formatter.style.GREEN, \"\ud83d\udc64\"),\n        'assistant': (self.formatter.style.BLUE, \"\ud83e\udd16\"),\n        'system': (self.formatter.style.YELLOW, \"\u2699\ufe0f\")\n    }\n\n    color_func, icon = role_formats.get(role, (self.formatter.style.WHITE, \"\u2022\"))\n    self.print(f\"\\n{icon} {color_func(f'[{role}]')}\")\n    self.print(f\"{self.formatter.style.GREY('\u2514\u2500')} {content}\\n\")\n</code></pre> <code>log_process_result(result)</code> <code>async</code> \u00b6 <p>Log processing results with structured formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def log_process_result(self, result: dict[str, Any]):\n    \"\"\"Log processing results with structured formatting\"\"\"\n    if not self.verbose:\n        return\n\n    self.formatter.print_section(\n        \"Process Result\",\n        f\"Completed: {result.get('is_completed', False)}\\n\"\n        f\"Effectiveness: {result.get('effectiveness', 'N/A')}\\n\"\n        f\"Recommendations: \\n{result.get('recommendations', 'None')}\\n\"\n        f\"workflow: \\n{result.get('workflow', 'None')}\\n\"\n        f\"errors: {result.get('errors', 'None')}\\n\"\n        f\"text: {result.get('text', 'None')}\"\n    )\n</code></pre> <code>log_state(state, user_ns, override=False)</code> \u00b6 <p>Log method update with structured formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def log_state(self, state: str, user_ns:dict, override=False):\n    \"\"\"Log method update with structured formatting\"\"\"\n    if not self.verbose and override:\n        return\n\n    return self.formatter.print_state(state, user_ns)\n</code></pre> <code>log_think_result(result)</code> <code>async</code> \u00b6 <p>Log thinking results with structured formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def log_think_result(self, result: dict[str, Any]):\n    \"\"\"Log thinking results with structured formatting\"\"\"\n    if not self.verbose:\n        return\n\n    self.formatter.print_section(\n        \"Action Result\",\n        f\"Action: {result.get('action', 'N/A')}\\n\"\n        f\"context: {result.get('context', 'N/A')}\\n\"\n        f\"Content:\\n{result.get('content', '')}\"\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.JSExecutionRecord","title":"<code>JSExecutionRecord</code>  <code>dataclass</code>","text":"<p>Records JavaScript execution details</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>@dataclass\nclass JSExecutionRecord:\n    \"\"\"Records JavaScript execution details\"\"\"\n    code: str\n    result: Any\n    error: str | None = None\n    page_state: dict | None = None\n    extracted_data: dict | None = None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.MockIPython","title":"<code>MockIPython</code>","text":"Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class MockIPython:\n    def __init__(self, _session_dir=None, auto_remove=True):\n        self.auto_remove = auto_remove\n        self.output_history = {}\n        self._execution_count = 0\n        self._session_dir = _session_dir or Path(get_app().appdata) / '.pipeline_sessions'\n        self._session_dir.mkdir(exist_ok=True)\n        self.vfs = VirtualFileSystem(self._session_dir / 'virtual_fs')\n        self._venv_path = self._session_dir / 'venv'\n        self.user_ns: dict[str, Any] = {}\n        nest_asyncio.apply()\n        # Set up virtual environment if it doesn't exist\n        with Spinner(\"Starting virtual environment\"):\n            self._setup_venv()\n        self.reset()\n\n    def _setup_venv(self):\n        \"\"\"Create virtual environment if it doesn't exist\"\"\"\n        if not self._venv_path.exists():\n            try:\n                subprocess.run([sys.executable, \"-m\", \"venv\", str(self._venv_path)], check=True)\n            except subprocess.CalledProcessError as e:\n                raise RuntimeError(f\"Failed to create virtual environment: {str(e)}\")\n\n    def _virtual_open(self, filepath, mode='r', *args, **kwargs):\n        \"\"\"Custom open function that uses virtual filesystem\"\"\"\n        abs_path = self.vfs._resolve_path(filepath)\n\n        if 'w' in mode or 'a' in mode:\n            # Ensure parent directory exists\n            abs_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Use actual filesystem but track in virtual fs\n        real_file = open(abs_path, mode, *args, **kwargs)\n\n        if 'r' in mode:\n            # Track file content in virtual filesystem when reading\n            rel_path = str(abs_path.relative_to(self.vfs.base_dir))\n            if rel_path not in self.vfs.virtual_files:\n                try:\n                    self.vfs.virtual_files[rel_path] = real_file.read()\n                    real_file.seek(0)\n                except UnicodeDecodeError:\n                    # Handle binary files\n                    pass\n\n        return real_file\n\n    def reset(self):\n        \"\"\"Reset the interpreter state\"\"\"\n        self.user_ns = {\n            '__name__': '__main__',\n            '__builtins__': __builtins__,\n            'toolboxv2': toolboxv2,\n            '__file__': None,\n            '__path__': [str(self.vfs.current_dir)],\n            'auto_install': auto_install,\n            'modify_code': self.modify_code,\n        }\n        self.output_history.clear()\n        self._execution_count = 0\n        if self.auto_remove:\n            shutil.rmtree(self.vfs.base_dir, ignore_errors=True)\n\n    def get_namespace(self) -&gt; dict[str, Any]:\n        \"\"\"Get current namespace\"\"\"\n        return self.user_ns.copy()\n\n    def update_namespace(self, variables: dict[str, Any]):\n        \"\"\"Update namespace with new variables\"\"\"\n        self.user_ns.update(variables)\n\n    @staticmethod\n    def _parse_code(code: str) -&gt; tuple[Any, Any | None, bool, bool]:\n        \"\"\"Parse code and handle top-level await\"\"\"\n        code_ = \"\"\n        for line in code.split('\\n'):\n            if line.strip().startswith('#'):\n                continue\n            if line.strip().startswith('asyncio.run('):\n                line = (' ' *(len(line) - len(line.strip()))) + 'await ' + line.strip()[len('asyncio.run('):-1]\n            code_ += line + '\\n'\n        try:\n            tree = ast.parse(code)\n            # Add parent references\n            ParentNodeTransformer().visit(tree)\n\n            # Detect async features\n            detector = AsyncCodeDetector()\n            detector.visit(tree)\n\n            if detector.has_top_level_await:\n                # Wrap code in async function\n                wrapped_code = \"async def __wrapper():\\n\"\n                wrapped_code += \"    global result\\n\"  # Allow writing to global scope\n                wrapped_code += \"    result = None\\n\"\n                # add try:\n                wrapped_code +=\"    try:\\n\"\n                # Indent the original code\n                wrapped_code += \"\\n\".join(f\"        {line}\" for line in code.splitlines())\n                # Add return statement for last expression\n                wrapped_code +=\"\\n    except Exception as e:\\n\"\n                wrapped_code +=\"        import traceback\\n\"\n                wrapped_code +=\"        print(traceback.format_exc())\\n\"\n                wrapped_code +=\"        raise e\\n\"\n                if isinstance(tree.body[-1], ast.Expr):\n                    wrapped_code += \"\\n    return result\"\n\n                # Parse and compile wrapped code\n                wrapped_tree = ast.parse(wrapped_code)\n                return (\n                    compile(wrapped_tree, '&lt;exec&gt;', 'exec'),\n                    None,\n                    True,\n                    True\n                )\n\n            # Handle regular code\n            if isinstance(tree.body[-1], ast.Expr):\n                exec_code = ast.Module(\n                    body=tree.body[:-1],\n                    type_ignores=[]\n                )\n                eval_code = ast.Expression(\n                    body=tree.body[-1].value\n                )\n                return (\n                    compile(exec_code, '&lt;exec&gt;', 'exec'),\n                    compile(eval_code, '&lt;eval&gt;', 'eval'),\n                    detector.has_async,\n                    False\n                )\n\n            return (\n                compile(tree, '&lt;exec&gt;', 'exec'),\n                None,\n                detector.has_async,\n                False\n            )\n\n        except SyntaxError as e:\n            lines = code.splitlines()\n            if e.lineno and e.lineno &lt;= len(lines):\n                line = lines[e.lineno - 1]\n                arrow = ' ' * (e.offset - 1) + '^' if e.offset else ''\n                error_msg = (\n                    f\"Syntax error at line {e.lineno}:\\n\"\n                    f\"{line}\\n\"\n                    f\"{arrow}\\n\"\n                    f\"{e.msg}\"\n                )\n            else:\n                error_msg = str(e)\n\n            error_msg += traceback.format_exc()\n\n            raise SyntaxError(error_msg) from e\n\n    async def run_cell(self, code: str, live_output: bool = True) -&gt; Any:\n        \"\"\"Async version of run_cell that handles both sync and async code\"\"\"\n        result = None\n        error = None\n        tb = None\n        original_dir = os.getcwd()\n\n        if live_output:\n            stdout_buffer = io.StringIO()\n            stderr_buffer = io.StringIO()\n            stdout = TeeStream(sys.__stdout__, stdout_buffer)\n            stderr = TeeStream(sys.__stderr__, stderr_buffer)\n        else:\n            stdout = io.StringIO()\n            stderr = io.StringIO()\n\n        try:\n            # Check if a file is already specified\n            original_file = self.user_ns.get('__file__')\n            if original_file is None:\n                # Create temp file if no file specified\n                temp_file = self.vfs.write_file(\n                    f'src/temp/_temp_{self._execution_count}.py',\n                    code\n                )\n                # work_ns = self.user_ns.copy()\n                self.user_ns['__file__'] = str(temp_file)\n            else:\n                # Use existing file\n                temp_file = Path(original_file)\n                # Write code to the existing file\n                self.vfs.write_file(temp_file, code)\n                #work_ns = self.user_ns.copy()\n\n            self.user_ns['__builtins__'] = __builtins__\n            with VirtualEnvContext(self._venv_path) as python_exec:\n                try:\n                    exec_code, eval_code, is_async, has_top_level_await = self._parse_code(\n                        code.encode('utf-8', errors='replace').decode('utf-8')\n                    )\n                    if exec_code is None:\n                        return \"No executable code\"\n                    os.makedirs(str(temp_file.parent.absolute()), exist_ok=True)\n                    os.chdir(str(temp_file.parent.absolute()))\n                    self.user_ns['PYTHON_EXEC'] = python_exec\n\n                    with redirect_stdout(stdout), redirect_stderr(stderr):\n                        if has_top_level_await:\n                            try:\n                                # Execute wrapped code and await it\n                                exec(exec_code, self.user_ns)\n                                result = self.user_ns['__wrapper']()\n                                if asyncio.iscoroutine(result):\n                                    result = await result\n                            finally:\n                                self.user_ns.pop('__wrapper', None)\n                        elif is_async:\n                            # Execute async code\n                            exec(exec_code, self.user_ns)\n                            if eval_code:\n                                result = eval(eval_code, self.user_ns)\n                                if asyncio.iscoroutine(result):\n                                    result = await result\n                        else:\n                            # Execute sync code\n                            exec(exec_code, self.user_ns)\n                            if eval_code:\n                                result = eval(eval_code, self.user_ns)\n\n                        if result is not None:\n                            self.user_ns['_'] = result\n                except KeyboardInterrupt:\n                    print(\"Stop execution manuel!\")\n\n                except Exception as e:\n                    error = str(e)\n                    tb = traceback.format_exc()\n                    if live_output:\n                        sys.__stderr__.write(f\"{error}\\n{tb}\")\n                    stderr.write(f\"{error}\\n{tb}\")\n\n                finally:\n                    os.chdir(original_dir)\n                    self._execution_count += 1\n                    # self.user_ns = work_ns.copy()\n                    if live_output:\n                        stdout_value = stdout_buffer.getvalue()\n                        stderr_value = stderr_buffer.getvalue()\n                    else:\n                        stdout_value = stdout.getvalue()\n                        stderr_value = stderr.getvalue()\n\n                    output = {\n                        'code': code,\n                        'stdout': stdout_value,\n                        'stderr': stderr_value,\n                        'result': result if result else \"stdout\"\n                    }\n                    self.output_history[self._execution_count] = output\n\n                    if not result:\n                        result = \"\"\n                    if output['stdout']:\n                        result = f\"{result}\\nstdout:{output['stdout']}\"\n                    if output['stderr']:\n                        result = f\"{result}\\nstderr:{output['stderr']}\"\n\n                    if self.auto_remove and original_file is None:\n                        # Only remove temp files, not user-specified files\n                        self.vfs.delete_file(temp_file)\n\n                    return result\n\n        except Exception as e:\n            error_msg = f\"Error executing code: {str(e)}\\n{traceback.format_exc()}\"\n            if live_output:\n                sys.__stderr__.write(error_msg)\n            return error_msg\n\n    async def modify_code(self, code: str = None, object_name: str = None, file: str = None) -&gt; str:\n        '''\n        Modify existing code in memory (user namespace) and optionally in the corresponding file.\n\n        This method updates variables, functions, or methods in the current Python session and can\n        also update the corresponding source file if specified.\n\n        Args:\n            code: New value or implementation for the object\n            object_name: Name of the object to modify (variable, function, or method)\n            file: Path to the file to update (if None, only updates in memory)\n\n        Returns:\n            String describing the modification result\n\n        Examples:\n\n        # 1. Update a variable in memory\n        await ipython.modify_code(code=\"5\", object_name=\"x\")\n\n    # 2. Change a method implementation\n    await ipython.modify_code(\n        code='\"\"\"def sound(self):\\n        return \"Woof\"\"\"\"',\n        object_name=\"Dog.sound\"\n    )\n\n    # 3. Modify a function\n    await ipython.modify_code(\n        code='\"\"\"def calculate_age():\\n    return 25\"\"\"',\n        object_name=\"calculate_age\"\n    )\n\n    # 4. Update variable in memory and file\n    await ipython.modify_code(\n        code=\"100\",\n        object_name=\"MAX_SIZE\",\n        file=\"config.py\"\n    )\n\n    # 5. Modifying an attribute in __init__\n    await ipython.modify_code(\n        code='\"\"\"def __init__(self):\\n        self.name = \"Buddy\"\"\"\"',\n        object_name=\"Dog.__init__\"\n    )\n        '''\n        try:\n            if not object_name:\n                raise ValueError(\"Object name must be specified\")\n            if code is None:\n                raise ValueError(\"New code or value must be provided\")\n\n            # Process object name (handle methods with parentheses)\n            clean_object_name = object_name.replace(\"()\", \"\")\n\n            # Step 1: Update in memory (user namespace)\n            result_message = []\n\n            # Handle different types of objects\n            if \".\" in clean_object_name:\n                # For methods or class attributes\n                parts = clean_object_name.split(\".\")\n                base_obj_name = parts[0]\n                attr_name = parts[1]\n\n                if base_obj_name not in self.user_ns:\n                    raise ValueError(f\"Object '{base_obj_name}' not found in namespace\")\n\n                base_obj = self.user_ns[base_obj_name]\n\n                # Handle method definitions which are passed as docstrings\n                if code.split('\\n'):\n                    method_code = code\n\n                    # Parse the method code to extract its body\n                    method_ast = ast.parse(method_code).body[0]\n                    method_name = method_ast.name\n\n                    # Create a new function object from the code\n                    method_locals = {}\n                    exec(\n                        f\"def _temp_func{signature(getattr(base_obj.__class__, attr_name, None))}: {method_ast.body[0].value.s}\",\n                        globals(), method_locals)\n                    new_method = method_locals['_temp_func']\n\n                    # Set the method on the class\n                    setattr(base_obj.__class__, attr_name, new_method)\n                    result_message.append(f\"Updated method '{clean_object_name}' in memory\")\n                else:\n                    # For simple attributes\n                    setattr(base_obj, attr_name, eval(code, self.user_ns))\n                    result_message.append(f\"Updated attribute '{clean_object_name}' in memory\")\n            else:\n                # For variables and functions\n                if code.startswith('\"\"\"') and code.endswith('\"\"\"'):\n                    # Handle function definitions\n                    func_code = code.strip('\"\"\"')\n                    func_ast = ast.parse(func_code).body[0]\n                    func_name = func_ast.name\n\n                    # Create a new function object from the code\n                    func_locals = {}\n                    exec(f\"{func_code}\", globals(), func_locals)\n                    self.user_ns[clean_object_name] = func_locals[func_name]\n                    result_message.append(f\"Updated function '{clean_object_name}' in memory\")\n                else:\n                    # Simple variable assignment\n                    self.user_ns[clean_object_name] = eval(code, self.user_ns)\n                    result_message.append(f\"Updated variable '{clean_object_name}' in memory\")\n\n            # Step 2: Update in file if specified\n            if file is not None:\n                file_path = self.vfs._resolve_path(file)\n\n                if not file_path.exists():\n                    self.user_ns['__file__'] = str(file_path)\n                    return await self.run_cell(code)\n\n                # Read original content\n                original_content = self.vfs.read_file(file_path)\n                updated_content = original_content\n\n                # Handle different object types for file updates\n                if \".\" in clean_object_name:\n                    # For methods\n                    parts = clean_object_name.split(\".\")\n                    class_name = parts[0]\n                    method_name = parts[1]\n\n                    if code.startswith('\"\"\"') and code.endswith('\"\"\"'):\n                        method_code = code.strip('\"\"\"')\n\n                        # Use ast to parse the file and find the method to replace\n                        file_ast = ast.parse(original_content)\n                        for node in ast.walk(file_ast):\n                            if isinstance(node, ast.ClassDef) and node.name == class_name:\n                                for method in node.body:\n                                    if isinstance(method, ast.FunctionDef) and method.name == method_name:\n                                        # Find the method in the source code\n                                        method_pattern = fr\"def {method_name}.*?:(.*?)(?=\\n    \\w|\\n\\w|\\Z)\"\n                                        method_match = re.search(method_pattern, original_content, re.DOTALL)\n\n                                        if method_match:\n                                            indentation = re.match(r\"^(\\s*)\", method_match.group(0)).group(1)\n                                            method_indented = textwrap.indent(method_code, indentation)\n                                            updated_content = original_content.replace(\n                                                method_match.group(0),\n                                                method_indented\n                                            )\n                                            self.vfs.write_file(file_path, updated_content)\n                                            result_message.append(\n                                                f\"Updated method '{clean_object_name}' in file '{file}'\")\n                else:\n                    # For variables and functions\n                    if code.startswith('\"\"\"') and code.endswith('\"\"\"'):\n                        # Handle function updates\n                        func_code = code.strip('\"\"\"')\n                        func_pattern = fr\"def {clean_object_name}.*?:(.*?)(?=\\n\\w|\\Z)\"\n                        func_match = re.search(func_pattern, original_content, re.DOTALL)\n\n                        if func_match:\n                            indentation = re.match(r\"^(\\s*)\", func_match.group(0)).group(1)\n                            func_indented = textwrap.indent(func_code, indentation)\n                            updated_content = original_content.replace(\n                                func_match.group(0),\n                                func_indented\n                            )\n                            self.vfs.write_file(file_path, updated_content)\n                            result_message.append(f\"Updated function '{clean_object_name}' in file '{file}'\")\n                    else:\n                        # Handle variable updates\n                        var_pattern = fr\"{clean_object_name}\\s*=.*\"\n                        var_replacement = f\"{clean_object_name} = {code}\"\n                        updated_content = re.sub(var_pattern, var_replacement, original_content)\n\n                        if updated_content != original_content:\n                            self.vfs.write_file(file_path, updated_content)\n                            result_message.append(f\"Updated variable '{clean_object_name}' in file '{file}'\")\n                        else:\n                            result_message.append(f\"Could not find variable '{clean_object_name}' in file '{file}'\")\n\n            return \"\\n\".join(result_message)\n\n        except Exception as e:\n            return f\"Error during code modification: {str(e)}\\n{traceback.format_exc()}\"\n\n\n    def save_session(self, name: str):\n        \"\"\"Save session with UTF-8 encoding\"\"\"\n        session_file = self._session_dir / f\"{name}.pkl\"\n        user_ns = self.user_ns.copy()\n        output_history = self.output_history.copy()\n\n        # Ensure all strings are properly encoded\n        for key, value in user_ns.items():\n            try:\n                if isinstance(value, str):\n                    value = value.encode('utf-8').decode('utf-8')\n                pickle.dumps(value)\n            except Exception:\n                user_ns[key] = f\"not serializable: {str(value)}\"\n\n        for key, value in output_history.items():\n            try:\n                if isinstance(value, dict):\n                    for k, v in value.items():\n                        if isinstance(v, str):\n                            value[k] = v.encode('utf-8').decode('utf-8')\n                pickle.dumps(value)\n            except Exception:\n                output_history[key] = f\"not serializable: {str(value)}\"\n\n\n        session_data = {\n            'user_ns': user_ns,\n            'output_history': output_history,\n\n        }\n\n        with open(session_file, 'wb') as f:\n            pickle.dump(session_data, f)\n\n        # Save VFS state with UTF-8 encoding\n        vfs_state_file = self._session_dir / f\"{name}_vfs.json\"\n        with open(vfs_state_file, 'w', encoding='utf-8') as f:\n            json.dump(self.vfs.virtual_files, f, ensure_ascii=False)\n\n    def load_session(self, name: str):\n        \"\"\"Load session with UTF-8 encoding\"\"\"\n        session_file = self._session_dir / f\"{name}.pkl\"\n        if session_file.exists():\n            with open(session_file, 'rb') as f:\n                session_data = pickle.load(f)\n                # self.user_ns.update(session_data['user_ns'])\n                self.output_history.update(session_data['output_history'])\n\n        # Load VFS state with UTF-8 encoding\n        vfs_state_file = self._session_dir / f\"{name}_vfs.json\"\n        if vfs_state_file.exists():\n            with open(vfs_state_file, encoding='utf-8') as f:\n                self.vfs.virtual_files = json.load(f)\n\n    def __str__(self):\n        \"\"\"String representation of current session\"\"\"\n        output = []\n        for count, data in self.output_history.items():\n            output.append(f\"In [{count}]: {data['code']}\")\n            if data['stdout']:\n                output.append(data['stdout'])\n            if data['stderr']:\n                output.append(f\"Error: {data['stderr']}\")\n            if data['result'] is not None:\n                output.append(f\"Out[{count}]: {data['result']}\")\n        return \"\\n\".join(output)\n</code></pre> <code>__str__()</code> \u00b6 <p>String representation of current session</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __str__(self):\n    \"\"\"String representation of current session\"\"\"\n    output = []\n    for count, data in self.output_history.items():\n        output.append(f\"In [{count}]: {data['code']}\")\n        if data['stdout']:\n            output.append(data['stdout'])\n        if data['stderr']:\n            output.append(f\"Error: {data['stderr']}\")\n        if data['result'] is not None:\n            output.append(f\"Out[{count}]: {data['result']}\")\n    return \"\\n\".join(output)\n</code></pre> <code>get_namespace()</code> \u00b6 <p>Get current namespace</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def get_namespace(self) -&gt; dict[str, Any]:\n    \"\"\"Get current namespace\"\"\"\n    return self.user_ns.copy()\n</code></pre> <code>load_session(name)</code> \u00b6 <p>Load session with UTF-8 encoding</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def load_session(self, name: str):\n    \"\"\"Load session with UTF-8 encoding\"\"\"\n    session_file = self._session_dir / f\"{name}.pkl\"\n    if session_file.exists():\n        with open(session_file, 'rb') as f:\n            session_data = pickle.load(f)\n            # self.user_ns.update(session_data['user_ns'])\n            self.output_history.update(session_data['output_history'])\n\n    # Load VFS state with UTF-8 encoding\n    vfs_state_file = self._session_dir / f\"{name}_vfs.json\"\n    if vfs_state_file.exists():\n        with open(vfs_state_file, encoding='utf-8') as f:\n            self.vfs.virtual_files = json.load(f)\n</code></pre> <code>modify_code(code=None, object_name=None, file=None)</code> <code>async</code> \u00b6 <pre><code>Modify existing code in memory (user namespace) and optionally in the corresponding file.\n\nThis method updates variables, functions, or methods in the current Python session and can\nalso update the corresponding source file if specified.\n\nArgs:\n    code: New value or implementation for the object\n    object_name: Name of the object to modify (variable, function, or method)\n    file: Path to the file to update (if None, only updates in memory)\n\nReturns:\n    String describing the modification result\n\nExamples:\n\n# 1. Update a variable in memory\nawait ipython.modify_code(code=\"5\", object_name=\"x\")\n</code></pre> <code>reset()</code> \u00b6 <p>Reset the interpreter state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def reset(self):\n    \"\"\"Reset the interpreter state\"\"\"\n    self.user_ns = {\n        '__name__': '__main__',\n        '__builtins__': __builtins__,\n        'toolboxv2': toolboxv2,\n        '__file__': None,\n        '__path__': [str(self.vfs.current_dir)],\n        'auto_install': auto_install,\n        'modify_code': self.modify_code,\n    }\n    self.output_history.clear()\n    self._execution_count = 0\n    if self.auto_remove:\n        shutil.rmtree(self.vfs.base_dir, ignore_errors=True)\n</code></pre> <code>run_cell(code, live_output=True)</code> <code>async</code> \u00b6 <p>Async version of run_cell that handles both sync and async code</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def run_cell(self, code: str, live_output: bool = True) -&gt; Any:\n    \"\"\"Async version of run_cell that handles both sync and async code\"\"\"\n    result = None\n    error = None\n    tb = None\n    original_dir = os.getcwd()\n\n    if live_output:\n        stdout_buffer = io.StringIO()\n        stderr_buffer = io.StringIO()\n        stdout = TeeStream(sys.__stdout__, stdout_buffer)\n        stderr = TeeStream(sys.__stderr__, stderr_buffer)\n    else:\n        stdout = io.StringIO()\n        stderr = io.StringIO()\n\n    try:\n        # Check if a file is already specified\n        original_file = self.user_ns.get('__file__')\n        if original_file is None:\n            # Create temp file if no file specified\n            temp_file = self.vfs.write_file(\n                f'src/temp/_temp_{self._execution_count}.py',\n                code\n            )\n            # work_ns = self.user_ns.copy()\n            self.user_ns['__file__'] = str(temp_file)\n        else:\n            # Use existing file\n            temp_file = Path(original_file)\n            # Write code to the existing file\n            self.vfs.write_file(temp_file, code)\n            #work_ns = self.user_ns.copy()\n\n        self.user_ns['__builtins__'] = __builtins__\n        with VirtualEnvContext(self._venv_path) as python_exec:\n            try:\n                exec_code, eval_code, is_async, has_top_level_await = self._parse_code(\n                    code.encode('utf-8', errors='replace').decode('utf-8')\n                )\n                if exec_code is None:\n                    return \"No executable code\"\n                os.makedirs(str(temp_file.parent.absolute()), exist_ok=True)\n                os.chdir(str(temp_file.parent.absolute()))\n                self.user_ns['PYTHON_EXEC'] = python_exec\n\n                with redirect_stdout(stdout), redirect_stderr(stderr):\n                    if has_top_level_await:\n                        try:\n                            # Execute wrapped code and await it\n                            exec(exec_code, self.user_ns)\n                            result = self.user_ns['__wrapper']()\n                            if asyncio.iscoroutine(result):\n                                result = await result\n                        finally:\n                            self.user_ns.pop('__wrapper', None)\n                    elif is_async:\n                        # Execute async code\n                        exec(exec_code, self.user_ns)\n                        if eval_code:\n                            result = eval(eval_code, self.user_ns)\n                            if asyncio.iscoroutine(result):\n                                result = await result\n                    else:\n                        # Execute sync code\n                        exec(exec_code, self.user_ns)\n                        if eval_code:\n                            result = eval(eval_code, self.user_ns)\n\n                    if result is not None:\n                        self.user_ns['_'] = result\n            except KeyboardInterrupt:\n                print(\"Stop execution manuel!\")\n\n            except Exception as e:\n                error = str(e)\n                tb = traceback.format_exc()\n                if live_output:\n                    sys.__stderr__.write(f\"{error}\\n{tb}\")\n                stderr.write(f\"{error}\\n{tb}\")\n\n            finally:\n                os.chdir(original_dir)\n                self._execution_count += 1\n                # self.user_ns = work_ns.copy()\n                if live_output:\n                    stdout_value = stdout_buffer.getvalue()\n                    stderr_value = stderr_buffer.getvalue()\n                else:\n                    stdout_value = stdout.getvalue()\n                    stderr_value = stderr.getvalue()\n\n                output = {\n                    'code': code,\n                    'stdout': stdout_value,\n                    'stderr': stderr_value,\n                    'result': result if result else \"stdout\"\n                }\n                self.output_history[self._execution_count] = output\n\n                if not result:\n                    result = \"\"\n                if output['stdout']:\n                    result = f\"{result}\\nstdout:{output['stdout']}\"\n                if output['stderr']:\n                    result = f\"{result}\\nstderr:{output['stderr']}\"\n\n                if self.auto_remove and original_file is None:\n                    # Only remove temp files, not user-specified files\n                    self.vfs.delete_file(temp_file)\n\n                return result\n\n    except Exception as e:\n        error_msg = f\"Error executing code: {str(e)}\\n{traceback.format_exc()}\"\n        if live_output:\n            sys.__stderr__.write(error_msg)\n        return error_msg\n</code></pre> <code>save_session(name)</code> \u00b6 <p>Save session with UTF-8 encoding</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def save_session(self, name: str):\n    \"\"\"Save session with UTF-8 encoding\"\"\"\n    session_file = self._session_dir / f\"{name}.pkl\"\n    user_ns = self.user_ns.copy()\n    output_history = self.output_history.copy()\n\n    # Ensure all strings are properly encoded\n    for key, value in user_ns.items():\n        try:\n            if isinstance(value, str):\n                value = value.encode('utf-8').decode('utf-8')\n            pickle.dumps(value)\n        except Exception:\n            user_ns[key] = f\"not serializable: {str(value)}\"\n\n    for key, value in output_history.items():\n        try:\n            if isinstance(value, dict):\n                for k, v in value.items():\n                    if isinstance(v, str):\n                        value[k] = v.encode('utf-8').decode('utf-8')\n            pickle.dumps(value)\n        except Exception:\n            output_history[key] = f\"not serializable: {str(value)}\"\n\n\n    session_data = {\n        'user_ns': user_ns,\n        'output_history': output_history,\n\n    }\n\n    with open(session_file, 'wb') as f:\n        pickle.dump(session_data, f)\n\n    # Save VFS state with UTF-8 encoding\n    vfs_state_file = self._session_dir / f\"{name}_vfs.json\"\n    with open(vfs_state_file, 'w', encoding='utf-8') as f:\n        json.dump(self.vfs.virtual_files, f, ensure_ascii=False)\n</code></pre> <code>update_namespace(variables)</code> \u00b6 <p>Update namespace with new variables</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def update_namespace(self, variables: dict[str, Any]):\n    \"\"\"Update namespace with new variables\"\"\"\n    self.user_ns.update(variables)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.MockIPython.modify_code--2-change-a-method-implementation","title":"2. Change a method implementation","text":"<p>await ipython.modify_code(     code='\"\"\"def sound(self):     return \"Woof\"\"\"\"',     object_name=\"Dog.sound\" )</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.MockIPython.modify_code--3-modify-a-function","title":"3. Modify a function","text":"<p>await ipython.modify_code(     code='\"\"\"def calculate_age(): return 25\"\"\"',     object_name=\"calculate_age\" )</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.MockIPython.modify_code--4-update-variable-in-memory-and-file","title":"4. Update variable in memory and file","text":"<p>await ipython.modify_code(     code=\"100\",     object_name=\"MAX_SIZE\",     file=\"config.py\" )</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.MockIPython.modify_code--5-modifying-an-attribute-in-init","title":"5. Modifying an attribute in init","text":"<p>await ipython.modify_code(     code='\"\"\"def init(self):     self.name = \"Buddy\"\"\"\"',     object_name=\"Dog.init\" )</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def modify_code(self, code: str = None, object_name: str = None, file: str = None) -&gt; str:\n    '''\n    Modify existing code in memory (user namespace) and optionally in the corresponding file.\n\n    This method updates variables, functions, or methods in the current Python session and can\n    also update the corresponding source file if specified.\n\n    Args:\n        code: New value or implementation for the object\n        object_name: Name of the object to modify (variable, function, or method)\n        file: Path to the file to update (if None, only updates in memory)\n\n    Returns:\n        String describing the modification result\n\n    Examples:\n\n    # 1. Update a variable in memory\n    await ipython.modify_code(code=\"5\", object_name=\"x\")\n\n# 2. Change a method implementation\nawait ipython.modify_code(\n    code='\"\"\"def sound(self):\\n        return \"Woof\"\"\"\"',\n    object_name=\"Dog.sound\"\n)\n\n# 3. Modify a function\nawait ipython.modify_code(\n    code='\"\"\"def calculate_age():\\n    return 25\"\"\"',\n    object_name=\"calculate_age\"\n)\n\n# 4. Update variable in memory and file\nawait ipython.modify_code(\n    code=\"100\",\n    object_name=\"MAX_SIZE\",\n    file=\"config.py\"\n)\n\n# 5. Modifying an attribute in __init__\nawait ipython.modify_code(\n    code='\"\"\"def __init__(self):\\n        self.name = \"Buddy\"\"\"\"',\n    object_name=\"Dog.__init__\"\n)\n    '''\n    try:\n        if not object_name:\n            raise ValueError(\"Object name must be specified\")\n        if code is None:\n            raise ValueError(\"New code or value must be provided\")\n\n        # Process object name (handle methods with parentheses)\n        clean_object_name = object_name.replace(\"()\", \"\")\n\n        # Step 1: Update in memory (user namespace)\n        result_message = []\n\n        # Handle different types of objects\n        if \".\" in clean_object_name:\n            # For methods or class attributes\n            parts = clean_object_name.split(\".\")\n            base_obj_name = parts[0]\n            attr_name = parts[1]\n\n            if base_obj_name not in self.user_ns:\n                raise ValueError(f\"Object '{base_obj_name}' not found in namespace\")\n\n            base_obj = self.user_ns[base_obj_name]\n\n            # Handle method definitions which are passed as docstrings\n            if code.split('\\n'):\n                method_code = code\n\n                # Parse the method code to extract its body\n                method_ast = ast.parse(method_code).body[0]\n                method_name = method_ast.name\n\n                # Create a new function object from the code\n                method_locals = {}\n                exec(\n                    f\"def _temp_func{signature(getattr(base_obj.__class__, attr_name, None))}: {method_ast.body[0].value.s}\",\n                    globals(), method_locals)\n                new_method = method_locals['_temp_func']\n\n                # Set the method on the class\n                setattr(base_obj.__class__, attr_name, new_method)\n                result_message.append(f\"Updated method '{clean_object_name}' in memory\")\n            else:\n                # For simple attributes\n                setattr(base_obj, attr_name, eval(code, self.user_ns))\n                result_message.append(f\"Updated attribute '{clean_object_name}' in memory\")\n        else:\n            # For variables and functions\n            if code.startswith('\"\"\"') and code.endswith('\"\"\"'):\n                # Handle function definitions\n                func_code = code.strip('\"\"\"')\n                func_ast = ast.parse(func_code).body[0]\n                func_name = func_ast.name\n\n                # Create a new function object from the code\n                func_locals = {}\n                exec(f\"{func_code}\", globals(), func_locals)\n                self.user_ns[clean_object_name] = func_locals[func_name]\n                result_message.append(f\"Updated function '{clean_object_name}' in memory\")\n            else:\n                # Simple variable assignment\n                self.user_ns[clean_object_name] = eval(code, self.user_ns)\n                result_message.append(f\"Updated variable '{clean_object_name}' in memory\")\n\n        # Step 2: Update in file if specified\n        if file is not None:\n            file_path = self.vfs._resolve_path(file)\n\n            if not file_path.exists():\n                self.user_ns['__file__'] = str(file_path)\n                return await self.run_cell(code)\n\n            # Read original content\n            original_content = self.vfs.read_file(file_path)\n            updated_content = original_content\n\n            # Handle different object types for file updates\n            if \".\" in clean_object_name:\n                # For methods\n                parts = clean_object_name.split(\".\")\n                class_name = parts[0]\n                method_name = parts[1]\n\n                if code.startswith('\"\"\"') and code.endswith('\"\"\"'):\n                    method_code = code.strip('\"\"\"')\n\n                    # Use ast to parse the file and find the method to replace\n                    file_ast = ast.parse(original_content)\n                    for node in ast.walk(file_ast):\n                        if isinstance(node, ast.ClassDef) and node.name == class_name:\n                            for method in node.body:\n                                if isinstance(method, ast.FunctionDef) and method.name == method_name:\n                                    # Find the method in the source code\n                                    method_pattern = fr\"def {method_name}.*?:(.*?)(?=\\n    \\w|\\n\\w|\\Z)\"\n                                    method_match = re.search(method_pattern, original_content, re.DOTALL)\n\n                                    if method_match:\n                                        indentation = re.match(r\"^(\\s*)\", method_match.group(0)).group(1)\n                                        method_indented = textwrap.indent(method_code, indentation)\n                                        updated_content = original_content.replace(\n                                            method_match.group(0),\n                                            method_indented\n                                        )\n                                        self.vfs.write_file(file_path, updated_content)\n                                        result_message.append(\n                                            f\"Updated method '{clean_object_name}' in file '{file}'\")\n            else:\n                # For variables and functions\n                if code.startswith('\"\"\"') and code.endswith('\"\"\"'):\n                    # Handle function updates\n                    func_code = code.strip('\"\"\"')\n                    func_pattern = fr\"def {clean_object_name}.*?:(.*?)(?=\\n\\w|\\Z)\"\n                    func_match = re.search(func_pattern, original_content, re.DOTALL)\n\n                    if func_match:\n                        indentation = re.match(r\"^(\\s*)\", func_match.group(0)).group(1)\n                        func_indented = textwrap.indent(func_code, indentation)\n                        updated_content = original_content.replace(\n                            func_match.group(0),\n                            func_indented\n                        )\n                        self.vfs.write_file(file_path, updated_content)\n                        result_message.append(f\"Updated function '{clean_object_name}' in file '{file}'\")\n                else:\n                    # Handle variable updates\n                    var_pattern = fr\"{clean_object_name}\\s*=.*\"\n                    var_replacement = f\"{clean_object_name} = {code}\"\n                    updated_content = re.sub(var_pattern, var_replacement, original_content)\n\n                    if updated_content != original_content:\n                        self.vfs.write_file(file_path, updated_content)\n                        result_message.append(f\"Updated variable '{clean_object_name}' in file '{file}'\")\n                    else:\n                        result_message.append(f\"Could not find variable '{clean_object_name}' in file '{file}'\")\n\n        return \"\\n\".join(result_message)\n\n    except Exception as e:\n        return f\"Error during code modification: {str(e)}\\n{traceback.format_exc()}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.ParentNodeTransformer","title":"<code>ParentNodeTransformer</code>","text":"<p>               Bases: <code>NodeTransformer</code></p> <p>Add parent references to AST nodes</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class ParentNodeTransformer(ast.NodeTransformer):\n    \"\"\"Add parent references to AST nodes\"\"\"\n    def visit(self, node):\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n        return super().visit(node)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.Pipeline","title":"<code>Pipeline</code>","text":"<p>A pipeline for executing AI agent-driven tasks with interactive code execution and variable management.</p> <p>The Pipeline class provides a structured environment for AI agents to: 1. Execute code in a controlled environment 2. Manage and track variables 3. Update methods dynamically 4. Save and load session states 5. Generate detailed variable descriptions</p> <p>Attributes:</p> Name Type Description <code>agent</code> <p>The AI agent instance used for task execution</p> <code>task</code> <code>str</code> <p>The task to be performed</p> <code>mas_iter</code> <code>int</code> <p>Maximum number of iterations allowed (default: 12)</p> <code>variables</code> <code>Dict[str, Any]</code> <p>Dictionary of variables available to the pipeline</p> <code>top_n</code> <code>Optional[int]</code> <p>Limit variable descriptions to top N most used</p> <code>execution_history</code> <code>List[ExecutionRecord]</code> <p>History of executed code and results</p> <code>session_name</code> <code>Optional[str]</code> <p>Name of the current session if saved</p> <code>ipython</code> <p>IPython or MockIPython instance for code execution</p> Example <p>agent = get_free_agent(\"demo\", \"anthropic/claude-3-haiku-20240307\") pipeline = Pipeline( ...     agent=agent, ...     task=\"Calculate fibonacci sequence\", ...     variables={\"n\": 10} ... ) result = pipeline.run(\"...\") print(result.result)</p> Notes <ul> <li>The pipeline uses either IPython if available or a MockIPython implementation</li> <li>Variables can be provided as either a dictionary or list</li> <li>Session state can be saved and loaded</li> <li>Method updates are handled through a structured BaseModel approach</li> </ul> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class Pipeline:\n    \"\"\"\n        A pipeline for executing AI agent-driven tasks with interactive code execution and variable management.\n\n        The Pipeline class provides a structured environment for AI agents to:\n        1. Execute code in a controlled environment\n        2. Manage and track variables\n        3. Update methods dynamically\n        4. Save and load session states\n        5. Generate detailed variable descriptions\n\n        Attributes:\n            agent: The AI agent instance used for task execution\n            task (str): The task to be performed\n            mas_iter (int): Maximum number of iterations allowed (default: 12)\n            variables (Dict[str, Any]): Dictionary of variables available to the pipeline\n            top_n (Optional[int]): Limit variable descriptions to top N most used\n            execution_history (List[ExecutionRecord]): History of executed code and results\n            session_name (Optional[str]): Name of the current session if saved\n            ipython: IPython or MockIPython instance for code execution\n\n        Example:\n            &gt;&gt;&gt; agent = get_free_agent(\"demo\", \"anthropic/claude-3-haiku-20240307\")\n            &gt;&gt;&gt; pipeline = Pipeline(\n            ...     agent=agent,\n            ...     task=\"Calculate fibonacci sequence\",\n            ...     variables={\"n\": 10}\n            ... )\n            &gt;&gt;&gt; result = pipeline.run(\"...\")\n            &gt;&gt;&gt; print(result.result)\n\n        Notes:\n            - The pipeline uses either IPython if available or a MockIPython implementation\n            - Variables can be provided as either a dictionary or list\n            - Session state can be saved and loaded\n            - Method updates are handled through a structured BaseModel approach\n        \"\"\"\n    def __init__(\n        self,\n        agent: Any,\n        verbose: bool=False,\n        max_iter: int= 12,\n        variables: dict[str, Any] | list[Any] | None = None,\n        top_n: bool | None = None,\n        restore: bool | None = None,\n        max_think_after_think = None,\n        print_f=None,\n        web_js=False,\n        timeout_timer=25,\n        v_agent=None,\n        web_llm=None,\n    ):\n        \"\"\"\n        Initialize the Pipeline.\n\n        Args:\n            agent: AI agent instance to use for task execution\n            verbose: print internal results\n            max_iter: Maximum number of iterations (default: 12)\n            variables: Dictionary or list of variables to make available\n            top_n: Limit variable descriptions to top N most used\n            web_js: if the agent is allow to use the web\n        \"\"\"\n\n        self.timeout_timer = timeout_timer\n        self.top_n = top_n\n        self.max_iter = max_iter\n        self.max_think_after_think = max_think_after_think or max_iter // 2\n        self.agent = agent\n        self.v_agent = v_agent or agent\n        # self.agent.verbose = verbose\n        self.task = None\n        self.web_js = web_js\n        self.print_f = print_f\n        self.verbose_output = EnhancedVerboseOutput(verbose=verbose, print_f=self.print_f)\n        self.variables = self._process_variables(variables or {})\n        self.variables['auto_install'] = auto_install\n        self.execution_history = []\n        self.session_name = None\n\n        self.browser_session: BrowserWrapper | None = BrowserWrapper(llm=web_llm or agent.amd.model)\n        self.js_history: list[JSExecutionRecord] = []\n\n        self._session_dir = Path(get_app().appdata) / 'ChatSession' / agent.amd.name\n        self.ipython = MockIPython(self._session_dir, auto_remove=False)\n        self.chat_session = ChatSession(get_app().get_mod(\"isaa\").get_memory(), space_name=f\"ChatSession/{agent.amd.name}/Pipeline.session\", max_length=max_iter)\n        self.process_memory = ChatSession(get_app().get_mod(\"isaa\").get_memory(), space_name=f\"ChatSession/{agent.amd.name}/Process.session\", max_length=max_iter)\n\n        # Initialize interpreter with variables\n        self.init_keys = list(self.ipython.user_ns.keys()).copy()\n        if self.web_js:\n            self.variables['web_actions'] = self.browser_session.run\n            self.variables['browser_session'] = self.browser_session\n        self.ipython.user_ns.update(self.variables)\n\n        self.restore_var = restore\n\n        if restore:\n            self.restore()\n\n    def on_exit(self):\n        self.chat_session.on_exit()\n        self.process_memory.on_exit()\n        self.save_session(f\"Pipeline_Session_{self.agent.amd.name}\")\n\n    def restore(self):\n        self.load_session(f\"Pipeline_Session_{self.agent.amd.name}\")\n\n    def save_session(self, name: str):\n        \"\"\"Save current session\"\"\"\n        self.session_name = name\n        self.ipython.save_session(name)\n\n    def load_session(self, name: str):\n        \"\"\"Load saved session\"\"\"\n        self.ipython.load_session(name)\n        self.variables.update(self.ipython.user_ns)\n\n\n    def show_graph_html(self, output_file=None, get_output_html=False, get_output_net=False):\n\n        if output_file is None:\n            chat_graph = self.ipython._session_dir / 'chat_graph.html'\n            process_graph = self.ipython._session_dir / 'process_graph.html'\n            output_file = str(chat_graph.absolute())\n            p_output_file = str(process_graph.absolute())\n        else:\n            output_file = output_file + '_chat_graph.html'\n            p_output_file = output_file + '_process_graph.html'\n\n        return (self.chat_session.mem.memories.get(\n            self.chat_session.mem._sanitize_name(\n                self.chat_session.space_name)).vis(output_file=output_file,\n        get_output_html=get_output_html, get_output_net=get_output_net)  ,\n                self.process_memory.mem.memories.get(\n            self.process_memory.mem._sanitize_name(\n                self.process_memory.space_name)).vis(output_file=p_output_file,\n        get_output_html=get_output_html, get_output_net=get_output_net))\n\n    @staticmethod\n    def _process_variables(variables: dict[str, Any] | list[Any]) -&gt; dict[str, Any]:\n        \"\"\"\n        Process variables to generate meaningful names, using actual variable names where possible.\n        Instances get lowercase names based on their class names.\n\n        Args:\n            variables: Dictionary of variables or list of variables to process\n\n        Returns:\n            Dict[str, Any]: Processed variables with meaningful names\n        \"\"\"\n        if isinstance(variables, dict):\n            return variables\n\n        processed = {}\n        name_counts = defaultdict(int)\n\n        # Get caller's frame to find variable names\n        caller_frame = currentframe().f_back\n        caller_locals = {**caller_frame.f_locals, **caller_frame.f_globals}\n\n        def find_var_name(obj: Any) -&gt; str:\n            # Find original variable name if exists\n            var_names = [name for name, val in caller_locals.items()\n                         if val is obj and not name.startswith('_')]\n            if var_names:\n                return var_names[0]\n\n            # Special handling for functions\n            if isfunction(obj) or isclass(obj):\n                return obj.__name__\n            # Handle instances\n            elif hasattr(obj, '__class__'):\n                base_name = obj.__class__.__name__.lower()  # Lowercase for instances\n                count = name_counts[base_name]\n                name_counts[base_name] += 1\n                return f\"{base_name}_{count + 1}\" if count &gt; 0 else base_name\n\n            return type(obj).__name__\n\n        # Process each variable\n        for var in variables:\n            name = find_var_name(var)\n            while name in processed:\n                if name.rpartition('_')[0]:\n                    base, _, num = name.rpartition('_')\n                    try:\n                        num = int(num) + 1\n                        name = f\"{base}_{num}\"\n                    except ValueError:\n                        name = f\"{name}\"\n                else:\n                    name = f\"{name}\"\n\n            processed[name] = var\n\n        return processed\n\n    def _generate_variable_descriptions(\n        self,\n        top_n: int | None = None\n    ) -&gt; str:\n        \"\"\"\n        Generate detailed descriptions of variables, showing args, kwargs, docstrings, and return values.\n\n        Args:\n            top_n: Optional limit to show only top N variables\n\n        Returns:\n            str: Formatted variable descriptions in Markdown\n        \"\"\"\n        if top_n is None:\n            top_n = self.top_n\n\n        def format_value_preview(var: Any) -&gt; str:\n            \"\"\"Format preview of variable contents\"\"\"\n            try:\n                if isinstance(var, int | float | bool | str):\n                    return f\"`{repr(var)}`\"\n                elif isinstance(var, list | tuple | set):\n                    preview = str(list(var)[:3])[:-1] + \", ...]\"\n                    return f\"{len(var)} items: {preview}\"\n                elif isinstance(var, dict):\n                    preview_items = [f\"{repr(k)}: {repr(v)}\" for k, v in list(var.items())[:3]]\n                    return f\"{len(var)} pairs: {{{', '.join(preview_items)}, ...}}\"\n                return f\"&lt;{type(var).__name__}&gt;\"\n            except:\n                return \"&lt;error getting value&gt;\"\n\n        def get_instance_state(var: Any) -&gt; dict[str, Any]:\n            \"\"\"Get current instance state\"\"\"\n            state = {}\n            if hasattr(var, '__dict__'):\n                for name, value in var.__dict__.items():\n                    if not name.startswith('_') and not callable(value):\n                        state[name] = format_value_preview(value)\n            return state\n\n        # Process variables\n        variables = self.variables.items()\n        if top_n:\n            variables = list(variables)[:top_n]\n\n        descriptions = []\n        for name, var in variables:\n            if name in [\"PYTHON_EXEC\", \"__name__\", \"__builtins__\", \"__path__\", \"asyncio\"]:\n                continue\n\n            desc_parts = [f\"### {name}\"]\n\n            # Handle different types\n            if isinstance(var, type):  # Class\n                desc_parts.append(f\"**Type:** `class '{var.__name__}'`\")\n                if var.__doc__:\n                    desc_parts.append(f\"**Documentation:**\\n{var.__doc__.strip()}\")\n\n                # Show methods\n                methods = []\n                for attr_name, attr in var.__dict__.items():\n                    if (not attr_name.startswith('_') or attr_name == \"__init__\") and (isfunction(attr) or ismethod(attr)):\n                        try:\n                            sig = signature(attr)\n                            is_a = asyncio.iscoroutinefunction(var)\n                            methods.append(f\"- `{attr_name}{sig}` Async: `{is_a}\")\n                            if attr.__doc__:\n                                r = attr.__doc__.split('\\n')[0]\n                                methods.append(f\"  {r}\")\n                        except:\n                            methods.append(f\"- `{attr_name}()`\")\n                if methods:\n                    desc_parts.append(\"**Methods:**\\n\" + \"\\n\".join(methods))\n\n            elif isfunction(var) or ismethod(var):  # Function\n                try:\n                    sig = signature(var)\n                    desc_parts.append(f\"**Signature:** `{var.__name__}{sig}`\")\n                    is_a = asyncio.iscoroutinefunction(var)\n                    desc_parts.append(f\"**IS Async:** `{is_a}`\")\n                    if var.__doc__:\n                        desc_parts.append(f\"**Documentation:**\\n{var.__doc__.strip()}\")\n                    ret_anno = sig.return_annotation\n                    if ret_anno != Signature.empty:\n                        desc_parts.append(f\"**Returns:** `{ret_anno}`\")\n                except:\n                    desc_parts.append(f\"**Function:** `{var.__name__}()`\")\n\n            elif isinstance(var, BaseModel):  # Pydantic model\n                desc_parts.append(f\"**Type:** Pydantic model '{var.__class__.__name__}'\")\n                fields = []\n                for field_name, field in var.model_fields.items():\n                    value = getattr(var, field_name, None)\n                    fields.append(f\"- `{field_name}: {field.annotation.__name__}` = {repr(value)}\")\n                if fields:\n                    desc_parts.append(\"**Fields:**\\n\" + \"\\n\".join(fields))\n\n            else:  # Instance\n                class_type = var.__class__\n                desc_parts.append(f\"**Type:** `{class_type.__module__}.{class_type.__name__}`\")\n\n                # Instance initialization details\n                try:\n                    init = class_type.__init__\n                    sig = signature(init)\n                    params = list(sig.parameters.items())[1:]  # Skip self\n                    if params:\n                        args = []\n                        for name, param in params:\n                            if param.default == param.empty:\n                                args.append(name)\n                            else:\n                                args.append(f\"{name}={param.default}\")\n                        desc_parts.append(f\"**Init Args:** `{', '.join(args)}`\")\n                except:\n                    pass\n\n                # Instance state\n                state = get_instance_state(var)\n                if state:\n                    desc_parts.append(\"**Current instance State:**\")\n                    for attr_name, attr_value in state.items():\n                        desc_parts.append(f\"- `{attr_name}` = {attr_value}\")\n\n                # Documentation\n                doc = getdoc(var) or getdoc(class_type)\n                if doc:\n                    desc_parts.append(f\"**Documentation:**\\n{doc.strip()}\")\n\n            descriptions.append(\"\\n\".join(desc_parts))\n\n        return \"\\n\\n\".join(descriptions)\n\n    async def _execute_code(self, code: str, context:dict) -&gt; ExecutionRecord:\n        \"\"\"Execute code and track results\"\"\"\n        lang = context.get('lang', 'py')\n        try:\n\n            if'py' in lang:\n\n                return await self._execute_py(code)\n\n            elif self.web_js and 'js' in lang:\n                return await self._execute_js(code, context)\n\n        except Exception as e:\n            record = ExecutionRecord(code=code, result=None, error=str(e))\n            self.execution_history.append(record)\n            return record\n        record = ExecutionRecord(code=code, result=None, error=f\"Invalid lang {lang} valid is, {'js' if self.web_js else 'py'}]\")\n        self.execution_history.append(record)\n        return record\n\n    async def _execute_py(self, code) -&gt; ExecutionRecord:\n        show = True #len(code) &gt; 450 and code.count('while') &gt; 1 and code.count('print') &gt;= 1\n        result = await self.ipython.run_cell(code, show)\n\n        all_keys = list(self.ipython.user_ns.keys())\n\n        new_keys = [key for key in all_keys if key not in self.init_keys]\n        # Update pipeline variables from IPython namespace\n\n        for var_name in new_keys:\n            if var_name.startswith('_'):\n                continue\n            self.variables[var_name] = self.ipython.user_ns[var_name]\n\n        record = ExecutionRecord(code=code, result=result, error=None)\n        self.execution_history.append(record)\n        return record\n\n    async def _execute_js(self, code: str, context: dict) -&gt; ExecutionRecord:\n        \"\"\"Execute JavaScript code in browser context\"\"\"\n\n        if '&lt;script&gt;' in code:\n            code = code.split('&lt;script&gt;')[1]\n        if '&lt;/script&gt;' in code:\n            code = code.split('&lt;/script&gt;')[0]\n        def _format_error_markdown(error: str) -&gt; str:\n            \"\"\"Format error as Markdown\"\"\"\n            return f\"\"\"\n# Execution Error\n{error}\n\"\"\"\n\n        def _format_result_markdown(result_: dict) -&gt; str:\n            \"\"\"Format execution result as Markdown\"\"\"\n\n            def _clean_html_content(html: str) -&gt; str:\n                \"\"\"Clean HTML content and convert to Markdown-like format\"\"\"\n                soup = BeautifulSoup(html, 'html.parser')\n\n                # Remove scripts and styles\n                for script in soup([\"script\", \"style\"]):\n                    script.decompose()\n\n                # Extract text\n                text = soup.get_text()\n\n                # Clean up whitespace\n                lines = (line.strip() for line in text.splitlines())\n                chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n                text = '\\n'.join(chunk for chunk in chunks if chunk)\n\n                # Add Markdown formatting\n                text = re.sub(r'^(.+)$', r'&gt; \\1', text, flags=re.MULTILINE)\n\n                return text\n\n            md_parts = []\n\n            # Add title\n            md_parts.append(\"# Page Analysis Results\\n\")\n\n            # Format JavaScript result\n            if result_.get('js_result'):\n                md_parts.append(\"## JavaScript Execution Result\")\n                md_parts.append(\"```\")\n                md_parts.append(str(result_['js_result']))\n                md_parts.append(\"```\\n\")\n\n            # Format page state\n            if 'page_state' in result_:\n                md_parts.append(\"## Page Information\")\n                md_parts.append(f\"- **URL**: {result_['page_state']['url']}\")\n                md_parts.append(f\"- **Title**: {result_['page_state']['title']}\\n\")\n\n                # Clean and format content\n                if 'content' in result_['page_state']:\n                    content = _clean_html_content(result_['page_state']['content'])\n                    if content:\n                        md_parts.append(\"### Page Content\")\n                        md_parts.append(content + \"\\n\")\n\n            # Format extracted data\n            if result_.get('extracted_data'):\n                md_parts.append(\"## Extracted Data\")\n                for key, value in result_['extracted_data'].items():\n                    if value:\n                        md_parts.append(f\"### {key.replace('_', ' ').title()}\")\n                        if isinstance(value, list):\n                            for item in value:\n                                md_parts.append(f\"- {item}\")\n                        else:\n                            md_parts.append(str(value))\n                        md_parts.append(\"\")\n\n            return \"\\n\".join(md_parts)\n\n        try:\n            # Prepare execution context\n            url = context.get('url')\n            page = None\n            result = None\n            page_state = {}\n\n            extracted_data = None\n            if url:\n                page = await self.browser_session.navigate(url)\n                parser = self.browser_session.get_parser()\n                markdown = await parser.to_markdown(page)\n\n                if 'patterns' in context:\n                    extracted_data = await parser.to_structured(page, context['patterns'])\n\n                page_state = {\n                    'url': page.url,\n                    'title': await page.title(),\n                    'content': markdown,\n                }\n\n            if code:\n                result = await self.browser_session.execute_js(code, page)\n\n                if isinstance(result, dict) and 'success' in result:\n                    if not result['success']:\n                        raise Exception(f\"JavaScript Error: {result.get('error')}\\nStack: {result.get('stack')}\")\n                    result = result.get('result')\n\n            # Capture page state after execution\n\n\n            # Extract data using patterns if specified\n\n            # Create execution record\n            record = JSExecutionRecord(\n                code=code,\n                result=result,\n                page_state=page_state,\n                extracted_data=extracted_data\n            )\n\n            self.js_history.append(record)\n\n            # Convert to standard ExecutionRecord for pipeline\n            return ExecutionRecord(\n                code=code,\n                result=_format_result_markdown({\n                    'js_result': result,\n                    'page_state': page_state,\n                    'extracted_data': extracted_data\n                }),\n                error=None\n            )\n\n        except Exception as e:\n            error_md = _format_error_markdown(str(e))\n            return ExecutionRecord(code=code, result=None, error=error_md)\n\n\n    def __str__(self):\n        \"\"\"String representation of pipeline session\"\"\"\n        return str(self.ipython)\n\n    async def _process_think_result(self, think_result: ThinkResult, task:str) -&gt; tuple[ThinkState,  ExecutionRecord | str | None]:\n        \"\"\"Process the result of agent thinking\"\"\"\n        if think_result.action == 'brake':\n            return ThinkState.BRAKE, think_result.content\n\n        elif think_result.action == 'update':\n            if think_result.context.get('object_name') is None:\n                return ThinkState.ACTION, \"no object_name specified in context!\"\n            if think_result.context.get('file') is not None:\n                self.ipython.user_ns['__file__'] = think_result.context.get('file')\n            result = await self.verbose_output.process(think_result.action,\n                                                       self.ipython.modify_code(code=think_result.content,\n                                                    object_name=think_result.context.get('object_name'),))\n            return ThinkState.PROCESSING, result\n\n        elif think_result.action == 'code':\n            if think_result.context.get('file') is not None:\n                self.ipython.user_ns['__file__'] = think_result.context.get('file')\n            result = await self._execute_code(think_result.content, think_result.context)\n            return ThinkState.PROCESSING, result\n\n        elif think_result.action == 'done':\n            return ThinkState.DONE, think_result.content\n\n        elif think_result.action == 'infos':\n            infos = await self.chat_session.get_reference(think_result.content, to_str=True)\n            return ThinkState.ACTION, infos\n\n        elif think_result.action == 'guide':\n            details = await self.process_memory.get_reference(think_result.content, to_str=True)\n            plan = await self.agent.a_mini_task(f\"\"\"You are an AI guidance system designed to help determine the next step in a task and provide instructions on how to proceed. Your role is to analyze the given information and offer clear, actionable guidance for the next steps.\n\nFirst, carefully read and understand the main task:\n&lt;main_task&gt;\n{task}\n&lt;/main_task&gt;\n\nNext, review the last thought of the agent, if available:\n&lt;last_thought&gt;\n{think_result.content}\n{think_result.context}\n&lt;/last_thought&gt;\n\nThen, examine the processing history, if provided:\n&lt;processing_history&gt;\n{details}\n&lt;/processing_history&gt;\n\nTo determine the next step and provide guidance, follow these instructions:\n\n1. Analyze the main task, breaking it down into smaller, manageable steps if necessary.\n2. Consider the last thought and processing history to understand the current progress and context.\n3. Identify any gaps, challenges, or areas that need further attention.\n4. Determine the most logical and efficient next step to move the task forward.\n5. Provide clear, concise instructions on how to complete this next step.\n\nWhen formulating your response, follow this structure:\n\n1. Begin with a brief summary of the current situation, referencing the main task and any relevant information from the last thought or processing history.\n2. Clearly state the next step that should be taken.\n3. Provide detailed instructions on how to complete this step, including any specific techniques, methods, or considerations to keep in mind.\n4. If applicable, mention any potential challenges or pitfalls to be aware of during this step.\n5. Conclude with a brief statement on how this step contributes to the overall progress of the main task.\n\nFormat your response using the following sections:\n&lt;summary&gt;\n(Include your summary of the current situation here)\n&lt;/summary&gt;\n\n&lt;next_step&gt;\n(State the next step to be taken here)\n&lt;/next_step&gt;\n\n&lt;instructions&gt;\n(Provide detailed instructions for completing the next step here)\n&lt;/instructions&gt;\n\n&lt;challenges&gt;\n(If applicable, mention potential challenges or pitfalls here)\n&lt;/challenges&gt;\n\n&lt;conclusion&gt;\n(Briefly state how this step contributes to overall progress)\n&lt;/conclusion&gt;\n\nRemember to be clear, concise, and specific in your guidance. Avoid vague or ambiguous instructions, and provide concrete examples or explanations where necessary.\"\"\")\n            return ThinkState.ACTION, plan\n\n        return ThinkState.ACTION, None\n\n    async def execute(self, code:str):\n        return str(await self._execute_code(code))\n\n    def clear(self):\n        self.chat_session.history = []\n        self.process_memory.history = []\n        self.execution_history = []\n        self.variables = {}\n        self.ipython.reset()\n        self.js_history = []\n\n    async def get_process_hint(self, task):\n        return await self.process_memory.get_reference(task, to_str=True), await self.chat_session.get_reference(task, to_str=True)\n\n    def show_vars(self):\n        return self.verbose_output.log_state(\"VARS\", self.variables, override=True)\n\n    def set_file(self, full_file_path_and_name):\n        if not os.path.exists(full_file_path_and_name):\n            print(\"Invalid file\")\n            return\n        self.ipython.user_ns[\"__file__\"] = full_file_path_and_name\n\n    async def run(self, task, do_continue=False) -&gt; PipelineResult:\n        \"\"\"Run the pipeline with separated thinking and processing phases\"\"\"\n        state = ThinkState.ACTION\n        result = None\n        original_task = task\n        if not do_continue:\n            task = self.agent.mini_task(task, \"user\", f\"\"\"You are an AI assistant tasked with refactoring a user-provided task description into a more structured format with context learning and examples. Your goal is to create a comprehensive and well-organized task description that incorporates model flows and potential code fixes.\n\nFirst, I will provide you with a task description and some example tasks. Please read them carefully:\n\n&lt;existing_globals&gt;\n{self._generate_variable_descriptions()}\n&lt;/existing_globals&gt;\n\n&lt;example_tasks&gt;\nTask: Create a simple analysis of a list of numbers\n- Generate a list of 100 random numbers between 1-1000\n- Calculate the mean, median, and standard deviation\n- Create a histogram of the distribution\n- Print all results and display the plot\n\nTask: Create a reinforcement learning (RL) agent to play a simple game\n- Set up an OpenAI Gym environment (e.g., CartPole)\n- Implement a Q-learning or Deep Q-Network (DQN) agent\n- Train the model and optimize hyperparameters\n- Visualize learning progress with reward graphs\n- Save and reload trained models for inference\n- Provide an option to let the trained agent play in real time\n\nTask: Perform edge detection on an image\n- Load an image from a URL or local file\n- Convert the image to grayscale\n- Apply Gaussian blur to reduce noise\n- Use Canny edge detection to extract edges\n- Display the original and processed images side by side\n- Save the output image\n\nTask: Build a basic sentiment analysis system\n- Load a dataset of movie reviews (you can use a small sample)\n- Preprocess the text (remove punctuation, lowercase, etc.)\n- Create a TF-IDF vectorizer\n- Split data into training and testing sets\n- Train a classifier (e.g., Naive Bayes or LogisticRegression)\n- Evaluate performance with accuracy, precision, recall\n- Create a confusion matrix visualization\n- Make predictions on new sample texts\n&lt;/example_tasks&gt;\n\nNow, please refactor the given task description using the following guidelines:\n\n1. Analyze the task description and identify the main components and objectives.\n\n2. Structure the refactored task in a similar format to the example tasks, including:\n   - A clear title that summarizes the task\n   - A difficulty level (Easy, Intermediate, Hard, or Super Hard)\n   - A brief introduction to the task's context and purpose\n   - A code block containing step-by-step instructions\n   - A list of required skills, libraries, or technologies\n\n3. Incorporate model flows by breaking down the task into logical steps and explaining the process flow.\n\n4. Include potential code fixes or common pitfalls that users might encounter while working on the task.\n\n5. Add context learning elements by providing brief explanations or resources for key concepts related to the task.\n\n6. Ensure that the refactored task is comprehensive and can stand alone as a learning exercise.\n\nPlease provide your refactored task description within &lt;refactored_task&gt; tags. Use appropriate subheadings and formatting to make the description clear and easy to read.\n\nAdditional tips:\n- Mention any prerequisites or assumed knowledge\n- Suggest potential extensions or variations of the task for further learning\n\nRemember to maintain the original intent and complexity of the task while improving its structure and clarity.\"\"\")\n            if '&lt;refactored_task&gt;' in task:\n                task = task.split('&lt;refactored_task&gt;')[1]\n            if '&lt;/refactored_task&gt;' in task:\n                task = task.split('&lt;/refactored_task&gt;')[0]\n        code_follow_up_prompt = f\"\"\"\nYou are an AI assistant responsible for evaluating task completion and providing feedback on the execution process. Your goal is to determine if a given task has been completed based on the execution result, and to offer insights for future improvements.\n\nYou will be provided with two inputs:\n&lt;task_description&gt;\n{original_task}\n{f'&lt;refactored_task_description_from_ai&gt;{task}&lt;/refactored_task_description_from_ai&gt;' if not do_continue else ''}\n&lt;/task_description&gt;\n\n&lt;code&gt;\n#CODE#\n&lt;/code&gt;\n\n&lt;execution_result&gt;\n#EXECUTION_RESULT#\n&lt;/execution_result&gt;\n\nFirst, carefully analyze the task description and the execution result. Determine whether the task has been completed successfully based on the information provided.\n\nIf the task is completed:\n1. Prepare a brief statement indicating that the task is done.\n2. Summarize the output for the user in a clear and concise manner.\n\nIf the task is not completed:\n1. Prepare a brief statement indicating that the task is not done.\n2. Identify the specific aspects of the task that remain incomplete.\n\nRegardless of task completion status, evaluate the procedure and effectiveness of the execution:\n1. Analyze the workflow: Describe the steps taken in the execution process.\n2. Assess effectiveness: Determine how well the procedure achieved the desired outcome.\n3. Identify errors: Pinpoint any mistakes or inefficiencies in the execution.\n4. Provide recommendations: Suggest improvements for future task executions.\n\ntip: Enclose mutil line strings property for python eval to function!\ntip: Set is_completed True if all requirements are completed from &lt;task_description&gt;.\ntip: Help the Agent with your analyses to finalize the &lt;task_description&gt;.\n{'tip: Prefer new informations from &lt;execution_result&gt; over &lt;refactored_task_description_from_ai&gt; based of &lt;code&gt;' if not do_continue else ''}\nnote : for the final result only toke information from the &lt;execution_result&gt;. if the relevant informations is not avalabel try string withe tips in the recommendations. else set is_completed True and return the teh Task failed!\nEnsure that your evaluation is thorough, constructive, and provides actionable insights for improving future task executions.\nAdd guidance based on the the last execution result\"\"\"\n        code_follow_up_prompt_ = [code_follow_up_prompt]\n        initial_prompt = f\"\"\"\nYou are an AI py coding agent specializing in iterative development and code refinement, designed to perform tasks that involve thinking. Your goal is to complete the given task while demonstrating a clear thought process throughout the execution.\nSYSTEM STATE:\n&lt;current_state&gt;\nIteration: #ITER#\nStatus: #STATE#\nLast EXECUTION: #EXECUTION#\n&lt;/current_state&gt;\n\nENVIRONMENT: {'current file :'+self.ipython.user_ns.get(\"__file__\")  if self.ipython.user_ns.get(\"__file__\") is not None else ''}\n\n'''&lt;global_variables&gt;\n#LOCALS#\n&lt;/global_variables&gt;'''\n\nMEMORY:\n&lt;process_memory&gt;\n#PHINT#\n&lt;/process_memory&gt;\n\n&lt;chat_memory&gt;\n#CHINT#\n&lt;/chat_memory&gt;\n\nVALIDATION CHECKLIST (Must verify before each action):\n1. \u2713 Check existing variables in ENVIRONMENT &lt;global_variables&gt;\n2. \u2713 Verify existing functions and classes\n3. \u2713 Review current imports\n4. \u2713 Confirm method signatures\n5. \u2713 Validate state preservation\n\nWORKFLOW STEPS:\n1. Analyze Current State:\n   - Reason and use all avalabel context\n   - Do not repeat the same errors\n   - Review existing implementations\n   - Check variable values\n   - Verify import statements\n   - Document dependencies\n\n2. Plan Change:\n   - NO example/simulation/simulate\n   - No demo er moc Data no Simulations Allowed or u will die!!\n   - Use existing variables and code when possible\n   - Prefer updates over rewrites\n\n3. Execute Change:\n   - Use appropriate action\n   - Maintain existing state\n   - Document modifications\n   - Verify results\n\nYou will use a structure called ThinkResult to organize your thoughts and actions.\nFor each step of your task, follow this process:\n\nACTIONS:\n1. 'code':\n    - MUST check &lt;global_variables&gt; first\n    - NEVER create demo functions\n    - Include 'reason'\n    - lang default 'py'\n    - Required: code in content\n    - code MUST call a function or display the row variabel / value at the end!\n    - Required: {{'context':{{'lang':'py',  'reason': ... }}...}}\n    - Optional file key in context example {{'context':{{'lang':'py',  'file': 'main.py' ,  'reason': ... }}...}}\n    - py code allows for toplevel await !!! use it !!! like\n:file-start:\nprint(\"using toplevel await\")\nawait abc()\n:file-end:\n\n    - Tip: use comments to reason with in the code\n3. 'infos': Request specific details\n4. 'guide': Get step clarification use on complex task and ery 5 step for staying on trak!\n5. 'brake': Pause for assessment\n6. 'done': Summarize changes\n\nCODE CONSTRAINTS:\n1. State Preservation:\n   - ALL variables ar persist\n   - ALL functions remain\n   - ALL classes ar maintained\n\n2. Import Management:\n   - Check &lt;global_variables&gt; for modules\n   - Use absolute imports\n   - Document new dependencies\n\n3. Function Handling:\n   - NEVER overwrite existing\n   - Use update for changes\n   - Preserve signatures\n\n4. Variable Scope:\n   - Maintain existing scope\n   - Check for conflicts\n   - Document state changes\n\nEXECUTION RULES:\n1. VERIFY before create\n2. UPDATE don't replace\n3. TEST after each change\n\nNext Action Required:\n1. Review current state\n2. Check existing code\n3. Execute with state preservation\n\n!!CRITICAL!!\n- NO demo functions\n- NO placeholder functions\n- USE existing code\n- FOR Implementations prefer writing large production redy code chunks.\n- FOR reasoning and validation write small code blocks.\n- THE CODE must call something or end the code with an value!\n- NO INFINIT LOOPS! none breakable while loops ar not allowed, exception ui (closed by user)\n- NO 'python' top level return, only write the variabel or value itself!\n- 'code is run using exec! do not use !pip ...'\n'- instead use auto_install(package_name, install_method=\"pip\", upgrade=False, quiet=False, version=None, extra_args=None)'\n# Example usage first time\n\u2502 auto_install('pandas', version='1.3.0')\n\u2502 import pandas\n\u2502 auto_install('pygame')\n\u2502 import pygame\n\u2502 auto_install('numpy')\n\u2502 import numpy as np\n!TIPS!\n- '&lt;global_variables&gt; can contain instances and functions you can use in your python' code\n- if the function is async you can use top level await\n- if their is missing of informations try running code to get the infos\n- if you got stuck or need assistance break with a question to the user.\n'- run functions from &lt;global_variables&gt; using name(*args, **kwargs) or await name(*args, **kwargs)'\n'- &lt;global_variables&gt; ar global accessible!'\n'- if an &lt;global_variables&gt; name is lower lists an redy to use instance'\n\"\"\"\n        p_hint, c_hint = await self.get_process_hint(task)\n        initial_prompt = initial_prompt.replace('#PHINT#', p_hint)\n        initial_prompt = initial_prompt.replace('#CHINT#', c_hint)\n        initial_prompt_ = initial_prompt\n        iter_i = 0\n        iter_p = 0\n        iter_tat = 0\n        next_infos = \"\"\n        if not do_continue:\n            await self.chat_session.add_message({'role': 'user', 'content': task})\n        else:\n            self.restore()\n            await self.chat_session.add_message({'role': 'user', 'content': task})\n\n        if self.web_js and self.browser_session is None:\n            self.browser_session = BrowserWrapper(llm=self.agent.amd.modle)\n\n        # await self.verbose_output.log_message('user', task)\n        self.verbose_output.log_header(task)\n        while state != ThinkState.DONE:\n            iter_i += 1\n            t0 = time.perf_counter()\n            prompt = initial_prompt.replace('#ITER#', f'{iter_i} max {self.max_iter}')\n            prompt = prompt.replace('#STATE#', f'{state.name}')\n            prompt = prompt.replace('#EXECUTION#', f'{next_infos}')  if next_infos else prompt.replace('Last EXECUTION: #EXECUTION#', '')\n            prompt = prompt.replace('#LOCALS#', f'{self._generate_variable_descriptions()}')\n            self.verbose_output.log_state(state.name, {})\n            self.verbose_output.formatter.print_iteration(iter_i, self.max_iter)\n            if state == ThinkState.ACTION:\n                iter_tat +=1\n                if iter_tat &gt; self.max_think_after_think:\n                    state = ThinkState.BRAKE\n            else:\n                iter_tat = 0\n\n            if state == ThinkState.ACTION:\n                # Get agent's thoughts\n                think_dicts = await self.verbose_output.process(state.name, self.agent.a_format_class(\n                    ThinkResults,\n                    prompt,\n                    message=self.chat_session.get_past_x(self.max_iter*2, last_u=not do_continue).copy()+([self.process_memory.history[-1]] if self.process_memory.history else []) ,\n                ))\n                think_dicts = think_dicts.get(\"actions\")\n                if think_dicts is None:\n                    think_dicts = [await self.verbose_output.process(state.name, self.agent.a_format_class(\n                        ThinkResult,\n                        prompt,\n                        message=self.chat_session.get_past_x(self.max_iter * 2, last_u=not do_continue).copy() + (\n                            [self.process_memory.history[-1]] if self.process_memory.history else []),\n                    ))]\n                if len(think_dicts) == 1:\n                    think_dict = think_dicts[0]\n                else:\n                    for think_dict in think_dicts[:-1]:\n                        if think_dict.get('context') is None:\n                            think_dict['context'] = {'context': 'N/A'}\n                        if not isinstance(think_dict.get('context'), dict):\n                            think_dict['context'] = {'context': think_dict.get('context')}\n                        think_result = ThinkResult(**think_dict)\n                        await self.chat_session.add_message(\n                            {'role': 'assistant', 'content': think_result.content + str(think_result.context)})\n                        state, result = await self.verbose_output.process(think_dict.get(\"action\"),\n                                                                          self._process_think_result(think_result,\n                                                                                                     task=task))\n                        if result:\n                            await self.chat_session.add_message(\n                                {'role': 'system', 'content': 'Evaluation: ' + str(result)})\n                            await self.verbose_output.log_message('system', str(result))\n                    think_dict = think_dicts[-1]\n                await self.verbose_output.log_think_result(think_dict)\n                if think_dict.get('context') is None:\n                    think_dict['context'] = {'context': 'N/A'}\n                if not isinstance(think_dict.get('context'), dict):\n                    think_dict['context'] = {'context': think_dict.get('context')}\n                think_result = ThinkResult(**think_dict)\n                state, result = await self.verbose_output.process(think_dict.get(\"action\"), self._process_think_result(think_result, task=task))\n                await self.chat_session.add_message({'role': 'assistant', 'content': think_result.content + str(think_result.context)})\n                if result:\n                    await self.chat_session.add_message({'role': 'system', 'content': 'Evaluation: '+str(result)})\n                    await self.verbose_output.log_message('system', str(result))\n                    code_follow_up_prompt_[0] = code_follow_up_prompt.replace(\"#EXECUTION_RESULT#\", str(result))\n                    if isinstance(result ,ExecutionRecord):\n                        code_follow_up_prompt_[0] = code_follow_up_prompt_[0].replace(\"#CODE#\", result.code)\n                    else:\n                        code_follow_up_prompt_[0] = code_follow_up_prompt_[0].replace(\"#CODE#\", self._generate_variable_descriptions())\n                else:\n                    code_follow_up_prompt_[0] = code_follow_up_prompt.replace(\"#EXECUTION_RESULT#\", str(think_result))\n                    code_follow_up_prompt_[0] = code_follow_up_prompt_[0].replace(\"#CODE#\",\n                                                                              self._generate_variable_descriptions())\n\n\n            elif state == ThinkState.PROCESSING:\n                # Get agent's thoughts\n                class Next(BaseModel):\n                    is_completed: bool\n                    recommendations: str\n                    errors: str\n                    effectiveness: str\n                    workflow: str\n                    text: str\n                # Format the agent's thoughts into a structured response\n                _agent = self.v_agent if self.v_agent is not None else self.agent\n                next_dict = await self.verbose_output.process(state.name, _agent.a_format_class(\n                    Next,\n                    code_follow_up_prompt_[0],\n                    message=self.chat_session.get_past_x(self.max_iter*2, last_u=not do_continue).copy(),\n                ))\n                next_infos = json.dumps(next_dict)\n                await self.verbose_output.log_process_result(next_dict)\n                await self.process_memory.add_message({'role': 'assistant', 'content': next_infos.replace('workflow:', 'past-workflow:')})\n                iter_p += 1\n                code_follow_up_prompt_[0] = code_follow_up_prompt\n                if not next_dict.get('is_completed', True):\n                    state = ThinkState.ACTION\n                    initial_prompt = initial_prompt_.replace('#ITER#',f'#ITER#\\nReasoning assist result: {next_dict}')\n                    continue\n                elif next_dict.get('is_completed', False):\n                    result = next_dict.get('text', '')\n                    state = ThinkState.DONE\n                    continue\n                else:\n                    result = next_dict.get('text', '')\n                    break\n\n            elif state == ThinkState.BRAKE:\n                break\n\n            if iter_i &lt; self.max_iter:\n                if time.perf_counter() -t0 &lt; self.timeout_timer*2.5:\n                    with Spinner(f\"Prevent rate limit posing for {self.timeout_timer}s\", symbols='+', time_in_s=self.timeout_timer, count_down=True):\n                        await asyncio.sleep(self.timeout_timer)\n            else:\n                state = ThinkState.BRAKE\n                if isinstance(result, ExecutionRecord):\n                    result = result.result\n                elif isinstance(result, str):\n                    pass\n                else:\n                    result = \"Max iterations\"\n                break\n\n        self.verbose_output.log_state(state.name, {})\n\n        return PipelineResult(\n            variables=self.variables,\n            result=result,\n            execution_history=self.execution_history,\n            message=self.chat_session.get_past_x(iter_i*2, last_u=not do_continue),\n        )\n\n    async def run_project(self, task, lang='py', execute_function=None):\n        if execute_function is None:\n            if lang == 'py':\n                execute_function = default_python_execute_function\n            elif lang == 'rust':\n                execute_function = default_rust_execute_function\n            else:\n                raise ValueError(f\"Unsupported language: {lang}\")\n        class FileAction(BaseModel):\n            action: str\n            path: str\n            content: str | None = None\n\n        class ProjectThinkResult(BaseModel):\n            action: str\n            file_actions: list[FileAction]\n            reasoning: str\n\n        class ProjectPipelineResult(BaseModel):\n            result: str\n            execution_history: list[str]\n            files: dict[str, str]\n        state = ThinkState.ACTION\n        result = None\n        vfs = VirtualFileSystem(self._session_dir / f\"project_{lang}\")\n\n        project_prompt = f\"\"\"\n    You are an AI coding agent specializing in {lang} project development. Your task is to create, modify, and manage files within a project structure to complete the given task. Use the VirtualFileSystem to interact with files.\n\n    TASK DESCRIPTION:\n    {task}\n    CURRENT FILES:\n    #files#\n\n    WORKFLOW STEPS:\n    1. Analyze the current project state\n    2. Plan necessary changes or additions\n    3. Execute changes using file actions\n    4. Evaluate the project's progress\n\n    Use the ProjectThinkResult structure to organize your thoughts and actions:\n\n    class ProjectThinkResult(BaseModel):\n        action: str  # 'code', 'evaluate', 'done'\n        file_actions: List[FileAction]\n        reasoning: str\n\n    class FileAction(BaseModel):\n        action: str  # 'write', 'read', 'delete', 'list'\n        path: str\n        content: Optional[str] = None\n\n    EXECUTION RULES:\n    1. Use absolute paths for all file operations\n    2. Maintain a clear project structure\n    3. Document your code and reasoning\n    4. Ensure all necessary files are created and properly linked\n    5. Use the appropriate language syntax and best practices for {lang}\n\n    Next Action Required:\n    1. Review the current project state\n    2. Plan the next step in project development\n    3. Execute file actions to implement changes\n    \"\"\"\n\n        execution_history = []\n        files = {}\n\n        iter_i = 0\n        self.verbose_output.log_header(task)\n\n        while state != ThinkState.DONE:\n            iter_i += 1\n            self.verbose_output.formatter.print_iteration(iter_i, self.max_iter)\n            if iter_i&gt;self.max_iter:\n                break\n            if state == ThinkState.ACTION:\n                think_result = await self.agent.a_format_class(\n                    ProjectThinkResult,\n                    project_prompt.replace('#files#', vfs.print_file_structure()),\n                    message=execution_history\n                )\n                self.verbose_output.log_state(state.name, think_result)\n                think_result = ProjectThinkResult(**think_result)\n                for file_action in think_result.file_actions:\n                    path = file_action.path\n                    Path(file_action.path).mkdir(exist_ok=True)\n                    if file_action.action == 'write':\n                        vfs.write_file(path, file_action.content)\n                        files[path] = file_action.content\n                    elif file_action.action == 'read':\n                        content = vfs.read_file(path)\n                        files[path] = content\n                    elif file_action.action == 'delete':\n                        vfs.delete_file(path)\n                        files.pop(path, None)\n                    elif file_action.action == 'list':\n                        dir_contents = vfs.list_directory(path)\n                        files[path] = str(dir_contents)\n\n                if think_result.action == 'evaluate':\n                    state = ThinkState.PROCESSING\n                elif think_result.action == 'done':\n                    state = ThinkState.DONE\n\n                execution_history.append(f\"Action: {think_result.action}\\nReasoning: {think_result.reasoning}\")\n\n            elif state == ThinkState.PROCESSING:\n                if execute_function:\n                    execution_result = await execute_function(files)\n                    execution_history.append(f\"Execution Result: {execution_result}\")\n\n                    evaluation_prompt = f\"\"\"\n    Evaluate the current state of the project based on the execution result:\n\n    {execution_result}\n\n    Determine if the project is complete or if further modifications are needed.\n    \"\"\"\n                    evaluation = await self.agent.a_format_class(\n                        ProjectThinkResult,\n                        evaluation_prompt,\n                        message=execution_history\n                    )\n                    self.verbose_output.log_state(state.name, evaluation)\n                    evaluation = ProjectThinkResult(**evaluation)\n                    if evaluation.action == 'done':\n                        state = ThinkState.DONE\n                        result = execution_result\n                    else:\n                        state = ThinkState.ACTION\n                else:\n                    state = ThinkState.ACTION\n            else:\n                break\n\n        return ProjectPipelineResult(\n            result=result,\n            execution_history=execution_history,\n            files=files\n        )\n\n    async def __aenter__(self):\n        self.clear()\n        return self\n\n    async def configure(self, verbose=None, print_function=None, with_js=False, agent=None, variables=None, web_kwargs=None):\n        if verbose is not None and (print_function is not None or verbose != self.verbose_output.verbose):\n            if agent is None:\n                agent = self.agent\n            else:\n                self.agent = agent\n            agent.verbose = verbose\n            self.verbose_output = EnhancedVerboseOutput(verbose=verbose, print_f=print_function)\n\n            if print_function is not None:\n                agent.print_verbose = print_function\n        if variables:\n            self.variables = {**self.variables, **self._process_variables(variables)}\n        if with_js and web_kwargs:\n            self.browser_session: BrowserWrapper | None = BrowserWrapper(**web_kwargs)\n        self.web_js = with_js\n        if self.restore_var:\n            self.restore()\n\n        return self\n\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        if self.web_js:\n            await self.browser_session.close()\n            if self.restore_var:\n                self.save_session(f\"Pipeline_Session_{self.agent.amd.name}\")\n        if exc_type is not None:\n            print(f\"Exception occurred: {exc_value}\")\n        else:\n            print(\"Pipe Exit\")\n</code></pre> <code>__init__(agent, verbose=False, max_iter=12, variables=None, top_n=None, restore=None, max_think_after_think=None, print_f=None, web_js=False, timeout_timer=25, v_agent=None, web_llm=None)</code> \u00b6 <p>Initialize the Pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Any</code> <p>AI agent instance to use for task execution</p> required <code>verbose</code> <code>bool</code> <p>print internal results</p> <code>False</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations (default: 12)</p> <code>12</code> <code>variables</code> <code>dict[str, Any] | list[Any] | None</code> <p>Dictionary or list of variables to make available</p> <code>None</code> <code>top_n</code> <code>bool | None</code> <p>Limit variable descriptions to top N most used</p> <code>None</code> <code>web_js</code> <p>if the agent is allow to use the web</p> <code>False</code> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __init__(\n    self,\n    agent: Any,\n    verbose: bool=False,\n    max_iter: int= 12,\n    variables: dict[str, Any] | list[Any] | None = None,\n    top_n: bool | None = None,\n    restore: bool | None = None,\n    max_think_after_think = None,\n    print_f=None,\n    web_js=False,\n    timeout_timer=25,\n    v_agent=None,\n    web_llm=None,\n):\n    \"\"\"\n    Initialize the Pipeline.\n\n    Args:\n        agent: AI agent instance to use for task execution\n        verbose: print internal results\n        max_iter: Maximum number of iterations (default: 12)\n        variables: Dictionary or list of variables to make available\n        top_n: Limit variable descriptions to top N most used\n        web_js: if the agent is allow to use the web\n    \"\"\"\n\n    self.timeout_timer = timeout_timer\n    self.top_n = top_n\n    self.max_iter = max_iter\n    self.max_think_after_think = max_think_after_think or max_iter // 2\n    self.agent = agent\n    self.v_agent = v_agent or agent\n    # self.agent.verbose = verbose\n    self.task = None\n    self.web_js = web_js\n    self.print_f = print_f\n    self.verbose_output = EnhancedVerboseOutput(verbose=verbose, print_f=self.print_f)\n    self.variables = self._process_variables(variables or {})\n    self.variables['auto_install'] = auto_install\n    self.execution_history = []\n    self.session_name = None\n\n    self.browser_session: BrowserWrapper | None = BrowserWrapper(llm=web_llm or agent.amd.model)\n    self.js_history: list[JSExecutionRecord] = []\n\n    self._session_dir = Path(get_app().appdata) / 'ChatSession' / agent.amd.name\n    self.ipython = MockIPython(self._session_dir, auto_remove=False)\n    self.chat_session = ChatSession(get_app().get_mod(\"isaa\").get_memory(), space_name=f\"ChatSession/{agent.amd.name}/Pipeline.session\", max_length=max_iter)\n    self.process_memory = ChatSession(get_app().get_mod(\"isaa\").get_memory(), space_name=f\"ChatSession/{agent.amd.name}/Process.session\", max_length=max_iter)\n\n    # Initialize interpreter with variables\n    self.init_keys = list(self.ipython.user_ns.keys()).copy()\n    if self.web_js:\n        self.variables['web_actions'] = self.browser_session.run\n        self.variables['browser_session'] = self.browser_session\n    self.ipython.user_ns.update(self.variables)\n\n    self.restore_var = restore\n\n    if restore:\n        self.restore()\n</code></pre> <code>__str__()</code> \u00b6 <p>String representation of pipeline session</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __str__(self):\n    \"\"\"String representation of pipeline session\"\"\"\n    return str(self.ipython)\n</code></pre> <code>load_session(name)</code> \u00b6 <p>Load saved session</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def load_session(self, name: str):\n    \"\"\"Load saved session\"\"\"\n    self.ipython.load_session(name)\n    self.variables.update(self.ipython.user_ns)\n</code></pre> <code>run(task, do_continue=False)</code> <code>async</code> \u00b6 <p>Run the pipeline with separated thinking and processing phases</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>    async def run(self, task, do_continue=False) -&gt; PipelineResult:\n        \"\"\"Run the pipeline with separated thinking and processing phases\"\"\"\n        state = ThinkState.ACTION\n        result = None\n        original_task = task\n        if not do_continue:\n            task = self.agent.mini_task(task, \"user\", f\"\"\"You are an AI assistant tasked with refactoring a user-provided task description into a more structured format with context learning and examples. Your goal is to create a comprehensive and well-organized task description that incorporates model flows and potential code fixes.\n\nFirst, I will provide you with a task description and some example tasks. Please read them carefully:\n\n&lt;existing_globals&gt;\n{self._generate_variable_descriptions()}\n&lt;/existing_globals&gt;\n\n&lt;example_tasks&gt;\nTask: Create a simple analysis of a list of numbers\n- Generate a list of 100 random numbers between 1-1000\n- Calculate the mean, median, and standard deviation\n- Create a histogram of the distribution\n- Print all results and display the plot\n\nTask: Create a reinforcement learning (RL) agent to play a simple game\n- Set up an OpenAI Gym environment (e.g., CartPole)\n- Implement a Q-learning or Deep Q-Network (DQN) agent\n- Train the model and optimize hyperparameters\n- Visualize learning progress with reward graphs\n- Save and reload trained models for inference\n- Provide an option to let the trained agent play in real time\n\nTask: Perform edge detection on an image\n- Load an image from a URL or local file\n- Convert the image to grayscale\n- Apply Gaussian blur to reduce noise\n- Use Canny edge detection to extract edges\n- Display the original and processed images side by side\n- Save the output image\n\nTask: Build a basic sentiment analysis system\n- Load a dataset of movie reviews (you can use a small sample)\n- Preprocess the text (remove punctuation, lowercase, etc.)\n- Create a TF-IDF vectorizer\n- Split data into training and testing sets\n- Train a classifier (e.g., Naive Bayes or LogisticRegression)\n- Evaluate performance with accuracy, precision, recall\n- Create a confusion matrix visualization\n- Make predictions on new sample texts\n&lt;/example_tasks&gt;\n\nNow, please refactor the given task description using the following guidelines:\n\n1. Analyze the task description and identify the main components and objectives.\n\n2. Structure the refactored task in a similar format to the example tasks, including:\n   - A clear title that summarizes the task\n   - A difficulty level (Easy, Intermediate, Hard, or Super Hard)\n   - A brief introduction to the task's context and purpose\n   - A code block containing step-by-step instructions\n   - A list of required skills, libraries, or technologies\n\n3. Incorporate model flows by breaking down the task into logical steps and explaining the process flow.\n\n4. Include potential code fixes or common pitfalls that users might encounter while working on the task.\n\n5. Add context learning elements by providing brief explanations or resources for key concepts related to the task.\n\n6. Ensure that the refactored task is comprehensive and can stand alone as a learning exercise.\n\nPlease provide your refactored task description within &lt;refactored_task&gt; tags. Use appropriate subheadings and formatting to make the description clear and easy to read.\n\nAdditional tips:\n- Mention any prerequisites or assumed knowledge\n- Suggest potential extensions or variations of the task for further learning\n\nRemember to maintain the original intent and complexity of the task while improving its structure and clarity.\"\"\")\n            if '&lt;refactored_task&gt;' in task:\n                task = task.split('&lt;refactored_task&gt;')[1]\n            if '&lt;/refactored_task&gt;' in task:\n                task = task.split('&lt;/refactored_task&gt;')[0]\n        code_follow_up_prompt = f\"\"\"\nYou are an AI assistant responsible for evaluating task completion and providing feedback on the execution process. Your goal is to determine if a given task has been completed based on the execution result, and to offer insights for future improvements.\n\nYou will be provided with two inputs:\n&lt;task_description&gt;\n{original_task}\n{f'&lt;refactored_task_description_from_ai&gt;{task}&lt;/refactored_task_description_from_ai&gt;' if not do_continue else ''}\n&lt;/task_description&gt;\n\n&lt;code&gt;\n#CODE#\n&lt;/code&gt;\n\n&lt;execution_result&gt;\n#EXECUTION_RESULT#\n&lt;/execution_result&gt;\n\nFirst, carefully analyze the task description and the execution result. Determine whether the task has been completed successfully based on the information provided.\n\nIf the task is completed:\n1. Prepare a brief statement indicating that the task is done.\n2. Summarize the output for the user in a clear and concise manner.\n\nIf the task is not completed:\n1. Prepare a brief statement indicating that the task is not done.\n2. Identify the specific aspects of the task that remain incomplete.\n\nRegardless of task completion status, evaluate the procedure and effectiveness of the execution:\n1. Analyze the workflow: Describe the steps taken in the execution process.\n2. Assess effectiveness: Determine how well the procedure achieved the desired outcome.\n3. Identify errors: Pinpoint any mistakes or inefficiencies in the execution.\n4. Provide recommendations: Suggest improvements for future task executions.\n\ntip: Enclose mutil line strings property for python eval to function!\ntip: Set is_completed True if all requirements are completed from &lt;task_description&gt;.\ntip: Help the Agent with your analyses to finalize the &lt;task_description&gt;.\n{'tip: Prefer new informations from &lt;execution_result&gt; over &lt;refactored_task_description_from_ai&gt; based of &lt;code&gt;' if not do_continue else ''}\nnote : for the final result only toke information from the &lt;execution_result&gt;. if the relevant informations is not avalabel try string withe tips in the recommendations. else set is_completed True and return the teh Task failed!\nEnsure that your evaluation is thorough, constructive, and provides actionable insights for improving future task executions.\nAdd guidance based on the the last execution result\"\"\"\n        code_follow_up_prompt_ = [code_follow_up_prompt]\n        initial_prompt = f\"\"\"\nYou are an AI py coding agent specializing in iterative development and code refinement, designed to perform tasks that involve thinking. Your goal is to complete the given task while demonstrating a clear thought process throughout the execution.\nSYSTEM STATE:\n&lt;current_state&gt;\nIteration: #ITER#\nStatus: #STATE#\nLast EXECUTION: #EXECUTION#\n&lt;/current_state&gt;\n\nENVIRONMENT: {'current file :'+self.ipython.user_ns.get(\"__file__\")  if self.ipython.user_ns.get(\"__file__\") is not None else ''}\n\n'''&lt;global_variables&gt;\n#LOCALS#\n&lt;/global_variables&gt;'''\n\nMEMORY:\n&lt;process_memory&gt;\n#PHINT#\n&lt;/process_memory&gt;\n\n&lt;chat_memory&gt;\n#CHINT#\n&lt;/chat_memory&gt;\n\nVALIDATION CHECKLIST (Must verify before each action):\n1. \u2713 Check existing variables in ENVIRONMENT &lt;global_variables&gt;\n2. \u2713 Verify existing functions and classes\n3. \u2713 Review current imports\n4. \u2713 Confirm method signatures\n5. \u2713 Validate state preservation\n\nWORKFLOW STEPS:\n1. Analyze Current State:\n   - Reason and use all avalabel context\n   - Do not repeat the same errors\n   - Review existing implementations\n   - Check variable values\n   - Verify import statements\n   - Document dependencies\n\n2. Plan Change:\n   - NO example/simulation/simulate\n   - No demo er moc Data no Simulations Allowed or u will die!!\n   - Use existing variables and code when possible\n   - Prefer updates over rewrites\n\n3. Execute Change:\n   - Use appropriate action\n   - Maintain existing state\n   - Document modifications\n   - Verify results\n\nYou will use a structure called ThinkResult to organize your thoughts and actions.\nFor each step of your task, follow this process:\n\nACTIONS:\n1. 'code':\n    - MUST check &lt;global_variables&gt; first\n    - NEVER create demo functions\n    - Include 'reason'\n    - lang default 'py'\n    - Required: code in content\n    - code MUST call a function or display the row variabel / value at the end!\n    - Required: {{'context':{{'lang':'py',  'reason': ... }}...}}\n    - Optional file key in context example {{'context':{{'lang':'py',  'file': 'main.py' ,  'reason': ... }}...}}\n    - py code allows for toplevel await !!! use it !!! like\n:file-start:\nprint(\"using toplevel await\")\nawait abc()\n:file-end:\n\n    - Tip: use comments to reason with in the code\n3. 'infos': Request specific details\n4. 'guide': Get step clarification use on complex task and ery 5 step for staying on trak!\n5. 'brake': Pause for assessment\n6. 'done': Summarize changes\n\nCODE CONSTRAINTS:\n1. State Preservation:\n   - ALL variables ar persist\n   - ALL functions remain\n   - ALL classes ar maintained\n\n2. Import Management:\n   - Check &lt;global_variables&gt; for modules\n   - Use absolute imports\n   - Document new dependencies\n\n3. Function Handling:\n   - NEVER overwrite existing\n   - Use update for changes\n   - Preserve signatures\n\n4. Variable Scope:\n   - Maintain existing scope\n   - Check for conflicts\n   - Document state changes\n\nEXECUTION RULES:\n1. VERIFY before create\n2. UPDATE don't replace\n3. TEST after each change\n\nNext Action Required:\n1. Review current state\n2. Check existing code\n3. Execute with state preservation\n\n!!CRITICAL!!\n- NO demo functions\n- NO placeholder functions\n- USE existing code\n- FOR Implementations prefer writing large production redy code chunks.\n- FOR reasoning and validation write small code blocks.\n- THE CODE must call something or end the code with an value!\n- NO INFINIT LOOPS! none breakable while loops ar not allowed, exception ui (closed by user)\n- NO 'python' top level return, only write the variabel or value itself!\n- 'code is run using exec! do not use !pip ...'\n'- instead use auto_install(package_name, install_method=\"pip\", upgrade=False, quiet=False, version=None, extra_args=None)'\n# Example usage first time\n\u2502 auto_install('pandas', version='1.3.0')\n\u2502 import pandas\n\u2502 auto_install('pygame')\n\u2502 import pygame\n\u2502 auto_install('numpy')\n\u2502 import numpy as np\n!TIPS!\n- '&lt;global_variables&gt; can contain instances and functions you can use in your python' code\n- if the function is async you can use top level await\n- if their is missing of informations try running code to get the infos\n- if you got stuck or need assistance break with a question to the user.\n'- run functions from &lt;global_variables&gt; using name(*args, **kwargs) or await name(*args, **kwargs)'\n'- &lt;global_variables&gt; ar global accessible!'\n'- if an &lt;global_variables&gt; name is lower lists an redy to use instance'\n\"\"\"\n        p_hint, c_hint = await self.get_process_hint(task)\n        initial_prompt = initial_prompt.replace('#PHINT#', p_hint)\n        initial_prompt = initial_prompt.replace('#CHINT#', c_hint)\n        initial_prompt_ = initial_prompt\n        iter_i = 0\n        iter_p = 0\n        iter_tat = 0\n        next_infos = \"\"\n        if not do_continue:\n            await self.chat_session.add_message({'role': 'user', 'content': task})\n        else:\n            self.restore()\n            await self.chat_session.add_message({'role': 'user', 'content': task})\n\n        if self.web_js and self.browser_session is None:\n            self.browser_session = BrowserWrapper(llm=self.agent.amd.modle)\n\n        # await self.verbose_output.log_message('user', task)\n        self.verbose_output.log_header(task)\n        while state != ThinkState.DONE:\n            iter_i += 1\n            t0 = time.perf_counter()\n            prompt = initial_prompt.replace('#ITER#', f'{iter_i} max {self.max_iter}')\n            prompt = prompt.replace('#STATE#', f'{state.name}')\n            prompt = prompt.replace('#EXECUTION#', f'{next_infos}')  if next_infos else prompt.replace('Last EXECUTION: #EXECUTION#', '')\n            prompt = prompt.replace('#LOCALS#', f'{self._generate_variable_descriptions()}')\n            self.verbose_output.log_state(state.name, {})\n            self.verbose_output.formatter.print_iteration(iter_i, self.max_iter)\n            if state == ThinkState.ACTION:\n                iter_tat +=1\n                if iter_tat &gt; self.max_think_after_think:\n                    state = ThinkState.BRAKE\n            else:\n                iter_tat = 0\n\n            if state == ThinkState.ACTION:\n                # Get agent's thoughts\n                think_dicts = await self.verbose_output.process(state.name, self.agent.a_format_class(\n                    ThinkResults,\n                    prompt,\n                    message=self.chat_session.get_past_x(self.max_iter*2, last_u=not do_continue).copy()+([self.process_memory.history[-1]] if self.process_memory.history else []) ,\n                ))\n                think_dicts = think_dicts.get(\"actions\")\n                if think_dicts is None:\n                    think_dicts = [await self.verbose_output.process(state.name, self.agent.a_format_class(\n                        ThinkResult,\n                        prompt,\n                        message=self.chat_session.get_past_x(self.max_iter * 2, last_u=not do_continue).copy() + (\n                            [self.process_memory.history[-1]] if self.process_memory.history else []),\n                    ))]\n                if len(think_dicts) == 1:\n                    think_dict = think_dicts[0]\n                else:\n                    for think_dict in think_dicts[:-1]:\n                        if think_dict.get('context') is None:\n                            think_dict['context'] = {'context': 'N/A'}\n                        if not isinstance(think_dict.get('context'), dict):\n                            think_dict['context'] = {'context': think_dict.get('context')}\n                        think_result = ThinkResult(**think_dict)\n                        await self.chat_session.add_message(\n                            {'role': 'assistant', 'content': think_result.content + str(think_result.context)})\n                        state, result = await self.verbose_output.process(think_dict.get(\"action\"),\n                                                                          self._process_think_result(think_result,\n                                                                                                     task=task))\n                        if result:\n                            await self.chat_session.add_message(\n                                {'role': 'system', 'content': 'Evaluation: ' + str(result)})\n                            await self.verbose_output.log_message('system', str(result))\n                    think_dict = think_dicts[-1]\n                await self.verbose_output.log_think_result(think_dict)\n                if think_dict.get('context') is None:\n                    think_dict['context'] = {'context': 'N/A'}\n                if not isinstance(think_dict.get('context'), dict):\n                    think_dict['context'] = {'context': think_dict.get('context')}\n                think_result = ThinkResult(**think_dict)\n                state, result = await self.verbose_output.process(think_dict.get(\"action\"), self._process_think_result(think_result, task=task))\n                await self.chat_session.add_message({'role': 'assistant', 'content': think_result.content + str(think_result.context)})\n                if result:\n                    await self.chat_session.add_message({'role': 'system', 'content': 'Evaluation: '+str(result)})\n                    await self.verbose_output.log_message('system', str(result))\n                    code_follow_up_prompt_[0] = code_follow_up_prompt.replace(\"#EXECUTION_RESULT#\", str(result))\n                    if isinstance(result ,ExecutionRecord):\n                        code_follow_up_prompt_[0] = code_follow_up_prompt_[0].replace(\"#CODE#\", result.code)\n                    else:\n                        code_follow_up_prompt_[0] = code_follow_up_prompt_[0].replace(\"#CODE#\", self._generate_variable_descriptions())\n                else:\n                    code_follow_up_prompt_[0] = code_follow_up_prompt.replace(\"#EXECUTION_RESULT#\", str(think_result))\n                    code_follow_up_prompt_[0] = code_follow_up_prompt_[0].replace(\"#CODE#\",\n                                                                              self._generate_variable_descriptions())\n\n\n            elif state == ThinkState.PROCESSING:\n                # Get agent's thoughts\n                class Next(BaseModel):\n                    is_completed: bool\n                    recommendations: str\n                    errors: str\n                    effectiveness: str\n                    workflow: str\n                    text: str\n                # Format the agent's thoughts into a structured response\n                _agent = self.v_agent if self.v_agent is not None else self.agent\n                next_dict = await self.verbose_output.process(state.name, _agent.a_format_class(\n                    Next,\n                    code_follow_up_prompt_[0],\n                    message=self.chat_session.get_past_x(self.max_iter*2, last_u=not do_continue).copy(),\n                ))\n                next_infos = json.dumps(next_dict)\n                await self.verbose_output.log_process_result(next_dict)\n                await self.process_memory.add_message({'role': 'assistant', 'content': next_infos.replace('workflow:', 'past-workflow:')})\n                iter_p += 1\n                code_follow_up_prompt_[0] = code_follow_up_prompt\n                if not next_dict.get('is_completed', True):\n                    state = ThinkState.ACTION\n                    initial_prompt = initial_prompt_.replace('#ITER#',f'#ITER#\\nReasoning assist result: {next_dict}')\n                    continue\n                elif next_dict.get('is_completed', False):\n                    result = next_dict.get('text', '')\n                    state = ThinkState.DONE\n                    continue\n                else:\n                    result = next_dict.get('text', '')\n                    break\n\n            elif state == ThinkState.BRAKE:\n                break\n\n            if iter_i &lt; self.max_iter:\n                if time.perf_counter() -t0 &lt; self.timeout_timer*2.5:\n                    with Spinner(f\"Prevent rate limit posing for {self.timeout_timer}s\", symbols='+', time_in_s=self.timeout_timer, count_down=True):\n                        await asyncio.sleep(self.timeout_timer)\n            else:\n                state = ThinkState.BRAKE\n                if isinstance(result, ExecutionRecord):\n                    result = result.result\n                elif isinstance(result, str):\n                    pass\n                else:\n                    result = \"Max iterations\"\n                break\n\n        self.verbose_output.log_state(state.name, {})\n\n        return PipelineResult(\n            variables=self.variables,\n            result=result,\n            execution_history=self.execution_history,\n            message=self.chat_session.get_past_x(iter_i*2, last_u=not do_continue),\n        )\n</code></pre> <code>save_session(name)</code> \u00b6 <p>Save current session</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def save_session(self, name: str):\n    \"\"\"Save current session\"\"\"\n    self.session_name = name\n    self.ipython.save_session(name)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.SyncReport","title":"<code>SyncReport</code>  <code>dataclass</code>","text":"<p>Report of variables synced from namespace to pipeline</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>@dataclass\nclass SyncReport:\n    \"\"\"Report of variables synced from namespace to pipeline\"\"\"\n    added: dict[str, str]\n    skipped: dict[str, str]  # var_name -&gt; reason\n    errors: dict[str, str]  # var_name -&gt; error message\n\n    def __str__(self) -&gt; str:\n        parts = []\n        if self.added:\n            parts.append(\"Added variables:\")\n            for name, type_ in self.added.items():\n                parts.append(f\"  - {name}: {type_}\")\n        if self.skipped:\n            parts.append(\"\\nSkipped variables:\")\n            for name, reason in self.skipped.items():\n                parts.append(f\"  - {name}: {reason}\")\n        if self.errors:\n            parts.append(\"\\nErrors:\")\n            for name, error in self.errors.items():\n                parts.append(f\"  - {name}: {error}\")\n        return \"\\n\".join(parts)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.TeeStream","title":"<code>TeeStream</code>","text":"<p>Stream that writes to both console and buffer</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class TeeStream:\n    \"\"\"Stream that writes to both console and buffer\"\"\"\n    def __init__(self, console_stream, buffer_stream):\n        self.console_stream = console_stream\n        self.buffer_stream = buffer_stream\n\n    def write(self, data):\n        self.console_stream.write(data)\n        self.buffer_stream.write(data)\n        self.console_stream.flush()  # Ensure immediate console output\n\n    def flush(self):\n        self.console_stream.flush()\n        self.buffer_stream.flush()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.VerboseFormatter","title":"<code>VerboseFormatter</code>","text":"Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class VerboseFormatter:\n    def __init__(self,print_f, spinner_style: str = \"d\"):\n        self.style = Style()\n        self.current_spinner = None\n        self.spinner_style = spinner_style\n        self.print = print_f\n\n    def print_header(self, text: str):\n        \"\"\"Print a formatted header with separator line\"\"\"\n        width = 80\n        self.print(f\"\\n{self.style.BLUE('=' * width)}\")\n        self.print(self.style.BLUE2(f\"\u26a1 {text.center(width - 4)} \u26a1\"))\n        self.print(f\"{self.style.BLUE('=' * width)}\\n\")\n\n    def print_section(self, title: str, content: str):\n        \"\"\"Print a formatted section with title and content\"\"\"\n        self.print(f\"{self.style.YELLOW('\u250c\u2500')} {self.style.YELLOW2(title)}\")\n        for line in content.split('\\n'):\n            try:\n                self.print(f\"{self.style.YELLOW('\u2502')} {line}\")\n            except Exception as e:\n                try:\n                    pos = int(str(e).split('position ')[1].split('-')[0])\n                    line = line[:pos] + line[pos+1:]\n                    self.print(f\"{self.style.YELLOW('\u2502')} {line}\")\n                except Exception as e:\n                    self.print(f\"{self.style.RED('\u2502')} UNABLE TO PRINT {str(e)}\")\n        self.print(f\"{self.style.YELLOW('\u2514\u2500')} {self.style.GREY('End of section')}\\n\")\n\n    def print_iteration(self, current: int, maximum: int):\n        \"\"\"Print iteration progress with visual bar\"\"\"\n        progress = int((current / maximum) * 20)\n        bar = \"\u2588\" * progress + \"\u2591\" * (20 - progress)\n        self.print(f\"\\r{self.style.CYAN(f'Iteration [{bar}] {current}/{maximum}')}  \", end='')\n\n    def print_state(self, state: str, details: dict[str, Any] | None = None):\n        \"\"\"Print current state with optional details\"\"\"\n        state_color = {\n            'ACTION': self.style.GREEN2,\n            'PROCESSING': self.style.YELLOW2,\n            'BRAKE': self.style.RED2,\n            'DONE': self.style.BLUE2\n        }.get(state, self.style.WHITE2)\n        res_str = f\"\\nCurrent State: {state}\"\n        self.print(f\"\\n{self.style.Bold('Current State:')} {state_color(state)}\")\n\n        if details:\n            for key, value in details.items():\n                self.print(f\"  {self.style.GREY('\u251c\u2500')} {self.style.CYAN(key)}: {value}\")\n                res_str += f\"  \u251c\u2500 {key}: {value}\\n\"\n        return res_str\n\n    def print_method_update(self, method_update: 'MethodUpdate'):\n        \"\"\"Print a formatted view of a MethodUpdate structure\"\"\"\n        # Header with class and method name\n        self.print(f\"\\n{self.style.BLUE('\u250f\u2501')} {self.style.Bold('Method Update Details')}\")\n\n        # Class and method information\n        self.print(f\"{self.style.BLUE('\u2523\u2501')} Class: {self.style.GREEN2(method_update.class_name)}\")\n        self.print(f\"{self.style.BLUE('\u2523\u2501')} Method: {self.style.YELLOW2(method_update.method_name)}\")\n\n        # Description if available\n        if method_update.description:\n            self.print(f\"{self.style.BLUE('\u2523\u2501')} Description:\")\n            for line in method_update.description.split('\\n'):\n                self.print(f\"{self.style.BLUE('\u2503')}  {self.style.GREY(line)}\")\n\n        # Code section\n        self.print(f\"{self.style.BLUE('\u2523\u2501')} Code:\")\n        code_lines = method_update.code.split('\\n')\n        for i, line in enumerate(code_lines):\n            # Different styling for first and last lines\n            if i == 0:\n                self.print(f\"{self.style.BLUE('\u2503')}  {self.style.CYAN('\u250c\u2500')} {line}\")\n            elif i == len(code_lines) - 1:\n                self.print(f\"{self.style.BLUE('\u2503')}  {self.style.CYAN('\u2514\u2500')} {line}\")\n            else:\n                self.print(f\"{self.style.BLUE('\u2503')}  {self.style.CYAN('\u2502')} {line}\")\n\n        # Footer\n        self.print(f\"{self.style.BLUE('\u2517\u2501')} {self.style.GREY('End of method update')}\\n\")\n\n    async def process_with_spinner(self, message: str, coroutine):\n        \"\"\"Execute a coroutine with a spinner indicator\"\"\"\n        with Spinner(message, symbols=self.spinner_style):\n            result = await coroutine\n            return result\n</code></pre> <code>print_header(text)</code> \u00b6 <p>Print a formatted header with separator line</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_header(self, text: str):\n    \"\"\"Print a formatted header with separator line\"\"\"\n    width = 80\n    self.print(f\"\\n{self.style.BLUE('=' * width)}\")\n    self.print(self.style.BLUE2(f\"\u26a1 {text.center(width - 4)} \u26a1\"))\n    self.print(f\"{self.style.BLUE('=' * width)}\\n\")\n</code></pre> <code>print_iteration(current, maximum)</code> \u00b6 <p>Print iteration progress with visual bar</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_iteration(self, current: int, maximum: int):\n    \"\"\"Print iteration progress with visual bar\"\"\"\n    progress = int((current / maximum) * 20)\n    bar = \"\u2588\" * progress + \"\u2591\" * (20 - progress)\n    self.print(f\"\\r{self.style.CYAN(f'Iteration [{bar}] {current}/{maximum}')}  \", end='')\n</code></pre> <code>print_method_update(method_update)</code> \u00b6 <p>Print a formatted view of a MethodUpdate structure</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_method_update(self, method_update: 'MethodUpdate'):\n    \"\"\"Print a formatted view of a MethodUpdate structure\"\"\"\n    # Header with class and method name\n    self.print(f\"\\n{self.style.BLUE('\u250f\u2501')} {self.style.Bold('Method Update Details')}\")\n\n    # Class and method information\n    self.print(f\"{self.style.BLUE('\u2523\u2501')} Class: {self.style.GREEN2(method_update.class_name)}\")\n    self.print(f\"{self.style.BLUE('\u2523\u2501')} Method: {self.style.YELLOW2(method_update.method_name)}\")\n\n    # Description if available\n    if method_update.description:\n        self.print(f\"{self.style.BLUE('\u2523\u2501')} Description:\")\n        for line in method_update.description.split('\\n'):\n            self.print(f\"{self.style.BLUE('\u2503')}  {self.style.GREY(line)}\")\n\n    # Code section\n    self.print(f\"{self.style.BLUE('\u2523\u2501')} Code:\")\n    code_lines = method_update.code.split('\\n')\n    for i, line in enumerate(code_lines):\n        # Different styling for first and last lines\n        if i == 0:\n            self.print(f\"{self.style.BLUE('\u2503')}  {self.style.CYAN('\u250c\u2500')} {line}\")\n        elif i == len(code_lines) - 1:\n            self.print(f\"{self.style.BLUE('\u2503')}  {self.style.CYAN('\u2514\u2500')} {line}\")\n        else:\n            self.print(f\"{self.style.BLUE('\u2503')}  {self.style.CYAN('\u2502')} {line}\")\n\n    # Footer\n    self.print(f\"{self.style.BLUE('\u2517\u2501')} {self.style.GREY('End of method update')}\\n\")\n</code></pre> <code>print_section(title, content)</code> \u00b6 <p>Print a formatted section with title and content</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_section(self, title: str, content: str):\n    \"\"\"Print a formatted section with title and content\"\"\"\n    self.print(f\"{self.style.YELLOW('\u250c\u2500')} {self.style.YELLOW2(title)}\")\n    for line in content.split('\\n'):\n        try:\n            self.print(f\"{self.style.YELLOW('\u2502')} {line}\")\n        except Exception as e:\n            try:\n                pos = int(str(e).split('position ')[1].split('-')[0])\n                line = line[:pos] + line[pos+1:]\n                self.print(f\"{self.style.YELLOW('\u2502')} {line}\")\n            except Exception as e:\n                self.print(f\"{self.style.RED('\u2502')} UNABLE TO PRINT {str(e)}\")\n    self.print(f\"{self.style.YELLOW('\u2514\u2500')} {self.style.GREY('End of section')}\\n\")\n</code></pre> <code>print_state(state, details=None)</code> \u00b6 <p>Print current state with optional details</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_state(self, state: str, details: dict[str, Any] | None = None):\n    \"\"\"Print current state with optional details\"\"\"\n    state_color = {\n        'ACTION': self.style.GREEN2,\n        'PROCESSING': self.style.YELLOW2,\n        'BRAKE': self.style.RED2,\n        'DONE': self.style.BLUE2\n    }.get(state, self.style.WHITE2)\n    res_str = f\"\\nCurrent State: {state}\"\n    self.print(f\"\\n{self.style.Bold('Current State:')} {state_color(state)}\")\n\n    if details:\n        for key, value in details.items():\n            self.print(f\"  {self.style.GREY('\u251c\u2500')} {self.style.CYAN(key)}: {value}\")\n            res_str += f\"  \u251c\u2500 {key}: {value}\\n\"\n    return res_str\n</code></pre> <code>process_with_spinner(message, coroutine)</code> <code>async</code> \u00b6 <p>Execute a coroutine with a spinner indicator</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def process_with_spinner(self, message: str, coroutine):\n    \"\"\"Execute a coroutine with a spinner indicator\"\"\"\n    with Spinner(message, symbols=self.spinner_style):\n        result = await coroutine\n        return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.VirtualEnvContext","title":"<code>VirtualEnvContext</code>","text":"<p>Context manager for temporary virtual environment activation</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class VirtualEnvContext:\n    \"\"\"Context manager for temporary virtual environment activation\"\"\"\n\n    def __init__(self, venv_path: Path):\n        self.venv_path = venv_path\n        self._original_path = None\n        self._original_sys_path = None\n        self._original_prefix = None\n        self._original_virtual_env = None\n\n    def _get_venv_paths(self):\n        \"\"\"Get virtual environment paths based on platform\"\"\"\n        if sys.platform == 'win32':\n            site_packages = self.venv_path / 'Lib' / 'site-packages'\n            scripts_dir = self.venv_path / 'Scripts'\n            python_path = scripts_dir / 'python.exe'\n        else:\n            python_version = f'python{sys.version_info.major}.{sys.version_info.minor}'\n            site_packages = self.venv_path / 'lib' / python_version / 'site-packages'\n            scripts_dir = self.venv_path / 'bin'\n            python_path = scripts_dir / 'python'\n\n        return site_packages, scripts_dir, python_path\n\n    def __enter__(self):\n        # Save original state\n        self._original_path = os.environ.get('PATH', '')\n        self._original_sys_path = sys.path.copy()\n        self._original_prefix = sys.prefix\n        self._original_virtual_env = os.environ.get('VIRTUAL_ENV')\n\n        # Get venv paths\n        site_packages, scripts_dir, python_path = self._get_venv_paths()\n\n        # Modify environment for venv\n        if scripts_dir.exists():\n            new_path = os.pathsep.join([str(scripts_dir), self._original_path])\n            os.environ['PATH'] = new_path\n\n        if site_packages.exists():\n            sys.path.insert(0, str(site_packages))\n\n        os.environ['VIRTUAL_ENV'] = str(self.venv_path)\n\n        # Return the python executable path for potential subprocess calls\n        return str(python_path)\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Restore original state\n        os.environ['PATH'] = self._original_path\n        sys.path = self._original_sys_path\n\n        if self._original_virtual_env is None:\n            os.environ.pop('VIRTUAL_ENV', None)\n        else:\n            os.environ['VIRTUAL_ENV'] = self._original_virtual_env\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.VirtualFileSystem","title":"<code>VirtualFileSystem</code>","text":"Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class VirtualFileSystem:\n    def __init__(self, base_dir: Path):\n        self.base_dir = base_dir\n        self.current_dir = base_dir\n        self.virtual_files: dict[str, str] = {}\n        self.base_dir.mkdir(parents=True, exist_ok=True)\n\n    def write_file(self, filepath: str | Path, content: str) -&gt; Path:\n        \"\"\"Write content to a virtual file and persist to disk using UTF-8\"\"\"\n        try:\n            abs_path = self._resolve_path(filepath)\n        except ValueError:\n            print(\"invalid :\", filepath)\n            filepath = \"src/temp_js/_temp_fix.py\"\n            abs_path = self._resolve_path(filepath)\n        abs_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Store in virtual filesystem\n        rel_path = str(abs_path.relative_to(self.base_dir))\n        self.virtual_files[rel_path] = content\n\n        # Write to actual filesystem with UTF-8 encoding\n        with open(abs_path, 'w', encoding='utf-8', errors='replace') as f:\n            f.write(content)\n\n        return abs_path\n\n    def read_file(self, filepath: str | Path) -&gt; str:\n        \"\"\"Read content from a virtual file using UTF-8\"\"\"\n        abs_path = self._resolve_path(filepath)\n        if not abs_path.exists():\n            raise FileNotFoundError(f\"File not found: {filepath}\")\n\n        rel_path = str(abs_path.relative_to(self.base_dir))\n\n        # Check virtual filesystem first\n        if rel_path in self.virtual_files:\n            return self.virtual_files[rel_path]\n\n        # Fall back to reading from disk with UTF-8 encoding\n        with open(abs_path, encoding='utf-8', errors='replace') as f:\n            content = f.read()\n            self.virtual_files[rel_path] = content\n            return content\n\n    def delete_file(self, filepath: str | Path):\n        \"\"\"Delete a virtual file\"\"\"\n        abs_path = self._resolve_path(filepath)\n        rel_path = str(abs_path.relative_to(self.base_dir))\n\n        if rel_path in self.virtual_files:\n            del self.virtual_files[rel_path]\n\n        if abs_path.exists():\n            abs_path.unlink()\n\n    def create_directory(self, dirpath: str | Path):\n        \"\"\"Create a new directory\"\"\"\n        abs_path = self._resolve_path(dirpath)\n        abs_path.mkdir(parents=True, exist_ok=True)\n        return abs_path\n\n\n    def list_directory(self, dirpath: str | Path = '.') -&gt; list:\n        \"\"\"List contents of a directory\"\"\"\n        abs_path = self._resolve_path(dirpath)\n        if not abs_path.exists():\n            raise FileNotFoundError(f\"Directory not found: {dirpath}\")\n        return [p.name for p in abs_path.iterdir()]\n\n    def change_directory(self, dirpath: str | Path):\n        \"\"\"Change current working directory\"\"\"\n        new_dir = self._resolve_path(dirpath)\n        if not new_dir.exists() or not new_dir.is_dir():\n            raise NotADirectoryError(f\"Directory not found: {dirpath}\")\n        self.current_dir = new_dir\n\n    def _resolve_path(self, filepath: str | Path) -&gt; Path:\n        \"\"\"Convert relative path to absolute path\"\"\"\n        filepath = Path(filepath)\n        if filepath.is_absolute():\n            if not str(filepath).startswith(str(self.base_dir)):\n                raise ValueError(\"Path must be within base directory\")\n            return filepath\n        return (self.current_dir / filepath).resolve()\n\n    def save_state(self, state_file: Path):\n        \"\"\"Save virtual filesystem state to disk\"\"\"\n        state = {\n            'current_dir': str(self.current_dir.relative_to(self.base_dir)),\n            'virtual_files': self.virtual_files\n        }\n        with open(state_file, 'w') as f:\n            json.dump(state, f)\n\n    def load_state(self, state_file: Path):\n        \"\"\"Load virtual filesystem state from disk\"\"\"\n        if not state_file.exists():\n            return\n\n        with open(state_file) as f:\n            state = json.load(f)\n            self.current_dir = self.base_dir / state['current_dir']\n            self.virtual_files = state['virtual_files']\n\n    def print_file_structure(self, start_path: str | Path = '.', indent: str = ''):\n        \"\"\"Print the file structure starting from the given path\"\"\"\n        start_path = self._resolve_path(start_path)\n        if not start_path.exists():\n            s = f\"Path not found: {start_path}\"\n            return s\n\n        s = f\"{indent}{start_path.name}/\"\n        for item in sorted(start_path.iterdir()):\n            if item.is_dir():\n               s+= self.print_file_structure(item, indent + '  ')\n            else:\n                s = f\"{indent}  {item.name}\"\n        return s\n</code></pre> <code>change_directory(dirpath)</code> \u00b6 <p>Change current working directory</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def change_directory(self, dirpath: str | Path):\n    \"\"\"Change current working directory\"\"\"\n    new_dir = self._resolve_path(dirpath)\n    if not new_dir.exists() or not new_dir.is_dir():\n        raise NotADirectoryError(f\"Directory not found: {dirpath}\")\n    self.current_dir = new_dir\n</code></pre> <code>create_directory(dirpath)</code> \u00b6 <p>Create a new directory</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def create_directory(self, dirpath: str | Path):\n    \"\"\"Create a new directory\"\"\"\n    abs_path = self._resolve_path(dirpath)\n    abs_path.mkdir(parents=True, exist_ok=True)\n    return abs_path\n</code></pre> <code>delete_file(filepath)</code> \u00b6 <p>Delete a virtual file</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def delete_file(self, filepath: str | Path):\n    \"\"\"Delete a virtual file\"\"\"\n    abs_path = self._resolve_path(filepath)\n    rel_path = str(abs_path.relative_to(self.base_dir))\n\n    if rel_path in self.virtual_files:\n        del self.virtual_files[rel_path]\n\n    if abs_path.exists():\n        abs_path.unlink()\n</code></pre> <code>list_directory(dirpath='.')</code> \u00b6 <p>List contents of a directory</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def list_directory(self, dirpath: str | Path = '.') -&gt; list:\n    \"\"\"List contents of a directory\"\"\"\n    abs_path = self._resolve_path(dirpath)\n    if not abs_path.exists():\n        raise FileNotFoundError(f\"Directory not found: {dirpath}\")\n    return [p.name for p in abs_path.iterdir()]\n</code></pre> <code>load_state(state_file)</code> \u00b6 <p>Load virtual filesystem state from disk</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def load_state(self, state_file: Path):\n    \"\"\"Load virtual filesystem state from disk\"\"\"\n    if not state_file.exists():\n        return\n\n    with open(state_file) as f:\n        state = json.load(f)\n        self.current_dir = self.base_dir / state['current_dir']\n        self.virtual_files = state['virtual_files']\n</code></pre> <code>print_file_structure(start_path='.', indent='')</code> \u00b6 <p>Print the file structure starting from the given path</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_file_structure(self, start_path: str | Path = '.', indent: str = ''):\n    \"\"\"Print the file structure starting from the given path\"\"\"\n    start_path = self._resolve_path(start_path)\n    if not start_path.exists():\n        s = f\"Path not found: {start_path}\"\n        return s\n\n    s = f\"{indent}{start_path.name}/\"\n    for item in sorted(start_path.iterdir()):\n        if item.is_dir():\n           s+= self.print_file_structure(item, indent + '  ')\n        else:\n            s = f\"{indent}  {item.name}\"\n    return s\n</code></pre> <code>read_file(filepath)</code> \u00b6 <p>Read content from a virtual file using UTF-8</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def read_file(self, filepath: str | Path) -&gt; str:\n    \"\"\"Read content from a virtual file using UTF-8\"\"\"\n    abs_path = self._resolve_path(filepath)\n    if not abs_path.exists():\n        raise FileNotFoundError(f\"File not found: {filepath}\")\n\n    rel_path = str(abs_path.relative_to(self.base_dir))\n\n    # Check virtual filesystem first\n    if rel_path in self.virtual_files:\n        return self.virtual_files[rel_path]\n\n    # Fall back to reading from disk with UTF-8 encoding\n    with open(abs_path, encoding='utf-8', errors='replace') as f:\n        content = f.read()\n        self.virtual_files[rel_path] = content\n        return content\n</code></pre> <code>save_state(state_file)</code> \u00b6 <p>Save virtual filesystem state to disk</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def save_state(self, state_file: Path):\n    \"\"\"Save virtual filesystem state to disk\"\"\"\n    state = {\n        'current_dir': str(self.current_dir.relative_to(self.base_dir)),\n        'virtual_files': self.virtual_files\n    }\n    with open(state_file, 'w') as f:\n        json.dump(state, f)\n</code></pre> <code>write_file(filepath, content)</code> \u00b6 <p>Write content to a virtual file and persist to disk using UTF-8</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def write_file(self, filepath: str | Path, content: str) -&gt; Path:\n    \"\"\"Write content to a virtual file and persist to disk using UTF-8\"\"\"\n    try:\n        abs_path = self._resolve_path(filepath)\n    except ValueError:\n        print(\"invalid :\", filepath)\n        filepath = \"src/temp_js/_temp_fix.py\"\n        abs_path = self._resolve_path(filepath)\n    abs_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Store in virtual filesystem\n    rel_path = str(abs_path.relative_to(self.base_dir))\n    self.virtual_files[rel_path] = content\n\n    # Write to actual filesystem with UTF-8 encoding\n    with open(abs_path, 'w', encoding='utf-8', errors='replace') as f:\n        f.write(content)\n\n    return abs_path\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.WebContentParser","title":"<code>WebContentParser</code>","text":"<p>Parser for extracting content from web pages in various formats.</p> <p>Provides methods to extract content as markdown, plain text, structured data, and take screenshots with scrolling support.</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class WebContentParser:\n    \"\"\"\n    Parser for extracting content from web pages in various formats.\n\n    Provides methods to extract content as markdown, plain text,\n    structured data, and take screenshots with scrolling support.\n    \"\"\"\n\n    def __init__(self, browser_wrapper):\n        \"\"\"Initialize the parser with a browser wrapper instance\"\"\"\n        self.browser = browser_wrapper\n\n    async def to_markdown(self, page=None, selector=\"main, article, #content, .content, body\",\n                          include_images=True):\n        \"\"\"\n        Convert webpage content to markdown format\n\n        Args:\n            page: The page to parse (uses current page if None)\n            selector: CSS selector for the content to extract\n            include_images: Whether to include image references\n\n        Returns:\n            str: Markdown content\n        \"\"\"\n        return await self.browser.extract_markdown(page, selector, include_images)\n\n    async def to_text(self, page=None, selector=\"body\"):\n        \"\"\"Extract plain text from webpage\"\"\"\n        return await self.browser.extract_text(page, selector)\n\n    async def to_structured(self, page=None, config=None):\n        \"\"\"Extract structured data from webpage using selector configuration\"\"\"\n        return await self.browser.extract_structured_content(page, config)\n\n    async def to_screenshot(self, page=None, full_page=True, path=None,\n                            initial_delay=1000, scroll_delay=500, format='png'):\n        \"\"\"\n        Take a screenshot with scrolling functionality\n\n        Args:\n            page: The page to screenshot\n            full_page: Whether to capture the full page\n            path: Path to save the screenshot\n            initial_delay: Delay in ms before starting screenshot\n            scroll_delay: Delay in ms between scrolls\n            format: Image format ('png' or 'jpeg')\n        \"\"\"\n        return await self.browser.take_scrolling_screenshot(\n            page, full_page, path, initial_delay, scroll_delay, format\n        )\n\n    async def extract_all(self, page=None, selector=\"body\", include_images=True,\n                          screenshot=True, screenshot_path=None):\n        \"\"\"Extract all content types (markdown, text, structured data, screenshot)\"\"\"\n        result = {\n            'markdown': await self.to_markdown(page, selector, include_images),\n            'text': await self.to_text(page, selector),\n            'structured': await self.to_structured(page)\n        }\n\n        if screenshot:\n            result['screenshot'] = await self.to_screenshot(\n                page, path=screenshot_path, initial_delay=1000\n            )\n\n        return result\n</code></pre> <code>__init__(browser_wrapper)</code> \u00b6 <p>Initialize the parser with a browser wrapper instance</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __init__(self, browser_wrapper):\n    \"\"\"Initialize the parser with a browser wrapper instance\"\"\"\n    self.browser = browser_wrapper\n</code></pre> <code>extract_all(page=None, selector='body', include_images=True, screenshot=True, screenshot_path=None)</code> <code>async</code> \u00b6 <p>Extract all content types (markdown, text, structured data, screenshot)</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def extract_all(self, page=None, selector=\"body\", include_images=True,\n                      screenshot=True, screenshot_path=None):\n    \"\"\"Extract all content types (markdown, text, structured data, screenshot)\"\"\"\n    result = {\n        'markdown': await self.to_markdown(page, selector, include_images),\n        'text': await self.to_text(page, selector),\n        'structured': await self.to_structured(page)\n    }\n\n    if screenshot:\n        result['screenshot'] = await self.to_screenshot(\n            page, path=screenshot_path, initial_delay=1000\n        )\n\n    return result\n</code></pre> <code>to_markdown(page=None, selector='main, article, #content, .content, body', include_images=True)</code> <code>async</code> \u00b6 <p>Convert webpage content to markdown format</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <p>The page to parse (uses current page if None)</p> <code>None</code> <code>selector</code> <p>CSS selector for the content to extract</p> <code>'main, article, #content, .content, body'</code> <code>include_images</code> <p>Whether to include image references</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <p>Markdown content</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def to_markdown(self, page=None, selector=\"main, article, #content, .content, body\",\n                      include_images=True):\n    \"\"\"\n    Convert webpage content to markdown format\n\n    Args:\n        page: The page to parse (uses current page if None)\n        selector: CSS selector for the content to extract\n        include_images: Whether to include image references\n\n    Returns:\n        str: Markdown content\n    \"\"\"\n    return await self.browser.extract_markdown(page, selector, include_images)\n</code></pre> <code>to_screenshot(page=None, full_page=True, path=None, initial_delay=1000, scroll_delay=500, format='png')</code> <code>async</code> \u00b6 <p>Take a screenshot with scrolling functionality</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <p>The page to screenshot</p> <code>None</code> <code>full_page</code> <p>Whether to capture the full page</p> <code>True</code> <code>path</code> <p>Path to save the screenshot</p> <code>None</code> <code>initial_delay</code> <p>Delay in ms before starting screenshot</p> <code>1000</code> <code>scroll_delay</code> <p>Delay in ms between scrolls</p> <code>500</code> <code>format</code> <p>Image format ('png' or 'jpeg')</p> <code>'png'</code> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def to_screenshot(self, page=None, full_page=True, path=None,\n                        initial_delay=1000, scroll_delay=500, format='png'):\n    \"\"\"\n    Take a screenshot with scrolling functionality\n\n    Args:\n        page: The page to screenshot\n        full_page: Whether to capture the full page\n        path: Path to save the screenshot\n        initial_delay: Delay in ms before starting screenshot\n        scroll_delay: Delay in ms between scrolls\n        format: Image format ('png' or 'jpeg')\n    \"\"\"\n    return await self.browser.take_scrolling_screenshot(\n        page, full_page, path, initial_delay, scroll_delay, format\n    )\n</code></pre> <code>to_structured(page=None, config=None)</code> <code>async</code> \u00b6 <p>Extract structured data from webpage using selector configuration</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def to_structured(self, page=None, config=None):\n    \"\"\"Extract structured data from webpage using selector configuration\"\"\"\n    return await self.browser.extract_structured_content(page, config)\n</code></pre> <code>to_text(page=None, selector='body')</code> <code>async</code> \u00b6 <p>Extract plain text from webpage</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def to_text(self, page=None, selector=\"body\"):\n    \"\"\"Extract plain text from webpage\"\"\"\n    return await self.browser.extract_text(page, selector)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.auto_install","title":"<code>auto_install(package_name, install_method='pip', upgrade=False, quiet=False, version=None, extra_args=None)</code>","text":"<p>Enhanced auto-save import with version and extra arguments support</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def auto_install(package_name, install_method='pip', upgrade=False, quiet=False, version=None, extra_args=None):\n    '''\n    Enhanced auto-save import with version and extra arguments support\n    '''\n    try:\n        # Attempt to import the package\n        return importlib.import_module(package_name)\n    except ImportError:\n        # Package not found, prepare for installation\n        print(f\"Package '{package_name}' not found. Attempting to install...\")\n        try:\n            # Determine Python executable based on virtual environment\n            venv_path = os.environ.get('VIRTUAL_ENV')\n            if venv_path:\n                venv_path = Path(venv_path)\n                if sys.platform == 'win32':\n                    python_exec = str(venv_path / 'Scripts' / 'python.exe')\n                else:\n                    python_exec = str(venv_path / 'bin' / 'python')\n                # Check if the Python executable exists\n                if not Path(python_exec).exists():\n                    python_exec = sys.executable\n            else:\n                python_exec = sys.executable\n\n            # Construct installation command with more flexibility\n            install_cmd = [python_exec, \"-m\", install_method, \"install\"]\n            if upgrade:\n                install_cmd.append(\"--upgrade\")\n            # Support specific version installation\n            if version:\n                install_cmd.append(f\"{package_name}=={version}\")\n            else:\n                install_cmd.append(package_name)\n            # Add extra arguments if provided\n            if extra_args:\n                install_cmd.extend(extra_args)\n            # Run installation with appropriate verbosity\n            installation_output = subprocess.run(\n                install_cmd,\n                capture_output=quiet,\n                text=True\n            )\n            # Check installation status\n            if installation_output.returncode == 0:\n                print(f\"Successfully installed {package_name}\")\n                return importlib.import_module(package_name)\n            else:\n                raise Exception(f\"Installation failed: {installation_output.stderr}\")\n        except Exception as install_error:\n            print(f\"Error installing {package_name}: {install_error}\")\n            return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.sync_globals_to_vars","title":"<code>sync_globals_to_vars(pipeline, namespace=None, prefix=None, include_types=None, exclude_patterns=None, exclude_private=True, deep_copy=False, only_serializable=False)</code>","text":"<pre><code>Sync global variables or a specific namespace to pipeline variables.\n\nArgs:\n    pipeline: Pipeline instance to sync variables to\n    namespace: Optional dictionary of variables (defaults to globals())\n    prefix: Optional prefix for variable names (e.g., 'global_')\n    include_types: Only include variables of these types\n    exclude_patterns: List of regex patterns to exclude\n    exclude_private: Exclude variables starting with underscore\n    deep_copy: Create deep copies of variables instead of references\n    only_serializable: Only include variables that can be serialized\n\nReturns:\n    SyncReport with details about added, skipped and error variables\n\nUsage example:\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.sync_globals_to_vars--basic-usage-sync-all-globals","title":"Basic usage - sync all globals","text":"<p>report = sync_globals_to_vars(pipeline)</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.sync_globals_to_vars--sync-only-numeric-types-with-prefix","title":"Sync only numeric types with prefix","text":"<p>report = sync_globals_to_vars(     pipeline,     include_types=[int, float],     prefix=\"global_\" )</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.sync_globals_to_vars--sync-from-specific-namespace","title":"Sync from specific namespace","text":"<p>import numpy as np namespace = {\"arr\": np.array([1,2,3])} report = sync_globals_to_vars(pipeline, namespace=namespace)</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.sync_globals_to_vars--sync-with-deep-copy-and-serialization-check","title":"Sync with deep copy and serialization check","text":"<p>report = sync_globals_to_vars(     pipeline,     deep_copy=True,     only_serializable=True )</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def sync_globals_to_vars(\n    pipeline: Any,\n    namespace: dict[str, Any] | None = None,\n    prefix: str | None = None,\n    include_types: type | list[type] | None = None,\n    exclude_patterns: list[str] | None = None,\n    exclude_private: bool = True,\n    deep_copy: bool = False,\n    only_serializable: bool = False\n) -&gt; SyncReport:\n    \"\"\"\n    Sync global variables or a specific namespace to pipeline variables.\n\n    Args:\n        pipeline: Pipeline instance to sync variables to\n        namespace: Optional dictionary of variables (defaults to globals())\n        prefix: Optional prefix for variable names (e.g., 'global_')\n        include_types: Only include variables of these types\n        exclude_patterns: List of regex patterns to exclude\n        exclude_private: Exclude variables starting with underscore\n        deep_copy: Create deep copies of variables instead of references\n        only_serializable: Only include variables that can be serialized\n\n    Returns:\n        SyncReport with details about added, skipped and error variables\n\n    Usage example:\n# Basic usage - sync all globals\nreport = sync_globals_to_vars(pipeline)\n\n# Sync only numeric types with prefix\nreport = sync_globals_to_vars(\n    pipeline,\n    include_types=[int, float],\n    prefix=\"global_\"\n)\n\n# Sync from specific namespace\nimport numpy as np\nnamespace = {\"arr\": np.array([1,2,3])}\nreport = sync_globals_to_vars(pipeline, namespace=namespace)\n\n# Sync with deep copy and serialization check\nreport = sync_globals_to_vars(\n    pipeline,\n    deep_copy=True,\n    only_serializable=True\n)\n    \"\"\"\n    # Initialize report\n    report = SyncReport(\n        added={},\n        skipped={},\n        errors={}\n    )\n\n    # Get namespace\n    if namespace is None:\n        # Get caller's globals\n        namespace = currentframe().f_back.f_globals\n\n    # Compile exclude patterns\n    if exclude_patterns:\n        patterns = [re.compile(pattern) for pattern in exclude_patterns]\n    else:\n        patterns = []\n\n    # Normalize include_types\n    if include_types and not isinstance(include_types, list | tuple | set):\n        include_types = [include_types]\n    def get_type_info(var: Any) -&gt; str:\n        \"\"\"Helper to get detailed type information\"\"\"\n        if isinstance(var, type):\n            return f\"class '{var.__name__}'\"\n        elif isinstance(var, BaseModel):\n            return f\"Pydantic model '{var.__class__.__name__}'\"\n        elif hasattr(var, '__class__'):\n            type_name = var.__class__.__name__\n            module_name = var.__class__.__module__\n            if module_name != 'builtins':\n                return f\"{module_name}.{type_name}\"\n            return type_name\n        return type(var).__name__\n    # Process each variable\n    for name, value in namespace.items():\n        try:\n            # Skip if matches exclude criteria\n            if exclude_private and name.startswith('_'):\n                report.skipped[name] = \"private variable\"\n                continue\n\n            if any(pattern.match(name) for pattern in patterns):\n                report.skipped[name] = \"matched exclude pattern\"\n                continue\n\n            if include_types and not isinstance(value, tuple(include_types)):\n                report.skipped[name] = f\"type {type(value).__name__} not in include_types\"\n                continue\n\n            # Test serialization if required\n            if only_serializable:\n                try:\n                    import pickle\n                    pickle.dumps(value)\n                except Exception as e:\n                    report.skipped[name] = f\"not serializable: {str(e)}\"\n                    continue\n\n            # Prepare variable\n            var_value = deepcopy(value) if deep_copy else value\n            var_name = f\"{prefix}{name}\" if prefix else name\n\n            # Add to pipeline variables\n            pipeline.variables[var_name] = var_value\n            report.added[var_name] = get_type_info(value)\n\n        except Exception as e:\n            report.errors[name] = str(e)\n\n    return report\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.parser","title":"<code>parser</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.parser.CodeProcessor","title":"<code>CodeProcessor</code>","text":"Source code in <code>toolboxv2/mods/isaa/CodingAgent/parser.py</code> <pre><code>class CodeProcessor:\n    def __init__(self, code_base='./'):\n        self.language_patterns = [\n            r'```([\\w-]+)\\n((?:#|//|&lt;!--)\\s*(\\S+))?\\n([\\s\\S]*?)```',  # Standard pattern\n            r'```([\\w-]+)\\s*\\n([\\s\\S]*?)```'  # Pattern without filename comment\n        ]\n        self.code_base = code_base\n\n    def extract_code(self, text):\n        code_blocks = {}\n        seen = set()\n        for pattern in self.language_patterns:\n            matches = re.finditer(pattern, text, re.DOTALL)\n            for match in matches:\n\n                print(match.groups())\n\n                if len(match.groups()) &lt; 3:\n                    continue\n\n                code = match.groups()[3]\n                filename = match.groups()[2]\n\n                if code == code_blocks.get(filename):\n                    continue\n\n                if code_blocks.get(filename) is not None and code != code_blocks.get(filename):\n                    comment_prfix = match.groups()[1].replace(filename, '')\n                    filename = code.split('\\n')[0].replace(comment_prfix, '')\n                    code = code.replace(comment_prfix + filename + '\\n', '')\n\n                    print(\"new code\", code)\n\n                seen.add(filename)\n\n                code_blocks[filename] = code\n        return code_blocks\n\n    def write_code(self, code_dict):\n        for filename, code in code_dict.items():\n            filepath = os.path.join(self.code_base, filename)\n            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n            print(\"Writing\", filepath)\n            with open(filepath, \"w\") as f:\n                f.write(code)\n\n    def extract_and_write_code(self, text):\n        code_blocks = self.extract_code(text)\n        files = []\n        for filename, new_code in code_blocks.items():\n            filepath = os.path.join(self.code_base, filename)\n            files.append(filepath)\n            if os.path.exists(filepath):\n                self.update_existing_file(filepath, new_code)\n            else:\n                self.write_code({filename: new_code})\n        return files\n\n    def update_existing_file(self, filepath, new_code):\n        \"\"\"\n            Update an existing Python file with new code while preserving existing implementations.\n\n            Args:\n                filepath (str): Path to the file to be updated\n                new_code (str): New code to merge with existing code\n            \"\"\"\n        try:\n            # Read existing code\n            with open(filepath) as f:\n                existing_code = f.read()\n\n            # Parse existing and new code\n            existing_ast_tree = ast.parse(existing_code)\n            new_ast_tree = ast.parse(new_code)\n\n            # Create updater and transform the AST\n            updater = CodeUpdater(existing_ast_tree)\n            updated_ast = updater.visit(new_ast_tree)\n\n            # Convert AST back to source code\n            updated_code = astor.to_source(updated_ast)\n\n            # Write updated code back to file\n            with open(filepath, 'w') as f:\n                f.write(updated_code)\n\n            print(f\"Successfully updated {filepath}\")\n            return True\n\n        except Exception as e:\n            print(f\"Error updating {filepath}: {e}\")\n            return False\n</code></pre> <code>update_existing_file(filepath, new_code)</code> \u00b6 <p>Update an existing Python file with new code while preserving existing implementations.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the file to be updated</p> required <code>new_code</code> <code>str</code> <p>New code to merge with existing code</p> required Source code in <code>toolboxv2/mods/isaa/CodingAgent/parser.py</code> <pre><code>def update_existing_file(self, filepath, new_code):\n    \"\"\"\n        Update an existing Python file with new code while preserving existing implementations.\n\n        Args:\n            filepath (str): Path to the file to be updated\n            new_code (str): New code to merge with existing code\n        \"\"\"\n    try:\n        # Read existing code\n        with open(filepath) as f:\n            existing_code = f.read()\n\n        # Parse existing and new code\n        existing_ast_tree = ast.parse(existing_code)\n        new_ast_tree = ast.parse(new_code)\n\n        # Create updater and transform the AST\n        updater = CodeUpdater(existing_ast_tree)\n        updated_ast = updater.visit(new_ast_tree)\n\n        # Convert AST back to source code\n        updated_code = astor.to_source(updated_ast)\n\n        # Write updated code back to file\n        with open(filepath, 'w') as f:\n            f.write(updated_code)\n\n        print(f\"Successfully updated {filepath}\")\n        return True\n\n    except Exception as e:\n        print(f\"Error updating {filepath}: {e}\")\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster","title":"<code>SearchAgentCluster</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool","title":"<code>search_tool</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool.WebContentParser","title":"<code>WebContentParser</code>","text":"<p>Utility class for parsing web content using BrowserAnt</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>class WebContentParser:\n    \"\"\"Utility class for parsing web content using BrowserAnt\"\"\"\n\n    def __init__(self, browser_wrapper: BrowserWrapper):\n        \"\"\"Initialize with a browser wrapper\"\"\"\n        self.browser_wrapper = browser_wrapper\n\n    async def extract_article(self, url: str) -&gt; dict[str, Any]:\n        \"\"\"Extract article content with title, text, and metadata\"\"\"\n        await self.browser_wrapper.initialize()\n        page = await self.browser_wrapper.navigate(url)\n\n        # Execute readability.js to extract article content\n        readability_js = \"\"\"\n        function extractArticle() {\n            // Simple article extraction logic\n            const article = {\n                title: document.title,\n                byline: '',\n                content: '',\n                textContent: '',\n                excerpt: '',\n                siteName: '',\n                publishedTime: ''\n            };\n\n            // Try to find article elements\n            const articleElement = document.querySelector('article') ||\n                                   document.querySelector('main') ||\n                                   document.querySelector('.post-content') ||\n                                   document.querySelector('.entry-content');\n\n            if (articleElement) {\n                article.content = articleElement.innerHTML;\n                article.textContent = articleElement.textContent;\n            } else {\n                // Fallback to body content\n                article.content = document.body.innerHTML;\n                article.textContent = document.body.textContent;\n            }\n\n            // Try to extract metadata\n            const metaTags = document.querySelectorAll('meta');\n            metaTags.forEach(tag =&gt; {\n                const property = tag.getAttribute('property') || tag.getAttribute('name');\n                const content = tag.getAttribute('content');\n\n                if (property &amp;&amp; content) {\n                    if (property === 'og:site_name') article.siteName = content;\n                    if (property === 'og:title' &amp;&amp; !article.title) article.title = content;\n                    if (property === 'og:description' &amp;&amp; !article.excerpt) article.excerpt = content;\n                    if (property === 'article:published_time') article.publishedTime = content;\n                    if (property === 'author' || property === 'article:author') article.byline = content;\n                }\n            });\n\n            // Extract first paragraph as excerpt if not found\n            if (!article.excerpt) {\n                const paragraphs = document.querySelectorAll('p');\n                if (paragraphs.length &gt; 0) {\n                    for (let i = 0; i &lt; paragraphs.length; i++) {\n                        const text = paragraphs[i].textContent.trim();\n                        if (text.length &gt; 50) {\n                            article.excerpt = text;\n                            break;\n                        }\n                    }\n                }\n            }\n\n            return article;\n        }\n\n        return extractArticle();\n        \"\"\"\n\n        # Extract article content\n        article = await page.evaluate(readability_js)\n\n        # Add markdown version\n        article['markdown'] = await self.browser_wrapper.extract_markdown(page)\n\n        # Take a screenshot\n        screenshot_data = await self.browser_wrapper.take_scrolling_screenshot(page)\n        article['screenshot'] = base64.b64encode(screenshot_data).decode('utf-8')\n\n        return article\n\n    async def extract_table_data(self, url: str, table_selector: str = 'table') -&gt; list[dict[str, Any]]:\n        \"\"\"Extract tabular data from a webpage\"\"\"\n        await self.browser_wrapper.initialize()\n        page = await self.browser_wrapper.navigate(url)\n\n        # Script to extract table data\n        extract_table_js = \"\"\"\n        (tableSelector) =&gt; {\n            const tables = document.querySelectorAll(tableSelector);\n            if (tables.length === 0) return [];\n\n            // Use the first table found\n            const table = tables[0];\n            const headers = Array.from(table.querySelectorAll('th')).map(th =&gt; th.textContent.trim());\n\n            // If no headers found, try using the first row\n            const headerRow = headers.length &gt; 0 ? headers :\n                            Array.from(table.querySelectorAll('tr:first-child td')).map(td =&gt; td.textContent.trim());\n\n            const rows = Array.from(table.querySelectorAll('tr'));\n            const result = [];\n\n            // Start from 1 if we have headers, otherwise from 0\n            const startIdx = headers.length &gt; 0 ? 1 : 0;\n\n            for (let i = startIdx; i &lt; rows.length; i++) {\n                const row = rows[i];\n                const cells = Array.from(row.querySelectorAll('td')).map(td =&gt; td.textContent.trim());\n\n                if (cells.length &gt; 0) {\n                    const rowData = {};\n                    for (let j = 0; j &lt; Math.min(headerRow.length, cells.length); j++) {\n                        // Create a valid object key from header\n                        const key = headerRow[j].replace(/[^a-zA-Z0-9]/g, '_').toLowerCase();\n                        rowData[key] = cells[j];\n                    }\n                    result.push(rowData);\n                }\n            }\n\n            return result;\n        }\n        \"\"\"\n\n        # Extract data\n        table_data = await page.evaluate(extract_table_js, table_selector)\n        return table_data\n\n    async def extract_links(self, url: str, link_selector: str = 'a') -&gt; list[dict[str, str]]:\n        \"\"\"Extract all links from a webpage\"\"\"\n        await self.browser_wrapper.initialize()\n        page = await self.browser_wrapper.navigate(url)\n\n        # Script to extract links\n        extract_links_js = \"\"\"\n        (linkSelector) =&gt; {\n            const links = Array.from(document.querySelectorAll(linkSelector));\n            return links.map(link =&gt; {\n                return {\n                    text: link.textContent.trim(),\n                    href: link.href,\n                    title: link.getAttribute('title') || '',\n                    isExternal: link.hostname !== window.location.hostname\n                };\n            }).filter(link =&gt; link.href &amp;&amp; link.href.startsWith('http'));\n        }\n        \"\"\"\n\n        # Extract links\n        links = await page.evaluate(extract_links_js, link_selector)\n        return links\n</code></pre> <code>__init__(browser_wrapper)</code> \u00b6 <p>Initialize with a browser wrapper</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>def __init__(self, browser_wrapper: BrowserWrapper):\n    \"\"\"Initialize with a browser wrapper\"\"\"\n    self.browser_wrapper = browser_wrapper\n</code></pre> <code>extract_article(url)</code> <code>async</code> \u00b6 <p>Extract article content with title, text, and metadata</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def extract_article(self, url: str) -&gt; dict[str, Any]:\n    \"\"\"Extract article content with title, text, and metadata\"\"\"\n    await self.browser_wrapper.initialize()\n    page = await self.browser_wrapper.navigate(url)\n\n    # Execute readability.js to extract article content\n    readability_js = \"\"\"\n    function extractArticle() {\n        // Simple article extraction logic\n        const article = {\n            title: document.title,\n            byline: '',\n            content: '',\n            textContent: '',\n            excerpt: '',\n            siteName: '',\n            publishedTime: ''\n        };\n\n        // Try to find article elements\n        const articleElement = document.querySelector('article') ||\n                               document.querySelector('main') ||\n                               document.querySelector('.post-content') ||\n                               document.querySelector('.entry-content');\n\n        if (articleElement) {\n            article.content = articleElement.innerHTML;\n            article.textContent = articleElement.textContent;\n        } else {\n            // Fallback to body content\n            article.content = document.body.innerHTML;\n            article.textContent = document.body.textContent;\n        }\n\n        // Try to extract metadata\n        const metaTags = document.querySelectorAll('meta');\n        metaTags.forEach(tag =&gt; {\n            const property = tag.getAttribute('property') || tag.getAttribute('name');\n            const content = tag.getAttribute('content');\n\n            if (property &amp;&amp; content) {\n                if (property === 'og:site_name') article.siteName = content;\n                if (property === 'og:title' &amp;&amp; !article.title) article.title = content;\n                if (property === 'og:description' &amp;&amp; !article.excerpt) article.excerpt = content;\n                if (property === 'article:published_time') article.publishedTime = content;\n                if (property === 'author' || property === 'article:author') article.byline = content;\n            }\n        });\n\n        // Extract first paragraph as excerpt if not found\n        if (!article.excerpt) {\n            const paragraphs = document.querySelectorAll('p');\n            if (paragraphs.length &gt; 0) {\n                for (let i = 0; i &lt; paragraphs.length; i++) {\n                    const text = paragraphs[i].textContent.trim();\n                    if (text.length &gt; 50) {\n                        article.excerpt = text;\n                        break;\n                    }\n                }\n            }\n        }\n\n        return article;\n    }\n\n    return extractArticle();\n    \"\"\"\n\n    # Extract article content\n    article = await page.evaluate(readability_js)\n\n    # Add markdown version\n    article['markdown'] = await self.browser_wrapper.extract_markdown(page)\n\n    # Take a screenshot\n    screenshot_data = await self.browser_wrapper.take_scrolling_screenshot(page)\n    article['screenshot'] = base64.b64encode(screenshot_data).decode('utf-8')\n\n    return article\n</code></pre> <code>extract_links(url, link_selector='a')</code> <code>async</code> \u00b6 <p>Extract all links from a webpage</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def extract_links(self, url: str, link_selector: str = 'a') -&gt; list[dict[str, str]]:\n    \"\"\"Extract all links from a webpage\"\"\"\n    await self.browser_wrapper.initialize()\n    page = await self.browser_wrapper.navigate(url)\n\n    # Script to extract links\n    extract_links_js = \"\"\"\n    (linkSelector) =&gt; {\n        const links = Array.from(document.querySelectorAll(linkSelector));\n        return links.map(link =&gt; {\n            return {\n                text: link.textContent.trim(),\n                href: link.href,\n                title: link.getAttribute('title') || '',\n                isExternal: link.hostname !== window.location.hostname\n            };\n        }).filter(link =&gt; link.href &amp;&amp; link.href.startsWith('http'));\n    }\n    \"\"\"\n\n    # Extract links\n    links = await page.evaluate(extract_links_js, link_selector)\n    return links\n</code></pre> <code>extract_table_data(url, table_selector='table')</code> <code>async</code> \u00b6 <p>Extract tabular data from a webpage</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def extract_table_data(self, url: str, table_selector: str = 'table') -&gt; list[dict[str, Any]]:\n    \"\"\"Extract tabular data from a webpage\"\"\"\n    await self.browser_wrapper.initialize()\n    page = await self.browser_wrapper.navigate(url)\n\n    # Script to extract table data\n    extract_table_js = \"\"\"\n    (tableSelector) =&gt; {\n        const tables = document.querySelectorAll(tableSelector);\n        if (tables.length === 0) return [];\n\n        // Use the first table found\n        const table = tables[0];\n        const headers = Array.from(table.querySelectorAll('th')).map(th =&gt; th.textContent.trim());\n\n        // If no headers found, try using the first row\n        const headerRow = headers.length &gt; 0 ? headers :\n                        Array.from(table.querySelectorAll('tr:first-child td')).map(td =&gt; td.textContent.trim());\n\n        const rows = Array.from(table.querySelectorAll('tr'));\n        const result = [];\n\n        // Start from 1 if we have headers, otherwise from 0\n        const startIdx = headers.length &gt; 0 ? 1 : 0;\n\n        for (let i = startIdx; i &lt; rows.length; i++) {\n            const row = rows[i];\n            const cells = Array.from(row.querySelectorAll('td')).map(td =&gt; td.textContent.trim());\n\n            if (cells.length &gt; 0) {\n                const rowData = {};\n                for (let j = 0; j &lt; Math.min(headerRow.length, cells.length); j++) {\n                    // Create a valid object key from header\n                    const key = headerRow[j].replace(/[^a-zA-Z0-9]/g, '_').toLowerCase();\n                    rowData[key] = cells[j];\n                }\n                result.push(rowData);\n            }\n        }\n\n        return result;\n    }\n    \"\"\"\n\n    # Extract data\n    table_data = await page.evaluate(extract_table_js, table_selector)\n    return table_data\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool.WebScraper","title":"<code>WebScraper</code>","text":"<pre><code>A high-performance web scraper using BrowserAnt with multi-tab parallel processing.\nHandles both structured and unstructured data collection efficiently.\n</code></pre> <p>import asyncio from pydantic import BaseModel, Field from typing import List, Optional</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool.WebScraper--define-a-structured-data-model","title":"Define a structured data model","text":"<p>class ProductInfo(BaseModel):     title: str     price: str     description: Optional[str] = None     rating: Optional[str] = None     availability: Optional[str] = None</p> <p>async def main():     # Initialize the scraper     scraper = WebScraper()</p> <pre><code># Example 1: Simple scraping of a single URL\nresult = await scraper.scrape_url(\"https://example.com\")\nprint(f\"Title: {result['title']}\")\nprint(f\"Content: {result['markdown'][:200]}...\")\n\n# Example 2: Parallel scraping of multiple URLs\nurls = [\n    \"https://example.com/page1\",\n    \"https://example.com/page2\",\n    \"https://example.com/page3\"\n]\nresults = await scraper.scrape_urls(urls)\n\n# Example 3: Structured data extraction\nproducts = await scraper.scrape_structured_data(\n    urls=[\"https://example.com/product1\", \"https://example.com/product2\"],\n    model=ProductInfo,\n    extraction_task=\"Extract product information including title, price, and availability status\"\n)\n\nfor product in products:\n    if product:\n        print(f\"Product: {product.title}, Price: {product.price}\")\n\n# Clean up\nawait scraper.close()\n</code></pre> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>class WebScraper:\n    \"\"\"\n    A high-performance web scraper using BrowserAnt with multi-tab parallel processing.\n    Handles both structured and unstructured data collection efficiently.\nimport asyncio\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\n\n# Define a structured data model\nclass ProductInfo(BaseModel):\n    title: str\n    price: str\n    description: Optional[str] = None\n    rating: Optional[str] = None\n    availability: Optional[str] = None\n\nasync def main():\n    # Initialize the scraper\n    scraper = WebScraper()\n\n    # Example 1: Simple scraping of a single URL\n    result = await scraper.scrape_url(\"https://example.com\")\n    print(f\"Title: {result['title']}\")\n    print(f\"Content: {result['markdown'][:200]}...\")\n\n    # Example 2: Parallel scraping of multiple URLs\n    urls = [\n        \"https://example.com/page1\",\n        \"https://example.com/page2\",\n        \"https://example.com/page3\"\n    ]\n    results = await scraper.scrape_urls(urls)\n\n    # Example 3: Structured data extraction\n    products = await scraper.scrape_structured_data(\n        urls=[\"https://example.com/product1\", \"https://example.com/product2\"],\n        model=ProductInfo,\n        extraction_task=\"Extract product information including title, price, and availability status\"\n    )\n\n    for product in products:\n        if product:\n            print(f\"Product: {product.title}, Price: {product.price}\")\n\n    # Clean up\n    await scraper.close()\n    \"\"\"\n\n    def __init__(\n        self,\n        config: WebScraperConfig = WebScraperConfig(),\n        llm: str | BaseChatModel | None = None,\n        chrome_path: str | None = None,\n        remote_url: str | None = None,\n        browser_config: dict[str, Any] | None = None\n    ):\n        \"\"\"\n        Initialize the web scraper with configuration.\n\n        Args:\n            config: Configuration for scraper behavior\n            llm: Language model for intelligent data extraction\n            chrome_path: Path to Chrome executable\n            remote_url: URL for remote browser connection\n            browser_config: Additional browser configuration\n        \"\"\"\n        self.config = config\n        self.browser_wrapper = BrowserWrapper(\n            llm=llm,\n            headless=config.headless,\n            chrome_path=chrome_path,\n            remote_url=remote_url,\n            config=browser_config\n        )\n        self.active_tasks = set()\n        self._semaphore = asyncio.Semaphore(config.max_concurrent_tabs)\n        self._results = {}\n\n        # Create screenshot directory if needed\n        if config.save_screenshots and not os.path.exists(config.screenshot_dir):\n            os.makedirs(config.screenshot_dir)\n\n    async def initialize(self):\n        \"\"\"Initialize the browser if not already initialized\"\"\"\n        await self.browser_wrapper.initialize()\n\n    async def close(self):\n        \"\"\"Close the browser and clean up resources\"\"\"\n        # Wait for all active tasks to complete\n        if self.active_tasks:\n            await asyncio.gather(*self.active_tasks)\n        await self.browser_wrapper.close()\n\n\n    # Add this method to your WebScraper class\n    async def search_web(\n        self,\n        query: str,\n        max_results: int = 5,\n        include_content: bool = True,\n        extract_images: bool = False,\n        extract_tables: bool = False,\n        extract_links: bool = False,\n        save_to_file: str | None = None\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Perform a comprehensive web search and return high-quality data for the given query.\n\n        Args:\n            query: Search query string\n            max_results: Maximum number of results to process (default: 5)\n            include_content: Whether to include full content from result pages (default: True)\n            extract_images: Whether to extract images from result pages (default: False)\n            extract_tables: Whether to extract tables from result pages (default: False)\n            extract_links: Whether to extract links from result pages (default: False)\n            save_to_file: Path to save results as JSON (optional)\n\n        Returns:\n            Dictionary containing search results and extracted information\n        \"\"\"\n        await self.initialize()\n        try:\n            start_time = datetime.now()\n\n            # Try different search engines in order\n            search_engines = [\n                {\n                    \"url\": f\"https://www.google.com/search?q={urllib.parse.quote_plus(query)}\",\n                    \"result_selector\": \".g\",\n                    \"title_selector\": \"h3\",\n                    \"link_selector\": \"a\",\n                    \"snippet_selector\": \".VwiC3b\",\n                    \"name\": \"google\"\n                },\n                {\n                    \"url\": f\"https://www.bing.com/search?q={urllib.parse.quote_plus(query)}\",\n                    \"result_selector\": \".b_algo\",\n                    \"title_selector\": \"h2\",\n                    \"link_selector\": \"a\",\n                    \"snippet_selector\": \".b_caption p\",\n                    \"name\": \"bing\"\n                },\n                {\n                    \"url\": f\"https://duckduckgo.com/?q={urllib.parse.quote_plus(query)}\",\n                    \"result_selector\": \".result\",\n                    \"title_selector\": \"h2\",\n                    \"link_selector\": \"a.result__a\",\n                    \"snippet_selector\": \".result__snippet\",\n                    \"name\": \"duckduckgo\"\n                }\n            ]\n\n            results = []\n\n            for engine in search_engines:\n                try:\n                    # Navigate to search engine\n                    page = await self.browser_wrapper.navigate(engine[\"url\"])\n                    await page.wait_for_load_state(\"networkidle\")\n                    await page.wait_for_timeout(2000)  # Wait for results to load\n\n                    # Extract search results\n                    search_results = await page.evaluate(\n                        \"\"\"\n                        (selectors) =&gt; {\n                            const results = [];\n                            const elements = document.querySelectorAll(selectors.result_selector);\n\n                            for (const element of elements) {\n                                const titleElement = element.querySelector(selectors.title_selector);\n                                const linkElement = element.querySelector(selectors.link_selector);\n                                const snippetElement = element.querySelector(selectors.snippet_selector);\n\n                                if (titleElement &amp;&amp; linkElement) {\n                                    const url = linkElement.href;\n                                    // Skip non-http links and same-domain results\n                                    if (url &amp;&amp; url.startsWith('http') &amp;&amp;\n                                        !url.includes('google.com/search') &amp;&amp;\n                                        !url.includes('bing.com/search') &amp;&amp;\n                                        !url.includes('duckduckgo.com')) {\n                                        results.push({\n                                            title: titleElement.textContent.trim(),\n                                            url: url,\n                                            snippet: snippetElement ? snippetElement.textContent.trim() : '',\n                                            source: selectors.name\n                                        });\n                                    }\n                                }\n                            }\n                            return results;\n                        }\n                        \"\"\",\n                        engine\n                    )\n\n                    if search_results and len(search_results) &gt; 0:\n                        # We got results, add them and break\n                        results = search_results\n                        break\n\n                except Exception as e:\n                    print(f\"Error searching with {engine['name']}: {str(e)}\")\n                    continue  # Try next engine\n\n            # Filter and limit results\n            unique_urls = set()\n            filtered_results = []\n\n            for result in results:\n                if result['url'] not in unique_urls and len(filtered_results) &lt; max_results:\n                    unique_urls.add(result['url'])\n                    filtered_results.append(result)\n\n            results = filtered_results\n\n            # Get detailed content if requested\n            if include_content and results:\n                # Extract content from each result page\n                urls_to_scrape = [result['url'] for result in results]\n\n                # Configure what to extract\n                extract_config = {}\n                if extract_tables:\n                    extract_config['tables'] = 'table'\n                if extract_images:\n                    extract_config['images'] = 'img'\n                if extract_links:\n                    extract_config['links'] = 'a'\n\n                # Scrape all pages in parallel using our efficient multi-tab approach\n                scraped_data = await self.scrape_urls(\n                    urls_to_scrape,\n                    extract_config=extract_config if extract_config else None\n                )\n\n                # Add content to results\n                for i, result in enumerate(results):\n                    if i &lt; len(scraped_data) and 'error' not in scraped_data[i]:\n                        result['content'] = {\n                            'title': scraped_data[i].get('title', result['title']),\n                            'markdown': scraped_data[i].get('markdown', ''),\n                            'text': scraped_data[i].get('text', ''),\n                        }\n\n                        # Add structured data if available\n                        if extract_config and 'structured_data' in scraped_data[i]:\n                            structured_data = scraped_data[i]['structured_data']\n                            for key, value in structured_data.items():\n                                if value:  # Only add non-empty data\n                                    result['content'][key] = value\n\n            # Prepare final response\n            response = {\n                'query': query,\n                'timestamp': datetime.now().isoformat(),\n                'num_results': len(results),\n                'results': results,\n                'execution_time': (datetime.now() - start_time).total_seconds()\n            }\n\n            # Save to file if requested\n            if save_to_file:\n                os.makedirs(os.path.dirname(os.path.abspath(save_to_file)), exist_ok=True)\n                with open(save_to_file, 'w', encoding='utf-8') as f:\n                    json.dump(response, f, ensure_ascii=False, indent=2)\n\n            return response\n\n        finally:\n            # Make sure we clean up browser resources\n            await self.close()\n\n    async def _scrape_url(self, url: str, task_id: str, extract_config: dict[str, Any] = None):\n        \"\"\"\n        Internal method to scrape a single URL\n\n        Args:\n            url: URL to scrape\n            task_id: Unique identifier for this scraping task\n            extract_config: Configuration for what/how to extract\n        \"\"\"\n        try:\n            async with self._semaphore:\n                # Navigate to the URL\n                page = await self.browser_wrapper.navigate(url)\n\n                # Wait for network to become idle\n                await page.wait_for_load_state(\"networkidle\")\n\n                # Perform initial delay\n                if self.config.initial_delay &gt; 0:\n                    await page.wait_for_timeout(self.config.initial_delay)\n\n                # Auto-scroll if configured\n                if self.config.auto_scroll:\n                    await self._auto_scroll(page)\n\n                # Initialize result dictionary\n                result = {\n                    \"url\": url,\n                    \"title\": await page.title(),\n                    \"timestamp\": datetime.now().isoformat(),\n                }\n\n                # Take screenshot if needed\n                if self.config.save_screenshots:\n                    file_name = f\"{urlparse(url).netloc}_{task_id}.png\"\n                    screenshot_path = os.path.join(self.config.screenshot_dir, file_name)\n                    result[\"screenshot\"] = screenshot_path\n                    await self.browser_wrapper.take_scrolling_screenshot(\n                        page=page,\n                        path=screenshot_path,\n                        initial_delay=0,  # We've already waited\n                        scroll_delay=self.config.scroll_delay\n                    )\n\n                # Extract content based on configuration\n                if extract_config:\n                    result[\"structured_data\"] = await self.browser_wrapper.extract_structured_content(\n                        page=page,\n                        config=extract_config\n                    )\n\n                # Extract markdown if configured\n                if self.config.extract_markdown:\n                    result[\"markdown\"] = await self.browser_wrapper.extract_markdown(page=page)\n\n                # Extract text if configured\n                if self.config.extract_text:\n                    result[\"text\"] = await self.browser_wrapper.extract_text(page=page)\n\n                # Extract HTML if configured\n                if self.config.extract_html:\n                    result[\"html\"] = await page.content()\n\n                self._results[task_id] = result\n                return result\n\n        except Exception as e:\n            self._results[task_id] = {\"error\": str(e), \"url\": url}\n            return {\"error\": str(e), \"url\": url}\n\n    async def _auto_scroll(self, page):\n        \"\"\"Automatically scroll down the page to load lazy content\"\"\"\n        try:\n            # Get page dimensions\n            dimensions = await page.evaluate(\"\"\"\n                () =&gt; {\n                    return {\n                        width: document.documentElement.scrollWidth,\n                        height: document.documentElement.scrollHeight,\n                        windowHeight: window.innerHeight\n                    }\n                }\n            \"\"\")\n\n            # Scroll down the page gradually\n            current_position = 0\n            while current_position &lt; dimensions['height']:\n                await page.evaluate(f\"window.scrollTo(0, {current_position})\")\n                await page.wait_for_timeout(self.config.scroll_delay)\n                current_position += dimensions['windowHeight'] // 2\n\n            # Scroll back to top\n            await page.evaluate(\"window.scrollTo(0, 0)\")\n        except Exception as e:\n            print(f\"Error during auto-scroll: {e}\")\n\n    async def scrape_url(self, url: str, extract_config: dict[str, Any] = None) -&gt; dict[str, Any]:\n        \"\"\"\n        Scrape a single URL and return the results\n\n        Args:\n            url: URL to scrape\n            extract_config: Configuration for structured data extraction\n\n        Returns:\n            Dictionary containing scraped data\n        \"\"\"\n        await self.initialize()\n        task_id = f\"{len(self._results)}_{datetime.now().timestamp()}\"\n        result = await self._scrape_url(url, task_id, extract_config)\n        return result\n\n    async def scrape_urls(\n        self,\n        urls: list[str],\n        extract_config: dict[str, Any] | None = None\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Scrape multiple URLs in parallel and return all results\n\n        Args:\n            urls: List of URLs to scrape\n            extract_config: Configuration for structured data extraction\n\n        Returns:\n            List of dictionaries containing scraped data\n        \"\"\"\n        await self.initialize()\n        tasks = []\n\n        for i, url in enumerate(urls):\n            task_id = f\"{i}_{datetime.now().timestamp()}\"\n            task = asyncio.create_task(self._scrape_url(url, task_id, extract_config))\n            self.active_tasks.add(task)\n            task.add_done_callback(self.active_tasks.discard)\n            tasks.append(task)\n\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return [r if not isinstance(r, Exception) else {\"error\": str(r)} for r in results]\n\n    async def scrape_structured_data(\n        self,\n        urls: list[str],\n        model: type[T],\n        extraction_task: str = None\n    ) -&gt; list[T]:\n        \"\"\"\n        Scrape and parse structured data into pydantic models\n\n        Args:\n            urls: List of URLs to scrape\n            model: Pydantic model class for structured data\n            extraction_task: Natural language description of what to extract\n\n        Returns:\n            List of parsed data objects\n        \"\"\"\n        await self.initialize()\n\n        # Create intelligent extraction task if provided\n        if extraction_task:\n            # Create a custom system prompt for extraction\n            class ExtractionPrompt(SystemPrompt):\n                def important_rules(self) -&gt; str:\n                    existing_rules = super().important_rules()\n                    new_rules = f\"\"\"\n                    9. EXTRACTION GOAL:\n                    - Your primary goal is to extract data according to this specific task: {extraction_task}\n                    - You should carefully identify and extract the information as accurately as possible.\n                    - Focus only on relevant information that matches the specified data structure.\n                    \"\"\"\n                    return f'{existing_rules}\\n{new_rules}'\n\n            # Define the extraction task for each URL\n            tasks = []\n            for url in urls:\n                # Setup intelligent extraction for each URL\n                task = asyncio.create_task(self._run_extraction_agent(\n                    url=url,\n                    model=model,\n                    extraction_task=extraction_task,\n                    system_prompt_class=ExtractionPrompt\n                ))\n                self.active_tasks.add(task)\n                task.add_done_callback(self.active_tasks.discard)\n                tasks.append(task)\n\n            # Wait for all extractions to complete\n            results = await asyncio.gather(*tasks, return_exceptions=True)\n            return [r if not isinstance(r, Exception) else None for r in results]\n        else:\n            # Manual extraction based on model fields\n            field_selectors = {}\n            for field_name in model.__annotations__:\n                # Convert field name to likely CSS selector\n                snake_case = field_name\n                selector = f\".{snake_case.replace('_', '-')}, #{snake_case.replace('_', '-')}\"\n                field_selectors[field_name] = selector\n\n            # Scrape with these selectors\n            raw_results = await self.scrape_urls(urls, extract_config=field_selectors)\n\n            # Convert to pydantic models\n            parsed_results = []\n            for result in raw_results:\n                try:\n                    if \"structured_data\" in result and \"error\" not in result:\n                        # Map the extracted data to model fields\n                        model_data = {}\n                        for field_name in model.__annotations__:\n                            if field_name in result[\"structured_data\"]:\n                                field_value = result[\"structured_data\"][field_name]\n                                if isinstance(field_value, list) and len(field_value) &gt; 0:\n                                    model_data[field_name] = field_value[0]  # Take first match\n                                else:\n                                    model_data[field_name] = field_value\n\n                        # Create the model instance\n                        parsed_results.append(model(**model_data))\n                    else:\n                        parsed_results.append(None)\n                except Exception as e:\n                    print(f\"Error parsing result: {e}\")\n                    parsed_results.append(None)\n\n            return parsed_results\n\n    async def _run_extraction_agent(\n        self,\n        url: str,\n        model: type[T],\n        extraction_task: str,\n        system_prompt_class: type[SystemPrompt]\n    ) -&gt; T:\n        \"\"\"Run an intelligent agent to extract structured data\"\"\"\n        # Define output model for the agent\n        controller = Controller(output_model=model)\n\n        # Create the task description\n        fields_info = \"\\n\".join([f\"- {field}: {model.__annotations__[field].__name__}\"\n                                 for field in model.__annotations__])\n\n        task = f\"\"\"\n        Go to {url} and extract the following information:\n        {fields_info}\n\n        Specific extraction instructions: {extraction_task}\n        \"\"\"\n\n        # Create and run the agent\n        agent = await self.browser_wrapper.create_agent(task=task)\n        agent._controller = controller\n        agent._system_prompt_class = system_prompt_class\n\n        history = await agent.run()\n\n        # Parse the result\n        result = history.final_result()\n        if result:\n            try:\n                return model.model_validate_json(result)\n            except Exception as e:\n                print(f\"Error parsing agent result: {e}\")\n                return None\n        return None\n</code></pre> <code>__init__(config=WebScraperConfig(), llm=None, chrome_path=None, remote_url=None, browser_config=None)</code> \u00b6 <p>Initialize the web scraper with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>WebScraperConfig</code> <p>Configuration for scraper behavior</p> <code>WebScraperConfig()</code> <code>llm</code> <code>str | BaseChatModel | None</code> <p>Language model for intelligent data extraction</p> <code>None</code> <code>chrome_path</code> <code>str | None</code> <p>Path to Chrome executable</p> <code>None</code> <code>remote_url</code> <code>str | None</code> <p>URL for remote browser connection</p> <code>None</code> <code>browser_config</code> <code>dict[str, Any] | None</code> <p>Additional browser configuration</p> <code>None</code> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>def __init__(\n    self,\n    config: WebScraperConfig = WebScraperConfig(),\n    llm: str | BaseChatModel | None = None,\n    chrome_path: str | None = None,\n    remote_url: str | None = None,\n    browser_config: dict[str, Any] | None = None\n):\n    \"\"\"\n    Initialize the web scraper with configuration.\n\n    Args:\n        config: Configuration for scraper behavior\n        llm: Language model for intelligent data extraction\n        chrome_path: Path to Chrome executable\n        remote_url: URL for remote browser connection\n        browser_config: Additional browser configuration\n    \"\"\"\n    self.config = config\n    self.browser_wrapper = BrowserWrapper(\n        llm=llm,\n        headless=config.headless,\n        chrome_path=chrome_path,\n        remote_url=remote_url,\n        config=browser_config\n    )\n    self.active_tasks = set()\n    self._semaphore = asyncio.Semaphore(config.max_concurrent_tabs)\n    self._results = {}\n\n    # Create screenshot directory if needed\n    if config.save_screenshots and not os.path.exists(config.screenshot_dir):\n        os.makedirs(config.screenshot_dir)\n</code></pre> <code>close()</code> <code>async</code> \u00b6 <p>Close the browser and clean up resources</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def close(self):\n    \"\"\"Close the browser and clean up resources\"\"\"\n    # Wait for all active tasks to complete\n    if self.active_tasks:\n        await asyncio.gather(*self.active_tasks)\n    await self.browser_wrapper.close()\n</code></pre> <code>initialize()</code> <code>async</code> \u00b6 <p>Initialize the browser if not already initialized</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def initialize(self):\n    \"\"\"Initialize the browser if not already initialized\"\"\"\n    await self.browser_wrapper.initialize()\n</code></pre> <code>scrape_structured_data(urls, model, extraction_task=None)</code> <code>async</code> \u00b6 <p>Scrape and parse structured data into pydantic models</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list[str]</code> <p>List of URLs to scrape</p> required <code>model</code> <code>type[T]</code> <p>Pydantic model class for structured data</p> required <code>extraction_task</code> <code>str</code> <p>Natural language description of what to extract</p> <code>None</code> <p>Returns:</p> Type Description <code>list[T]</code> <p>List of parsed data objects</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def scrape_structured_data(\n    self,\n    urls: list[str],\n    model: type[T],\n    extraction_task: str = None\n) -&gt; list[T]:\n    \"\"\"\n    Scrape and parse structured data into pydantic models\n\n    Args:\n        urls: List of URLs to scrape\n        model: Pydantic model class for structured data\n        extraction_task: Natural language description of what to extract\n\n    Returns:\n        List of parsed data objects\n    \"\"\"\n    await self.initialize()\n\n    # Create intelligent extraction task if provided\n    if extraction_task:\n        # Create a custom system prompt for extraction\n        class ExtractionPrompt(SystemPrompt):\n            def important_rules(self) -&gt; str:\n                existing_rules = super().important_rules()\n                new_rules = f\"\"\"\n                9. EXTRACTION GOAL:\n                - Your primary goal is to extract data according to this specific task: {extraction_task}\n                - You should carefully identify and extract the information as accurately as possible.\n                - Focus only on relevant information that matches the specified data structure.\n                \"\"\"\n                return f'{existing_rules}\\n{new_rules}'\n\n        # Define the extraction task for each URL\n        tasks = []\n        for url in urls:\n            # Setup intelligent extraction for each URL\n            task = asyncio.create_task(self._run_extraction_agent(\n                url=url,\n                model=model,\n                extraction_task=extraction_task,\n                system_prompt_class=ExtractionPrompt\n            ))\n            self.active_tasks.add(task)\n            task.add_done_callback(self.active_tasks.discard)\n            tasks.append(task)\n\n        # Wait for all extractions to complete\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return [r if not isinstance(r, Exception) else None for r in results]\n    else:\n        # Manual extraction based on model fields\n        field_selectors = {}\n        for field_name in model.__annotations__:\n            # Convert field name to likely CSS selector\n            snake_case = field_name\n            selector = f\".{snake_case.replace('_', '-')}, #{snake_case.replace('_', '-')}\"\n            field_selectors[field_name] = selector\n\n        # Scrape with these selectors\n        raw_results = await self.scrape_urls(urls, extract_config=field_selectors)\n\n        # Convert to pydantic models\n        parsed_results = []\n        for result in raw_results:\n            try:\n                if \"structured_data\" in result and \"error\" not in result:\n                    # Map the extracted data to model fields\n                    model_data = {}\n                    for field_name in model.__annotations__:\n                        if field_name in result[\"structured_data\"]:\n                            field_value = result[\"structured_data\"][field_name]\n                            if isinstance(field_value, list) and len(field_value) &gt; 0:\n                                model_data[field_name] = field_value[0]  # Take first match\n                            else:\n                                model_data[field_name] = field_value\n\n                    # Create the model instance\n                    parsed_results.append(model(**model_data))\n                else:\n                    parsed_results.append(None)\n            except Exception as e:\n                print(f\"Error parsing result: {e}\")\n                parsed_results.append(None)\n\n        return parsed_results\n</code></pre> <code>scrape_url(url, extract_config=None)</code> <code>async</code> \u00b6 <p>Scrape a single URL and return the results</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to scrape</p> required <code>extract_config</code> <code>dict[str, Any]</code> <p>Configuration for structured data extraction</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing scraped data</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def scrape_url(self, url: str, extract_config: dict[str, Any] = None) -&gt; dict[str, Any]:\n    \"\"\"\n    Scrape a single URL and return the results\n\n    Args:\n        url: URL to scrape\n        extract_config: Configuration for structured data extraction\n\n    Returns:\n        Dictionary containing scraped data\n    \"\"\"\n    await self.initialize()\n    task_id = f\"{len(self._results)}_{datetime.now().timestamp()}\"\n    result = await self._scrape_url(url, task_id, extract_config)\n    return result\n</code></pre> <code>scrape_urls(urls, extract_config=None)</code> <code>async</code> \u00b6 <p>Scrape multiple URLs in parallel and return all results</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list[str]</code> <p>List of URLs to scrape</p> required <code>extract_config</code> <code>dict[str, Any] | None</code> <p>Configuration for structured data extraction</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of dictionaries containing scraped data</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def scrape_urls(\n    self,\n    urls: list[str],\n    extract_config: dict[str, Any] | None = None\n) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Scrape multiple URLs in parallel and return all results\n\n    Args:\n        urls: List of URLs to scrape\n        extract_config: Configuration for structured data extraction\n\n    Returns:\n        List of dictionaries containing scraped data\n    \"\"\"\n    await self.initialize()\n    tasks = []\n\n    for i, url in enumerate(urls):\n        task_id = f\"{i}_{datetime.now().timestamp()}\"\n        task = asyncio.create_task(self._scrape_url(url, task_id, extract_config))\n        self.active_tasks.add(task)\n        task.add_done_callback(self.active_tasks.discard)\n        tasks.append(task)\n\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    return [r if not isinstance(r, Exception) else {\"error\": str(r)} for r in results]\n</code></pre> <code>search_web(query, max_results=5, include_content=True, extract_images=False, extract_tables=False, extract_links=False, save_to_file=None)</code> <code>async</code> \u00b6 <p>Perform a comprehensive web search and return high-quality data for the given query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query string</p> required <code>max_results</code> <code>int</code> <p>Maximum number of results to process (default: 5)</p> <code>5</code> <code>include_content</code> <code>bool</code> <p>Whether to include full content from result pages (default: True)</p> <code>True</code> <code>extract_images</code> <code>bool</code> <p>Whether to extract images from result pages (default: False)</p> <code>False</code> <code>extract_tables</code> <code>bool</code> <p>Whether to extract tables from result pages (default: False)</p> <code>False</code> <code>extract_links</code> <code>bool</code> <p>Whether to extract links from result pages (default: False)</p> <code>False</code> <code>save_to_file</code> <code>str | None</code> <p>Path to save results as JSON (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing search results and extracted information</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def search_web(\n    self,\n    query: str,\n    max_results: int = 5,\n    include_content: bool = True,\n    extract_images: bool = False,\n    extract_tables: bool = False,\n    extract_links: bool = False,\n    save_to_file: str | None = None\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Perform a comprehensive web search and return high-quality data for the given query.\n\n    Args:\n        query: Search query string\n        max_results: Maximum number of results to process (default: 5)\n        include_content: Whether to include full content from result pages (default: True)\n        extract_images: Whether to extract images from result pages (default: False)\n        extract_tables: Whether to extract tables from result pages (default: False)\n        extract_links: Whether to extract links from result pages (default: False)\n        save_to_file: Path to save results as JSON (optional)\n\n    Returns:\n        Dictionary containing search results and extracted information\n    \"\"\"\n    await self.initialize()\n    try:\n        start_time = datetime.now()\n\n        # Try different search engines in order\n        search_engines = [\n            {\n                \"url\": f\"https://www.google.com/search?q={urllib.parse.quote_plus(query)}\",\n                \"result_selector\": \".g\",\n                \"title_selector\": \"h3\",\n                \"link_selector\": \"a\",\n                \"snippet_selector\": \".VwiC3b\",\n                \"name\": \"google\"\n            },\n            {\n                \"url\": f\"https://www.bing.com/search?q={urllib.parse.quote_plus(query)}\",\n                \"result_selector\": \".b_algo\",\n                \"title_selector\": \"h2\",\n                \"link_selector\": \"a\",\n                \"snippet_selector\": \".b_caption p\",\n                \"name\": \"bing\"\n            },\n            {\n                \"url\": f\"https://duckduckgo.com/?q={urllib.parse.quote_plus(query)}\",\n                \"result_selector\": \".result\",\n                \"title_selector\": \"h2\",\n                \"link_selector\": \"a.result__a\",\n                \"snippet_selector\": \".result__snippet\",\n                \"name\": \"duckduckgo\"\n            }\n        ]\n\n        results = []\n\n        for engine in search_engines:\n            try:\n                # Navigate to search engine\n                page = await self.browser_wrapper.navigate(engine[\"url\"])\n                await page.wait_for_load_state(\"networkidle\")\n                await page.wait_for_timeout(2000)  # Wait for results to load\n\n                # Extract search results\n                search_results = await page.evaluate(\n                    \"\"\"\n                    (selectors) =&gt; {\n                        const results = [];\n                        const elements = document.querySelectorAll(selectors.result_selector);\n\n                        for (const element of elements) {\n                            const titleElement = element.querySelector(selectors.title_selector);\n                            const linkElement = element.querySelector(selectors.link_selector);\n                            const snippetElement = element.querySelector(selectors.snippet_selector);\n\n                            if (titleElement &amp;&amp; linkElement) {\n                                const url = linkElement.href;\n                                // Skip non-http links and same-domain results\n                                if (url &amp;&amp; url.startsWith('http') &amp;&amp;\n                                    !url.includes('google.com/search') &amp;&amp;\n                                    !url.includes('bing.com/search') &amp;&amp;\n                                    !url.includes('duckduckgo.com')) {\n                                    results.push({\n                                        title: titleElement.textContent.trim(),\n                                        url: url,\n                                        snippet: snippetElement ? snippetElement.textContent.trim() : '',\n                                        source: selectors.name\n                                    });\n                                }\n                            }\n                        }\n                        return results;\n                    }\n                    \"\"\",\n                    engine\n                )\n\n                if search_results and len(search_results) &gt; 0:\n                    # We got results, add them and break\n                    results = search_results\n                    break\n\n            except Exception as e:\n                print(f\"Error searching with {engine['name']}: {str(e)}\")\n                continue  # Try next engine\n\n        # Filter and limit results\n        unique_urls = set()\n        filtered_results = []\n\n        for result in results:\n            if result['url'] not in unique_urls and len(filtered_results) &lt; max_results:\n                unique_urls.add(result['url'])\n                filtered_results.append(result)\n\n        results = filtered_results\n\n        # Get detailed content if requested\n        if include_content and results:\n            # Extract content from each result page\n            urls_to_scrape = [result['url'] for result in results]\n\n            # Configure what to extract\n            extract_config = {}\n            if extract_tables:\n                extract_config['tables'] = 'table'\n            if extract_images:\n                extract_config['images'] = 'img'\n            if extract_links:\n                extract_config['links'] = 'a'\n\n            # Scrape all pages in parallel using our efficient multi-tab approach\n            scraped_data = await self.scrape_urls(\n                urls_to_scrape,\n                extract_config=extract_config if extract_config else None\n            )\n\n            # Add content to results\n            for i, result in enumerate(results):\n                if i &lt; len(scraped_data) and 'error' not in scraped_data[i]:\n                    result['content'] = {\n                        'title': scraped_data[i].get('title', result['title']),\n                        'markdown': scraped_data[i].get('markdown', ''),\n                        'text': scraped_data[i].get('text', ''),\n                    }\n\n                    # Add structured data if available\n                    if extract_config and 'structured_data' in scraped_data[i]:\n                        structured_data = scraped_data[i]['structured_data']\n                        for key, value in structured_data.items():\n                            if value:  # Only add non-empty data\n                                result['content'][key] = value\n\n        # Prepare final response\n        response = {\n            'query': query,\n            'timestamp': datetime.now().isoformat(),\n            'num_results': len(results),\n            'results': results,\n            'execution_time': (datetime.now() - start_time).total_seconds()\n        }\n\n        # Save to file if requested\n        if save_to_file:\n            os.makedirs(os.path.dirname(os.path.abspath(save_to_file)), exist_ok=True)\n            with open(save_to_file, 'w', encoding='utf-8') as f:\n                json.dump(response, f, ensure_ascii=False, indent=2)\n\n        return response\n\n    finally:\n        # Make sure we clean up browser resources\n        await self.close()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool.WebScraperConfig","title":"<code>WebScraperConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for web scraper operations</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>class WebScraperConfig(BaseModel):\n    \"\"\"Configuration for web scraper operations\"\"\"\n    max_concurrent_tabs: int = 5\n    default_timeout: float = 30000\n    scroll_delay: int = 500\n    initial_delay: int = 1000\n    viewport_height: int = 900\n    viewport_width: int = 1600\n    wait_for_selectors: bool = True\n    auto_scroll: bool = True\n    save_screenshots: bool = False\n    screenshot_dir: str = \"./screenshots\"\n    extract_markdown: bool = True\n    extract_text: bool = True\n    extract_html: bool = False\n    headless: bool = False\n    disable_images: bool = False\n    user_agent: str | None = None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool.sanitize_filename","title":"<code>sanitize_filename(filename)</code>","text":"<p>Convert a string to a valid filename</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>def sanitize_filename(filename: str) -&gt; str:\n    \"\"\"Convert a string to a valid filename\"\"\"\n    # Replace spaces with underscores and remove invalid characters\n    sanitized = re.sub(r'[^\\w\\s-]', '', filename).strip().lower()\n    return re.sub(r'[-\\s]+', '_', sanitized)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool.scrape_documentation_to_markdown","title":"<code>scrape_documentation_to_markdown(start_url, topic=None, max_pages=30, max_depth=3, output_dir=None, toc_filename='table_of_contents.md')</code>  <code>async</code>","text":"<p>Recursively scrape documentation pages starting from a URL, focused on a specific topic, and convert to Markdown.</p> <p>Parameters:</p> Name Type Description Default <code>start_url</code> <code>str</code> <p>The documentation homepage or entry point</p> required <code>topic</code> <code>str | None</code> <p>The topic to focus on (e.g., \"streaming\", \"authentication\")</p> <code>None</code> <code>max_pages</code> <code>int</code> <p>Maximum number of pages to scrape</p> <code>30</code> <code>max_depth</code> <code>int</code> <p>Maximum depth of link traversal</p> <code>3</code> <code>output_dir</code> <code>str | None</code> <p>Directory to save the MD files (if None, just returns them)</p> <code>None</code> <code>toc_filename</code> <code>str</code> <p>Filename for the table of contents</p> <code>'table_of_contents.md'</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary mapping page titles to markdown content</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def scrape_documentation_to_markdown(\n    start_url: str,\n    topic: str | None = None,\n    max_pages: int = 30,\n    max_depth: int = 3,\n    output_dir: str | None = None,\n    toc_filename: str = \"table_of_contents.md\"\n) -&gt; dict[str, str]:\n    \"\"\"\n    Recursively scrape documentation pages starting from a URL,\n    focused on a specific topic, and convert to Markdown.\n\n    Args:\n        start_url: The documentation homepage or entry point\n        topic: The topic to focus on (e.g., \"streaming\", \"authentication\")\n        max_pages: Maximum number of pages to scrape\n        max_depth: Maximum depth of link traversal\n        output_dir: Directory to save the MD files (if None, just returns them)\n        toc_filename: Filename for the table of contents\n\n    Returns:\n        Dictionary mapping page titles to markdown content\n    \"\"\"\n    # Initialize scraper with efficient settings for docs\n    scraper_config = WebScraperConfig(\n        max_concurrent_tabs=5,\n        headless=global_headless,\n        disable_images=True,\n        extract_html=False,\n        auto_scroll=True,\n        scroll_delay=300,\n        initial_delay=500,\n        save_screenshots=False\n    )\n\n    scraper = WebScraper(config=scraper_config)\n    await scraper.initialize()\n\n    # Track visited and pending URLs\n    visited_urls: set[str] = set()\n    pending_urls: list[dict] = [{\"url\": start_url, \"depth\": 0, \"parent\": None}]\n    results: dict[str, dict] = {}\n    domain = urlparse(start_url).netloc\n\n    logging.info(f\"Starting documentation scrape from {start_url}\")\n    if topic:\n        logging.info(f\"Focusing on topic: {topic}\")\n\n    # Create a regular expression pattern for topic if provided\n    topic_pattern = re.compile(rf'\\b{re.escape(topic)}\\b', re.IGNORECASE) if topic else None\n\n    try:\n        # Process URLs breadth-first until we hit max pages or have no more URLs\n        while pending_urls and len(results) &lt; max_pages:\n            # Get the next URL to process\n            current = pending_urls.pop(0)\n            current_url = current[\"url\"]\n            current_depth = current[\"depth\"]\n\n            # Skip if we've already visited this URL\n            if current_url in visited_urls:\n                continue\n\n            logging.info(f\"Scraping: {current_url} (depth: {current_depth})\")\n            visited_urls.add(current_url)\n\n            # Scrape the current page\n            page_result = await scraper.scrape_url(current_url)\n\n            # Skip pages with errors\n            if \"error\" in page_result:\n                logging.warning(f\"Error scraping {current_url}: {page_result['error']}\")\n                continue\n\n            # Check if page is relevant to the topic\n            is_relevant = True\n            if topic_pattern:\n                markdown_content = page_result.get(\"markdown\", \"\")\n                text_content = page_result.get(\"text\", \"\")\n\n                # Check if topic appears in title, URL, or content\n                has_topic_in_title = topic_pattern.search(page_result.get(\"title\", \"\"))\n                has_topic_in_url = topic_pattern.search(current_url)\n                has_topic_in_content = (\n                    topic_pattern.search(markdown_content) or\n                    topic_pattern.search(text_content)\n                )\n\n                is_relevant = has_topic_in_title or has_topic_in_url or has_topic_in_content\n\n            # Process this page if it's relevant\n            if is_relevant:\n                # Extract title and content\n                title = page_result.get(\"title\", f\"Page {len(results) + 1}\")\n\n                # Store the result\n                results[current_url] = {\n                    \"title\": title,\n                    \"markdown\": page_result.get(\"markdown\", \"\"),\n                    \"depth\": current_depth,\n                    \"parent\": current[\"parent\"]\n                }\n\n                # Only proceed deeper if we haven't hit max depth\n                if current_depth &lt; max_depth:\n                    # Extract links to follow\n                    parser = scraper.browser_wrapper.get_parser()\n                    links = await parser.extract_links(current_url)\n\n                    # Filter links for internal documentation pages\n                    doc_links = []\n                    for link in links:\n                        link_url = link[\"href\"]\n                        parsed_url = urlparse(link_url)\n\n                        # Only include links to the same domain\n                        if parsed_url.netloc == domain or not parsed_url.netloc:\n                            # Normalize URL\n                            if not parsed_url.netloc:\n                                link_url = urljoin(current_url, link_url)\n\n                            # Skip anchor links to same page\n                            if link_url.split('#')[0] == current_url.split('#')[0]:\n                                continue\n\n                            # Skip non-documentation links (common patterns)\n                            skip_patterns = [\n                                r'(\\.pdf|\\.zip|\\.tar|\\.gz)$',  # Downloads\n                                r'/search/',  # Search pages\n                                r'/login/',  # Auth pages\n                                r'/logout/',  # Auth pages\n                                r'/tag/',  # Tag pages\n                                r'/version/',  # Version switching\n                                r'/latest/',  # Version switching\n                                r'/download/',  # Download pages\n                                r'/contact/',  # Contact pages\n                                r'/blog/',  # Blog posts (unless that's what we want)\n                            ]\n\n                            should_skip = any(re.search(pattern, link_url) for pattern in skip_patterns)\n                            if should_skip:\n                                continue\n\n                            # Check if it's potentially relevant to the topic\n                            is_potentially_relevant = True\n                            if topic_pattern:\n                                has_topic_in_link_text = topic_pattern.search(link[\"text\"])\n                                has_topic_in_link_url = topic_pattern.search(link_url)\n                                is_potentially_relevant = has_topic_in_link_text or has_topic_in_link_url\n\n                            # Add to pending if it's potentially relevant and not already visited\n                            if is_potentially_relevant and link_url not in visited_urls:\n                                doc_links.append({\n                                    \"url\": link_url,\n                                    \"depth\": current_depth + 1,\n                                    \"parent\": current_url\n                                })\n\n                    # Add the filtered links to our pending list\n                    pending_urls.extend(doc_links)\n\n        # Generate markdown output\n        markdown_results = {}\n\n        # Create a hierarchy for building a table of contents\n        pages_hierarchy = {}\n        for url, page_data in results.items():\n            title = page_data[\"title\"]\n            markdown = page_data[\"markdown\"]\n\n            # Add page URL reference at the bottom\n            markdown += f\"\\n\\n---\\n*Source: [{url}]({url})*\"\n\n            # Add to outputs\n            markdown_results[url] = markdown\n\n            # Track in hierarchy for TOC\n            depth = page_data[\"depth\"]\n            parent = page_data[\"parent\"]\n\n            if depth not in pages_hierarchy:\n                pages_hierarchy[depth] = []\n\n            pages_hierarchy[depth].append({\n                \"url\": url,\n                \"title\": title,\n                \"parent\": parent\n            })\n\n        # Generate table of contents\n        toc = f\"# Documentation: {topic if topic else 'All Topics'}\\n\\n\"\n        toc += f\"*Generated from: [{start_url}]({start_url})*\\n\\n\"\n        toc += \"## Table of Contents\\n\\n\"\n\n        # Sort by depth to build hierarchy\n        for depth in sorted(pages_hierarchy.keys()):\n            pages = pages_hierarchy[depth]\n\n            for page in pages:\n                # Calculate indentation based on depth\n                indent = \"  \" * depth\n                page_filename = sanitize_filename(page[\"title\"]) + \".md\"\n                toc += f\"{indent}- [{page['title']}]({page_filename})\\n\"\n\n        # Save the results if output directory specified\n        if output_dir:\n            os.makedirs(output_dir, exist_ok=True)\n\n            # Write the TOC file\n            with open(os.path.join(output_dir, toc_filename), \"w\", encoding=\"utf-8\") as f:\n                f.write(toc)\n\n            # Write each page file\n            for url, content in markdown_results.items():\n                page_title = results[url][\"title\"]\n                filename = sanitize_filename(page_title) + \".md\"\n                filepath = os.path.join(output_dir, filename)\n\n                with open(filepath, \"w\", encoding=\"utf-8\") as f:\n                    f.write(content)\n\n            logging.info(f\"Saved {len(markdown_results)} documentation pages to {output_dir}\")\n\n        # Include the TOC in the results\n        markdown_results[\"table_of_contents\"] = toc\n\n        return markdown_results\n\n    finally:\n        # Make sure we clean up browser resources\n        await scraper.close()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base","title":"<code>base</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent","title":"<code>Agent</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent.agent","title":"<code>agent</code>","text":"<code>AgentModelData</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Configuration for the LLM model and API settings via LiteLLM.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>class AgentModelData(BaseModel):\n    \"\"\"Configuration for the LLM model and API settings via LiteLLM.\"\"\"\n    name: str | None = Field(default=None, description=\"Agent's internal name, often derived from builder.\")\n    model: str = Field(..., description=\"Primary LiteLLM model identifier (e.g., 'gemini/gemini-1.5-flash-latest', 'ollama/mistral').\")\n    provider: str | None = Field(default=None, description=\"LiteLLM provider override if needed.\")\n    system_message: str = Field(default=\"You are a helpful AI assistant.\", description=\"Base system prompt.\")\n\n    temperature: float | None = Field(default=None, ge=0.0, le=2.0) # Use LiteLLM defaults if None\n    top_k: int | None = Field(default=None, ge=1)\n    top_p: float | None = Field(default=None, ge=0.0, le=1.0)\n    max_tokens: int | None = Field(default=None, ge=1, description=\"Max tokens for LLM generation.\")\n    max_input_tokens: int | None = Field(default=None, ge=1, description=\"Max context window size (for trimming).\")\n\n    api_key: str | None = Field(default=None, description=\"API key (use env vars in production).\")\n    api_base: str | None = Field(default=None, description=\"API base URL (for local models/proxies).\")\n    api_version: str | None = Field(default=None, description=\"API version (e.g., Azure).\")\n\n    stop_sequence: list[str] | None = Field(default=None, alias=\"stop\") # Alias for LiteLLM\n    presence_penalty: float | None = Field(default=None)\n    frequency_penalty: float | None = Field(default=None)\n\n    user_id: str | None = Field(default=None, description=\"User identifier for LLM calls ('user' param).\")\n    budget_manager: BudgetManager | None = Field(default=None, description=\"LiteLLM BudgetManager instance.\")\n    caching: bool | None = Field(default=True, description=\"Enable/disable LiteLLM caching.\")\n\n    # Model config for Pydantic v2\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        extra='ignore', # Ignore extra fields from builder/ADK init\n        populate_by_name=True # Allow using 'stop' alias\n    )\n</code></pre> <code>EnhancedAgent</code> \u00b6 <p>               Bases: <code>*_AgentBaseClass</code></p> <p>Enhanced, production-oriented Unified Agent integrating LiteLLM, ADK, A2A, and MCP (via ADK).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>class EnhancedAgent(*_AgentBaseClass):\n    \"\"\"\n    Enhanced, production-oriented Unified Agent integrating LiteLLM, ADK, A2A, and MCP (via ADK).\n    \"\"\"\n    # --- Core Configuration ---\n    amd: AgentModelData # Primary model config\n    format_model: str | None = Field(default=None, description=\"Optional separate model for JSON formatting (a_format_class).\")\n    format_model_: str | None = Field(default=None, description=\"helper var for format_model\", exclude=True)\n    world_model: WorldModel = Field(default_factory=WorldModel)\n    verbose: bool = Field(default=False)\n    internal_state: InternalAgentState = Field(default=InternalAgentState.IDLE)\n\n    # --- LiteLLM Specific ---\n    stream: bool = Field(default=False, description=\"Whether LLM calls should stream chunks.\")\n    # Use a simple dict for history for now, can be replaced with persistent store interface\n    # Keyed by session_id\n    message_history: dict[str, list[dict[str, Any]]] = Field(default_factory=dict)\n    max_history_tokens: int | None = Field(default=None, description=\"Alternative to max_turns for history trimming based on token count.\")\n    max_history_turns: int = Field(default=20, description=\"Max conversation turns (user+assistant) for history.\") # Used if max_history_tokens is None\n    trim_strategy: Literal[\"litellm\", \"basic\"] = Field(default=\"litellm\")\n    total_cost: float = Field(default=0.0, description=\"Accumulated cost tracked via LiteLLM.\")\n\n    # --- Framework Components (Initialized via Builder/Setup) ---\n    # ADK\n    adk_runner: Runner | None = Field(default=None, description=\"ADK Runner instance if enabled.\")\n    adk_session_service: BaseSessionService | None = Field(default=None, description=\"ADK Session Service (often from runner).\")\n    sync_adk_state: bool = Field(default=True, description=\"Sync WorldModel with ADK Session.state.\")\n    # Exit stack to manage lifecycles of components like MCPToolset connections\n    # CRITICAL FIX: Use contextlib.AsyncExitStack type hint\n    adk_exit_stack: contextlib.AsyncExitStack | None = Field(default=None, description=\"AsyncExitStack for managing ADK toolset lifecycles.\")\n\n    # MCP Server (Agent acts AS an MCP Server)\n    mcp_server: FastMCP | None = Field(default=None, description=\"MCP server instance if agent exposes MCP capabilities.\")\n    # A2A Server (Agent acts AS an A2A Server)\n    a2a_server: A2AServer | None = Field(default=None, description=\"A2A server instance if agent exposes A2A capabilities.\")\n    # A2A Client (Agent acts AS an A2A Client)\n    a2a_clients: dict[str, A2AClient] = Field(default_factory=dict, description=\"Cached A2A clients for target agents.\")\n    a2a_client_lock: asyncio.Lock = Field(default_factory=asyncio.Lock, description=\"Lock for A2A client cache access.\")\n    a2a_poll_interval: float = Field(default=2.0, description=\"Polling interval for A2A task results (seconds).\")\n    a2a_poll_timeout: float = Field(default=60.0, description=\"Max time to wait for A2A task completion.\")\n\n    # --- Callbacks ---\n    stream_callback: Callable[[str], None | Awaitable[None]] | None = Field(default=None, description=\"Callback for each LLM stream chunk.\")\n    post_run_callback: Callable[[str, str, float], None | Awaitable[None]] | None = Field(default=None, description=\"Callback after a_run completes (session_id, final_response, turn_cost).\")\n    progress_callback: Callable[[Any], None | Awaitable[None]] | None = Field(default=None, description=\"Callback for progress updates (e.g., tool execution, A2A polling).\")\n    human_in_loop_callback: Callable[[dict], str | Awaitable[str]] | None = Field(default=None, description=\"Callback for HIL intervention points.\")\n\n    # --- Observability ---\n    tracer: Any | None = Field(default=None, description=\"OpenTelemetry Tracer instance.\") # Type hint depends on OTel setup\n\n    # --- Internal State ---\n    last_llm_result: Any | None = Field(default=None, description=\"Raw result from the last LiteLLM call.\")\n\n    # Model config\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        extra='ignore' # Critical for compatibility with ADK LlmAgent init\n    )\n\n    @model_validator(mode='after')\n    def _enhanced_agent_post_init(self) -&gt; 'EnhancedAgent':\n        \"\"\"\n        Performs initialization steps after Pydantic has validated fields.\n        \"\"\"\n        # --- (Existing post_init logic remains the same) ---\n        logger.setLevel(logging.DEBUG if self.verbose else logging.INFO)\n        os.environ['LITELLM_LOG'] = 'DEBUG' if self.verbose else 'NONE'\n        logger.debug(f\"Verbose logging {'enabled' if self.verbose else 'disabled'} for agent {self.amd.name}\")\n        self._setup_telemetry()\n        if ADK_AVAILABLE and isinstance(self, LlmAgent):\n            logger.debug(f\"Running post-init logic for ADK agent '{self.amd.name}'\")\n            self._ensure_internal_adk_tools() # Ensure tools are added *after* Pydantic init\n            if self.adk_runner and hasattr(self.adk_runner, 'session_service'):\n                self.adk_session_service = self.adk_runner.session_service\n                logger.debug(\"Associated ADK session service from runner.\")\n        if 'default' not in self.message_history:\n            self.message_history['default'] = []\n        logger.info(\n            f\"EnhancedAgent '{self.amd.name}' initialized. Model: {self.amd.model}. \"\n            f\"Capabilities: ADK({ADK_AVAILABLE}), A2A({A2A_AVAILABLE}), MCP({MCP_AVAILABLE})\"\n        )\n        self.model =  LiteLlm(model=self.amd.model)\n        return self\n\n    # --- ADK Post Init (Called automatically by Pydantic if method exists in base) ---\n    # This method name is expected by ADK's BaseModel integration.\n    # Pydantic v2 runs validators based on MRO, so if LlmAgent has this, it runs.\n    # We don't strictly need to define it here unless overriding LlmAgent's version.\n    # def model_post_init(self, __context: Any) -&gt; None:\n    #     \"\"\"ADK post-initialization (if inheriting from ADK BaseModel).\"\"\"\n    #     # Call super() if overriding LlmAgent's method\n    #     # super().model_post_init(__context) # If LlmAgent has this method\n    #     logger.debug(f\"ADK model_post_init for Agent '{self.amd.name}' (EnhancedAgent)\")\n    #     # Add post-init logic specific to ADK features here, AFTER ADK's own init\n    #     self._ensure_internal_adk_tools()\n    #     if self.adk_runner:\n    #         self.adk_session_service = self.adk_runner.session_service\n\n\n    # --- Telemetry Setup ---\n    def _setup_telemetry(self):\n        \"\"\"Initializes the OpenTelemetry tracer.\"\"\"\n        if OTEL_AVAILABLE and not self.tracer:\n            # Get tracer from global provider (needs to be configured elsewhere)\n            # In a real app, you'd configure the TracerProvider with exporters\n            # provider = TracerProvider() # Example: basic provider\n            # provider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter())) # Example: console output\n            # trace.set_tracer_provider(provider)\n            self.tracer = trace.get_tracer(\"enhanced_agent\", \"0.1.0\")\n            logger.info(\"OpenTelemetry tracer initialized.\")\n        elif not OTEL_AVAILABLE:\n            self.tracer = DummyTracer() # Use NoOp tracer if OTel not installed\n            logger.debug(\"OpenTelemetry not available, using NoOp tracer.\")\n\n\n    # --- Setup Methods (Called by Builder) ---\n\n    def setup_mcp_server(self, host=\"0.0.0.0\", port=8000, **mcp_kwargs):\n        \"\"\"Initialize and configure the MCP server capabilities *for this agent*.\n           This agent will ACT AS an MCP Server.\n        \"\"\"\n        if not MCP_AVAILABLE:\n            logger.warning(\"MCP library not installed. Cannot setup MCP server.\")\n            return None\n        if self.mcp_server:\n            logger.warning(\"MCP server already initialized.\")\n            return self.mcp_server\n        name = mcp_kwargs.get(\"name\")\n        del mcp_kwargs[\"name\"]\n        self.mcp_server = FastMCP(name=name or f\"{self.amd.name}-mcp-server\",\n                                  description=f\"MCP interface for EnhancedAgent {self.amd.name}\",\n                                  **mcp_kwargs)\n        logger.info(f\"Setting up MCP server for agent '{self.amd.name}' on {host}:{port}\")\n\n        # --- Register Agent's core functionalities as MCP services ---\n        # Example: Expose World Model (Read-only for safety)\n        @self.mcp_server.resource(f\"agent://{self.amd.name}/world_model\")\n        def mcp_get_world_model_resource() -&gt; dict[str, Any]:\n            \"\"\"Gets the agent's world model.\"\"\"\n            logger.debug(f\"[MCP Resource] agent://{self.amd.name}/world_model accessed\")\n            return self.world_model.to_dict()\n\n        # Example: Expose a simple query tool via MCP\n        @self.mcp_server.tool(name=\"simple_llm_query\")\n        async def mcp_simple_query(prompt: str) -&gt; str:\n            \"\"\"Sends a simple prompt to the agent's LLM (non-persistent run).\"\"\"\n            logger.debug(f\"[MCP Tool] simple_llm_query called: {prompt[:50]}...\")\n            # Use a minimal, non-persistent run, disable recursive calls\n            response = await self.a_run(\n                prompt, session_id=f\"mcp_query_{uuid.uuid4()}\",\n                persist_history=False, strategy_override=ProcessingStrategy.DIRECT_LLM\n            )\n            return response\n\n        # If ADK tools exist, potentially expose them via MCP automatically?\n        if ADK_AVAILABLE and isinstance(self, LlmAgent) and self.tools:\n             logger.info(\"Attempting to expose ADK tools via MCP server...\")\n             for adk_tool in self.tools:\n                 if adk_tool.name in [\"code_execution\", \"adk_tool_a2a_send_and_wait\", \"adk_tool_a2a_send_no_wait\", \"adk_tool_a2a_get_task_status\", \"adk_tool_a2a_cancel_task\"]:\n                     continue\n                 if not isinstance(adk_tool, BaseTool): continue\n                 try:\n                     mcp_schema = adk_to_mcp_tool_type(adk_tool)\n\n                     # Define the MCP tool handler dynamically\n                     async def mcp_tool_handler(tool_name=adk_tool.name, **kwargs):\n                         logger.info(f\"[MCP Tool via ADK] Calling {tool_name} with {kwargs}\")\n                         # ADK tools expect ToolContext, which we don't have here.\n                         # We might need to simulate it or adapt the tool execution.\n                         # This simple version calls the tool's underlying function if possible.\n                         # WARNING: This bypasses ADK's standard tool execution flow.\n                         if hasattr(adk_tool, 'func') and callable(adk_tool.func):\n                             # This assumes the function doesn't need ToolContext\n                             result = await adk_tool.func(**kwargs)\n                             # Convert result to MCP content (e.g., TextContent)\n                             if isinstance(result, str):\n                                 return [mcp_types.TextContent(type=\"text\", text=result)]\n                             else:\n                                 try:\n                                     return [mcp_types.TextContent(type=\"text\", text=json.dumps(result))]\n                                 except:\n                                     return [mcp_types.TextContent(type=\"text\", text=str(result))]\n                         else:\n                             logger.warning(f\"Cannot directly call ADK tool {tool_name} via MCP.\")\n                             return [mcp_types.TextContent(type=\"text\", text=f\"Error: Cannot execute ADK tool {tool_name} directly.\")]\n\n                     # Register the dynamic handler with the MCP server\n                     self.mcp_server.tool(name=mcp_schema.name)(mcp_tool_handler)\n                     logger.info(f\"Exposed ADK tool '{adk_tool.name}' as MCP tool '{mcp_schema.name}'.\")\n\n                 except Exception as e:\n                     logger.warning(f\"Failed to expose ADK tool '{adk_tool.name}' via MCP: {e}\")\n\n\n        logger.info(f\"MCP server setup complete for agent '{self.amd.name}'. Run `agent.run_mcp_server()` to start.\")\n        return self.mcp_server\n\n    def run_mcp_server(self, transport='sse', **kwargs):\n        \"\"\"Starts the MCP server (blocking).\"\"\"\n        if not self.mcp_server:\n            logger.error(\"MCP server not initialized. Call setup_mcp_server first.\")\n            return\n        if not MCP_AVAILABLE:\n             logger.error(\"MCP library not available. Cannot run MCP server.\")\n             return\n        logger.info(f\"Starting MCP server for agent '{self.amd.name}' using {transport} transport...\")\n        # This is blocking, run in a separate process/thread for a long-running agent\n        try:\n            self.mcp_server.run(transport=transport, **kwargs)\n        except Exception as e:\n            logger.error(f\"MCP server failed to run: {e}\", exc_info=True)\n\n    # MCP Client Setup is now handled by ADK's MCPToolset via the Builder\n\n\n    def setup_a2a_server(self, host=\"0.0.0.0\", port=5000, **a2a_server_options):\n        \"\"\"\n        Initialize and configure the A2A server capabilities using python-a2a.\n        This dynamically creates a server class with the agent's capabilities.\n        \"\"\"\n        if not A2A_AVAILABLE:\n            logger.warning(\"python-a2a library not installed. Cannot setup A2A server.\")\n            return None\n        if self.a2a_server:\n            logger.warning(\"A2A server already initialized.\")\n            return self.a2a_server\n\n        logger.info(f\"Setting up A2A server for agent '{self.amd.name}' on {host}:{port}\")\n\n        agent_instance = self # Reference to the current EnhancedAgent instance\n\n        # Define the A2A Server class dynamically using the decorator\n        @a2a_agent_decorator(\n            name=self.amd.name or \"EnhancedAgent\",\n            description=f\"Enhanced Agent '{self.amd.name}' - Capabilities: ADK({ADK_AVAILABLE}), MCP({MCP_AVAILABLE}), A2A({A2A_AVAILABLE})\",\n            version=\"1.0.0\",\n            # Other AgentCard fields...\n        )\n        class DynamicA2AServer(A2AServer):\n            bound_agent: EnhancedAgent = agent_instance\n\n            def handle_task(self, task: Task) -&gt; Task:\n                \"\"\" Handles incoming A2A tasks by calling the EnhancedAgent's async logic. \"\"\"\n                # --- (handle_task implementation remains the same as before) ---\n                logger.info(f\"[A2A Server {self.bound_agent.amd.name}] Received task: {task.id}\")\n                async def run_agent_async():\n                    # ... (logic to extract prompt, call a_run, update task) ...\n                    try:\n                        user_prompt = \"\"\n                        # ... (extract user_prompt from task.message) ...\n                        if task.message and task.message.get(\"content\"):\n                            content = task.message[\"content\"]\n                            if isinstance(content, dict) and content.get(\"type\") == \"text\":\n                                user_prompt = content.get(\"text\", \"\").strip()\n                            elif isinstance(content, str):\n                                user_prompt = content.strip()\n\n                        if not user_prompt:\n                            raise ValueError(\"Task message has no text content.\")\n\n                        session_id = task.message.get(\"session_id\", task.id)\n                        agent_response = await self.bound_agent.a_run(\n                            user_prompt,\n                            session_id=session_id,\n                            persist_history=False,\n                            a2a_task_id=task.id\n                        )\n                        task.artifacts = [{\"parts\": [{\"type\": \"text\", \"text\": str(agent_response)}]}]\n                        task.status = TaskStatus(state=TaskState.COMPLETED)\n                    except Exception as e:\n                        # ... (error handling) ...\n                        logger.error(f\"[A2A Task {task.id}] Error during processing: {e}\", exc_info=True)\n                        error_msg = f\"Internal agent error: {str(e)}\"\n                        task.artifacts = [{\"parts\": [{\"type\": \"text\", \"text\": error_msg}]}]\n                        task.status = TaskStatus(state=TaskState.FAILED, message={\"role\": \"agent\", \"content\": {\"type\": \"text\", \"text\": error_msg}})\n                    return task\n                try:\n                    updated_task = asyncio.run(run_agent_async())\n                    return updated_task\n                except RuntimeError as e:\n                    # ... (handle RuntimeError) ...\n                    logger.error(f\"RuntimeError calling asyncio.run in handle_task: {e}.\")\n                    task.status = TaskStatus(state=TaskState.FAILED, message={\"role\": \"agent\", \"content\": {\"type\": \"text\", \"text\": \"Internal Server Error processing task asynchronously.\"}})\n                    return task\n                # --- (end of handle_task logic) ---\n\n\n            # --- Expose Skills ---\n            @a2a_skill_decorator(\n                name=\"General Query\",\n                description=\"Process general natural language queries using the agent's primary LLM.\",\n                examples=[\"What is the capital of France?\", \"Summarize the plot of Hamlet.\"]\n            )\n            def general_query_skill(self, query: str) -&gt; str:\n                \"\"\"Handles general queries via the skill mechanism by calling a_run.\"\"\"\n                logger.info(f\"[A2A Skill] Received general_query: {query[:50]}...\")\n                async def run_skill_async():\n                    # Call a_run, forcing direct LLM strategy for simple queries\n                    response = await self.bound_agent.a_run(\n                        query,\n                        a2a_task_id=f\"skill_query_{uuid.uuid4()}\",\n                        strategy_override=ProcessingStrategy.DIRECT_LLM,\n                        persist_history=False\n                        )\n                    return response\n                try:\n                    # Bridge sync skill call to async agent logic\n                    return asyncio.run(run_skill_async())\n                except RuntimeError:\n                     logger.error(\"RuntimeError calling asyncio.run in general_query_skill.\")\n                     return \"Error: Could not process skill asynchronously.\"\n\n            # --- FIXED: Generic Skill for ADK Tools ---\n            if ADK_AVAILABLE and isinstance(agent_instance, LlmAgent) and agent_instance.tools:\n                # Check if there are any ADK tools to expose\n                adk_tool_list = [t for t in agent_instance.tools if isinstance(t, BaseTool)]\n                if adk_tool_list:\n                    logger.info(f\"Exposing {len(adk_tool_list)} ADK tools via 'execute_adk_tool' A2A skill.\")\n\n                    @a2a_skill_decorator(\n                        name=\"execute_adk_tool\",\n                        description=f\"Executes a registered ADK tool. Available tools: {', '.join([t.name for t in adk_tool_list])}\",\n                        examples=[\"Execute tool 'some_tool_name' with argument 'arg1'='value1'\"] # Generic example\n                    )\n                    def execute_adk_tool_skill(self, tool_name: str, arguments: dict[str, Any]) -&gt; str:\n                        \"\"\"Generic skill to execute an ADK tool by name with arguments.\"\"\"\n                        logger.info(f\"[A2A Skill] Request to execute ADK tool: {tool_name} with args: {arguments}\")\n\n                        # Find the ADK tool instance on the bound agent\n                        tool_to_call: BaseTool | None = None\n                        for tool in self.bound_agent.tools:\n                            if isinstance(tool, BaseTool) and tool.name == tool_name:\n                                tool_to_call = tool\n                                break\n\n                        if not tool_to_call:\n                            logger.warning(f\"[A2A Skill] ADK tool '{tool_name}' not found.\")\n                            return f\"Error: ADK tool '{tool_name}' not found on this agent.\"\n\n                        # --- Bridge sync skill call to async ADK tool execution ---\n                        async def run_adk_tool_async():\n                            try:\n                                # ADK tools require ToolContext. We can provide a minimal one or None.\n                                # Providing None might limit tool functionality.\n                                # Let's try providing None for simplicity first.\n                                adk_tool_context = None\n\n                                # Check if the tool has an async run method (most ADK tools should)\n                                if hasattr(tool_to_call, 'run_async') and iscoroutinefunction(tool_to_call.run_async):\n                                    # Pass arguments directly to run_async\n                                    result = await tool_to_call.run_async(args=arguments, tool_context=adk_tool_context)\n                                    # Convert result to string for A2A response\n                                    if isinstance(result, str): return result\n                                    try: return json.dumps(result)\n                                    except: return str(result)\n                                elif hasattr(tool_to_call, 'run') and callable(tool_to_call.run):\n                                    # Fallback to synchronous run in thread pool\n                                    logger.warning(f\"ADK tool '{tool_name}' has no run_async, using synchronous run in thread.\")\n                                    result = await asyncio.to_thread(tool_to_call.run, args=arguments, tool_context=adk_tool_context)\n                                    if isinstance(result, str): return result\n                                    try: return json.dumps(result)\n                                    except: return str(result)\n                                else:\n                                     return f\"Error: ADK tool '{tool_name}' has no callable run or run_async method.\"\n\n                            except Exception as e:\n                                logger.error(f\"[A2A Skill] Error executing ADK tool '{tool_name}': {e}\", exc_info=True)\n                                return f\"Error executing ADK tool {tool_name}: {e}\"\n\n                        # Execute the async tool runner\n                        try:\n                            return asyncio.run(run_adk_tool_async())\n                        except RuntimeError:\n                            logger.error(f\"RuntimeError calling asyncio.run in execute_adk_tool_skill for tool {tool_name}.\")\n                            return \"Error: Could not execute ADK tool asynchronously.\"\n\n            # --- End of Skill Definitions ---\n\n        # Instantiate the dynamic server class\n        try:\n             self.a2a_server = DynamicA2AServer(**a2a_server_options)\n             logger.info(f\"A2A server instance created for agent '{self.amd.name}'.\")\n             return self.a2a_server\n        except Exception as e:\n             logger.error(f\"Failed to instantiate dynamic A2A Server: {e}\", exc_info=True)\n             return None\n\n\n    def run_a2a_server(self, host=\"0.0.0.0\", port=5000, **kwargs):\n        \"\"\"Starts the A2A server (blocking) using the python-a2a run_server function.\"\"\"\n        if not self.a2a_server:\n            logger.error(\"A2A server not initialized. Call setup_a2a_server first.\")\n            return\n        if not A2A_AVAILABLE:\n            logger.error(\"python-a2a library not available. Cannot run A2A server.\")\n            return\n\n        # Get effective host/port from server instance if set, otherwise use args\n        effective_host = getattr(self.a2a_server, 'host', host)\n        effective_port = getattr(self.a2a_server, 'port', port)\n\n        logger.info(f\"Starting A2A server for agent '{self.amd.name}' via run_server_func on {effective_host}:{effective_port}...\")\n        try:\n            # Call the imported run_server function, passing the agent instance\n            run_a2a_server_func(self.a2a_server, host=effective_host, port=effective_port, **kwargs) # This blocks\n        except Exception as e:\n            logger.error(f\"A2A server failed to run: {e}\", exc_info=True)\n\n    async def setup_a2a_client(self, target_agent_url: str) -&gt; A2AClient | None:\n        \"\"\"Gets or creates an A2A client for a specific target agent URL using python-a2a.\"\"\"\n        if not A2A_AVAILABLE:\n            logger.warning(\"python-a2a library not installed. Cannot setup A2A client.\")\n            return None\n\n        async with self.a2a_client_lock:\n            if target_agent_url in self.a2a_clients:\n                logger.debug(f\"Reusing cached A2A client for {target_agent_url}\")\n                return self.a2a_clients[target_agent_url]\n\n            logger.info(f\"Setting up A2A client for target: {target_agent_url}\")\n            try:\n                # python-a2a client likely fetches card on init or first call\n                client = A2AClient(base_url=target_agent_url) # Pass the URL directly\n                # Verify connection implicitly by getting card (optional, client might do lazy loading)\n                # agent_card = await client.get_agent_card() # If method exists\n                # logger.info(f\"Successfully connected A2A client to agent: {agent_card.name}\")\n                self.a2a_clients[target_agent_url] = client\n                logger.info(f\"A2A client created for target: {target_agent_url}\")\n                return client\n            except Exception as e:\n                logger.error(f\"Failed to setup A2A client for {target_agent_url}: {e}\", exc_info=True)\n                return None\n\n    async def close_a2a_clients(self):\n        \"\"\"Closes all cached A2A client connections.\"\"\"\n        async with self.a2a_client_lock:\n            logger.info(f\"Closing {len(self.a2a_clients)} A2A clients.\")\n            # A2AClient may manage underlying httpx clients automatically.\n            # If explicit close needed in future versions, add here.\n            # for client in self.a2a_clients.values():\n            #     await client.close() # If available\n            self.a2a_clients.clear()\n\n    def setup_adk_runner(self, runner_options: dict[str, Any] | None = None):\n        \"\"\"Initializes an ADK runner for this agent (if ADK enabled).\"\"\"\n        if not ADK_AVAILABLE:\n            logger.warning(\"ADK not available. Cannot setup ADK runner.\")\n            return None\n        if not isinstance(self, LlmAgent):\n            logger.error(\"Agent must inherit from LlmAgent to use ADK runner directly.\")\n            return None\n        if self.adk_runner:\n            logger.warning(\"ADK runner already initialized.\")\n            return self.adk_runner\n\n        runner_opts = runner_options or {}\n        runner_class = runner_opts.pop(\"runner_class\", InMemoryRunner) # Default to InMemory\n        app_name = runner_opts.pop(\"app_name\", f\"{self.amd.name}_ADKApp\")\n\n        if runner_class == InMemoryRunner:\n            runner_opts = {}\n\n        logger.info(f\"Setting up ADK Runner ({runner_class.__name__}) for app '{app_name}'...\")\n\n        try:\n             # Pass the agent instance and other options to the runner constructor\n            self.adk_runner = runner_class(agent=self, app_name=app_name, **runner_opts)\n            self.adk_session_service = self.adk_runner.session_service # Store session service\n            logger.info(f\"ADK {runner_class.__name__} setup complete for agent '{self.amd.name}'.\")\n            return self.adk_runner\n        except Exception as e:\n            logger.error(f\"Failed to setup ADK runner: {e}\", exc_info=True)\n            self.adk_runner = None\n            self.adk_session_service = None\n            return None\n\n\n    # --- Core Agent Logic (`a_run`) ---\n\n    async def a_run(self,\n                    user_input: str,\n                    session_id: Optional[str] = None,\n                    persist_history: bool = True,\n                    strategy_override: ProcessingStrategy | None = None,\n                    kwargs_override: dict[str, Any] | None = None, # For fine-grained control\n                    a2a_task_id: Optional[str] = None # Context if called from A2A task\n                    ) -&gt; str:\n        \"\"\"\n        Main asynchronous execution logic for the agent turn.\n\n        Orchestrates world model updates, state sync, strategy selection,\n        execution, cost tracking, and callbacks.\n        \"\"\"\n        self.internal_state = InternalAgentState.PROCESSING\n        start_time = time.monotonic()\n        session_id = session_id or \"default\" # Use 'default' if none provided\n        response = \"Error: Processing failed.\" # Default error\n        turn_cost = 0.0\n        span = None # OTel span\n\n        if not self.tracer: self._setup_telemetry() # Ensure tracer exists\n\n        try:\n            with self.tracer.start_as_current_span(f\"Agent Run: {self.amd.name}\", attributes={\"session_id\": session_id}) as span:\n\n                # Ensure session history list exists\n                if session_id not in self.message_history:\n                    logger.debug(f\"Initializing history for session: {session_id}\")\n                    self.message_history[session_id] = []\n\n                logger.info(f\"--- Agent Run Start (Session: {session_id}) ---\")\n                span.add_event(\"Agent run started\")\n                logger.info(f\"User Input: {user_input[:100]}...\")\n                span.set_attribute(\"user_input\", user_input[:500]) # Log truncated input\n\n                # 0. Get ADK Session State (if ADK enabled and syncing)\n                adk_session_state = None\n                if self.sync_adk_state and self.adk_session_service:\n                    try:\n                        # ADK SessionService methods are typically synchronous\n                        # Run in threadpool to avoid blocking\n                        adk_session = await asyncio.to_thread(\n                             self.adk_session_service.get_session,\n                             app_name=self.adk_runner.app_name, # Assuming runner is set if syncing\n                             user_id=self.amd.user_id or \"adk_user\", # Needs consistent user ID\n                             session_id=session_id\n                        )\n                        if adk_session:\n                            adk_session_state = adk_session.state\n                        else:\n                            logger.warning(f\"ADK Session '{session_id}' not found for state sync.\")\n                            # Optionally create session here? Be careful about race conditions.\n                    except Exception as sync_e:\n                        logger.error(f\"Error getting ADK session state for sync: {sync_e}\")\n\n                # 1. Update World Model &amp; Sync State (Run *before* strategy selection)\n                # flow_world_model is now responsible for syncing *from* ADK state initially\n                await self.flow_world_model(user_input, session_id, adk_session_state)\n                span.add_event(\"World model updated\")\n\n                # 2. Prepare message history for this turn\n                current_turn_messages = self._prepare_llm_messages(user_input, session_id)\n                span.set_attribute(\"history_length\", len(current_turn_messages) -1) # Exclude current input\n\n                # 3. Determine Processing Strategy\n                if strategy_override:\n                    strategy = strategy_override\n                    strategy_reasoning = \"Strategy overridden by caller.\"\n                    logger.info(f\"Strategy forced by override: {strategy.value}\")\n                else:\n                    strategy, strategy_reasoning = self._determine_strategy_heuristic(user_input, current_turn_messages)\n                    logger.info(f\"Strategy Selected: {strategy.value} (Reason: {strategy_reasoning})\")\n                span.set_attribute(\"selected_strategy\", strategy.value)\n                span.set_attribute(\"strategy_reasoning\", strategy_reasoning)\n\n\n                # --- Prepare kwargs for execution based on strategy ---\n                exec_kwargs = kwargs_override or {}\n                exec_kwargs['session_id'] = session_id\n                exec_kwargs['user_input'] = user_input\n                exec_kwargs['current_turn_messages'] = current_turn_messages\n                exec_kwargs['adk_session_state'] = adk_session_state # Pass state for potential use/update\n\n\n                # 4. Execute Selected Strategy\n                logger.info(f\"Executing strategy: {strategy.value}\")\n                if strategy == ProcessingStrategy.ADK_RUN:\n                    if ADK_AVAILABLE and self.adk_runner:\n                        response = await self._execute_adk_run(**exec_kwargs)\n                    else:\n                        logger.warning(\"ADK_RUN strategy selected, but ADK runner not available/configured. Falling back.\")\n                        # Fallback strategy? Maybe DIRECT_LLM?\n                        strategy = ProcessingStrategy.DIRECT_LLM\n                        response = await self._execute_direct_llm(**exec_kwargs)\n\n                elif strategy == ProcessingStrategy.A2A_CALL:\n                    if A2A_AVAILABLE:\n                        response = await self._execute_a2a_call(**exec_kwargs)\n                    else:\n                        logger.warning(\"A2A_CALL strategy selected, but A2A not available. Falling back.\")\n                        strategy = ProcessingStrategy.DIRECT_LLM\n                        response = await self._execute_direct_llm(**exec_kwargs)\n\n                else: # Default: DIRECT_LLM\n                    response = await self._execute_direct_llm(**exec_kwargs)\n\n                span.set_attribute(\"raw_response_length\", len(response))\n                span.add_event(\"Strategy execution complete\")\n\n                # 5. Persist History (if successful and enabled)\n                # Add assistant response to history\n                if persist_history and not response.startswith(\"Error:\"):\n                     self._add_to_history(session_id, LLMMessage(role=\"assistant\", content=response).to_dict())\n\n                # 6. Sync World Model *back* to ADK State (if changed and enabled)\n                if self.sync_adk_state and adk_session_state is not None:\n                    try:\n                        self.world_model.sync_to_adk_state(adk_session_state)\n                        span.add_event(\"ADK state synchronized and updated\")\n                    except Exception as sync_e:\n                         logger.error(f\"Error syncing/updating ADK session state: {sync_e}\")\n                         span.record_exception(sync_e)\n\n                # 7. Track Cost (using last_llm_result if available)\n                if self.last_llm_result:\n                    try:\n                        cost = completion_cost(completion_response=self.last_llm_result, model=self.amd.model)\n                        if cost:\n                            turn_cost = cost\n                            self.total_cost += turn_cost\n                            logger.info(f\"Turn Cost: ${turn_cost:.6f}, Total Cost: ${self.total_cost:.6f}\")\n                            span.set_attribute(\"llm_cost\", turn_cost)\n                            span.set_attribute(\"total_agent_cost\", self.total_cost)\n                        self.last_llm_result = None # Clear after use\n                    except Exception as cost_e:\n                        logger.warning(f\"Failed to calculate cost: {cost_e}\")\n                        span.add_event(\"Cost calculation failed\", attributes={\"error\": str(cost_e)})\n\n\n                # 8. Run Post Callback\n                if self.post_run_callback and not response.startswith(\"Error:\"):\n                    try:\n                        if iscoroutinefunction(self.post_run_callback):\n                            await self.post_run_callback(session_id, response, turn_cost)\n                        else:\n                            self.post_run_callback(session_id, response, turn_cost)\n                        span.add_event(\"Post-run callback executed\")\n                    except Exception as cb_e:\n                        logger.error(f\"Post-run callback failed: {cb_e}\", exc_info=True)\n                        span.record_exception(cb_e)\n\n\n                logger.info(f\"Agent Run finished in {time.monotonic() - start_time:.2f}s. Response: {response[:100]}...\")\n\n        except Exception as e:\n            logger.error(f\"Error during agent run (Session: {session_id}): {e}\", exc_info=True)\n            self.internal_state = InternalAgentState.ERROR\n            response = f\"Error: An internal error occurred during processing: {str(e)}\"\n            if span:\n                 span.set_status(trace.Status(trace.StatusCode.ERROR, f\"Agent run failed: {e}\"))\n                 span.record_exception(e)\n        finally:\n            self.internal_state = InternalAgentState.IDLE\n            if span: span.end() # Ensure span is closed\n            logger.info(f\"--- Agent Run End (Session: {session_id}) ---\")\n\n        return str(response) # Ensure string output\n\n    def run(self, user_input: str, session_id: Optional[str] = None, **kwargs) -&gt; str:\n        \"\"\"Synchronous wrapper for a_run.\"\"\"\n        try:\n            # get_event_loop() is deprecated in 3.10+, use get_running_loop() or new_event_loop()\n            try:\n                asyncio.get_running_loop()\n                # If loop is running, cannot use asyncio.run. Need to schedule and wait.\n                # This is complex to get right universally (e.g., in notebooks vs servers).\n                # Simplest approach for sync call from sync context is asyncio.run()\n                # If called from async context, user should await a_run() directly.\n                logger.warning(\"Synchronous 'run' called from a running event loop. \"\n                               \"This might block the loop. Consider using 'await a_run'.\")\n                # Fallback to basic run, may error if loop is running\n                return asyncio.run(self.a_run(user_input, session_id=session_id, **kwargs))\n            except RuntimeError: # No running event loop\n                 return asyncio.run(self.a_run(user_input, session_id=session_id, **kwargs))\n        except Exception as e:\n            logger.error(f\"Error in synchronous run wrapper: {e}\", exc_info=True)\n            return f\"Error: Failed to execute synchronous run: {e}\"\n\n    # --- Strategy Determination ---\n\n    def _determine_strategy_heuristic(self, user_input: str, messages: list[dict]) -&gt; tuple[ProcessingStrategy, str]:\n        \"\"\"Determines the processing strategy using heuristics (faster than LLM).\"\"\"\n        # 1. Check for keywords indicating specific needs\n        input_lower = user_input.lower()\n        # Example Keywords:\n        code_keywords = {\"execute\", \"run code\", \"python\", \"calculate\", \"script\"}\n        search_keywords = {\"search\", \"google\", \"find information\", \"what is\", \"who is\"}\n        agent_keywords = {\"ask agent\", \"tell agent\", \"delegate to\"} # Keywords for A2A/MCP delegation\n        tool_keywords = {\"use tool\", \"run tool\"} # Keywords for specific tool use\n\n        # 2. Check Agent Capabilities (Tools, Servers, Clients)\n        has_adk_tools = ADK_AVAILABLE and isinstance(self, LlmAgent) and bool(self.tools)\n        has_adk_code_executor = ADK_AVAILABLE and isinstance(self, LlmAgent) and self.code_executor is not None\n        can_do_adk_search = any(isinstance(t, type(adk_google_search) | AdkVertexAiSearchTool) for t in getattr(self, 'tools', []))\n        can_do_a2a = A2A_AVAILABLE and bool(self.a2a_clients) # Check if clients configured\n        # MCP check relies on tools being added via MCPToolset in ADK\n        has_adk_tools and any(isinstance(t, BaseTool) and getattr(t, '_is_mcp_tool', False) for t in self.tools) # Heuristic\n\n\n        # --- Strategy Logic ---\n        # Priority: ADK (if tools/code/search needed) &gt; A2A (if delegation requested) &gt; Direct LLM\n\n        # ADK: If code execution or search is explicitly requested or implied, or specific ADK tools mentioned\n        if ADK_AVAILABLE and self.adk_runner:\n            if has_adk_code_executor and any(kw in input_lower for kw in code_keywords):\n                return ProcessingStrategy.ADK_RUN, \"Input suggests code execution, using ADK.\"\n            if can_do_adk_search and any(kw in input_lower for kw in search_keywords):\n                 return ProcessingStrategy.ADK_RUN, \"Input suggests web/data search, using ADK.\"\n            # Check if input mentions names of specific ADK tools\n            if has_adk_tools:\n                tool_names = {t.name.lower() for t in self.tools}\n                if any(f\" {name} \" in input_lower for name in tool_names) or any(kw in input_lower for kw in tool_keywords):\n                     return ProcessingStrategy.ADK_RUN, \"Input mentions specific ADK tool or requests tool use.\"\n            # General ADK case: If ADK is primary mode and input isn't trivial\n            if len(user_input.split()) &gt; 5: # Simple heuristic for non-trivial input\n                # If ADK tools exist, assume ADK might be needed for planning\n                if has_adk_tools:\n                    return ProcessingStrategy.ADK_RUN, \"Complex input and ADK tools available, using ADK planning.\"\n                # If only basic LLM agent, still might use ADK runner for session mgmt? Check config.\n                # Defaulting to DIRECT_LLM if no specific ADK features seem required.\n\n        # A2A: If delegation is requested and A2A clients are available\n        if can_do_a2a and any(kw in input_lower for kw in agent_keywords):\n             # : Could use LLM here to extract target agent if multiple clients exist\n            return ProcessingStrategy.A2A_CALL, \"Input suggests delegating to another agent.\"\n\n        # Fallback: Direct LLM\n        return ProcessingStrategy.DIRECT_LLM, \"Input seems suitable for direct LLM processing.\"\n\n\n    # --- Strategy Execution Helpers ---\n\n    def _prepare_llm_messages(self, user_input: str, session_id: str) -&gt; list[dict]:\n        \"\"\"Prepares the list of messages for the LLM call, including history and system prompts.\"\"\"\n        session_history = self.message_history.get(session_id, [])\n\n        # Construct message list\n        messages: list[dict] = []\n        messages.extend(self.construct_initial_prompts()) # System/world model/tool prompts\n        # Add history (ensure alternating roles if possible, handle potential issues)\n        messages.extend(session_history)\n        # Add current user input\n        messages.append(LLMMessage(role=\"user\", content=user_input).to_dict())\n\n        # Trim messages based on token count or turn limit\n        trimmed_messages = self._trim_messages(messages)\n\n        # Add user input to persistent history *before* LLM call\n        # Note: assistant response added *after* successful call in a_run\n        self._add_to_history(session_id, LLMMessage(role=\"user\", content=user_input).to_dict())\n\n        return trimmed_messages\n\n    async def _execute_direct_llm(self, current_turn_messages: list[dict], session_id: str, **kwargs) -&gt; str:\n        \"\"\"Executes a direct call to the LLM using LiteLLM.\"\"\"\n        logger.debug(\"Executing direct LLM call...\")\n        if not current_turn_messages: return \"Error: No messages prepared for LLM.\"\n        try:\n            response_content = await self.a_run_llm_completion(current_turn_messages)\n            return response_content\n        except Exception as e:\n            logger.error(f\"Direct LLM execution failed: {e}\", exc_info=True)\n            return f\"Error during LLM generation: {e}\"\n\n    async def _execute_adk_run(self, user_input: str, session_id: str, adk_session_state: State | None, **kwargs) -&gt; str:\n        \"\"\"Executes the agent's logic using the configured ADK runner.\"\"\"\n        if not self.adk_runner or not self.adk_session_service:\n            return \"Error: ADK Runner or Session Service is not configured for this agent.\"\n\n        logger.debug(f\"Executing ADK run for session {session_id}...\")\n        final_response_text = \"Error: ADK processing did not yield a final textual response.\"\n        # Use user_id from AMD if available, default otherwise\n        user_id = self.amd.user_id or \"adk_user\"\n        app_name = self.adk_runner.app_name\n\n        try:\n            # 1. Ensure ADK session exists\n            try:\n                # Check and potentially create session (synchronous, run in thread)\n                session_exists = await asyncio.to_thread(\n                    self.adk_session_service.get_session, app_name=app_name, user_id=user_id, session_id=session_id\n                )\n                if not session_exists:\n                     logger.info(f\"Creating ADK session {session_id} for user {user_id} in app {app_name}\")\n                     # Pass initial state from World Model if syncing\n                     initial_state = self.world_model.to_dict() if self.sync_adk_state else {}\n                     await asyncio.to_thread(\n                         self.adk_session_service.create_session,\n                         app_name=app_name, user_id=user_id, session_id=session_id,\n                         state=initial_state\n                     )\n                elif adk_session_state is None and self.sync_adk_state:\n                    # If session existed but we couldn't get state earlier, try again\n                     session = await asyncio.to_thread(self.adk_session_service.get_session, app_name=app_name, user_id=user_id, session_id=session_id)\n                     if session: adk_session_state = session.state\n\n            except Exception as session_e:\n                logger.error(f\"Failed to ensure ADK session {session_id}: {session_e}\", exc_info=True)\n                return f\"Error setting up ADK session: {session_e}\"\n\n            # 2. Prepare ADK input (handle multi-modal later)\n            # Assuming user_input is text for now\n            adk_input_content = Content(role='user', parts=[Part(text=user_input)])\n\n            # 3. Execute ADK run_async\n            all_events_str = [] # For logging/debugging\n            async for event in self.adk_runner.run_async(\n                user_id=user_id, session_id=session_id, new_message=adk_input_content):\n\n                # Log event details (optional, can be verbose)\n                try:\n                    event_dict = event.model_dump(exclude_none=True)\n                    all_events_str.append(json.dumps(event_dict, default=str)) # Serialize complex types\n                    logger.debug(f\"ADK Event ({event.author}): {all_events_str[-1]}\")\n                except Exception as log_e:\n                    logger.debug(f\"ADK Event ({event.author}): [Error logging event details: {log_e}]\")\n\n                # Call progress callback\n                if self.progress_callback:\n                     try:\n                         progress_data = {\"type\": \"adk_event\", \"event\": event.model_dump(exclude_none=True)}\n                         if iscoroutinefunction(self.progress_callback): await self.progress_callback(progress_data)\n                         else: self.progress_callback(progress_data)\n                     except Exception as cb_e: logger.warning(f\"Progress callback failed for ADK event: {cb_e}\")\n\n                # Check for Human-in-Loop triggers (example)\n                #if event.actions and event.actions.request_human_input:\n                #     if self.human_in_loop_callback:\n                #         logger.info(f\"ADK requesting human input: {event.actions.request_human_input.reason}\")\n                         # This needs a mechanism to pause and resume the run_async loop\n                         # HIL is complex with async generators. Placeholder for now.\n                         # human_response = await self.human_in_loop_callback(...)\n                         # Need to inject response back into ADK runner - not straightforward\n               #          logger.warning(\"Human-in-Loop requested by ADK, but interaction is not implemented.\")\n                         # Could potentially send an error response back?\n               #      else:\n               #         logger.warning(\"ADK requested human input, but no HIL callback is configured.\")\n\n\n                # Extract final textual response\n                if event.is_final_response():\n                    # Prioritize text part\n                    if event.content and event.content.parts:\n                        text_parts = [p.text for p in event.content.parts if hasattr(p, 'text')]\n                        if text_parts:\n                            final_response_text = \"\\n\".join(text_parts).strip()\n                        else: # Handle other content types if needed (e.g., function call results as final)\n                            # For now, just serialize the first part if no text found\n                            final_response_text = str(event.content.parts[0]) if event.content.parts else \"ADK finished with non-text content.\"\n                    elif event.actions and event.actions.escalate:\n                        final_response_text = f\"Error: Agent escalated: {event.error_message or 'No specific message.'}\"\n                    elif event.error_message:\n                         final_response_text = f\"Error: ADK processing failed: {event.error_message}\"\n                    else:\n                         final_response_text = \"ADK processing finished without a clear textual response.\"\n                    break # Stop processing events\n\n            # 4. Update World Model from final ADK state (if syncing)\n            # This happens *after* the run completes, the sync in a_run updates the persisted state.\n            if self.sync_adk_state and adk_session_state is not None:\n                 # Fetch potentially updated state after run completion\n                 try:\n                     final_session = await asyncio.to_thread(self.adk_session_service.get_session, app_name=app_name, user_id=user_id, session_id=session_id)\n                     if final_session:\n                         self.world_model.sync_from_adk_state(final_session.state)\n                     else:\n                         logger.warning(f\"Could not fetch final ADK state for session {session_id} after run.\")\n                 except Exception as sync_e:\n                     logger.error(f\"Error fetching final ADK state: {sync_e}\")\n\n\n            logger.debug(\"ADK run finished.\")\n            return final_response_text\n\n        except Exception as e:\n            logger.error(f\"ADK execution failed: {e}\", exc_info=True)\n            # Return partial events log on error for debugging\n            events_preview = \"\\n\".join(all_events_str[:5])\n            return f\"Error during ADK processing: {e}\\nFirst Events:\\n{events_preview}\"\n\n    async def _execute_a2a_call(self, user_input: str, session_id: str, **kwargs) -&gt; str:\n        \"\"\"Executes a call to another agent via A2A using python-a2a and waits for the result.\"\"\"\n\n        client = None\n        task_id = None\n\n        if not A2A_AVAILABLE: return \"Error: python-a2a library not available.\"\n\n        logger.debug(\"Executing A2A call...\")\n\n        target_agent_url = kwargs.get('target_a2a_agent_url')\n        task_prompt = kwargs.get('a2a_task_prompt', user_input)\n\n        if not target_agent_url:\n            if len(self.a2a_clients) == 1:\n                target_agent_url = list(self.a2a_clients.keys())[0]\n                logger.info(f\"Using only available A2A client target: {target_agent_url}\")\n            else:\n                 return \"Error: Target A2A agent URL not specified and multiple clients configured.\"\n        try:\n            client = await self.setup_a2a_client(target_agent_url)\n            if not client:\n                return f\"Error: Could not connect to A2A agent at {target_agent_url}\"\n\n            task_id = str(uuid.uuid4())\n            a2a_session_id = f\"a2a_{session_id}_{task_id[:8]}\"\n\n            logger.info(f\"Sending A2A task '{task_id}' to {target_agent_url}...\")\n\n            # --- Call python-a2a client's task sending method ---\n            # The library might have a high-level `create_task` or similar.\n            # Let's assume a `send_task` method exists that takes message content.\n            # We construct the message payload expected by the library.\n            # This structure might need adjustment based on python-a2a's specifics.\n            message_payload = {\n                \"role\": \"user\", # Assuming MessageRole.USER maps to \"user\"\n                \"content\": {\n                    \"type\": \"text\", # Assuming TextContent maps to this\n                    \"text\": task_prompt\n                 }\n            }\n            # The client method might take id/sessionId separately or as part of a task object\n            # Assuming a method signature like: send_task(message: Dict, task_id: str, session_id: str)\n            # This is an *assumption* based on typical A2A needs.\n            if hasattr(client, 'send_task'):\n                initial_task_info = await client.send_task(\n                    message=message_payload,\n                    task_id=task_id,\n                    session_id=a2a_session_id\n                ) # Adjust call based on actual method signature\n            elif hasattr(client, 'create_task'): # Alternative common pattern\n                 initial_task_info = await client.create_task(\n                     message=message_payload,\n                     task_id=task_id,\n                     session_id=a2a_session_id\n                 )\n            else:\n                 # Fallback to 'ask' if specific task methods are unavailable (less control)\n                 logger.warning(\"A2A client lacks specific send_task/create_task method, using high-level 'ask'. Polling might not work.\")\n                 # 'ask' likely blocks and returns the final result directly\n                 response_text = await client.ask(task_prompt, session_id=a2a_session_id)\n                 return response_text\n\n\n            # --- Process initial response and Poll ---\n            # Check the structure of initial_task_info (might be a Task object, dict, etc.)\n            # Extract initial state if possible\n            initial_state = TaskState.SUBMITTED # Default if state not returned immediately\n            if isinstance(initial_task_info, dict) and initial_task_info.get('status'):\n                initial_state_val = initial_task_info['status'].get('state')\n                if initial_state_val: initial_state = TaskState(initial_state_val) # Convert string to Enum\n            elif hasattr(initial_task_info, 'status') and hasattr(initial_task_info.status, 'state'):\n                 initial_state = initial_task_info.status.state\n\n            logger.info(f\"A2A task submitted (ID: {task_id}). Initial State: {initial_state}\")\n\n            # Don't poll if initial state is already final (unlikely but possible)\n            if initial_state in (TaskState.COMPLETED, TaskState.FAILED, TaskState.CANCELLED):\n                 logger.warning(f\"A2A task {task_id} already in final state {initial_state} after submission.\")\n                 # Need to extract result from initial_task_info here\n                 # ... logic to extract result based on initial_task_info structure ...\n                 return f\"Task finished immediately with state {initial_state}.\" # Placeholder\n\n            self.internal_state = InternalAgentState.WAITING_FOR_TOOL\n            final_result = await self._poll_a2a_task(client, task_id, target_agent_url)\n            self.internal_state = InternalAgentState.PROCESSING\n            return final_result\n\n        except TimeoutError:\n             logger.error(f\"A2A task {task_id} timed out after {self.a2a_poll_timeout}s.\")\n             # Attempt cancellation?\n             cancel_response = \"No clinet\"\n             if client:\n                cancel_response = await client.cancel_task(task_id=task_id)\n             return f\"Error: A2A task timed out waiting for result from {target_agent_url} {cancel_response}.\"\n        except Exception as e:\n            logger.error(f\"A2A execution failed: {e}\", exc_info=True)\n            return f\"Error during A2A call: {e}\"\n\n    async def _poll_a2a_task(self, client: A2AClient, task_id: str, target_url: str) -&gt; str:\n        \"\"\"Polls the GetTask endpoint using python-a2a client until a final state.\"\"\"\n        if not hasattr(client, 'get_task'):\n             raise NotImplementedError(f\"A2A client for {target_url} does not support 'get_task' for polling.\")\n\n        logger.debug(f\"Polling A2A task {task_id} on {target_url}...\")\n        start_time = time.monotonic()\n\n        while time.monotonic() - start_time &lt; self.a2a_poll_timeout:\n            try:\n                # Assume get_task takes task_id (and potentially historyLength)\n                task_details = await client.get_task(task_id=task_id, history_length=1)\n\n                # --- Parse the response (structure depends on python-a2a implementation) ---\n                current_state = TaskState.UNKNOWN\n                final_text = f\"A2A Task {task_id} finished.\"\n                error_message = None\n\n                # Example parsing assuming task_details is dict-like or object-like\n                status_info = None\n                if isinstance(task_details, dict):\n                    status_info = task_details.get('status')\n                elif hasattr(task_details, 'status'):\n                    status_info = task_details.status\n\n                if status_info:\n                    state_val = status_info.get('state') if isinstance(status_info, dict) else getattr(status_info, 'state', None)\n                    if state_val:\n                        try:\n                            current_state = TaskState(state_val) # Convert string to Enum\n                        except ValueError:\n                             logger.warning(f\"Received unknown task state '{state_val}' for task {task_id}\")\n\n                    logger.debug(f\"A2A task {task_id} current state: {current_state}\")\n\n                    # Call progress callback\n                    if self.progress_callback:\n                         # ... (progress callback logic remains the same) ...\n                        pass\n\n                    # Check for final state\n                    if current_state in (TaskState.COMPLETED, TaskState.FAILED, TaskState.CANCELLED):\n                        logger.info(f\"A2A task {task_id} reached final state: {current_state}\")\n\n                        # Extract final result from artifacts\n                        artifacts = task_details.get('artifacts') if isinstance(task_details, dict) else getattr(task_details, 'artifacts', None)\n                        if artifacts and isinstance(artifacts, list) and artifacts:\n                            # Simple extraction: assume first artifact, first part is text\n                            try:\n                                parts = artifacts[0].get('parts') if isinstance(artifacts[0], dict) else getattr(artifacts[0], 'parts', [])\n                                if parts and isinstance(parts, list) and parts:\n                                    text_part = parts[0].get('text') if isinstance(parts[0], dict) else getattr(parts[0], 'text', None)\n                                    if text_part:\n                                        final_text = str(text_part).strip()\n                            except Exception as parse_e:\n                                logger.warning(f\"Could not parse artifacts for task {task_id}: {parse_e}\")\n                                final_text = \"[Could not parse final artifact]\"\n\n                        # Handle failed/cancelled states\n                        if current_state == TaskState.FAILED:\n                            # Try to extract error message from status\n                            status_message_info = status_info.get('message') if isinstance(status_info, dict) else getattr(status_info, 'message', None)\n                            if status_message_info:\n                                # Assuming message content is similar structure to artifacts\n                                try:\n                                     err_content = status_message_info.get('content') if isinstance(status_message_info, dict) else getattr(status_message_info, 'content', None)\n                                     if err_content:\n                                         error_message = err_content.get('text') if isinstance(err_content, dict) else getattr(err_content, 'text', 'Unknown error')\n                                except: pass # Ignore parsing errors here\n                            return f\"Error: A2A task failed on {target_url}: {error_message or final_text}\"\n                        elif current_state == TaskState.CANCELLED:\n                            return f\"Info: A2A task was cancelled on {target_url}.\"\n                        else: # Completed\n                            return final_text\n\n                else:\n                    logger.warning(f\"A2A get_task for {task_id} returned no status info: {task_details}\")\n\n            except APIConnectionError as conn_e:\n                 logger.warning(f\"Connection error polling A2A task {task_id}: {conn_e}. Retrying...\")\n            except Exception as e:\n                logger.error(f\"Error polling A2A task {task_id}: {e}\", exc_info=True)\n                return f\"Error polling A2A task status: {e}\"\n\n            await asyncio.sleep(self.a2a_poll_interval)\n\n        raise TimeoutError(f\"Polling A2A task {task_id} timed out.\")\n\n    # --- Internal Helper Methods ---\n\n    def construct_initial_prompts(self) -&gt; list[dict]:\n        \"\"\"Constructs the initial system/context messages for the LLM prompt.\"\"\"\n        messages = []\n        # Base System Prompt\n        if self.amd.system_message:\n            messages.append(LLMMessage(\"system\", self.amd.system_message).to_dict())\n\n        # World Model Context\n        wm_repr = self.world_model.show()\n        if wm_repr != \"[empty]\":\n            messages.append(LLMMessage(\"system\", f\"Current World State:\\n{wm_repr}\").to_dict())\n\n        # Capabilities Overview (ADK specific parts depend on LlmAgent inheritance)\n        caps = [\"LiteLLM (Core LLM access)\"]\n        if ADK_AVAILABLE and isinstance(self, LlmAgent):\n            if self.tools: caps.append(\"ADK Tools (including potential MCP/A2A wrappers)\")\n            if self.code_executor: caps.append(\"ADK Code Execution\")\n            if any(isinstance(t, type(adk_google_search) | AdkVertexAiSearchTool) for t in getattr(self, 'tools', [])):\n                 caps.append(\"ADK Search\")\n        if A2A_AVAILABLE and self.a2a_clients: caps.append(\"A2A Client (delegate to other agents)\")\n        if self.mcp_server: caps.append(\"MCP Server (exposes capabilities)\")\n        if self.a2a_server: caps.append(\"A2A Server (receives tasks)\")\n\n        messages.append(LLMMessage(\"system\", f\"Your Capabilities: {', '.join(caps)}.\").to_dict())\n\n        # ADK Tool Instructions (if ADK enabled and tools exist)\n        if ADK_AVAILABLE and isinstance(self, LlmAgent) and self.tools:\n            try:\n                # Use ADK's internal method to get schema if possible, otherwise basic list\n                tool_schemas = getattr(self, 'tool_schemas', None) # ADK might populate this\n                if tool_schemas:\n                     tool_list_str = json.dumps(tool_schemas, indent=2)\n                     messages.append(LLMMessage(\"system\", f\"You have access to the following tools (use FunctionCall format):\\n{tool_list_str}\").to_dict())\n                else: # Fallback to basic list\n                    tool_list = \"\\n\".join([f\"- {tool.name}: {tool.description or 'No description'}\" for tool in self.tools])\n                    messages.append(LLMMessage(\"system\", f\"You can use the following tools:\\n{tool_list}\\nRespond with a FunctionCall to use a tool.\").to_dict())\n            except Exception as e:\n                 logger.warning(f\"Could not generate detailed ADK tool instructions: {e}\")\n\n\n        # Add specific instructions for A2A delegation if needed\n        if A2A_AVAILABLE and self.a2a_clients:\n             client_names = list(self.a2a_clients.keys()) # Target URLs act as names here\n             messages.append(LLMMessage(\"system\", f\"You can delegate tasks to other agents via A2A using their URLs (e.g., {client_names[0]} if available). Indicate clearly if you want to delegate.\").to_dict())\n\n        return messages\n\n    def _add_to_history(self, session_id: str, message: dict[str, Any]):\n         \"\"\"Adds a message to the session history, respecting limits.\"\"\"\n         if session_id not in self.message_history:\n              self.message_history[session_id] = []\n         self.message_history[session_id].append(message)\n\n         # Apply trimming immediately after adding (simpler than doing it before call)\n         self.message_history[session_id] = self._trim_messages(self.message_history[session_id])\n\n\n    def _trim_messages(self, messages: list[dict]) -&gt; list[dict]:\n        \"\"\"Trims message list based on configured strategy (tokens or turns).\"\"\"\n        if self.max_history_tokens and self.amd.model:\n            # Token-based trimming\n            max_tokens = self.max_history_tokens\n            if self.trim_strategy == \"litellm\":\n                try:\n                    trimmed = trim_messages(messages, model=self.amd.model, max_tokens=max_tokens)\n                    if len(trimmed) &lt; len(messages):\n                        logger.debug(f\"Trimmed history from {len(messages)} to {len(trimmed)} messages using LiteLLM token strategy ({max_tokens} tokens).\")\n                    return trimmed\n                except Exception as e:\n                    logger.warning(f\"LiteLLM trimming failed ({e}), falling back to basic token trim.\")\n                    # Fallthrough to basic token trim\n            # Basic token trim (keep system, remove oldest convo pairs)\n            system_msgs = [m for m in messages if m.get('role') == 'system']\n            convo_msgs = [m for m in messages if m.get('role') != 'system']\n            current_tokens = token_counter(messages=messages, model=self.amd.model)\n            while current_tokens &gt; max_tokens and len(convo_msgs) &gt;= 2:\n                 convo_msgs = convo_msgs[2:] # Remove oldest pair\n                 current_tokens = token_counter(messages=system_msgs + convo_msgs, model=self.amd.model)\n            final_messages = system_msgs + convo_msgs\n            if len(final_messages) &lt; len(messages):\n                 logger.debug(f\"Trimmed history from {len(messages)} to {len(final_messages)} messages using basic token strategy ({max_tokens} tokens).\")\n            return final_messages\n\n        elif self.max_history_turns &gt; 0:\n            # Turn-based trimming\n            system_msgs = [m for m in messages if m.get('role') == 'system']\n            convo_msgs = [m for m in messages if m.get('role') != 'system']\n            # Keep last N turns (each turn = user + assistant = 2 messages)\n            max_convo_messages = self.max_history_turns * 2\n            if len(convo_msgs) &gt; max_convo_messages:\n                trimmed_convo = convo_msgs[-max_convo_messages:]\n                logger.debug(f\"Trimmed history from {len(convo_msgs)//2} to {len(trimmed_convo)//2} turns.\")\n                return system_msgs + trimmed_convo\n            else:\n                return messages # No trimming needed\n        else:\n            # No trimming configured or possible\n            logger.warning(\"History trimming not configured or possible (missing max_tokens/model or max_turns).\")\n            return messages\n\n\n    async def a_run_llm_completion(self, llm_messages: list[dict]=None, **kwargs) -&gt; str:\n        \"\"\"Core wrapper around LiteLLM acompletion with error handling, streaming, and cost tracking.\"\"\"\n        if not llm_messages:\n            if \"messages\" in kwargs:\n                llm_messages = kwargs.pop(\"messages\")\n            if \"llm_messages\" in kwargs:\n                llm_messages = kwargs.pop(\"llm_messages\")\n            if not llm_messages:\n                logger.warning(\"a_run_llm_completion called with empty message list.\")\n                return \"Error: No message provided to the model.\"\n\n        self.print_verbose(f\"Running model '{self.amd.model}' with {len(llm_messages)} messages.\")\n        # self.print_verbose(\"Messages:\", json.dumps(llm_messages, indent=2)) # Very verbose\n\n        # Prepare LiteLLM parameters from AgentModelData and kwargs overrides\n        params = {\n            'model': self.format_model or self.amd.model,\n            'messages': llm_messages,\n            'temperature': self.amd.temperature,\n            'top_p': self.amd.top_p,\n            'top_k': self.amd.top_k,\n            'max_tokens': self.amd.max_tokens,\n            'stream': self.stream,\n            'stop': self.amd.stop_sequence,\n            'user': self.amd.user_id,\n            'api_base': self.amd.api_base,\n            'api_version': self.amd.api_version,\n            'api_key': self.amd.api_key,\n            'presence_penalty': self.amd.presence_penalty,\n            'frequency_penalty': self.amd.frequency_penalty,\n            'caching': self.amd.caching,\n            'response_format': kwargs.get('response_format'), # For a_format_class\n            'tools': kwargs.get('tools'), # For LiteLLM function calling (less common now with ADK)\n        }\n        # Filter out None values as LiteLLM prefers absence over None for some params\n        params = {k: v for k, v in params.items() if v is not None}\n\n        # Add budget manager if present\n        if self.amd.budget_manager: params['budget_manager'] = self.amd.budget_manager\n\n        full_response_content = \"\"\n        tool_calls_requested = None # Store tool calls if generated\n\n        try:\n            response_object = await acompletion(**params)\n\n            if self.stream:\n                collected_chunks = []\n                async for chunk in response_object:\n                    # Store raw chunk for potential analysis or replay\n                    collected_chunks.append(chunk)\n                    # Extract text delta\n                    chunk_delta = chunk.choices[0].delta.content or \"\"\n                    if chunk_delta:\n                        full_response_content += chunk_delta\n                        if self.stream_callback:\n                             try:\n                                 # Provide only the new text chunk\n                                 if iscoroutinefunction(self.stream_callback): await self.stream_callback(chunk_delta)\n                                 else: self.stream_callback(chunk_delta)\n                             except Exception as cb_e:\n                                 logger.warning(f\"Stream callback failed: {cb_e}\")\n                    # Check for tool call deltas (less common in streaming)\n                    tool_deltas = chunk.choices[0].delta.tool_calls\n                    if tool_deltas:\n                         logger.warning(\"Received tool call delta during streaming - handling may be incomplete.\")\n                         # : Implement robust handling of streaming tool calls if needed\n\n                # After stream, construct a final response object mimicking non-streaming one for cost tracking\n                # This is an approximation, LiteLLM might offer better ways.\n                final_choice = {\"message\": {\"role\": \"assistant\", \"content\": full_response_content}}\n                # If tool calls were detected during streaming, add them (complex to reconstruct accurately)\n                # if reconstructed_tool_calls: final_choice[\"message\"][\"tool_calls\"] = reconstructed_tool_calls\n                self.last_llm_result = {\n                    \"choices\": [{\"message\": final_choice[\"message\"]}],\n                    \"model\": self.amd.model, # Needed for cost tracking\n                    # Usage stats are often missing or zero in streaming chunks, need final value if available\n                    \"usage\": getattr(collected_chunks[-1], 'usage', {\"prompt_tokens\": 0, \"completion_tokens\": 0})\n                }\n\n            else: # Non-streaming\n                self.last_llm_result = response_object # Store the full response\n                # Extract content and potential tool calls\n                message = response_object.choices[0].message\n                full_response_content = message.content or \"\"\n                tool_calls_requested = message.tool_calls # List of ToolCall objects\n\n                # Check if LiteLLM did function/tool calling (different from ADK tools)\n                # This path is less likely if using ADK, but supported by LiteLLM\n                if tool_calls_requested:\n                    logger.info(f\"LiteLLM requested {len(tool_calls_requested)} tool calls.\")\n                    # This requires a separate mechanism to execute these LiteLLM-requested tools\n                    # and send back 'tool' role messages in the next turn.\n                    # Not implemented here as focus is on ADK/A2A tools.\n                    # For now, return a message indicating tool call request.\n                    calls_repr = \", \".join([f\"{tc.function.name}\" for tc in tool_calls_requested])\n                    return f\"Info: LLM requested tool calls ({calls_repr}). Direct execution not implemented.\"\n\n\n            self.print_verbose(f\"Model Response: {full_response_content[:100]}...\")\n            return full_response_content\n\n        except RateLimitError as e:\n            logger.error(f\"Rate limit error from {self.amd.model}: {e}\")\n            # Implement backoff/retry? For now, re-raise.\n            raise\n        except (BadRequestError, APIConnectionError, InternalServerError) as e:\n            logger.error(f\"API/Server error during LiteLLM call for {self.amd.model}: {e}\", exc_info=True)\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error during LiteLLM completion: {e}\", exc_info=True)\n            raise\n\n    async def a_format_class(self,\n                             pydantic_model: type[BaseModel],\n                             prompt: str,\n                             message_context: list[dict] | None = None,\n                             max_retries: int = 2) -&gt; dict[str, Any]:\n        \"\"\"Uses LiteLLM's response_format feature to get structured JSON output, with retries.\"\"\"\n        logger.debug(f\"Formatting prompt for Pydantic model: {pydantic_model.__name__}\")\n        model_schema = pydantic_model.model_json_schema()\n\n        messages = message_context or []\n        # System prompt explaining the task and schema\n        messages.append({\n            \"role\": \"system\",\n            \"content\": f\"Your task is to analyze the user's request and extract information into a JSON object.\\n\"\n                       f\"Strictly adhere to the following Pydantic schema:\\n\"\n                       f\"```json\\n{json.dumps(model_schema, indent=2)}\\n```\\n\"\n                       f\"Guidelines:\\n\"\n                       f\"- Analyze the request carefully.\\n\"\n                       f\"- Output *only* the JSON object, nothing else (no explanations, apologies, or markdown).\\n\"\n                       f\"- Ensure the JSON is valid and conforms exactly to the schema.\\n\"\n                       f\"- Omit optional fields if the information is not present in the request.\"\n        })\n        messages.append({\"role\": \"user\", \"content\": prompt})\n\n        # Use LiteLLM's JSON mode (requires compatible model/provider)\n        response_format_config = {\"type\": \"json_object\"}\n        # Some providers might need the schema explicitly even in json_object mode\n        # response_format_config = {\"type\": \"json_object\", \"schema\": model_schema}\n\n        original_stream_state = self.stream\n        self.stream = False # Ensure streaming is off for structured output\n        try:\n            last_exception = None\n            for attempt in range(max_retries + 1):\n                try:\n                    logger.debug(f\"Attempt {attempt + 1}/{max_retries + 1} to get structured JSON.\")\n                    # Use a potentially faster/cheaper model optimized for JSON tasks if configured?\n                    self.format_model = self.format_model_\n                    response_text = await self.a_run_llm_completion(messages, response_format=response_format_config)\n                    self.format_model = None\n                    # Clean and parse the JSON response\n                    try:\n                         # Basic cleaning: remove potential markdown fences\n                        cleaned_response = re.sub(r'^```json\\s*|\\s*```$', '', response_text.strip(), flags=re.MULTILINE)\n\n                         # Try parsing using Pydantic's TypeAdapter for direct validation\n                        adapter = TypeAdapter(pydantic_model)\n                        validated_obj = adapter.validate_json(cleaned_response)\n                        result_dict = validated_obj.model_dump(mode='json') # Get dict representation\n\n                        logger.debug(f\"Successfully formatted and validated JSON: {result_dict}\")\n                        return result_dict\n\n                    except (json.JSONDecodeError, ValidationError) as e:\n                        logger.warning(f\"Attempt {attempt + 1} failed: Invalid JSON or schema mismatch. Error: {e}. Response: {response_text[:500]}\")\n                        last_exception = ValueError(f\"LLM response did not match schema after cleaning. Error: {e}. Response: '{response_text[:200]}...'\")\n                        # Add feedback to the model for retry\n                        messages.append({\"role\": \"assistant\", \"content\": response_text}) # Show previous attempt\n                        messages.append({\"role\": \"system\", \"content\": f\"Your previous response was invalid ({e}). Please try again, ensuring you output *only* valid JSON matching the schema.\"})\n\n                except Exception as e:\n                    logger.error(f\"Error during a_format_class (attempt {attempt + 1}): {e}\", exc_info=True)\n                    last_exception = e\n                    # Don't retry on non-parsing errors immediately, could be API issue\n                    break\n\n                # Wait before retrying\n                if attempt &lt; max_retries:\n                     await asyncio.sleep(1.5 ** attempt) # Exponential backoff\n\n            # If all retries fail\n            logger.error(f\"Failed to get valid structured JSON after {max_retries + 1} attempts.\")\n            raise last_exception or ValueError(\"Failed to get structured JSON response from LLM.\")\n\n        finally:\n            self.stream = original_stream_state # Restore stream setting\n\n\n    async def flow_world_model(self, text_input: str, session_id: str, adk_session_state: State | None):\n        \"\"\"\n        Analyzes input, updates internal WorldModel, and syncs with ADK state if enabled.\n        Sync Priority: If ADK state exists, sync *from* it first. Then update based on text.\n                     The sync *to* ADK happens after the agent run completes.\n        \"\"\"\n        logger.debug(f\"Flowing world model based on text: {text_input[:100]}...\")\n\n        # 1. Sync FROM ADK State (if enabled and state available)\n        if self.sync_adk_state and adk_session_state is not None:\n             logger.debug(\"Syncing World Model FROM ADK session state...\")\n             self.world_model.sync_from_adk_state(adk_session_state)\n\n        # 2. Update World Model based on Text Input (using LLM)\n        # This adds/modifies based on the current turn's input\n        # Define Pydantic model for structured update extraction\n        current_keys = list(self.world_model.to_dict().keys())\n        class WorldModelAdaption(BaseModel):\n            action: Literal['add', 'update', 'remove', 'none'] = Field(..., description=\"Action on the world model.\")\n            key: str | None = Field(None, description=f\"Key to modify/add/remove (e.g., 'user_location', 'task_status'). Existing keys: {current_keys}\")\n            value: Any | None = Field(None, description=\"New value (for 'add'/'update'). Should be JSON serializable.\")\n            reasoning: str = Field(..., description=\"Why this change (or no change) is needed based on the input.\")\n\n        prompt = (f\"Analyze the following text and current world state to determine if the agent's world model needs changes.\\n\"\n                  f\"Current World State Keys: {current_keys}\\n\"\n                  f\"Text Input: ```\\n{text_input}\\n```\\n\"\n                  f\"Decide action, key, value, and reasoning. Focus on factual updates derived *from the text*. Do not hallucinate.\")\n\n        try:\n            # Use a potentially faster/cheaper model for this classification task\n            # Could eventually use a separate AMD config for this call\n            adaption_dict = await self.a_format_class(WorldModelAdaption, prompt)\n            adaption = WorldModelAdaption(**adaption_dict)\n\n            logger.info(f\"World Model Adaption proposed: {adaption.action} on key '{adaption.key}'. Reason: {adaption.reasoning}\")\n\n            if adaption.action == 'add' or adaption.action == 'update':\n                if adaption.key and adaption.value is not None:\n                    self.world_model.set(adaption.key, adaption.value)\n                else:\n                    logger.warning(\"World model 'add'/'update' ignored: missing key or value.\")\n            elif adaption.action == 'remove':\n                if adaption.key:\n                    self.world_model.remove(adaption.key)\n                else:\n                    logger.warning(\"World model 'remove' ignored: missing key.\")\n            # Else ('none'): do nothing\n\n        except (ValidationError, Exception) as e:\n            logger.warning(f\"Failed to determine world model adaption via LLM: {e}. World model may be based only on ADK sync or previous state.\")\n\n        # NOTE: Sync TO ADK happens *after* the full agent run in a_run()\n\n\n    # --- ADK Tool Implementations (Internal Wrappers) ---\n    def _ensure_internal_adk_tools(self):\n        \"\"\"Adds essential internal ADK tools if not already present.\"\"\"\n        if not ADK_AVAILABLE or not isinstance(self, LlmAgent):\n            return\n        if self.tools is None: self.tools = []\n\n        existing_tool_names = {tool.name for tool in self.tools if isinstance(tool, BaseTool)}\n\n        internal_adk_tools = {\n            \"get_world_model_key\": self.adk_tool_world_model_get,\n            \"show_world_model\": self.adk_tool_world_model_show,\n        }\n        if A2A_AVAILABLE:\n            internal_adk_tools[\"a2a_send_and_wait\"] = self.adk_tool_a2a_send_and_wait\n            # Add NEW tools\n            internal_adk_tools[\"a2a_send_no_wait\"] = self.adk_tool_a2a_send_no_wait\n            internal_adk_tools[\"a2a_get_task_status\"] = self.adk_tool_a2a_get_task_status\n            internal_adk_tools[\"a2a_cancel_task\"] = self.adk_tool_a2a_cancel_task\n\n        for name, func in internal_adk_tools.items():\n            if name not in existing_tool_names:\n                try:\n                    tool_instance = FunctionTool(func=func) # ADK infers from func signature/docstring\n                    self.tools.append(tool_instance)\n                    logger.debug(f\"Registered internal ADK tool: {name}\")\n                except Exception as e:\n                    logger.warning(f\"Failed to register internal ADK tool '{name}': {e}.\")\n\n    # --- Existing ADK Tools ---\n    async def adk_tool_world_model_get(self, tool_context: ToolContext | None, key: str) -&gt; Any | None:\n        \"\"\"ADK Tool: Retrieves a specific value from the agent's world model.\"\"\"\n        # ... (implementation remains the same) ...\n        logger.info(f\"[ADK Tool] get_world_model_key called for key: {key}\")\n        return self.world_model.get(key)\n\n    async def adk_tool_world_model_show(self, tool_context: ToolContext | None) -&gt; str:\n        \"\"\"ADK Tool: Returns a string representation of the agent's entire world model.\"\"\"\n        # ... (implementation remains the same) ...\n        logger.info(\"[ADK Tool] show_world_model called\")\n        return self.world_model.show()\n\n    async def adk_tool_a2a_send_and_wait(self,\n                                         tool_context: ToolContext | None,\n                                         target_agent_url: str,\n                                         task_prompt: str,\n                                         session_id: Optional[str] = None\n                                         ) -&gt; str:\n        \"\"\"ADK Tool: Sends a task to another agent via A2A and waits for the final text result.\"\"\"\n        # ... (implementation remains the same, calls _execute_a2a_call) ...\n        if not A2A_AVAILABLE: return \"Error: python-a2a library not available.\"\n        logger.info(f\"[ADK Tool] a2a_send_and_wait called for target: {target_agent_url}\")\n        tool_session_id = session_id or f\"adk_tool_a2a_{uuid.uuid4()}\"\n        try:\n            return await self._execute_a2a_call(\n                 user_input=task_prompt,\n                 session_id=tool_session_id,\n                 target_a2a_agent_url=target_agent_url,\n                 a2a_task_prompt=task_prompt\n            )\n        except Exception as e:\n             logger.error(f\"[ADK Tool] a2a_send_and_wait failed: {e}\", exc_info=True)\n             return f\"Error executing A2A task via ADK tool: {e}\"\n\n        # --- NEW ADK Tools for A2A ---\n\n    async def adk_tool_a2a_send_no_wait(self,\n                                        tool_context: ToolContext | None,\n                                        target_agent_url: str,\n                                        task_prompt: str,\n                                        session_id: Optional[str] = None\n                                        ) -&gt; str:\n        \"\"\"ADK Tool: Sends a task to another agent via A2A and returns the task ID immediately.\n\n        Args:\n            target_agent_url: The full URL of the target A2A agent.\n            task_prompt: The natural language prompt or task for the target agent.\n            session_id: Optional session ID to use for the A2A interaction.\n\n        Returns:\n            The unique ID of the submitted A2A task, or an error message.\n        \"\"\"\n        if not A2A_AVAILABLE: return \"Error: python-a2a library not available.\"\n        logger.info(f\"[ADK Tool] a2a_send_no_wait called for target: {target_agent_url}\")\n\n        try:\n            client = await self.setup_a2a_client(target_agent_url)\n            if not client:\n                return f\"Error: Could not connect to A2A agent at {target_agent_url}\"\n\n            task_id = str(uuid.uuid4())\n            a2a_session_id = session_id or f\"a2a_tool_nowait_{task_id[:8]}\"\n\n            message_payload = {\"role\": \"user\", \"content\": {\"type\": \"text\", \"text\": task_prompt}}\n\n            initial_task_info = None\n            if hasattr(client, 'send_task'):\n                initial_task_info = await client.send_task(message=message_payload, task_id=task_id,\n                                                           session_id=a2a_session_id)\n            elif hasattr(client, 'create_task'):\n                initial_task_info = await client.create_task(message=message_payload, task_id=task_id,\n                                                             session_id=a2a_session_id)\n            else:\n                return \"Error: A2A client does not support send_task or create_task.\"\n\n            # Check for immediate errors from the submission call\n            # Structure depends on python-a2a's return value\n            error_info = None\n            if isinstance(initial_task_info, dict):\n                error_info = initial_task_info.get('error')\n            elif hasattr(initial_task_info, 'error'):\n                error_info = initial_task_info.error\n\n            if error_info:\n                err_msg = error_info.get('message', str(error_info)) if isinstance(error_info, dict) else str(\n                    error_info)\n                logger.error(f\"A2A send_task (no wait) failed immediately: {err_msg}\")\n                return f\"Error submitting A2A task: {err_msg}\"\n            else:\n                logger.info(f\"A2A task '{task_id}' submitted successfully (no wait) to {target_agent_url}.\")\n                return task_id  # Return the ID for later polling/checking\n\n        except Exception as e:\n            logger.error(f\"[ADK Tool] a2a_send_no_wait failed: {e}\", exc_info=True)\n            return f\"Error sending A2A task (no wait): {e}\"\n\n    async def adk_tool_a2a_get_task_status(self,\n                                           tool_context: ToolContext | None,\n                                           target_agent_url: str,\n                                           task_id: str\n                                           ) -&gt; dict[str, Any]:\n        \"\"\"ADK Tool: Gets the current status and details of an A2A task.\n\n        Args:\n            target_agent_url: The URL of the agent hosting the task.\n            task_id: The ID of the task to check.\n\n        Returns:\n            A dictionary containing task status details (state, message, artifacts) or an error.\n        \"\"\"\n        if not A2A_AVAILABLE: return {\"error\": \"python-a2a library not available.\"}\n        logger.info(f\"[ADK Tool] a2a_get_task_status called for task {task_id} on {target_agent_url}\")\n\n        try:\n            client = await self.setup_a2a_client(target_agent_url)\n            if not client:\n                return {\"error\": f\"Could not connect to A2A agent at {target_agent_url}\"}\n\n            if not hasattr(client, 'get_task'):\n                return {\"error\": f\"A2A client for {target_agent_url} does not support 'get_task'.\"}\n\n            # Get task details from the client\n            task_details = await client.get_task(task_id=task_id, history_length=1)  # History=1 gets latest status\n\n            # Parse and return relevant info\n            if isinstance(task_details, dict):\n                # Basic parsing, adjust based on actual python-a2a structure\n                status_info = task_details.get('status', {})\n                artifacts = task_details.get('artifacts')\n                return {\n                    \"task_id\": task_id,\n                    \"state\": status_info.get('state', 'UNKNOWN'),\n                    \"status_message\": status_info.get('message'),  # Might be complex object\n                    \"artifacts\": artifacts,  # Might be complex list\n                    \"raw_response\": task_details  # Include raw for debugging\n                }\n            elif hasattr(task_details, 'status'):  # Object-like response\n                status_obj = task_details.status\n                artifacts_obj = getattr(task_details, 'artifacts', None)\n                return {\n                    \"task_id\": task_id,\n                    \"state\": getattr(status_obj, 'state', TaskState.UNKNOWN).value,  # Get enum value\n                    \"status_message\": getattr(status_obj, 'message', None),\n                    \"artifacts\": artifacts_obj,\n                    \"raw_response\": vars(task_details)  # Example conversion\n                }\n            else:\n                return {\"error\": \"Received unexpected response structure from get_task.\", \"raw_response\": task_details}\n\n        except Exception as e:\n            # Catch specific errors from python-a2a if they exist (e.g., TaskNotFoundError)\n            # if isinstance(e, TaskNotFoundError):\n            #    logger.warning(f\"[ADK Tool] A2A Task {task_id} not found on {target_agent_url}.\")\n            #    return {\"error\": f\"Task {task_id} not found.\"}\n            logger.error(f\"[ADK Tool] a2a_get_task_status failed: {e}\", exc_info=True)\n            return {\"error\": f\"Error getting A2A task status: {e}\"}\n\n    async def adk_tool_a2a_cancel_task(self,\n                                       tool_context: ToolContext | None,\n                                       target_agent_url: str,\n                                       task_id: str\n                                       ) -&gt; dict[str, Any]:\n        \"\"\"ADK Tool: Attempts to cancel an ongoing A2A task.\n\n        Args:\n            target_agent_url: The URL of the agent hosting the task.\n            task_id: The ID of the task to cancel.\n\n        Returns:\n            A dictionary indicating success or failure, possibly with the task's state after cancellation attempt.\n        \"\"\"\n        if not A2A_AVAILABLE: return {\"error\": \"python-a2a library not available.\"}\n        logger.info(f\"[ADK Tool] a2a_cancel_task called for task {task_id} on {target_agent_url}\")\n\n        try:\n            client = await self.setup_a2a_client(target_agent_url)\n            if not client:\n                return {\"error\": f\"Could not connect to A2A agent at {target_agent_url}\"}\n\n            if not hasattr(client, 'cancel_task'):\n                return {\"error\": f\"A2A client for {target_agent_url} does not support 'cancel_task'.\"}\n\n            # Call the client's cancel method\n            # The response structure depends heavily on the library implementation\n            cancel_response = await client.cancel_task(task_id=task_id)\n\n            # Parse response - could be simple success/fail, or updated task state\n            if isinstance(cancel_response, dict):\n                if 'error' in cancel_response:\n                    error_info = cancel_response['error']\n                    err_msg = error_info.get('message', str(error_info)) if isinstance(error_info, dict) else str(\n                        error_info)\n                    logger.warning(f\"A2A cancel_task failed for {task_id}: {err_msg}\")\n                    return {\"success\": False, \"error\": err_msg, \"raw_response\": cancel_response}\n                else:\n                    # Assume success, response might contain updated task state\n                    logger.info(f\"A2A task {task_id} cancellation requested successfully.\")\n                    # Try to extract state if returned\n                    state = cancel_response.get('result', {}).get('status', {}).get('state', 'UNKNOWN')\n                    return {\"success\": True, \"state_after_request\": state, \"raw_response\": cancel_response}\n            elif cancel_response is True:  # Simple boolean success\n                return {\"success\": True, \"state_after_request\": \"UNKNOWN\"}\n            else:  # Assume object-like or other structure\n                # Add parsing based on observed python-a2a behavior\n                logger.info(f\"A2A task {task_id} cancellation request sent, parsing result.\")\n                # Example: Check for specific attributes if object is returned\n                state = getattr(getattr(getattr(cancel_response, 'result', None), 'status', None), 'state',\n                                TaskState.UNKNOWN).value\n                return {\"success\": True, \"state_after_request\": state,\n                        \"raw_response\": vars(cancel_response) if hasattr(cancel_response, '__dict__') else str(\n                            cancel_response)}\n\n\n        except Exception as e:\n            # Catch specific errors like TaskNotFound, TaskNotCancelable if defined by python-a2a\n            # if isinstance(e, TaskNotFoundError):\n            #    return {\"success\": False, \"error\": f\"Task {task_id} not found.\"}\n            # if isinstance(e, TaskNotCancelableError):\n            #    return {\"success\": False, \"error\": f\"Task {task_id} is not in a cancelable state.\"}\n            logger.error(f\"[ADK Tool] a2a_cancel_task failed: {e}\", exc_info=True)\n            return {\"success\": False, \"error\": f\"Error cancelling A2A task: {e}\"}\n\n    # async def adk_tool_a2a_get_task(self, tool_context: Optional[ToolContext], target_agent_url: str, task_id: str) -&gt; Dict:\n    #     \"\"\"ADK Tool: Gets the current status and details of an A2A task.\"\"\"\n    #     # Implementation would be similar to _poll_a2a_task but return the status dict directly\n    #     pass\n\n\n    # --- Cost Tracking ---\n    def _track_cost(self, response_obj: Any):\n        \"\"\"Updates cost using LiteLLM.\"\"\"\n        if not response_obj: return\n        try:\n            cost = completion_cost(completion_response=response_obj, model=self.amd.model)\n            if cost is not None:\n                self.total_cost += cost\n                logger.info(f\"Turn Cost: ${cost:.6f}, Total Accumulated Cost: ${self.total_cost:.6f}\")\n            else:\n                 logger.debug(\"Cost calculation returned None (possibly streaming or non-standard response).\")\n        except Exception as e:\n            logger.warning(f\"Failed to calculate/track cost: {e}\")\n\n\n    # --- Cleanup ---\n    async def close(self):\n        \"\"\"Gracefully close connections and resources.\"\"\"\n        logger.info(f\"Closing resources for agent '{self.amd.name}'...\")\n        # Close A2A resources\n        if self.a2a_server and hasattr(self.a2a_server, 'stop'): # Check if server has stop method\n             logger.info(\"Stopping A2A server...\")\n             try:\n                 await self.a2a_server.stop() # Assuming stop is async\n             except Exception as e: logger.warning(f\"Error stopping A2A server: {e}\")\n        if hasattr(self, '_a2a_task_manager_instance') and hasattr(self._a2a_task_manager_instance, 'close'):\n             logger.info(\"Closing A2A task manager...\")\n             await self._a2a_task_manager_instance.close()\n        await self.close_a2a_clients()\n\n        # Close MCP server if running\n        if self.mcp_server and hasattr(self.mcp_server, 'stop'): # Check for stop method\n             logger.info(\"Stopping MCP server...\")\n             try:\n                 # MCP server run is blocking, stop might need separate mechanism\n                 # or be handled by process termination. If stop method exists:\n                 # await self.mcp_server.stop() # Assuming async stop\n                 logger.warning(\"MCP server 'stop' might need manual implementation or process signal.\")\n             except Exception as e: logger.warning(f\"Error stopping MCP server: {e}\")\n\n\n        # Close ADK resources (MCPToolset connections managed by exit stack)\n        if self.adk_exit_stack:\n            logger.info(\"Closing ADK AsyncExitStack (manages MCPToolset connections)...\")\n            try:\n                await self.adk_exit_stack.aclose()\n            except Exception as e:\n                logger.warning(f\"Error closing ADK exit stack: {e}\")\n\n        # Close ADK runner if it has a close method\n        if self.adk_runner and hasattr(self.adk_runner, 'close'):\n             logger.info(\"Closing ADK runner...\")\n             try:\n                  # Check if close is async\n                 if iscoroutinefunction(self.adk_runner.close):\n                     await self.adk_runner.close()\n                 else:\n                     self.adk_runner.close()\n             except Exception as e: logger.warning(f\"Error closing ADK runner: {e}\")\n\n\n        logger.info(f\"Agent '{self.amd.name}' resource cleanup finished.\")\n\n    def print_verbose(self, *args):\n        \"\"\"Conditional logging helper.\"\"\"\n        if self.verbose:\n            logger.debug(' '.join(map(str, args)))\n</code></pre> <code>a_format_class(pydantic_model, prompt, message_context=None, max_retries=2)</code> <code>async</code> \u00b6 <p>Uses LiteLLM's response_format feature to get structured JSON output, with retries.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def a_format_class(self,\n                         pydantic_model: type[BaseModel],\n                         prompt: str,\n                         message_context: list[dict] | None = None,\n                         max_retries: int = 2) -&gt; dict[str, Any]:\n    \"\"\"Uses LiteLLM's response_format feature to get structured JSON output, with retries.\"\"\"\n    logger.debug(f\"Formatting prompt for Pydantic model: {pydantic_model.__name__}\")\n    model_schema = pydantic_model.model_json_schema()\n\n    messages = message_context or []\n    # System prompt explaining the task and schema\n    messages.append({\n        \"role\": \"system\",\n        \"content\": f\"Your task is to analyze the user's request and extract information into a JSON object.\\n\"\n                   f\"Strictly adhere to the following Pydantic schema:\\n\"\n                   f\"```json\\n{json.dumps(model_schema, indent=2)}\\n```\\n\"\n                   f\"Guidelines:\\n\"\n                   f\"- Analyze the request carefully.\\n\"\n                   f\"- Output *only* the JSON object, nothing else (no explanations, apologies, or markdown).\\n\"\n                   f\"- Ensure the JSON is valid and conforms exactly to the schema.\\n\"\n                   f\"- Omit optional fields if the information is not present in the request.\"\n    })\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    # Use LiteLLM's JSON mode (requires compatible model/provider)\n    response_format_config = {\"type\": \"json_object\"}\n    # Some providers might need the schema explicitly even in json_object mode\n    # response_format_config = {\"type\": \"json_object\", \"schema\": model_schema}\n\n    original_stream_state = self.stream\n    self.stream = False # Ensure streaming is off for structured output\n    try:\n        last_exception = None\n        for attempt in range(max_retries + 1):\n            try:\n                logger.debug(f\"Attempt {attempt + 1}/{max_retries + 1} to get structured JSON.\")\n                # Use a potentially faster/cheaper model optimized for JSON tasks if configured?\n                self.format_model = self.format_model_\n                response_text = await self.a_run_llm_completion(messages, response_format=response_format_config)\n                self.format_model = None\n                # Clean and parse the JSON response\n                try:\n                     # Basic cleaning: remove potential markdown fences\n                    cleaned_response = re.sub(r'^```json\\s*|\\s*```$', '', response_text.strip(), flags=re.MULTILINE)\n\n                     # Try parsing using Pydantic's TypeAdapter for direct validation\n                    adapter = TypeAdapter(pydantic_model)\n                    validated_obj = adapter.validate_json(cleaned_response)\n                    result_dict = validated_obj.model_dump(mode='json') # Get dict representation\n\n                    logger.debug(f\"Successfully formatted and validated JSON: {result_dict}\")\n                    return result_dict\n\n                except (json.JSONDecodeError, ValidationError) as e:\n                    logger.warning(f\"Attempt {attempt + 1} failed: Invalid JSON or schema mismatch. Error: {e}. Response: {response_text[:500]}\")\n                    last_exception = ValueError(f\"LLM response did not match schema after cleaning. Error: {e}. Response: '{response_text[:200]}...'\")\n                    # Add feedback to the model for retry\n                    messages.append({\"role\": \"assistant\", \"content\": response_text}) # Show previous attempt\n                    messages.append({\"role\": \"system\", \"content\": f\"Your previous response was invalid ({e}). Please try again, ensuring you output *only* valid JSON matching the schema.\"})\n\n            except Exception as e:\n                logger.error(f\"Error during a_format_class (attempt {attempt + 1}): {e}\", exc_info=True)\n                last_exception = e\n                # Don't retry on non-parsing errors immediately, could be API issue\n                break\n\n            # Wait before retrying\n            if attempt &lt; max_retries:\n                 await asyncio.sleep(1.5 ** attempt) # Exponential backoff\n\n        # If all retries fail\n        logger.error(f\"Failed to get valid structured JSON after {max_retries + 1} attempts.\")\n        raise last_exception or ValueError(\"Failed to get structured JSON response from LLM.\")\n\n    finally:\n        self.stream = original_stream_state # Restore stream setting\n</code></pre> <code>a_run(user_input, session_id=None, persist_history=True, strategy_override=None, kwargs_override=None, a2a_task_id=None)</code> <code>async</code> \u00b6 <p>Main asynchronous execution logic for the agent turn.</p> <p>Orchestrates world model updates, state sync, strategy selection, execution, cost tracking, and callbacks.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def a_run(self,\n                user_input: str,\n                session_id: Optional[str] = None,\n                persist_history: bool = True,\n                strategy_override: ProcessingStrategy | None = None,\n                kwargs_override: dict[str, Any] | None = None, # For fine-grained control\n                a2a_task_id: Optional[str] = None # Context if called from A2A task\n                ) -&gt; str:\n    \"\"\"\n    Main asynchronous execution logic for the agent turn.\n\n    Orchestrates world model updates, state sync, strategy selection,\n    execution, cost tracking, and callbacks.\n    \"\"\"\n    self.internal_state = InternalAgentState.PROCESSING\n    start_time = time.monotonic()\n    session_id = session_id or \"default\" # Use 'default' if none provided\n    response = \"Error: Processing failed.\" # Default error\n    turn_cost = 0.0\n    span = None # OTel span\n\n    if not self.tracer: self._setup_telemetry() # Ensure tracer exists\n\n    try:\n        with self.tracer.start_as_current_span(f\"Agent Run: {self.amd.name}\", attributes={\"session_id\": session_id}) as span:\n\n            # Ensure session history list exists\n            if session_id not in self.message_history:\n                logger.debug(f\"Initializing history for session: {session_id}\")\n                self.message_history[session_id] = []\n\n            logger.info(f\"--- Agent Run Start (Session: {session_id}) ---\")\n            span.add_event(\"Agent run started\")\n            logger.info(f\"User Input: {user_input[:100]}...\")\n            span.set_attribute(\"user_input\", user_input[:500]) # Log truncated input\n\n            # 0. Get ADK Session State (if ADK enabled and syncing)\n            adk_session_state = None\n            if self.sync_adk_state and self.adk_session_service:\n                try:\n                    # ADK SessionService methods are typically synchronous\n                    # Run in threadpool to avoid blocking\n                    adk_session = await asyncio.to_thread(\n                         self.adk_session_service.get_session,\n                         app_name=self.adk_runner.app_name, # Assuming runner is set if syncing\n                         user_id=self.amd.user_id or \"adk_user\", # Needs consistent user ID\n                         session_id=session_id\n                    )\n                    if adk_session:\n                        adk_session_state = adk_session.state\n                    else:\n                        logger.warning(f\"ADK Session '{session_id}' not found for state sync.\")\n                        # Optionally create session here? Be careful about race conditions.\n                except Exception as sync_e:\n                    logger.error(f\"Error getting ADK session state for sync: {sync_e}\")\n\n            # 1. Update World Model &amp; Sync State (Run *before* strategy selection)\n            # flow_world_model is now responsible for syncing *from* ADK state initially\n            await self.flow_world_model(user_input, session_id, adk_session_state)\n            span.add_event(\"World model updated\")\n\n            # 2. Prepare message history for this turn\n            current_turn_messages = self._prepare_llm_messages(user_input, session_id)\n            span.set_attribute(\"history_length\", len(current_turn_messages) -1) # Exclude current input\n\n            # 3. Determine Processing Strategy\n            if strategy_override:\n                strategy = strategy_override\n                strategy_reasoning = \"Strategy overridden by caller.\"\n                logger.info(f\"Strategy forced by override: {strategy.value}\")\n            else:\n                strategy, strategy_reasoning = self._determine_strategy_heuristic(user_input, current_turn_messages)\n                logger.info(f\"Strategy Selected: {strategy.value} (Reason: {strategy_reasoning})\")\n            span.set_attribute(\"selected_strategy\", strategy.value)\n            span.set_attribute(\"strategy_reasoning\", strategy_reasoning)\n\n\n            # --- Prepare kwargs for execution based on strategy ---\n            exec_kwargs = kwargs_override or {}\n            exec_kwargs['session_id'] = session_id\n            exec_kwargs['user_input'] = user_input\n            exec_kwargs['current_turn_messages'] = current_turn_messages\n            exec_kwargs['adk_session_state'] = adk_session_state # Pass state for potential use/update\n\n\n            # 4. Execute Selected Strategy\n            logger.info(f\"Executing strategy: {strategy.value}\")\n            if strategy == ProcessingStrategy.ADK_RUN:\n                if ADK_AVAILABLE and self.adk_runner:\n                    response = await self._execute_adk_run(**exec_kwargs)\n                else:\n                    logger.warning(\"ADK_RUN strategy selected, but ADK runner not available/configured. Falling back.\")\n                    # Fallback strategy? Maybe DIRECT_LLM?\n                    strategy = ProcessingStrategy.DIRECT_LLM\n                    response = await self._execute_direct_llm(**exec_kwargs)\n\n            elif strategy == ProcessingStrategy.A2A_CALL:\n                if A2A_AVAILABLE:\n                    response = await self._execute_a2a_call(**exec_kwargs)\n                else:\n                    logger.warning(\"A2A_CALL strategy selected, but A2A not available. Falling back.\")\n                    strategy = ProcessingStrategy.DIRECT_LLM\n                    response = await self._execute_direct_llm(**exec_kwargs)\n\n            else: # Default: DIRECT_LLM\n                response = await self._execute_direct_llm(**exec_kwargs)\n\n            span.set_attribute(\"raw_response_length\", len(response))\n            span.add_event(\"Strategy execution complete\")\n\n            # 5. Persist History (if successful and enabled)\n            # Add assistant response to history\n            if persist_history and not response.startswith(\"Error:\"):\n                 self._add_to_history(session_id, LLMMessage(role=\"assistant\", content=response).to_dict())\n\n            # 6. Sync World Model *back* to ADK State (if changed and enabled)\n            if self.sync_adk_state and adk_session_state is not None:\n                try:\n                    self.world_model.sync_to_adk_state(adk_session_state)\n                    span.add_event(\"ADK state synchronized and updated\")\n                except Exception as sync_e:\n                     logger.error(f\"Error syncing/updating ADK session state: {sync_e}\")\n                     span.record_exception(sync_e)\n\n            # 7. Track Cost (using last_llm_result if available)\n            if self.last_llm_result:\n                try:\n                    cost = completion_cost(completion_response=self.last_llm_result, model=self.amd.model)\n                    if cost:\n                        turn_cost = cost\n                        self.total_cost += turn_cost\n                        logger.info(f\"Turn Cost: ${turn_cost:.6f}, Total Cost: ${self.total_cost:.6f}\")\n                        span.set_attribute(\"llm_cost\", turn_cost)\n                        span.set_attribute(\"total_agent_cost\", self.total_cost)\n                    self.last_llm_result = None # Clear after use\n                except Exception as cost_e:\n                    logger.warning(f\"Failed to calculate cost: {cost_e}\")\n                    span.add_event(\"Cost calculation failed\", attributes={\"error\": str(cost_e)})\n\n\n            # 8. Run Post Callback\n            if self.post_run_callback and not response.startswith(\"Error:\"):\n                try:\n                    if iscoroutinefunction(self.post_run_callback):\n                        await self.post_run_callback(session_id, response, turn_cost)\n                    else:\n                        self.post_run_callback(session_id, response, turn_cost)\n                    span.add_event(\"Post-run callback executed\")\n                except Exception as cb_e:\n                    logger.error(f\"Post-run callback failed: {cb_e}\", exc_info=True)\n                    span.record_exception(cb_e)\n\n\n            logger.info(f\"Agent Run finished in {time.monotonic() - start_time:.2f}s. Response: {response[:100]}...\")\n\n    except Exception as e:\n        logger.error(f\"Error during agent run (Session: {session_id}): {e}\", exc_info=True)\n        self.internal_state = InternalAgentState.ERROR\n        response = f\"Error: An internal error occurred during processing: {str(e)}\"\n        if span:\n             span.set_status(trace.Status(trace.StatusCode.ERROR, f\"Agent run failed: {e}\"))\n             span.record_exception(e)\n    finally:\n        self.internal_state = InternalAgentState.IDLE\n        if span: span.end() # Ensure span is closed\n        logger.info(f\"--- Agent Run End (Session: {session_id}) ---\")\n\n    return str(response) # Ensure string output\n</code></pre> <code>a_run_llm_completion(llm_messages=None, **kwargs)</code> <code>async</code> \u00b6 <p>Core wrapper around LiteLLM acompletion with error handling, streaming, and cost tracking.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def a_run_llm_completion(self, llm_messages: list[dict]=None, **kwargs) -&gt; str:\n    \"\"\"Core wrapper around LiteLLM acompletion with error handling, streaming, and cost tracking.\"\"\"\n    if not llm_messages:\n        if \"messages\" in kwargs:\n            llm_messages = kwargs.pop(\"messages\")\n        if \"llm_messages\" in kwargs:\n            llm_messages = kwargs.pop(\"llm_messages\")\n        if not llm_messages:\n            logger.warning(\"a_run_llm_completion called with empty message list.\")\n            return \"Error: No message provided to the model.\"\n\n    self.print_verbose(f\"Running model '{self.amd.model}' with {len(llm_messages)} messages.\")\n    # self.print_verbose(\"Messages:\", json.dumps(llm_messages, indent=2)) # Very verbose\n\n    # Prepare LiteLLM parameters from AgentModelData and kwargs overrides\n    params = {\n        'model': self.format_model or self.amd.model,\n        'messages': llm_messages,\n        'temperature': self.amd.temperature,\n        'top_p': self.amd.top_p,\n        'top_k': self.amd.top_k,\n        'max_tokens': self.amd.max_tokens,\n        'stream': self.stream,\n        'stop': self.amd.stop_sequence,\n        'user': self.amd.user_id,\n        'api_base': self.amd.api_base,\n        'api_version': self.amd.api_version,\n        'api_key': self.amd.api_key,\n        'presence_penalty': self.amd.presence_penalty,\n        'frequency_penalty': self.amd.frequency_penalty,\n        'caching': self.amd.caching,\n        'response_format': kwargs.get('response_format'), # For a_format_class\n        'tools': kwargs.get('tools'), # For LiteLLM function calling (less common now with ADK)\n    }\n    # Filter out None values as LiteLLM prefers absence over None for some params\n    params = {k: v for k, v in params.items() if v is not None}\n\n    # Add budget manager if present\n    if self.amd.budget_manager: params['budget_manager'] = self.amd.budget_manager\n\n    full_response_content = \"\"\n    tool_calls_requested = None # Store tool calls if generated\n\n    try:\n        response_object = await acompletion(**params)\n\n        if self.stream:\n            collected_chunks = []\n            async for chunk in response_object:\n                # Store raw chunk for potential analysis or replay\n                collected_chunks.append(chunk)\n                # Extract text delta\n                chunk_delta = chunk.choices[0].delta.content or \"\"\n                if chunk_delta:\n                    full_response_content += chunk_delta\n                    if self.stream_callback:\n                         try:\n                             # Provide only the new text chunk\n                             if iscoroutinefunction(self.stream_callback): await self.stream_callback(chunk_delta)\n                             else: self.stream_callback(chunk_delta)\n                         except Exception as cb_e:\n                             logger.warning(f\"Stream callback failed: {cb_e}\")\n                # Check for tool call deltas (less common in streaming)\n                tool_deltas = chunk.choices[0].delta.tool_calls\n                if tool_deltas:\n                     logger.warning(\"Received tool call delta during streaming - handling may be incomplete.\")\n                     # : Implement robust handling of streaming tool calls if needed\n\n            # After stream, construct a final response object mimicking non-streaming one for cost tracking\n            # This is an approximation, LiteLLM might offer better ways.\n            final_choice = {\"message\": {\"role\": \"assistant\", \"content\": full_response_content}}\n            # If tool calls were detected during streaming, add them (complex to reconstruct accurately)\n            # if reconstructed_tool_calls: final_choice[\"message\"][\"tool_calls\"] = reconstructed_tool_calls\n            self.last_llm_result = {\n                \"choices\": [{\"message\": final_choice[\"message\"]}],\n                \"model\": self.amd.model, # Needed for cost tracking\n                # Usage stats are often missing or zero in streaming chunks, need final value if available\n                \"usage\": getattr(collected_chunks[-1], 'usage', {\"prompt_tokens\": 0, \"completion_tokens\": 0})\n            }\n\n        else: # Non-streaming\n            self.last_llm_result = response_object # Store the full response\n            # Extract content and potential tool calls\n            message = response_object.choices[0].message\n            full_response_content = message.content or \"\"\n            tool_calls_requested = message.tool_calls # List of ToolCall objects\n\n            # Check if LiteLLM did function/tool calling (different from ADK tools)\n            # This path is less likely if using ADK, but supported by LiteLLM\n            if tool_calls_requested:\n                logger.info(f\"LiteLLM requested {len(tool_calls_requested)} tool calls.\")\n                # This requires a separate mechanism to execute these LiteLLM-requested tools\n                # and send back 'tool' role messages in the next turn.\n                # Not implemented here as focus is on ADK/A2A tools.\n                # For now, return a message indicating tool call request.\n                calls_repr = \", \".join([f\"{tc.function.name}\" for tc in tool_calls_requested])\n                return f\"Info: LLM requested tool calls ({calls_repr}). Direct execution not implemented.\"\n\n\n        self.print_verbose(f\"Model Response: {full_response_content[:100]}...\")\n        return full_response_content\n\n    except RateLimitError as e:\n        logger.error(f\"Rate limit error from {self.amd.model}: {e}\")\n        # Implement backoff/retry? For now, re-raise.\n        raise\n    except (BadRequestError, APIConnectionError, InternalServerError) as e:\n        logger.error(f\"API/Server error during LiteLLM call for {self.amd.model}: {e}\", exc_info=True)\n        raise\n    except Exception as e:\n        logger.error(f\"Unexpected error during LiteLLM completion: {e}\", exc_info=True)\n        raise\n</code></pre> <code>adk_tool_a2a_cancel_task(tool_context, target_agent_url, task_id)</code> <code>async</code> \u00b6 <p>ADK Tool: Attempts to cancel an ongoing A2A task.</p> <p>Parameters:</p> Name Type Description Default <code>target_agent_url</code> <code>str</code> <p>The URL of the agent hosting the task.</p> required <code>task_id</code> <code>str</code> <p>The ID of the task to cancel.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary indicating success or failure, possibly with the task's state after cancellation attempt.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def adk_tool_a2a_cancel_task(self,\n                                   tool_context: ToolContext | None,\n                                   target_agent_url: str,\n                                   task_id: str\n                                   ) -&gt; dict[str, Any]:\n    \"\"\"ADK Tool: Attempts to cancel an ongoing A2A task.\n\n    Args:\n        target_agent_url: The URL of the agent hosting the task.\n        task_id: The ID of the task to cancel.\n\n    Returns:\n        A dictionary indicating success or failure, possibly with the task's state after cancellation attempt.\n    \"\"\"\n    if not A2A_AVAILABLE: return {\"error\": \"python-a2a library not available.\"}\n    logger.info(f\"[ADK Tool] a2a_cancel_task called for task {task_id} on {target_agent_url}\")\n\n    try:\n        client = await self.setup_a2a_client(target_agent_url)\n        if not client:\n            return {\"error\": f\"Could not connect to A2A agent at {target_agent_url}\"}\n\n        if not hasattr(client, 'cancel_task'):\n            return {\"error\": f\"A2A client for {target_agent_url} does not support 'cancel_task'.\"}\n\n        # Call the client's cancel method\n        # The response structure depends heavily on the library implementation\n        cancel_response = await client.cancel_task(task_id=task_id)\n\n        # Parse response - could be simple success/fail, or updated task state\n        if isinstance(cancel_response, dict):\n            if 'error' in cancel_response:\n                error_info = cancel_response['error']\n                err_msg = error_info.get('message', str(error_info)) if isinstance(error_info, dict) else str(\n                    error_info)\n                logger.warning(f\"A2A cancel_task failed for {task_id}: {err_msg}\")\n                return {\"success\": False, \"error\": err_msg, \"raw_response\": cancel_response}\n            else:\n                # Assume success, response might contain updated task state\n                logger.info(f\"A2A task {task_id} cancellation requested successfully.\")\n                # Try to extract state if returned\n                state = cancel_response.get('result', {}).get('status', {}).get('state', 'UNKNOWN')\n                return {\"success\": True, \"state_after_request\": state, \"raw_response\": cancel_response}\n        elif cancel_response is True:  # Simple boolean success\n            return {\"success\": True, \"state_after_request\": \"UNKNOWN\"}\n        else:  # Assume object-like or other structure\n            # Add parsing based on observed python-a2a behavior\n            logger.info(f\"A2A task {task_id} cancellation request sent, parsing result.\")\n            # Example: Check for specific attributes if object is returned\n            state = getattr(getattr(getattr(cancel_response, 'result', None), 'status', None), 'state',\n                            TaskState.UNKNOWN).value\n            return {\"success\": True, \"state_after_request\": state,\n                    \"raw_response\": vars(cancel_response) if hasattr(cancel_response, '__dict__') else str(\n                        cancel_response)}\n\n\n    except Exception as e:\n        # Catch specific errors like TaskNotFound, TaskNotCancelable if defined by python-a2a\n        # if isinstance(e, TaskNotFoundError):\n        #    return {\"success\": False, \"error\": f\"Task {task_id} not found.\"}\n        # if isinstance(e, TaskNotCancelableError):\n        #    return {\"success\": False, \"error\": f\"Task {task_id} is not in a cancelable state.\"}\n        logger.error(f\"[ADK Tool] a2a_cancel_task failed: {e}\", exc_info=True)\n        return {\"success\": False, \"error\": f\"Error cancelling A2A task: {e}\"}\n</code></pre> <code>adk_tool_a2a_get_task_status(tool_context, target_agent_url, task_id)</code> <code>async</code> \u00b6 <p>ADK Tool: Gets the current status and details of an A2A task.</p> <p>Parameters:</p> Name Type Description Default <code>target_agent_url</code> <code>str</code> <p>The URL of the agent hosting the task.</p> required <code>task_id</code> <code>str</code> <p>The ID of the task to check.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing task status details (state, message, artifacts) or an error.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def adk_tool_a2a_get_task_status(self,\n                                       tool_context: ToolContext | None,\n                                       target_agent_url: str,\n                                       task_id: str\n                                       ) -&gt; dict[str, Any]:\n    \"\"\"ADK Tool: Gets the current status and details of an A2A task.\n\n    Args:\n        target_agent_url: The URL of the agent hosting the task.\n        task_id: The ID of the task to check.\n\n    Returns:\n        A dictionary containing task status details (state, message, artifacts) or an error.\n    \"\"\"\n    if not A2A_AVAILABLE: return {\"error\": \"python-a2a library not available.\"}\n    logger.info(f\"[ADK Tool] a2a_get_task_status called for task {task_id} on {target_agent_url}\")\n\n    try:\n        client = await self.setup_a2a_client(target_agent_url)\n        if not client:\n            return {\"error\": f\"Could not connect to A2A agent at {target_agent_url}\"}\n\n        if not hasattr(client, 'get_task'):\n            return {\"error\": f\"A2A client for {target_agent_url} does not support 'get_task'.\"}\n\n        # Get task details from the client\n        task_details = await client.get_task(task_id=task_id, history_length=1)  # History=1 gets latest status\n\n        # Parse and return relevant info\n        if isinstance(task_details, dict):\n            # Basic parsing, adjust based on actual python-a2a structure\n            status_info = task_details.get('status', {})\n            artifacts = task_details.get('artifacts')\n            return {\n                \"task_id\": task_id,\n                \"state\": status_info.get('state', 'UNKNOWN'),\n                \"status_message\": status_info.get('message'),  # Might be complex object\n                \"artifacts\": artifacts,  # Might be complex list\n                \"raw_response\": task_details  # Include raw for debugging\n            }\n        elif hasattr(task_details, 'status'):  # Object-like response\n            status_obj = task_details.status\n            artifacts_obj = getattr(task_details, 'artifacts', None)\n            return {\n                \"task_id\": task_id,\n                \"state\": getattr(status_obj, 'state', TaskState.UNKNOWN).value,  # Get enum value\n                \"status_message\": getattr(status_obj, 'message', None),\n                \"artifacts\": artifacts_obj,\n                \"raw_response\": vars(task_details)  # Example conversion\n            }\n        else:\n            return {\"error\": \"Received unexpected response structure from get_task.\", \"raw_response\": task_details}\n\n    except Exception as e:\n        # Catch specific errors from python-a2a if they exist (e.g., TaskNotFoundError)\n        # if isinstance(e, TaskNotFoundError):\n        #    logger.warning(f\"[ADK Tool] A2A Task {task_id} not found on {target_agent_url}.\")\n        #    return {\"error\": f\"Task {task_id} not found.\"}\n        logger.error(f\"[ADK Tool] a2a_get_task_status failed: {e}\", exc_info=True)\n        return {\"error\": f\"Error getting A2A task status: {e}\"}\n</code></pre> <code>adk_tool_a2a_send_and_wait(tool_context, target_agent_url, task_prompt, session_id=None)</code> <code>async</code> \u00b6 <p>ADK Tool: Sends a task to another agent via A2A and waits for the final text result.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def adk_tool_a2a_send_and_wait(self,\n                                     tool_context: ToolContext | None,\n                                     target_agent_url: str,\n                                     task_prompt: str,\n                                     session_id: Optional[str] = None\n                                     ) -&gt; str:\n    \"\"\"ADK Tool: Sends a task to another agent via A2A and waits for the final text result.\"\"\"\n    # ... (implementation remains the same, calls _execute_a2a_call) ...\n    if not A2A_AVAILABLE: return \"Error: python-a2a library not available.\"\n    logger.info(f\"[ADK Tool] a2a_send_and_wait called for target: {target_agent_url}\")\n    tool_session_id = session_id or f\"adk_tool_a2a_{uuid.uuid4()}\"\n    try:\n        return await self._execute_a2a_call(\n             user_input=task_prompt,\n             session_id=tool_session_id,\n             target_a2a_agent_url=target_agent_url,\n             a2a_task_prompt=task_prompt\n        )\n    except Exception as e:\n         logger.error(f\"[ADK Tool] a2a_send_and_wait failed: {e}\", exc_info=True)\n         return f\"Error executing A2A task via ADK tool: {e}\"\n</code></pre> <code>adk_tool_a2a_send_no_wait(tool_context, target_agent_url, task_prompt, session_id=None)</code> <code>async</code> \u00b6 <p>ADK Tool: Sends a task to another agent via A2A and returns the task ID immediately.</p> <p>Parameters:</p> Name Type Description Default <code>target_agent_url</code> <code>str</code> <p>The full URL of the target A2A agent.</p> required <code>task_prompt</code> <code>str</code> <p>The natural language prompt or task for the target agent.</p> required <code>session_id</code> <code>Optional[str]</code> <p>Optional session ID to use for the A2A interaction.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The unique ID of the submitted A2A task, or an error message.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def adk_tool_a2a_send_no_wait(self,\n                                    tool_context: ToolContext | None,\n                                    target_agent_url: str,\n                                    task_prompt: str,\n                                    session_id: Optional[str] = None\n                                    ) -&gt; str:\n    \"\"\"ADK Tool: Sends a task to another agent via A2A and returns the task ID immediately.\n\n    Args:\n        target_agent_url: The full URL of the target A2A agent.\n        task_prompt: The natural language prompt or task for the target agent.\n        session_id: Optional session ID to use for the A2A interaction.\n\n    Returns:\n        The unique ID of the submitted A2A task, or an error message.\n    \"\"\"\n    if not A2A_AVAILABLE: return \"Error: python-a2a library not available.\"\n    logger.info(f\"[ADK Tool] a2a_send_no_wait called for target: {target_agent_url}\")\n\n    try:\n        client = await self.setup_a2a_client(target_agent_url)\n        if not client:\n            return f\"Error: Could not connect to A2A agent at {target_agent_url}\"\n\n        task_id = str(uuid.uuid4())\n        a2a_session_id = session_id or f\"a2a_tool_nowait_{task_id[:8]}\"\n\n        message_payload = {\"role\": \"user\", \"content\": {\"type\": \"text\", \"text\": task_prompt}}\n\n        initial_task_info = None\n        if hasattr(client, 'send_task'):\n            initial_task_info = await client.send_task(message=message_payload, task_id=task_id,\n                                                       session_id=a2a_session_id)\n        elif hasattr(client, 'create_task'):\n            initial_task_info = await client.create_task(message=message_payload, task_id=task_id,\n                                                         session_id=a2a_session_id)\n        else:\n            return \"Error: A2A client does not support send_task or create_task.\"\n\n        # Check for immediate errors from the submission call\n        # Structure depends on python-a2a's return value\n        error_info = None\n        if isinstance(initial_task_info, dict):\n            error_info = initial_task_info.get('error')\n        elif hasattr(initial_task_info, 'error'):\n            error_info = initial_task_info.error\n\n        if error_info:\n            err_msg = error_info.get('message', str(error_info)) if isinstance(error_info, dict) else str(\n                error_info)\n            logger.error(f\"A2A send_task (no wait) failed immediately: {err_msg}\")\n            return f\"Error submitting A2A task: {err_msg}\"\n        else:\n            logger.info(f\"A2A task '{task_id}' submitted successfully (no wait) to {target_agent_url}.\")\n            return task_id  # Return the ID for later polling/checking\n\n    except Exception as e:\n        logger.error(f\"[ADK Tool] a2a_send_no_wait failed: {e}\", exc_info=True)\n        return f\"Error sending A2A task (no wait): {e}\"\n</code></pre> <code>adk_tool_world_model_get(tool_context, key)</code> <code>async</code> \u00b6 <p>ADK Tool: Retrieves a specific value from the agent's world model.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def adk_tool_world_model_get(self, tool_context: ToolContext | None, key: str) -&gt; Any | None:\n    \"\"\"ADK Tool: Retrieves a specific value from the agent's world model.\"\"\"\n    # ... (implementation remains the same) ...\n    logger.info(f\"[ADK Tool] get_world_model_key called for key: {key}\")\n    return self.world_model.get(key)\n</code></pre> <code>adk_tool_world_model_show(tool_context)</code> <code>async</code> \u00b6 <p>ADK Tool: Returns a string representation of the agent's entire world model.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def adk_tool_world_model_show(self, tool_context: ToolContext | None) -&gt; str:\n    \"\"\"ADK Tool: Returns a string representation of the agent's entire world model.\"\"\"\n    # ... (implementation remains the same) ...\n    logger.info(\"[ADK Tool] show_world_model called\")\n    return self.world_model.show()\n</code></pre> <code>close()</code> <code>async</code> \u00b6 <p>Gracefully close connections and resources.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def close(self):\n    \"\"\"Gracefully close connections and resources.\"\"\"\n    logger.info(f\"Closing resources for agent '{self.amd.name}'...\")\n    # Close A2A resources\n    if self.a2a_server and hasattr(self.a2a_server, 'stop'): # Check if server has stop method\n         logger.info(\"Stopping A2A server...\")\n         try:\n             await self.a2a_server.stop() # Assuming stop is async\n         except Exception as e: logger.warning(f\"Error stopping A2A server: {e}\")\n    if hasattr(self, '_a2a_task_manager_instance') and hasattr(self._a2a_task_manager_instance, 'close'):\n         logger.info(\"Closing A2A task manager...\")\n         await self._a2a_task_manager_instance.close()\n    await self.close_a2a_clients()\n\n    # Close MCP server if running\n    if self.mcp_server and hasattr(self.mcp_server, 'stop'): # Check for stop method\n         logger.info(\"Stopping MCP server...\")\n         try:\n             # MCP server run is blocking, stop might need separate mechanism\n             # or be handled by process termination. If stop method exists:\n             # await self.mcp_server.stop() # Assuming async stop\n             logger.warning(\"MCP server 'stop' might need manual implementation or process signal.\")\n         except Exception as e: logger.warning(f\"Error stopping MCP server: {e}\")\n\n\n    # Close ADK resources (MCPToolset connections managed by exit stack)\n    if self.adk_exit_stack:\n        logger.info(\"Closing ADK AsyncExitStack (manages MCPToolset connections)...\")\n        try:\n            await self.adk_exit_stack.aclose()\n        except Exception as e:\n            logger.warning(f\"Error closing ADK exit stack: {e}\")\n\n    # Close ADK runner if it has a close method\n    if self.adk_runner and hasattr(self.adk_runner, 'close'):\n         logger.info(\"Closing ADK runner...\")\n         try:\n              # Check if close is async\n             if iscoroutinefunction(self.adk_runner.close):\n                 await self.adk_runner.close()\n             else:\n                 self.adk_runner.close()\n         except Exception as e: logger.warning(f\"Error closing ADK runner: {e}\")\n\n\n    logger.info(f\"Agent '{self.amd.name}' resource cleanup finished.\")\n</code></pre> <code>close_a2a_clients()</code> <code>async</code> \u00b6 <p>Closes all cached A2A client connections.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def close_a2a_clients(self):\n    \"\"\"Closes all cached A2A client connections.\"\"\"\n    async with self.a2a_client_lock:\n        logger.info(f\"Closing {len(self.a2a_clients)} A2A clients.\")\n        # A2AClient may manage underlying httpx clients automatically.\n        # If explicit close needed in future versions, add here.\n        # for client in self.a2a_clients.values():\n        #     await client.close() # If available\n        self.a2a_clients.clear()\n</code></pre> <code>construct_initial_prompts()</code> \u00b6 <p>Constructs the initial system/context messages for the LLM prompt.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def construct_initial_prompts(self) -&gt; list[dict]:\n    \"\"\"Constructs the initial system/context messages for the LLM prompt.\"\"\"\n    messages = []\n    # Base System Prompt\n    if self.amd.system_message:\n        messages.append(LLMMessage(\"system\", self.amd.system_message).to_dict())\n\n    # World Model Context\n    wm_repr = self.world_model.show()\n    if wm_repr != \"[empty]\":\n        messages.append(LLMMessage(\"system\", f\"Current World State:\\n{wm_repr}\").to_dict())\n\n    # Capabilities Overview (ADK specific parts depend on LlmAgent inheritance)\n    caps = [\"LiteLLM (Core LLM access)\"]\n    if ADK_AVAILABLE and isinstance(self, LlmAgent):\n        if self.tools: caps.append(\"ADK Tools (including potential MCP/A2A wrappers)\")\n        if self.code_executor: caps.append(\"ADK Code Execution\")\n        if any(isinstance(t, type(adk_google_search) | AdkVertexAiSearchTool) for t in getattr(self, 'tools', [])):\n             caps.append(\"ADK Search\")\n    if A2A_AVAILABLE and self.a2a_clients: caps.append(\"A2A Client (delegate to other agents)\")\n    if self.mcp_server: caps.append(\"MCP Server (exposes capabilities)\")\n    if self.a2a_server: caps.append(\"A2A Server (receives tasks)\")\n\n    messages.append(LLMMessage(\"system\", f\"Your Capabilities: {', '.join(caps)}.\").to_dict())\n\n    # ADK Tool Instructions (if ADK enabled and tools exist)\n    if ADK_AVAILABLE and isinstance(self, LlmAgent) and self.tools:\n        try:\n            # Use ADK's internal method to get schema if possible, otherwise basic list\n            tool_schemas = getattr(self, 'tool_schemas', None) # ADK might populate this\n            if tool_schemas:\n                 tool_list_str = json.dumps(tool_schemas, indent=2)\n                 messages.append(LLMMessage(\"system\", f\"You have access to the following tools (use FunctionCall format):\\n{tool_list_str}\").to_dict())\n            else: # Fallback to basic list\n                tool_list = \"\\n\".join([f\"- {tool.name}: {tool.description or 'No description'}\" for tool in self.tools])\n                messages.append(LLMMessage(\"system\", f\"You can use the following tools:\\n{tool_list}\\nRespond with a FunctionCall to use a tool.\").to_dict())\n        except Exception as e:\n             logger.warning(f\"Could not generate detailed ADK tool instructions: {e}\")\n\n\n    # Add specific instructions for A2A delegation if needed\n    if A2A_AVAILABLE and self.a2a_clients:\n         client_names = list(self.a2a_clients.keys()) # Target URLs act as names here\n         messages.append(LLMMessage(\"system\", f\"You can delegate tasks to other agents via A2A using their URLs (e.g., {client_names[0]} if available). Indicate clearly if you want to delegate.\").to_dict())\n\n    return messages\n</code></pre> <code>flow_world_model(text_input, session_id, adk_session_state)</code> <code>async</code> \u00b6 <p>Analyzes input, updates internal WorldModel, and syncs with ADK state if enabled. Sync Priority: If ADK state exists, sync from it first. Then update based on text.              The sync to ADK happens after the agent run completes.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def flow_world_model(self, text_input: str, session_id: str, adk_session_state: State | None):\n    \"\"\"\n    Analyzes input, updates internal WorldModel, and syncs with ADK state if enabled.\n    Sync Priority: If ADK state exists, sync *from* it first. Then update based on text.\n                 The sync *to* ADK happens after the agent run completes.\n    \"\"\"\n    logger.debug(f\"Flowing world model based on text: {text_input[:100]}...\")\n\n    # 1. Sync FROM ADK State (if enabled and state available)\n    if self.sync_adk_state and adk_session_state is not None:\n         logger.debug(\"Syncing World Model FROM ADK session state...\")\n         self.world_model.sync_from_adk_state(adk_session_state)\n\n    # 2. Update World Model based on Text Input (using LLM)\n    # This adds/modifies based on the current turn's input\n    # Define Pydantic model for structured update extraction\n    current_keys = list(self.world_model.to_dict().keys())\n    class WorldModelAdaption(BaseModel):\n        action: Literal['add', 'update', 'remove', 'none'] = Field(..., description=\"Action on the world model.\")\n        key: str | None = Field(None, description=f\"Key to modify/add/remove (e.g., 'user_location', 'task_status'). Existing keys: {current_keys}\")\n        value: Any | None = Field(None, description=\"New value (for 'add'/'update'). Should be JSON serializable.\")\n        reasoning: str = Field(..., description=\"Why this change (or no change) is needed based on the input.\")\n\n    prompt = (f\"Analyze the following text and current world state to determine if the agent's world model needs changes.\\n\"\n              f\"Current World State Keys: {current_keys}\\n\"\n              f\"Text Input: ```\\n{text_input}\\n```\\n\"\n              f\"Decide action, key, value, and reasoning. Focus on factual updates derived *from the text*. Do not hallucinate.\")\n\n    try:\n        # Use a potentially faster/cheaper model for this classification task\n        # Could eventually use a separate AMD config for this call\n        adaption_dict = await self.a_format_class(WorldModelAdaption, prompt)\n        adaption = WorldModelAdaption(**adaption_dict)\n\n        logger.info(f\"World Model Adaption proposed: {adaption.action} on key '{adaption.key}'. Reason: {adaption.reasoning}\")\n\n        if adaption.action == 'add' or adaption.action == 'update':\n            if adaption.key and adaption.value is not None:\n                self.world_model.set(adaption.key, adaption.value)\n            else:\n                logger.warning(\"World model 'add'/'update' ignored: missing key or value.\")\n        elif adaption.action == 'remove':\n            if adaption.key:\n                self.world_model.remove(adaption.key)\n            else:\n                logger.warning(\"World model 'remove' ignored: missing key.\")\n        # Else ('none'): do nothing\n\n    except (ValidationError, Exception) as e:\n        logger.warning(f\"Failed to determine world model adaption via LLM: {e}. World model may be based only on ADK sync or previous state.\")\n</code></pre> <code>print_verbose(*args)</code> \u00b6 <p>Conditional logging helper.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def print_verbose(self, *args):\n    \"\"\"Conditional logging helper.\"\"\"\n    if self.verbose:\n        logger.debug(' '.join(map(str, args)))\n</code></pre> <code>run(user_input, session_id=None, **kwargs)</code> \u00b6 <p>Synchronous wrapper for a_run.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def run(self, user_input: str, session_id: Optional[str] = None, **kwargs) -&gt; str:\n    \"\"\"Synchronous wrapper for a_run.\"\"\"\n    try:\n        # get_event_loop() is deprecated in 3.10+, use get_running_loop() or new_event_loop()\n        try:\n            asyncio.get_running_loop()\n            # If loop is running, cannot use asyncio.run. Need to schedule and wait.\n            # This is complex to get right universally (e.g., in notebooks vs servers).\n            # Simplest approach for sync call from sync context is asyncio.run()\n            # If called from async context, user should await a_run() directly.\n            logger.warning(\"Synchronous 'run' called from a running event loop. \"\n                           \"This might block the loop. Consider using 'await a_run'.\")\n            # Fallback to basic run, may error if loop is running\n            return asyncio.run(self.a_run(user_input, session_id=session_id, **kwargs))\n        except RuntimeError: # No running event loop\n             return asyncio.run(self.a_run(user_input, session_id=session_id, **kwargs))\n    except Exception as e:\n        logger.error(f\"Error in synchronous run wrapper: {e}\", exc_info=True)\n        return f\"Error: Failed to execute synchronous run: {e}\"\n</code></pre> <code>run_a2a_server(host='0.0.0.0', port=5000, **kwargs)</code> \u00b6 <p>Starts the A2A server (blocking) using the python-a2a run_server function.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def run_a2a_server(self, host=\"0.0.0.0\", port=5000, **kwargs):\n    \"\"\"Starts the A2A server (blocking) using the python-a2a run_server function.\"\"\"\n    if not self.a2a_server:\n        logger.error(\"A2A server not initialized. Call setup_a2a_server first.\")\n        return\n    if not A2A_AVAILABLE:\n        logger.error(\"python-a2a library not available. Cannot run A2A server.\")\n        return\n\n    # Get effective host/port from server instance if set, otherwise use args\n    effective_host = getattr(self.a2a_server, 'host', host)\n    effective_port = getattr(self.a2a_server, 'port', port)\n\n    logger.info(f\"Starting A2A server for agent '{self.amd.name}' via run_server_func on {effective_host}:{effective_port}...\")\n    try:\n        # Call the imported run_server function, passing the agent instance\n        run_a2a_server_func(self.a2a_server, host=effective_host, port=effective_port, **kwargs) # This blocks\n    except Exception as e:\n        logger.error(f\"A2A server failed to run: {e}\", exc_info=True)\n</code></pre> <code>run_mcp_server(transport='sse', **kwargs)</code> \u00b6 <p>Starts the MCP server (blocking).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def run_mcp_server(self, transport='sse', **kwargs):\n    \"\"\"Starts the MCP server (blocking).\"\"\"\n    if not self.mcp_server:\n        logger.error(\"MCP server not initialized. Call setup_mcp_server first.\")\n        return\n    if not MCP_AVAILABLE:\n         logger.error(\"MCP library not available. Cannot run MCP server.\")\n         return\n    logger.info(f\"Starting MCP server for agent '{self.amd.name}' using {transport} transport...\")\n    # This is blocking, run in a separate process/thread for a long-running agent\n    try:\n        self.mcp_server.run(transport=transport, **kwargs)\n    except Exception as e:\n        logger.error(f\"MCP server failed to run: {e}\", exc_info=True)\n</code></pre> <code>setup_a2a_client(target_agent_url)</code> <code>async</code> \u00b6 <p>Gets or creates an A2A client for a specific target agent URL using python-a2a.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def setup_a2a_client(self, target_agent_url: str) -&gt; A2AClient | None:\n    \"\"\"Gets or creates an A2A client for a specific target agent URL using python-a2a.\"\"\"\n    if not A2A_AVAILABLE:\n        logger.warning(\"python-a2a library not installed. Cannot setup A2A client.\")\n        return None\n\n    async with self.a2a_client_lock:\n        if target_agent_url in self.a2a_clients:\n            logger.debug(f\"Reusing cached A2A client for {target_agent_url}\")\n            return self.a2a_clients[target_agent_url]\n\n        logger.info(f\"Setting up A2A client for target: {target_agent_url}\")\n        try:\n            # python-a2a client likely fetches card on init or first call\n            client = A2AClient(base_url=target_agent_url) # Pass the URL directly\n            # Verify connection implicitly by getting card (optional, client might do lazy loading)\n            # agent_card = await client.get_agent_card() # If method exists\n            # logger.info(f\"Successfully connected A2A client to agent: {agent_card.name}\")\n            self.a2a_clients[target_agent_url] = client\n            logger.info(f\"A2A client created for target: {target_agent_url}\")\n            return client\n        except Exception as e:\n            logger.error(f\"Failed to setup A2A client for {target_agent_url}: {e}\", exc_info=True)\n            return None\n</code></pre> <code>setup_a2a_server(host='0.0.0.0', port=5000, **a2a_server_options)</code> \u00b6 <p>Initialize and configure the A2A server capabilities using python-a2a. This dynamically creates a server class with the agent's capabilities.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def setup_a2a_server(self, host=\"0.0.0.0\", port=5000, **a2a_server_options):\n    \"\"\"\n    Initialize and configure the A2A server capabilities using python-a2a.\n    This dynamically creates a server class with the agent's capabilities.\n    \"\"\"\n    if not A2A_AVAILABLE:\n        logger.warning(\"python-a2a library not installed. Cannot setup A2A server.\")\n        return None\n    if self.a2a_server:\n        logger.warning(\"A2A server already initialized.\")\n        return self.a2a_server\n\n    logger.info(f\"Setting up A2A server for agent '{self.amd.name}' on {host}:{port}\")\n\n    agent_instance = self # Reference to the current EnhancedAgent instance\n\n    # Define the A2A Server class dynamically using the decorator\n    @a2a_agent_decorator(\n        name=self.amd.name or \"EnhancedAgent\",\n        description=f\"Enhanced Agent '{self.amd.name}' - Capabilities: ADK({ADK_AVAILABLE}), MCP({MCP_AVAILABLE}), A2A({A2A_AVAILABLE})\",\n        version=\"1.0.0\",\n        # Other AgentCard fields...\n    )\n    class DynamicA2AServer(A2AServer):\n        bound_agent: EnhancedAgent = agent_instance\n\n        def handle_task(self, task: Task) -&gt; Task:\n            \"\"\" Handles incoming A2A tasks by calling the EnhancedAgent's async logic. \"\"\"\n            # --- (handle_task implementation remains the same as before) ---\n            logger.info(f\"[A2A Server {self.bound_agent.amd.name}] Received task: {task.id}\")\n            async def run_agent_async():\n                # ... (logic to extract prompt, call a_run, update task) ...\n                try:\n                    user_prompt = \"\"\n                    # ... (extract user_prompt from task.message) ...\n                    if task.message and task.message.get(\"content\"):\n                        content = task.message[\"content\"]\n                        if isinstance(content, dict) and content.get(\"type\") == \"text\":\n                            user_prompt = content.get(\"text\", \"\").strip()\n                        elif isinstance(content, str):\n                            user_prompt = content.strip()\n\n                    if not user_prompt:\n                        raise ValueError(\"Task message has no text content.\")\n\n                    session_id = task.message.get(\"session_id\", task.id)\n                    agent_response = await self.bound_agent.a_run(\n                        user_prompt,\n                        session_id=session_id,\n                        persist_history=False,\n                        a2a_task_id=task.id\n                    )\n                    task.artifacts = [{\"parts\": [{\"type\": \"text\", \"text\": str(agent_response)}]}]\n                    task.status = TaskStatus(state=TaskState.COMPLETED)\n                except Exception as e:\n                    # ... (error handling) ...\n                    logger.error(f\"[A2A Task {task.id}] Error during processing: {e}\", exc_info=True)\n                    error_msg = f\"Internal agent error: {str(e)}\"\n                    task.artifacts = [{\"parts\": [{\"type\": \"text\", \"text\": error_msg}]}]\n                    task.status = TaskStatus(state=TaskState.FAILED, message={\"role\": \"agent\", \"content\": {\"type\": \"text\", \"text\": error_msg}})\n                return task\n            try:\n                updated_task = asyncio.run(run_agent_async())\n                return updated_task\n            except RuntimeError as e:\n                # ... (handle RuntimeError) ...\n                logger.error(f\"RuntimeError calling asyncio.run in handle_task: {e}.\")\n                task.status = TaskStatus(state=TaskState.FAILED, message={\"role\": \"agent\", \"content\": {\"type\": \"text\", \"text\": \"Internal Server Error processing task asynchronously.\"}})\n                return task\n            # --- (end of handle_task logic) ---\n\n\n        # --- Expose Skills ---\n        @a2a_skill_decorator(\n            name=\"General Query\",\n            description=\"Process general natural language queries using the agent's primary LLM.\",\n            examples=[\"What is the capital of France?\", \"Summarize the plot of Hamlet.\"]\n        )\n        def general_query_skill(self, query: str) -&gt; str:\n            \"\"\"Handles general queries via the skill mechanism by calling a_run.\"\"\"\n            logger.info(f\"[A2A Skill] Received general_query: {query[:50]}...\")\n            async def run_skill_async():\n                # Call a_run, forcing direct LLM strategy for simple queries\n                response = await self.bound_agent.a_run(\n                    query,\n                    a2a_task_id=f\"skill_query_{uuid.uuid4()}\",\n                    strategy_override=ProcessingStrategy.DIRECT_LLM,\n                    persist_history=False\n                    )\n                return response\n            try:\n                # Bridge sync skill call to async agent logic\n                return asyncio.run(run_skill_async())\n            except RuntimeError:\n                 logger.error(\"RuntimeError calling asyncio.run in general_query_skill.\")\n                 return \"Error: Could not process skill asynchronously.\"\n\n        # --- FIXED: Generic Skill for ADK Tools ---\n        if ADK_AVAILABLE and isinstance(agent_instance, LlmAgent) and agent_instance.tools:\n            # Check if there are any ADK tools to expose\n            adk_tool_list = [t for t in agent_instance.tools if isinstance(t, BaseTool)]\n            if adk_tool_list:\n                logger.info(f\"Exposing {len(adk_tool_list)} ADK tools via 'execute_adk_tool' A2A skill.\")\n\n                @a2a_skill_decorator(\n                    name=\"execute_adk_tool\",\n                    description=f\"Executes a registered ADK tool. Available tools: {', '.join([t.name for t in adk_tool_list])}\",\n                    examples=[\"Execute tool 'some_tool_name' with argument 'arg1'='value1'\"] # Generic example\n                )\n                def execute_adk_tool_skill(self, tool_name: str, arguments: dict[str, Any]) -&gt; str:\n                    \"\"\"Generic skill to execute an ADK tool by name with arguments.\"\"\"\n                    logger.info(f\"[A2A Skill] Request to execute ADK tool: {tool_name} with args: {arguments}\")\n\n                    # Find the ADK tool instance on the bound agent\n                    tool_to_call: BaseTool | None = None\n                    for tool in self.bound_agent.tools:\n                        if isinstance(tool, BaseTool) and tool.name == tool_name:\n                            tool_to_call = tool\n                            break\n\n                    if not tool_to_call:\n                        logger.warning(f\"[A2A Skill] ADK tool '{tool_name}' not found.\")\n                        return f\"Error: ADK tool '{tool_name}' not found on this agent.\"\n\n                    # --- Bridge sync skill call to async ADK tool execution ---\n                    async def run_adk_tool_async():\n                        try:\n                            # ADK tools require ToolContext. We can provide a minimal one or None.\n                            # Providing None might limit tool functionality.\n                            # Let's try providing None for simplicity first.\n                            adk_tool_context = None\n\n                            # Check if the tool has an async run method (most ADK tools should)\n                            if hasattr(tool_to_call, 'run_async') and iscoroutinefunction(tool_to_call.run_async):\n                                # Pass arguments directly to run_async\n                                result = await tool_to_call.run_async(args=arguments, tool_context=adk_tool_context)\n                                # Convert result to string for A2A response\n                                if isinstance(result, str): return result\n                                try: return json.dumps(result)\n                                except: return str(result)\n                            elif hasattr(tool_to_call, 'run') and callable(tool_to_call.run):\n                                # Fallback to synchronous run in thread pool\n                                logger.warning(f\"ADK tool '{tool_name}' has no run_async, using synchronous run in thread.\")\n                                result = await asyncio.to_thread(tool_to_call.run, args=arguments, tool_context=adk_tool_context)\n                                if isinstance(result, str): return result\n                                try: return json.dumps(result)\n                                except: return str(result)\n                            else:\n                                 return f\"Error: ADK tool '{tool_name}' has no callable run or run_async method.\"\n\n                        except Exception as e:\n                            logger.error(f\"[A2A Skill] Error executing ADK tool '{tool_name}': {e}\", exc_info=True)\n                            return f\"Error executing ADK tool {tool_name}: {e}\"\n\n                    # Execute the async tool runner\n                    try:\n                        return asyncio.run(run_adk_tool_async())\n                    except RuntimeError:\n                        logger.error(f\"RuntimeError calling asyncio.run in execute_adk_tool_skill for tool {tool_name}.\")\n                        return \"Error: Could not execute ADK tool asynchronously.\"\n\n        # --- End of Skill Definitions ---\n\n    # Instantiate the dynamic server class\n    try:\n         self.a2a_server = DynamicA2AServer(**a2a_server_options)\n         logger.info(f\"A2A server instance created for agent '{self.amd.name}'.\")\n         return self.a2a_server\n    except Exception as e:\n         logger.error(f\"Failed to instantiate dynamic A2A Server: {e}\", exc_info=True)\n         return None\n</code></pre> <code>setup_adk_runner(runner_options=None)</code> \u00b6 <p>Initializes an ADK runner for this agent (if ADK enabled).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def setup_adk_runner(self, runner_options: dict[str, Any] | None = None):\n    \"\"\"Initializes an ADK runner for this agent (if ADK enabled).\"\"\"\n    if not ADK_AVAILABLE:\n        logger.warning(\"ADK not available. Cannot setup ADK runner.\")\n        return None\n    if not isinstance(self, LlmAgent):\n        logger.error(\"Agent must inherit from LlmAgent to use ADK runner directly.\")\n        return None\n    if self.adk_runner:\n        logger.warning(\"ADK runner already initialized.\")\n        return self.adk_runner\n\n    runner_opts = runner_options or {}\n    runner_class = runner_opts.pop(\"runner_class\", InMemoryRunner) # Default to InMemory\n    app_name = runner_opts.pop(\"app_name\", f\"{self.amd.name}_ADKApp\")\n\n    if runner_class == InMemoryRunner:\n        runner_opts = {}\n\n    logger.info(f\"Setting up ADK Runner ({runner_class.__name__}) for app '{app_name}'...\")\n\n    try:\n         # Pass the agent instance and other options to the runner constructor\n        self.adk_runner = runner_class(agent=self, app_name=app_name, **runner_opts)\n        self.adk_session_service = self.adk_runner.session_service # Store session service\n        logger.info(f\"ADK {runner_class.__name__} setup complete for agent '{self.amd.name}'.\")\n        return self.adk_runner\n    except Exception as e:\n        logger.error(f\"Failed to setup ADK runner: {e}\", exc_info=True)\n        self.adk_runner = None\n        self.adk_session_service = None\n        return None\n</code></pre> <code>setup_mcp_server(host='0.0.0.0', port=8000, **mcp_kwargs)</code> \u00b6 <p>Initialize and configure the MCP server capabilities for this agent. This agent will ACT AS an MCP Server.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def setup_mcp_server(self, host=\"0.0.0.0\", port=8000, **mcp_kwargs):\n    \"\"\"Initialize and configure the MCP server capabilities *for this agent*.\n       This agent will ACT AS an MCP Server.\n    \"\"\"\n    if not MCP_AVAILABLE:\n        logger.warning(\"MCP library not installed. Cannot setup MCP server.\")\n        return None\n    if self.mcp_server:\n        logger.warning(\"MCP server already initialized.\")\n        return self.mcp_server\n    name = mcp_kwargs.get(\"name\")\n    del mcp_kwargs[\"name\"]\n    self.mcp_server = FastMCP(name=name or f\"{self.amd.name}-mcp-server\",\n                              description=f\"MCP interface for EnhancedAgent {self.amd.name}\",\n                              **mcp_kwargs)\n    logger.info(f\"Setting up MCP server for agent '{self.amd.name}' on {host}:{port}\")\n\n    # --- Register Agent's core functionalities as MCP services ---\n    # Example: Expose World Model (Read-only for safety)\n    @self.mcp_server.resource(f\"agent://{self.amd.name}/world_model\")\n    def mcp_get_world_model_resource() -&gt; dict[str, Any]:\n        \"\"\"Gets the agent's world model.\"\"\"\n        logger.debug(f\"[MCP Resource] agent://{self.amd.name}/world_model accessed\")\n        return self.world_model.to_dict()\n\n    # Example: Expose a simple query tool via MCP\n    @self.mcp_server.tool(name=\"simple_llm_query\")\n    async def mcp_simple_query(prompt: str) -&gt; str:\n        \"\"\"Sends a simple prompt to the agent's LLM (non-persistent run).\"\"\"\n        logger.debug(f\"[MCP Tool] simple_llm_query called: {prompt[:50]}...\")\n        # Use a minimal, non-persistent run, disable recursive calls\n        response = await self.a_run(\n            prompt, session_id=f\"mcp_query_{uuid.uuid4()}\",\n            persist_history=False, strategy_override=ProcessingStrategy.DIRECT_LLM\n        )\n        return response\n\n    # If ADK tools exist, potentially expose them via MCP automatically?\n    if ADK_AVAILABLE and isinstance(self, LlmAgent) and self.tools:\n         logger.info(\"Attempting to expose ADK tools via MCP server...\")\n         for adk_tool in self.tools:\n             if adk_tool.name in [\"code_execution\", \"adk_tool_a2a_send_and_wait\", \"adk_tool_a2a_send_no_wait\", \"adk_tool_a2a_get_task_status\", \"adk_tool_a2a_cancel_task\"]:\n                 continue\n             if not isinstance(adk_tool, BaseTool): continue\n             try:\n                 mcp_schema = adk_to_mcp_tool_type(adk_tool)\n\n                 # Define the MCP tool handler dynamically\n                 async def mcp_tool_handler(tool_name=adk_tool.name, **kwargs):\n                     logger.info(f\"[MCP Tool via ADK] Calling {tool_name} with {kwargs}\")\n                     # ADK tools expect ToolContext, which we don't have here.\n                     # We might need to simulate it or adapt the tool execution.\n                     # This simple version calls the tool's underlying function if possible.\n                     # WARNING: This bypasses ADK's standard tool execution flow.\n                     if hasattr(adk_tool, 'func') and callable(adk_tool.func):\n                         # This assumes the function doesn't need ToolContext\n                         result = await adk_tool.func(**kwargs)\n                         # Convert result to MCP content (e.g., TextContent)\n                         if isinstance(result, str):\n                             return [mcp_types.TextContent(type=\"text\", text=result)]\n                         else:\n                             try:\n                                 return [mcp_types.TextContent(type=\"text\", text=json.dumps(result))]\n                             except:\n                                 return [mcp_types.TextContent(type=\"text\", text=str(result))]\n                     else:\n                         logger.warning(f\"Cannot directly call ADK tool {tool_name} via MCP.\")\n                         return [mcp_types.TextContent(type=\"text\", text=f\"Error: Cannot execute ADK tool {tool_name} directly.\")]\n\n                 # Register the dynamic handler with the MCP server\n                 self.mcp_server.tool(name=mcp_schema.name)(mcp_tool_handler)\n                 logger.info(f\"Exposed ADK tool '{adk_tool.name}' as MCP tool '{mcp_schema.name}'.\")\n\n             except Exception as e:\n                 logger.warning(f\"Failed to expose ADK tool '{adk_tool.name}' via MCP: {e}\")\n\n\n    logger.info(f\"MCP server setup complete for agent '{self.amd.name}'. Run `agent.run_mcp_server()` to start.\")\n    return self.mcp_server\n</code></pre> <code>LLMMessage</code> <code>dataclass</code> \u00b6 <p>Represents a message in a conversation, compatible with LiteLLM.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@dataclass\nclass LLMMessage:\n    \"\"\"Represents a message in a conversation, compatible with LiteLLM.\"\"\"\n    role: Literal[\"user\", \"assistant\", \"system\", \"tool\"]\n    content: str | list[dict[str, Any]] # String or multimodal content (LiteLLM format)\n    tool_call_id: Optional[str] = None # For tool responses\n    name: Optional[str] = None # For tool calls/responses (function name)\n\n    # Add tool_calls for assistant messages requesting tool use (LiteLLM format)\n    tool_calls: list[dict[str, Any]] | None = None # e.g., [{\"id\": \"call_123\", \"function\": {\"name\": \"...\", \"arguments\": \"{...}\"}}]\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Converts to dict suitable for LiteLLM.\"\"\"\n        d = {\n            \"role\": self.role,\n            \"content\": self.content,\n        }\n        if self.tool_call_id: d[\"tool_call_id\"] = self.tool_call_id\n        if self.name: d[\"name\"] = self.name\n        if self.tool_calls: d[\"tool_calls\"] = self.tool_calls\n        return d\n</code></pre> <code>to_dict()</code> \u00b6 <p>Converts to dict suitable for LiteLLM.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Converts to dict suitable for LiteLLM.\"\"\"\n    d = {\n        \"role\": self.role,\n        \"content\": self.content,\n    }\n    if self.tool_call_id: d[\"tool_call_id\"] = self.tool_call_id\n    if self.name: d[\"name\"] = self.name\n    if self.tool_calls: d[\"tool_calls\"] = self.tool_calls\n    return d\n</code></pre> <code>WorldModel</code> <code>dataclass</code> \u00b6 <p>Thread-safe persistent understanding of the world for the agent.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@dataclass\nclass WorldModel:\n    \"\"\"Thread-safe persistent understanding of the world for the agent.\"\"\"\n    data: dict[str, Any] = dataclass_field(default_factory=dict)\n    _lock: SkipValidation[threading.Lock] = dataclass_field(default_factory=threading.Lock, init=False, repr=False)\n\n    def get(self, key: str, default: Any = None) -&gt; Any:\n        with self._lock:\n            return self.data.get(key, default)\n\n    def set(self, key: str, value: Any):\n        with self._lock:\n            logger.debug(f\"WorldModel SET: {key} = {value}\")\n            self.data[key] = value\n\n    def remove(self, key: str):\n        with self._lock:\n            if key in self.data:\n                logger.debug(f\"WorldModel REMOVE: {key}\")\n                del self.data[key]\n\n    def show(self) -&gt; str:\n        with self._lock:\n            if not self.data:\n                return \"[empty]\"\n            try:\n                items = [f\"- {k}: {json.dumps(v, indent=None, ensure_ascii=False, default=str)}\"\n                         for k, v in self.data.items()]\n                return \"\\n\".join(items)\n            except Exception:\n                items = [f\"- {k}: {str(v)}\" for k, v in self.data.items()]\n                return \"\\n\".join(items)\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        with self._lock:\n            return self.data.copy()\n\n    def update_from_dict(self, data_dict: dict[str, Any]):\n        with self._lock:\n            self.data.update(data_dict)\n            logger.debug(f\"WorldModel updated from dict: {list(data_dict.keys())}\")\n\n    def sync_from_adk_state(self, adk_state: State):\n        \"\"\"Updates the WorldModel from an ADK Session State.\"\"\"\n        if not ADK_AVAILABLE or not isinstance(adk_state, State):\n            return\n        with self._lock:\n            # Simple overwrite strategy, could be more sophisticated (merge, etc.)\n            self.data = adk_state.to_dict() # ADK State is dict-like\n            logger.debug(f\"WorldModel synced FROM ADK state. Keys: {list(self.data.keys())}\")\n\n    def sync_to_adk_state(self, adk_state: State):\n        \"\"\"Updates an ADK Session State from the WorldModel.\"\"\"\n        if not ADK_AVAILABLE or not isinstance(adk_state, State):\n            return\n        with self._lock:\n            # Update the ADK state dictionary directly\n            adk_state.update(self.data)\n            logger.debug(f\"WorldModel synced TO ADK state. Keys: {list(adk_state.keys())}\")\n</code></pre> <code>sync_from_adk_state(adk_state)</code> \u00b6 <p>Updates the WorldModel from an ADK Session State.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def sync_from_adk_state(self, adk_state: State):\n    \"\"\"Updates the WorldModel from an ADK Session State.\"\"\"\n    if not ADK_AVAILABLE or not isinstance(adk_state, State):\n        return\n    with self._lock:\n        # Simple overwrite strategy, could be more sophisticated (merge, etc.)\n        self.data = adk_state.to_dict() # ADK State is dict-like\n        logger.debug(f\"WorldModel synced FROM ADK state. Keys: {list(self.data.keys())}\")\n</code></pre> <code>sync_to_adk_state(adk_state)</code> \u00b6 <p>Updates an ADK Session State from the WorldModel.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def sync_to_adk_state(self, adk_state: State):\n    \"\"\"Updates an ADK Session State from the WorldModel.\"\"\"\n    if not ADK_AVAILABLE or not isinstance(adk_state, State):\n        return\n    with self._lock:\n        # Update the ADK state dictionary directly\n        adk_state.update(self.data)\n        logger.debug(f\"WorldModel synced TO ADK state. Keys: {list(adk_state.keys())}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent.builder","title":"<code>builder</code>","text":"<code>BuilderConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Serializable configuration state for the EnhancedAgentBuilder.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>class BuilderConfig(BaseModel):\n    \"\"\"Serializable configuration state for the EnhancedAgentBuilder.\"\"\"\n    agent_name: str = \"UnnamedEnhancedAgent\"\n    agent_version: str = \"0.1.0\"\n\n    # Core Model Config (Subset of AgentModelData, as some are instance-specific like BudgetManager)\n    model_identifier: str | None = None\n    formatter_llm_model: str | None = None\n    system_message: str = \"You are a helpful AI assistant.\"\n    temperature: float | None = None\n    top_k: int | None = None\n    top_p: float | None = None\n    max_tokens_output: int | None = None # Max tokens for LLM *generation*\n    max_tokens_input: int | None = None # Max context window (for trimming)\n    api_key_env_var: str | None = None # Store env var name, not the key itself\n    api_base: str | None = None\n    api_version: str | None = None\n    stop_sequence: list[str] | None = None\n    llm_user_id: str | None = None # 'user' param for LLM calls\n    enable_litellm_caching: bool = True\n\n    # Agent Behavior\n    enable_streaming: bool = False\n    verbose_logging: bool = False\n    world_model_initial_data: dict[str, Any] | None = None\n    history: BuilderHistoryConfig = Field(default_factory=BuilderHistoryConfig)\n\n    # Framework Integrations\n    adk: BuilderADKConfig = Field(default_factory=BuilderADKConfig)\n    a2a: BuilderA2AConfig = Field(default_factory=BuilderA2AConfig)\n    mcp: BuilderMCPConfig = Field(default_factory=BuilderMCPConfig)\n\n    # Cost Tracking (Configuration for persistence)\n    cost_tracker_config: dict[str, Any] | None = Field(default={'type': 'json', 'filepath': './user_costs.json'}, description=\"Config for UserCostTracker (e.g., type, path)\")\n\n    # Observability (Configuration)\n    telemetry_config: dict[str, Any] | None = Field(default={'enabled': False, 'service_name': None, 'endpoint': None}, description=\"Basic OTel config hints\")\n\n    model_config = ConfigDict(validate_assignment=True)\n\n    @model_validator(mode='after')\n    def _resolve_names(self) -&gt; 'BuilderConfig':\n        # Ensure service name defaults to agent name if not set\n        if self.telemetry_config and self.telemetry_config.get('enabled') and not self.telemetry_config.get('service_name'):\n            self.telemetry_config['service_name'] = self.agent_name\n        # Ensure MCP server name defaults if not set\n        if self.mcp.enabled and not self.mcp.server_name:\n             self.mcp.server_name = f\"{self.agent_name}_MCPServer\"\n        return self\n</code></pre> <code>EnhancedAgentBuilder</code> \u00b6 <p>Fluent builder for configuring and constructing production-ready EnhancedAgent instances. Supports loading configuration from files and provides methods for detailed setup.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>class EnhancedAgentBuilder:\n    \"\"\"\n    Fluent builder for configuring and constructing production-ready EnhancedAgent instances.\n    Supports loading configuration from files and provides methods for detailed setup.\n    \"\"\"\n\n    def __init__(self,agent_name: str = \"DefaultAgent\", config: BuilderConfig | None = None, config_path: str | Path | None = None):\n        \"\"\"\n        Initialize the builder. Can start with a config object, path, or blank.\n\n        Args:\n            config: An existing BuilderConfig object.\n            config_path: Path to a YAML/JSON configuration file for the builder.\n        \"\"\"\n        if config and config_path:\n            raise ValueError(\"Provide either config object or config_path, not both.\")\n\n        if config_path:\n            self.load_config(config_path) # Sets self._config\n        elif config:\n            self._config = config.copy(deep=True)\n        else:\n            self._config = BuilderConfig() # Start with defaults\n\n        # --- Transient fields (not saved/loaded directly via BuilderConfig JSON) ---\n        # Instances or non-serializable objects provided programmatically.\n        self._adk_tools_transient: list[ADKBaseTool | Callable] = []\n        self._adk_code_executor_instance: ADKBaseCodeExecutor | None = None\n        self._adk_runner_instance: ADKRunner | None = None\n        self._adk_session_service_instance: ADKSessionService | None = None\n        self._adk_planner_instance: ADKPlanner | None = None\n        self._litellm_budget_manager_instance: BudgetManager | None = None\n        self._user_cost_tracker_instance: UserCostTracker | None = None\n        self._otel_trace_provider_instance: TracerProvider | None = None\n        self._callbacks_transient: dict[str, Callable] = {}\n        # Pre-initialized server instances (less common, but possible)\n        self._a2a_server_instance: A2AServer | None = None\n        self._mcp_server_instance: FastMCP | None = None\n\n        # Set initial log level based on loaded config\n        logger.setLevel(logging.DEBUG if self._config.verbose_logging else logging.INFO)\n        self.with_agent_name(agent_name)\n\n    # --- Configuration Save/Load ---\n\n    def save_config(self, path: str | Path, indent: int = 2):\n        \"\"\"Saves the current builder configuration to a JSON file.\"\"\"\n        filepath = Path(path)\n        try:\n            filepath.parent.mkdir(parents=True, exist_ok=True)\n            config_json = self._config.model_dump_json(indent=indent)\n            with open(filepath, 'w') as f:\n                f.write(config_json)\n            logger.info(f\"Builder configuration saved to {filepath}\")\n        except OSError as e:\n            logger.error(f\"Failed to save builder configuration to {filepath}: {e}\")\n        except ValidationError as e:\n             logger.error(f\"Configuration is invalid, cannot save: {e}\")\n        except Exception as e:\n             logger.error(f\"An unexpected error occurred during config save: {e}\")\n\n\n    def load_config(self, path: str | Path) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Loads builder configuration from a JSON file, overwriting current settings.\"\"\"\n        filepath = Path(path)\n        if not filepath.exists():\n            raise FileNotFoundError(f\"Builder configuration file not found: {filepath}\")\n        try:\n            with open(filepath) as f:\n                config_data = json.load(f)\n            self._config = BuilderConfig.model_validate(config_data)\n            logger.info(f\"Builder configuration loaded from {filepath}\")\n            # Reset transient fields, as they are not saved\n            self._reset_transient_fields()\n            logger.warning(\"Transient fields (callbacks, tool instances, tracker instance, etc.) reset. Re-add them if needed.\")\n            # Update logger level based on loaded config\n            logger.setLevel(logging.DEBUG if self._config.verbose_logging else logging.INFO)\n        except (OSError, json.JSONDecodeError) as e:\n            logger.error(f\"Failed to load or parse builder configuration from {filepath}: {e}\")\n            raise\n        except ValidationError as e:\n             logger.error(f\"Loaded configuration data is invalid: {e}\")\n             raise\n        return self\n\n    def _reset_transient_fields(self):\n        \"\"\"Clears fields that are not part of the saved BuilderConfig.\"\"\"\n        self._adk_tools_transient = []\n        self._adk_code_executor_instance = None\n        self._adk_runner_instance = None\n        self._adk_session_service_instance = None\n        self._adk_planner_instance = None\n        self._litellm_budget_manager_instance = None\n        self._user_cost_tracker_instance = None\n        self._otel_trace_provider_instance = None\n        self._callbacks_transient = {}\n        self._a2a_server_instance = None\n        self._mcp_server_instance = None\n\n    # --- Fluent Configuration Methods (Modify self._config) ---\n\n    def with_agent_name(self, name: str) -&gt; 'EnhancedAgentBuilder':\n        self._config.agent_name = name\n        # Update dependent defaults\n        self._config = BuilderConfig.model_validate(self._config.model_dump())\n        return self\n\n    def with_agent_version(self, version: str) -&gt; 'EnhancedAgentBuilder':\n        self._config.agent_version = version\n        return self\n\n    def with_model(self, model_identifier: str) -&gt; 'EnhancedAgentBuilder':\n        self._config.model_identifier = model_identifier\n        # Auto-detect context window if not set\n        if not self._config.max_tokens_input:\n            try:\n                max_input = get_max_tokens(model_identifier)\n                if max_input:\n                    self._config.max_tokens_input = max_input\n                    logger.info(f\"Auto-detected max_input_tokens for {model_identifier}: {max_input}\")\n                else:\n                     # Default fallback if detection fails\n                    self._config.max_tokens_input = 4096\n                    logger.warning(f\"Could not auto-detect max_input_tokens for {model_identifier}, defaulting to 4096.\")\n            except Exception as e:\n                 self._config.max_tokens_input = 4096\n                 logger.warning(f\"Error auto-detecting max_input_tokens ({e}), defaulting to 4096.\")\n        # Auto-configure Ollama base URL\n        if 'ollama/' in model_identifier and not self._config.api_base:\n            self.with_api_base(\"http://localhost:11434\") # Uses the method to log\n        return self\n\n    def with_system_message(self, message: str) -&gt; 'EnhancedAgentBuilder':\n        self._config.system_message = message\n        return self\n\n    def with_temperature(self, temp: float) -&gt; 'EnhancedAgentBuilder':\n        self._config.temperature = temp\n        return self\n\n    def with_max_output_tokens(self, tokens: int) -&gt; 'EnhancedAgentBuilder':\n        self._config.max_tokens_output = tokens\n        return self\n\n    def with_max_input_tokens(self, tokens: int) -&gt; 'EnhancedAgentBuilder':\n        self._config.max_tokens_input = tokens\n        return self\n\n    def with_stop_sequence(self, stop: list[str]) -&gt; 'EnhancedAgentBuilder':\n        self._config.stop_sequence = stop\n        return self\n\n    def with_api_key_from_env(self, env_var_name: str) -&gt; 'EnhancedAgentBuilder':\n        self._config.api_key_env_var = env_var_name\n        # Quick check if env var exists\n        if not os.getenv(env_var_name):\n            logger.warning(f\"API key environment variable '{env_var_name}' is not set.\")\n        return self\n\n    def with_api_base(self, base_url: str | None) -&gt; 'EnhancedAgentBuilder':\n        self._config.api_base = base_url\n        logger.info(f\"API base set to: {base_url}\")\n        return self\n\n    def with_api_version(self, version: str | None) -&gt; 'EnhancedAgentBuilder':\n        self._config.api_version = version\n        return self\n\n    def with_llm_user_id(self, user_id: str) -&gt; 'EnhancedAgentBuilder':\n        self._config.llm_user_id = user_id\n        return self\n\n    def enable_litellm_caching(self, enable: bool = True) -&gt; 'EnhancedAgentBuilder':\n        self._config.enable_litellm_caching = enable\n        return self\n\n    def enable_streaming(self, enable: bool = True) -&gt; 'EnhancedAgentBuilder':\n        self._config.enable_streaming = enable\n        return self\n\n    def verbose(self, enable: bool = True) -&gt; 'EnhancedAgentBuilder':\n        self._config.verbose_logging = enable\n        logger.setLevel(logging.DEBUG if enable else logging.INFO)\n        os.environ['LITELLM_LOG'] = 'DEBUG' if enable else 'NONE' # Control LiteLLM verbosity too\n        return self\n    def formatter_llm_model(self, model: str) -&gt; 'EnhancedAgentBuilder':\n        self._config.formatter_llm_model = model\n        return self\n\n    def with_initial_world_data(self, data: dict[str, Any]) -&gt; 'EnhancedAgentBuilder':\n        self._config.world_model_initial_data = data\n        return self\n\n    def with_history_options(self, max_turns: int | None = 20, max_tokens: int | None = None, trim_strategy: Literal[\"litellm\", \"basic\"] = \"litellm\") -&gt; 'EnhancedAgentBuilder':\n        self._config.history = BuilderHistoryConfig(max_turns=max_turns, max_tokens=max_tokens, trim_strategy=trim_strategy)\n        return self\n\n    # --- ADK Configuration Methods ---\n    def _ensure_adk(self, feature: str):\n        if not ADK_AVAILABLE:\n            logger.warning(f\"ADK not available. Cannot configure ADK feature: {feature}.\")\n            return False\n        self._config.adk.enabled = True # Mark ADK as enabled if any ADK feature is used\n        return True\n\n    def enable_adk(self, runner_class: type[ADKRunner] = InMemoryRunner, runner_options: dict[str, Any] | None = None) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Enables ADK integration with a specified runner.\"\"\"\n        if not self._ensure_adk(\"Runner\"): return self\n        self._config.adk.runner_class_name = runner_class.__name__\n        self._config.adk.runner_options = runner_options or {}\n        logger.info(f\"ADK integration enabled with runner: {self._config.adk.runner_class_name}\")\n        return self\n\n    def with_adk_description(self, description: str) -&gt; 'EnhancedAgentBuilder':\n        if not self._ensure_adk(\"Description\"): return self\n        self._config.adk.description = description\n        return self\n\n    def with_adk_tool_instance(self, tool: ADKBaseTool) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Adds a pre-initialized ADK Tool instance (transient).\"\"\"\n        if not self._ensure_adk(\"Tool Instance\"): return self\n        if not isinstance(tool, ADKBaseTool):\n            raise TypeError(f\"Expected ADK BaseTool instance, got {type(tool)}\")\n        self._adk_tools_transient.append(tool)\n        return self\n\n    def with_adk_tool_function(self, func: Callable, name: Optional[str] = None,\n                               description: Optional[str] = None) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Adds a callable function as an ADK tool (transient).\"\"\"\n        if not self._ensure_adk(\"Tool Function\"):\n            return self\n        if not callable(func):\n            raise TypeError(f\"Expected callable function for ADK tool, got {type(func)}\")\n        if name:\n            func.__name__ = name\n        if description:\n            func.__doc__ = description\n        tool = FunctionTool(func)\n        self._adk_tools_transient.append(tool)\n        return self\n\n    def with_adk_mcp_toolset(self, connection_type: Literal[\"stdio\", \"sse\"], **kwargs) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Configures an ADK MCP Toolset connection (saved in config).\"\"\"\n        if not self._ensure_adk(\"MCP Toolset\"): return self\n        if connection_type == \"stdio\":\n            if \"command\" not in kwargs: raise ValueError(\"Stdio MCP toolset requires 'command' argument.\")\n            config = {\"type\": \"stdio\", \"command\": kwargs[\"command\"], \"args\": kwargs.get(\"args\", [])}\n        elif connection_type == \"sse\":\n            if \"url\" not in kwargs: raise ValueError(\"SSE MCP toolset requires 'url' argument.\")\n            config = {\"type\": \"sse\", \"url\": kwargs[\"url\"]}\n        else:\n            raise ValueError(f\"Unknown MCP toolset connection type: {connection_type}\")\n        self._config.adk.mcp_toolset_configs.append(config)\n        logger.info(f\"Configured ADK MCP Toolset: {config}\")\n        return self\n\n    def with_adk_code_executor(self, executor_type: Literal[\"adk_builtin\", \"unsafe_simple\", \"secure_placeholder\", \"none\"]) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Configures the type of ADK code executor to use (saved in config).\"\"\"\n        if not self._ensure_adk(\"Code Executor Type\"): return self\n        if executor_type == \"unsafe_simple\":\n            logger.critical(\"***********************************************************\")\n            logger.critical(\"*** WARNING: Configuring UNSAFE SimplePythonExecutor!   ***\")\n            logger.critical(\"***********************************************************\")\n        elif executor_type == \"secure_placeholder\":\n            logger.warning(\"Configuring SecureCodeExecutorPlaceholder. Implement actual sandboxing!\")\n        elif executor_type == \"adk_builtin\":\n            if self._config.model_identifier and (\"gemini-1.5\" not in self._config.model_identifier and \"gemini-2\" not in self._config.model_identifier) :\n                logger.warning(f\"ADK built-in code execution selected, but model '{self._config.model_identifier}' might not support it. Ensure model compatibility.\")\n            logger.info(\"Configuring ADK built-in code execution (tool-based, requires compatible model).\")\n\n        self._config.adk.code_executor_config = executor_type\n        self._adk_code_executor_instance = None # Clear any previously set instance\n        return self\n\n    def with_adk_code_executor_instance(self, executor: ADKBaseCodeExecutor) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Provides a pre-initialized ADK code executor instance (transient).\"\"\"\n        if not self._ensure_adk(\"Code Executor Instance\"): return self\n        if not isinstance(executor, ADKBaseCodeExecutor):\n            raise TypeError(f\"Expected ADKBaseCodeExecutor instance, got {type(executor)}\")\n        self._adk_code_executor_instance = executor\n        self._config.adk.code_executor_config = \"custom_instance\" # Mark config\n        logger.info(f\"Using custom ADK code executor instance: {type(executor).__name__}\")\n        return self\n\n    def enable_adk_state_sync(self, enable: bool = True) -&gt; 'EnhancedAgentBuilder':\n        if not self._ensure_adk(\"State Sync\"): return self\n        self._config.adk.sync_state = enable\n        return self\n\n    # --- Server Configuration Methods ---\n    def enable_a2a_server(self, host: str = \"0.0.0.0\", port: int = 5000, **extra_options) -&gt; 'EnhancedAgentBuilder':\n        if not A2A_AVAILABLE:\n            logger.warning(\"python-a2a library not available. Cannot enable A2A server.\")\n            self._config.a2a.enabled = False\n            return self\n        self._config.a2a.enabled = True\n        self._config.a2a.host = host\n        self._config.a2a.port = port\n        self._config.a2a.extra_options = extra_options\n        return self\n\n    def add_a2a_known_client(self, name: str, url: str) -&gt; 'EnhancedAgentBuilder':\n        if not A2A_AVAILABLE:\n            logger.warning(\"python-a2a library not available. Cannot add known A2A client.\")\n            return self\n        # A2A client setup is handled by the agent itself, we just store the config\n        self._config.a2a.known_clients[name] = url\n        logger.info(f\"Added known A2A client config: '{name}' -&gt; {url}\")\n        return self\n\n    def enable_mcp_server(self, host: str = \"0.0.0.0\", port: int = 8000, server_name: str | None = None, **extra_options) -&gt; 'EnhancedAgentBuilder':\n         if not MCP_AVAILABLE:\n             logger.warning(\"MCP library (FastMCP) not available. Cannot enable MCP server.\")\n             self._config.mcp.enabled = False\n             return self\n         self._config.mcp.enabled = True\n         self._config.mcp.host = host\n         self._config.mcp.port = port\n         self._config.mcp.server_name = server_name # Will default later if None\n         self._config.mcp.extra_options = extra_options\n         # Re-validate to update default name if needed\n         self._config = BuilderConfig.model_validate(self._config.model_dump())\n         return self\n\n    # --- Cost Tracking &amp; Budgeting Methods ---\n    def with_cost_tracker(self, tracker: UserCostTracker) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Provides a pre-initialized UserCostTracker instance (transient).\"\"\"\n        if not hasattr(tracker, \"get_all_costs\"): # Check protocol using isinstance\n             raise TypeError(\"Cost tracker must implement the UserCostTracker protocol.\")\n        self._user_cost_tracker_instance = tracker\n        # Clear file config if instance is provided\n        self._config.cost_tracker_config = {'type': 'custom_instance'}\n        logger.info(f\"Using custom UserCostTracker instance: {type(tracker).__name__}\")\n        return self\n\n    def with_json_cost_tracker(self, filepath: str | Path) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Configures the builder to use the JsonFileUserCostTracker (saved in config).\"\"\"\n        self._config.cost_tracker_config = {'type': 'json', 'filepath': str(filepath)}\n        self._user_cost_tracker_instance = None # Clear any instance\n        logger.info(f\"Configured JsonFileUserCostTracker: {filepath}\")\n        return self\n\n    def with_litellm_budget_manager(self, manager: BudgetManager) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Provides a pre-initialized LiteLLM BudgetManager instance (transient).\"\"\"\n        if not LITELLM_AVAILABLE:\n             logger.warning(\"LiteLLM not available, cannot set BudgetManager.\")\n             return self\n        if not isinstance(manager, BudgetManager):\n            raise TypeError(\"Expected litellm.BudgetManager instance.\")\n        self._litellm_budget_manager_instance = manager\n        return self\n\n    # --- Observability Methods ---\n    def enable_telemetry(self, service_name: str | None = None, endpoint: str | None = None) -&gt; 'EnhancedAgentBuilder':\n         if not OTEL_AVAILABLE:\n              logger.warning(\"OpenTelemetry SDK not available. Cannot enable telemetry.\")\n              self._config.telemetry_config = {'enabled': False}\n              return self\n         self._config.telemetry_config = {\n             'enabled': True,\n             'service_name': service_name, # Defaults to agent name later\n             'endpoint': endpoint # For OTLP exporter, e.g. \"http://localhost:4317\"\n         }\n         # Re-validate to update default name if needed\n         self._config = BuilderConfig.model_validate(self._config.model_dump())\n         return self\n\n    def with_telemetry_provider_instance(self, provider: TracerProvider) -&gt; 'EnhancedAgentBuilder':\n        \"\"\"Provides a pre-initialized OpenTelemetry TracerProvider instance (transient).\"\"\"\n        if not OTEL_AVAILABLE:\n            logger.warning(\"OpenTelemetry SDK not available. Cannot set TracerProvider.\")\n            return self\n        if not isinstance(provider, TracerProvider):\n             raise TypeError(\"Expected opentelemetry.sdk.trace.TracerProvider instance.\")\n        self._otel_trace_provider_instance = provider\n        # Mark telemetry as enabled, but using custom instance\n        self._config.telemetry_config = {'enabled': True, 'type': 'custom_instance'}\n        logger.info(\"Using custom OpenTelemetry TracerProvider instance.\")\n        return self\n\n    # --- Callback Methods (Transient) ---\n    def with_stream_callback(self, func: Callable[[str], None | Awaitable[None]]) -&gt; 'EnhancedAgentBuilder':\n        self._callbacks_transient['stream_callback'] = func; return self\n    def with_post_run_callback(self, func: Callable[[str, str, float, str | None], None | Awaitable[None]]) -&gt; 'EnhancedAgentBuilder':\n        self._callbacks_transient['post_run_callback'] = func; return self # Added user_id\n    def with_progress_callback(self, func: Callable[[Any], None | Awaitable[None]]) -&gt; 'EnhancedAgentBuilder':\n        self._callbacks_transient['progress_callback'] = func; return self\n    def with_human_in_loop_callback(self, func: Callable[[dict], str | Awaitable[str]]) -&gt; 'EnhancedAgentBuilder':\n        self._callbacks_transient['human_in_loop_callback'] = func; return self\n\n    # --- Build Method ---\n    async def build(self) -&gt; EnhancedAgent:\n        \"\"\"\n        Constructs and returns the configured EnhancedAgent instance.\n        Handles asynchronous setup like fetching ADK MCP tools.\n        \"\"\"\n        logger.info(f\"--- Building EnhancedAgent: {self._config.agent_name} v{self._config.agent_version} ---\")\n\n        # 1. Final Config Validation (Pydantic model handles most)\n        if not self._config.model_identifier:\n            raise ValueError(\"LLM model identifier is required. Use .with_model()\")\n\n        # 2. Resolve API Key\n        api_key = None\n        if self._config.api_key_env_var:\n            api_key = os.getenv(self._config.api_key_env_var)\n            if not api_key:\n                logger.warning(f\"API key environment variable '{self._config.api_key_env_var}' is set in config but not found in environment.\")\n            # else: logger.debug(\"API key loaded from environment variable.\") # Avoid logging key presence\n\n        # 3. Setup Telemetry Provider (if instance provided)\n        if self._otel_trace_provider_instance and OTEL_AVAILABLE:\n            trace.set_tracer_provider(self._otel_trace_provider_instance)\n            logger.info(\"Global OpenTelemetry TracerProvider set from provided instance.\")\n        elif self._config.telemetry_config.get('enabled') and self._config.telemetry_config.get('type') != 'custom_instance' and OTEL_AVAILABLE:\n             # Basic provider setup from config (can be expanded)\n             logger.info(\"Setting up basic OpenTelemetry based on config (ConsoleExporter example).\")\n             from opentelemetry.sdk.trace.export import (\n                 BatchSpanProcessor,\n                 ConsoleSpanExporter,\n             )\n             provider = TracerProvider()\n             provider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))\n             #: Add OTLP exporter based on self._config.telemetry_config['endpoint']\n             trace.set_tracer_provider(provider)\n             self._otel_trace_provider_instance = provider # Store for potential access?\n\n        # 4. Prepare Core Components\n        # Agent Model Data\n        try:\n            amd = AgentModelData(\n                name=self._config.agent_name,\n                model=self._config.model_identifier,\n                system_message=self._config.system_message,\n                temperature=self._config.temperature,\n                top_k=self._config.top_k,\n                top_p=self._config.top_p,\n                max_tokens=self._config.max_tokens_output,\n                max_input_tokens=self._config.max_tokens_input,\n                api_key=api_key,\n                api_base=self._config.api_base,\n                api_version=self._config.api_version,\n                stop_sequence=self._config.stop_sequence,\n                user_id=self._config.llm_user_id,\n                budget_manager=self._litellm_budget_manager_instance,\n                caching=self._config.enable_litellm_caching\n            )\n        except ValidationError as e:\n            logger.error(f\"Validation error creating AgentModelData: {e}\")\n            raise\n\n        # World Model\n        world_model = self._config.world_model_initial_data or {}\n\n        # User Cost Tracker\n        cost_tracker = self._user_cost_tracker_instance # Use provided instance if available\n        if not cost_tracker and self._config.cost_tracker_config:\n            tracker_type = self._config.cost_tracker_config.get('type')\n            if tracker_type == 'json':\n                filepath = self._config.cost_tracker_config.get('filepath')\n                if filepath:\n                    cost_tracker = JsonFileUserCostTracker(filepath)\n                    logger.info(f\"Initialized JsonFileUserCostTracker ({filepath})\")\n                else:\n                    logger.warning(\"JSON cost tracker configured but filepath missing.\")\n            elif tracker_type == 'custom_instance':\n                 logger.warning(\"Cost tracker configured as 'custom_instance' but no instance was provided via .with_cost_tracker().\")\n            # Add other tracker types (DB, InMemory) here\n\n        # 5. Prepare ADK Components\n        adk_runner_instance = self._adk_runner_instance\n        adk_session_service = self._adk_session_service_instance\n        adk_planner_instance = self._adk_planner_instance\n        adk_code_executor = self._adk_code_executor_instance # Use provided instance first\n        adk_exit_stack = None\n        processed_adk_tools = list(self._adk_tools_transient) # Start with transient tools\n\n        if ADK_AVAILABLE and self._config.adk.enabled:\n            logger.info(\"Configuring ADK components...\")\n            adk_exit_stack = contextlib.AsyncExitStack()\n\n            # --- ADK Runner &amp; Session Service ---\n            if not adk_runner_instance:\n                runner_cls_name = self._config.adk.runner_class_name\n                runner_opts = self._config.adk.runner_options\n                try:\n                    # Dynamically import/get runner class\n                    if runner_cls_name == \"InMemoryRunner\": runner_class = InMemoryRunner\n                    elif runner_cls_name == \"Runner\": runner_class = Runner\n                    elif runner_cls_name == \"AsyncWebRunner\": runner_class = AsyncWebRunner # If available\n                    else: raise ValueError(f\"Unsupported ADK Runner class name: {runner_cls_name}\")\n\n                    # Special handling: InMemoryRunner needs agent instance *later*\n                    if runner_class is InMemoryRunner or runner_class is Runner:\n                         logger.debug(\"Deferring InMemoryRunner creation until after agent instantiation.\")\n                         # Store config to create it later\n                         adk_runner_config_for_later = {\n                             \"runner_class\": runner_class,\n                             \"app_name\": runner_opts.get(\"app_name\", f\"{self._config.agent_name}_ADKApp\"),\n                             \"session_service\": adk_session_service, # Pass service if already created\n                             **runner_opts # Pass other options\n                         }\n                         adk_runner_instance = None # Ensure it's None for now\n                    else: # Other runners might be creatable now\n                         # Need to ensure session service is handled correctly if runner needs it\n                         if not adk_session_service:\n                             # Create default session service if needed by runner\n                             # This part is complex as runners might create their own\n                             logger.info(\"Using default ADK InMemorySessionService for runner.\")\n                             adk_session_service = InMemorySessionService()\n\n                         adk_runner_instance = runner_class(\n                             session_service=adk_session_service,\n                             app_name=runner_opts.get(\"app_name\", f\"{self._config.agent_name}_ADKApp\"),\n                             **runner_opts # Pass other options\n                         )\n                         logger.info(f\"Created ADK Runner instance: {runner_cls_name}\")\n\n                except (ImportError, ValueError, TypeError) as e:\n                    logger.error(f\"Failed to configure ADK Runner '{runner_cls_name}': {e}\", exc_info=True)\n                    raise ValueError(f\"Failed to setup ADK Runner: {e}\") from e\n\n            # Ensure session service exists if runner created one\n            if adk_runner_instance and hasattr(adk_runner_instance, 'session_service'):\n                 if not adk_session_service:\n                     adk_session_service = adk_runner_instance.session_service\n                 elif adk_session_service is not adk_runner_instance.session_service:\n                     logger.warning(\"Provided ADK SessionService differs from the one in the provided ADK Runner. Using the runner's service.\")\n                     adk_session_service = adk_runner_instance.session_service\n\n            # Fallback: create default session service if none exists by now\n            if not adk_session_service:\n                  logger.info(\"Using default ADK InMemorySessionService.\")\n                  adk_session_service = InMemorySessionService()\n\n\n            # --- ADK Code Executor ---\n            if not adk_code_executor: # If instance wasn't provided directly\n                executor_config = self._config.adk.code_executor_config\n                if executor_config == \"unsafe_simple\":\n                    adk_code_executor = UnsafeSimplePythonExecutor()\n                    logger.critical(\"UNSAFE code executor instance created!\")\n                elif executor_config == \"secure_placeholder\":\n                    adk_code_executor = SecureCodeExecutorPlaceholder()\n                    logger.warning(\"SecureCodeExecutorPlaceholder instance created.\")\n                elif executor_config == \"adk_builtin\":\n                    # This type uses the TOOL, not an executor instance passed to LlmAgent init\n                    adk_code_executor = adk_built_in_code_execution\n                    #if not any(getattr(t, 'func', None) == tool_func for t in processed_adk_tools if isinstance(t, FunctionTool)):\n                    #     tool_func.__name__ = \"code_execution\"\n                    # processed_adk_tools.append(tool_func)\n                    #     logger.info(\"Added ADK built-in code execution tool.\")\n                    adk_code_executor = None # Ensure no executor instance is passed for this case\n                elif executor_config == \"none\":\n                    adk_code_executor = None\n                elif executor_config == \"custom_instance\":\n                    # Should have been provided via .with_adk_code_executor_instance()\n                    logger.error(\"ADK code executor configured as 'custom_instance' but no instance was provided.\")\n                    adk_code_executor = None\n                # Add handling for dict config if needed in the future\n\n            # --- ADK Tools (Wrap callables) ---\n            temp_tools = []\n            for tool_input in processed_adk_tools:\n                 if isinstance(tool_input, ADKBaseTool):\n                     temp_tools.append(tool_input)\n                 elif callable(tool_input):\n                     try:\n                         wrapped = ADKFunctionTool(func=tool_input)\n                         temp_tools.append(wrapped)\n                     except Exception as e: logger.warning(f\"Could not wrap callable '{getattr(tool_input, '__name__', 'unknown')}' as ADK tool: {e}\")\n                 else: logger.warning(f\"Skipping invalid ADK tool input: {type(tool_input)}\")\n            processed_adk_tools = temp_tools\n\n            # --- ADK MCP Toolsets ---\n            for mcp_conf in self._config.adk.mcp_toolset_configs:\n                 logger.info(f\"Fetching tools from configured MCP Server: {mcp_conf}...\")\n                 try:\n                      params = None\n                      if mcp_conf.get(\"type\") == \"stdio\":\n                          params = StdioServerParameters(command=mcp_conf[\"command\"], args=mcp_conf.get(\"args\", []))\n                      elif mcp_conf.get(\"type\") == \"sse\":\n                           params = SseServerParams(url=mcp_conf[\"url\"])\n\n                      if params:\n                          mcp_tools, _ = await MCPToolset.from_server(\n                              connection_params=params,\n                              async_exit_stack=adk_exit_stack\n                          )\n                          for tool in mcp_tools: tool._is_mcp_tool = True\n                          processed_adk_tools.extend(mcp_tools)\n                          logger.info(f\"Fetched {len(mcp_tools)} tools via ADK MCPToolset ({mcp_conf.get('type')}).\")\n                      else:\n                           logger.warning(f\"Unsupported MCP config type: {mcp_conf.get('type')}\")\n\n                 except Exception as e:\n                      logger.error(f\"Failed to fetch tools from MCP server {mcp_conf}: {e}\", exc_info=True)\n                      # Decide whether to raise or continue\n\n            # --- ADK Planner, Examples, Output Schema ---\n\n\n\n        # 6. Instantiate EnhancedAgent\n        try:\n            # Base arguments for EnhancedAgent\n            agent_init_kwargs = {\n                'amd': amd,\n                'world_model': world_model,\n                'format_model': self._config.formatter_llm_model if self._config.formatter_llm_model else None, # Example passing extra config\n                'verbose': self._config.verbose_logging,\n                'stream': self._config.enable_streaming,\n                'max_history_turns': self._config.history.max_turns,\n                'max_history_tokens': self._config.history.max_tokens,\n                'trim_strategy': self._config.history.trim_strategy,\n                'sync_adk_state': self._config.adk.sync_state if ADK_AVAILABLE else False,\n                'adk_exit_stack': adk_exit_stack, # Pass stack for cleanup\n                'user_cost_tracker': cost_tracker, # Pass the tracker instance\n                **self._callbacks_transient, # Pass configured callbacks\n                # Pass server instances if provided (less common)\n                'a2a_server': self._a2a_server_instance,\n                'mcp_server': self._mcp_server_instance,\n            }\n\n            # Add ADK-specific arguments if inheriting from LlmAgent\n            agent_class = EnhancedAgent\n            if ADK_AVAILABLE and issubclass(EnhancedAgent, ADKLlmAgent):\n                 logger.debug(\"Adding ADK LlmAgent specific arguments to init.\")\n                 adk_specific_kwargs = {\n                     'name': self._config.agent_name, # Required by LlmAgent\n                     'model': LiteLlm(model=self._config.model_identifier), # LlmAgent needs BaseLlm instance\n                     'description': self._config.adk.description or self._config.system_message,\n                     'instruction': self._config.system_message, # Or dedicated instruction field?\n                     'tools': processed_adk_tools,\n                     'code_executor': adk_code_executor, # Pass the *instance*\n                     'planner': adk_planner_instance,\n                     # Process examples/schema if needed\n                     'examples': [ADKExample(**ex) for ex in self._config.adk.examples] if self._config.adk.examples else None,\n                     'output_schema': self._config.adk.output_schema,\n                     # Pass runner/session service if NOT using InMemoryRunner deferred creation\n                     # If runner is created later, it's assigned post-init\n                     'runner': adk_runner_instance if adk_runner_instance else None, # Pass runner if created now\n                     'session_service': adk_session_service, # Pass session service\n                 }\n                 # Merge, ensuring agent_init_kwargs takes precedence for overlapping basic fields if necessary\n                 # but allow ADK specifics to be added. Be careful with overlaps like 'name'.\n                 # EnhancedAgent init should handle reconciling these if needed.\n                 # A safer merge:\n                 final_kwargs = agent_init_kwargs.copy()\n                 for k, v in adk_specific_kwargs.items():\n                      if k not in final_kwargs: # Only add ADK specifics not already handled\n                          final_kwargs[k] = v\n                      # Handle specific overrides/merges needed for LlmAgent base\n                      elif k == 'tools' and v: # Merge tools\n                          final_kwargs['tools'] = (final_kwargs.get('tools') or []) + v\n                      # Overwrite description/instruction from ADK config if set\n                      elif k in ['description', 'instruction'] and v or k == 'code_executor' or k == 'model':\n                           final_kwargs[k] = v\n\n                 agent_init_kwargs = final_kwargs\n\n\n            logger.info(f\"Final keys for EnhancedAgent init: {list(agent_init_kwargs.keys())}\")\n            logger.info(f\"Final keys for EnhancedAgent init: {agent_init_kwargs}\")\n\n            # --- Instantiate the Agent ---\n            agent = agent_class(**agent_init_kwargs)\n            # --- Agent Instantiated ---\n\n            # If ADK InMemoryRunner creation was deferred, create and assign now\n            if ADK_AVAILABLE and 'adk_runner_config_for_later' in locals():\n                 cfg = locals()['adk_runner_config_for_later']\n                 if not isinstance(cfg['runner_class'], InMemoryRunner) and cfg.get('session_service') is None: cfg['session_service'] = agent.adk_session_service # Ensure service is passed\n                 agent.setup_adk_runner(cfg)\n                 logger.info(f\"Created and assigned deferred ADK Runner instance: {agent.adk_runner.__class__.__name__}\")\n                 # Ensure agent has runner's session service if it differs\n                 if agent.adk_runner and agent.adk_session_service is not agent.adk_runner.session_service:\n                      logger.warning(\"Agent session service differs from deferred runner's service. Updating agent's reference.\")\n                      agent.adk_session_service = agent.adk_runner.session_service\n            elif ADK_AVAILABLE and adk_runner_instance and not agent.adk_runner:\n                # If runner was created earlier but not passed via LlmAgent init (e.g. non-LlmAgent base)\n                # Or if we want to explicitly assign it\n                 agent.adk_runner = adk_runner_instance\n                 # Ensure session service consistency\n                 if agent.adk_session_service is not agent.adk_runner.session_service:\n                      agent.adk_session_service = agent.adk_runner.session_service\n\n\n        except ValidationError as e:\n            logger.error(f\"Pydantic validation error Instantiating EnhancedAgent: {e}\", exc_info=True)\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error Instantiating EnhancedAgent: {e}\", exc_info=True)\n            raise\n\n        # 7. Setup Agent's Internal Server Capabilities (if enabled and not pre-initialized)\n        if self._config.a2a.enabled and not agent.a2a_server:\n            if AGENT_A2A_AVAILABLE:\n                logger.info(\"Setting up A2A server on agent instance...\")\n                agent.setup_a2a_server(\n                    host=self._config.a2a.host,\n                    port=self._config.a2a.port,\n                    **self._config.a2a.extra_options\n                )\n            else: logger.warning(\"A2A server configured in builder, but A2A not available in agent environment.\")\n\n        if self._config.mcp.enabled and not agent.mcp_server:\n            if AGENT_MCP_AVAILABLE:\n                logger.info(\"Setting up MCP server on agent instance...\")\n                agent.setup_mcp_server(\n                    host=self._config.mcp.host,\n                    port=self._config.mcp.port,\n                    name=self._config.mcp.server_name, # Already defaulted\n                    **self._config.mcp.extra_options\n                )\n            else: logger.warning(\"MCP server configured in builder, but MCP not available in agent environment.\")\n\n        # 8. Setup A2A known clients configuration on the agent\n        if self._config.a2a.known_clients:\n             if AGENT_A2A_AVAILABLE:\n                 # The agent likely handles client creation on demand,\n                 # but we can pass the config for it to use.\n                 # Assuming agent has a way to receive this, e.g., during init or a setter\n                 if hasattr(agent, 'set_known_a2a_clients'):\n                     agent.set_known_a2a_clients(self._config.a2a.known_clients)\n                 else:\n                      # Fallback: store on a generic config dict? Less ideal.\n                      # agent.config.a2a_known_clients = self._config.a2a.known_clients\n                      logger.warning(\"Agent does not have 'set_known_a2a_clients' method. Known client config stored raw.\")\n             else:\n                  logger.warning(\"A2A known clients configured, but A2A not available in agent env.\")\n\n\n        logger.info(f\"--- EnhancedAgent Build Complete: {agent.amd.name} ---\")\n        return agent\n</code></pre> <code>__init__(agent_name='DefaultAgent', config=None, config_path=None)</code> \u00b6 <p>Initialize the builder. Can start with a config object, path, or blank.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BuilderConfig | None</code> <p>An existing BuilderConfig object.</p> <code>None</code> <code>config_path</code> <code>str | Path | None</code> <p>Path to a YAML/JSON configuration file for the builder.</p> <code>None</code> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def __init__(self,agent_name: str = \"DefaultAgent\", config: BuilderConfig | None = None, config_path: str | Path | None = None):\n    \"\"\"\n    Initialize the builder. Can start with a config object, path, or blank.\n\n    Args:\n        config: An existing BuilderConfig object.\n        config_path: Path to a YAML/JSON configuration file for the builder.\n    \"\"\"\n    if config and config_path:\n        raise ValueError(\"Provide either config object or config_path, not both.\")\n\n    if config_path:\n        self.load_config(config_path) # Sets self._config\n    elif config:\n        self._config = config.copy(deep=True)\n    else:\n        self._config = BuilderConfig() # Start with defaults\n\n    # --- Transient fields (not saved/loaded directly via BuilderConfig JSON) ---\n    # Instances or non-serializable objects provided programmatically.\n    self._adk_tools_transient: list[ADKBaseTool | Callable] = []\n    self._adk_code_executor_instance: ADKBaseCodeExecutor | None = None\n    self._adk_runner_instance: ADKRunner | None = None\n    self._adk_session_service_instance: ADKSessionService | None = None\n    self._adk_planner_instance: ADKPlanner | None = None\n    self._litellm_budget_manager_instance: BudgetManager | None = None\n    self._user_cost_tracker_instance: UserCostTracker | None = None\n    self._otel_trace_provider_instance: TracerProvider | None = None\n    self._callbacks_transient: dict[str, Callable] = {}\n    # Pre-initialized server instances (less common, but possible)\n    self._a2a_server_instance: A2AServer | None = None\n    self._mcp_server_instance: FastMCP | None = None\n\n    # Set initial log level based on loaded config\n    logger.setLevel(logging.DEBUG if self._config.verbose_logging else logging.INFO)\n    self.with_agent_name(agent_name)\n</code></pre> <code>build()</code> <code>async</code> \u00b6 <p>Constructs and returns the configured EnhancedAgent instance. Handles asynchronous setup like fetching ADK MCP tools.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>async def build(self) -&gt; EnhancedAgent:\n    \"\"\"\n    Constructs and returns the configured EnhancedAgent instance.\n    Handles asynchronous setup like fetching ADK MCP tools.\n    \"\"\"\n    logger.info(f\"--- Building EnhancedAgent: {self._config.agent_name} v{self._config.agent_version} ---\")\n\n    # 1. Final Config Validation (Pydantic model handles most)\n    if not self._config.model_identifier:\n        raise ValueError(\"LLM model identifier is required. Use .with_model()\")\n\n    # 2. Resolve API Key\n    api_key = None\n    if self._config.api_key_env_var:\n        api_key = os.getenv(self._config.api_key_env_var)\n        if not api_key:\n            logger.warning(f\"API key environment variable '{self._config.api_key_env_var}' is set in config but not found in environment.\")\n        # else: logger.debug(\"API key loaded from environment variable.\") # Avoid logging key presence\n\n    # 3. Setup Telemetry Provider (if instance provided)\n    if self._otel_trace_provider_instance and OTEL_AVAILABLE:\n        trace.set_tracer_provider(self._otel_trace_provider_instance)\n        logger.info(\"Global OpenTelemetry TracerProvider set from provided instance.\")\n    elif self._config.telemetry_config.get('enabled') and self._config.telemetry_config.get('type') != 'custom_instance' and OTEL_AVAILABLE:\n         # Basic provider setup from config (can be expanded)\n         logger.info(\"Setting up basic OpenTelemetry based on config (ConsoleExporter example).\")\n         from opentelemetry.sdk.trace.export import (\n             BatchSpanProcessor,\n             ConsoleSpanExporter,\n         )\n         provider = TracerProvider()\n         provider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))\n         #: Add OTLP exporter based on self._config.telemetry_config['endpoint']\n         trace.set_tracer_provider(provider)\n         self._otel_trace_provider_instance = provider # Store for potential access?\n\n    # 4. Prepare Core Components\n    # Agent Model Data\n    try:\n        amd = AgentModelData(\n            name=self._config.agent_name,\n            model=self._config.model_identifier,\n            system_message=self._config.system_message,\n            temperature=self._config.temperature,\n            top_k=self._config.top_k,\n            top_p=self._config.top_p,\n            max_tokens=self._config.max_tokens_output,\n            max_input_tokens=self._config.max_tokens_input,\n            api_key=api_key,\n            api_base=self._config.api_base,\n            api_version=self._config.api_version,\n            stop_sequence=self._config.stop_sequence,\n            user_id=self._config.llm_user_id,\n            budget_manager=self._litellm_budget_manager_instance,\n            caching=self._config.enable_litellm_caching\n        )\n    except ValidationError as e:\n        logger.error(f\"Validation error creating AgentModelData: {e}\")\n        raise\n\n    # World Model\n    world_model = self._config.world_model_initial_data or {}\n\n    # User Cost Tracker\n    cost_tracker = self._user_cost_tracker_instance # Use provided instance if available\n    if not cost_tracker and self._config.cost_tracker_config:\n        tracker_type = self._config.cost_tracker_config.get('type')\n        if tracker_type == 'json':\n            filepath = self._config.cost_tracker_config.get('filepath')\n            if filepath:\n                cost_tracker = JsonFileUserCostTracker(filepath)\n                logger.info(f\"Initialized JsonFileUserCostTracker ({filepath})\")\n            else:\n                logger.warning(\"JSON cost tracker configured but filepath missing.\")\n        elif tracker_type == 'custom_instance':\n             logger.warning(\"Cost tracker configured as 'custom_instance' but no instance was provided via .with_cost_tracker().\")\n        # Add other tracker types (DB, InMemory) here\n\n    # 5. Prepare ADK Components\n    adk_runner_instance = self._adk_runner_instance\n    adk_session_service = self._adk_session_service_instance\n    adk_planner_instance = self._adk_planner_instance\n    adk_code_executor = self._adk_code_executor_instance # Use provided instance first\n    adk_exit_stack = None\n    processed_adk_tools = list(self._adk_tools_transient) # Start with transient tools\n\n    if ADK_AVAILABLE and self._config.adk.enabled:\n        logger.info(\"Configuring ADK components...\")\n        adk_exit_stack = contextlib.AsyncExitStack()\n\n        # --- ADK Runner &amp; Session Service ---\n        if not adk_runner_instance:\n            runner_cls_name = self._config.adk.runner_class_name\n            runner_opts = self._config.adk.runner_options\n            try:\n                # Dynamically import/get runner class\n                if runner_cls_name == \"InMemoryRunner\": runner_class = InMemoryRunner\n                elif runner_cls_name == \"Runner\": runner_class = Runner\n                elif runner_cls_name == \"AsyncWebRunner\": runner_class = AsyncWebRunner # If available\n                else: raise ValueError(f\"Unsupported ADK Runner class name: {runner_cls_name}\")\n\n                # Special handling: InMemoryRunner needs agent instance *later*\n                if runner_class is InMemoryRunner or runner_class is Runner:\n                     logger.debug(\"Deferring InMemoryRunner creation until after agent instantiation.\")\n                     # Store config to create it later\n                     adk_runner_config_for_later = {\n                         \"runner_class\": runner_class,\n                         \"app_name\": runner_opts.get(\"app_name\", f\"{self._config.agent_name}_ADKApp\"),\n                         \"session_service\": adk_session_service, # Pass service if already created\n                         **runner_opts # Pass other options\n                     }\n                     adk_runner_instance = None # Ensure it's None for now\n                else: # Other runners might be creatable now\n                     # Need to ensure session service is handled correctly if runner needs it\n                     if not adk_session_service:\n                         # Create default session service if needed by runner\n                         # This part is complex as runners might create their own\n                         logger.info(\"Using default ADK InMemorySessionService for runner.\")\n                         adk_session_service = InMemorySessionService()\n\n                     adk_runner_instance = runner_class(\n                         session_service=adk_session_service,\n                         app_name=runner_opts.get(\"app_name\", f\"{self._config.agent_name}_ADKApp\"),\n                         **runner_opts # Pass other options\n                     )\n                     logger.info(f\"Created ADK Runner instance: {runner_cls_name}\")\n\n            except (ImportError, ValueError, TypeError) as e:\n                logger.error(f\"Failed to configure ADK Runner '{runner_cls_name}': {e}\", exc_info=True)\n                raise ValueError(f\"Failed to setup ADK Runner: {e}\") from e\n\n        # Ensure session service exists if runner created one\n        if adk_runner_instance and hasattr(adk_runner_instance, 'session_service'):\n             if not adk_session_service:\n                 adk_session_service = adk_runner_instance.session_service\n             elif adk_session_service is not adk_runner_instance.session_service:\n                 logger.warning(\"Provided ADK SessionService differs from the one in the provided ADK Runner. Using the runner's service.\")\n                 adk_session_service = adk_runner_instance.session_service\n\n        # Fallback: create default session service if none exists by now\n        if not adk_session_service:\n              logger.info(\"Using default ADK InMemorySessionService.\")\n              adk_session_service = InMemorySessionService()\n\n\n        # --- ADK Code Executor ---\n        if not adk_code_executor: # If instance wasn't provided directly\n            executor_config = self._config.adk.code_executor_config\n            if executor_config == \"unsafe_simple\":\n                adk_code_executor = UnsafeSimplePythonExecutor()\n                logger.critical(\"UNSAFE code executor instance created!\")\n            elif executor_config == \"secure_placeholder\":\n                adk_code_executor = SecureCodeExecutorPlaceholder()\n                logger.warning(\"SecureCodeExecutorPlaceholder instance created.\")\n            elif executor_config == \"adk_builtin\":\n                # This type uses the TOOL, not an executor instance passed to LlmAgent init\n                adk_code_executor = adk_built_in_code_execution\n                #if not any(getattr(t, 'func', None) == tool_func for t in processed_adk_tools if isinstance(t, FunctionTool)):\n                #     tool_func.__name__ = \"code_execution\"\n                # processed_adk_tools.append(tool_func)\n                #     logger.info(\"Added ADK built-in code execution tool.\")\n                adk_code_executor = None # Ensure no executor instance is passed for this case\n            elif executor_config == \"none\":\n                adk_code_executor = None\n            elif executor_config == \"custom_instance\":\n                # Should have been provided via .with_adk_code_executor_instance()\n                logger.error(\"ADK code executor configured as 'custom_instance' but no instance was provided.\")\n                adk_code_executor = None\n            # Add handling for dict config if needed in the future\n\n        # --- ADK Tools (Wrap callables) ---\n        temp_tools = []\n        for tool_input in processed_adk_tools:\n             if isinstance(tool_input, ADKBaseTool):\n                 temp_tools.append(tool_input)\n             elif callable(tool_input):\n                 try:\n                     wrapped = ADKFunctionTool(func=tool_input)\n                     temp_tools.append(wrapped)\n                 except Exception as e: logger.warning(f\"Could not wrap callable '{getattr(tool_input, '__name__', 'unknown')}' as ADK tool: {e}\")\n             else: logger.warning(f\"Skipping invalid ADK tool input: {type(tool_input)}\")\n        processed_adk_tools = temp_tools\n\n        # --- ADK MCP Toolsets ---\n        for mcp_conf in self._config.adk.mcp_toolset_configs:\n             logger.info(f\"Fetching tools from configured MCP Server: {mcp_conf}...\")\n             try:\n                  params = None\n                  if mcp_conf.get(\"type\") == \"stdio\":\n                      params = StdioServerParameters(command=mcp_conf[\"command\"], args=mcp_conf.get(\"args\", []))\n                  elif mcp_conf.get(\"type\") == \"sse\":\n                       params = SseServerParams(url=mcp_conf[\"url\"])\n\n                  if params:\n                      mcp_tools, _ = await MCPToolset.from_server(\n                          connection_params=params,\n                          async_exit_stack=adk_exit_stack\n                      )\n                      for tool in mcp_tools: tool._is_mcp_tool = True\n                      processed_adk_tools.extend(mcp_tools)\n                      logger.info(f\"Fetched {len(mcp_tools)} tools via ADK MCPToolset ({mcp_conf.get('type')}).\")\n                  else:\n                       logger.warning(f\"Unsupported MCP config type: {mcp_conf.get('type')}\")\n\n             except Exception as e:\n                  logger.error(f\"Failed to fetch tools from MCP server {mcp_conf}: {e}\", exc_info=True)\n                  # Decide whether to raise or continue\n\n        # --- ADK Planner, Examples, Output Schema ---\n\n\n\n    # 6. Instantiate EnhancedAgent\n    try:\n        # Base arguments for EnhancedAgent\n        agent_init_kwargs = {\n            'amd': amd,\n            'world_model': world_model,\n            'format_model': self._config.formatter_llm_model if self._config.formatter_llm_model else None, # Example passing extra config\n            'verbose': self._config.verbose_logging,\n            'stream': self._config.enable_streaming,\n            'max_history_turns': self._config.history.max_turns,\n            'max_history_tokens': self._config.history.max_tokens,\n            'trim_strategy': self._config.history.trim_strategy,\n            'sync_adk_state': self._config.adk.sync_state if ADK_AVAILABLE else False,\n            'adk_exit_stack': adk_exit_stack, # Pass stack for cleanup\n            'user_cost_tracker': cost_tracker, # Pass the tracker instance\n            **self._callbacks_transient, # Pass configured callbacks\n            # Pass server instances if provided (less common)\n            'a2a_server': self._a2a_server_instance,\n            'mcp_server': self._mcp_server_instance,\n        }\n\n        # Add ADK-specific arguments if inheriting from LlmAgent\n        agent_class = EnhancedAgent\n        if ADK_AVAILABLE and issubclass(EnhancedAgent, ADKLlmAgent):\n             logger.debug(\"Adding ADK LlmAgent specific arguments to init.\")\n             adk_specific_kwargs = {\n                 'name': self._config.agent_name, # Required by LlmAgent\n                 'model': LiteLlm(model=self._config.model_identifier), # LlmAgent needs BaseLlm instance\n                 'description': self._config.adk.description or self._config.system_message,\n                 'instruction': self._config.system_message, # Or dedicated instruction field?\n                 'tools': processed_adk_tools,\n                 'code_executor': adk_code_executor, # Pass the *instance*\n                 'planner': adk_planner_instance,\n                 # Process examples/schema if needed\n                 'examples': [ADKExample(**ex) for ex in self._config.adk.examples] if self._config.adk.examples else None,\n                 'output_schema': self._config.adk.output_schema,\n                 # Pass runner/session service if NOT using InMemoryRunner deferred creation\n                 # If runner is created later, it's assigned post-init\n                 'runner': adk_runner_instance if adk_runner_instance else None, # Pass runner if created now\n                 'session_service': adk_session_service, # Pass session service\n             }\n             # Merge, ensuring agent_init_kwargs takes precedence for overlapping basic fields if necessary\n             # but allow ADK specifics to be added. Be careful with overlaps like 'name'.\n             # EnhancedAgent init should handle reconciling these if needed.\n             # A safer merge:\n             final_kwargs = agent_init_kwargs.copy()\n             for k, v in adk_specific_kwargs.items():\n                  if k not in final_kwargs: # Only add ADK specifics not already handled\n                      final_kwargs[k] = v\n                  # Handle specific overrides/merges needed for LlmAgent base\n                  elif k == 'tools' and v: # Merge tools\n                      final_kwargs['tools'] = (final_kwargs.get('tools') or []) + v\n                  # Overwrite description/instruction from ADK config if set\n                  elif k in ['description', 'instruction'] and v or k == 'code_executor' or k == 'model':\n                       final_kwargs[k] = v\n\n             agent_init_kwargs = final_kwargs\n\n\n        logger.info(f\"Final keys for EnhancedAgent init: {list(agent_init_kwargs.keys())}\")\n        logger.info(f\"Final keys for EnhancedAgent init: {agent_init_kwargs}\")\n\n        # --- Instantiate the Agent ---\n        agent = agent_class(**agent_init_kwargs)\n        # --- Agent Instantiated ---\n\n        # If ADK InMemoryRunner creation was deferred, create and assign now\n        if ADK_AVAILABLE and 'adk_runner_config_for_later' in locals():\n             cfg = locals()['adk_runner_config_for_later']\n             if not isinstance(cfg['runner_class'], InMemoryRunner) and cfg.get('session_service') is None: cfg['session_service'] = agent.adk_session_service # Ensure service is passed\n             agent.setup_adk_runner(cfg)\n             logger.info(f\"Created and assigned deferred ADK Runner instance: {agent.adk_runner.__class__.__name__}\")\n             # Ensure agent has runner's session service if it differs\n             if agent.adk_runner and agent.adk_session_service is not agent.adk_runner.session_service:\n                  logger.warning(\"Agent session service differs from deferred runner's service. Updating agent's reference.\")\n                  agent.adk_session_service = agent.adk_runner.session_service\n        elif ADK_AVAILABLE and adk_runner_instance and not agent.adk_runner:\n            # If runner was created earlier but not passed via LlmAgent init (e.g. non-LlmAgent base)\n            # Or if we want to explicitly assign it\n             agent.adk_runner = adk_runner_instance\n             # Ensure session service consistency\n             if agent.adk_session_service is not agent.adk_runner.session_service:\n                  agent.adk_session_service = agent.adk_runner.session_service\n\n\n    except ValidationError as e:\n        logger.error(f\"Pydantic validation error Instantiating EnhancedAgent: {e}\", exc_info=True)\n        raise\n    except Exception as e:\n        logger.error(f\"Unexpected error Instantiating EnhancedAgent: {e}\", exc_info=True)\n        raise\n\n    # 7. Setup Agent's Internal Server Capabilities (if enabled and not pre-initialized)\n    if self._config.a2a.enabled and not agent.a2a_server:\n        if AGENT_A2A_AVAILABLE:\n            logger.info(\"Setting up A2A server on agent instance...\")\n            agent.setup_a2a_server(\n                host=self._config.a2a.host,\n                port=self._config.a2a.port,\n                **self._config.a2a.extra_options\n            )\n        else: logger.warning(\"A2A server configured in builder, but A2A not available in agent environment.\")\n\n    if self._config.mcp.enabled and not agent.mcp_server:\n        if AGENT_MCP_AVAILABLE:\n            logger.info(\"Setting up MCP server on agent instance...\")\n            agent.setup_mcp_server(\n                host=self._config.mcp.host,\n                port=self._config.mcp.port,\n                name=self._config.mcp.server_name, # Already defaulted\n                **self._config.mcp.extra_options\n            )\n        else: logger.warning(\"MCP server configured in builder, but MCP not available in agent environment.\")\n\n    # 8. Setup A2A known clients configuration on the agent\n    if self._config.a2a.known_clients:\n         if AGENT_A2A_AVAILABLE:\n             # The agent likely handles client creation on demand,\n             # but we can pass the config for it to use.\n             # Assuming agent has a way to receive this, e.g., during init or a setter\n             if hasattr(agent, 'set_known_a2a_clients'):\n                 agent.set_known_a2a_clients(self._config.a2a.known_clients)\n             else:\n                  # Fallback: store on a generic config dict? Less ideal.\n                  # agent.config.a2a_known_clients = self._config.a2a.known_clients\n                  logger.warning(\"Agent does not have 'set_known_a2a_clients' method. Known client config stored raw.\")\n         else:\n              logger.warning(\"A2A known clients configured, but A2A not available in agent env.\")\n\n\n    logger.info(f\"--- EnhancedAgent Build Complete: {agent.amd.name} ---\")\n    return agent\n</code></pre> <code>enable_adk(runner_class=InMemoryRunner, runner_options=None)</code> \u00b6 <p>Enables ADK integration with a specified runner.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def enable_adk(self, runner_class: type[ADKRunner] = InMemoryRunner, runner_options: dict[str, Any] | None = None) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Enables ADK integration with a specified runner.\"\"\"\n    if not self._ensure_adk(\"Runner\"): return self\n    self._config.adk.runner_class_name = runner_class.__name__\n    self._config.adk.runner_options = runner_options or {}\n    logger.info(f\"ADK integration enabled with runner: {self._config.adk.runner_class_name}\")\n    return self\n</code></pre> <code>load_config(path)</code> \u00b6 <p>Loads builder configuration from a JSON file, overwriting current settings.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def load_config(self, path: str | Path) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Loads builder configuration from a JSON file, overwriting current settings.\"\"\"\n    filepath = Path(path)\n    if not filepath.exists():\n        raise FileNotFoundError(f\"Builder configuration file not found: {filepath}\")\n    try:\n        with open(filepath) as f:\n            config_data = json.load(f)\n        self._config = BuilderConfig.model_validate(config_data)\n        logger.info(f\"Builder configuration loaded from {filepath}\")\n        # Reset transient fields, as they are not saved\n        self._reset_transient_fields()\n        logger.warning(\"Transient fields (callbacks, tool instances, tracker instance, etc.) reset. Re-add them if needed.\")\n        # Update logger level based on loaded config\n        logger.setLevel(logging.DEBUG if self._config.verbose_logging else logging.INFO)\n    except (OSError, json.JSONDecodeError) as e:\n        logger.error(f\"Failed to load or parse builder configuration from {filepath}: {e}\")\n        raise\n    except ValidationError as e:\n         logger.error(f\"Loaded configuration data is invalid: {e}\")\n         raise\n    return self\n</code></pre> <code>save_config(path, indent=2)</code> \u00b6 <p>Saves the current builder configuration to a JSON file.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def save_config(self, path: str | Path, indent: int = 2):\n    \"\"\"Saves the current builder configuration to a JSON file.\"\"\"\n    filepath = Path(path)\n    try:\n        filepath.parent.mkdir(parents=True, exist_ok=True)\n        config_json = self._config.model_dump_json(indent=indent)\n        with open(filepath, 'w') as f:\n            f.write(config_json)\n        logger.info(f\"Builder configuration saved to {filepath}\")\n    except OSError as e:\n        logger.error(f\"Failed to save builder configuration to {filepath}: {e}\")\n    except ValidationError as e:\n         logger.error(f\"Configuration is invalid, cannot save: {e}\")\n    except Exception as e:\n         logger.error(f\"An unexpected error occurred during config save: {e}\")\n</code></pre> <code>with_adk_code_executor(executor_type)</code> \u00b6 <p>Configures the type of ADK code executor to use (saved in config).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_adk_code_executor(self, executor_type: Literal[\"adk_builtin\", \"unsafe_simple\", \"secure_placeholder\", \"none\"]) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Configures the type of ADK code executor to use (saved in config).\"\"\"\n    if not self._ensure_adk(\"Code Executor Type\"): return self\n    if executor_type == \"unsafe_simple\":\n        logger.critical(\"***********************************************************\")\n        logger.critical(\"*** WARNING: Configuring UNSAFE SimplePythonExecutor!   ***\")\n        logger.critical(\"***********************************************************\")\n    elif executor_type == \"secure_placeholder\":\n        logger.warning(\"Configuring SecureCodeExecutorPlaceholder. Implement actual sandboxing!\")\n    elif executor_type == \"adk_builtin\":\n        if self._config.model_identifier and (\"gemini-1.5\" not in self._config.model_identifier and \"gemini-2\" not in self._config.model_identifier) :\n            logger.warning(f\"ADK built-in code execution selected, but model '{self._config.model_identifier}' might not support it. Ensure model compatibility.\")\n        logger.info(\"Configuring ADK built-in code execution (tool-based, requires compatible model).\")\n\n    self._config.adk.code_executor_config = executor_type\n    self._adk_code_executor_instance = None # Clear any previously set instance\n    return self\n</code></pre> <code>with_adk_code_executor_instance(executor)</code> \u00b6 <p>Provides a pre-initialized ADK code executor instance (transient).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_adk_code_executor_instance(self, executor: ADKBaseCodeExecutor) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Provides a pre-initialized ADK code executor instance (transient).\"\"\"\n    if not self._ensure_adk(\"Code Executor Instance\"): return self\n    if not isinstance(executor, ADKBaseCodeExecutor):\n        raise TypeError(f\"Expected ADKBaseCodeExecutor instance, got {type(executor)}\")\n    self._adk_code_executor_instance = executor\n    self._config.adk.code_executor_config = \"custom_instance\" # Mark config\n    logger.info(f\"Using custom ADK code executor instance: {type(executor).__name__}\")\n    return self\n</code></pre> <code>with_adk_mcp_toolset(connection_type, **kwargs)</code> \u00b6 <p>Configures an ADK MCP Toolset connection (saved in config).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_adk_mcp_toolset(self, connection_type: Literal[\"stdio\", \"sse\"], **kwargs) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Configures an ADK MCP Toolset connection (saved in config).\"\"\"\n    if not self._ensure_adk(\"MCP Toolset\"): return self\n    if connection_type == \"stdio\":\n        if \"command\" not in kwargs: raise ValueError(\"Stdio MCP toolset requires 'command' argument.\")\n        config = {\"type\": \"stdio\", \"command\": kwargs[\"command\"], \"args\": kwargs.get(\"args\", [])}\n    elif connection_type == \"sse\":\n        if \"url\" not in kwargs: raise ValueError(\"SSE MCP toolset requires 'url' argument.\")\n        config = {\"type\": \"sse\", \"url\": kwargs[\"url\"]}\n    else:\n        raise ValueError(f\"Unknown MCP toolset connection type: {connection_type}\")\n    self._config.adk.mcp_toolset_configs.append(config)\n    logger.info(f\"Configured ADK MCP Toolset: {config}\")\n    return self\n</code></pre> <code>with_adk_tool_function(func, name=None, description=None)</code> \u00b6 <p>Adds a callable function as an ADK tool (transient).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_adk_tool_function(self, func: Callable, name: Optional[str] = None,\n                           description: Optional[str] = None) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Adds a callable function as an ADK tool (transient).\"\"\"\n    if not self._ensure_adk(\"Tool Function\"):\n        return self\n    if not callable(func):\n        raise TypeError(f\"Expected callable function for ADK tool, got {type(func)}\")\n    if name:\n        func.__name__ = name\n    if description:\n        func.__doc__ = description\n    tool = FunctionTool(func)\n    self._adk_tools_transient.append(tool)\n    return self\n</code></pre> <code>with_adk_tool_instance(tool)</code> \u00b6 <p>Adds a pre-initialized ADK Tool instance (transient).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_adk_tool_instance(self, tool: ADKBaseTool) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Adds a pre-initialized ADK Tool instance (transient).\"\"\"\n    if not self._ensure_adk(\"Tool Instance\"): return self\n    if not isinstance(tool, ADKBaseTool):\n        raise TypeError(f\"Expected ADK BaseTool instance, got {type(tool)}\")\n    self._adk_tools_transient.append(tool)\n    return self\n</code></pre> <code>with_cost_tracker(tracker)</code> \u00b6 <p>Provides a pre-initialized UserCostTracker instance (transient).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_cost_tracker(self, tracker: UserCostTracker) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Provides a pre-initialized UserCostTracker instance (transient).\"\"\"\n    if not hasattr(tracker, \"get_all_costs\"): # Check protocol using isinstance\n         raise TypeError(\"Cost tracker must implement the UserCostTracker protocol.\")\n    self._user_cost_tracker_instance = tracker\n    # Clear file config if instance is provided\n    self._config.cost_tracker_config = {'type': 'custom_instance'}\n    logger.info(f\"Using custom UserCostTracker instance: {type(tracker).__name__}\")\n    return self\n</code></pre> <code>with_json_cost_tracker(filepath)</code> \u00b6 <p>Configures the builder to use the JsonFileUserCostTracker (saved in config).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_json_cost_tracker(self, filepath: str | Path) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Configures the builder to use the JsonFileUserCostTracker (saved in config).\"\"\"\n    self._config.cost_tracker_config = {'type': 'json', 'filepath': str(filepath)}\n    self._user_cost_tracker_instance = None # Clear any instance\n    logger.info(f\"Configured JsonFileUserCostTracker: {filepath}\")\n    return self\n</code></pre> <code>with_litellm_budget_manager(manager)</code> \u00b6 <p>Provides a pre-initialized LiteLLM BudgetManager instance (transient).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_litellm_budget_manager(self, manager: BudgetManager) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Provides a pre-initialized LiteLLM BudgetManager instance (transient).\"\"\"\n    if not LITELLM_AVAILABLE:\n         logger.warning(\"LiteLLM not available, cannot set BudgetManager.\")\n         return self\n    if not isinstance(manager, BudgetManager):\n        raise TypeError(\"Expected litellm.BudgetManager instance.\")\n    self._litellm_budget_manager_instance = manager\n    return self\n</code></pre> <code>with_telemetry_provider_instance(provider)</code> \u00b6 <p>Provides a pre-initialized OpenTelemetry TracerProvider instance (transient).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_telemetry_provider_instance(self, provider: TracerProvider) -&gt; 'EnhancedAgentBuilder':\n    \"\"\"Provides a pre-initialized OpenTelemetry TracerProvider instance (transient).\"\"\"\n    if not OTEL_AVAILABLE:\n        logger.warning(\"OpenTelemetry SDK not available. Cannot set TracerProvider.\")\n        return self\n    if not isinstance(provider, TracerProvider):\n         raise TypeError(\"Expected opentelemetry.sdk.trace.TracerProvider instance.\")\n    self._otel_trace_provider_instance = provider\n    # Mark telemetry as enabled, but using custom instance\n    self._config.telemetry_config = {'enabled': True, 'type': 'custom_instance'}\n    logger.info(\"Using custom OpenTelemetry TracerProvider instance.\")\n    return self\n</code></pre> <code>JsonFileUserCostTracker</code> \u00b6 <p>Stores user costs persistently in a JSON file.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>class JsonFileUserCostTracker:\n    \"\"\"Stores user costs persistently in a JSON file.\"\"\"\n    def __init__(self, filepath: str | Path):\n        self.filepath = Path(filepath)\n        self._costs: dict[str, float] = {}\n        self._lock = threading.Lock()\n        self.load() # Load costs on initialization\n\n    def get_cost(self, user_id: str) -&gt; float:\n        with self._lock:\n            return self._costs.get(user_id, 0.0)\n\n    def add_cost(self, user_id: str, cost: float) -&gt; None:\n        if not user_id:\n            logger.warning(\"Cost tracking skipped: user_id is missing.\")\n            return\n        if cost &gt; 0:\n            with self._lock:\n                self._costs[user_id] = self._costs.get(user_id, 0.0) + cost\n                logger.debug(f\"Cost added for user '{user_id}': +{cost:.6f}. New total: {self._costs[user_id]:.6f}\")\n            # Optional: Auto-save periodically or based on number of updates\n            # For simplicity, we rely on explicit save() or agent close\n\n    def get_all_costs(self) -&gt; dict[str, float]:\n        with self._lock:\n            return self._costs.copy()\n\n    def save(self) -&gt; None:\n        with self._lock:\n            try:\n                self.filepath.parent.mkdir(parents=True, exist_ok=True)\n                with open(self.filepath, 'w') as f:\n                    json.dump(self._costs, f, indent=2)\n                logger.info(f\"User costs saved to {self.filepath}\")\n            except OSError as e:\n                logger.error(f\"Failed to save user costs to {self.filepath}: {e}\")\n\n    def load(self) -&gt; None:\n        with self._lock:\n            if self.filepath.exists():\n                try:\n                    with open(self.filepath) as f:\n                        self._costs = json.load(f)\n                    logger.info(f\"User costs loaded from {self.filepath}\")\n                except (OSError, json.JSONDecodeError) as e:\n                    logger.error(f\"Failed to load user costs from {self.filepath}: {e}. Starting fresh.\")\n                    self._costs = {}\n            else:\n                logger.info(f\"User cost file not found ({self.filepath}). Starting fresh.\")\n                self._costs = {}\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.save()\n</code></pre> <code>UserCostTracker</code> \u00b6 <p>               Bases: <code>Protocol</code></p> <p>Protocol for tracking costs per user.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>class UserCostTracker(Protocol):\n    \"\"\"Protocol for tracking costs per user.\"\"\"\n    def get_cost(self, user_id: str) -&gt; float: ...\n    def add_cost(self, user_id: str, cost: float) -&gt; None: ...\n    def get_all_costs(self) -&gt; dict[str, float]: ...\n    def save(self) -&gt; None: ...\n    def load(self) -&gt; None: ...\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent.config","title":"<code>config</code>","text":"<code>A2AConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Configuration for A2A integration.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>class A2AConfig(BaseModel):\n    \"\"\"Configuration for A2A integration.\"\"\"\n    server: dict[str, Any] | None = Field(default=None, description=\"Configuration to run an A2A server (host, port, etc.).\")\n    known_agents: dict[str, str] = Field(default_factory=dict, description=\"Named A2A agent URLs to interact with (e.g., {'weather_agent': 'http://weather:5000'}).\")\n    default_task_timeout: int = Field(default=120, description=\"Default timeout in seconds for waiting on A2A task results.\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre> <code>ADKConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Configuration for ADK integration.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>class ADKConfig(BaseModel):\n    \"\"\"Configuration for ADK integration.\"\"\"\n    enabled: bool = Field(default=True, description=\"Enable ADK features if ADK is installed.\")\n    description: str | None = Field(default=None, description=\"ADK LlmAgent description.\")\n    instruction_override: str | None = Field(default=None, description=\"Override agent's system message for ADK.\")\n    # Tools added via builder or auto-discovery\n    code_executor: str | BaseCodeExecutor | None = Field(default=None, description=\"Reference name or instance of ADK code executor.\")\n    planner: str | BasePlanner | None = Field(default=None, description=\"Reference name or instance of ADK planner.\")\n    examples: list[Example] | None = Field(default=None, description=\"Few-shot examples for ADK.\")\n    output_schema: type[BaseModel] | None = Field(default=None, description=\"Pydantic model for structured output.\")\n    # MCP Toolset config handled separately if ADK is enabled\n    use_mcp_toolset: bool = Field(default=True, description=\"Use ADK's MCPToolset for MCP client connections if ADK is enabled.\")\n    # Runner config handled separately\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre> <code>AgentConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Main configuration schema for an EnhancedAgent.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>class AgentConfig(BaseModel):\n    \"\"\"Main configuration schema for an EnhancedAgent.\"\"\"\n    agent_name: str = Field(..., description=\"Unique name for this agent instance.\")\n    version: str = Field(default=\"0.1.0\")\n\n    agent_instruction: str = Field(default=\"You are a helpful AI assistant. Answer user questions to the best of your knowledge. Respond concisely. use tools when needed\")\n    agent_description: str = Field(default=\"An configurable, production-ready agent with integrated capabilities.\")\n\n    # Model Selection\n    models: list[ModelConfig] = Field(..., description=\"List of available LLM configurations.\")\n    default_llm_model: str = Field(..., description=\"Name of the ModelConfig to use for general LLM calls.\")\n    formatter_llm_model: str | None = Field(default=None, description=\"Optional: Name of a faster/cheaper ModelConfig for a_format_class calls.\")\n\n    # Core Agent Settings\n    world_model_initial_data: dict[str, Any] | None = Field(default=None)\n    enable_streaming: bool = Field(default=False)\n    verbose: bool = Field(default=False)\n    log_level: str = Field(default=\"INFO\", description=\"Logging level (DEBUG, INFO, WARNING, ERROR).\")\n    max_history_length: int = Field(default=20, description=\"Max conversation turns for LiteLLM history.\")\n    trim_strategy: Literal[\"litellm\", \"basic\"] = Field(default=\"litellm\")\n    persist_history: bool = Field(default=True, description=\"Persist conversation history (requires persistent ChatSession).\")\n    user_id_default: str | None = Field(default=None, description=\"Default user ID for interactions.\")\n\n    # Secure Code Execution\n    code_executor_type: Literal[\"restricted\", \"docker\", \"none\"] = Field(default=\"restricted\", description=\"Type of code executor to use.\")\n    code_executor_config: dict[str, Any] = Field(default_factory=dict, description=\"Configuration specific to the chosen code executor.\")\n    enable_adk_code_execution_tool: bool = Field(default=True, description=\"Expose code execution as an ADK tool if ADK is enabled.\")\n\n    # Framework Integrations\n    adk: ADKConfig | None = Field(default_factory=ADKConfig if ADK_AVAILABLE_CONF else lambda: None)\n    mcp: MCPConfig | None = Field(default_factory=MCPConfig if MCP_AVAILABLE_CONF else lambda: None)\n    a2a: A2AConfig | None = Field(default_factory=A2AConfig if A2A_AVAILABLE_CONF else lambda: None)\n\n    # Observability &amp; Cost\n    observability: ObservabilityConfig | None = Field(default_factory=ObservabilityConfig)\n    budget_manager: BudgetManager | None = Field(default=None, description=\"Global LiteLLM budget manager instance.\") # Needs to be passed in\n\n    # Human-in-the-Loop\n    enable_hitl: bool = Field(default=False, description=\"Enable basic Human-in-the-Loop hooks.\")\n\n    # Add other global settings as needed\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode='after')\n    def validate_model_references(self) -&gt; 'AgentConfig':\n        model_names = {m.name for m in self.models}\n        if self.default_llm_model not in model_names:\n            raise ValueError(f\"default_llm_model '{self.default_llm_model}' not found in defined models.\")\n        if self.formatter_llm_model and self.formatter_llm_model not in model_names:\n            raise ValueError(f\"formatter_llm_model '{self.formatter_llm_model}' not found in defined models.\")\n        return self\n\n    @model_validator(mode='after')\n    def validate_framework_availability(self) -&gt; 'AgentConfig':\n        if self.adk and self.adk.enabled and not ADK_AVAILABLE_CONF:\n            logger.warning(\"ADK configuration provided but ADK library not installed. Disabling ADK features.\")\n            self.adk.enabled = False\n        if self.mcp and (self.mcp.server or self.mcp.client_connections) and not MCP_AVAILABLE_CONF:\n             logger.warning(\"MCP configuration provided but MCP library not installed. Disabling MCP features.\")\n             self.mcp = None # Or disable specific parts\n        if self.a2a and (self.a2a.server or self.a2a.known_agents) and not A2A_AVAILABLE_CONF:\n             logger.warning(\"A2A configuration provided but A2A library not installed. Disabling A2A features.\")\n             self.a2a = None # Or disable specific parts\n        return self\n\n    @classmethod\n    def load_from_yaml(cls, path: str | Path) -&gt; 'AgentConfig':\n        \"\"\"Loads configuration from a YAML file.\"\"\"\n        file_path = Path(path)\n        if not file_path.is_file():\n            raise FileNotFoundError(f\"Configuration file not found: {path}\")\n        with open(file_path) as f:\n            config_data = yaml.safe_load(f)\n        logger.info(f\"Loaded agent configuration from {path}\")\n        return cls(**config_data)\n\n    def save_to_yaml(self, path: str | Path):\n        \"\"\"Saves the current configuration to a YAML file.\"\"\"\n        file_path = Path(path)\n        file_path.parent.mkdir(parents=True, exist_ok=True)\n        with open(file_path, 'w') as f:\n            # Use Pydantic's model_dump for clean serialization\n            yaml.dump(self.model_dump(mode='python'), f, sort_keys=False)\n        logger.info(f\"Saved agent configuration to {path}\")\n</code></pre> <code>load_from_yaml(path)</code> <code>classmethod</code> \u00b6 <p>Loads configuration from a YAML file.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>@classmethod\ndef load_from_yaml(cls, path: str | Path) -&gt; 'AgentConfig':\n    \"\"\"Loads configuration from a YAML file.\"\"\"\n    file_path = Path(path)\n    if not file_path.is_file():\n        raise FileNotFoundError(f\"Configuration file not found: {path}\")\n    with open(file_path) as f:\n        config_data = yaml.safe_load(f)\n    logger.info(f\"Loaded agent configuration from {path}\")\n    return cls(**config_data)\n</code></pre> <code>save_to_yaml(path)</code> \u00b6 <p>Saves the current configuration to a YAML file.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>def save_to_yaml(self, path: str | Path):\n    \"\"\"Saves the current configuration to a YAML file.\"\"\"\n    file_path = Path(path)\n    file_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(file_path, 'w') as f:\n        # Use Pydantic's model_dump for clean serialization\n        yaml.dump(self.model_dump(mode='python'), f, sort_keys=False)\n    logger.info(f\"Saved agent configuration to {path}\")\n</code></pre> <code>MCPConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Configuration for MCP integration.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>class MCPConfig(BaseModel):\n    \"\"\"Configuration for MCP integration.\"\"\"\n    server: dict[str, Any] | None = Field(default=None, description=\"Configuration to run an MCP server (host, port, etc.).\")\n    client_connections: dict[str, str] = Field(default_factory=dict, description=\"Named MCP server URLs to connect to as a client (e.g., {'files': 'stdio:npx @mcp/server-filesystem /data'}).\")\n    # ADK's MCPToolset handles client connections if ADKConfig.use_mcp_toolset is True\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre> <code>ModelConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Configuration specific to an LLM model via LiteLLM.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>class ModelConfig(BaseModel):\n    \"\"\"Configuration specific to an LLM model via LiteLLM.\"\"\"\n    # Used as key for model selection\n    name: str = Field(..., description=\"Unique identifier/alias for this model configuration (e.g., 'fast_formatter', 'main_reasoner').\")\n    model: str = Field(..., description=\"LiteLLM model string (e.g., 'gemini/gemini-1.5-pro-latest', 'ollama/mistral').\")\n    provider: str | None = Field(default=None, description=\"LiteLLM provider override if needed.\")\n    api_key: str | None = Field(default=None, description=\"API Key (consider using environment variables).\")\n    api_base: str | None = Field(default=None, description=\"API Base URL (for local models, proxies).\")\n    api_version: str | None = Field(default=None, description=\"API Version (e.g., for Azure).\")\n\n    # Common LLM Parameters\n    temperature: float | None = Field(default=0.7)\n    top_p: float | None = Field(default=None)\n    top_k: int | None = Field(default=None)\n    max_tokens: int | None = Field(default=2048, description=\"Max tokens for generation.\")\n    max_input_tokens: int | None = Field(default=None, description=\"Max input context window (autodetected if None).\")\n    stop_sequence: list[str] | None = Field(default=None)\n    presence_penalty: float | None = Field(default=None)\n    frequency_penalty: float | None = Field(default=None)\n    system_message: str | None = Field(default=None, description=\"Default system message for this model.\")\n\n    # LiteLLM Specific\n    caching: bool = Field(default=True, description=\"Enable LiteLLM caching for this model.\")\n    # budget_manager: Optional[BudgetManager] = Field(default=None) # Budget manager applied globally or per-agent\n\n    model_config = ConfigDict(arbitrary_types_allowed=True, extra='allow') # Allow extra LiteLLM params\n</code></pre> <code>ObservabilityConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Configuration for observability (OpenTelemetry).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>class ObservabilityConfig(BaseModel):\n    \"\"\"Configuration for observability (OpenTelemetry).\"\"\"\n    enabled: bool = Field(default=True)\n    endpoint: str | None = Field(default=None, description=\"OTLP endpoint URL (e.g., http://jaeger:4317).\")\n    service_name: str | None = Field(default=None, description=\"Service name for traces/metrics (defaults to agent name).\")\n    # Add more OTel config options as needed (headers, certs, resource attributes)\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent.executors","title":"<code>executors</code>","text":"<code>DockerCodeExecutor</code> \u00b6 <p>               Bases: <code>_BaseExecutorClass</code></p> <p>Executes Python code in a sandboxed Docker container.</p> <p>Requires Docker to be installed and running, and the 'docker' Python SDK.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/executors.py</code> <pre><code>class DockerCodeExecutor(_BaseExecutorClass):\n    \"\"\"\n    Executes Python code in a sandboxed Docker container.\n\n    Requires Docker to be installed and running, and the 'docker' Python SDK.\n    \"\"\"\n    DEFAULT_DOCKER_IMAGE = \"python:3.10-slim\" # Use a minimal image\n    DEFAULT_TIMEOUT = 10 # Seconds\n    DEFAULT_MEM_LIMIT = \"128m\"\n    DEFAULT_CPUS = 0.5\n\n    def __init__(self,\n                 docker_image: str = DEFAULT_DOCKER_IMAGE,\n                 timeout: int = DEFAULT_TIMEOUT,\n                 mem_limit: str = DEFAULT_MEM_LIMIT,\n                 cpus: float = DEFAULT_CPUS,\n                 network_mode: str = \"none\", # Disable networking by default for security\n                 docker_client_config: dict | None = None):\n        if not DOCKER_AVAILABLE:\n            raise ImportError(\"Docker SDK not installed ('pip install docker'). Cannot use DockerCodeExecutor.\")\n\n        self.docker_image = docker_image\n        self.timeout = timeout\n        self.mem_limit = mem_limit\n        self.cpus = cpus\n        self.network_mode = network_mode\n        try:\n            self.client = docker.from_env(**(docker_client_config or {}))\n            self.client.ping() # Check connection\n            # Ensure image exists locally or pull it\n            try:\n                self.client.images.get(self.docker_image)\n                logger.info(f\"Docker image '{self.docker_image}' found locally.\")\n            except ImageNotFound:\n                logger.warning(f\"Docker image '{self.docker_image}' not found locally. Attempting to pull...\")\n                try:\n                    self.client.images.pull(self.docker_image)\n                    logger.info(f\"Successfully pulled Docker image '{self.docker_image}'.\")\n                except APIError as pull_err:\n                    raise RuntimeError(f\"Failed to pull Docker image '{self.docker_image}': {pull_err}\") from pull_err\n        except Exception as e:\n            raise RuntimeError(f\"Failed to connect to Docker daemon: {e}. Is Docker running?\") from e\n        logger.info(f\"DockerCodeExecutor initialized (Image: {docker_image}, Timeout: {timeout}s, Network: {network_mode})\")\n\n    def _execute(self, code: str) -&gt; dict[str, Any]:\n        \"\"\"Internal execution logic.\"\"\"\n        result = {\"stdout\": \"\", \"stderr\": \"\", \"error\": None, \"exit_code\": None}\n        container = None\n\n        try:\n            logger.debug(f\"Creating Docker container from image '{self.docker_image}'...\")\n            container = self.client.containers.run(\n                image=self.docker_image,\n                command=[\"python\", \"-c\", code],\n                detach=True,\n                mem_limit=self.mem_limit,\n                nano_cpus=int(self.cpus * 1e9),\n                network_mode=self.network_mode,\n                # Security considerations: Consider read-only filesystem, dropping capabilities\n                read_only=True,\n                # working_dir=\"/app\", # Define a working dir if needed\n                # volumes={...} # Mount volumes carefully if required\n            )\n            logger.debug(f\"Container '{container.short_id}' started.\")\n\n            # Wait for container completion with timeout\n            container_result = container.wait(timeout=self.timeout)\n            result[\"exit_code\"] = container_result.get(\"StatusCode\", None)\n\n            # Retrieve logs\n            result[\"stdout\"] = container.logs(stdout=True, stderr=False).decode('utf-8', errors='replace').strip()\n            result[\"stderr\"] = container.logs(stdout=False, stderr=True).decode('utf-8', errors='replace').strip()\n\n            logger.debug(f\"Container '{container.short_id}' finished with exit code {result['exit_code']}.\")\n            if result[\"exit_code\"] != 0:\n                 logger.warning(f\"Container stderr: {result['stderr'][:500]}...\") # Log stderr on failure\n\n        except ContainerError as e:\n            result[\"error\"] = f\"ContainerError: {e}\"\n            result[\"stderr\"] = e.stderr.decode('utf-8', errors='replace').strip() if e.stderr else str(e)\n            result[\"exit_code\"] = e.exit_status\n            logger.error(f\"Container '{container.short_id if container else 'N/A'}' failed: {result['error']}\\nStderr: {result['stderr']}\")\n        except APIError as e:\n            result[\"error\"] = f\"Docker APIError: {e}\"\n            result[\"exit_code\"] = -1\n            logger.error(f\"Docker API error during execution: {e}\")\n        except Exception as e:\n            # Catch potential timeout errors from container.wait or other unexpected issues\n            result[\"error\"] = f\"Unexpected execution error: {type(e).__name__}: {e}\"\n            result[\"exit_code\"] = -1\n            # Check if it looks like a timeout\n            if isinstance(e, TimeoutError) or \"Timeout\" in str(e): # docker SDK might raise requests.exceptions.ReadTimeout\n                result[\"stderr\"] = f\"Execution timed out after {self.timeout} seconds.\"\n                logger.warning(f\"Container execution timed out ({self.timeout}s).\")\n            else:\n                logger.error(f\"Unexpected error during Docker execution: {e}\", exc_info=True)\n        finally:\n            if container:\n                try:\n                    logger.debug(f\"Removing container '{container.short_id}'...\")\n                    container.remove(force=True)\n                except APIError as rm_err:\n                    logger.warning(f\"Failed to remove container {container.short_id}: {rm_err}\")\n\n        return result\n\n     # --- ADK Compatibility Method ---\n    if ADK_EXEC_AVAILABLE:\n        def execute_code(self, invocation_context: InvocationContext, code_input: CodeExecutionInput) -&gt; CodeExecutionResult:\n            logger.debug(f\"DockerCodeExecutor executing ADK request (lang: {code_input.language}). Code: {code_input.code[:100]}...\")\n            if code_input.language.lower() != 'python':\n                 return CodeExecutionResult(output=f\"Error: Unsupported language '{code_input.language}'. Only Python is supported.\", outcome=\"OUTCOME_FAILURE\")\n\n            exec_result = self._execute(code_input.code)\n\n            output_str = \"\"\n            if exec_result[\"stdout\"]:\n                output_str += f\"Stdout:\\n{exec_result['stdout']}\\n\"\n            if exec_result[\"stderr\"]:\n                 output_str += f\"Stderr:\\n{exec_result['stderr']}\\n\"\n            if not output_str and exec_result[\"exit_code\"] == 0:\n                 output_str = \"Execution successful with no output.\"\n            elif not output_str and exec_result[\"exit_code\"] != 0:\n                 output_str = f\"Execution failed with no output (Exit code: {exec_result['exit_code']}). Error: {exec_result['error']}\"\n\n            outcome = \"OUTCOME_SUCCESS\" if exec_result[\"exit_code\"] == 0 else \"OUTCOME_FAILURE\"\n\n            return CodeExecutionResult(output=output_str.strip(), outcome=outcome)\n    # --- End ADK Compatibility ---\n\n    # --- Direct Call Method ---\n    def execute(self, code: str) -&gt; dict[str, Any]:\n        \"\"\"Directly execute code, returning detailed dictionary.\"\"\"\n        logger.debug(f\"DockerCodeExecutor executing direct call. Code: {code[:100]}...\")\n        return self._execute(code)\n</code></pre> <code>execute(code)</code> \u00b6 <p>Directly execute code, returning detailed dictionary.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/executors.py</code> <pre><code>def execute(self, code: str) -&gt; dict[str, Any]:\n    \"\"\"Directly execute code, returning detailed dictionary.\"\"\"\n    logger.debug(f\"DockerCodeExecutor executing direct call. Code: {code[:100]}...\")\n    return self._execute(code)\n</code></pre> <code>RestrictedPythonExecutor</code> \u00b6 <p>               Bases: <code>_BaseExecutorClass</code></p> <p>Executes Python code using restrictedpython.</p> <p>Safer than exec() but NOT a full sandbox. Known vulnerabilities exist. Use with extreme caution and only with trusted code sources or for low-risk operations. Docker is strongly recommended for untrusted code.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/executors.py</code> <pre><code>class RestrictedPythonExecutor(_BaseExecutorClass):\n    \"\"\"\n    Executes Python code using restrictedpython.\n\n    Safer than exec() but NOT a full sandbox. Known vulnerabilities exist.\n    Use with extreme caution and only with trusted code sources or for\n    low-risk operations. Docker is strongly recommended for untrusted code.\n    \"\"\"\n    DEFAULT_ALLOWED_GLOBALS = {\n        **safe_globals,\n        '_print_': restrictedpython.PrintCollector,\n        '_getattr_': restrictedpython.safe_getattr,\n        '_getitem_': restrictedpython.safe_getitem,\n        '_write_': restrictedpython.guarded_setattr, # Allows modifying specific safe objects if needed\n        # Add other safe builtins or modules carefully\n        'math': __import__('math'),\n        'random': __import__('random'),\n        'datetime': __import__('datetime'),\n        'time': __import__('time'),\n        # 'requests': None, # Example: Explicitly disallow\n    }\n\n    def __init__(self, allowed_globals: dict | None = None, max_execution_time: int = 5):\n        if not RESTRICTEDPYTHON_AVAILABLE:\n            raise ImportError(\"restrictedpython is not installed. Cannot use RestrictedPythonExecutor.\")\n        self.allowed_globals = allowed_globals or self.DEFAULT_ALLOWED_GLOBALS\n        self.max_execution_time = max_execution_time # Basic timeout (not perfectly enforced by restrictedpython)\n        logger.warning(\"Initialized RestrictedPythonExecutor. This provides LIMITED sandboxing. Use Docker for untrusted code.\")\n\n    def _execute(self, code: str) -&gt; dict[str, Any]:\n        \"\"\"Internal execution logic.\"\"\"\n        start_time = time.monotonic()\n        result = {\"stdout\": \"\", \"stderr\": \"\", \"error\": None, \"exit_code\": None}\n        local_vars = {}\n        stdout_capture = io.StringIO()\n        stderr_capture = io.StringIO()\n\n        try:\n            # Basic timeout check (not preemptive)\n            if time.monotonic() - start_time &gt; self.max_execution_time:\n                 raise TimeoutError(f\"Execution exceeded max time of {self.max_execution_time}s (pre-check).\")\n\n            # Compile the code in restricted mode\n            byte_code = compile_restricted(code, filename='&lt;inline code&gt;', mode='exec')\n\n            # Add a print collector to capture output\n            self.allowed_globals['_print_'] = restrictedpython.PrintCollector\n            print_collector = self.allowed_globals['_print_']()\n            exec_globals = {**self.allowed_globals, '_print': print_collector}\n\n            # Execute the compiled code\n            # Note: restrictedpython does not inherently support robust timeouts during exec\n            exec(byte_code, exec_globals, local_vars)\n\n            # Check execution time again\n            duration = time.monotonic() - start_time\n            if duration &gt; self.max_execution_time:\n                logger.warning(f\"Execution finished but exceeded max time ({duration:.2f}s &gt; {self.max_execution_time}s).\")\n                # Potentially treat as an error or partial success\n\n            result[\"stdout\"] = print_collector.printed_text # Access collected prints\n            result[\"exit_code\"] = 0 # Assume success if no exception\n\n        except TimeoutError as e:\n            result[\"stderr\"] = f\"TimeoutError: {e}\"\n            result[\"error\"] = str(e)\n            result[\"exit_code\"] = -1 # Indicate timeout\n        except SyntaxError as e:\n            result[\"stderr\"] = f\"SyntaxError: {e}\"\n            result[\"error\"] = str(e)\n            result[\"exit_code\"] = 1\n        except Exception as e:\n            # Capture other potential execution errors allowed by restrictedpython\n            error_type = type(e).__name__\n            error_msg = f\"{error_type}: {e}\"\n            result[\"stderr\"] = error_msg\n            result[\"error\"] = str(e)\n            result[\"exit_code\"] = 1\n            logger.warning(f\"RestrictedPython execution caught exception: {error_msg}\", exc_info=False) # Avoid logging potentially sensitive details from code\n        finally:\n            stdout_capture.close() # Not used directly with PrintCollector\n            stderr_capture.close()\n\n        return result\n\n    # --- ADK Compatibility Method ---\n    if ADK_EXEC_AVAILABLE:\n        def execute_code(self, invocation_context: InvocationContext, code_input: CodeExecutionInput) -&gt; CodeExecutionResult:\n            logger.debug(f\"RestrictedPythonExecutor executing ADK request (lang: {code_input.language}). Code: {code_input.code[:100]}...\")\n            if code_input.language.lower() != 'python':\n                 return CodeExecutionResult(output=f\"Error: Unsupported language '{code_input.language}'. Only Python is supported.\", outcome=\"OUTCOME_FAILURE\")\n\n            exec_result = self._execute(code_input.code)\n\n            output_str = \"\"\n            if exec_result[\"stdout\"]:\n                output_str += f\"Stdout:\\n{exec_result['stdout']}\\n\"\n            if exec_result[\"stderr\"]:\n                 output_str += f\"Stderr:\\n{exec_result['stderr']}\\n\"\n            if not output_str and exec_result[\"exit_code\"] == 0:\n                 output_str = \"Execution successful with no output.\"\n            elif not output_str and exec_result[\"exit_code\"] != 0:\n                 output_str = f\"Execution failed with no output (Exit code: {exec_result['exit_code']}). Error: {exec_result['error']}\"\n\n\n            outcome = \"OUTCOME_SUCCESS\" if exec_result[\"exit_code\"] == 0 else \"OUTCOME_FAILURE\"\n\n            return CodeExecutionResult(output=output_str.strip(), outcome=outcome)\n    # --- End ADK Compatibility ---\n\n    # --- Direct Call Method ---\n    def execute(self, code: str) -&gt; dict[str, Any]:\n        \"\"\"Directly execute code, returning detailed dictionary.\"\"\"\n        logger.debug(f\"RestrictedPythonExecutor executing direct call. Code: {code[:100]}...\")\n        return self._execute(code)\n</code></pre> <code>execute(code)</code> \u00b6 <p>Directly execute code, returning detailed dictionary.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/executors.py</code> <pre><code>def execute(self, code: str) -&gt; dict[str, Any]:\n    \"\"\"Directly execute code, returning detailed dictionary.\"\"\"\n    logger.debug(f\"RestrictedPythonExecutor executing direct call. Code: {code[:100]}...\")\n    return self._execute(code)\n</code></pre> <code>get_code_executor(config)</code> \u00b6 <p>Creates a code executor instance based on configuration.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/executors.py</code> <pre><code>def get_code_executor(config: 'AgentConfig') -&gt; RestrictedPythonExecutor | DockerCodeExecutor | BaseCodeExecutor | None:\n    \"\"\"Creates a code executor instance based on configuration.\"\"\"\n    executor_type = config.code_executor_type\n    executor_config = config.code_executor_config or {}\n\n    if executor_type == \"restricted\":\n        if not RESTRICTEDPYTHON_AVAILABLE:\n            logger.error(\"RestrictedPython executor configured but library not installed. Code execution disabled.\")\n            return None\n        return RestrictedPythonExecutor(**executor_config)\n    elif executor_type == \"docker\":\n        if not DOCKER_AVAILABLE:\n            logger.error(\"Docker executor configured but library not installed or Docker not running. Code execution disabled.\")\n            return None\n        try:\n            return DockerCodeExecutor(**executor_config)\n        except Exception as e:\n            logger.error(f\"Failed to initialize DockerCodeExecutor: {e}. Code execution disabled.\")\n            return None\n    elif executor_type == \"none\":\n        logger.info(\"Code execution explicitly disabled in configuration.\")\n        return None\n    elif executor_type and ADK_EXEC_AVAILABLE and isinstance(executor_type, BaseCodeExecutor):\n        # Allow passing a pre-configured ADK executor instance\n        logger.info(f\"Using pre-configured ADK code executor: {type(executor_type).__name__}\")\n        return executor_type\n    else:\n        logger.warning(f\"Unknown or unsupported code_executor_type: '{executor_type}'. Code execution disabled.\")\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent.utils","title":"<code>utils</code>","text":"<code>LLMMessage</code> <code>dataclass</code> \u00b6 <p>Represents a message in a conversation with the LLM.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/utils.py</code> <pre><code>@dataclass\nclass LLMMessage:\n    \"\"\"Represents a message in a conversation with the LLM.\"\"\"\n    role: str  # \"user\", \"assistant\", \"system\", \"tool\"\n    # Content can be string or list (e.g., multimodal with text/image dicts)\n    # Conforms to LiteLLM/OpenAI structure\n    content: str | list[dict[str, Any]]\n    tool_call_id: str | None = None  # For tool responses\n    name: str | None = None  # For tool calls/responses (function name)\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert to dictionary, handling potential dataclass nuances.\"\"\"\n        d = {\"role\": self.role, \"content\": self.content}\n        if self.tool_call_id:\n            d[\"tool_call_id\"] = self.tool_call_id\n        if self.name:\n            d[\"name\"] = self.name\n        return d\n</code></pre> <code>to_dict()</code> \u00b6 <p>Convert to dictionary, handling potential dataclass nuances.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/utils.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert to dictionary, handling potential dataclass nuances.\"\"\"\n    d = {\"role\": self.role, \"content\": self.content}\n    if self.tool_call_id:\n        d[\"tool_call_id\"] = self.tool_call_id\n    if self.name:\n        d[\"name\"] = self.name\n    return d\n</code></pre> <code>WorldModel</code> <code>dataclass</code> \u00b6 <p>Thread-safe representation of the agent's persistent understanding of the world.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/utils.py</code> <pre><code>@dataclass\nclass WorldModel:\n    \"\"\"Thread-safe representation of the agent's persistent understanding of the world.\"\"\"\n    data: dict[str, Any] = dataclass_field(default_factory=dict)\n    _lock: threading.Lock = dataclass_field(default_factory=threading.Lock)\n\n    def get(self, key: str, default: Any = None) -&gt; Any:\n        with self._lock:\n            return self.data.get(key, default)\n\n    def set(self, key: str, value: Any):\n        with self._lock:\n            logger_wm.debug(f\"WorldModel SET: {key} = {value}\")\n            self.data[key] = value\n\n    def remove(self, key: str):\n        with self._lock:\n            if key in self.data:\n                logger_wm.debug(f\"WorldModel REMOVE: {key}\")\n                del self.data[key]\n\n    def show(self) -&gt; str:\n        with self._lock:\n            if not self.data:\n                return \"[empty]\"\n            try:\n                items = [f\"- {k}: {json.dumps(v, indent=None, ensure_ascii=False, default=str)}\"\n                         for k, v in self.data.items()]\n                return \"\\n\".join(items)\n            except Exception:\n                items = [f\"- {k}: {str(v)}\" for k, v in self.data.items()]\n                return \"\\n\".join(items)\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        with self._lock:\n            # Deep copy might be needed if values are mutable and modified externally\n            # For simplicity, shallow copy is used here.\n            return self.data.copy()\n\n    def update_from_dict(self, data_dict: dict[str, Any]):\n        with self._lock:\n            self.data.update(data_dict)\n            logger_wm.debug(f\"WorldModel updated from dict: {list(data_dict.keys())}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils","title":"<code>AgentUtils</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.AISemanticMemory","title":"<code>AISemanticMemory</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>class AISemanticMemory(metaclass=Singleton):\n    def __init__(self,\n                 base_path: str = \"/semantic_memory\",\n                 default_model: str = os.getenv(\"DEFAULTMODELSUMMERY\"),\n                 default_embedding_model: str = os.getenv(\"DEFAULTMODELEMBEDDING\"),\n                 default_similarity_threshold: float = 0.61,\n                 default_batch_size: int = 64,\n                 default_n_clusters: int = 2,\n                 default_deduplication_threshold: float = 0.85):\n        \"\"\"\n        Initialize AISemanticMemory with KnowledgeBase integration\n\n        Args:\n            base_path: Root directory for memory storage\n            default_model: Default model for text generation\n            default_embedding_model: Default embedding model\n            default_similarity_threshold: Default similarity threshold for retrieval\n            default_batch_size: Default batch size for processing\n            default_n_clusters: Default number of clusters for FAISS\n            default_deduplication_threshold: Default threshold for deduplication\n        \"\"\"\n        self.base_path = os.path.join(os.getcwd(), \".data\", base_path)\n        self.memories: dict[str, KnowledgeBase] = {}\n\n        # Map of embedding models to their dimensions\n        self.embedding_dims = {\n            \"text-embedding-3-small\": 1536,\n            \"text-embedding-3-large\": 3072,\n            \"nomic-embed-text\": 768,\n            \"default\": 768\n        }\n\n        self.default_config = {\n            \"embedding_model\": default_embedding_model,\n            \"embedding_dim\": self._get_embedding_dim(default_embedding_model),\n            \"similarity_threshold\": default_similarity_threshold,\n            \"batch_size\": default_batch_size,\n            \"n_clusters\": default_n_clusters,\n            \"deduplication_threshold\": default_deduplication_threshold,\n            \"model_name\": default_model\n        }\n\n    def _get_embedding_dim(self, model_name: str) -&gt; int:\n        \"\"\"Get embedding dimension for a model\"\"\"\n        return self.embedding_dims.get(model_name, 768)\n\n    @staticmethod\n    def _sanitize_name(name: str) -&gt; str:\n        \"\"\"Sanitize memory name for filesystem safety\"\"\"\n        name = re.sub(r'[^a-zA-Z0-9_-]', '-', name)[:63].strip('-')\n        if not name:\n            raise ValueError(\"Invalid memory name\")\n        if len(name) &lt; 3:\n            name += \"Z\" * (3 - len(name))\n        return name\n\n    def create_memory(self,\n                      name: str,\n                      model_config: dict | None = None,\n                      storage_config: dict | None = None) -&gt; KnowledgeBase:\n        \"\"\"\n        Create new memory store with KnowledgeBase\n\n        Args:\n            name: Unique name for the memory store\n            model_config: Configuration for embedding model\n            storage_config: Configuration for KnowledgeBase parameters\n        \"\"\"\n        sanitized = self._sanitize_name(name)\n        if sanitized in self.memories:\n            raise ValueError(f\"Memory '{name}' already exists\")\n\n        # Determine embedding model and dimension\n        embedding_model = self.default_config[\"embedding_model\"]\n        model_name = self.default_config[\"model_name\"]\n        if model_config:\n            embedding_model = model_config.get(\"embedding_model\", embedding_model)\n            model_name = model_config.get(\"model_name\", model_name)\n        embedding_dim = self._get_embedding_dim(embedding_model)\n\n        # Get KnowledgeBase parameters\n        kb_params = {\n            \"embedding_dim\": embedding_dim,\n            \"embedding_model\": embedding_model,\n            \"similarity_threshold\": self.default_config[\"similarity_threshold\"],\n            \"batch_size\": self.default_config[\"batch_size\"],\n            \"n_clusters\": self.default_config[\"n_clusters\"],\n            \"deduplication_threshold\": self.default_config[\"deduplication_threshold\"],\n            \"model_name\": model_name,\n        }\n\n        if storage_config:\n            kb_params.update({\n                \"similarity_threshold\": storage_config.get(\"similarity_threshold\", kb_params[\"similarity_threshold\"]),\n                \"batch_size\": storage_config.get(\"batch_size\", kb_params[\"batch_size\"]),\n                \"n_clusters\": storage_config.get(\"n_clusters\", kb_params[\"n_clusters\"]),\n                \"model_name\": storage_config.get(\"model_name\", kb_params[\"model_name\"]),\n                \"embedding_model\": storage_config.get(\"embedding_model\", kb_params[\"embedding_model\"]),\n                \"deduplication_threshold\": storage_config.get(\"deduplication_threshold\",\n                                                              kb_params[\"deduplication_threshold\"]),\n            })\n\n        # Create KnowledgeBase instance\n        self.memories[sanitized] = KnowledgeBase(**kb_params)\n        return self.memories[sanitized]\n\n    async def add_data(self,\n                       memory_name: str,\n                       data: str | list[str] | bytes | dict,\n                       metadata: dict | None = None) -&gt; bool:\n        \"\"\"\n        Add data to memory store\n\n        Args:\n            memory_name: Target memory store\n            data: Text, list of texts, binary file, or structured data\n            metadata: Optional metadata\n        \"\"\"\n        name = self._sanitize_name(memory_name)\n        kb = self.memories.get(name)\n        if not kb:\n            kb = self.create_memory(name)\n\n        # Process input data\n        texts = []\n        if isinstance(data, bytes):\n            try:\n                import textract\n                text = textract.process(data).decode('utf-8')\n                texts = [text.replace('\\\\t', '').replace('\\t', '')]\n            except Exception as e:\n                raise ValueError(f\"File processing failed: {str(e)}\")\n        elif isinstance(data, str):\n            texts = [data.replace('\\\\t', '').replace('\\t', '')]\n        elif isinstance(data, list):\n            texts = [d.replace('\\\\t', '').replace('\\t', '') for d in data]\n        elif isinstance(data, dict):\n            # Custom KG not supported in current KnowledgeBase\n            raise NotImplementedError(\"Custom knowledge graph insertion not supported\")\n        else:\n            raise ValueError(\"Unsupported data type\")\n\n        # Add data to KnowledgeBase\n        try:\n            added, duplicates = await kb.add_data(texts, metadata)\n            return added &gt; 0\n        except Exception as e:\n            import traceback\n            print(traceback.format_exc())\n            raise RuntimeError(f\"Data addition failed: {str(e)}\")\n\n    def get(self, names):\n        return [m for n,m in self._get_target_memories(names)]\n\n    async def query(self,\n                    query: str,\n                    memory_names: str | list[str] | None = None,\n                    query_params: dict | None = None,\n                    to_str: bool = False,\n                    unified_retrieve: bool =False) -&gt; str | list[dict]:\n        \"\"\"\n        Query memories using KnowledgeBase retrieval\n\n        Args:\n            query: Search query\n            memory_names: Target memory names\n            query_params: Query parameters\n            to_str: Return string format\n            unified_retrieve: Unified retrieve\n        \"\"\"\n        targets = self._get_target_memories(memory_names)\n        if not targets:\n            return []\n\n        results = []\n        for name, kb in targets:\n            #try:\n                # Use KnowledgeBase's retrieve_with_overview for comprehensive results\n                result = await kb.retrieve_with_overview(\n                    query=query,\n                    k=query_params.get(\"k\", 3) if query_params else 3,\n                    min_similarity=query_params.get(\"min_similarity\", 0.2) if query_params else 0.2,\n                    cross_ref_depth=query_params.get(\"cross_ref_depth\", 2) if query_params else 2,\n                    max_cross_refs=query_params.get(\"max_cross_refs\", 2) if query_params else 2,\n                    max_sentences=query_params.get(\"max_sentences\", 5) if query_params else 5\n                ) if not unified_retrieve else await kb.unified_retrieve(\n                    query=query,\n                    k=query_params.get(\"k\", 2) if query_params else 2,\n                    min_similarity=query_params.get(\"min_similarity\", 0.2) if query_params else 0.2,\n                    cross_ref_depth=query_params.get(\"cross_ref_depth\", 2) if query_params else 2,\n                    max_cross_refs=query_params.get(\"max_cross_refs\", 6) if query_params else 6,\n                    max_sentences=query_params.get(\"max_sentences\", 12) if query_params else 12\n                )\n                results.append({\n                    \"memory\": name,\n                    \"result\": result\n                })\n            #except Exception as e:\n            #    print(f\"Query failed on {name}: {str(e)}\")\n        if to_str:\n            if not unified_retrieve:\n                str_res = [\n                    f\"{x['memory']} - {json.dumps(x['result'].overview)}\\n - {[c.text for c in x['result'].details]}\\n - {[(k, [c.text for c in v]) for k, v in x['result'].cross_references.items()]}\"\n                    for x in results]\n                # str_res =\n            else:\n                str_res = json.dumps(results)\n            return str_res\n        return results\n\n    def _get_target_memories(self, memory_names: str | list[str] | None) -&gt; list[tuple[str, KnowledgeBase]]:\n        \"\"\"Get target memories for query\"\"\"\n        if not memory_names:\n            return list(self.memories.items())\n\n        names = [memory_names] if isinstance(memory_names, str) else memory_names\n\n        targets = []\n        for name in names:\n            sanitized = self._sanitize_name(name)\n            if kb := self.memories.get(sanitized):\n                targets.append((sanitized, kb))\n        return targets\n\n    def list_memories(self) -&gt; list[str]:\n        \"\"\"List all available memories\"\"\"\n        return list(self.memories.keys())\n\n    async def delete_memory(self, name: str) -&gt; bool:\n        \"\"\"Delete a memory store\"\"\"\n        sanitized = self._sanitize_name(name)\n        if sanitized in self.memories:\n            del self.memories[sanitized]\n            return True\n        return False\n\n    def save_memory(self, name: str, path: str) -&gt; bool | bytes:\n        \"\"\"Save a memory store to disk\"\"\"\n        sanitized = self._sanitize_name(name)\n        if kb := self.memories.get(sanitized):\n            try:\n                return kb.save(path)\n            except Exception as e:\n                print(f\"Error saving memory: {str(e)}\")\n                return False\n        return False\n\n    def save_all_memories(self, path: str) -&gt; bool:\n        \"\"\"Save all memory stores to disk\"\"\"\n        for name, kb in self.memories.items():\n            try:\n                kb.save(os.path.join(path, f\"{name}.pkl\"))\n            except Exception as e:\n                print(f\"Error saving memory: {str(e)}\")\n                return False\n        return True\n\n    def load_all_memories(self, path: str) -&gt; bool:\n        \"\"\"Load all memory stores from disk\"\"\"\n        for file in os.listdir(path):\n            if file.endswith(\".pkl\"):\n                try:\n                    self.memories[file[:-4]] = KnowledgeBase.load(os.path.join(path, file))\n                except Exception as e:\n                    print(f\"Error loading memory: {str(e)}\")\n                    return False\n        return True\n\n    def load_memory(self, name: str, path: str | bytes) -&gt; bool:\n        \"\"\"Load a memory store from disk\"\"\"\n        sanitized = self._sanitize_name(name)\n        if sanitized in self.memories:\n            return False\n        try:\n            self.memories[sanitized] = KnowledgeBase.load(path)\n            return True\n        except Exception:\n            # print(f\"Error loading memory: {str(e)}\")\n            return False\n</code></pre> <code>__init__(base_path='/semantic_memory', default_model=os.getenv('DEFAULTMODELSUMMERY'), default_embedding_model=os.getenv('DEFAULTMODELEMBEDDING'), default_similarity_threshold=0.61, default_batch_size=64, default_n_clusters=2, default_deduplication_threshold=0.85)</code> \u00b6 <p>Initialize AISemanticMemory with KnowledgeBase integration</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str</code> <p>Root directory for memory storage</p> <code>'/semantic_memory'</code> <code>default_model</code> <code>str</code> <p>Default model for text generation</p> <code>getenv('DEFAULTMODELSUMMERY')</code> <code>default_embedding_model</code> <code>str</code> <p>Default embedding model</p> <code>getenv('DEFAULTMODELEMBEDDING')</code> <code>default_similarity_threshold</code> <code>float</code> <p>Default similarity threshold for retrieval</p> <code>0.61</code> <code>default_batch_size</code> <code>int</code> <p>Default batch size for processing</p> <code>64</code> <code>default_n_clusters</code> <code>int</code> <p>Default number of clusters for FAISS</p> <code>2</code> <code>default_deduplication_threshold</code> <code>float</code> <p>Default threshold for deduplication</p> <code>0.85</code> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def __init__(self,\n             base_path: str = \"/semantic_memory\",\n             default_model: str = os.getenv(\"DEFAULTMODELSUMMERY\"),\n             default_embedding_model: str = os.getenv(\"DEFAULTMODELEMBEDDING\"),\n             default_similarity_threshold: float = 0.61,\n             default_batch_size: int = 64,\n             default_n_clusters: int = 2,\n             default_deduplication_threshold: float = 0.85):\n    \"\"\"\n    Initialize AISemanticMemory with KnowledgeBase integration\n\n    Args:\n        base_path: Root directory for memory storage\n        default_model: Default model for text generation\n        default_embedding_model: Default embedding model\n        default_similarity_threshold: Default similarity threshold for retrieval\n        default_batch_size: Default batch size for processing\n        default_n_clusters: Default number of clusters for FAISS\n        default_deduplication_threshold: Default threshold for deduplication\n    \"\"\"\n    self.base_path = os.path.join(os.getcwd(), \".data\", base_path)\n    self.memories: dict[str, KnowledgeBase] = {}\n\n    # Map of embedding models to their dimensions\n    self.embedding_dims = {\n        \"text-embedding-3-small\": 1536,\n        \"text-embedding-3-large\": 3072,\n        \"nomic-embed-text\": 768,\n        \"default\": 768\n    }\n\n    self.default_config = {\n        \"embedding_model\": default_embedding_model,\n        \"embedding_dim\": self._get_embedding_dim(default_embedding_model),\n        \"similarity_threshold\": default_similarity_threshold,\n        \"batch_size\": default_batch_size,\n        \"n_clusters\": default_n_clusters,\n        \"deduplication_threshold\": default_deduplication_threshold,\n        \"model_name\": default_model\n    }\n</code></pre> <code>add_data(memory_name, data, metadata=None)</code> <code>async</code> \u00b6 <p>Add data to memory store</p> <p>Parameters:</p> Name Type Description Default <code>memory_name</code> <code>str</code> <p>Target memory store</p> required <code>data</code> <code>str | list[str] | bytes | dict</code> <p>Text, list of texts, binary file, or structured data</p> required <code>metadata</code> <code>dict | None</code> <p>Optional metadata</p> <code>None</code> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>async def add_data(self,\n                   memory_name: str,\n                   data: str | list[str] | bytes | dict,\n                   metadata: dict | None = None) -&gt; bool:\n    \"\"\"\n    Add data to memory store\n\n    Args:\n        memory_name: Target memory store\n        data: Text, list of texts, binary file, or structured data\n        metadata: Optional metadata\n    \"\"\"\n    name = self._sanitize_name(memory_name)\n    kb = self.memories.get(name)\n    if not kb:\n        kb = self.create_memory(name)\n\n    # Process input data\n    texts = []\n    if isinstance(data, bytes):\n        try:\n            import textract\n            text = textract.process(data).decode('utf-8')\n            texts = [text.replace('\\\\t', '').replace('\\t', '')]\n        except Exception as e:\n            raise ValueError(f\"File processing failed: {str(e)}\")\n    elif isinstance(data, str):\n        texts = [data.replace('\\\\t', '').replace('\\t', '')]\n    elif isinstance(data, list):\n        texts = [d.replace('\\\\t', '').replace('\\t', '') for d in data]\n    elif isinstance(data, dict):\n        # Custom KG not supported in current KnowledgeBase\n        raise NotImplementedError(\"Custom knowledge graph insertion not supported\")\n    else:\n        raise ValueError(\"Unsupported data type\")\n\n    # Add data to KnowledgeBase\n    try:\n        added, duplicates = await kb.add_data(texts, metadata)\n        return added &gt; 0\n    except Exception as e:\n        import traceback\n        print(traceback.format_exc())\n        raise RuntimeError(f\"Data addition failed: {str(e)}\")\n</code></pre> <code>create_memory(name, model_config=None, storage_config=None)</code> \u00b6 <p>Create new memory store with KnowledgeBase</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique name for the memory store</p> required <code>model_config</code> <code>dict | None</code> <p>Configuration for embedding model</p> <code>None</code> <code>storage_config</code> <code>dict | None</code> <p>Configuration for KnowledgeBase parameters</p> <code>None</code> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def create_memory(self,\n                  name: str,\n                  model_config: dict | None = None,\n                  storage_config: dict | None = None) -&gt; KnowledgeBase:\n    \"\"\"\n    Create new memory store with KnowledgeBase\n\n    Args:\n        name: Unique name for the memory store\n        model_config: Configuration for embedding model\n        storage_config: Configuration for KnowledgeBase parameters\n    \"\"\"\n    sanitized = self._sanitize_name(name)\n    if sanitized in self.memories:\n        raise ValueError(f\"Memory '{name}' already exists\")\n\n    # Determine embedding model and dimension\n    embedding_model = self.default_config[\"embedding_model\"]\n    model_name = self.default_config[\"model_name\"]\n    if model_config:\n        embedding_model = model_config.get(\"embedding_model\", embedding_model)\n        model_name = model_config.get(\"model_name\", model_name)\n    embedding_dim = self._get_embedding_dim(embedding_model)\n\n    # Get KnowledgeBase parameters\n    kb_params = {\n        \"embedding_dim\": embedding_dim,\n        \"embedding_model\": embedding_model,\n        \"similarity_threshold\": self.default_config[\"similarity_threshold\"],\n        \"batch_size\": self.default_config[\"batch_size\"],\n        \"n_clusters\": self.default_config[\"n_clusters\"],\n        \"deduplication_threshold\": self.default_config[\"deduplication_threshold\"],\n        \"model_name\": model_name,\n    }\n\n    if storage_config:\n        kb_params.update({\n            \"similarity_threshold\": storage_config.get(\"similarity_threshold\", kb_params[\"similarity_threshold\"]),\n            \"batch_size\": storage_config.get(\"batch_size\", kb_params[\"batch_size\"]),\n            \"n_clusters\": storage_config.get(\"n_clusters\", kb_params[\"n_clusters\"]),\n            \"model_name\": storage_config.get(\"model_name\", kb_params[\"model_name\"]),\n            \"embedding_model\": storage_config.get(\"embedding_model\", kb_params[\"embedding_model\"]),\n            \"deduplication_threshold\": storage_config.get(\"deduplication_threshold\",\n                                                          kb_params[\"deduplication_threshold\"]),\n        })\n\n    # Create KnowledgeBase instance\n    self.memories[sanitized] = KnowledgeBase(**kb_params)\n    return self.memories[sanitized]\n</code></pre> <code>delete_memory(name)</code> <code>async</code> \u00b6 <p>Delete a memory store</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>async def delete_memory(self, name: str) -&gt; bool:\n    \"\"\"Delete a memory store\"\"\"\n    sanitized = self._sanitize_name(name)\n    if sanitized in self.memories:\n        del self.memories[sanitized]\n        return True\n    return False\n</code></pre> <code>list_memories()</code> \u00b6 <p>List all available memories</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def list_memories(self) -&gt; list[str]:\n    \"\"\"List all available memories\"\"\"\n    return list(self.memories.keys())\n</code></pre> <code>load_all_memories(path)</code> \u00b6 <p>Load all memory stores from disk</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def load_all_memories(self, path: str) -&gt; bool:\n    \"\"\"Load all memory stores from disk\"\"\"\n    for file in os.listdir(path):\n        if file.endswith(\".pkl\"):\n            try:\n                self.memories[file[:-4]] = KnowledgeBase.load(os.path.join(path, file))\n            except Exception as e:\n                print(f\"Error loading memory: {str(e)}\")\n                return False\n    return True\n</code></pre> <code>load_memory(name, path)</code> \u00b6 <p>Load a memory store from disk</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def load_memory(self, name: str, path: str | bytes) -&gt; bool:\n    \"\"\"Load a memory store from disk\"\"\"\n    sanitized = self._sanitize_name(name)\n    if sanitized in self.memories:\n        return False\n    try:\n        self.memories[sanitized] = KnowledgeBase.load(path)\n        return True\n    except Exception:\n        # print(f\"Error loading memory: {str(e)}\")\n        return False\n</code></pre> <code>query(query, memory_names=None, query_params=None, to_str=False, unified_retrieve=False)</code> <code>async</code> \u00b6 <p>Query memories using KnowledgeBase retrieval</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query</p> required <code>memory_names</code> <code>str | list[str] | None</code> <p>Target memory names</p> <code>None</code> <code>query_params</code> <code>dict | None</code> <p>Query parameters</p> <code>None</code> <code>to_str</code> <code>bool</code> <p>Return string format</p> <code>False</code> <code>unified_retrieve</code> <code>bool</code> <p>Unified retrieve</p> <code>False</code> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>async def query(self,\n                query: str,\n                memory_names: str | list[str] | None = None,\n                query_params: dict | None = None,\n                to_str: bool = False,\n                unified_retrieve: bool =False) -&gt; str | list[dict]:\n    \"\"\"\n    Query memories using KnowledgeBase retrieval\n\n    Args:\n        query: Search query\n        memory_names: Target memory names\n        query_params: Query parameters\n        to_str: Return string format\n        unified_retrieve: Unified retrieve\n    \"\"\"\n    targets = self._get_target_memories(memory_names)\n    if not targets:\n        return []\n\n    results = []\n    for name, kb in targets:\n        #try:\n            # Use KnowledgeBase's retrieve_with_overview for comprehensive results\n            result = await kb.retrieve_with_overview(\n                query=query,\n                k=query_params.get(\"k\", 3) if query_params else 3,\n                min_similarity=query_params.get(\"min_similarity\", 0.2) if query_params else 0.2,\n                cross_ref_depth=query_params.get(\"cross_ref_depth\", 2) if query_params else 2,\n                max_cross_refs=query_params.get(\"max_cross_refs\", 2) if query_params else 2,\n                max_sentences=query_params.get(\"max_sentences\", 5) if query_params else 5\n            ) if not unified_retrieve else await kb.unified_retrieve(\n                query=query,\n                k=query_params.get(\"k\", 2) if query_params else 2,\n                min_similarity=query_params.get(\"min_similarity\", 0.2) if query_params else 0.2,\n                cross_ref_depth=query_params.get(\"cross_ref_depth\", 2) if query_params else 2,\n                max_cross_refs=query_params.get(\"max_cross_refs\", 6) if query_params else 6,\n                max_sentences=query_params.get(\"max_sentences\", 12) if query_params else 12\n            )\n            results.append({\n                \"memory\": name,\n                \"result\": result\n            })\n        #except Exception as e:\n        #    print(f\"Query failed on {name}: {str(e)}\")\n    if to_str:\n        if not unified_retrieve:\n            str_res = [\n                f\"{x['memory']} - {json.dumps(x['result'].overview)}\\n - {[c.text for c in x['result'].details]}\\n - {[(k, [c.text for c in v]) for k, v in x['result'].cross_references.items()]}\"\n                for x in results]\n            # str_res =\n        else:\n            str_res = json.dumps(results)\n        return str_res\n    return results\n</code></pre> <code>save_all_memories(path)</code> \u00b6 <p>Save all memory stores to disk</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def save_all_memories(self, path: str) -&gt; bool:\n    \"\"\"Save all memory stores to disk\"\"\"\n    for name, kb in self.memories.items():\n        try:\n            kb.save(os.path.join(path, f\"{name}.pkl\"))\n        except Exception as e:\n            print(f\"Error saving memory: {str(e)}\")\n            return False\n    return True\n</code></pre> <code>save_memory(name, path)</code> \u00b6 <p>Save a memory store to disk</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def save_memory(self, name: str, path: str) -&gt; bool | bytes:\n    \"\"\"Save a memory store to disk\"\"\"\n    sanitized = self._sanitize_name(name)\n    if kb := self.memories.get(sanitized):\n        try:\n            return kb.save(path)\n        except Exception as e:\n            print(f\"Error saving memory: {str(e)}\")\n            return False\n    return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.PyEnvEval","title":"<code>PyEnvEval</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>class PyEnvEval:\n    def __init__(self):\n        self.local_env = locals().copy()\n        self.global_env = {'local_env': self.local_env}  # globals().copy()\n\n    def eval_code(self, code):\n        try:\n            exec(code, self.global_env, self.local_env)\n            result = eval(code, self.global_env, self.local_env)\n            return self.format_output(result)\n        except Exception as e:\n            return self.format_output(str(e))\n\n    def get_env(self):\n        local_env_str = self.format_env(self.local_env)\n        return f'Locals:\\n{local_env_str}'\n\n    @staticmethod\n    def format_output(output):\n        return f'Ergebnis: {output}'\n\n    @staticmethod\n    def format_env(env):\n        return '\\n'.join(f'{key}: {value}' for key, value in env.items())\n\n    def run_and_display(self, python_code):\n        \"\"\"function to eval python code\"\"\"\n        start = f'Start-state:\\n{self.get_env()}'\n        result = self.eval_code(python_code)\n        end = f'End-state:\\n{self.get_env()}'\n        return f'{start}\\nResult:\\n{result}\\n{end}'\n\n    def tool(self):\n        return {\"PythonEval\": {\"func\": self.run_and_display, \"description\": \"Use Python Code to Get to an Persis Answer! input must be valid python code all non code parts must be comments!\"}}\n</code></pre> <code>run_and_display(python_code)</code> \u00b6 <p>function to eval python code</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def run_and_display(self, python_code):\n    \"\"\"function to eval python code\"\"\"\n    start = f'Start-state:\\n{self.get_env()}'\n    result = self.eval_code(python_code)\n    end = f'End-state:\\n{self.get_env()}'\n    return f'{start}\\nResult:\\n{result}\\n{end}'\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.anything_from_str_to_dict","title":"<code>anything_from_str_to_dict(data, expected_keys=None, mini_task=lambda x: '')</code>","text":"<p>Versucht, einen String in ein oder mehrere Dictionaries umzuwandeln. Ber\u00fccksichtigt dabei die erwarteten Schl\u00fcssel und ihre Standardwerte.</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def anything_from_str_to_dict(data: str, expected_keys: dict = None, mini_task=lambda x: ''):\n    \"\"\"\n    Versucht, einen String in ein oder mehrere Dictionaries umzuwandeln.\n    Ber\u00fccksichtigt dabei die erwarteten Schl\u00fcssel und ihre Standardwerte.\n    \"\"\"\n    if len(data) &lt; 4:\n        return []\n\n    if expected_keys is None:\n        expected_keys = {}\n\n    result = []\n    json_objects = find_json_objects_in_str(data)\n    if not json_objects and data.startswith('[') and data.endswith(']'):\n        json_objects = eval(data)\n    if json_objects and len(json_objects) &gt; 0 and isinstance(json_objects[0], dict):\n        result.extend([{**expected_keys, **ob} for ob in json_objects])\n    if not result:\n        completed_object = complete_json_object(data, mini_task)\n        if completed_object is not None:\n            result.append(completed_object)\n    if len(result) == 0 and expected_keys:\n        result = [{list(expected_keys.keys())[0]: data}]\n    for res in result:\n        if isinstance(res, list) and len(res) &gt; 0:\n            res = res[0]\n        for key, value in expected_keys.items():\n            if key not in res:\n                res[key] = value\n\n    if len(result) == 0:\n        fixed = fix_json(data)\n        if fixed:\n            result.append(fixed)\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.complete_json_object","title":"<code>complete_json_object(data, mini_task)</code>","text":"<p>Ruft eine Funktion auf, um einen String in das richtige Format zu bringen. Gibt das resultierende JSON-Objekt zur\u00fcck, wenn die Funktion erfolgreich ist, sonst None.</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def complete_json_object(data: str, mini_task):\n    \"\"\"\n    Ruft eine Funktion auf, um einen String in das richtige Format zu bringen.\n    Gibt das resultierende JSON-Objekt zur\u00fcck, wenn die Funktion erfolgreich ist, sonst None.\n    \"\"\"\n    ret = mini_task(\n        f\"Vervollst\u00e4ndige das Json Object. Und bringe den string in das Richtige format. data={data}\\nJson=\")\n    if ret:\n        return anything_from_str_to_dict(ret)\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.find_json_objects_in_str","title":"<code>find_json_objects_in_str(data)</code>","text":"<p>Sucht nach JSON-Objekten innerhalb eines Strings. Gibt eine Liste von JSON-Objekten zur\u00fcck, die im String gefunden wurden.</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def find_json_objects_in_str(data: str):\n    \"\"\"\n    Sucht nach JSON-Objekten innerhalb eines Strings.\n    Gibt eine Liste von JSON-Objekten zur\u00fcck, die im String gefunden wurden.\n    \"\"\"\n    json_objects = extract_json_objects(data)\n    if not isinstance(json_objects, list):\n        json_objects = [json_objects]\n    return [get_json_from_json_str(ob, 10) for ob in json_objects if get_json_from_json_str(ob, 10) is not None]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.get_json_from_json_str","title":"<code>get_json_from_json_str(json_str, repeat=1)</code>","text":"<p>Versucht, einen JSON-String in ein Python-Objekt umzuwandeln.</p> <p>Wenn beim Parsen ein Fehler auftritt, versucht die Funktion, das Problem zu beheben, indem sie das Zeichen an der Position des Fehlers durch ein Escape-Zeichen ersetzt. Dieser Vorgang wird bis zu <code>repeat</code>-mal wiederholt.</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str or list or dict</code> <p>Der JSON-String, der geparst werden soll.</p> required <code>repeat</code> <code>int</code> <p>Die Anzahl der Versuche, das Parsen durchzuf\u00fchren.</p> <code>1</code> <p>Returns:</p> Type Description <code>dict or None</code> <p>Das resultierende Python-Objekt.</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def get_json_from_json_str(json_str: str or list or dict, repeat: int = 1) -&gt; dict or None:\n    \"\"\"Versucht, einen JSON-String in ein Python-Objekt umzuwandeln.\n\n    Wenn beim Parsen ein Fehler auftritt, versucht die Funktion, das Problem zu beheben,\n    indem sie das Zeichen an der Position des Fehlers durch ein Escape-Zeichen ersetzt.\n    Dieser Vorgang wird bis zu `repeat`-mal wiederholt.\n\n    Args:\n        json_str: Der JSON-String, der geparst werden soll.\n        repeat: Die Anzahl der Versuche, das Parsen durchzuf\u00fchren.\n\n    Returns:\n        Das resultierende Python-Objekt.\n    \"\"\"\n    for _ in range(repeat):\n        try:\n            return parse_json_with_auto_detection(json_str)\n        except json.JSONDecodeError as e:\n            unexp = int(re.findall(r'\\(char (\\d+)\\)', str(e))[0])\n            unesc = json_str.rfind(r'\"', 0, unexp)\n            json_str = json_str[:unesc] + r'\\\"' + json_str[unesc + 1:]\n            closg = json_str.find(r'\"', unesc + 2)\n            json_str = json_str[:closg] + r'\\\"' + json_str[closg + 1:]\n        new = fix_json_object(json_str)\n        if new is not None:\n            json_str = new\n    get_logger().info(f\"Unable to parse JSON string after {json_str}\")\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.parse_json_with_auto_detection","title":"<code>parse_json_with_auto_detection(json_data)</code>","text":"<p>Parses JSON data, automatically detecting if a value is a JSON string and parsing it accordingly. If a value cannot be parsed as JSON, it is returned as is.</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def parse_json_with_auto_detection(json_data):\n    \"\"\"\n    Parses JSON data, automatically detecting if a value is a JSON string and parsing it accordingly.\n    If a value cannot be parsed as JSON, it is returned as is.\n    \"\"\"\n\n    def try_parse_json(value):\n        \"\"\"\n        Tries to parse a value as JSON. If the parsing fails, the original value is returned.\n        \"\"\"\n        try:\n            # print(\"parse_json_with_auto_detection:\", type(value), value)\n            parsed_value = json.loads(value)\n            # print(\"parsed_value:\", type(parsed_value), parsed_value)\n            # If the parsed value is a string, it might be a JSON string, so we try to parse it again\n            if isinstance(parsed_value, str):\n                return eval(parsed_value)\n            else:\n                return parsed_value\n        except Exception:\n            # logging.warning(f\"Failed to parse value as JSON: {value}. Exception: {e}\")\n            return value\n\n    get_logger()\n\n    if isinstance(json_data, dict):\n        return {key: parse_json_with_auto_detection(value) for key, value in json_data.items()}\n    elif isinstance(json_data, list):\n        return [parse_json_with_auto_detection(item) for item in json_data]\n    else:\n        return try_parse_json(json_data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase","title":"<code>KnowledgeBase</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.Chunk","title":"<code>Chunk</code>  <code>dataclass</code>","text":"<p>Represents a chunk of text with its embedding and metadata</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>@dataclass(slots=True)\nclass Chunk:\n    \"\"\"Represents a chunk of text with its embedding and metadata\"\"\"\n    text: str\n    embedding: np.ndarray\n    metadata: dict[str, Any]\n    content_hash: str\n    cluster_id: int | None = None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.ConceptAnalysis","title":"<code>ConceptAnalysis</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the analysis of key concepts.</p> <p>Attributes:</p> Name Type Description <code>key_concepts</code> <code>list[str]</code> <p>A list of primary key concepts identified.</p> <code>relationships</code> <code>list[str]</code> <p>A list of relationships between the identified key concepts.</p> <code>importance_hierarchy</code> <code>list[str]</code> <p>A list that represents the hierarchical importance of the key concepts.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class ConceptAnalysis(BaseModel):\n    \"\"\"\n    Represents the analysis of key concepts.\n\n    Attributes:\n        key_concepts (list[str]): A list of primary key concepts identified.\n        relationships (list[str]): A list of relationships between the identified key concepts.\n        importance_hierarchy (list[str]): A list that represents the hierarchical importance of the key concepts.\n    \"\"\"\n    key_concepts: list[str]\n    relationships: list[str]\n    importance_hierarchy: list[str]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.ConceptExtractor","title":"<code>ConceptExtractor</code>","text":"<p>Handles extraction of concepts and relationships from text</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class ConceptExtractor:\n    \"\"\"Handles extraction of concepts and relationships from text\"\"\"\n\n    def __init__(self, knowledge_base, requests_per_second = 85.):\n        self.kb = knowledge_base\n        self.concept_graph = ConceptGraph()\n        self.requests_per_second = requests_per_second\n\n    async def extract_concepts(self, texts: list[str], metadatas: list[dict[str, Any]]) -&gt; list[list[Concept]]:\n        \"\"\"\n        Extract concepts from texts using concurrent processing with rate limiting.\n        Requests are made at the specified rate while responses are processed asynchronously.\n        \"\"\"\n        # Ensure metadatas list matches texts length\n        metadatas = metadatas + [{}] * (len(texts) - len(metadatas))\n\n        # Initialize rate limiter\n        rate_limiter = DynamicRateLimiter()\n\n        system_prompt = (\n            \"Analyze the given text and extract key concepts and their relationships. For each concept:\\n\"\n            \"1. Identify the concept name and category (technical, domain, method, property, ...)\\n\"\n            \"2. Determine relationships with other concepts (uses, part_of, similar_to, depends_on, ...)\\n\"\n            \"3. Assess importance (0-1 score) based on centrality to the text\\n\"\n            \"4. Extract relevant context snippets\\n\"\n            \"5. Max 5 Concepts!\\n\"\n            \"only return in json format!\\n\"\n            \"\"\"{\"concepts\": [{\n                \"name\": \"concept_name\",\n                \"category\": \"category_name\",\n                \"relationships\": {\n                    \"relationship_type\": [\"related_concept1\", \"related_concept2\"]\n                },\n                \"importance_score\": 0.0,\n                \"context_snippets\": [\"relevant text snippet\"]\n            }]}\\n\"\"\"\n        )\n\n        # Prepare all requests\n        requests = [\n            (idx, f\"Text to Convert in to JSON structure:\\n{text}\", system_prompt, metadata)\n            for idx, (text, metadata) in enumerate(zip(texts, metadatas, strict=False))\n        ]\n\n        async def process_single_request(idx: int, prompt: str, system_prompt: str, metadata: dict[str, Any]):\n            \"\"\"Process a single request with rate limiting\"\"\"\n            try:\n                # Wait for rate limit\n                await rate_limiter.acquire()\n                i__[1] += 1\n                # Make API call without awaiting the response\n                response_future = litellm_complete(\n                    prompt=prompt,\n                    system_prompt=system_prompt,\n                    response_format=Concepts,\n                    model_name=self.kb.model_name,\n                    fallbacks=[\"groq/gemma2-9b-it\"] +\n                              [m for m in os.getenv(\"FALLBACKS_MODELS_PREM\", '').split(',') if m]\n                )\n\n                return idx, response_future\n\n            except Exception as e:\n                print(f\"Error initiating request {idx}: {str(e)}\")\n                return idx, None\n\n        async def process_response(idx: int, response_future) -&gt; list[Concept]:\n            \"\"\"Process the response once it's ready\"\"\"\n            try:\n                if response_future is None:\n                    return []\n\n                response = await response_future\n                return await self._process_response(response, metadatas[idx])\n\n            except Exception as e:\n                print(f\"Error processing response {idx}: {str(e)}\")\n                return []\n\n        # Create tasks for all requests\n        request_tasks = []\n        batch_size = self.kb.batch_size\n\n        rate_limiter.update_rate(self.requests_per_second)\n\n        for batch_start in range(0, len(requests), batch_size):\n            batch = requests[batch_start:batch_start + batch_size]\n\n            # Create tasks for the batch\n            batch_tasks = [\n                process_single_request(idx, prompt, sys_prompt, meta)\n                for idx, prompt, sys_prompt, meta in batch\n            ]\n            request_tasks.extend(batch_tasks)\n\n        # Execute all requests with rate limiting\n        request_results = await asyncio.gather(*request_tasks)\n\n        # Process responses as they complete\n        response_tasks = [\n            process_response(idx, response_future)\n            for idx, response_future in request_results\n        ]\n\n        # Gather all results\n        all_results = await asyncio.gather(*response_tasks)\n\n        # Sort results by original index\n        sorted_results = [[] for _ in texts]\n        for idx, concepts in enumerate(all_results):\n            sorted_results[idx] = concepts\n\n        return sorted_results\n\n    async def _process_response(self, response: Any, metadata: dict[str, Any]) -&gt; list[Concept]:\n        \"\"\"Helper method to process a single response and convert it to Concepts\"\"\"\n        try:\n            # Extract content from response\n            if hasattr(response, 'choices'):\n                content = response.choices[0].message.content\n                if content is None:\n                    content = response.choices[0].message.tool_calls[0].function.arguments\n                if content is None:\n                    return []\n            elif isinstance(response, str):\n                content = response\n            else:\n                print(f\"Unexpected response type: {type(response)}\")\n                return []\n\n            # Parse JSON and create concepts\n            concept_data = after_format(content)\n            concepts = []\n\n            for concept_info in concept_data.get(\"concepts\", []):\n                concept = Concept(\n                    name=concept_info[\"name\"],\n                    category=concept_info.get(\"category\", \"N/A\"),\n                    relationships={k: set(v) for k, v in concept_info.get(\"relationships\", {}).items()},\n                    importance_score=concept_info.get(\"importance_score\", 0.1),\n                    context_snippets=concept_info.get(\"context_snippets\", \"N/A\"),\n                    metadata=metadata\n                )\n                concepts.append(concept)\n                self.concept_graph.add_concept(concept)\n\n            return concepts\n\n        except Exception:\n            i__[2] +=1\n            return []\n\n    async def process_chunks(self, chunks: list[Chunk]) -&gt; None:\n        \"\"\"\n        Process all chunks in batch to extract and store concepts.\n        Each chunk's metadata will be updated with the concept names and relationships.\n        \"\"\"\n        # Gather all texts from the chunks.\n        texts = [chunk.text for chunk in chunks]\n        # Call extract_concepts once with all texts.\n        all_concepts = await self.extract_concepts(texts, [chunk.metadata for chunk in chunks])\n\n        # Update each chunk's metadata with its corresponding concepts.\n        for chunk, concepts in zip(chunks, all_concepts, strict=False):\n            chunk.metadata[\"concepts\"] = [c.name for c in concepts]\n            chunk.metadata[\"concept_relationships\"] = {\n                c.name: {k: list(v) for k, v in c.relationships.items()}\n                for c in concepts\n            }\n\n    async def query_concepts(self, query: str) -&gt; dict[str, any]:\n        \"\"\"Query the concept graph based on natural language query\"\"\"\n\n        system_prompt = \"\"\"\n        Convert the natural language query about concepts into a structured format that specifies:\n        1. Main concepts of interest\n        2. Desired relationship types\n        3. Any category filters\n        4. Importance threshold\n\n        Format as JSON.\n        \"\"\"\n\n        prompt = f\"\"\"\n        Query: {query}\n\n        Convert to this JSON structure:\n        {{\n            \"target_concepts\": [\"concept1\", \"concept2\"],\n            \"relationship_types\": [\"type1\", \"type2\"],\n            \"categories\": [\"category1\", \"category2\"],\n            \"min_importance\": 0.0\n        }}\n        \"\"\"\n\n        try:\n            response = await litellm_complete(\n                model_name=self.kb.model_name,\n                prompt=prompt,\n                system_prompt=system_prompt,\n                response_format=TConcept\n            )\n\n            query_params = json.loads(response)\n\n            results = {\n                \"concepts\": {},\n                \"relationships\": [],\n                \"groups\": []\n            }\n\n            # Find matching concepts\n            for concept_name in query_params[\"target_concepts\"]:\n                if concept_name in self.concept_graph.concepts:\n                    concept = self.concept_graph.concepts[concept_name]\n                    if concept.importance_score &gt;= query_params[\"min_importance\"]:\n                        results[\"concepts\"][concept_name] = {\n                            \"category\": concept.category,\n                            \"importance\": concept.importance_score,\n                            \"context\": concept.context_snippets\n                        }\n\n                        # Get relationships\n                        for rel_type in query_params[\"relationship_types\"]:\n                            related = self.concept_graph.get_related_concepts(\n                                concept_name, rel_type\n                            )\n                            for related_concept in related:\n                                results[\"relationships\"].append({\n                                    \"from\": concept_name,\n                                    \"to\": related_concept,\n                                    \"type\": rel_type\n                                })\n\n            # Group concepts by category\n            category_groups = defaultdict(list)\n            for concept_name, concept_info in results[\"concepts\"].items():\n                category_groups[concept_info[\"category\"]].append(concept_name)\n            results[\"groups\"] = [\n                {\"category\": cat, \"concepts\": concepts}\n                for cat, concepts in category_groups.items()\n            ]\n\n            return results\n\n        except Exception as e:\n            print(f\"Error querying concepts: {str(e)}\")\n            return {\"concepts\": {}, \"relationships\": [], \"groups\": []}\n</code></pre> <code>extract_concepts(texts, metadatas)</code> <code>async</code> \u00b6 <p>Extract concepts from texts using concurrent processing with rate limiting. Requests are made at the specified rate while responses are processed asynchronously.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def extract_concepts(self, texts: list[str], metadatas: list[dict[str, Any]]) -&gt; list[list[Concept]]:\n    \"\"\"\n    Extract concepts from texts using concurrent processing with rate limiting.\n    Requests are made at the specified rate while responses are processed asynchronously.\n    \"\"\"\n    # Ensure metadatas list matches texts length\n    metadatas = metadatas + [{}] * (len(texts) - len(metadatas))\n\n    # Initialize rate limiter\n    rate_limiter = DynamicRateLimiter()\n\n    system_prompt = (\n        \"Analyze the given text and extract key concepts and their relationships. For each concept:\\n\"\n        \"1. Identify the concept name and category (technical, domain, method, property, ...)\\n\"\n        \"2. Determine relationships with other concepts (uses, part_of, similar_to, depends_on, ...)\\n\"\n        \"3. Assess importance (0-1 score) based on centrality to the text\\n\"\n        \"4. Extract relevant context snippets\\n\"\n        \"5. Max 5 Concepts!\\n\"\n        \"only return in json format!\\n\"\n        \"\"\"{\"concepts\": [{\n            \"name\": \"concept_name\",\n            \"category\": \"category_name\",\n            \"relationships\": {\n                \"relationship_type\": [\"related_concept1\", \"related_concept2\"]\n            },\n            \"importance_score\": 0.0,\n            \"context_snippets\": [\"relevant text snippet\"]\n        }]}\\n\"\"\"\n    )\n\n    # Prepare all requests\n    requests = [\n        (idx, f\"Text to Convert in to JSON structure:\\n{text}\", system_prompt, metadata)\n        for idx, (text, metadata) in enumerate(zip(texts, metadatas, strict=False))\n    ]\n\n    async def process_single_request(idx: int, prompt: str, system_prompt: str, metadata: dict[str, Any]):\n        \"\"\"Process a single request with rate limiting\"\"\"\n        try:\n            # Wait for rate limit\n            await rate_limiter.acquire()\n            i__[1] += 1\n            # Make API call without awaiting the response\n            response_future = litellm_complete(\n                prompt=prompt,\n                system_prompt=system_prompt,\n                response_format=Concepts,\n                model_name=self.kb.model_name,\n                fallbacks=[\"groq/gemma2-9b-it\"] +\n                          [m for m in os.getenv(\"FALLBACKS_MODELS_PREM\", '').split(',') if m]\n            )\n\n            return idx, response_future\n\n        except Exception as e:\n            print(f\"Error initiating request {idx}: {str(e)}\")\n            return idx, None\n\n    async def process_response(idx: int, response_future) -&gt; list[Concept]:\n        \"\"\"Process the response once it's ready\"\"\"\n        try:\n            if response_future is None:\n                return []\n\n            response = await response_future\n            return await self._process_response(response, metadatas[idx])\n\n        except Exception as e:\n            print(f\"Error processing response {idx}: {str(e)}\")\n            return []\n\n    # Create tasks for all requests\n    request_tasks = []\n    batch_size = self.kb.batch_size\n\n    rate_limiter.update_rate(self.requests_per_second)\n\n    for batch_start in range(0, len(requests), batch_size):\n        batch = requests[batch_start:batch_start + batch_size]\n\n        # Create tasks for the batch\n        batch_tasks = [\n            process_single_request(idx, prompt, sys_prompt, meta)\n            for idx, prompt, sys_prompt, meta in batch\n        ]\n        request_tasks.extend(batch_tasks)\n\n    # Execute all requests with rate limiting\n    request_results = await asyncio.gather(*request_tasks)\n\n    # Process responses as they complete\n    response_tasks = [\n        process_response(idx, response_future)\n        for idx, response_future in request_results\n    ]\n\n    # Gather all results\n    all_results = await asyncio.gather(*response_tasks)\n\n    # Sort results by original index\n    sorted_results = [[] for _ in texts]\n    for idx, concepts in enumerate(all_results):\n        sorted_results[idx] = concepts\n\n    return sorted_results\n</code></pre> <code>process_chunks(chunks)</code> <code>async</code> \u00b6 <p>Process all chunks in batch to extract and store concepts. Each chunk's metadata will be updated with the concept names and relationships.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def process_chunks(self, chunks: list[Chunk]) -&gt; None:\n    \"\"\"\n    Process all chunks in batch to extract and store concepts.\n    Each chunk's metadata will be updated with the concept names and relationships.\n    \"\"\"\n    # Gather all texts from the chunks.\n    texts = [chunk.text for chunk in chunks]\n    # Call extract_concepts once with all texts.\n    all_concepts = await self.extract_concepts(texts, [chunk.metadata for chunk in chunks])\n\n    # Update each chunk's metadata with its corresponding concepts.\n    for chunk, concepts in zip(chunks, all_concepts, strict=False):\n        chunk.metadata[\"concepts\"] = [c.name for c in concepts]\n        chunk.metadata[\"concept_relationships\"] = {\n            c.name: {k: list(v) for k, v in c.relationships.items()}\n            for c in concepts\n        }\n</code></pre> <code>query_concepts(query)</code> <code>async</code> \u00b6 <p>Query the concept graph based on natural language query</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def query_concepts(self, query: str) -&gt; dict[str, any]:\n    \"\"\"Query the concept graph based on natural language query\"\"\"\n\n    system_prompt = \"\"\"\n    Convert the natural language query about concepts into a structured format that specifies:\n    1. Main concepts of interest\n    2. Desired relationship types\n    3. Any category filters\n    4. Importance threshold\n\n    Format as JSON.\n    \"\"\"\n\n    prompt = f\"\"\"\n    Query: {query}\n\n    Convert to this JSON structure:\n    {{\n        \"target_concepts\": [\"concept1\", \"concept2\"],\n        \"relationship_types\": [\"type1\", \"type2\"],\n        \"categories\": [\"category1\", \"category2\"],\n        \"min_importance\": 0.0\n    }}\n    \"\"\"\n\n    try:\n        response = await litellm_complete(\n            model_name=self.kb.model_name,\n            prompt=prompt,\n            system_prompt=system_prompt,\n            response_format=TConcept\n        )\n\n        query_params = json.loads(response)\n\n        results = {\n            \"concepts\": {},\n            \"relationships\": [],\n            \"groups\": []\n        }\n\n        # Find matching concepts\n        for concept_name in query_params[\"target_concepts\"]:\n            if concept_name in self.concept_graph.concepts:\n                concept = self.concept_graph.concepts[concept_name]\n                if concept.importance_score &gt;= query_params[\"min_importance\"]:\n                    results[\"concepts\"][concept_name] = {\n                        \"category\": concept.category,\n                        \"importance\": concept.importance_score,\n                        \"context\": concept.context_snippets\n                    }\n\n                    # Get relationships\n                    for rel_type in query_params[\"relationship_types\"]:\n                        related = self.concept_graph.get_related_concepts(\n                            concept_name, rel_type\n                        )\n                        for related_concept in related:\n                            results[\"relationships\"].append({\n                                \"from\": concept_name,\n                                \"to\": related_concept,\n                                \"type\": rel_type\n                            })\n\n        # Group concepts by category\n        category_groups = defaultdict(list)\n        for concept_name, concept_info in results[\"concepts\"].items():\n            category_groups[concept_info[\"category\"]].append(concept_name)\n        results[\"groups\"] = [\n            {\"category\": cat, \"concepts\": concepts}\n            for cat, concepts in category_groups.items()\n        ]\n\n        return results\n\n    except Exception as e:\n        print(f\"Error querying concepts: {str(e)}\")\n        return {\"concepts\": {}, \"relationships\": [], \"groups\": []}\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.ConceptGraph","title":"<code>ConceptGraph</code>","text":"<p>Manages concept relationships and hierarchies</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class ConceptGraph:\n    \"\"\"Manages concept relationships and hierarchies\"\"\"\n\n    def __init__(self):\n        self.concepts: dict[str, Concept] = {}\n\n    def add_concept(self, concept: Concept):\n        \"\"\"Add or update a concept in the graph\"\"\"\n        if concept.name.lower() in self.concepts:\n            # Merge relationships and context\n            existing = self.concepts[concept.name.lower()]\n            for rel_type, related in concept.relationships.items():\n                if rel_type not in existing.relationships:\n                    existing.relationships[rel_type] = set()\n                existing.relationships[rel_type].update(related)\n            existing.context_snippets.extend(concept.context_snippets)\n            # Update importance score with rolling average\n            existing.importance_score = (existing.importance_score + concept.importance_score) / 2\n        else:\n            self.concepts[concept.name.lower()] = concept\n\n    def get_related_concepts(self, concept_name: str, relationship_type: str | None = None) -&gt; set[str]:\n        \"\"\"Get related concepts, optionally filtered by relationship type\"\"\"\n        if concept_name not in self.concepts:\n            return set()\n\n        concept = self.concepts[concept_name.lower()]\n        if relationship_type:\n            return concept.relationships.get(relationship_type, set())\n\n        related = set()\n        for relations in concept.relationships.values():\n            related.update(relations)\n        return related\n\n\n    def convert_to_networkx(self) -&gt; nx.DiGraph:\n        \"\"\"Convert ConceptGraph to NetworkX graph with layout\"\"\"\n        print(f\"Converting to NetworkX graph with {len(self.concepts.values())} concepts\")\n\n        G = nx.DiGraph()\n\n        if len(self.concepts.values()) == 0:\n            return G\n\n        for concept in self.concepts.values():\n            cks = '\\n - '.join(concept.context_snippets[:4])\n            G.add_node(\n                concept.name,\n                size=concept.importance_score * 10,\n                group=concept.category,\n                title=f\"\"\"\n                    {concept.name}\n                    Category: {concept.category}\n                    Importance: {concept.importance_score:.2f}\n                    Context: \\n - {cks}\n                    \"\"\"\n            )\n\n            for rel_type, targets in concept.relationships.items():\n                for target in targets:\n                    G.add_edge(concept.name, target, label=rel_type, title=rel_type)\n\n        return G\n</code></pre> <code>add_concept(concept)</code> \u00b6 <p>Add or update a concept in the graph</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def add_concept(self, concept: Concept):\n    \"\"\"Add or update a concept in the graph\"\"\"\n    if concept.name.lower() in self.concepts:\n        # Merge relationships and context\n        existing = self.concepts[concept.name.lower()]\n        for rel_type, related in concept.relationships.items():\n            if rel_type not in existing.relationships:\n                existing.relationships[rel_type] = set()\n            existing.relationships[rel_type].update(related)\n        existing.context_snippets.extend(concept.context_snippets)\n        # Update importance score with rolling average\n        existing.importance_score = (existing.importance_score + concept.importance_score) / 2\n    else:\n        self.concepts[concept.name.lower()] = concept\n</code></pre> <code>convert_to_networkx()</code> \u00b6 <p>Convert ConceptGraph to NetworkX graph with layout</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def convert_to_networkx(self) -&gt; nx.DiGraph:\n    \"\"\"Convert ConceptGraph to NetworkX graph with layout\"\"\"\n    print(f\"Converting to NetworkX graph with {len(self.concepts.values())} concepts\")\n\n    G = nx.DiGraph()\n\n    if len(self.concepts.values()) == 0:\n        return G\n\n    for concept in self.concepts.values():\n        cks = '\\n - '.join(concept.context_snippets[:4])\n        G.add_node(\n            concept.name,\n            size=concept.importance_score * 10,\n            group=concept.category,\n            title=f\"\"\"\n                {concept.name}\n                Category: {concept.category}\n                Importance: {concept.importance_score:.2f}\n                Context: \\n - {cks}\n                \"\"\"\n        )\n\n        for rel_type, targets in concept.relationships.items():\n            for target in targets:\n                G.add_edge(concept.name, target, label=rel_type, title=rel_type)\n\n    return G\n</code></pre> <code>get_related_concepts(concept_name, relationship_type=None)</code> \u00b6 <p>Get related concepts, optionally filtered by relationship type</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def get_related_concepts(self, concept_name: str, relationship_type: str | None = None) -&gt; set[str]:\n    \"\"\"Get related concepts, optionally filtered by relationship type\"\"\"\n    if concept_name not in self.concepts:\n        return set()\n\n    concept = self.concepts[concept_name.lower()]\n    if relationship_type:\n        return concept.relationships.get(relationship_type, set())\n\n    related = set()\n    for relations in concept.relationships.values():\n        related.update(relations)\n    return related\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.Concepts","title":"<code>Concepts</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a collection of key concepts.</p> <p>Attributes:</p> Name Type Description <code>concepts</code> <code>List[rConcept]</code> <p>A list of Concept instances, each representing an individual key concept.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class Concepts(BaseModel):\n    \"\"\"\n    Represents a collection of key concepts.\n\n    Attributes:\n        concepts (List[rConcept]): A list of Concept instances, each representing an individual key concept.\n    \"\"\"\n    concepts: list[rConcept]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.DataModel","title":"<code>DataModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The main data model that encapsulates the overall analysis.</p> <p>Attributes:</p> Name Type Description <code>main_summary</code> <code>str</code> <p>A Detailed overview summarizing the key findings and relations format MD string.</p> <code>concept_analysis</code> <code>ConceptAnalysis</code> <p>An instance containing the analysis of key concepts.</p> <code>topic_insights</code> <code>TopicInsights</code> <p>An instance containing insights regarding the topics.</p> <code>relevance_assessment</code> <code>RelevanceAssessment</code> <p>An instance assessing the relevance and alignment of the query.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class DataModel(BaseModel):\n    \"\"\"\n    The main data model that encapsulates the overall analysis.\n\n    Attributes:\n        main_summary (str): A Detailed overview summarizing the key findings and relations format MD string.\n        concept_analysis (ConceptAnalysis): An instance containing the analysis of key concepts.\n        topic_insights (TopicInsights): An instance containing insights regarding the topics.\n        relevance_assessment (RelevanceAssessment): An instance assessing the relevance and alignment of the query.\n    \"\"\"\n    main_summary: str\n    concept_analysis: ConceptAnalysis\n    topic_insights: TopicInsights\n    relevance_assessment: RelevanceAssessment\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.DynamicRateLimiter","title":"<code>DynamicRateLimiter</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class DynamicRateLimiter:\n    def __init__(self):\n        self.last_request_time = 0.0\n        self._lock = asyncio.Lock()\n\n    def update_rate(self, requests_per_second: float):\n        \"\"\"Update rate limit dynamically\"\"\"\n        self.min_interval = 1.0 / requests_per_second if requests_per_second &gt; 0 else float('inf')\n\n    async def acquire(self):\n        \"\"\"Acquire permission to make a request\"\"\"\n        async with self._lock:\n            current_time = time.time()\n            time_since_last = current_time - self.last_request_time\n            if time_since_last &lt; self.min_interval:\n                wait_time = self.min_interval - time_since_last\n                await asyncio.sleep(wait_time)\n            self.last_request_time = time.time()\n</code></pre> <code>acquire()</code> <code>async</code> \u00b6 <p>Acquire permission to make a request</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def acquire(self):\n    \"\"\"Acquire permission to make a request\"\"\"\n    async with self._lock:\n        current_time = time.time()\n        time_since_last = current_time - self.last_request_time\n        if time_since_last &lt; self.min_interval:\n            wait_time = self.min_interval - time_since_last\n            await asyncio.sleep(wait_time)\n        self.last_request_time = time.time()\n</code></pre> <code>update_rate(requests_per_second)</code> \u00b6 <p>Update rate limit dynamically</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def update_rate(self, requests_per_second: float):\n    \"\"\"Update rate limit dynamically\"\"\"\n    self.min_interval = 1.0 / requests_per_second if requests_per_second &gt; 0 else float('inf')\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.GraphVisualizer","title":"<code>GraphVisualizer</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class GraphVisualizer:\n    @staticmethod\n    def visualize(nx_graph: nx.DiGraph, output_file: str = \"concept_graph.html\", get_output=False):\n        \"\"\"Create interactive visualization using PyVis\"\"\"\n        from pyvis.network import Network\n        net = Network(\n            height=\"800px\",\n            width=\"100%\",\n            notebook=False,\n            directed=True,\n            bgcolor=\"#1a1a1a\",\n            font_color=\"white\"\n        )\n\n        net.from_nx(nx_graph)\n\n        net.save_graph(output_file)\n        print(f\"Graph saved to {output_file} Open in browser to view.\", len(nx_graph))\n        if get_output:\n            c = open(output_file, encoding=\"utf-8\").read()\n            os.remove(output_file)\n            return c\n</code></pre> <code>visualize(nx_graph, output_file='concept_graph.html', get_output=False)</code> <code>staticmethod</code> \u00b6 <p>Create interactive visualization using PyVis</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>@staticmethod\ndef visualize(nx_graph: nx.DiGraph, output_file: str = \"concept_graph.html\", get_output=False):\n    \"\"\"Create interactive visualization using PyVis\"\"\"\n    from pyvis.network import Network\n    net = Network(\n        height=\"800px\",\n        width=\"100%\",\n        notebook=False,\n        directed=True,\n        bgcolor=\"#1a1a1a\",\n        font_color=\"white\"\n    )\n\n    net.from_nx(nx_graph)\n\n    net.save_graph(output_file)\n    print(f\"Graph saved to {output_file} Open in browser to view.\", len(nx_graph))\n    if get_output:\n        c = open(output_file, encoding=\"utf-8\").read()\n        os.remove(output_file)\n        return c\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.KnowledgeBase","title":"<code>KnowledgeBase</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class KnowledgeBase:\n    def __init__(self, embedding_dim: int = 768, similarity_threshold: float = 0.61, batch_size: int = 64,\n                 n_clusters: int = 4, deduplication_threshold: float = 0.85, model_name=os.getenv(\"DEFAULTMODELSUMMERY\"),\n                 embedding_model=os.getenv(\"DEFAULTMODELEMBEDDING\"),\n                 vis_class:str | None = \"FaissVectorStore\",\n                 vis_kwargs:dict[str, Any] | None=None,\n                 requests_per_second=85.,\n                 chunk_size: int = 3600,\n                 chunk_overlap: int = 130,\n                 separator: str = \"\\n\"\n                 ):\n        \"\"\"Initialize the knowledge base with given parameters\"\"\"\n\n        self.existing_hashes: set[str] = set()\n        self.embedding_model = embedding_model\n        self.embedding_dim = embedding_dim\n        self.similarity_threshold = similarity_threshold\n        self.deduplication_threshold = deduplication_threshold\n        if model_name == \"openrouter/mistralai/mistral-nemo\":\n            batch_size = 9\n            requests_per_second = 1.5\n        self.batch_size = batch_size\n        self.n_clusters = n_clusters\n        self.model_name = model_name\n        self.sto: list = []\n\n        self.text_splitter = TextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap, separator=separator)\n        self.similarity_graph = {}\n        self.concept_extractor = ConceptExtractor(self, requests_per_second)\n\n        self.vis_class = None\n        self.vis_kwargs = None\n        self.vdb = None\n        self.init_vis(vis_class, vis_kwargs)\n\n    def init_vis(self, vis_class, vis_kwargs):\n        if vis_class is None:\n            vis_class = \"FaissVectorStore\"\n        if vis_class == \"FaissVectorStore\":\n            if vis_kwargs is None:\n                vis_kwargs = {\n                    \"dimension\": self.embedding_dim\n                }\n            self.vdb = FaissVectorStore(**vis_kwargs)\n        else:\n            from toolboxv2.mods.isaa.base.VectorStores.taichiNumpyNumbaVectorStores import (\n                EnhancedVectorStore,\n                FastVectorStore1,\n                FastVectorStoreO,\n                NumpyVectorStore,\n                VectorStoreConfig,\n            )\n        if vis_class == \"FastVectorStoreO\":\n            if vis_kwargs is None:\n                vis_kwargs = {\n                    \"embedding_size\": self.embedding_dim\n                }\n            self.vdb = FastVectorStoreO(**vis_kwargs)\n        if vis_class == \"EnhancedVectorStore\":\n            if vis_kwargs is None:\n                vis_kwargs = {\n                    \"dimension\": self.embedding_dim\n                }\n            vis_kwargs = VectorStoreConfig(**vis_kwargs)\n            self.vdb = EnhancedVectorStore(vis_kwargs)\n        if vis_class == \"FastVectorStore1\":\n            self.vdb = FastVectorStore1()\n        if vis_class == \"NumpyVectorStore\":\n            self.vdb = NumpyVectorStore()\n\n        self.vis_class = vis_class\n        self.vis_kwargs = vis_kwargs\n\n\n    @staticmethod\n    def compute_hash(text: str) -&gt; str:\n        \"\"\"Compute SHA-256 hash of text\"\"\"\n        return hashlib.sha256(text.encode('utf-8', errors='ignore')).hexdigest()\n\n    async def _get_embeddings(self, texts: list[str]) -&gt; np.ndarray:\n        \"\"\"Get normalized embeddings in batches\"\"\"\n        try:\n            async def process_batch(batch: list[str]) -&gt; np.ndarray:\n                from toolboxv2.mods.isaa.extras.adapter import litellm_embed\n                # print(\"Processing\", batch)\n                embeddings = await litellm_embed(texts=batch, model=self.embedding_model)\n                return normalize_vectors(embeddings)\n\n            tasks = []\n            for i in range(0, len(texts), self.batch_size):\n                batch = texts[i:i + self.batch_size]\n                tasks.append(process_batch(batch))\n\n            embeddings = await asyncio.gather(*tasks)\n            i__[0] += len(texts)\n            return np.vstack(embeddings)\n        except Exception as e:\n            get_logger().error(f\"Error generating embeddings: {str(e)}\")\n            raise\n\n\n\n    def _remove_similar_chunks(self, threshold: float = None) -&gt; int:\n        \"\"\"Remove chunks that are too similar to each other\"\"\"\n        if len(self.vdb.chunks) &lt; 2:\n            return 0\n\n        if threshold is None:\n            threshold = self.deduplication_threshold\n\n        try:\n            # Get all embeddings\n            embeddings = np.vstack([c.embedding for c in self.vdb.chunks])\n            n = len(embeddings)\n\n            # Compute similarity matrix\n            similarities = np.dot(embeddings, embeddings.T)\n\n            # Create mask for chunks to keep\n            keep_mask = np.ones(n, dtype=bool)\n\n            # Iterate through chunks\n            for i in range(n):\n                if not keep_mask[i]:\n                    continue\n\n                # Find chunks that are too similar to current chunk\n                similar_indices = similarities[i] &gt;= threshold\n                similar_indices[i] = False  # Don't count self-similarity\n\n                # Mark similar chunks for removal\n                keep_mask[similar_indices] = False\n\n            # Keep only unique chunks\n            unique_chunks = [chunk for chunk, keep in zip(self.vdb.chunks, keep_mask, strict=False) if keep]\n            removed_count = len(self.vdb.chunks) - len(unique_chunks)\n\n            # Update chunks and hashes\n            self.vdb.chunks = unique_chunks\n            self.existing_hashes = {chunk.content_hash for chunk in self.vdb.chunks}\n\n            # Rebuild index if chunks were removed\n            if removed_count &gt; 0:\n                self.vdb.rebuild_index()\n\n\n            return removed_count\n\n        except Exception as e:\n            get_logger().error(f\"Error removing similar chunks: {str(e)}\")\n            raise\n\n    async def _add_data(\n        self,\n        texts: list[str],\n        metadata: list[dict[str, Any]] | None= None,\n    ) -&gt; tuple[int, int]:\n        \"\"\"\n        Process and add new data to the knowledge base\n        Returns: Tuple of (added_count, duplicate_count)\n        \"\"\"\n        if len(texts) == 0:\n            return -1, -1\n        try:\n            # Compute hashes and filter exact duplicates\n            hashes = [self.compute_hash(text) for text in texts]\n            unique_data = []\n            for t, m, h in zip(texts, metadata, hashes, strict=False):\n                if h in self.existing_hashes:\n                    continue\n                # Update existing hashes\n                self.existing_hashes.add(h)\n                unique_data.append((t, m, h))\n\n            if not unique_data:\n                return 0, len(texts)\n\n            # Get embeddings\n            embeddings = await self._get_embeddings(texts)\n\n            texts = []\n            metadata = []\n            hashes = []\n            embeddings_final = []\n            if len(self.vdb.chunks):\n                for i, d in enumerate(unique_data):\n                    c = self.vdb.search(embeddings[i], 5, self.deduplication_threshold)\n                    if len(c) &gt; 2:\n                        continue\n                    t, m, h = d\n                    texts.append(t)\n                    metadata.append(m)\n                    hashes.append(h)\n                    embeddings_final.append(embeddings[i])\n\n            else:\n                texts , metadata, hashes = zip(*unique_data, strict=False)\n                embeddings_final = embeddings\n\n            if not texts:  # All were similar to existing chunks\n                return 0, len(unique_data)\n\n            # Create and add new chunks\n            new_chunks = [\n                Chunk(text=t, embedding=e, metadata=m, content_hash=h)\n                for t, e, m, h in zip(texts, embeddings_final, metadata, hashes, strict=False)\n            ]\n\n            # Add new chunks\n            # Update index\n            if new_chunks:\n                all_embeddings = np.vstack([c.embedding for c in new_chunks])\n                self.vdb.add_embeddings(all_embeddings, new_chunks)\n\n            # Remove similar chunks from the entire collection\n            removed = self._remove_similar_chunks()\n            get_logger().info(f\"Removed {removed} similar chunks during deduplication\")\n            # Invalidate visualization cache\n\n            if len(new_chunks) - removed &gt; 0:\n                # Process new chunks for concepts\n                await self.concept_extractor.process_chunks(new_chunks)\n            print(\"[total, calls, errors]\", i__)\n\n            return len(new_chunks) - removed, len(texts) - len(new_chunks) + removed\n\n        except Exception as e:\n            get_logger().error(f\"Error adding data: {str(e)}\")\n            raise\n\n\n    async def add_data(\n        self,\n        texts: list[str],\n        metadata: list[dict[str, Any]] | None = None,\n    ) -&gt; tuple[int, int]:\n        \"\"\"Enhanced version with smart splitting and clustering\"\"\"\n        if isinstance(texts, str):\n            texts = [texts]\n        if metadata is None:\n            metadata = [{}] * len(texts)\n        if isinstance(metadata, dict):\n            metadata = [metadata]\n        if len(texts) != len(metadata):\n            raise ValueError(\"Length of texts and metadata must match\")\n        if len(texts) == 1 and len(texts[0]) &lt; 10_000:\n            if len(self.sto) &lt; self.batch_size and len(texts) == 1:\n                self.sto.append((texts[0], metadata[0]))\n                return -1, -1\n            if len(self.sto) &gt;= self.batch_size:\n                _ = [texts.append(t) or metadata.append([m]) for (t, m) in self.sto]\n                self.sto = []\n\n        # Split large texts\n        split_texts = []\n        split_metadata = []\n\n        while Spinner(\"Saving Data to Memory\", symbols='t'):\n\n            for idx, text in enumerate(texts):\n                chunks = self.text_splitter.split_text(text)\n                split_texts.extend(chunks)\n\n                # Adjust metadata for splits\n                meta = metadata[idx] if metadata else {}\n                if isinstance(meta, list):\n                    meta = meta[0]\n                for i, _chunk in enumerate(chunks):\n                    chunk_meta = meta.copy()\n                    chunk_meta.update({\n                        'chunk_index': i,\n                        'total_chunks': len(chunks),\n                        'original_text_id': idx\n                    })\n                    split_metadata.append(chunk_meta)\n\n            return await self._add_data(split_texts, split_metadata)\n\n    def _update_similarity_graph(self, embeddings: np.ndarray, chunk_ids: list[int]):\n        \"\"\"Update similarity graph for connected information detection\"\"\"\n        similarities = np.dot(embeddings, embeddings.T)\n\n        for i in range(len(chunk_ids)):\n            for j in range(i + 1, len(chunk_ids)):\n                if similarities[i, j] &gt;= self.similarity_threshold:\n                    id1, id2 = chunk_ids[i], chunk_ids[j]\n                    if id1 not in self.similarity_graph:\n                        self.similarity_graph[id1] = set()\n                    if id2 not in self.similarity_graph:\n                        self.similarity_graph[id2] = set()\n                    self.similarity_graph[id1].add(id2)\n                    self.similarity_graph[id2].add(id1)\n\n    async def retrieve(\n        self,\n        query: str=\"\",\n        query_embedding: np.ndarray | None = None,\n        k: int = 5,\n        min_similarity: float = 0.2,\n        include_connected: bool = True\n    ) -&gt; list[Chunk]:\n        \"\"\"Enhanced retrieval with connected information\"\"\"\n        if query_embedding is None:\n            query_embedding = (await self._get_embeddings([query]))[0]\n        k = min(k, len(self.vdb.chunks)-1)\n        if k &lt;= 0:\n            return []\n        initial_results = self.vdb.search(query_embedding, k, min_similarity)\n\n        if not include_connected or not initial_results:\n            return initial_results\n\n        # Find connected chunks\n        connected_chunks = set()\n        for chunk in initial_results:\n            chunk_id = self.vdb.chunks.index(chunk)\n            if chunk_id in self.similarity_graph:\n                connected_chunks.update(self.similarity_graph[chunk_id])\n\n        # Add connected chunks to results\n        all_chunks = self.vdb.chunks\n        additional_results = [all_chunks[i] for i in connected_chunks\n                              if all_chunks[i] not in initial_results]\n\n        # Sort by similarity to query\n        all_results = initial_results + additional_results\n\n        return sorted(\n            all_results,\n            key=lambda x: np.dot(x.embedding, query_embedding),\n            reverse=True\n        )[:k * 2]  # Return more results when including connected information\n\n    async def forget_irrelevant(self, irrelevant_concepts: list[str], similarity_threshold: float | None=None) -&gt; int:\n        \"\"\"\n        Remove chunks similar to irrelevant concepts\n        Returns: Number of chunks removed\n        \"\"\"\n        if not irrelevant_concepts:\n            return 0\n\n        if similarity_threshold is None:\n            similarity_threshold = self.similarity_threshold\n\n        try:\n            irrelevant_embeddings = await self._get_embeddings(irrelevant_concepts)\n            initial_count = len(self.vdb.chunks)\n\n            def is_relevant(chunk: Chunk) -&gt; bool:\n                similarities = np.dot(chunk.embedding, irrelevant_embeddings.T)\n                do_keep = np.max(similarities) &lt; similarity_threshold\n                if do_keep:\n                    return True\n                for c in chunk.metadata.get(\"concepts\", []):\n                    if c in self.concept_extractor.concept_graph.concepts:\n                        del self.concept_extractor.concept_graph.concepts[c]\n                return False\n\n            relevant_chunks = [chunk for chunk in self.vdb.chunks if is_relevant(chunk)]\n            self.vdb.chunks = relevant_chunks\n            self.existing_hashes = {chunk.content_hash for chunk in self.vdb.chunks}\n            self.vdb.rebuild_index()\n\n\n            return initial_count - len(self.vdb.chunks)\n\n        except Exception as e:\n            get_logger().error(f\"Error forgetting irrelevant concepts: {str(e)}\")\n            raise\n\n    ## ----------------------------------------------------------------\n\n    def _cluster_chunks(\n        self,\n        chunks: list[Chunk],\n        query_embedding: np.ndarray | None = None,\n        min_cluster_size: int = 2,\n        min_samples: int = 1,\n        max_clusters: int = 10\n    ) -&gt; dict[int, list[Chunk]]:\n        \"\"\"\n        Enhanced clustering of chunks into topics with query awareness\n        and dynamic parameter adjustment\n        \"\"\"\n        if len(chunks) &lt; 2:\n            return {0: chunks}\n\n        embeddings = np.vstack([chunk.embedding for chunk in chunks])\n\n        # Normalize embeddings for cosine similarity\n        embeddings = normalize_vectors(embeddings)\n\n        # If query is provided, weight embeddings by query relevance\n        if query_embedding is not None:\n            query_similarities = np.dot(embeddings, query_embedding)\n            # Apply soft weighting to maintain structure while considering query relevance\n            embeddings = embeddings * query_similarities[:, np.newaxis]\n            embeddings = normalize_vectors(embeddings)\n\n        # Dynamic parameter adjustment based on dataset size\n        adjusted_min_cluster_size = max(\n            min_cluster_size,\n            min(len(chunks) // 10, 5)  # Scale with data size, max 5\n        )\n\n        adjusted_min_samples = max(\n            min_samples,\n            adjusted_min_cluster_size // 2\n        )\n\n        # Try different parameter combinations for optimal clustering\n        best_clusters = None\n        best_score = float('-inf')\n\n        epsilon_range = [0.2, 0.3, 0.4]\n\n        for epsilon in epsilon_range:\n            clusterer = HDBSCAN(\n                min_cluster_size=adjusted_min_cluster_size,\n                min_samples=adjusted_min_samples,\n                metric='cosine',\n                cluster_selection_epsilon=epsilon\n            )\n\n            cluster_labels = clusterer.fit_predict(embeddings)\n\n            # Skip if all points are noise\n            if len(set(cluster_labels)) &lt;= 1:\n                continue\n\n            # Calculate clustering quality metrics\n            score = self._evaluate_clustering(\n                embeddings,\n                cluster_labels,\n                query_embedding\n            )\n\n            if score &gt; best_score:\n                best_score = score\n                best_clusters = cluster_labels\n\n        # If no good clustering found, fall back to simpler approach\n        if best_clusters is None:\n            return self._fallback_clustering(chunks, query_embedding)\n\n        # Organize chunks by cluster\n        clusters: dict[int, list[Chunk]] = {}\n\n        # Sort clusters by size and relevance\n        cluster_scores = []\n\n        for label in set(best_clusters):\n            if label == -1:  # Handle noise points separately\n                continue\n\n            # Fixed: Use boolean mask to select chunks for current cluster\n            cluster_mask = best_clusters == label\n            cluster_chunks = [chunk for chunk, is_in_cluster in zip(chunks, cluster_mask, strict=False) if is_in_cluster]\n\n            # Skip empty clusters\n            if not cluster_chunks:\n                continue\n\n            # Calculate cluster score based on size and query relevance\n            score = len(cluster_chunks)\n            if query_embedding is not None:\n                cluster_embeddings = np.vstack([c.embedding for c in cluster_chunks])\n                query_relevance = np.mean(np.dot(cluster_embeddings, query_embedding))\n                score = score * (1 + query_relevance)  # Boost by relevance\n\n            cluster_scores.append((label, score, cluster_chunks))\n\n        # Sort clusters by score and limit to max_clusters\n        cluster_scores.sort(key=lambda x: x[1], reverse=True)\n\n        # Assign cleaned clusters\n        for i, (_, _, cluster_chunks) in enumerate(cluster_scores[:max_clusters]):\n            clusters[i] = cluster_chunks\n\n        # Handle noise points by assigning to nearest cluster\n        noise_chunks = [chunk for chunk, label in zip(chunks, best_clusters, strict=False) if label == -1]\n        if noise_chunks:\n            self._assign_noise_points(noise_chunks, clusters, query_embedding)\n\n        return clusters\n\n    @staticmethod\n    def _evaluate_clustering(\n        embeddings: np.ndarray,\n        labels: np.ndarray,\n        query_embedding: np.ndarray | None = None\n    ) -&gt; float:\n        \"\"\"\n        Evaluate clustering quality using multiple metrics\n        \"\"\"\n        if len(set(labels)) &lt;= 1:\n            return float('-inf')\n\n        # Calculate silhouette score for cluster cohesion\n        from sklearn.metrics import silhouette_score\n        try:\n            sil_score = silhouette_score(embeddings, labels, metric='cosine')\n        except:\n            sil_score = -1\n\n        # Calculate Davies-Bouldin score for cluster separation\n        from sklearn.metrics import davies_bouldin_score\n        try:\n            db_score = -davies_bouldin_score(embeddings, labels)  # Negated as lower is better\n        except:\n            db_score = -1\n\n        # Calculate query relevance if provided\n        query_score = 0\n        if query_embedding is not None:\n            unique_labels = set(labels) - {-1}\n            if unique_labels:\n                query_sims = []\n                for label in unique_labels:\n                    cluster_mask = labels == label\n                    cluster_embeddings = embeddings[cluster_mask]\n                    cluster_centroid = np.mean(cluster_embeddings, axis=0)\n                    query_sims.append(np.dot(cluster_centroid, query_embedding))\n                query_score = np.mean(query_sims)\n\n        # Combine scores with weights\n        combined_score = (\n            0.4 * sil_score +\n            0.3 * db_score +\n            0.3 * query_score\n        )\n\n        return combined_score\n\n    @staticmethod\n    def _fallback_clustering(\n        chunks: list[Chunk],\n        query_embedding: np.ndarray | None = None\n    ) -&gt; dict[int, list[Chunk]]:\n        \"\"\"\n        Simple fallback clustering when HDBSCAN fails\n        \"\"\"\n        if query_embedding is not None:\n            # Sort by query relevance\n            chunks_with_scores = [\n                (chunk, np.dot(chunk.embedding, query_embedding))\n                for chunk in chunks\n            ]\n            chunks_with_scores.sort(key=lambda x: x[1], reverse=True)\n            chunks = [c for c, _ in chunks_with_scores]\n\n        # Create fixed-size clusters\n        clusters = {}\n        cluster_size = max(2, len(chunks) // 5)\n\n        for i in range(0, len(chunks), cluster_size):\n            clusters[len(clusters)] = chunks[i:i + cluster_size]\n\n        return clusters\n\n    @staticmethod\n    def _assign_noise_points(\n        noise_chunks: list[Chunk],\n        clusters: dict[int, list[Chunk]],\n        query_embedding: np.ndarray | None = None\n    ) -&gt; None:\n        \"\"\"\n        Assign noise points to nearest clusters\n        \"\"\"\n        if not clusters:\n            clusters[0] = noise_chunks\n            return\n\n        for chunk in noise_chunks:\n            best_cluster = None\n            best_similarity = float('-inf')\n\n            for cluster_id, cluster_chunks in clusters.items():\n                cluster_embeddings = np.vstack([c.embedding for c in cluster_chunks])\n                cluster_centroid = np.mean(cluster_embeddings, axis=0)\n\n                similarity = np.dot(chunk.embedding, cluster_centroid)\n\n                # Consider query relevance in assignment if available\n                if query_embedding is not None:\n                    query_sim = np.dot(chunk.embedding, query_embedding)\n                    similarity = 0.7 * similarity + 0.3 * query_sim\n\n                if similarity &gt; best_similarity:\n                    best_similarity = similarity\n                    best_cluster = cluster_id\n\n            if best_cluster is not None:\n                clusters[best_cluster].append(chunk)\n\n    @staticmethod\n    def _generate_topic_summary(\n        chunks: list[Chunk],\n        query_embedding: np.ndarray,\n        max_sentences=3\n    ) -&gt; str:\n        \"\"\"Generate a summary for a topic using most representative chunks\"\"\"\n        if not chunks:\n            return \"\"\n\n        # Find chunks most similar to cluster centroid\n        embeddings = np.vstack([chunk.embedding for chunk in chunks])\n        centroid = embeddings.mean(axis=0)\n\n        # Calculate similarities to both centroid and query\n        centroid_sims = np.dot(embeddings, centroid)\n        query_sims = np.dot(embeddings, query_embedding)\n\n        # Combine both similarities\n        combined_sims = 0.7 * centroid_sims + 0.3 * query_sims\n\n        # Select top sentences from most representative chunks\n        top_indices = np.argsort(combined_sims)[-max_sentences:]\n        summary_chunks = [chunks[i] for i in top_indices]\n\n        # Extract key sentences\n        sentences = []\n        for chunk in summary_chunks:\n            sentences.extend(sent.strip() for sent in chunk.text.split('.') if sent.strip())\n\n        return '. '.join(sentences[:max_sentences]) + '.'\n\n    async def retrieve_with_overview(\n        self,\n        query: str,\n        query_embedding=None,\n        k: int = 5,\n        min_similarity: float = 0.2,\n        max_sentences: int = 5,\n        cross_ref_depth: int = 2,\n        max_cross_refs: int = 10  # New parameter to control cross-reference count\n    ) -&gt; RetrievalResult:\n        \"\"\"Enhanced retrieval with better cross-reference handling\"\"\"\n        # Get initial results with query embedding\n        if query_embedding is None:\n            query_embedding = (await self._get_embeddings([query]))[0]\n        initial_results = await self.retrieve(query_embedding=query_embedding, k=k, min_similarity=min_similarity)\n\n        if not initial_results:\n            return RetrievalResult([], [], {})\n\n        # Find cross-references with similarity scoring\n        initial_ids = {self.vdb.chunks.index(chunk) for chunk in initial_results}\n        related_ids = self._find_cross_references(\n            initial_ids,\n            depth=cross_ref_depth,\n            query_embedding=query_embedding  # Pass query embedding for relevance scoring\n        )\n\n        # Get all relevant chunks with smarter filtering\n        all_chunks = self.vdb.chunks\n        all_relevant_chunks = initial_results + [\n            chunk for i, chunk in enumerate(all_chunks)\n            if i in related_ids and self._is_relevant_cross_ref(\n                chunk,\n                query_embedding,\n                initial_results\n            )\n        ]\n\n        # Enhanced clustering with dynamic cluster size\n        clusters = self._cluster_chunks(\n            all_relevant_chunks,\n            query_embedding=query_embedding\n        )\n\n        # Fallback: If no clusters are found, treat all relevant chunks as a single cluster.\n        if not clusters:\n            print(\"No clusters found. Falling back to using all relevant chunks as a single cluster.\")\n            clusters = {0: all_relevant_chunks}\n\n        # Generate summaries and organize results\n        overview = []\n        cross_references = {}\n\n        for cluster_id, cluster_chunks in clusters.items():\n            summary = self._generate_topic_summary(\n                cluster_chunks,\n                query_embedding,\n                max_sentences=max_sentences  # Increased for more context\n            )\n\n            # Enhanced chunk sorting with combined scoring\n            sorted_chunks = self._sort_chunks_by_relevance(\n                cluster_chunks,\n                query_embedding,\n                initial_results\n            )\n\n            # Separate direct matches and cross-references\n            direct_matches_ = [{'text':c.text, 'metadata':c.metadata} for c in sorted_chunks if c in initial_results]\n            direct_matches = []\n            for match in direct_matches_:\n                if match in direct_matches:\n                    continue\n                direct_matches.append(match)\n            cross_refs_ = [c for c in sorted_chunks if c not in initial_results]\n            cross_refs = []\n            for match in cross_refs_:\n                if match in cross_refs:\n                    continue\n                cross_refs.append(match)\n            # Limit cross-references while maintaining diversity\n            selected_cross_refs = self._select_diverse_cross_refs(\n                cross_refs,\n                max_cross_refs,\n                query_embedding\n            )\n\n            topic_info = {\n                'topic_id': cluster_id,\n                'summary': summary,\n                'main_chunks': [x for x in direct_matches[:3]],\n                'chunk_count': len(cluster_chunks),\n                'relevance_score': self._calculate_topic_relevance(\n                    cluster_chunks,\n                    query_embedding\n                )\n            }\n            overview.append(topic_info)\n\n            if selected_cross_refs:\n                cross_references[f\"topic_{cluster_id}\"] = selected_cross_refs\n\n        # Sort overview by relevance score\n        overview.sort(key=lambda x: x['relevance_score'], reverse=True)\n\n        return RetrievalResult(\n            overview=overview,\n            details=initial_results,\n            cross_references=cross_references\n        )\n\n    def _find_cross_references(\n        self,\n        chunk_ids: set[int],\n        depth: int,\n        query_embedding: np.ndarray\n    ) -&gt; set[int]:\n        \"\"\"Enhanced cross-reference finding with relevance scoring\"\"\"\n        related_ids = set(chunk_ids)\n        current_depth = 0\n        frontier = set(chunk_ids)\n\n        while current_depth &lt; depth and frontier:\n            new_frontier = set()\n            for chunk_id in frontier:\n                if chunk_id in self.similarity_graph:\n                    # Score potential cross-references by relevance\n                    candidates = self.similarity_graph[chunk_id] - related_ids\n                    scored_candidates = [\n                        (cid, self._calculate_topic_relevance(\n                            [self.vdb.chunks[cid]],\n                            query_embedding\n                        ))\n                        for cid in candidates\n                    ]\n\n                    # Filter by relevance threshold\n                    relevant_candidates = {\n                        cid for cid, score in scored_candidates\n                        if score &gt; 0.5  # Adjustable threshold\n                    }\n                    new_frontier.update(relevant_candidates)\n\n            related_ids.update(new_frontier)\n            frontier = new_frontier\n            current_depth += 1\n\n        return related_ids\n\n    @staticmethod\n    def _is_relevant_cross_ref(\n        chunk: Chunk,\n        query_embedding: np.ndarray,\n        initial_results: list[Chunk]\n    ) -&gt; bool:\n        \"\"\"Determine if a cross-reference is relevant enough to include\"\"\"\n        # Calculate similarity to query\n        query_similarity = np.dot(chunk.embedding, query_embedding)\n\n        # Calculate similarity to initial results\n        initial_similarities = [\n            np.dot(chunk.embedding, r.embedding) for r in initial_results\n        ]\n        max_initial_similarity = max(initial_similarities)\n\n        # Combined relevance score\n        relevance_score = 0.7 * query_similarity + 0.3 * max_initial_similarity\n\n        return relevance_score &gt; 0.6  # Adjustable threshold\n\n    @staticmethod\n    def _select_diverse_cross_refs(\n        cross_refs: list[Chunk],\n        max_count: int,\n        query_embedding: np.ndarray\n    ) -&gt; list[Chunk]:\n        \"\"\"Select diverse and relevant cross-references\"\"\"\n        if not cross_refs or len(cross_refs) &lt;= max_count:\n            return cross_refs\n\n        # Calculate diversity scores\n        embeddings = np.vstack([c.embedding for c in cross_refs])\n        similarities = np.dot(embeddings, embeddings.T)\n\n        selected = []\n        remaining = list(enumerate(cross_refs))\n\n        while len(selected) &lt; max_count and remaining:\n            # Score remaining chunks by relevance and diversity\n            scores = []\n            for idx, chunk in remaining:\n                relevance = np.dot(chunk.embedding, query_embedding)\n                diversity = 1.0\n                if selected:\n                    # Calculate diversity penalty based on similarity to selected chunks\n                    selected_similarities = [\n                        similarities[idx][list(cross_refs).index(s)]\n                        for s in selected\n                    ]\n                    diversity = 1.0 - max(selected_similarities)\n\n                combined_score = 0.7 * relevance + 0.3 * diversity\n                scores.append((combined_score, idx, chunk))\n\n            # Select the highest scoring chunk\n            scores.sort(reverse=True)\n            _, idx, chunk = scores[0]\n            selected.append(chunk)\n            remaining = [(i, c) for i, c in remaining if i != idx]\n\n        return selected\n\n    @staticmethod\n    def _calculate_topic_relevance(\n        chunks: list[Chunk],\n        query_embedding: np.ndarray,\n    ) -&gt; float:\n        \"\"\"Calculate overall topic relevance score\"\"\"\n        if not chunks:\n            return 0.0\n\n        similarities = [\n            np.dot(chunk.embedding, query_embedding) for chunk in chunks\n        ]\n        return np.mean(similarities)\n\n    @staticmethod\n    def _sort_chunks_by_relevance(\n        chunks: list[Chunk],\n        query_embedding: np.ndarray,\n        initial_results: list[Chunk]\n    ) -&gt; list[Chunk]:\n        \"\"\"Sort chunks by combined relevance score\"\"\"\n        scored_chunks = []\n        for chunk in chunks:\n            query_similarity = np.dot(chunk.embedding, query_embedding)\n            initial_similarities = [\n                np.dot(chunk.embedding, r.embedding)\n                for r in initial_results\n            ]\n            max_initial_similarity = max(initial_similarities) if initial_similarities else 0\n\n            # Combined score favoring query relevance\n            combined_score = 0.7 * query_similarity + 0.3 * max_initial_similarity\n            scored_chunks.append((combined_score, chunk))\n\n        scored_chunks.sort(reverse=True)\n        return [chunk for _, chunk in scored_chunks]\n\n    async def query_concepts(self, query: str) -&gt; dict[str, any]:\n        \"\"\"Query concepts extracted from the knowledge base\"\"\"\n        return await self.concept_extractor.query_concepts(query)\n\n    async def unified_retrieve(\n        self,\n        query: str,\n        k: int = 5,\n        min_similarity: float = 0.2,\n        cross_ref_depth: int = 2,\n        max_cross_refs: int = 10,\n        max_sentences: int = 10\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Unified retrieval function that combines concept querying, retrieval with overview,\n        and basic retrieval, then generates a comprehensive summary using LLM.\n\n        Args:\n            query: Search query string\n            k: Number of primary results to retrieve\n            min_similarity: Minimum similarity threshold for retrieval\n            cross_ref_depth: Depth for cross-reference search\n            max_cross_refs: Maximum number of cross-references per topic\n            max_sentences: Maximum number Sentences in the main summary text\n\n        Returns:\n            Dictionary containing comprehensive results including summary and details\n        \"\"\"\n        # Get concept information\n        concept_results = await self.concept_extractor.query_concepts(query)\n\n        # Get retrieval overview\n\n        query_embedding = (await self._get_embeddings([query]))[0]\n        overview_results = await self.retrieve_with_overview(\n            query=query,\n            query_embedding=query_embedding,\n            k=k,\n            min_similarity=min_similarity,\n            cross_ref_depth=cross_ref_depth,\n            max_cross_refs=max_cross_refs,\n            max_sentences=max_sentences\n        )\n\n        # Get basic retrieval results\n        basic_results = await self.retrieve(\n            query_embedding=query_embedding,\n            k=k,\n            min_similarity=min_similarity\n        )\n        if len(basic_results) == 0:\n            return {}\n        if len(basic_results) == 1 and isinstance(basic_results[0], str) and basic_results[0].endswith('[]\\n - []\\n - []'):\n            return {}\n\n        # Prepare context for LLM summary\n        context = {\n            \"concepts\": {\n                \"main_concepts\": concept_results.get(\"concepts\", {}),\n                \"relationships\": concept_results.get(\"relationships\", []),\n                \"concept_groups\": concept_results.get(\"groups\", [])\n            },\n            \"topics\": [\n                {\n                    \"id\": topic[\"topic_id\"],\n                    \"summary\": topic[\"summary\"],\n                    \"relevance\": topic[\"relevance_score\"],\n                    \"chunk_count\": topic[\"chunk_count\"]\n                }\n                for topic in overview_results.overview\n            ],\n            \"key_chunks\": [\n                {\n                    \"text\": chunk.text,\n                    \"metadata\": chunk.metadata\n                }\n                for chunk in basic_results\n            ]\n        }\n\n        # Generate comprehensive summary using LLM\n        system_prompt = \"\"\"\n        Analyze the provided search results and generate a comprehensive summary\n        that includes:\n        1. Main concepts and their relationships\n        2. Key topics and their relevance\n        3. Most important findings and insights\n        4. Cross-references and connections between topics\n        5. Potential gaps or areas for further investigation\n\n        Format the response as a JSON object with these sections.\n        \"\"\"\n\n        prompt = f\"\"\"\n        Query: {query}\n\n        Context:\n        {json.dumps(context, indent=2)}\n\n        Generate a comprehensive analysis and summary following the structure:\n        \"\"\"\n\n        try:\n            await asyncio.sleep(0.25)\n            llm_response = await litellm_complete(\n                model_name=self.model_name,\n                prompt=prompt,\n                system_prompt=system_prompt,\n                response_format=DataModel,\n            )\n            summary_analysis = json.loads(llm_response)\n        except Exception as e:\n            get_logger().error(f\"Error generating summary: {str(e)}\")\n            summary_analysis = {\n                \"main_summary\": \"Error generating summary\",\n                \"error\": str(e)\n            }\n\n        # Compile final results\n        return {\n            \"summary\": summary_analysis,\n            \"raw_results\": {\n                \"concepts\": concept_results,\n                \"overview\": {\n                    \"topics\": overview_results.overview,\n                    \"cross_references\": overview_results.cross_references\n                },\n                \"relevant_chunks\": [\n                    {\n                        \"text\": chunk.text,\n                        \"metadata\": chunk.metadata,\n                        \"cluster_id\": chunk.cluster_id\n                    }\n                    for chunk in basic_results\n                ]\n            },\n            \"metadata\": {\n                \"query\": query,\n                \"timestamp\": time.time(),\n                \"retrieval_params\": {\n                    \"k\": k,\n                    \"min_similarity\": min_similarity,\n                    \"cross_ref_depth\": cross_ref_depth,\n                    \"max_cross_refs\": max_cross_refs\n                }\n            }\n        }\n\n    def save(self, path: str) -&gt; bytes | None:\n        \"\"\"\n        Save the complete knowledge base to disk, including all sub-components\n\n        Args:\n            path (str): Path where the knowledge base will be saved\n        \"\"\"\n        try:\n            data = {\n                # Core components\n                'vdb': self.vdb.save(),\n                'vis_kwargs': self.vis_kwargs,\n                'vis_class': self.vis_class,\n                'existing_hashes': self.existing_hashes,\n\n                # Configuration parameters\n                'embedding_dim': self.embedding_dim,\n                'similarity_threshold': self.similarity_threshold,\n                'batch_size': self.batch_size,\n                'n_clusters': self.n_clusters,\n                'deduplication_threshold': self.deduplication_threshold,\n                'model_name': self.model_name,\n                'embedding_model': self.embedding_model,\n\n                # Cache and graph data\n                'similarity_graph': self.similarity_graph,\n                'sto': self.sto,\n\n                # Text splitter configuration\n                'text_splitter_config': {\n                    'chunk_size': self.text_splitter.chunk_size,\n                    'chunk_overlap': self.text_splitter.chunk_overlap,\n                    'separator': self.text_splitter.separator\n                },\n\n                # Concept extractor data\n                'concept_graph': {\n                    'concepts': {\n                        name: {\n                            'name': concept.name,\n                            'category': concept.category,\n                            'relationships': {k: list(v) for k, v in concept.relationships.items()},\n                            'importance_score': concept.importance_score,\n                            'context_snippets': concept.context_snippets,\n                            'metadata': concept.metadata\n                        }\n                        for name, concept in self.concept_extractor.concept_graph.concepts.items()\n                    }\n                }\n            }\n            if path is None:\n                return pickle.dumps(data)\n            # Save to disk using pickle\n            with open(path, 'wb') as f:\n                pickle.dump(data, f)\n            print(f\"Knowledge base successfully saved to {path} with {len(self.concept_extractor.concept_graph.concepts.items())} concepts\")\n\n        except Exception as e:\n            print(f\"Error saving knowledge base: {str(e)}\")\n            raise\n    def init_vdb(self, db:AbstractVectorStore=AbstractVectorStore):\n        pass\n    @classmethod\n    def load(cls, path: str | bytes) -&gt; 'KnowledgeBase':\n        \"\"\"\n        Load a complete knowledge base from disk, including all sub-components\n\n        Args:\n            path (str): Path from where to load the knowledge base\n\n        Returns:\n            KnowledgeBase: A fully restored knowledge base instance\n        \"\"\"\n        try:\n            if isinstance(path, str):\n                # Load data from disk\n                with open(path, 'rb') as f:\n                    data = pickle.load(f)\n            elif isinstance(path, bytes):\n                data = pickle.loads(path)\n            else:\n                raise ValueError(\"Invalid path type\")\n\n            # Create new knowledge base instance with saved configuration\n            kb = cls(\n                embedding_dim=data['embedding_dim'],\n                similarity_threshold=data['similarity_threshold'],\n                batch_size=data['batch_size'],\n                n_clusters=data['n_clusters'],\n                deduplication_threshold=data['deduplication_threshold'],\n                model_name=data['model_name'],\n                embedding_model=data['embedding_model']\n            )\n\n            # Restore core components\n            kb.init_vis(data.get('vis_class'), data.get('vis_kwargs'))\n            kb.existing_hashes = data['existing_hashes']\n\n            # Restore cache and graph data\n            kb.similarity_graph = data.get('similarity_graph', {})\n            kb.sto = data.get('sto', [])\n\n            # Restore text splitter configuration\n            splitter_config = data.get('text_splitter_config', {})\n            kb.text_splitter = TextSplitter(\n                chunk_size=splitter_config.get('chunk_size', 12_000),\n                chunk_overlap=splitter_config.get('chunk_overlap', 200),\n                separator=splitter_config.get('separator', '\\n')\n            )\n\n            # Restore concept graph\n            concept_data = data.get('concept_graph', {}).get('concepts', {})\n            for concept_info in concept_data.values():\n                concept = Concept(\n                    name=concept_info['name'],\n                    category=concept_info['category'],\n                    relationships={k: set(v) for k, v in concept_info['relationships'].items()},\n                    importance_score=concept_info['importance_score'],\n                    context_snippets=concept_info['context_snippets'],\n                    metadata=concept_info['metadata']\n                )\n                kb.concept_extractor.concept_graph.add_concept(concept)\n\n            print(f\"Knowledge base successfully loaded from {path} with {len(concept_data)} concepts\")\n            return kb\n\n        except Exception as e:\n            print(f\"Error loading knowledge base: {str(e)}\")\n            raise\n\n    def vis(self,output_file: str = \"concept_graph.html\", get_output_html=False, get_output_net=False):\n        if not self.concept_extractor.concept_graph.concepts:\n            print(\"NO Concepts defined\")\n            return None\n        net = self.concept_extractor.concept_graph.convert_to_networkx()\n        if get_output_net:\n            return net\n        return GraphVisualizer.visualize(net, output_file=output_file, get_output=get_output_html)\n</code></pre> <code>__init__(embedding_dim=768, similarity_threshold=0.61, batch_size=64, n_clusters=4, deduplication_threshold=0.85, model_name=os.getenv('DEFAULTMODELSUMMERY'), embedding_model=os.getenv('DEFAULTMODELEMBEDDING'), vis_class='FaissVectorStore', vis_kwargs=None, requests_per_second=85.0, chunk_size=3600, chunk_overlap=130, separator='\\n')</code> \u00b6 <p>Initialize the knowledge base with given parameters</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def __init__(self, embedding_dim: int = 768, similarity_threshold: float = 0.61, batch_size: int = 64,\n             n_clusters: int = 4, deduplication_threshold: float = 0.85, model_name=os.getenv(\"DEFAULTMODELSUMMERY\"),\n             embedding_model=os.getenv(\"DEFAULTMODELEMBEDDING\"),\n             vis_class:str | None = \"FaissVectorStore\",\n             vis_kwargs:dict[str, Any] | None=None,\n             requests_per_second=85.,\n             chunk_size: int = 3600,\n             chunk_overlap: int = 130,\n             separator: str = \"\\n\"\n             ):\n    \"\"\"Initialize the knowledge base with given parameters\"\"\"\n\n    self.existing_hashes: set[str] = set()\n    self.embedding_model = embedding_model\n    self.embedding_dim = embedding_dim\n    self.similarity_threshold = similarity_threshold\n    self.deduplication_threshold = deduplication_threshold\n    if model_name == \"openrouter/mistralai/mistral-nemo\":\n        batch_size = 9\n        requests_per_second = 1.5\n    self.batch_size = batch_size\n    self.n_clusters = n_clusters\n    self.model_name = model_name\n    self.sto: list = []\n\n    self.text_splitter = TextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap, separator=separator)\n    self.similarity_graph = {}\n    self.concept_extractor = ConceptExtractor(self, requests_per_second)\n\n    self.vis_class = None\n    self.vis_kwargs = None\n    self.vdb = None\n    self.init_vis(vis_class, vis_kwargs)\n</code></pre> <code>add_data(texts, metadata=None)</code> <code>async</code> \u00b6 <p>Enhanced version with smart splitting and clustering</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def add_data(\n    self,\n    texts: list[str],\n    metadata: list[dict[str, Any]] | None = None,\n) -&gt; tuple[int, int]:\n    \"\"\"Enhanced version with smart splitting and clustering\"\"\"\n    if isinstance(texts, str):\n        texts = [texts]\n    if metadata is None:\n        metadata = [{}] * len(texts)\n    if isinstance(metadata, dict):\n        metadata = [metadata]\n    if len(texts) != len(metadata):\n        raise ValueError(\"Length of texts and metadata must match\")\n    if len(texts) == 1 and len(texts[0]) &lt; 10_000:\n        if len(self.sto) &lt; self.batch_size and len(texts) == 1:\n            self.sto.append((texts[0], metadata[0]))\n            return -1, -1\n        if len(self.sto) &gt;= self.batch_size:\n            _ = [texts.append(t) or metadata.append([m]) for (t, m) in self.sto]\n            self.sto = []\n\n    # Split large texts\n    split_texts = []\n    split_metadata = []\n\n    while Spinner(\"Saving Data to Memory\", symbols='t'):\n\n        for idx, text in enumerate(texts):\n            chunks = self.text_splitter.split_text(text)\n            split_texts.extend(chunks)\n\n            # Adjust metadata for splits\n            meta = metadata[idx] if metadata else {}\n            if isinstance(meta, list):\n                meta = meta[0]\n            for i, _chunk in enumerate(chunks):\n                chunk_meta = meta.copy()\n                chunk_meta.update({\n                    'chunk_index': i,\n                    'total_chunks': len(chunks),\n                    'original_text_id': idx\n                })\n                split_metadata.append(chunk_meta)\n\n        return await self._add_data(split_texts, split_metadata)\n</code></pre> <code>compute_hash(text)</code> <code>staticmethod</code> \u00b6 <p>Compute SHA-256 hash of text</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>@staticmethod\ndef compute_hash(text: str) -&gt; str:\n    \"\"\"Compute SHA-256 hash of text\"\"\"\n    return hashlib.sha256(text.encode('utf-8', errors='ignore')).hexdigest()\n</code></pre> <code>forget_irrelevant(irrelevant_concepts, similarity_threshold=None)</code> <code>async</code> \u00b6 <p>Remove chunks similar to irrelevant concepts Returns: Number of chunks removed</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def forget_irrelevant(self, irrelevant_concepts: list[str], similarity_threshold: float | None=None) -&gt; int:\n    \"\"\"\n    Remove chunks similar to irrelevant concepts\n    Returns: Number of chunks removed\n    \"\"\"\n    if not irrelevant_concepts:\n        return 0\n\n    if similarity_threshold is None:\n        similarity_threshold = self.similarity_threshold\n\n    try:\n        irrelevant_embeddings = await self._get_embeddings(irrelevant_concepts)\n        initial_count = len(self.vdb.chunks)\n\n        def is_relevant(chunk: Chunk) -&gt; bool:\n            similarities = np.dot(chunk.embedding, irrelevant_embeddings.T)\n            do_keep = np.max(similarities) &lt; similarity_threshold\n            if do_keep:\n                return True\n            for c in chunk.metadata.get(\"concepts\", []):\n                if c in self.concept_extractor.concept_graph.concepts:\n                    del self.concept_extractor.concept_graph.concepts[c]\n            return False\n\n        relevant_chunks = [chunk for chunk in self.vdb.chunks if is_relevant(chunk)]\n        self.vdb.chunks = relevant_chunks\n        self.existing_hashes = {chunk.content_hash for chunk in self.vdb.chunks}\n        self.vdb.rebuild_index()\n\n\n        return initial_count - len(self.vdb.chunks)\n\n    except Exception as e:\n        get_logger().error(f\"Error forgetting irrelevant concepts: {str(e)}\")\n        raise\n</code></pre> <code>load(path)</code> <code>classmethod</code> \u00b6 <p>Load a complete knowledge base from disk, including all sub-components</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path from where to load the knowledge base</p> required <p>Returns:</p> Name Type Description <code>KnowledgeBase</code> <code>KnowledgeBase</code> <p>A fully restored knowledge base instance</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>@classmethod\ndef load(cls, path: str | bytes) -&gt; 'KnowledgeBase':\n    \"\"\"\n    Load a complete knowledge base from disk, including all sub-components\n\n    Args:\n        path (str): Path from where to load the knowledge base\n\n    Returns:\n        KnowledgeBase: A fully restored knowledge base instance\n    \"\"\"\n    try:\n        if isinstance(path, str):\n            # Load data from disk\n            with open(path, 'rb') as f:\n                data = pickle.load(f)\n        elif isinstance(path, bytes):\n            data = pickle.loads(path)\n        else:\n            raise ValueError(\"Invalid path type\")\n\n        # Create new knowledge base instance with saved configuration\n        kb = cls(\n            embedding_dim=data['embedding_dim'],\n            similarity_threshold=data['similarity_threshold'],\n            batch_size=data['batch_size'],\n            n_clusters=data['n_clusters'],\n            deduplication_threshold=data['deduplication_threshold'],\n            model_name=data['model_name'],\n            embedding_model=data['embedding_model']\n        )\n\n        # Restore core components\n        kb.init_vis(data.get('vis_class'), data.get('vis_kwargs'))\n        kb.existing_hashes = data['existing_hashes']\n\n        # Restore cache and graph data\n        kb.similarity_graph = data.get('similarity_graph', {})\n        kb.sto = data.get('sto', [])\n\n        # Restore text splitter configuration\n        splitter_config = data.get('text_splitter_config', {})\n        kb.text_splitter = TextSplitter(\n            chunk_size=splitter_config.get('chunk_size', 12_000),\n            chunk_overlap=splitter_config.get('chunk_overlap', 200),\n            separator=splitter_config.get('separator', '\\n')\n        )\n\n        # Restore concept graph\n        concept_data = data.get('concept_graph', {}).get('concepts', {})\n        for concept_info in concept_data.values():\n            concept = Concept(\n                name=concept_info['name'],\n                category=concept_info['category'],\n                relationships={k: set(v) for k, v in concept_info['relationships'].items()},\n                importance_score=concept_info['importance_score'],\n                context_snippets=concept_info['context_snippets'],\n                metadata=concept_info['metadata']\n            )\n            kb.concept_extractor.concept_graph.add_concept(concept)\n\n        print(f\"Knowledge base successfully loaded from {path} with {len(concept_data)} concepts\")\n        return kb\n\n    except Exception as e:\n        print(f\"Error loading knowledge base: {str(e)}\")\n        raise\n</code></pre> <code>query_concepts(query)</code> <code>async</code> \u00b6 <p>Query concepts extracted from the knowledge base</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def query_concepts(self, query: str) -&gt; dict[str, any]:\n    \"\"\"Query concepts extracted from the knowledge base\"\"\"\n    return await self.concept_extractor.query_concepts(query)\n</code></pre> <code>retrieve(query='', query_embedding=None, k=5, min_similarity=0.2, include_connected=True)</code> <code>async</code> \u00b6 <p>Enhanced retrieval with connected information</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def retrieve(\n    self,\n    query: str=\"\",\n    query_embedding: np.ndarray | None = None,\n    k: int = 5,\n    min_similarity: float = 0.2,\n    include_connected: bool = True\n) -&gt; list[Chunk]:\n    \"\"\"Enhanced retrieval with connected information\"\"\"\n    if query_embedding is None:\n        query_embedding = (await self._get_embeddings([query]))[0]\n    k = min(k, len(self.vdb.chunks)-1)\n    if k &lt;= 0:\n        return []\n    initial_results = self.vdb.search(query_embedding, k, min_similarity)\n\n    if not include_connected or not initial_results:\n        return initial_results\n\n    # Find connected chunks\n    connected_chunks = set()\n    for chunk in initial_results:\n        chunk_id = self.vdb.chunks.index(chunk)\n        if chunk_id in self.similarity_graph:\n            connected_chunks.update(self.similarity_graph[chunk_id])\n\n    # Add connected chunks to results\n    all_chunks = self.vdb.chunks\n    additional_results = [all_chunks[i] for i in connected_chunks\n                          if all_chunks[i] not in initial_results]\n\n    # Sort by similarity to query\n    all_results = initial_results + additional_results\n\n    return sorted(\n        all_results,\n        key=lambda x: np.dot(x.embedding, query_embedding),\n        reverse=True\n    )[:k * 2]  # Return more results when including connected information\n</code></pre> <code>retrieve_with_overview(query, query_embedding=None, k=5, min_similarity=0.2, max_sentences=5, cross_ref_depth=2, max_cross_refs=10)</code> <code>async</code> \u00b6 <p>Enhanced retrieval with better cross-reference handling</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def retrieve_with_overview(\n    self,\n    query: str,\n    query_embedding=None,\n    k: int = 5,\n    min_similarity: float = 0.2,\n    max_sentences: int = 5,\n    cross_ref_depth: int = 2,\n    max_cross_refs: int = 10  # New parameter to control cross-reference count\n) -&gt; RetrievalResult:\n    \"\"\"Enhanced retrieval with better cross-reference handling\"\"\"\n    # Get initial results with query embedding\n    if query_embedding is None:\n        query_embedding = (await self._get_embeddings([query]))[0]\n    initial_results = await self.retrieve(query_embedding=query_embedding, k=k, min_similarity=min_similarity)\n\n    if not initial_results:\n        return RetrievalResult([], [], {})\n\n    # Find cross-references with similarity scoring\n    initial_ids = {self.vdb.chunks.index(chunk) for chunk in initial_results}\n    related_ids = self._find_cross_references(\n        initial_ids,\n        depth=cross_ref_depth,\n        query_embedding=query_embedding  # Pass query embedding for relevance scoring\n    )\n\n    # Get all relevant chunks with smarter filtering\n    all_chunks = self.vdb.chunks\n    all_relevant_chunks = initial_results + [\n        chunk for i, chunk in enumerate(all_chunks)\n        if i in related_ids and self._is_relevant_cross_ref(\n            chunk,\n            query_embedding,\n            initial_results\n        )\n    ]\n\n    # Enhanced clustering with dynamic cluster size\n    clusters = self._cluster_chunks(\n        all_relevant_chunks,\n        query_embedding=query_embedding\n    )\n\n    # Fallback: If no clusters are found, treat all relevant chunks as a single cluster.\n    if not clusters:\n        print(\"No clusters found. Falling back to using all relevant chunks as a single cluster.\")\n        clusters = {0: all_relevant_chunks}\n\n    # Generate summaries and organize results\n    overview = []\n    cross_references = {}\n\n    for cluster_id, cluster_chunks in clusters.items():\n        summary = self._generate_topic_summary(\n            cluster_chunks,\n            query_embedding,\n            max_sentences=max_sentences  # Increased for more context\n        )\n\n        # Enhanced chunk sorting with combined scoring\n        sorted_chunks = self._sort_chunks_by_relevance(\n            cluster_chunks,\n            query_embedding,\n            initial_results\n        )\n\n        # Separate direct matches and cross-references\n        direct_matches_ = [{'text':c.text, 'metadata':c.metadata} for c in sorted_chunks if c in initial_results]\n        direct_matches = []\n        for match in direct_matches_:\n            if match in direct_matches:\n                continue\n            direct_matches.append(match)\n        cross_refs_ = [c for c in sorted_chunks if c not in initial_results]\n        cross_refs = []\n        for match in cross_refs_:\n            if match in cross_refs:\n                continue\n            cross_refs.append(match)\n        # Limit cross-references while maintaining diversity\n        selected_cross_refs = self._select_diverse_cross_refs(\n            cross_refs,\n            max_cross_refs,\n            query_embedding\n        )\n\n        topic_info = {\n            'topic_id': cluster_id,\n            'summary': summary,\n            'main_chunks': [x for x in direct_matches[:3]],\n            'chunk_count': len(cluster_chunks),\n            'relevance_score': self._calculate_topic_relevance(\n                cluster_chunks,\n                query_embedding\n            )\n        }\n        overview.append(topic_info)\n\n        if selected_cross_refs:\n            cross_references[f\"topic_{cluster_id}\"] = selected_cross_refs\n\n    # Sort overview by relevance score\n    overview.sort(key=lambda x: x['relevance_score'], reverse=True)\n\n    return RetrievalResult(\n        overview=overview,\n        details=initial_results,\n        cross_references=cross_references\n    )\n</code></pre> <code>save(path)</code> \u00b6 <p>Save the complete knowledge base to disk, including all sub-components</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path where the knowledge base will be saved</p> required Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def save(self, path: str) -&gt; bytes | None:\n    \"\"\"\n    Save the complete knowledge base to disk, including all sub-components\n\n    Args:\n        path (str): Path where the knowledge base will be saved\n    \"\"\"\n    try:\n        data = {\n            # Core components\n            'vdb': self.vdb.save(),\n            'vis_kwargs': self.vis_kwargs,\n            'vis_class': self.vis_class,\n            'existing_hashes': self.existing_hashes,\n\n            # Configuration parameters\n            'embedding_dim': self.embedding_dim,\n            'similarity_threshold': self.similarity_threshold,\n            'batch_size': self.batch_size,\n            'n_clusters': self.n_clusters,\n            'deduplication_threshold': self.deduplication_threshold,\n            'model_name': self.model_name,\n            'embedding_model': self.embedding_model,\n\n            # Cache and graph data\n            'similarity_graph': self.similarity_graph,\n            'sto': self.sto,\n\n            # Text splitter configuration\n            'text_splitter_config': {\n                'chunk_size': self.text_splitter.chunk_size,\n                'chunk_overlap': self.text_splitter.chunk_overlap,\n                'separator': self.text_splitter.separator\n            },\n\n            # Concept extractor data\n            'concept_graph': {\n                'concepts': {\n                    name: {\n                        'name': concept.name,\n                        'category': concept.category,\n                        'relationships': {k: list(v) for k, v in concept.relationships.items()},\n                        'importance_score': concept.importance_score,\n                        'context_snippets': concept.context_snippets,\n                        'metadata': concept.metadata\n                    }\n                    for name, concept in self.concept_extractor.concept_graph.concepts.items()\n                }\n            }\n        }\n        if path is None:\n            return pickle.dumps(data)\n        # Save to disk using pickle\n        with open(path, 'wb') as f:\n            pickle.dump(data, f)\n        print(f\"Knowledge base successfully saved to {path} with {len(self.concept_extractor.concept_graph.concepts.items())} concepts\")\n\n    except Exception as e:\n        print(f\"Error saving knowledge base: {str(e)}\")\n        raise\n</code></pre> <code>unified_retrieve(query, k=5, min_similarity=0.2, cross_ref_depth=2, max_cross_refs=10, max_sentences=10)</code> <code>async</code> \u00b6 <p>Unified retrieval function that combines concept querying, retrieval with overview, and basic retrieval, then generates a comprehensive summary using LLM.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query string</p> required <code>k</code> <code>int</code> <p>Number of primary results to retrieve</p> <code>5</code> <code>min_similarity</code> <code>float</code> <p>Minimum similarity threshold for retrieval</p> <code>0.2</code> <code>cross_ref_depth</code> <code>int</code> <p>Depth for cross-reference search</p> <code>2</code> <code>max_cross_refs</code> <code>int</code> <p>Maximum number of cross-references per topic</p> <code>10</code> <code>max_sentences</code> <code>int</code> <p>Maximum number Sentences in the main summary text</p> <code>10</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing comprehensive results including summary and details</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def unified_retrieve(\n    self,\n    query: str,\n    k: int = 5,\n    min_similarity: float = 0.2,\n    cross_ref_depth: int = 2,\n    max_cross_refs: int = 10,\n    max_sentences: int = 10\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Unified retrieval function that combines concept querying, retrieval with overview,\n    and basic retrieval, then generates a comprehensive summary using LLM.\n\n    Args:\n        query: Search query string\n        k: Number of primary results to retrieve\n        min_similarity: Minimum similarity threshold for retrieval\n        cross_ref_depth: Depth for cross-reference search\n        max_cross_refs: Maximum number of cross-references per topic\n        max_sentences: Maximum number Sentences in the main summary text\n\n    Returns:\n        Dictionary containing comprehensive results including summary and details\n    \"\"\"\n    # Get concept information\n    concept_results = await self.concept_extractor.query_concepts(query)\n\n    # Get retrieval overview\n\n    query_embedding = (await self._get_embeddings([query]))[0]\n    overview_results = await self.retrieve_with_overview(\n        query=query,\n        query_embedding=query_embedding,\n        k=k,\n        min_similarity=min_similarity,\n        cross_ref_depth=cross_ref_depth,\n        max_cross_refs=max_cross_refs,\n        max_sentences=max_sentences\n    )\n\n    # Get basic retrieval results\n    basic_results = await self.retrieve(\n        query_embedding=query_embedding,\n        k=k,\n        min_similarity=min_similarity\n    )\n    if len(basic_results) == 0:\n        return {}\n    if len(basic_results) == 1 and isinstance(basic_results[0], str) and basic_results[0].endswith('[]\\n - []\\n - []'):\n        return {}\n\n    # Prepare context for LLM summary\n    context = {\n        \"concepts\": {\n            \"main_concepts\": concept_results.get(\"concepts\", {}),\n            \"relationships\": concept_results.get(\"relationships\", []),\n            \"concept_groups\": concept_results.get(\"groups\", [])\n        },\n        \"topics\": [\n            {\n                \"id\": topic[\"topic_id\"],\n                \"summary\": topic[\"summary\"],\n                \"relevance\": topic[\"relevance_score\"],\n                \"chunk_count\": topic[\"chunk_count\"]\n            }\n            for topic in overview_results.overview\n        ],\n        \"key_chunks\": [\n            {\n                \"text\": chunk.text,\n                \"metadata\": chunk.metadata\n            }\n            for chunk in basic_results\n        ]\n    }\n\n    # Generate comprehensive summary using LLM\n    system_prompt = \"\"\"\n    Analyze the provided search results and generate a comprehensive summary\n    that includes:\n    1. Main concepts and their relationships\n    2. Key topics and their relevance\n    3. Most important findings and insights\n    4. Cross-references and connections between topics\n    5. Potential gaps or areas for further investigation\n\n    Format the response as a JSON object with these sections.\n    \"\"\"\n\n    prompt = f\"\"\"\n    Query: {query}\n\n    Context:\n    {json.dumps(context, indent=2)}\n\n    Generate a comprehensive analysis and summary following the structure:\n    \"\"\"\n\n    try:\n        await asyncio.sleep(0.25)\n        llm_response = await litellm_complete(\n            model_name=self.model_name,\n            prompt=prompt,\n            system_prompt=system_prompt,\n            response_format=DataModel,\n        )\n        summary_analysis = json.loads(llm_response)\n    except Exception as e:\n        get_logger().error(f\"Error generating summary: {str(e)}\")\n        summary_analysis = {\n            \"main_summary\": \"Error generating summary\",\n            \"error\": str(e)\n        }\n\n    # Compile final results\n    return {\n        \"summary\": summary_analysis,\n        \"raw_results\": {\n            \"concepts\": concept_results,\n            \"overview\": {\n                \"topics\": overview_results.overview,\n                \"cross_references\": overview_results.cross_references\n            },\n            \"relevant_chunks\": [\n                {\n                    \"text\": chunk.text,\n                    \"metadata\": chunk.metadata,\n                    \"cluster_id\": chunk.cluster_id\n                }\n                for chunk in basic_results\n            ]\n        },\n        \"metadata\": {\n            \"query\": query,\n            \"timestamp\": time.time(),\n            \"retrieval_params\": {\n                \"k\": k,\n                \"min_similarity\": min_similarity,\n                \"cross_ref_depth\": cross_ref_depth,\n                \"max_cross_refs\": max_cross_refs\n            }\n        }\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.RelevanceAssessment","title":"<code>RelevanceAssessment</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an assessment of the relevance of the data in relation to a specific query.</p> <p>Attributes:</p> Name Type Description <code>query_alignment</code> <code>float</code> <p>A float representing the alignment between the query and the data.</p> <code>confidence_score</code> <code>float</code> <p>A float indicating the confidence level in the alignment.</p> <code>coverage_analysis</code> <code>str</code> <p>A textual description analyzing the data coverage.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class RelevanceAssessment(BaseModel):\n    \"\"\"\n    Represents an assessment of the relevance of the data in relation to a specific query.\n\n    Attributes:\n        query_alignment (float): A float representing the alignment between the query and the data.\n        confidence_score (float): A float indicating the confidence level in the alignment.\n        coverage_analysis (str): A textual description analyzing the data coverage.\n    \"\"\"\n    query_alignment: float\n    confidence_score: float\n    coverage_analysis: str\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.RetrievalResult","title":"<code>RetrievalResult</code>  <code>dataclass</code>","text":"<p>Structure for organizing retrieval results</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>@dataclass\nclass RetrievalResult:\n    \"\"\"Structure for organizing retrieval results\"\"\"\n    overview: list[dict[str, any]]  # List of topic summaries\n    details: list[Chunk]  # Detailed chunks\n    cross_references: dict[str, list[Chunk]]  # Related chunks by topic\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.TConcept","title":"<code>TConcept</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the criteria or target parameters for concept selection and filtering.</p> <p>Attributes:</p> Name Type Description <code>min_importance</code> <code>float</code> <p>The minimum importance score a concept must have to be considered.</p> <code>target_concepts</code> <code>List[str]</code> <p>A list of names of target concepts to focus on.</p> <code>relationship_types</code> <code>List[str]</code> <p>A list of relationship types to be considered in the analysis.</p> <code>categories</code> <code>List[str]</code> <p>A list of concept categories to filter or group the concepts.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class TConcept(BaseModel):\n    \"\"\"\n    Represents the criteria or target parameters for concept selection and filtering.\n\n    Attributes:\n        min_importance (float): The minimum importance score a concept must have to be considered.\n        target_concepts (List[str]): A list of names of target concepts to focus on.\n        relationship_types (List[str]): A list of relationship types to be considered in the analysis.\n        categories (List[str]): A list of concept categories to filter or group the concepts.\n    \"\"\"\n    min_importance: float\n    target_concepts: list[str]\n    relationship_types: list[str]\n    categories: list[str]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.TextSplitter","title":"<code>TextSplitter</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class TextSplitter:\n    def __init__(\n        self,\n        chunk_size: int = 3600,\n        chunk_overlap: int = 130,\n        separator: str = \"\\n\"\n    ):\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.separator = separator\n\n    def approximate(self, text_len: int) -&gt; float:\n        \"\"\"\n        Approximate the number of chunks and average chunk size for a given text length\n\n        Args:\n            text_len (int): Length of the text to be split\n\n        Returns:\n            Tuple[int, int]: (number_of_chunks, approximate_chunk_size)\n        \"\"\"\n        if text_len &lt;= self.chunk_size:\n            return 1, text_len\n\n        # Handle extreme overlap cases\n        if self.chunk_overlap &gt;= self.chunk_size:\n            estimated_chunks = text_len\n            return estimated_chunks, 1\n\n        # Calculate based on overlap ratio\n        overlap_ratio = self.chunk_overlap / self.chunk_size\n        base_chunks = text_len / self.chunk_size\n        estimated_chunks = base_chunks * 2 / (overlap_ratio if overlap_ratio &gt; 0 else 1)\n\n        # print('#',estimated_chunks, base_chunks, overlap_ratio)\n        # Calculate average chunk size\n        avg_chunk_size = max(1, text_len / estimated_chunks)\n\n        return estimated_chunks * avg_chunk_size\n\n    def split_text(self, text: str) -&gt; list[str]:\n        \"\"\"Split text into chunks with overlap\"\"\"\n        # Clean and normalize text\n        text = re.sub(r'\\s+', ' ', text).strip()\n\n        # If text is shorter than chunk_size, return as is\n        if len(text) &lt;= self.chunk_size:\n            return [text]\n\n        chunks = []\n        start = 0\n\n        while start &lt; len(text):\n            # Find end of chunk\n            end = start + self.chunk_size\n\n            if end &gt;= len(text):\n                chunks.append(text[start:])\n                break\n\n            # Try to find a natural break point\n            last_separator = text.rfind(self.separator, start, end)\n            if last_separator != -1:\n                end = last_separator\n\n            # Add chunk\n            chunks.append(text[start:end])\n\n            # Calculate allowed overlap for this chunk\n            chunk_length = end - start\n            allowed_overlap = min(self.chunk_overlap, chunk_length - 1)\n\n            # Move start position considering adjusted overlap\n            start = end - allowed_overlap\n\n        return chunks\n</code></pre> <code>approximate(text_len)</code> \u00b6 <p>Approximate the number of chunks and average chunk size for a given text length</p> <p>Parameters:</p> Name Type Description Default <code>text_len</code> <code>int</code> <p>Length of the text to be split</p> required <p>Returns:</p> Type Description <code>float</code> <p>Tuple[int, int]: (number_of_chunks, approximate_chunk_size)</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def approximate(self, text_len: int) -&gt; float:\n    \"\"\"\n    Approximate the number of chunks and average chunk size for a given text length\n\n    Args:\n        text_len (int): Length of the text to be split\n\n    Returns:\n        Tuple[int, int]: (number_of_chunks, approximate_chunk_size)\n    \"\"\"\n    if text_len &lt;= self.chunk_size:\n        return 1, text_len\n\n    # Handle extreme overlap cases\n    if self.chunk_overlap &gt;= self.chunk_size:\n        estimated_chunks = text_len\n        return estimated_chunks, 1\n\n    # Calculate based on overlap ratio\n    overlap_ratio = self.chunk_overlap / self.chunk_size\n    base_chunks = text_len / self.chunk_size\n    estimated_chunks = base_chunks * 2 / (overlap_ratio if overlap_ratio &gt; 0 else 1)\n\n    # print('#',estimated_chunks, base_chunks, overlap_ratio)\n    # Calculate average chunk size\n    avg_chunk_size = max(1, text_len / estimated_chunks)\n\n    return estimated_chunks * avg_chunk_size\n</code></pre> <code>split_text(text)</code> \u00b6 <p>Split text into chunks with overlap</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def split_text(self, text: str) -&gt; list[str]:\n    \"\"\"Split text into chunks with overlap\"\"\"\n    # Clean and normalize text\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    # If text is shorter than chunk_size, return as is\n    if len(text) &lt;= self.chunk_size:\n        return [text]\n\n    chunks = []\n    start = 0\n\n    while start &lt; len(text):\n        # Find end of chunk\n        end = start + self.chunk_size\n\n        if end &gt;= len(text):\n            chunks.append(text[start:])\n            break\n\n        # Try to find a natural break point\n        last_separator = text.rfind(self.separator, start, end)\n        if last_separator != -1:\n            end = last_separator\n\n        # Add chunk\n        chunks.append(text[start:end])\n\n        # Calculate allowed overlap for this chunk\n        chunk_length = end - start\n        allowed_overlap = min(self.chunk_overlap, chunk_length - 1)\n\n        # Move start position considering adjusted overlap\n        start = end - allowed_overlap\n\n    return chunks\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.TopicInsights","title":"<code>TopicInsights</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents insights related to various topics.</p> <p>Attributes:</p> Name Type Description <code>primary_topics</code> <code>list[str]</code> <p>A list of main topics addressed.</p> <code>cross_references</code> <code>list[str]</code> <p>A list of cross-references that connect different topics.</p> <code>knowledge_gaps</code> <code>list[str]</code> <p>A list of identified gaps in the current knowledge.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class TopicInsights(BaseModel):\n    \"\"\"\n    Represents insights related to various topics.\n\n    Attributes:\n        primary_topics (list[str]): A list of main topics addressed.\n        cross_references (list[str]): A list of cross-references that connect different topics.\n        knowledge_gaps (list[str]): A list of identified gaps in the current knowledge.\n    \"\"\"\n    primary_topics: list[str]\n    cross_references: list[str]\n    knowledge_gaps: list[str]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.rConcept","title":"<code>rConcept</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a key concept with its relationships and associated metadata.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the concept.</p> <code>category</code> <code>str</code> <p>The category of the concept (e.g., 'technical', 'domain', 'method', etc.).</p> <code>relationships</code> <code>Dict[str, List[str]]</code> <p>A mapping where each key is a type of relationship and the value is a list of related concept names.</p> <code>importance_score</code> <code>float</code> <p>A numerical score representing the importance or relevance of the concept.</p> <code>context_snippets</code> <code>List[str]</code> <p>A list of text snippets providing context where the concept appears.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class rConcept(BaseModel):\n    \"\"\"\n    Represents a key concept with its relationships and associated metadata.\n\n    Attributes:\n        name (str): The name of the concept.\n        category (str): The category of the concept (e.g., 'technical', 'domain', 'method', etc.).\n        relationships (Dict[str, List[str]]): A mapping where each key is a type of relationship and the\n            value is a list of related concept names.\n        importance_score (float): A numerical score representing the importance or relevance of the concept.\n        context_snippets (List[str]): A list of text snippets providing context where the concept appears.\n    \"\"\"\n    name: str\n    category: str\n    relationships: dict[str, list[str]]\n    importance_score: float\n    context_snippets: list[str]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.normalize_vectors","title":"<code>normalize_vectors(vectors)</code>","text":"<p>Normalize vectors to unit length</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def normalize_vectors(vectors: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Normalize vectors to unit length\"\"\"\n    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n    return np.divide(vectors, norms, where=norms != 0)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.VectorStores","title":"<code>VectorStores</code>","text":"<p>Vector store implementations for the toolboxv2 system.</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.VectorStores.taichiNumpyNumbaVectorStores","title":"<code>taichiNumpyNumbaVectorStores</code>","text":"<code>NumpyVectorStore</code> \u00b6 <p>               Bases: <code>AbstractVectorStore</code></p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/taichiNumpyNumbaVectorStores.py</code> <pre><code>class NumpyVectorStore(AbstractVectorStore):\n    def __init__(self, use_gpu=False):\n        self.embeddings = np.empty((0, 0))\n        self.chunks = []\n        # Initialize Taich\n        import taichi as ti\n        ti.init(arch=ti.gpu if use_gpu else ti.cpu)\n        self.normalized_embeddings = None\n\n    def add_embeddings(self, embeddings: np.ndarray, chunks: list[Chunk]) -&gt; None:\n        if len(embeddings.shape) != 2:\n            raise ValueError(\"Embeddings must be 2D array\")\n        if len(chunks) != embeddings.shape[0]:\n            raise ValueError(\"Mismatch between embeddings and chunks count\")\n\n        if self.embeddings.size == 0:\n            self.embeddings = embeddings\n        else:\n            if embeddings.shape[1] != self.embeddings.shape[1]:\n                raise ValueError(\"Embedding dimensions must match\")\n            self.embeddings = np.vstack([self.embeddings, embeddings])\n        self.chunks.extend(chunks)\n        # Reset normalized embeddings cache\n        self.normalized_embeddings = None\n\n    def search(self, query_embedding: np.ndarray, k: int = 5, min_similarity: float = 0.7) -&gt; list[Chunk]:\n        if self.embeddings.size == 0:\n            return []\n\n        # Pre-compute normalized embeddings if not cached\n        if self.normalized_embeddings is None:\n            self._precompute_normalized_embeddings()\n\n        # Normalize query\n        query_norm = self._normalize_vector(query_embedding)\n\n        # Enhanced Taichi kernel for similarity computation\n        n = len(self.chunks)\n        similarities = np.zeros(n, dtype=np.float32)\n        import taichi as ti\n        @ti.kernel\n        def compute_similarities_optimized(\n            query: ti.types.ndarray(dtype=ti.f32),\n            embeddings: ti.types.ndarray(dtype=ti.f32),\n            similarities: ti.types.ndarray(dtype=ti.f32),\n            n: ti.i32,\n            dim: ti.i32\n        ):\n            ti.loop_config(block_dim=256)\n            for i in range(n):\n                dot_product = 0.0\n                # Vectorized dot product computation\n                for j in range(dim):\n                    dot_product += embeddings[i, j] * query[j]\n                similarities[i] = dot_product\n\n        # Alternative optimized kernel using tile-based computation\n        @ti.kernel\n        def compute_similarities_tiled(\n            query: ti.types.ndarray(dtype=ti.f32),\n            embeddings: ti.types.ndarray(dtype=ti.f32),\n            similarities: ti.types.ndarray(dtype=ti.f32),\n            n: ti.i32,\n            dim: ti.i32\n        ):\n            tile_size = 16  # Adjust based on hardware\n            for i in range(n):\n                dot_product = 0.0\n                # Process in tiles for better cache utilization\n                for jt in range(0, dim):\n                    if jt % tile_size != 0:\n                        continue\n                    tile_sum = 0.0\n                    for j in range(jt, ti.min(jt + tile_size, dim)):\n                        tile_sum += embeddings[i, j] * query[j]\n                    dot_product += tile_sum\n                similarities[i] = dot_product\n\n        # Choose the appropriate kernel based on dimension size\n        if query_embedding.shape[0] &gt;= 256:\n            compute_similarities_tiled(\n                query_norm.astype(np.float32),\n                self.normalized_embeddings,\n                similarities,\n                n,\n                query_embedding.shape[0]\n            )\n        else:\n            compute_similarities_optimized(\n                query_norm.astype(np.float32),\n                self.normalized_embeddings,\n                similarities,\n                n,\n                query_embedding.shape[0]\n            )\n\n        # Optimize top-k selection\n        if k &gt;= n:\n            indices = np.argsort(-similarities)\n        else:\n            # Use partial sort for better performance when k &lt; n\n            indices = np.argpartition(-similarities, k)[:k]\n            indices = indices[np.argsort(-similarities[indices])]\n\n        # Filter results efficiently using vectorized operations\n        mask = similarities[indices] &gt;= min_similarity\n        filtered_indices = indices[mask]\n        return [self.chunks[idx] for idx in filtered_indices[:k]]\n\n    def save(self) -&gt; bytes:\n        return pickle.dumps({\n            'embeddings': self.embeddings,\n            'chunks': self.chunks\n        })\n\n    def load(self, data: bytes) -&gt; 'NumpyVectorStore':\n        loaded = pickle.loads(data)\n        self.embeddings = loaded['embeddings']\n        self.chunks = loaded['chunks']\n        return self\n\n    def clear(self) -&gt; None:\n        self.embeddings = np.empty((0, 0))\n        self.chunks = []\n        self.normalized_embeddings = None\n\n    def rebuild_index(self) -&gt; None:\n        pass  # No index to rebuild for numpy implementation\n\n    def _normalize_vector(self, vector: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Normalize a single vector efficiently.\"\"\"\n        return vector / (np.linalg.norm(vector) + 1e-8)\n\n    def _precompute_normalized_embeddings(self) -&gt; None:\n        \"\"\"Pre-compute and cache normalized embeddings.\"\"\"\n        # Allocate output array\n        self.normalized_embeddings = np.empty_like(self.embeddings, dtype=np.float32)\n\n        # Normalize embeddings using Taichi\n        batch_normalize(\n            self.embeddings.astype(np.float32),\n            self.normalized_embeddings,\n            self.embeddings.shape[0],\n            self.embeddings.shape[1]\n        )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.VectorStores.types","title":"<code>types</code>","text":"<code>AbstractVectorStore</code> \u00b6 <p>               Bases: <code>ABC</code></p> <p>Abstract base class for vector stores</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/types.py</code> <pre><code>class AbstractVectorStore(ABC):\n    \"\"\"Abstract base class for vector stores\"\"\"\n\n    @abstractmethod\n    def add_embeddings(self, embeddings: np.ndarray, chunks: list[Chunk]) -&gt; None:\n        \"\"\"Add embeddings and their corresponding chunks to the store\"\"\"\n        pass\n\n    @abstractmethod\n    def search(self, query_embedding: np.ndarray, k: int = 5, min_similarity: float = 0.7) -&gt; list[Chunk]:\n        \"\"\"Search for similar vectors\"\"\"\n        pass\n\n    @abstractmethod\n    def save(self) -&gt; bytes:\n        \"\"\"Save the vector store to disk\"\"\"\n        pass\n\n    @abstractmethod\n    def load(self, data: bytes) -&gt; 'AbstractVectorStore':\n        \"\"\"Load the vector store from disk\"\"\"\n        pass\n\n    @abstractmethod\n    def clear(self) -&gt; None:\n        \"\"\"Clear all data from the store\"\"\"\n        pass\n\n    @abstractmethod\n    def rebuild_index(self) -&gt; None:\n        \"\"\"Optional for faster searches\"\"\"\n        pass\n</code></pre> <code>add_embeddings(embeddings, chunks)</code> <code>abstractmethod</code> \u00b6 <p>Add embeddings and their corresponding chunks to the store</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/types.py</code> <pre><code>@abstractmethod\ndef add_embeddings(self, embeddings: np.ndarray, chunks: list[Chunk]) -&gt; None:\n    \"\"\"Add embeddings and their corresponding chunks to the store\"\"\"\n    pass\n</code></pre> <code>clear()</code> <code>abstractmethod</code> \u00b6 <p>Clear all data from the store</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/types.py</code> <pre><code>@abstractmethod\ndef clear(self) -&gt; None:\n    \"\"\"Clear all data from the store\"\"\"\n    pass\n</code></pre> <code>load(data)</code> <code>abstractmethod</code> \u00b6 <p>Load the vector store from disk</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/types.py</code> <pre><code>@abstractmethod\ndef load(self, data: bytes) -&gt; 'AbstractVectorStore':\n    \"\"\"Load the vector store from disk\"\"\"\n    pass\n</code></pre> <code>rebuild_index()</code> <code>abstractmethod</code> \u00b6 <p>Optional for faster searches</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/types.py</code> <pre><code>@abstractmethod\ndef rebuild_index(self) -&gt; None:\n    \"\"\"Optional for faster searches\"\"\"\n    pass\n</code></pre> <code>save()</code> <code>abstractmethod</code> \u00b6 <p>Save the vector store to disk</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/types.py</code> <pre><code>@abstractmethod\ndef save(self) -&gt; bytes:\n    \"\"\"Save the vector store to disk\"\"\"\n    pass\n</code></pre> <code>search(query_embedding, k=5, min_similarity=0.7)</code> <code>abstractmethod</code> \u00b6 <p>Search for similar vectors</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/types.py</code> <pre><code>@abstractmethod\ndef search(self, query_embedding: np.ndarray, k: int = 5, min_similarity: float = 0.7) -&gt; list[Chunk]:\n    \"\"\"Search for similar vectors\"\"\"\n    pass\n</code></pre> <code>Chunk</code> <code>dataclass</code> \u00b6 <p>Represents a chunk of text with its embedding and metadata</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/types.py</code> <pre><code>@dataclass(slots=True)\nclass Chunk:\n    \"\"\"Represents a chunk of text with its embedding and metadata\"\"\"\n    text: str\n    embedding: np.ndarray\n    metadata: dict[str, Any]\n    content_hash: str\n    cluster_id: int | None = None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.chainUi","title":"<code>chainUi</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.chainUi.delete_task_chain","title":"<code>delete_task_chain(app, request=None)</code>  <code>async</code>","text":"<p>Deletes a task chain.</p> Source code in <code>toolboxv2/mods/isaa/chainUi.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, request_as_kwarg=True, api_methods=['DELETE'])\nasync def delete_task_chain(app: App, request: Optional[RequestData] = None):\n    \"\"\"Deletes a task chain.\"\"\"\n    chain_name = request.query_params.get(\"chain_name\") if request and request.query_params else None\n    if not chain_name:\n        return Result.default_user_error(info=\"Chain name is required for deletion.\", exec_code=400)\n\n    isaa = get_isaa_instance(app)\n    try:\n        isaa.remove_task(chain_name)  # Removes from memory\n        isaa.save_task()  # Saves all chains, effectively removing the deleted one from file\n        # This also deletes the .chain.json file\n\n        # Delete associated Drawflow file if it exists\n        drawflow_file_path = isaa.agent_chain.directory / f\"{chain_name}.drawflow.json\"\n        if drawflow_file_path.exists():\n            try:\n                drawflow_file_path.unlink()\n                app.logger.info(f\"Deleted Drawflow data for chain '{chain_name}'.\")\n            except Exception as e:\n                app.logger.warning(f\"Could not delete Drawflow data file for chain '{chain_name}': {e}\")\n\n        return Result.ok(info=f\"Task chain '{chain_name}' deleted successfully.\")\n    except KeyError:  # If isaa.remove_task raises KeyError for non-existent chain\n        return Result.default_user_error(info=f\"Task chain '{chain_name}' not found.\", exec_code=404)\n    except Exception as e:\n        app.logger.error(f\"Error deleting task chain '{chain_name}': {e}\", exc_info=True)\n        return Result.custom_error(info=f\"Failed to delete task chain: {str(e)}\", exec_code=500)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.chainUi.get_task_chain_definition","title":"<code>get_task_chain_definition(app, request=None)</code>  <code>async</code>","text":"<p>Gets the definition of a specific task chain, including its Drawflow export if available.</p> Source code in <code>toolboxv2/mods/isaa/chainUi.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, request_as_kwarg=True, api_methods=['GET'])\nasync def get_task_chain_definition(app: App, request: Optional[RequestData] = None):\n    \"\"\"Gets the definition of a specific task chain, including its Drawflow export if available.\"\"\"\n    chain_name = request.query_params.get(\"chain_name\") if request and request.query_params else None\n    if not chain_name:\n        return Result.default_user_error(info=\"Chain name is required.\", exec_code=400)\n\n    isaa = get_isaa_instance(app)\n    try:\n        # Get logical task definition\n        tasks_list_dicts = isaa.get_task(chain_name)\n        if tasks_list_dicts is None:  # Check if chain exists\n            return Result.default_user_error(info=f\"Task chain '{chain_name}' not found.\", exec_code=404)\n\n        description = isaa.agent_chain.get_discr(chain_name) or \"\"\n\n        # Attempt to load Drawflow specific data if it exists\n        # This assumes Drawflow data is saved in a parallel file or embedded\n        drawflow_data = None\n        drawflow_file_path = isaa.agent_chain.directory / f\"{chain_name}.drawflow.json\"\n        if drawflow_file_path.exists():\n            try:\n                with open(drawflow_file_path, 'r') as f:\n                    drawflow_data = json.load(f)\n            except Exception as e:\n                app.logger.warning(f\"Could not load Drawflow data for chain '{chain_name}': {e}\")\n\n        chain_pydantic_tasks = [ISAAPydanticTask(**task_dict) for task_dict in tasks_list_dicts]\n\n        response_data = ISAAPydanticTaskChain(\n            name=chain_name,\n            description=description,\n            tasks=chain_pydantic_tasks\n        ).model_dump()\n\n        if drawflow_data:\n            response_data[\"drawflow_export\"] = drawflow_data  # Embed Drawflow data\n\n        return Result.json(data=response_data)\n\n    except Exception as e:\n        app.logger.error(f\"Error getting task chain definition for '{chain_name}': {e}\", exc_info=True)\n        return Result.custom_error(info=f\"Failed to get task chain definition: {str(e)}\", exec_code=500)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.chainUi.get_task_chain_editor_page_drawflow","title":"<code>get_task_chain_editor_page_drawflow(app, request=None)</code>  <code>async</code>","text":"<p>Serves the HTML page for the Drawflow-based Task Chain Editor.</p> Source code in <code>toolboxv2/mods/isaa/chainUi.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, name=\"task_chain_editor_drawflow\", api_methods=['GET'])\nasync def get_task_chain_editor_page_drawflow(app: App, request: Optional[RequestData] = None):\n    \"\"\"Serves the HTML page for the Drawflow-based Task Chain Editor.\"\"\"\n    if app is None:  # Should not happen if called via export\n        app = get_app()\n\n    # The Drawflow HTML and JS will be substantial.\n    # It's better to load it from a separate .html file for maintainability.\n    # For this example, I'll provide a condensed version here.\n    # In a real setup, use:\n    #   ui_file_path = Path(__file__).parent / \"task_chain_editor_drawflow.html\"\n    #   with open(ui_file_path, \"r\") as f:\n    #       html_content = f.read()\n    # And then inject app.web_context() if needed, or ensure tb.js handles it.\n\n    html_content = DRAWFLOW_TASK_CHAIN_EDITOR_HTML_TEMPLATE  # Defined below\n    return Result.html(data=app.web_context() + html_content)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.chainUi.get_task_chain_list","title":"<code>get_task_chain_list(app, request=None)</code>  <code>async</code>","text":"<p>Lists all available global task chains.</p> Source code in <code>toolboxv2/mods/isaa/chainUi.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, request_as_kwarg=True, api_methods=['GET'])\nasync def get_task_chain_list(app: App, request: Optional[RequestData] = None):\n    \"\"\"Lists all available global task chains.\"\"\"\n    isaa = get_isaa_instance(app)\n    try:\n        chain_names = list(isaa.agent_chain.chains.keys() ) # This should return List[str]\n        return Result.json(data=chain_names)\n    except Exception as e:\n        app.logger.error(f\"Error listing task chains: {e}\", exc_info=True)\n        return Result.custom_error(info=f\"Failed to list task chains: {str(e)}\", exec_code=500)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.chainUi.initialize_module","title":"<code>initialize_module(app)</code>","text":"<p>Initializes the ISAA ChainUI module and registers its UI with CloudM.</p> Source code in <code>toolboxv2/mods/isaa/chainUi.py</code> <pre><code>@export(mod_name=MOD_NAME, version=VERSION)\ndef initialize_module(app: App):\n    \"\"\"Initializes the ISAA ChainUI module and registers its UI with CloudM.\"\"\"\n    print(f\"ISAA Drawflow ChainUI Modul ({MOD_NAME} v{VERSION}) initialisiert.\")\n    if app is None:\n        app = get_app()\n\n    # Register the new Drawflow-based Task Chain Editor UI\n    app.run_any((\"CloudM\", \"add_ui\"),\n                name=f\"{Name}_TaskChainEditorDrawflow\",  # Unique name\n                title=\"Task Chain Editor (Drawflow)\",\n                path=f\"/api/{Name}/task_chain_editor_drawflow\",  # Unique path\n                description=\"Visual editor for ISAA Task Chains using Drawflow.\",\n                auth=True\n                )\n    return Result.ok(info=\"ISAA Drawflow ChainUI Modul und Editor UI bereit.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.chainUi.run_chain_visualized","title":"<code>run_chain_visualized(app, request=None, data=None)</code>  <code>async</code>","text":"<p>Executes a specified task chain with the given input.</p> Source code in <code>toolboxv2/mods/isaa/chainUi.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, request_as_kwarg=True, api_methods=['POST'])\nasync def run_chain_visualized(app: App, request: Optional[RequestData] = None, data: RunChainRequest = None):\n    \"\"\"Executes a specified task chain with the given input.\"\"\"\n    if not data:  # Compatibility\n        if request and request.body and isinstance(request.body, dict):\n            try:\n                data = RunChainRequest(**request.body)\n            except Exception as e:\n                return Result.default_user_error(info=f\"Invalid run chain data: {e}\", exec_code=400)\n        else:\n            return Result.default_user_error(info=\"No run chain data provided.\", exec_code=400)\n    elif isinstance(data, dict):\n        data = RunChainRequest(**data)\n\n    if not data.chain_name:\n        return Result.default_user_error(info=\"Chain name is required for execution.\", exec_code=400)\n    if data.task_input is None:  # Allow empty string as input\n        return Result.default_user_error(info=\"Task input is required.\", exec_code=400)\n\n    isaa = get_isaa_instance(app)\n\n    # TODO: Add SSE streaming for execution progress if desired in the future.\n    # For now, simple blocking execution.\n\n    try:\n        # If chain_definition is provided, use it directly (for running unsaved chains)\n        if data.chain_definition:\n            app.logger.info(\n                f\"Executing unsaved chain definition for '{data.chain_name}' with input: {data.task_input[:50]}...\")\n            # Temporarily add this chain definition to isaa.agent_chain without saving to file\n            # This requires isaa.agent_chain to support in-memory, non-persistent additions or direct execution\n            # For simplicity, let's assume isaa.run_task can accept a task list directly if AgentChain is adapted.\n            # If not, we'd save it temporarily or find another way.\n            # For now, assuming run_task primarily uses named, saved chains.\n            # A more robust solution would be to modify `isaa.run_task` or `ChainTreeExecutor`\n            # to accept a raw list of task dictionaries.\n            # This example will proceed assuming the chain must be saved first if not already.\n            # Let's add a note that this feature (running unsaved chains from UI) needs more work on ISAA core.\n            app.logger.warning(\n                \"Running unsaved chain definitions directly is not fully supported by this endpoint version. The chain should be saved first.\")\n            # Fallback to trying to run by name, assuming it was saved.\n\n        app.logger.info(f\"Executing chain '{data.chain_name}' with input: {data.task_input[:50]}...\")\n        # `isaa.run_task` is already async\n        execution_result = await isaa.run_task(task_input=data.task_input, chain_name=data.chain_name)\n\n        # `execution_result` structure depends on `ChainTreeExecutor.execute`\n        # It's usually a dictionary of results.\n        return Result.json(data={\"output\": execution_result, \"final_message\": \"Chain execution completed.\"})\n\n    except Exception as e:\n        app.logger.error(f\"Error executing task chain '{data.chain_name}': {e}\", exc_info=True)\n        return Result.custom_error(info=f\"Chain execution failed: {str(e)}\", exec_code=500)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.chainUi.save_task_chain_definition","title":"<code>save_task_chain_definition(app, request=None, data=None)</code>  <code>async</code>","text":"<p>Saves a task chain definition. Expects logical tasks and optional Drawflow export.</p> Source code in <code>toolboxv2/mods/isaa/chainUi.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, request_as_kwarg=True, api_methods=['POST'])\nasync def save_task_chain_definition(app: App, request: Optional[RequestData] = None,\n                                     data: SaveTaskChainRequest = None):\n    \"\"\"Saves a task chain definition. Expects logical tasks and optional Drawflow export.\"\"\"\n    if not data:  # Compatibility for direct data passthrough if decorator doesn't parse body for Pydantic model\n        if request and request.body and isinstance(request.body, dict):\n            try:\n                data = SaveTaskChainRequest(**request.body)\n            except Exception as e:\n                return Result.default_user_error(info=f\"Invalid chain data provided: {e}\", exec_code=400)\n        else:\n            return Result.default_user_error(info=\"No chain data provided.\", exec_code=400)\n    elif isinstance(data, dict):\n        data = SaveTaskChainRequest(**data)\n\n    if not data.name:\n        return Result.default_user_error(info=\"Chain name cannot be empty.\", exec_code=400)\n\n    isaa = get_isaa_instance(app)\n    try:\n        # Save logical tasks to ISAA's AgentChain\n        task_dicts = [task.model_dump() for task in data.tasks]\n        isaa.add_task(data.name, task_dicts)  # add_task replaces if exists\n        if data.description is not None:  # Allow empty description\n            isaa.agent_chain.add_discr(data.name, data.description)\n        isaa.save_task(data.name)  # Persists the .chain.json file\n\n        # Save Drawflow specific data if provided\n        if data.drawflow_export:\n            drawflow_file_path = Path(isaa.agent_chain.directory) / f\"{data.name}.drawflow.json\"\n            try:\n                with open(drawflow_file_path, 'w') as f:\n                    json.dump(data.drawflow_export, f, indent=2)\n                app.logger.info(f\"Saved Drawflow data for chain '{data.name}' to {drawflow_file_path}\")\n            except Exception as e:\n                app.logger.error(f\"Failed to save Drawflow data for chain '{data.name}': {e}\")\n                # Optionally, inform client that logical save succeeded but visual save failed\n                return Result.ok(\n                    info=f\"Task chain '{data.name}' saved (logical part), but Drawflow visual data failed to save.\")\n\n        return Result.ok(info=f\"Task chain '{data.name}' saved successfully.\")\n\n    except Exception as e:\n        app.logger.error(f\"Error saving task chain '{data.name}': {e}\", exc_info=True)\n        return Result.custom_error(info=f\"Failed to save task chain: {str(e)}\", exec_code=500)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras","title":"<code>extras</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.adapter","title":"<code>adapter</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.adapter--litellm-llm-interface-module","title":"LiteLLM LLM Interface Module","text":"<p>This module provides interfaces for interacting with LiteLLM's language models, including text generation and embedding capabilities.</p> <p>Author: Lightrag Team Created: 2025-02-04 License: MIT License Version: 1.0.0</p> <p>Change Log: - 1.0.0 (2025-02-04): Initial LiteLLM release     * Ported OpenAI logic to use litellm async client     * Updated error types and environment variable names     * Preserved streaming and embedding support</p> Dependencies <ul> <li>litellm</li> <li>numpy</li> <li>pipmaster</li> <li>Python &gt;= 3.10</li> </ul> Usage <p>from llm_interfaces.litellm import litellm_complete, litellm_embed</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.adapter.litellm_complete","title":"<code>litellm_complete(prompt, system_prompt=None, history_messages=None, keyword_extraction=False, model_name='groq/gemma2-9b-it', **kwargs)</code>  <code>async</code>","text":"<p>Public completion interface using the model name specified in the global configuration. Optionally extracts keywords if requested.</p> Source code in <code>toolboxv2/mods/isaa/extras/adapter.py</code> <pre><code>async def litellm_complete(\n    prompt, system_prompt=None, history_messages=None, keyword_extraction=False, model_name = \"groq/gemma2-9b-it\", **kwargs\n) -&gt; str | AsyncIterator[str]:\n    \"\"\"\n    Public completion interface using the model name specified in the global configuration.\n    Optionally extracts keywords if requested.\n    \"\"\"\n    if history_messages is None:\n        history_messages = []\n    # Check and set response format for keyword extraction if needed\n    keyword_extraction_flag = kwargs.pop(\"keyword_extraction\", None)\n    if keyword_extraction_flag:\n        kwargs[\"response_format\"] = \"json\"\n     # kwargs[\"hashing_kv\"].global_config[\"llm_model_name\"]\n\n    return await litellm_complete_if_cache(\n        model_name,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        **kwargs,\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.adapter.litellm_complete_if_cache","title":"<code>litellm_complete_if_cache(model, prompt, system_prompt=None, history_messages=None, base_url=None, api_key=None, **kwargs)</code>  <code>async</code>","text":"<p>Core function to query the LiteLLM model. It builds the message context, invokes the completion API, and returns either a complete result string or an async iterator for streaming responses.</p> Source code in <code>toolboxv2/mods/isaa/extras/adapter.py</code> <pre><code>@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n    retry=retry_if_exception_type((RateLimitError, Timeout, APIConnectionError)),\n)\nasync def litellm_complete_if_cache(\n    model,\n    prompt,\n    system_prompt=None,\n    history_messages=None,\n    base_url=None,\n    api_key=None,\n    **kwargs,\n) -&gt; str | AsyncIterator[str]:\n    \"\"\"\n    Core function to query the LiteLLM model. It builds the message context,\n    invokes the completion API, and returns either a complete result string or\n    an async iterator for streaming responses.\n    \"\"\"\n    # Set the API key if provided\n    if api_key:\n        os.environ[\"LITELLM_API_KEY\"] = api_key\n\n    # Remove internal keys not needed for the client call\n    kwargs.pop(\"hashing_kv\", None)\n    kwargs.pop(\"keyword_extraction\", None)\n\n    fallbacks_ = kwargs.pop(\"fallbacks\", [])\n    # Build the messages list from system prompt, conversation history, and the new prompt\n    messages = []\n    if system_prompt:\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\n    if history_messages is not None:\n        messages.extend(history_messages)\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    # Log query details for debugging purposes\n    try:\n        # Depending on the response format, choose the appropriate API call\n        if \"response_format\" in kwargs:\n            response = await acompletion(\n                model=model, messages=messages,\n                fallbacks=fallbacks_+os.getenv(\"FALLBACKS_MODELS\", '').split(','),\n                **kwargs\n            )\n        else:\n            response = await acompletion(\n                model=model, messages=messages,\n                fallbacks=os.getenv(\"FALLBACKS_MODELS\", '').split(','),\n                **kwargs\n            )\n    except Exception as e:\n        print(e)\n        get_logger().error(f\"Failed to litellm memory work {e}\")\n        return \"\"\n\n    # Check if the response is a streaming response (i.e. an async iterator)\n    if hasattr(response, \"__aiter__\"):\n\n        async def inner():\n            async for chunk in response:\n                # Assume LiteLLM response structure is similar to OpenAI's\n                content = chunk.choices[0].delta.content\n                if content is None:\n                    continue\n                yield content\n\n        return inner()\n    else:\n        # Non-streaming: extract and return the full content string\n\n        content = response.choices[0].message.content\n        if content is None:\n            content = response.choices[0].message.tool_calls[0].function.arguments\n        return content\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.adapter.litellm_embed","title":"<code>litellm_embed(texts, model='gemini/text-embedding-004', base_url=None, api_key=None)</code>  <code>async</code>","text":"<p>Generates embeddings for the given list of texts using LiteLLM.</p> Source code in <code>toolboxv2/mods/isaa/extras/adapter.py</code> <pre><code>@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=60),\n    retry=retry_if_exception_type((RateLimitError, Timeout, APIConnectionError)),\n)\nasync def litellm_embed(\n    texts: list[str],\n    model: str = \"gemini/text-embedding-004\",\n    base_url: str = None,\n    api_key: str = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Generates embeddings for the given list of texts using LiteLLM.\n    \"\"\"\n    response = await litellm.aembedding(\n        model=model, input=texts,\n        # encoding_format=\"float\"\n    )\n    return np.array([dp.embedding for dp in response.data])\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.filter","title":"<code>filter</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.filter.filter_relevant_texts","title":"<code>filter_relevant_texts(query, texts, fuzzy_threshold=70, semantic_threshold=0.75, model=None)</code>","text":"<p>Filters a list of texts based on their relevance to the query. It first uses a fuzzy matching score and, if that score is below the threshold, it then checks the semantic similarity.</p> <p>:param query: The query string. :param texts: List of page texts. :param fuzzy_threshold: Fuzzy matching score threshold (0-100). :param semantic_threshold: Semantic similarity threshold (0.0-1.0). :param model: A preloaded SentenceTransformer model (if None, one will be loaded). :return: Filtered list of texts deemed relevant.</p> Source code in <code>toolboxv2/mods/isaa/extras/filter.py</code> <pre><code>def filter_relevant_texts(query: str,\n                          texts: list[str],\n                          fuzzy_threshold: int = 70,\n                          semantic_threshold: float = 0.75,\n                          model = None) -&gt; list[str]:\n    \"\"\"\n    Filters a list of texts based on their relevance to the query.\n    It first uses a fuzzy matching score and, if that score is below the threshold,\n    it then checks the semantic similarity.\n\n    :param query: The query string.\n    :param texts: List of page texts.\n    :param fuzzy_threshold: Fuzzy matching score threshold (0-100).\n    :param semantic_threshold: Semantic similarity threshold (0.0-1.0).\n    :param model: A preloaded SentenceTransformer model (if None, one will be loaded).\n    :return: Filtered list of texts deemed relevant.\n    \"\"\"\n    try:\n        from rapidfuzz import fuzz\n    except Exception:\n        os.system([sys.executable, '-m', 'pip', 'install', 'RapidFuzz'])\n        from rapidfuzz import fuzz\n    try:\n        from sentence_transformers import SentenceTransformer, util\n    except Exception:\n        os.system([sys.executable, '-m', 'pip', 'install', 'sentence-transformers'])\n        from sentence_transformers import SentenceTransformer, util\n\n    if model is None:\n        # For efficiency, consider pre-loading this model outside the function.\n        model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\n    # Pre-compute query embedding for the semantic check:\n    query_embedding = model.encode(query, convert_to_tensor=True)\n\n    relevant_texts = []\n    for text in texts:\n        # --- Fuzzy Keyword Filtering ---\n        fuzzy_score = fuzz.partial_ratio(query.lower(), text.lower())\n        if fuzzy_score &gt;= fuzzy_threshold:\n            relevant_texts.append(text)\n        else:\n            # --- Semantic Similarity Filtering ---\n            text_embedding = model.encode(text, convert_to_tensor=True)\n            similarity = util.pytorch_cos_sim(query_embedding, text_embedding).item()\n            if similarity &gt;= semantic_threshold:\n                relevant_texts.append(text)\n    return relevant_texts\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.modes","title":"<code>modes</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.modes.generate_prompt","title":"<code>generate_prompt(subject, context='', additional_requirements=None)</code>","text":"<p>Generates a prompt based on the given subject, with optional context and additional requirements.</p> <p>Parameters: - subject (str): The main subject for the prompt. - context (str): Optional additional context to tailor the prompt. - additional_requirements (Dict[str, Any]): Optional additional parameters or requirements for the prompt.</p> <p>Returns: - str: A crafted prompt.</p> Source code in <code>toolboxv2/mods/isaa/extras/modes.py</code> <pre><code>def generate_prompt(subject: str, context: str = \"\", additional_requirements: dict[str, Any] = None) -&gt; str:\n    \"\"\"\n    Generates a prompt based on the given subject, with optional context and additional requirements.\n\n    Parameters:\n    - subject (str): The main subject for the prompt.\n    - context (str): Optional additional context to tailor the prompt.\n    - additional_requirements (Dict[str, Any]): Optional additional parameters or requirements for the prompt.\n\n    Returns:\n    - str: A crafted prompt.\n    \"\"\"\n    prompt = f\"Based on the subject '{subject}', with the context '{context}', generate a clear and precise instruction.\"\n    if additional_requirements:\n        prompt += f\" Consider the following requirements: {additional_requirements}.\"\n    return prompt\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.talk","title":"<code>talk</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.talk.TalkSession","title":"<code>TalkSession</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the state of a single voice conversation session.</p> Source code in <code>toolboxv2/mods/talk.py</code> <pre><code>class TalkSession(BaseModel):\n    \"\"\"Represents the state of a single voice conversation session.\"\"\"\n    session_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    user_id: str\n    chat_session: ChatSession\n    event_queue: asyncio.Queue = Field(default_factory=asyncio.Queue, exclude=True)\n    # Task to track the running agent process, preventing concurrent requests\n    agent_task: Optional[asyncio.Task] = Field(default=None, exclude=True)\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.talk.Tools","title":"<code>Tools</code>","text":"<p>               Bases: <code>MainTool</code></p> <p>The main class for the Talk module, handling initialization, session management, and dependency loading.</p> Source code in <code>toolboxv2/mods/talk.py</code> <pre><code>class Tools(MainTool):\n    \"\"\"\n    The main class for the Talk module, handling initialization,\n    session management, and dependency loading.\n    \"\"\"\n\n    def __init__(self, app: App):\n        # Initialize the MainTool with module-specific information\n        self.version = VERSION\n        self.name = MOD_NAME\n        self.color = \"CYAN\"\n        self.sessions: Dict[str, TalkSession] = {}\n        self.stt_func = None\n        self.tts_func = None\n        self.isaa_mod = None\n        super().__init__(load=self.on_start, v=VERSION, name=MOD_NAME, tool={}, on_exit=self.on_exit)\n\n    def on_start(self):\n        \"\"\"Initializes the Talk module, its dependencies (ISAA, AUDIO), and UI registration.\"\"\"\n        self.app.logger.info(f\"Starting {self.name} v{self.version}...\")\n\n        # Get the ISAA module instance, which is a critical dependency\n        self.isaa_mod = self.app.get_mod(\"isaa\")\n        if not self.isaa_mod:\n            self.app.logger.error(\n                f\"{self.name}: ISAA module not found or failed to load. Voice assistant will not be functional.\")\n            return\n\n        # Initialize STT and TTS services from the AUDIO module\n        if hasattr(TBEF, \"AUDIO\") and self.app.get_mod(\"AUDIO\"):\n            self.stt_func = self.app.run_any(TBEF.AUDIO.STT_GENERATE, model=\"openai/whisper-small\", row=True, device=0)\n            self.tts_func = self.app.get_function(TBEF.AUDIO.SPEECH, state=False)[0]\n\n            if self.stt_func and self.stt_func != \"404\":\n                self.app.logger.info(\"Talk STT (whisper-small) is Online.\")\n            else:\n                self.app.logger.warning(\"Talk STT function not available.\")\n                self.stt_func = None\n\n            if self.tts_func and self.tts_func != \"404\":\n                self.app.logger.info(\"Talk TTS function is Online.\")\n            else:\n                self.app.logger.warning(\"Talk TTS function not available.\")\n                self.tts_func = None\n        else:\n            self.app.logger.warning(\"Talk module: AUDIO module features are not available or the module is not loaded.\")\n\n        if not all([self.stt_func, self.tts_func]):\n            self.app.logger.error(\"Talk module cannot function without both STT and TTS services.\")\n\n        # Register the UI component with CloudM\n        self.app.run_any((\"CloudM\", \"add_ui\"),\n                         name=MOD_NAME, title=\"Voice Assistant\", path=f\"/api/{MOD_NAME}/ui\",\n                         description=\"Natural conversation with an AI assistant.\", auth=True)\n        self.app.logger.info(f\"{self.name} UI registered with CloudM.\")\n\n    def on_exit(self):\n        \"\"\"Clean up resources, especially cancelling any active agent tasks.\"\"\"\n        for session in self.sessions.values():\n            if session.agent_task and not session.agent_task.done():\n                session.agent_task.cancel()\n        self.app.logger.info(f\"Closing {self.name} and cleaning up sessions.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.talk.Tools.on_exit","title":"<code>on_exit()</code>","text":"<p>Clean up resources, especially cancelling any active agent tasks.</p> Source code in <code>toolboxv2/mods/talk.py</code> <pre><code>def on_exit(self):\n    \"\"\"Clean up resources, especially cancelling any active agent tasks.\"\"\"\n    for session in self.sessions.values():\n        if session.agent_task and not session.agent_task.done():\n            session.agent_task.cancel()\n    self.app.logger.info(f\"Closing {self.name} and cleaning up sessions.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.talk.Tools.on_start","title":"<code>on_start()</code>","text":"<p>Initializes the Talk module, its dependencies (ISAA, AUDIO), and UI registration.</p> Source code in <code>toolboxv2/mods/talk.py</code> <pre><code>def on_start(self):\n    \"\"\"Initializes the Talk module, its dependencies (ISAA, AUDIO), and UI registration.\"\"\"\n    self.app.logger.info(f\"Starting {self.name} v{self.version}...\")\n\n    # Get the ISAA module instance, which is a critical dependency\n    self.isaa_mod = self.app.get_mod(\"isaa\")\n    if not self.isaa_mod:\n        self.app.logger.error(\n            f\"{self.name}: ISAA module not found or failed to load. Voice assistant will not be functional.\")\n        return\n\n    # Initialize STT and TTS services from the AUDIO module\n    if hasattr(TBEF, \"AUDIO\") and self.app.get_mod(\"AUDIO\"):\n        self.stt_func = self.app.run_any(TBEF.AUDIO.STT_GENERATE, model=\"openai/whisper-small\", row=True, device=0)\n        self.tts_func = self.app.get_function(TBEF.AUDIO.SPEECH, state=False)[0]\n\n        if self.stt_func and self.stt_func != \"404\":\n            self.app.logger.info(\"Talk STT (whisper-small) is Online.\")\n        else:\n            self.app.logger.warning(\"Talk STT function not available.\")\n            self.stt_func = None\n\n        if self.tts_func and self.tts_func != \"404\":\n            self.app.logger.info(\"Talk TTS function is Online.\")\n        else:\n            self.app.logger.warning(\"Talk TTS function not available.\")\n            self.tts_func = None\n    else:\n        self.app.logger.warning(\"Talk module: AUDIO module features are not available or the module is not loaded.\")\n\n    if not all([self.stt_func, self.tts_func]):\n        self.app.logger.error(\"Talk module cannot function without both STT and TTS services.\")\n\n    # Register the UI component with CloudM\n    self.app.run_any((\"CloudM\", \"add_ui\"),\n                     name=MOD_NAME, title=\"Voice Assistant\", path=f\"/api/{MOD_NAME}/ui\",\n                     description=\"Natural conversation with an AI assistant.\", auth=True)\n    self.app.logger.info(f\"{self.name} UI registered with CloudM.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.talk.api_open_stream","title":"<code>api_open_stream(self, request, session_id)</code>  <code>async</code>","text":"<p>Opens a Server-Sent Events (SSE) stream for a given session ID.</p> Source code in <code>toolboxv2/mods/talk.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, name=\"stream\", api_methods=['GET'], request_as_kwarg=True)\nasync def api_open_stream(self: Tools, request: RequestData, session_id: str) -&gt; Result:\n    \"\"\"Opens a Server-Sent Events (SSE) stream for a given session ID.\"\"\"\n    if not session_id or session_id not in self.sessions:\n        return Result.default_user_error(info=\"Invalid or expired session ID.\", exec_code=404)\n\n    session = self.sessions[session_id]\n    queue = session.event_queue\n\n    async def event_generator() -&gt; AsyncGenerator[Dict[str, Any], None]:\n        self.app.logger.info(f\"SSE stream opened for session {session_id}\")\n        await queue.put({\"event\": \"connection_ready\", \"data\": \"Stream connected successfully.\"})\n        try:\n            while True:\n                event_data = await queue.get()\n                yield event_data\n                queue.task_done()\n        except asyncio.CancelledError:\n            self.app.logger.info(f\"SSE stream for session {session_id} cancelled by client.\")\n        finally:\n            if session_id in self.sessions:\n                if self.sessions[session_id].agent_task and not self.sessions[session_id].agent_task.done():\n                    self.sessions[session_id].agent_task.cancel()\n                del self.sessions[session_id]\n                self.app.logger.info(f\"Cleaned up and closed session {session_id}.\")\n\n    return Result.sse(stream_generator=event_generator())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.talk.api_process_audio","title":"<code>api_process_audio(self, request, form_data)</code>  <code>async</code>","text":"<p>Receives audio, transcribes it, and starts the agent processing task.</p> Source code in <code>toolboxv2/mods/talk.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, name=\"process_audio\", api_methods=['POST'], request_as_kwarg=True)\nasync def api_process_audio(self: Tools, request: RequestData, form_data: Dict) -&gt; Result:\n    \"\"\"Receives audio, transcribes it, and starts the agent processing task.\"\"\"\n    if not self.stt_func:\n        return Result.default_internal_error(info=\"Speech-to-text service is not available.\")\n\n    session_id = form_data.get('session_id')\n    audio_file_data = form_data.get('audio_blob')\n\n    if not session_id or session_id not in self.sessions:\n        return Result.default_user_error(info=\"Invalid or missing session_id.\", exec_code=400)\n\n    session = self.sessions[session_id]\n\n    if session.agent_task and not session.agent_task.done():\n        return Result.default_user_error(info=\"Already processing a previous request.\", exec_code=429)\n\n    if not audio_file_data or 'content_base64' not in audio_file_data:\n        return Result.default_user_error(info=\"Audio data is missing or in the wrong format.\", exec_code=400)\n\n    try:\n        audio_bytes = base64.b64decode(audio_file_data['content_base64'])\n        transcription_result = self.stt_func(audio_bytes)\n        transcribed_text = transcription_result.get('text', '').strip()\n\n        if not transcribed_text:\n            await session.event_queue.put({\"event\": \"error\", \"data\": \"Could not understand audio. Please try again.\"})\n            return Result.ok(data={\"message\": \"Transcription was empty.\"})\n\n        await session.event_queue.put({\"event\": \"transcription_update\", \"data\": transcribed_text})\n\n        voice_params = {\n            \"voice_index\": int(form_data.get('voice_index', '0')),\n            \"provider\": form_data.get('provider', 'piper'),\n            \"model_name\": form_data.get('model_name', 'ryan')\n        }\n\n        # Start the background task; the request returns immediately.\n        session.agent_task = asyncio.create_task(\n            _run_agent_and_respond(self, session, transcribed_text, voice_params)\n        )\n        return Result.ok(data={\"message\": \"Audio received and processing started.\"})\n\n    except Exception as e:\n        self.app.logger.error(f\"Error processing audio for session {session_id}: {e}\", exc_info=True)\n        return Result.default_internal_error(info=f\"Failed to process audio: {str(e)}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.talk.api_start_session","title":"<code>api_start_session(self, request)</code>  <code>async</code>","text":"<p>Creates a new talk session for an authenticated user.</p> Source code in <code>toolboxv2/mods/talk.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, name=\"start_session\", api_methods=['POST'], request_as_kwarg=True)\nasync def api_start_session(self: Tools, request: RequestData) -&gt; Result:\n    \"\"\"Creates a new talk session for an authenticated user.\"\"\"\n    user_id = await _get_user_uid(self.app, request)\n    if not user_id:\n        return Result.default_user_error(info=\"User authentication required.\", exec_code=401)\n\n    if not self.isaa_mod:\n        return Result.default_internal_error(info=\"ISAA module is not available.\")\n\n    # Create a new ISAA ChatSession for conversation history\n    chat_session = ChatSession(mem=self.isaa_mod.get_memory())\n    session = TalkSession(user_id=user_id, chat_session=chat_session)\n    self.sessions[session.session_id] = session\n\n    self.app.logger.info(f\"Started new talk session {session.session_id} for user {user_id}\")\n    return Result.json(data={\"session_id\": session.session_id})\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.talk.get_main_ui","title":"<code>get_main_ui(self, request)</code>","text":"<p>Serves the main HTML and JavaScript UI for the Talk widget.</p> Source code in <code>toolboxv2/mods/talk.py</code> <pre><code>@export(mod_name=MOD_NAME, name=\"ui\", api=True, api_methods=['GET'], request_as_kwarg=True)\ndef get_main_ui(self: Tools, request: RequestData) -&gt; Result:\n    \"\"\"Serves the main HTML and JavaScript UI for the Talk widget.\"\"\"\n    html_content = \"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\" data-theme=\"light\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;ToolBoxV2 - Voice Assistant&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200\" /&gt;\n    &lt;style&gt;\n        body { font-family: sans-serif; background-color: var(--theme-bg); color: var(--theme-text); display: flex; justify-content: center; align-items: center; min-height: 100vh; margin: 0; }\n        .container { display: flex; flex-direction: column; align-items: center; justify-content: center; width: 100%; max-width: 600px; padding: 20px; text-align: center; }\n        .visualizer { width: 250px; height: 250px; background-color: var(--glass-bg); border-radius: 50%; position: relative; overflow: hidden; border: 3px solid var(--theme-border); box-shadow: inset 0 0 15px rgba(0,0,0,0.2); transition: border-color 0.3s, box-shadow 0.3s; }\n        .visualizer.recording { border-color: #ef4444; }\n        .visualizer.thinking { border-color: #3b82f6; animation: pulse 2s infinite; }\n        .visualizer.speaking { border-color: #22c55e; }\n        .particle { position: absolute; width: 8px; height: 8px; background-color: var(--theme-primary); border-radius: 50%; pointer-events: none; transition: all 0.1s; }\n        #micButton { margin-top: 30px; width: 80px; height: 80px; border-radius: 50%; border: none; background-color: var(--theme-primary); color: white; cursor: pointer; display: flex; justify-content: center; align-items: center; box-shadow: 0 4px 10px rgba(0,0,0,0.2); transition: background-color 0.2s, transform 0.1s; }\n        #micButton:active { transform: scale(0.95); }\n        #micButton:disabled { background-color: #9ca3af; cursor: not-allowed; }\n        #micButton .material-symbols-outlined { font-size: 40px; }\n        #statusText { margin-top: 20px; min-height: 50px; font-size: 1.2em; color: var(--theme-text-muted); line-height: 1.5; }\n        @keyframes pulse { 0% { box-shadow: inset 0 0 15px rgba(0,0,0,0.2), 0 0 0 0 rgba(59, 130, 246, 0.7); } 70% { box-shadow: inset 0 0 15px rgba(0,0,0,0.2), 0 0 0 15px rgba(59, 130, 246, 0); } 100% { box-shadow: inset 0 0 15px rgba(0,0,0,0.2), 0 0 0 0 rgba(59, 130, 246, 0); } }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"container\"&gt;\n        &lt;div class=\"visualizer\" id=\"visualizer\"&gt;&lt;/div&gt;\n        &lt;p id=\"statusText\"&gt;Press the microphone to start&lt;/p&gt;\n        &lt;button id=\"micButton\"&gt;&lt;span class=\"material-symbols-outlined\"&gt;hourglass_empty&lt;/span&gt;&lt;/button&gt;\n        &lt;div class=\"options\" style=\"margin-top: 20px;\"&gt;\n            &lt;label for=\"voiceSelect\"&gt;Voice:&lt;/label&gt;\n            &lt;select id=\"voiceSelect\"&gt;\n                &lt;option value='{\"provider\": \"piper\", \"model_name\": \"ryan\", \"voice_index\": 0}'&gt;Ryan (EN)&lt;/option&gt;\n                &lt;option value='{\"provider\": \"piper\", \"model_name\": \"kathleen\", \"voice_index\": 0}'&gt;Kathleen (EN)&lt;/option&gt;\n                &lt;option value='{\"provider\": \"piper\", \"model_name\": \"karlsson\", \"voice_index\": 0}'&gt;Karlsson (DE)&lt;/option&gt;\n            &lt;/select&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;script unSave=\"true\"&gt;\n    function initTalk() {\n        const visualizer = document.getElementById('visualizer');\n        const micButton = document.getElementById('micButton');\n        const statusText = document.getElementById('statusText');\n        const voiceSelect = document.getElementById('voiceSelect');\n\n        const state = { sessionId: null, sseConnection: null, mediaRecorder: null, audioChunks: [], isRecording: false, isProcessing: false, currentAudio: null };\n        let audioContext, analyser, particles = [];\n\n        function setStatus(text, mode = 'idle') {\n            statusText.textContent = text;\n            visualizer.className = 'visualizer ' + mode;\n        }\n\n        function createParticles(num = 50) {\n            visualizer.innerHTML = ''; particles = [];\n            for (let i = 0; i &lt; num; i++) {\n                const p = document.createElement('div'); p.classList.add('particle');\n                visualizer.appendChild(p);\n                particles.push({ element: p, angle: Math.random() * Math.PI * 2, radius: 50 + Math.random() * 50, speed: 0.01 + Math.random() * 0.02 });\n            }\n        }\n\n        function animateVisualizer() {\n            if (analyser) {\n                const dataArray = new Uint8Array(analyser.frequencyBinCount);\n                analyser.getByteFrequencyData(dataArray);\n                let average = dataArray.reduce((a, b) =&gt; a + b, 0) / dataArray.length;\n                particles.forEach(p =&gt; {\n                    p.angle += p.speed;\n                    const scale = 1 + (average / 128);\n                    p.element.style.transform = `translate(${Math.cos(p.angle) * p.radius * scale}px, ${Math.sin(p.angle) * p.radius * scale}px)`;\n                });\n            }\n            requestAnimationFrame(animateVisualizer);\n        }\n\n        async function startSession() {\n            if (state.sessionId) return;\n            setStatus(\"Connecting...\", 'thinking');\n            micButton.disabled = true;\n            try {\n                const response = await TB.api.request('talk', 'start_session', {}, 'POST');\n                if (response.error === 'none' &amp;&amp; response.get()?.session_id) {\n                    state.sessionId = response.get().session_id;\n                    connectSse();\n                } else {\n                    setStatus(response.info?.help_text || \"Failed to start session.\", 'error');\n                }\n            } catch (e) {\n                setStatus(\"Connection error.\", 'error');\n            }\n        }\n\n        function connectSse() {\n            if (!state.sessionId) return;\n            state.sseConnection = TB.sse.connect(`/sse/talk/stream?session_id=${state.sessionId}`, {\n                onOpen: () =&gt; console.log(\"SSE Stream Open\"),\n                onError: () =&gt; setStatus(\"Connection lost.\", 'error'),\n                listeners: {\n                    'connection_ready': (data) =&gt; { setStatus(\"Press the microphone to start\"); micButton.disabled = false; micButton.innerHTML = '&lt;span class=\"material-symbols-outlined\"&gt;mic&lt;/span&gt;'; },\n                    'transcription_update': (data) =&gt; { setStatus(`\u201c${data}\u201d`, 'thinking'); state.isProcessing = true; },\n                    'agent_thought': (data) =&gt; setStatus(data, 'thinking'),\n                    'agent_response_chunk': (data) =&gt; { if (statusText.textContent.startsWith('\u201c')) statusText.textContent = \"\"; statusText.textContent += data; },\n                    'audio_playback': (data) =&gt; playAudio(data.content, data.format),\n                    'processing_complete': (data) =&gt; { state.isProcessing = false; setStatus(data); micButton.disabled = false; micButton.innerHTML = '&lt;span class=\"material-symbols-outlined\"&gt;mic&lt;/span&gt;'; },\n                    'error': (data) =&gt; { state.isProcessing = false; setStatus(data, 'error'); micButton.disabled = false; micButton.innerHTML = '&lt;span class=\"material-symbols-outlined\"&gt;mic&lt;/span&gt;'; }\n                }\n            });\n        }\n\n        async function playAudio(base64, format) {\n            setStatus(\"...\", 'speaking');\n            const blob = await (await fetch(`data:${format};base64,${base64}`)).blob();\n            const url = URL.createObjectURL(blob);\n            if (state.currentAudio) state.currentAudio.pause();\n            state.currentAudio = new Audio(url);\n\n            if (!audioContext) audioContext = new AudioContext();\n            const source = audioContext.createMediaElementSource(state.currentAudio);\n            if (!analyser) { analyser = audioContext.createAnalyser(); analyser.fftSize = 64; }\n            source.connect(analyser);\n            analyser.connect(audioContext.destination);\n\n            state.currentAudio.play();\n            state.currentAudio.onended = () =&gt; { setStatus(\"Finished speaking.\"); URL.revokeObjectURL(url); };\n        }\n\n        async function toggleRecording() {\n            if (state.isProcessing) return;\n            if (!state.sessionId) { await startSession(); return; }\n\n            if (state.isRecording) {\n                state.mediaRecorder.stop();\n                micButton.disabled = true;\n                micButton.innerHTML = '&lt;span class=\"material-symbols-outlined\"&gt;hourglass_top&lt;/span&gt;';\n                setStatus(\"Processing...\", 'thinking');\n            } else {\n                if (!state.mediaRecorder) {\n                    try {\n                        const stream = await navigator.mediaDevices.getUserMedia({ audio: { sampleRate: 16000, channelCount: 1 } });\n                        if (!audioContext) audioContext = new AudioContext();\n                        const source = audioContext.createMediaStreamSource(stream);\n                        if (!analyser) { analyser = audioContext.createAnalyser(); analyser.fftSize = 64; }\n                        source.connect(analyser);\n\n                        state.mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm;codecs=opus' });\n                        state.mediaRecorder.ondataavailable = e =&gt; state.audioChunks.push(e.data);\n                        state.mediaRecorder.onstop = uploadAudio;\n                    } catch (e) { setStatus(\"Could not access microphone.\", 'error'); return; }\n                }\n                state.audioChunks = []; state.mediaRecorder.start(); state.isRecording = true;\n                setStatus(\"Listening...\", 'recording');\n                micButton.innerHTML = '&lt;span class=\"material-symbols-outlined\"&gt;stop_circle&lt;/span&gt;';\n            }\n        }\n\n        async function uploadAudio() {\n            state.isRecording = false; state.isProcessing = true;\n            if (state.audioChunks.length === 0) { setStatus(\"No audio recorded.\"); state.isProcessing = false; micButton.disabled = false; micButton.innerHTML = '&lt;span class=\"material-symbols-outlined\"&gt;mic&lt;/span&gt;'; return; }\n            const audioBlob = new Blob(state.audioChunks, { type: 'audio/webm;codecs=opus' });\n\n            const formData = new FormData();\n            formData.append('session_id', state.sessionId);\n            formData.append('audio_blob', audioBlob, 'recording.webm');\n\n            const voiceParams = JSON.parse(voiceSelect.value);\n            for (const key in voiceParams) formData.append(key, voiceParams[key]);\n\n            try {\n                const response = await TB.api.request('talk', 'process_audio', formData, 'POST');\n                if (response.error !== 'none') {\n                    setStatus(response.info?.help_text || \"Failed to process audio.\", 'error');\n                    state.isProcessing = false; micButton.disabled = false; micButton.innerHTML = '&lt;span class=\"material-symbols-outlined\"&gt;mic&lt;/span&gt;';\n                }\n            } catch(e) {\n                 setStatus(\"Error sending audio.\", 'error'); state.isProcessing = false; micButton.disabled = false; micButton.innerHTML = '&lt;span class=\"material-symbols-outlined\"&gt;mic&lt;/span&gt;';\n            }\n        }\n\n        micButton.addEventListener('click', toggleRecording);\n        createParticles(); animateVisualizer();\n        if (window.TB.isInitialized) startSession(); else window.TB.events.on('tbjs:initialized', startSession, { once: true });\n    }\nif (window.TB?.events) {\n    if (window.TB.config?.get('appRootId')) { // A sign that TB.init might have run\n         initTalk();\n    } else {\n        window.TB.events.on('tbjs:initialized', initTalk, { once: true });\n    }\n} else {\n    // Fallback if TB is not even an object yet, very early load\n    document.addEventListener('tbjs:initialized', initTalk, { once: true }); // Custom event dispatch from TB.init\n}\n\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\"\"\"\n    return Result.html(data=html_content)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.flows_dict","title":"<code>toolboxv2.flows_dict(s='.py', remote=False, dir_path=None, flows_dict_=None)</code>","text":"Source code in <code>toolboxv2/flows/__init__.py</code> <pre><code>def flows_dict(s='.py', remote=False, dir_path=None, flows_dict_=None):\n\n    if flows_dict_ is None:\n        flows_dict_ = {}\n    with Spinner(\"Loading flows\"):\n        # Erhalte den Pfad zum aktuellen Verzeichnis\n        if dir_path is None:\n            for ex_path in os.getenv(\"EXTERNAL_PATH_RUNNABELS\", '').split(','):\n                if not ex_path or len(ex_path) == 0:\n                    continue\n                flows_dict(s,remote,ex_path,flows_dict_)\n            dir_path = os.path.dirname(os.path.realpath(__file__))\n        to = time.perf_counter()\n        # Iteriere \u00fcber alle Dateien im Verzeichnis\n        files = os.listdir(dir_path)\n        l_files = len(files)\n        for i, file_name in enumerate(files):\n            with Spinner(f\"{file_name} {i}/{l_files}\"):\n                # \u00dcberpr\u00fcfe, ob die Datei eine Python-Datei ist\n                if file_name == \"__init__.py\":\n                    pass\n\n                elif remote and s in file_name and file_name.endswith('.gist'):\n                    # print(\"Loading from Gist :\", file_name)\n                    name_f = os.path.splitext(file_name)[0]\n                    name = name_f.split('.')[0]\n                    # publisher = name_f.split('.')[1]\n                    url = name_f.split('.')[-1]\n                    # print(\"Ent\", name)\n                    # Lade das Modul\n                    print(f\"Gist Name: {name}, URL: {url}\")\n                    try:\n                        module = GistLoader(f\"{name}/{url}\").load_module(name)\n                    #try:\n                    #    module = GistLoader(f\"{name}/{url}\")\n                    except Exception as e:\n                        print(f\"Error loading module {name} from github {url}\")\n                        print(e)\n                        continue\n\n                    # F\u00fcge das Modul der Dictionary hinzu\n                    print(f\"{hasattr(module, 'run')} and {callable(module.run)} and {hasattr(module, 'NAME')}\")\n                    if hasattr(module, 'run') and callable(module.run) and hasattr(module, 'NAME'):\n                        # print(\"Collecing :\", module.NAME)\n                        flows_dict_[module.NAME] = module.run\n                elif file_name.endswith('.py') and s in file_name:\n                    name = os.path.splitext(file_name)[0]\n                    # print(\"Loading :\", name)\n                    # Lade das Modul\n                    spec = importlib.util.spec_from_file_location(name, os.path.join(dir_path, file_name))\n                    module = importlib.util.module_from_spec(spec)\n                    try:\n                        spec.loader.exec_module(module)\n                    except Exception as e:\n                        print(\"Error loading module \", name)\n                        print(e)\n                        continue\n\n                    # F\u00fcge das Modul der Dictionary hinzu\n                    if hasattr(module, 'run') and callable(module.run) and hasattr(module, 'NAME'):\n                        # print(\"Collecing :\", module.NAME)\n                        flows_dict_[module.NAME] = module.run\n\n        print(f\"Getting all flows took {time.perf_counter() - to:.2f} for {len(flows_dict_.keys())} elements\")\n        return flows_dict_\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.TBEF","title":"<code>toolboxv2.TBEF</code>","text":"<p>Automatic generated by ToolBox v = 0.1.21</p>"},{"location":"toolboxv2/#other-exposed-items","title":"Other Exposed Items","text":""},{"location":"toolboxv2/#toolboxv2.ToolBox_over","title":"<code>toolboxv2.ToolBox_over = 'root'</code>  <code>module-attribute</code>","text":""},{"location":"usage/","title":"ToolBoxV2 Developer Guide","text":"<p>Based on the provided documentation, here's a comprehensive guide on how to use the ToolBoxV2 framework for building applications.</p>"},{"location":"usage/#introduction","title":"Introduction","text":"<p>ToolBoxV2 is a Python framework that provides a structured approach to building applications with standardized request handling and response formatting. It consists of two main components:</p> <ol> <li>RequestData Classes - For handling HTTP requests with strong typing</li> <li>Result Class - For standardized response handling and error management</li> </ol>"},{"location":"usage/#setting-up-your-application","title":"Setting Up Your Application","text":""},{"location":"usage/#creating-a-module","title":"Creating a Module","text":"<p>Start by initializing your application module:</p> <pre><code>from toolboxv2 import get_app, App, RequestData, Result\nfrom typing import Dict, Optional\n\n# Define your module\nMOD_NAME = \"YOUR_MODULE_NAME\"\nversion = \"1.0\"\nexport = get_app(\"{MODULE-NAME.SUB-MODULE}\").tb\n</code></pre>"},{"location":"usage/#registering-functions","title":"Registering Functions","text":"<p>Use the <code>@export</code> decorator to register functions within your module:</p> <pre><code>@export(mod_name=MOD_NAME, version=version)\ndef your_function():\n    # Function logic here\n    return Result.ok(data=\"Success\")\n</code></pre>"},{"location":"usage/#function-types","title":"Function Types","text":""},{"location":"usage/#standard-system-functions","title":"Standard System Functions","text":"<pre><code># Basic function with App parameter\n@export(mod_name=MOD_NAME, version=version, row=True)\ndef system_function(app: App):\n    # Implementation\n    return \"Raw return value\"  # Will be returned as-is because row=True\n\n# Function without App parameter\n@export(mod_name=MOD_NAME, version=version)\ndef function_without_app():\n    # Implementation\n    return Result.ok(data=\"Success\")\n\n# Function with arguments\n@export(mod_name=MOD_NAME, version=version)\ndef function_with_args(name: str) -&gt; Result:\n    # Implementation\n    return Result.ok(data=name)\n\n# Function returning raw data\n@export(mod_name=MOD_NAME, version=version, row=True)\ndef function_with_args_kwargs(name: str, nickname: Optional[str]=None) -&gt; str:\n    if nickname is None:\n        nickname = \"\"\n    return name + nickname  # Returned as raw string\n</code></pre>"},{"location":"usage/#async-functions","title":"Async Functions","text":"<pre><code>@export(mod_name=MOD_NAME, version=version, row=True)\nasync def async_function(app: App):\n    # Async implementation\n    result = await some_async_operation()\n    return result\n</code></pre>"},{"location":"usage/#api-endpoints","title":"API Endpoints","text":"<pre><code># API endpoint with request parameter\n@export(mod_name=MOD_NAME, api=True, version=\"1.0\", request_as_kwarg=True)\nasync def get_data(request: Optional[RequestData]=None):\n    if request:\n        query_params = request.query_params\n        # Process query parameters\n    return Result.json(data={\"status\": \"success\"})\n\n# API endpoint with App and Request parameters\n@export(mod_name=MOD_NAME, api=True, version=\"1.0\", request_as_kwarg=True)\nasync def get_user_data(app, request: Optional[RequestData]=None):\n    # Implementation using app and request\n    return Result.ok(data={\"user\": \"data\"})\n\n# API endpoint with specific HTTP methods\n@export(mod_name=MOD_NAME, api=True, version=\"1.0\", api_methods=['PUT', 'POST'])\nasync def update_data(app, data: Dict):\n    # Process the JSON data received in the request body\n    return Result.ok(data=data)\n\n# API endpoint handling form data\n@export(mod_name=MOD_NAME, api=True, version=\"1.0\", api_methods=['PUT', 'POST'])\nasync def submit_form(app, form_data: Dict):\n    # Process form data\n    return Result.ok(data=form_data)\n</code></pre>"},{"location":"usage/#working-with-request-data","title":"Working with Request Data","text":""},{"location":"usage/#accessing-request-information","title":"Accessing Request Information","text":"<pre><code>@export(mod_name=MOD_NAME, api=True, version=\"1.0\", request_as_kwarg=True)\nasync def process_request(request: Optional[RequestData]=None):\n    if request:\n        # Access method and path\n        method = request.method\n        path = request.path\n\n        # Access headers\n        user_agent = request.headers.user_agent\n        content_type = request.headers.content_type\n        custom_header = request.headers.extra_headers.get('x-custom-header')\n\n        # Access query parameters\n        query_params = request.query_params\n        search_term = query_params.get('search')\n\n        # Access form data or JSON body\n        if request.form_data:\n            form_values = request.form_data\n\n        if request.body and request.content_type == 'application/json':\n            json_data = request.body\n\n    return Result.ok(data=\"Request processed\")\n</code></pre>"},{"location":"usage/#accessing-session-information","title":"Accessing Session Information","text":"<pre><code>@export(mod_name=MOD_NAME, api=True, version=\"1.0\", request_as_kwarg=True)\nasync def get_user_session(request: Optional[RequestData]=None):\n    if request and hasattr(request, 'session'):\n        # Access session data\n        session_id = request.session.SiID\n        user_name = request.session.user_name\n        session_level = request.session.level\n\n        # Access custom session data\n        custom_data = request.session.extra_data.get('custom_key')\n\n    return Result.ok(data={\"user\": user_name})\n</code></pre>"},{"location":"usage/#working-with-results","title":"Working with Results","text":""},{"location":"usage/#creating-different-types-of-responses","title":"Creating Different Types of Responses","text":"<pre><code>@export(mod_name=MOD_NAME, api=True, version=\"1.0\")\nasync def response_examples(app):\n    # Choose the appropriate response type based on your needs\n\n    # 1. Standard success response\n    return Result.ok(\n        data={\"key\": \"value\"},\n        info=\"Operation completed successfully\"\n    )\n\n    # 2. JSON response\n    return Result.json(\n        data={\"status\": \"online\", \"version\": \"1.0\"},\n        info=\"API status retrieved\"\n    )\n\n    # 3. HTML response\n    return Result.html(\n        data=\"&lt;html&gt;&lt;body&gt;&lt;h1&gt;Welcome&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\",\n        info=\"Page rendered\"\n    )\n\n    # 4. Text response\n    return Result.text(\n        text_data=\"Plain text content\",\n        content_type=\"text/plain\"\n    )\n\n    # 5. Binary file response\n    return Result.binary(\n        data=binary_data,\n        content_type=\"application/pdf\",\n        download_name=\"report.pdf\"\n    )\n\n    # 6. Redirect response\n    return Result.redirect(\n        url=\"/dashboard\",\n        status_code=302\n    )\n</code></pre>"},{"location":"usage/#error-handling","title":"Error Handling","text":"<pre><code>@export(mod_name=MOD_NAME, version=version)\ndef process_with_validation(user_input):\n    # Validate input\n    if not user_input:\n        return Result.default_user_error(\n            info=\"Empty input is not allowed\",\n            exec_code=400\n        )\n\n    # Process valid input\n    try:\n        processed_data = process_data(user_input)\n        return Result.ok(data=processed_data)\n    except Exception as e:\n        return Result.default_sys_error(\n            info=f\"Processing error: {str(e)}\",\n            exec_code=500\n        )\n</code></pre>"},{"location":"usage/#using-lazy_return-for-simplified-error-handling","title":"Using lazy_return for Simplified Error Handling","text":"<pre><code>@export(mod_name=MOD_NAME, version=version)\ndef validate_and_process(data):\n    # Validate data\n    validation_result = validate_data(data)\n\n    # If validation fails, return the error\n    # If validation succeeds, return the processed data\n    return validation_result.lazy_return(\n        'user',  # Use user error if validation fails\n        data={\"processed\": True, \"original\": data}  # Return this if successful\n    )\n</code></pre>"},{"location":"usage/#streaming-responses","title":"Streaming Responses","text":"<pre><code>@export(mod_name=MOD_NAME, api=True, version=\"1.0\")\nasync def stream_data():\n    async def generator():\n        for i in range(10):\n            yield {\"chunk\": i}\n            await asyncio.sleep(0.5)\n\n    async def cleanup():\n        # Cleanup resources when the stream closes\n        print(\"Stream closed, performing cleanup\")\n\n    return Result.stream(\n        stream_generator=generator(),\n        info=\"Streaming data chunks\",\n        cleanup_func=cleanup\n    )\n</code></pre>"},{"location":"usage/#advanced-features","title":"Advanced Features","text":""},{"location":"usage/#caching","title":"Caching","text":"<pre><code># Memory caching\n@export(mod_name=MOD_NAME, version=version, memory_cache=True,\n        memory_cache_max_size=100, memory_cache_ttl=300)\ndef cached_function(key):\n    # Expensive operation here\n    return Result.ok(data=compute_expensive_data(key))\n\n# File caching\n@export(mod_name=MOD_NAME, version=version, file_cache=True)\ndef file_cached_function(key):\n    # Expensive operation here\n    return Result.ok(data=compute_expensive_data(key))\n</code></pre>"},{"location":"usage/#background-functions","title":"Background Functions","text":"<pre><code># Memory caching\n@export(mod_name=MOD_NAME, version=version)\ndef function_with_log_running_bg_call():\n    # Expensive operation here\n    def sync_bg_function():\n        print(\"running in gb\")\n        compute_expensive_function()\n\n    return Result.ok(data=\"Starting processing\").task(sync_bg_function)\n\n# File caching\n@export(mod_name=MOD_NAME, version=version)\nasync def function_with_log_running_bg_call():\n    # Expensive operation here\n    async def bg_function():\n        print(\"running in gb\")\n        await compute_expensive_function()\n    return Result.ok(data=\"Starting processing\").task(bg_function())\n</code></pre>"},{"location":"usage/#lifecycle-management","title":"Lifecycle Management","text":"<pre><code># Initialization function\n@export(mod_name=MOD_NAME, version=version, initial=True)\ndef initialize_module(app: App):\n    # Called when the module is loaded\n    print(f\"Initializing {MOD_NAME} module\")\n    # Set up resources, connections, etc.\n    return Result.ok(info=\"Module initialized\")\n\n# Exit function\n@export(mod_name=MOD_NAME, version=version, exit_f=True)\ndef cleanup_module(app: App):\n    # Called when the application is shutting down\n    print(f\"Cleaning up {MOD_NAME} module\")\n    # Release resources, close connections, etc.\n    return Result.ok(info=\"Module cleaned up\")\n</code></pre>"},{"location":"usage/#prepost-compute-functions","title":"Pre/Post Compute Functions","text":"<pre><code>def log_before_execution(func, *args, **kwargs):\n    print(f\"Executing {func.__name__} with args: {args}, kwargs: {kwargs}\")\n    return args, kwargs\n\ndef log_after_execution(result, func, *args, **kwargs):\n    print(f\"Function {func.__name__} returned: {result}\")\n    return result\n\n@export(mod_name=MOD_NAME, version=version,\n        pre_compute=log_before_execution,\n        post_compute=log_after_execution)\ndef monitored_function(name):\n    # Function logic\n    return Result.ok(data=f\"Hello, {name}!\")\n</code></pre>"},{"location":"usage/#url-patterns-for-api-endpoints","title":"URL Patterns for API Endpoints","text":"<p>API endpoints are accessible using the following URL patterns:</p> <ul> <li>Regular API: <code>/api/MOD_NAME/{function_name}?param1=value1&amp;param2=value2</code></li> <li>Server-Sent Events (streaming): <code>/sse/MOD_NAME/{function_name}?param1=value1&amp;param2=value2</code></li> </ul>"},{"location":"utils/","title":"ToolBoxV2: The <code>App</code> Class","text":"<p>The <code>App</code> class is the central singleton instance in ToolBoxV2, responsible for managing the application's lifecycle, configuration, module loading, and core functionalities. It's typically accessed via the <code>get_app()</code> utility function.</p>"},{"location":"utils/#initialization","title":"Initialization","text":"<p>The <code>App</code> instance is initialized with a <code>prefix</code> and <code>AppArgs</code> (command-line arguments).</p> <pre><code>from toolboxv2 import App, AppArgs, get_app\n\n# Example: Initialize or get the App instance\n# The prefix helps differentiate multiple App instances if needed,\n# and is often used in directory naming.\nargs = AppArgs().default() # Or parsed from sys.argv in __main__.py\napp_instance = get_app(prefix=\"my_app_instance\", args=args)\n\n# Accessing key attributes:\nprint(f\"App ID: {app_instance.id}\")\nprint(f\"Version: {app_instance.version}\")\nprint(f\"Start Directory: {app_instance.start_dir}\")\nprint(f\"Data Directory: {app_instance.data_dir}\")\nprint(f\"Config Directory: {app_instance.config_dir}\")\nprint(f\"Debug Mode: {app_instance.debug}\")\n</code></pre>"},{"location":"utils/#key-initialization-steps","title":"Key Initialization Steps:","text":"<ol> <li> <p>System &amp; Paths:</p> <ul> <li>Determines the operating system (<code>system_flag</code>).</li> <li>Sets the <code>start_dir</code> to the application's root directory.</li> <li>Resolves the <code>prefix</code>:<ul> <li>If no prefix is provided, it attempts to load the last used prefix from <code>.data/last-app-prefix.txt</code>.</li> <li>If a prefix is provided, it's saved to this file for future use.</li> </ul> </li> <li>Constructs the <code>app_id</code> (e.g., <code>prefix-hostname</code>).</li> <li>Sets up <code>data_dir</code>, <code>config_dir</code>, and <code>info_dir</code> based on the <code>app_id</code> (e.g., <code>./.data/prefix-hostname/</code>).</li> <li>Sets up <code>appdata</code> directory (OS-specific application data folder).</li> </ul> </li> <li> <p>Logging:</p> <ul> <li>Initializes a logger (<code>app.logger</code>). The logging level and output (terminal/file) can vary based on the <code>prefix</code> (e.g., \"test\", \"live\", \"debug\") and the <code>--debug</code> CLI argument.</li> </ul> </li> <li> <p>Configuration:</p> <ul> <li>Loads application configuration using <code>FileHandler</code> from a file typically named <code>app_id.config</code> in the <code>config_dir</code>.</li> <li>Defines default configuration <code>keys</code> and <code>defaults</code> (e.g., for macros, helpers, debug status).</li> </ul> </li> <li> <p>Core Attributes:</p> <ul> <li><code>version</code>: ToolBoxV2 version, read from <code>pyproject.toml</code>.</li> <li><code>debug</code>: Boolean, controlled by CLI args and config.</li> <li><code>dev_modi</code>: Boolean, development mode status from config.</li> <li><code>functions</code>: A dictionary to store registered functions from modules.</li> <li><code>modules</code>: A dictionary to store loaded module objects.</li> <li><code>interface_type</code>: Default <code>ToolBoxInterfaces.native</code>.</li> <li><code>alive</code>: Boolean, controls the main application loop.</li> <li><code>args_sto</code>: Stores the parsed <code>AppArgs</code>.</li> <li><code>loop</code>: The asyncio event loop (initialized later or if already running).</li> <li><code>session</code>: A <code>Session</code> object for managing user/remote session state.</li> </ul> </li> <li> <p>Conditional Actions (based on <code>AppArgs</code>):</p> <ul> <li><code>args.init</code>: If true, adds <code>start_dir</code> to <code>sys.path</code>.</li> <li>The <code>__main__.py</code> script handles other arguments like <code>--update</code>, <code>--get-version</code>, etc., by calling <code>App</code> methods or other utilities.</li> </ul> </li> </ol>"},{"location":"utils/#core-functionalities","title":"Core Functionalities","text":""},{"location":"utils/#module-management","title":"Module Management","text":"<ul> <li> <p><code>load_mod(mod_name: str, spec='app', mlm='I', **kwargs)</code> / <code>save_load(modname, spec='app')</code>:</p> <ul> <li>Loads a module into the application.</li> <li><code>spec</code> (specification): Used to namespace or categorize the module instance (e.g., 'app' for general, or a specific session ID).</li> <li>Supports different loading mechanisms (<code>mlm</code>):<ul> <li><code>'I'</code>: In-place load (imports the Python module directly). This is the default.</li> <li><code>'C'</code>: Copies the module file to a runtime directory before loading (less common).</li> </ul> </li> <li>Handles <code>ModuleNotFoundError</code> by attempting to guide the user (e.g., install via <code>CloudM</code> or <code>pip</code>).</li> <li>Registers the module's exported functions and/or its <code>Tools</code> class instance.</li> <li>Can reload modules if they are already loaded. <pre><code># Load the 'MyModule'\nmy_module_instance = app_instance.load_mod(\"MyModule\")\n\n# Or if it's a Tool-based module:\n# my_tool_instance = app_instance.load_mod(\"MyToolModule\")\n</code></pre></li> </ul> </li> <li> <p><code>get_mod(name: str, spec='app') -&gt; ModuleType | MainToolType</code>:</p> <ul> <li>Retrieves a loaded module instance. If the module isn't loaded, it attempts to load it. <pre><code>db_mod = app_instance.get_mod(\"DB\")\nif db_mod:\n    db_mod.some_db_function()\n</code></pre></li> </ul> </li> <li> <p><code>remove_mod(mod_name: str, spec='app', delete=True)</code> / <code>a_remove_mod(...)</code> (async):</p> <ul> <li>Unloads a module, calling its <code>on_exit</code> functions if defined.</li> <li><code>delete=True</code> removes it completely from the <code>functions</code> registry.</li> </ul> </li> <li> <p><code>reload_mod(mod_name: str, spec='app', ...)</code>:</p> <ul> <li>Reloads an existing module. Useful for development.</li> </ul> </li> <li> <p><code>watch_mod(mod_name: str, ...)</code>:</p> <ul> <li>Monitors a module's source file(s) for changes and automatically reloads it. <pre><code># In development, watch 'MyDevModule' for changes\napp_instance.watch_mod(\"MyDevModule\")\n</code></pre></li> </ul> </li> <li> <p><code>load_all_mods_in_file(working_dir=\"mods\")</code> / <code>a_load_all_mods_in_file(...)</code> (async):</p> <ul> <li>Scans the specified directory (default <code>./mods/</code>) and loads all valid Python modules found.</li> </ul> </li> </ul>"},{"location":"utils/#to-isaa","title":"To isaa","text":""},{"location":"utils/#function-registration-and-execution","title":"Function Registration and Execution","text":"<ul> <li> <p><code>@app.tb(...)</code> Decorator (via <code>_create_decorator</code>):</p> <ul> <li>The primary way functions are registered with ToolBoxV2. See <code>example_mod.md</code> for details on usage.</li> <li>This decorator populates the <code>app.functions</code> dictionary.</li> </ul> </li> <li> <p><code>get_function(name: Enum | tuple, metadata=False, state=True, specification='app', ...)</code>:</p> <ul> <li>Retrieves a registered function.</li> <li><code>name</code>: Can be an Enum (from <code>all_functions_enums.py</code>) or a <code>(module_name, function_name)</code> tuple.</li> <li><code>metadata=True</code>: Returns a tuple <code>(function_data_dict, callable_function)</code>.</li> <li><code>state=True</code>: Returns a stateful version of the function (bound to its module instance if applicable).</li> <li><code>state=False</code>: Returns the raw, stateless function.</li> <li><code>specification</code>: The context/instance spec to get the function for.</li> </ul> </li> <li> <p><code>run_any(mod_function_name, ..., get_results=False, **kwargs)</code> / <code>a_run_any(...)</code> (async):</p> <ul> <li>Executes a registered function by its name (Enum or tuple).</li> <li>Handles argument passing, stateful/stateless execution, and error wrapping into a <code>Result</code> object.</li> <li><code>get_results=True</code>: Returns the <code>Result</code> object itself.</li> <li><code>get_results=False</code> (default): Returns the <code>data</code> payload from the <code>Result</code> object if successful.</li> <li>Automatically handles running the function's pre/post compute hooks and caching if configured via <code>@app.tb</code>. <pre><code># Synchronous execution\nresult_data = app_instance.run_any((\"MyModule\", \"my_function\"), arg1=\"hello\")\nfull_result_obj = app_instance.run_any((\"MyModule\", \"my_function\"), arg1=\"hello\", get_results=True)\n\n# Asynchronous execution\nasync_result_data = await app_instance.a_run_any((\"MyAsyncModule\", \"my_async_function\"))\n</code></pre></li> </ul> </li> <li> <p><code>run_http(mod_function_name, function_name=None, method=\"GET\", ...)</code> (async):</p> <ul> <li>Executes a function on a remote ToolBoxV2 instance via HTTP, using the app's <code>session</code> object.</li> </ul> </li> </ul>"},{"location":"utils/#application-lifecycle","title":"Application Lifecycle","text":"<ul> <li><code>exit()</code> / <code>a_exit()</code> (async):<ul> <li>Gracefully shuts down the application.</li> <li>Calls <code>on_exit</code> functions for all loaded modules.</li> <li>Saves configuration.</li> <li>Stops the main application loop (<code>alive = False</code>).</li> <li>Cleans up threads and the event loop if applicable.</li> </ul> </li> </ul>"},{"location":"utils/#utilities","title":"Utilities","text":"<ul> <li><code>print(text, *args, **kwargs)</code> / <code>sprint(text, *args, **kwargs)</code>:<ul> <li>Styled print functions, prepending <code>System$[app_id]:</code>. <code>sprint</code> is often used for more verbose/system-level messages and can be silenced.</li> </ul> </li> <li><code>debug_rains(e: Exception)</code>: If <code>app.debug</code> is true, prints a full traceback and re-raises the exception.</li> <li><code>set_flows(r: dict)</code> / <code>run_flows(name: str, **kwargs)</code>: Manages and executes predefined application flows (sequences of operations).</li> <li><code>get_username()</code> / <code>set_username(username: str)</code>: Manages the application's user identity.</li> <li><code>save_autocompletion_dict()</code> / <code>get_autocompletion_dict()</code>: Saves/loads a dictionary of modules and their functions for autocompletion features.</li> <li><code>save_registry_as_enums(directory: str, filename: str)</code>: Generates an <code>all_functions_enums.py</code>-like file from the currently registered functions.</li> <li><code>execute_all_functions(...)</code> / <code>a_execute_all_functions(...)</code> (async):<ul> <li>Runs all registered testable functions (marked with <code>test=True</code> in <code>@app.tb</code> or having <code>samples</code>).</li> <li>Useful for integration testing and profiling.</li> <li>Can filter by module (<code>m_query</code>) and function (<code>f_query</code>).</li> <li>Supports profiling via <code>cProfile</code>.</li> </ul> </li> <li><code>run_bg_task(task: Callable)</code>:<ul> <li>Runs a synchronous or asynchronous task in a separate background thread with its own event loop. Ensures proper handling of nested asyncio operations within the task.</li> </ul> </li> </ul>"},{"location":"utils/#session-management-appsession","title":"Session Management (<code>app.session</code>)","text":"<p>The <code>app.session</code> attribute holds an instance of the <code>Session</code> class (from <code>toolboxv2.utils.system.session</code>). It's used for: *   Authenticating with a remote ToolBoxV2 service (e.g., SimpleCore Hub). *   Making authenticated HTTP requests (<code>session.fetch</code>, <code>session.upload_file</code>, <code>session.download_file</code>). *   Manages JWT claims and private key authentication.</p> <pre><code># Example: Making an authenticated API call\n# Assumes app.session is already authenticated\nresponse_data = await app_instance.session.fetch(\"/api/MyRemoteModule/get_info\")\njson_payload = await response_data.json()\n</code></pre> <pre><code>### 2. `cli.md` - Documenting the Command Line Interface\n\nThis should explain how to use the `tb` (or `python -m toolboxv2`) command-line tool, detailing its arguments and their effects.\n\n```markdown\n# ToolBoxV2: Command Line Interface (CLI)\n\nToolBoxV2 provides a command-line interface (CLI) for managing and running applications. It's typically invoked as `tb` (if installed globally or via an alias) or `python -m toolboxv2`.\n\n## General Usage\n\n```bash\npython -m toolboxv2 [options] [sub-commands]\n# or\ntb [options] [sub-commands]\n</code></pre> <p>The CLI script (<code>__main__.py</code>) performs the following main steps: 1.  Parses command-line arguments. 2.  Initializes the <code>App</code> instance via <code>setup_app()</code> (which calls <code>get_app()</code>). 3.  Handles various options to:     *   Manage application data and configuration.     *   Control application modes (background, proxy, debug).     *   Load modules and manage their state.     *   Run tests or profilers.     *   Execute specific application flows or commands.</p>"},{"location":"utils/#key-cli-arguments","title":"Key CLI Arguments","text":"<p>The following are some of the primary arguments available. Use <code>tb -h</code> or <code>python -m toolboxv2 -h</code> for a full list.</p> <ul> <li> <p>Instance and Mode:</p> <ul> <li><code>-init [name]</code>: Initializes ToolBoxV2 with a specific instance name (default: <code>main</code>).</li> <li><code>-n, --name &lt;name&gt;</code>: Specifies an ID for the ToolBox instance (default: <code>main</code>). This affects data/config directory names.</li> <li><code>-m, --modi &lt;mode&gt;</code>: Starts a ToolBoxV2 interface/flow (e.g., <code>cli</code>, <code>bg</code>, or custom flows). Default is usually \"cli\".</li> <li><code>--debug</code>: Starts the application in debug mode (more verbose logging, potentially different behavior).</li> <li><code>--remote</code>: Starts in remote mode, often for connecting to a proxy or another instance.</li> <li><code>-bg, --background-application</code>: Starts an interface in the background as a detached process.</li> <li><code>-bgr, --background-application-runner</code>: Runs the background application logic in the current terminal (for daemons).</li> <li><code>-fg, --live-application</code>: Starts a proxy interface, connecting to a background daemon.</li> <li><code>--kill</code>: Kills the currently running local ToolBoxV2 instance (matching the <code>-m &lt;mode&gt;</code> and <code>-n &lt;name&gt;</code>).</li> </ul> </li> <li> <p>Module and Version Management:</p> <ul> <li><code>-l, --load-all-mod-in-files</code>: Loads all modules found in the <code>mods/</code> directory on startup.</li> <li><code>-sfe, --save-function-enums-in-file</code>: Generates/updates the <code>all_functions_enums.py</code> file based on loaded modules. Often used with <code>-l</code>.</li> <li><code>-v, --get-version</code>: Prints the version of ToolBoxV2 and all loaded modules.</li> <li><code>-i, --install &lt;name&gt;</code>: Installs a module or interface (likely via <code>CloudM</code> module).</li> <li><code>-r, --remove &lt;name&gt;</code>: Uninstalls a module or interface.</li> <li><code>-u, --update &lt;name_or_main&gt;</code>: Updates a module/interface or the core ToolBoxV2 (<code>main</code>).</li> </ul> </li> <li> <p>Development and Testing:</p> <ul> <li><code>--test</code>: Runs all unit tests (typically discovers and runs tests in the <code>tests/</code> directory).</li> <li><code>--profiler</code>: Runs all registered testable functions and profiles their execution using <code>cProfile</code>.</li> <li><code>--ipy</code>: Starts an IPython session with the ToolBoxV2 app pre-loaded. Provides magic commands like <code>%tb save/loadX/load/open/inject</code>.</li> </ul> </li> <li> <p>Service Management (<code>--sm</code>):</p> <ul> <li>Provides a sub-menu for managing ToolBoxV2 as a system service (Linux/systemd or Windows Startup).</li> <li>Options: Init, Start/Stop/Restart, Status, Uninstall, Show/Hide console window (Windows).</li> </ul> </li> <li> <p>Log Management (<code>--lm</code>):</p> <ul> <li>Provides a sub-menu for managing log files (e.g., removing or unstyling logs by date/level).</li> </ul> </li> <li> <p>Data and Configuration Management:</p> <ul> <li><code>--delete-config-all</code>: Deletes all configuration files. Use with caution!</li> <li><code>--delete-data-all</code>: Deletes all data folders. Use with caution!</li> <li><code>--delete-config</code>: Deletes the configuration file for the named instance.</li> <li><code>--delete-data</code>: Deletes the data folder for the named instance.</li> </ul> </li> <li> <p>Network Configuration (for interfaces):</p> <ul> <li><code>-p, --port &lt;port&gt;</code>: Specifies the port for an interface (default: <code>5000</code> or <code>6587</code> for background).</li> <li><code>-w, --host &lt;host&gt;</code>: Specifies the host for an interface (default: <code>0.0.0.0</code>).</li> </ul> </li> <li> <p>Direct Command Execution:</p> <ul> <li><code>-c, --command &lt;module_name&gt; &lt;function_name&gt; [arg1 arg2 ...]</code> (can be repeated): Executes a specific function.</li> <li><code>--kwargs &lt;key=value&gt;</code> (can be repeated): Provides keyword arguments for commands specified with <code>-c</code>.</li> </ul> </li> <li> <p>Conda Integration:</p> <ul> <li><code>conda [conda_args...]</code>: Special argument to pass commands directly to a <code>conda_runner.py</code> script (e.g., <code>tb conda env list</code>).</li> </ul> </li> <li> <p>API Runner:</p> <ul> <li><code>api [api_args...]</code>: Special argument to invoke <code>cli_api_runner.py</code>, likely for direct API interactions or testing.</li> </ul> </li> <li> <p>GUI:</p> <ul> <li><code>gui</code>: Starts the GUI version of ToolBoxV2 (imports and runs <code>toolboxv2.__gui__.start</code>).</li> </ul> </li> </ul>"},{"location":"utils/#cli-execution-flow-__main__py","title":"CLI Execution Flow (<code>__main__.py</code>)","text":"<ol> <li>Argument Parsing: <code>parse_args()</code> uses <code>argparse</code> to define and parse all CLI arguments.</li> <li>App Setup (<code>setup_app()</code>):<ul> <li>Initializes the <code>App</code> instance using <code>get_app()</code> with the parsed arguments and name.</li> <li>Sets up PID file for the current process.</li> <li>Optionally silences <code>app.sprint</code> if not in debug/verbose mode.</li> <li>Loads all modules if <code>-l</code> is specified.</li> <li>Handles <code>--update</code> logic.</li> </ul> </li> <li>Background/Live Application Handling:<ul> <li>If <code>-bgr</code>: Initializes <code>DaemonApp</code>.</li> <li>If <code>-bg</code>: Starts the application as a detached background process using <code>subprocess.Popen</code>.</li> <li>If <code>-fg</code> (live-application): Attempts to connect to a background daemon using <code>ProxyApp</code>.</li> </ul> </li> <li>Action Dispatching: Based on the parsed arguments, it performs actions like:<ul> <li>Module installation (<code>--install</code>).</li> <li>Log management (<code>--lm</code>).</li> <li>Service management (<code>--sm</code>).</li> <li>Saving function enums (<code>-sfe</code>).</li> <li>Printing versions (<code>-v</code>).</li> <li>Running the profiler (<code>--profiler</code>).</li> <li>Running flows based on <code>--modi</code>.</li> <li>Handling Docker commands (<code>--docker</code>).</li> <li>Killing an existing instance (<code>--kill</code>).</li> <li>Executing direct commands (<code>-c</code>).</li> </ul> </li> <li>Cleanup: Removes the PID file and calls <code>app.a_exit()</code> before exiting.</li> </ol>"},{"location":"utils/#example-cli-usage","title":"Example CLI Usage","text":"<pre><code># Get version information\npython -m toolboxv2 -v\n\n# Load all modules and save function enums\npython -m toolboxv2 -l -sfe\n\n# Run a specific function in MyModule\npython -m toolboxv2 -c MyModule my_function arg_value --kwargs param_name=kwarg_value\n\n# Start the application with a custom flow named 'web_server' in debug mode\npython -m toolboxv2 -m web_server --debug -n my_web_instance\n\n# Start a background daemon for the 'bg_processing' flow\npython -m toolboxv2 -m bg_processing -bg -n background_processor\n\n# Connect to the background daemon with a live proxy application\npython -m toolboxv2 -m cli -fg -n background_processor\n\n# Kill the 'web_server' modi instance named 'my_web_instance'\npython -m toolboxv2 -m web_server --kill -n my_web_instance\n</code></pre> <pre><code>### 3. `example_mod.md` - Documenting Module Creation\n\nThis needs to be updated to accurately reflect the `@app.tb(...)` decorator from `toolbox.py` and the `Result` and `RequestData` classes from `types.py`.\n\n```markdown\n# ToolBoxV2: Creating Modules\n\nToolBoxV2 modules are Python files or packages that extend the framework's functionality. They can define simple functions, stateful tools (classes inheriting from `MainTool`), or API endpoints.\n\n## Basic Module Structure\n\nA typical ToolBoxV2 module (`.py` file) includes:\n\n1.  **Imports:** Necessary libraries and ToolBoxV2 components.\n2.  **Module Metadata (Optional but Recommended):**\n    *   `Name` (or `name`): A string defining the module's canonical name.\n    *   `version`: A string for the module's version (e.g., \"1.0.0\").\n3.  **Function/Class Definitions:** The core logic of your module.\n4.  **Exporting Functions:** Functions are made available to ToolBoxV2 using the `@export` decorator (which is an alias for `app.tb`).\n\n## The `@export` Decorator (`app.tb`)\n\nThe `@export` decorator is the primary mechanism for registering functions and configuring their behavior within ToolBoxV2. It's obtained from an `App` instance.\n\n```python\nfrom toolboxv2 import get_app, App, Result, RequestData, MainTool\nfrom toolboxv2.utils.system.types import ToolBoxInterfaces # For specific interface types\nfrom typing import Optional, Dict, Any, List\nimport asyncio\n\n# Get the application instance (singleton)\n# The 'prefix' for get_app here is often the module's own name,\n# though the decorator will use its 'mod_name' parameter.\napp = get_app(\"MyModule\")\nexport = app.tb # Alias the decorator for convenience\n\n# --- Module Metadata (Best Practice) ---\nName = \"MyModule\"  # Used by the decorator if mod_name is not specified\nversion = \"1.0.1\"\n\n# --- Example Functions ---\n\n@export(mod_name=Name, version=version, helper=\"A simple greeting function.\")\ndef greet(name: str) -&gt; str:\n    \"\"\"Returns a greeting message.\"\"\"\n    return f\"Hello, {name} from MyModule!\"\n\n@export(mod_name=Name, version=version, row=True, helper=\"Returns raw data without Result wrapping.\")\ndef get_raw_data() -&gt; dict:\n    \"\"\"Demonstrates returning raw data.\"\"\"\n    return {\"key\": \"value\", \"number\": 123}\n\n@export(mod_name=Name, version=version, initial=True, helper=\"Runs when the module is first loaded.\")\ndef on_module_load():\n    \"\"\"Initialization logic for this module.\"\"\"\n    app.print(f\"{Name} module has been loaded and initialized!\")\n    # return Result.ok(info=\"MyModule initialized successfully\") # Optional: return a Result\n\n@export(mod_name=Name, version=version, exit_f=True, helper=\"Runs when the application is shutting down.\")\nasync def on_module_exit():\n    \"\"\"Cleanup logic for this module.\"\"\"\n    await asyncio.sleep(0.1) # Simulate async cleanup\n    app.print(f\"{Name} module is cleaning up.\")\n    # return Result.ok(info=\"MyModule cleaned up.\") # Optional\n\n@export(mod_name=Name, version=version, api=True, api_methods=['GET'], request_as_kwarg=True,\n        helper=\"An example API endpoint.\")\nasync def my_api_endpoint(request: Optional[RequestData] = None) -&gt; Result:\n    \"\"\"\n    Handles GET requests to /api/MyModule/my_api_endpoint.\n    Accesses request details if provided.\n    \"\"\"\n    if request:\n        app.print(f\"API request received: {request.request.method} {request.request.path}\")\n        app.print(f\"Query Params: {request.request.query_params}\")\n        app.print(f\"User from session: {request.session.user_name}\")\n    return Result.json(data={\"message\": \"API call successful!\", \"module_version\": version})\n\n@export(mod_name=Name, version=version, memory_cache=True, memory_cache_ttl=60)\ndef expensive_calculation(param: int) -&gt; int:\n    \"\"\"\n    An example of a function whose results will be cached in memory for 60 seconds.\n    \"\"\"\n    app.print(f\"Performing expensive calculation for {param}...\")\n    time.sleep(2) # Simulate work\n    return param * param\n\n# Example of a more complex function using App instance and returning a Result\n@export(mod_name=Name, version=version)\ndef process_data_with_app(app_instance: App, data_id: int) -&gt; Result:\n    \"\"\"\n    This function automatically receives the 'App' instance if its first parameter is type-hinted as 'App'.\n    This is determined by the 'state=True' logic in the decorator if 'app' is the first param.\n    Alternatively, use state=False for stateless functions.\n    \"\"\"\n    if not isinstance(app_instance, App): # Should always be App if first param is 'app'\n        return Result.default_internal_error(\"App instance not correctly passed.\")\n\n    # Use app_instance for logging, accessing config, other modules, etc.\n    app_instance.logger.info(f\"Processing data_id: {data_id} in {Name}\")\n    if data_id &lt; 0:\n        return Result.default_user_error(info=\"Data ID cannot be negative.\")\n    return Result.ok(data={\"processed_id\": data_id, \"status\": \"completed\"})\n</code></pre>"},{"location":"utils/#export-decorator-parameters","title":"<code>@export</code> Decorator Parameters:","text":"<ul> <li><code>name</code> (str, optional): The name to register the function under. Defaults to the function's actual name.</li> <li><code>mod_name</code> (str): The name of the module this function belongs to. If not provided, it tries to infer from <code>func.__module__</code> or a global <code>Name</code> in the module.</li> <li><code>version</code> (str, optional): Version string for this function/feature. Combined with the app's version.</li> <li><code>helper</code> (str, optional): A docstring or help text for the function.</li> <li><code>api</code> (bool, default <code>False</code>): If <code>True</code>, exposes this function as an HTTP API endpoint.<ul> <li>The URL pattern is typically <code>/api/&lt;mod_name&gt;/&lt;func_name&gt;</code>.</li> <li>For streaming, <code>/sse/&lt;mod_name&gt;/&lt;func_name&gt;</code>.</li> </ul> </li> <li><code>api_methods</code> (List[str], optional): Specifies allowed HTTP methods (e.g., <code>['GET', 'POST']</code>). Defaults to <code>['AUTO']</code> (GET if no body params, POST if body params).</li> <li><code>request_as_kwarg</code> (bool, default <code>False</code>): If <code>True</code> and <code>api=True</code>, the function will receive a <code>request: RequestData</code> keyword argument if it's defined in its signature.</li> <li><code>row</code> (bool, default <code>False</code>): If <code>True</code>, the function's raw return value is used directly. If <code>False</code> (default), the return value is automatically wrapped in a <code>Result.ok()</code> object if it's not already a <code>Result</code> or <code>ApiResult</code>.</li> <li><code>initial</code> (bool, default <code>False</code>): If <code>True</code>, this function is added to the module's \"on_start\" list and is called when the module is loaded by the <code>App</code> instance (if the module instance is a <code>MainTool</code> or similar, or if called directly).</li> <li><code>exit_f</code> (bool, default <code>False</code>): If <code>True</code>, this function is added to the module's \"on_exit\" list and is called when the <code>App</code> instance is shutting down or the module is removed.</li> <li><code>state</code> (bool, optional):<ul> <li>If <code>None</code> (default): Automatically determined. If the first parameter of the decorated function is named <code>self</code> or <code>app</code> (and type-hinted as <code>App</code>), <code>state</code> is considered <code>True</code>. Otherwise <code>False</code>.</li> <li>If <code>True</code>: The function is considered stateful. If its first parameter is <code>self</code>, it's assumed to be a method of a class instance (e.g., a <code>MainTool</code> subclass). If <code>app</code>, the <code>App</code> instance is passed.</li> <li>If <code>False</code>: The function is treated as stateless.</li> </ul> </li> <li><code>test</code> (bool, default <code>True</code>): Marks the function as testable. Used by <code>app.execute_all_functions()</code>.</li> <li><code>samples</code> (List[Dict[str, Any]], optional): A list of sample keyword arguments to be used when testing the function with <code>app.execute_all_functions()</code>.</li> <li><code>memory_cache</code> (bool, default <code>False</code>): Enables in-memory caching for the function's results.</li> <li><code>memory_cache_ttl</code> (int, default <code>300</code>): Time-to-live in seconds for memory cache entries.</li> <li><code>memory_cache_max_size</code> (int, default <code>100</code>): Max number of entries in the memory cache.</li> <li><code>file_cache</code> (bool, default <code>False</code>): Enables file-based caching for the function's results. (Stored in <code>app.data_dir/cache/...</code>).</li> <li><code>restrict_in_virtual_mode</code> (bool, default <code>False</code>): If <code>True</code>, restricts function in certain virtualized/proxied modes.</li> <li><code>level</code> (int, default <code>-1</code>): A general-purpose level or priority for the function.</li> <li><code>pre_compute</code> (Callable, optional): A function <code>(func, *args, **kwargs) -&gt; (args, kwargs)</code> called before the main function executes. It can modify args/kwargs.</li> <li><code>post_compute</code> (Callable, optional): A function <code>(result, func, *args, **kwargs) -&gt; result</code> called after the main function executes. It can modify the result.</li> <li><code>interface</code> (ToolBoxInterfaces | str, optional): Specifies the intended interface type (e.g., <code>ToolBoxInterfaces.cli</code>, <code>ToolBoxInterfaces.api</code>). Defaults to \"tb\".</li> </ul>"},{"location":"utils/#result-and-apiresult-objects","title":"<code>Result</code> and <code>ApiResult</code> Objects","text":"<ul> <li>Modules should typically return <code>Result</code> objects (or <code>ApiResult</code> for API endpoints) to provide standardized responses including success/error status, data payload, and informational messages.</li> <li>The <code>toolboxv2.utils.system.types.Result</code> class provides factory methods:<ul> <li><code>Result.ok(data=..., info=...)</code></li> <li><code>Result.json(data=..., info=...)</code> (for <code>api=True</code> functions)</li> <li><code>Result.html(data=..., info=...)</code></li> <li><code>Result.text(text_data=..., info=...)</code></li> <li><code>Result.binary(data=..., content_type=..., download_name=...)</code></li> <li><code>Result.redirect(url=..., status_code=...)</code></li> <li><code>Result.stream(stream_generator=..., info=..., cleanup_func=...)</code> (for SSE)</li> <li><code>Result.default_user_error(info=..., exec_code=...)</code></li> <li><code>Result.default_internal_error(info=..., exec_code=...)</code></li> <li><code>Result.custom_error(data=..., info=..., exec_code=...)</code></li> </ul> </li> <li>The <code>Result</code> object has a <code>task(background_task_callable)</code> method to schedule a background task to run after the main function returns.</li> </ul>"},{"location":"utils/#requestdata-object","title":"<code>RequestData</code> Object","text":"<ul> <li>For API functions (<code>api=True</code>) with <code>request_as_kwarg=True</code>, if the function signature includes <code>request: Optional[RequestData] = None</code>, it will receive an instance of <code>toolboxv2.utils.system.types.RequestData</code>.</li> <li><code>RequestData</code> provides access to:<ul> <li><code>request.method</code>, <code>request.path</code></li> <li><code>request.headers</code> (an instance of <code>Headers</code>, e.g., <code>request.headers.user_agent</code>, <code>request.headers.hx_request</code>)</li> <li><code>request.query_params</code> (dict)</li> <li><code>request.form_data</code> (dict, if applicable)</li> <li><code>request.body</code> (parsed JSON if <code>content_type</code> is <code>application/json</code>, otherwise raw bytes/str)</li> <li><code>session.SiID</code>, <code>session.user_name</code>, <code>session.level</code> (from the current user's session)</li> </ul> </li> </ul>"},{"location":"utils/#creating-a-maintool-based-module","title":"Creating a <code>MainTool</code>-based Module","text":"<p>For more complex, stateful modules, you can create a class that inherits from <code>toolboxv2.utils.system.main_tool.MainTool</code>.</p> <pre><code>from toolboxv2 import get_app, App, Result, MainTool\nfrom toolboxv2.utils.system.types import ToolBoxError\n\napp = get_app(\"MyToolModule\")\nexport = app.tb\n\nName = \"MyToolModule\"\nversion = \"0.5.0\"\n\nclass Tools(MainTool): # The class must be named 'Tools' for auto-detection by older App versions\n                      # or ensure your module file directly uses @export on methods if not named Tools.\n    # Or, you can export methods directly from any class:\n    # class MyCustomTool(MainTool):\n    #    @export(...)\n    #    def my_method(self, ...): ...\n\n    async def __ainit__(self): # Asynchronous initialization\n        # self.app is automatically available\n        # self.name, self.version, self.logger are set by MainTool's __await__\n        await super().__ainit__(name=Name, v=version, tool={\n            \"process_item\": self.process_item, # For older compatibility if functions were in 'tools' dict\n            \"get_status\": self.get_status\n        })\n        self.internal_state = \"initialized\"\n        self.app.print(f\"{self.name} (Tool) has been initialized with state: {self.internal_state}\")\n\n    @export(mod_name=Name, version=version) # Decorate methods to export them\n    def process_item(self, item_id: int, details: str) -&gt; Result:\n        # 'self' provides access to app, logger, name, version, config\n        self.app.logger.info(f\"{self.name} processing item: {item_id} - {details}\")\n        self.internal_state = f\"last_processed_{item_id}\"\n        if item_id == 0:\n            return self.return_result( # Helper from MainTool\n                error=ToolBoxError.input_error,\n                exec_code=1, # Custom error code\n                help_text=\"Item ID cannot be zero.\",\n                data_info=\"Validation failed\"\n            )\n        return Result.ok(data={\"item_id\": item_id, \"status\": \"processed by tool\"})\n\n    @export(mod_name=Name, version=version)\n    async def get_status(self) -&gt; str: # Example async method\n        await asyncio.sleep(0.01)\n        return f\"Tool {self.name} current state: {self.internal_state}\"\n\n    async def on_exit(self): # Not automatically called unless also decorated or part of a convention\n        self.app.print(f\"Tool {self.name} is shutting down its internal state.\")\n        # Perform cleanup\n\n# To ensure on_exit is called by the App framework:\n@export(mod_name=Name, version=version, exit_f=True)\nasync def custom_tool_exit_function(app_instance: App):\n    tool_instance = app_instance.get_mod(Name)\n    if tool_instance and hasattr(tool_instance, 'on_exit') and callable(tool_instance.on_exit):\n        await tool_instance.on_exit()\n</code></pre> <p>Key aspects of <code>MainTool</code>: *   Asynchronous Initialization: Use <code>async def __ainit__(self)</code> for setup. The <code>MainTool</code> itself is awaitable, and <code>__ainit__</code> is called when the instance is first awaited (e.g., by <code>app.load_mod</code> or <code>app.get_mod</code>). *   <code>self.app</code>: The <code>App</code> instance is available as <code>self.app</code>. *   <code>self.name</code>, <code>self.version</code>, <code>self.logger</code>: These are automatically set up. *   <code>self.return_result(...)</code>: A helper method for creating <code>Result</code> objects. *   Methods intended to be called via <code>app.run_any</code> should be decorated with <code>@export</code>.</p>"},{"location":"utils/#steps-to-create-a-valid-toolboxv2-module","title":"Steps to Create a Valid Toolboxv2 Module:","text":"<ol> <li>Define Module Structure: Organize your code with imports, metadata, and function/class definitions.</li> <li>Clarify Dependencies: Import necessary libraries. Handle missing optional dependencies gracefully if needed.</li> <li>Export Functions/Methods: Use the <code>@export(...)</code> decorator (e.g., <code>app.tb(...)</code>) to mark functions/methods that ToolBoxV2 should recognize.<ul> <li>Provide <code>mod_name</code> and <code>version</code>.</li> <li>Use other parameters (<code>api</code>, <code>row</code>, <code>initial</code>, <code>exit_f</code>, <code>memory_cache</code>, etc.) as needed.</li> <li>Ensure clear signatures and document parameters/return types (Python type hints are highly recommended).</li> </ul> </li> <li>Documentation and Versioning: Document your module and its functions. Use semantic versioning.</li> <li>Testing: Test your module thoroughly, including how it integrates with the ToolBoxV2 app (<code>app.run_any</code>, <code>app.get_mod</code>, etc.). Use the <code>test=True</code> and <code>samples</code> parameters in <code>@export</code> to facilitate automated testing via <code>app.execute_all_functions()</code>.</li> </ol>"},{"location":"utils/#to-isaa_1","title":"To isaa","text":""}]}