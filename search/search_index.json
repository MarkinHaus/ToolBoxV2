{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ToolBoxV2 \ud83e\uddf0","text":"<p>ToolBoxV2 is a flexible, modular framework designed for creating and managing a wide range of tools, functions, and complete applications. It supports deployment locally, on the web, or as cross-platform desktop/mobile applications.</p> <p>At its core, ToolBoxV2 integrates a Python backend with a Rust server and a Tauri-based UI, offering a powerful and versatile development experience.</p> <ul> <li>Free software: Custom License</li> <li>Officel Web page: https://simplecore.app/</li> <li>GitHub Repository: https://github.com/MarkinHaus/ToolBoxV2</li> </ul>"},{"location":"#key-goals-features","title":"Key Goals &amp; Features","text":"<p>ToolBoxV2 aims to simplify the development and usage of digital tools by:</p> <ul> <li>\ud83d\udd0c Modularity: Build applications from reusable Python modules (<code>mods</code>) and utilities (<code>utils</code>).</li> <li>\u2699\ufe0f Automation: Facilitate automation of tasks through CLI interactions and programmable APIs.</li> <li>\ud83c\udf10 Cross-Platform Interfaces:<ul> <li>Develop Desktop Applications using Tauri (Rust + Web UI).</li> <li>Create Web Applications with the <code>tbjs</code> frontend framework.</li> <li>Interact via a robust Command Line Interface (CLI).</li> </ul> </li> <li>\ud83d\ude80 Performance &amp; Safety: Leverage Rust for backend server components (Actix) and Python for scripting and application logic.</li> <li>\ud83e\udde9 Extensibility: Easily create and integrate new functions, tools, or full mini-applications.</li> <li>System Independence: Strives to make applications and tools runnable across different operating systems.</li> <li>Unified Development: Provides a cohesive environment for Python, Rust, and web technologies.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li> <p>Installation:     For detailed instructions on how to install the core Python library or set up the full-stack development environment, please see the Installation Guide.     <pre><code># Quick install for the Python package\npip install ToolBoxV2\n</code></pre></p> </li> <li> <p>Developer Guide:     To learn how to create modules, use the <code>App</code> class, and interact with the CLI, explore the full Developer Documentation. </p> </li> <li> <p>Explore the Code:     Dive into the GitHub Repository to see the project structure and contribute.</p> </li> </ul>"},{"location":"#example-use-cases","title":"Example Use Cases","text":"<p>ToolBoxV2 can be used for: *   Personal productivity tools (calendars, note-takers). *   Development utilities and automation scripts. *   Custom internal business applications. *   Interactive data processing and visualization tools. *   And much more!</p>"},{"location":"#account-management","title":"Account Management","text":"<p>For detailed information on how to manage user accounts, please see the Account Management documentation.</p>"},{"location":"#p2p-p2p_rpc_protocolmd","title":"p2p p2p_rpc_protocol.md","text":""},{"location":"#credits","title":"Credits","text":"<p>This package was created with inspiration from project structures like those generated by Cookiecutter and templates such as giswqs/pypackage.</p> <p>\u00a9 2022\u20132025 Markin Hausmanns \u2013 All rights reserved.</p>"},{"location":"account_management/","title":"Account Management","text":"<p>This document provides a guide to managing user accounts in ToolBoxV2 through the command-line interface (CLI). These commands are available through the <code>helper</code> module.</p>"},{"location":"account_management/#initial-system-setup","title":"Initial System Setup","text":"<p>Before any other account management commands can be used, the system must be initialized. This is done with the <code>init_system</code> command, which creates the first administrative user.</p>"},{"location":"account_management/#init_system","title":"<code>init_system</code>","text":"<p>This command will launch an interactive prompt to guide you through creating the first user account. This user will have the highest level of permissions.</p> <p>Usage:</p> <pre><code>tb -c helper init_system\n</code></pre> <p>The system will prompt you for a username and an email address. Upon successful creation, a new cryptographic key pair will be generated for the user, which will be used for authentication.</p>"},{"location":"account_management/#user-management","title":"User Management","text":"<p>These commands allow you to create, delete, and list users.</p>"},{"location":"account_management/#create-user","title":"<code>create-user</code>","text":"<p>Creates a new user.</p> <p>Usage:</p> <pre><code>tb -c helper create-user &lt;username&gt; &lt;email&gt;\n</code></pre> <ul> <li><code>&lt;username&gt;</code>: The desired username for the new user.</li> <li><code>&lt;email&gt;</code>: The email address for the new user.</li> </ul>"},{"location":"account_management/#delete-user","title":"<code>delete-user</code>","text":"<p>Deletes a user and all associated data, including their cryptographic keys.</p> <p>Usage:</p> <pre><code>tb -c helper delete-user &lt;username&gt;\n</code></pre> <ul> <li><code>&lt;username&gt;</code>: The username of the user to delete.</li> </ul>"},{"location":"account_management/#list-users","title":"<code>list-users</code>","text":"<p>Displays a list of all registered users, including their username, email, and permission level.</p> <p>Usage:</p> <pre><code>tb -c helper list-users\n</code></pre>"},{"location":"account_management/#device-and-access-management","title":"Device and Access Management","text":"<p>These commands are used to manage how users can access their accounts.</p>"},{"location":"account_management/#create-invitation","title":"<code>create-invitation</code>","text":"<p>Generates a one-time invitation code that allows a user to link a new device to their account.</p> <p>Usage:</p> <pre><code>tb -c helper create-invitation &lt;username&gt;\n</code></pre> <ul> <li><code>&lt;username&gt;</code>: The username of the user for whom to create the invitation.</li> </ul>"},{"location":"account_management/#send-magic-link","title":"<code>send-magic-link</code>","text":"<p>Sends a magic login link to the user's registered email address. This link can be used to log in without a password or key.</p> <p>Usage:</p> <pre><code>tb -c helper send-magic-link &lt;username&gt;\n</code></pre> <ul> <li><code>&lt;username&gt;</code>: The username of the user to whom the magic link should be sent.</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"clis/","title":"CLIS","text":"<p>Of course. Here are three rich helper guides and usage examples for your enhanced CLI applications, formatted in Markdown.</p>"},{"location":"clis/#p2p-tunnel-manager-tmc_p2p_clipy-user-guide","title":"\ud83d\ude80 P2P Tunnel Manager (<code>tmc_p2p_cli.py</code>) - User Guide","text":"<p>This utility provides a robust command-line interface to manage instances of the P2P tunneling application. It creates isolated environments for each peer and relay, making it easy to configure, run, and debug complex network setups.</p>"},{"location":"clis/#quickstart","title":"Quickstart","text":"<ol> <li> <p>Build the application: <pre><code>tb p2p build\n</code></pre></p> </li> <li> <p>Start a Relay Server: <pre><code>tb p2p start-relay main-relay --password \"a-secure-password\"\n</code></pre></p> </li> <li> <p>Start Peers:</p> <ul> <li>Provider (exposes a local service):     <pre><code>tb p2p start-peer api-provider --peer-id service-A \\\n  --relay-addr 127.0.0.1:9000 --relay-pass \"a-secure-password\" \\\n  --forward 127.0.0.1:3000\n</code></pre></li> <li>Consumer (accesses the provider's service):     <pre><code>tb p2p start-peer api-consumer --target service-A \\\n  --relay-addr 127.0.0.1:9000 --relay-pass \"a-secure-password\" \\\n  --listen 127.0.0.1:8000\n</code></pre></li> </ul> </li> <li> <p>Check Status &amp; Logs: <pre><code>tb p2p status\ntb p2p logs api-provider\n</code></pre></p> </li> <li> <p>Stop Instances: <pre><code># Stop one instance\ntb p2p stop api-consumer\n\n# Stop all instances\ntb p2p stop\n</code></pre></p> </li> </ol>"},{"location":"clis/#blob-db-cluster-manager-db_clipy-user-guide","title":"\ud83d\ude80 Blob DB Cluster Manager (<code>db_cli.py</code>) - User Guide","text":"<p>This CLI is a powerful tool for managing a distributed cluster of <code>r_blob_db</code> instances. It handles configuration, state, and provides commands for starting, stopping, and health-checking the entire cluster or individual nodes.</p>"},{"location":"clis/#quickstart_1","title":"Quickstart","text":"<ol> <li> <p>Build the application: <pre><code>tb db build\n</code></pre></p> </li> <li> <p>Initialize the Cluster: The first time you run a command, a <code>cluster_config.json</code> is created with a default two-node setup. You can customize this file.</p> </li> <li> <p>Start the Cluster: <pre><code># Starts all instances defined in cluster_config.json\ntb db start\n</code></pre></p> </li> <li> <p>Check Status &amp; Health: <pre><code>tb db status\ntb db health\n</code></pre></p> </li> <li> <p>Stop the Cluster: <pre><code># Stop a single instance\ntb db stop --instance-id instance-01\n\n# Stop all instances\ntb db stop\n</code></pre></p> </li> </ol>"},{"location":"clis/#example-2-performing-a-rolling-update-on-the-live-cluster","title":"Example 2: Performing a Rolling Update on the Live Cluster","text":"<p>Scenario: You've developed <code>v1.1.0</code> of <code>r_blob_db</code> and need to update your running <code>v1.0.0</code> cluster without any downtime. The rolling update process updates one node at a time, ensuring the cluster remains available.</p> <ol> <li> <p>Check Current Cluster Health:     Before starting, ensure all nodes are healthy.     <pre><code>tb db health\n</code></pre> You should see all instances report <code>\u2705 OK</code>.</p> </li> <li> <p>Build the New Version:     Compile the new version of your application. The manager will automatically find the new binary.     <pre><code># Assuming your code is updated to v1.1.0\ntb db build\n</code></pre></p> </li> <li> <p>Initiate the Rolling Update:     Execute the <code>update</code> command, specifying the new version string.     <pre><code>tb db update --version \"v1.1.0\"\n</code></pre></p> </li> <li> <p>Monitor the Process:     The CLI will provide detailed, real-time feedback:     <pre><code>--- Starting Rolling Update to Version v1.1.0 ---\n\n[1/2] Updating instance 'instance-01'...\n\u23f9\ufe0f  Instance 'instance-01' stopped.\n\ud83d\ude80 Starting instance 'instance-01' on port 3001...\n\u2705 Instance 'instance-01' started successfully. (PID: 12346)\n...\n\u29d6 Waiting for 'instance-01' to become healthy...\n\u2705 Instance 'instance-01' is healthy with new version.\n\n[2/2] Updating instance 'instance-02'...\n...\n--- Rolling Update Complete ---\n</code></pre></p> </li> <li> <p>Verify the Update:     Run the health check again. All instances should now report <code>OK</code> and show <code>server_version: v1.1.0</code>.     <pre><code>tb db health\n</code></pre></p> </li> </ol>"},{"location":"clis/#api-server-manager-api_managerpy-user-guide","title":"\ud83d\ude80 API Server Manager (<code>api_manager.py</code>) - User Guide","text":"<p>This manager is designed for high-availability web services. Its standout feature is the ability to perform zero-downtime updates on POSIX systems (Linux/macOS) by passing the active network socket from the old process to the new one, ensuring no client requests are dropped during the update.</p>"},{"location":"clis/#quickstart_2","title":"Quickstart","text":"<ol> <li> <p>Build the application: <pre><code># Assuming the CLI entrypoint is mapped to `tb`\ntb api build\n</code></pre></p> </li> <li> <p>Start the Server:</p> <ul> <li>On Linux/macOS (with Zero-Downtime enabled): <pre><code>tb api start --posix-zdt\n</code></pre></li> <li>On Windows (uses graceful restart): <pre><code>tb api start\n</code></pre></li> </ul> </li> <li> <p>Check Status: <pre><code>tb api status\n</code></pre></p> </li> <li> <p>Update the Server: <pre><code># First, build the new version\ntb api build\n\n# Then, run the update\ntb api update --version \"v1.2.0\" --posix-zdt\n</code></pre></p> </li> <li> <p>Stop the Server: <pre><code>tb api stop\n</code></pre></p> </li> </ol>"},{"location":"clis/#example-3-zero-downtime-deployment-on-a-linux-server","title":"Example 3: Zero-Downtime Deployment on a Linux Server","text":"<p>Scenario: Your API server is handling live traffic. You need to deploy a critical security patch (<code>v1.0.1</code>) without interrupting any ongoing client connections.</p> <ol> <li> <p>Check Initial State:     Ensure the server is running correctly. The <code>--posix-zdt</code> flag confirms that the manager is aware of the socket file descriptor.     <pre><code>tb api status --posix-zdt\n</code></pre> Output: <pre><code>--- Server Status ---\n  \u2705 RUNNING\n    PID:        11223\n    Version:    v1.0.0\n    Executable: /path/to/project/src-core/simple-core-server\n    Listening FD: 4 (POSIX ZDT Active)\n</code></pre></p> </li> <li> <p>Build the New Version:     Compile the patched version of the code.     <pre><code>tb api build\n</code></pre></p> </li> <li> <p>Execute the Zero-Downtime Update:     Run the <code>update</code> command with the <code>--posix-zdt</code> flag.     <pre><code>tb api update --version \"v1.0.1\" --posix-zdt\n</code></pre></p> </li> <li> <p>Observe the Magic:     The manager performs the following sequence seamlessly:</p> <ul> <li>It finds the persistent socket file descriptor (<code>FD: 4</code>).</li> <li>It starts the new server process (<code>v1.0.1</code>), passing it ownership of the active socket. The new server begins accepting new connections on the same port immediately.</li> <li>Once the new server is running, the manager sends a <code>SIGTERM</code> signal to the old process (<code>v1.0.0</code>).</li> <li>The old process stops accepting new connections but finishes handling any in-flight requests before shutting down.</li> <li>The state file is updated with the new PID and version.</li> </ul> <p>Terminal Output: <pre><code>--- [POSIX] Starting Zero-Downtime Update to v1.0.1 ---\n\u2705 New server started (PID: 11255).\n\u23f9\ufe0f  Process 11223 stopped.\n--- Update Complete. New PID: 11255 ---\n</code></pre></p> </li> <li> <p>Final Verification:     Check the status again. The server is still <code>RUNNING</code>, but now with the new PID and version. No client would have noticed the switch.     <pre><code>tb api status --posix-zdt\n</code></pre></p> </li> </ol> <p>and wit h the same focus new the last ui update + use real Posix by global fag: import contextlib</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/MarkinHaus/ToolBoxV2/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>ToolBox could always use more documentation, whether as part of the official ToolBox docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/MarkinHaus/ToolBoxV2/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up ToolBoxV2 for local development.</p> <ol> <li> <p>Fork the ToolBoxV2 repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> </li> </ol> <pre><code>$ git clone git@github.com:MarkinHaus/ToolBoxV2.git\n</code></pre> <ol> <li>Install your local copy into a virtualenv. Assuming you have    virtualenvwrapper installed, this is how you set up your fork for    local development:</li> </ol> <pre><code>$ mkvirtualenv ToolBoxV2\n$ cd ToolBoxV2/\n$ python setup.py develop\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li>When you're done making changes, check that your changes pass flake8    and the tests, including testing other Python versions with tox:</li> </ol> <pre><code>$ flake8 ToolBoxV2 tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.    Put your new functionality into a function with a docstring, and add    the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and    for PyPy. Check https://github.com/MarkinHaus/ToolBoxV2/pull_requests and make sure that the tests pass for all    supported Python versions.</li> </ol>"},{"location":"db_usage_guide/","title":"ToolBoxV2 Database (DB) Module Guide","text":"<p>This guide provides a comprehensive overview of how to use the ToolBoxV2 <code>DB</code> module. It is designed for both human developers and AI agents to understand how to perform persistent key-value storage within the framework.</p>"},{"location":"db_usage_guide/#1-core-concepts","title":"1. Core Concepts","text":"<p>The <code>DB</code> module acts as a standardized abstraction layer for various key-value storage backends. This design allows you to write your application logic once, using a consistent API (<code>get</code>, <code>set</code>, <code>delete</code>, etc.), while the underlying storage engine can be swapped out with minimal configuration changes.</p>"},{"location":"db_usage_guide/#key-features","title":"Key Features:","text":"<ul> <li>Unified API: A simple, consistent set of functions for all database operations.</li> <li>Multiple Backends (Modes): Supports different storage mechanisms, from simple local files to distributed blobs and Redis caches.</li> <li>Automatic Encoding: Data is automatically handled, allowing you to work with standard Python types (strings, dicts, lists) without manual serialization.</li> <li>Environment-Driven Configuration: Database modes and credentials can be configured via environment variables, allowing for easy switching between development, testing, and production setups.</li> </ul>"},{"location":"db_usage_guide/#database-modes","title":"Database Modes","text":"<p>The module can operate in several modes. The default mode is <code>CLUSTER_BLOB</code>, which is the recommended, most robust option for production environments within the ToolBoxV2 ecosystem.</p> <ul> <li><code>CLUSTER_BLOB</code> (CB): The default and recommended mode. It uses the <code>BlobDB</code> backend, which stores data as encrypted blobs in the configured root storage. This is ideal for user-specific, secure, and distributed data storage.</li> <li><code>LOCAL_DICT</code> (LC): Uses <code>MiniDictDB</code>. A simple, file-based dictionary stored locally. It's excellent for local development, testing, or storing non-critical application state.</li> <li><code>LOCAL_REDIS</code> (LR): Connects to a local Redis instance. Use this for high-performance caching or when you need the advanced data structures offered by Redis.</li> <li><code>REMOTE_REDIS</code> (RR): Connects to a remote Redis server. Suitable for shared state and caching in a distributed environment.</li> </ul>"},{"location":"db_usage_guide/#2-basic-usage","title":"2. Basic Usage","text":"<p>To use the DB module, you first need to get a handle to it from the main <code>app</code> instance. All interactions then happen through this instance.</p> <pre><code># Assuming 'app' is your ToolBoxV2 App instance\ndb = app.get_mod(\"DB\")\n\n# Or, if you have a specific instance of the DB module\ndb_spec = app.get_mod(\"DB\", spec=\"MySpecialDB\")\n</code></pre>"},{"location":"db_usage_guide/#storing-data-set","title":"Storing Data (<code>set</code>)","text":"<p>The <code>set</code> function stores a value associated with a key. It overwrites any existing value.</p> <pre><code># Store a simple string\ndb.set(\"user:123:name\", \"Alice\")\n\n# Store a dictionary (will be automatically JSON-serialized)\nuser_profile = {\"email\": \"alice@example.com\", \"level\": 10}\ndb.set(\"user:123:profile\", user_profile)\n\n# Store a list\ndb.set(\"user:123:roles\", [\"admin\", \"editor\"])\n</code></pre>"},{"location":"db_usage_guide/#retrieving-data-get","title":"Retrieving Data (<code>get</code>)","text":"<p>The <code>get</code> function retrieves a value by its key. The data is returned as a <code>Result</code> object.</p> <pre><code>result = db.get(\"user:123:name\")\n\nif result.is_ok():\n    user_name = result.get() # .get() extracts the data from the Result\n    print(f\"User name: {user_name}\") # Output: User name: Alice\n\nprofile_result = db.get(\"user:123:profile\")\nif profile_result.is_ok():\n    # The DB module automatically deserializes the JSON string back into a dict\n    profile_data = profile_result.get()\n    print(f\"User email: {profile_data['email']}\") # Output: User email: alice@example.com\n</code></pre>"},{"location":"db_usage_guide/#special-get-queries-all-and-all-k","title":"Special <code>get</code> Queries: <code>all</code> and <code>all-k</code>","text":"<p>The <code>get</code> method supports special query strings to retrieve all keys or all key-value pairs from the database.</p> <ul> <li><code>get('all-k')</code>: Returns a list of all keys in the database.</li> <li><code>get('all')</code>: Returns a list of all key-value pairs (as tuples) in the database.</li> </ul> <p>Example:</p> <pre><code># Get all keys\nall_keys_result = db.get(\"all-k\")\nif all_keys_result.is_ok():\n    keys = all_keys_result.get()\n    print(f\"All keys in the database: {keys}\")\n    # Output: All keys in the database: [\\'user:123:name\\', \\'user:123:profile\\', \\'user:123:logs\\']\n\n# Get all items (key-value pairs)\nall_items_result = db.get(\"all\")\nif all_items_result.is_ok():\n    items = all_items_result.get()\n    print(f\"All items: {items}\")\n    # Output: All items: [(\\'user:123:name\\', \\'Alice\\'), (\\'user:123:profile\\', {\\'email\\': \\'alice@example.com\\', \\'level\\': 10}), ...]\n</code></pre>"},{"location":"db_usage_guide/#checking-for-existence-if_exist","title":"Checking for Existence (<code>if_exist</code>)","text":"<p>To check if a key exists without retrieving its value, use <code>if_exist</code>.</p> <pre><code>if db.if_exist(\"user:123:name\").get():\n    print(\"User 123 exists!\")\nelse:\n    print(\"User 123 not found.\")\n</code></pre>"},{"location":"db_usage_guide/#deleting-data-delete","title":"Deleting Data (<code>delete</code>)","text":"<pre><code># Delete a single key\ndelete_result = db.delete(\"user:123:roles\")\nif delete_result.is_ok():\n    print(\"Roles deleted.\")\n\n# Delete multiple keys using a matching prefix (supported in Redis/Dict modes)\n# This would delete all keys starting with \"user:123:\"\ndb.delete(\"user:123:\", matching=True)\n</code></pre>"},{"location":"db_usage_guide/#appending-to-a-list-append_on_set","title":"Appending to a List (<code>append_on_set</code>)","text":"<p>This function is useful for adding items to a key that stores a list. If the key doesn't exist, it's created as a new list.</p> <pre><code># Assuming \"user:123:logs\" doesn't exist yet\ndb.append_on_set(\"user:123:logs\", \"User logged in.\")\n\n# Append another log\ndb.append_on_set(\"user:123:logs\", \"User updated profile.\")\n\n# Retrieve the list\nlogs_result = db.get(\"user:123:logs\")\n# logs_result.get() will be [\"User logged in.\", \"User updated profile.\"]\n</code></pre>"},{"location":"db_usage_guide/#3-configuration-and-mode-switching","title":"3. Configuration and Mode Switching","text":"<p>While the default mode is <code>CLUSTER_BLOB</code>, you can change it for development or other specific needs.</p>"},{"location":"db_usage_guide/#configuration-via-environment-variables","title":"Configuration via Environment Variables","text":"<p>The easiest way to configure the DB module is through environment variables in your <code>.env</code> file. The module will read these on startup.</p> Mode <code>DB_MODE_KEY</code> Required Environment Variables Cluster Blob <code>CB</code> (None - Uses application's internal blob storage and encryption) Local Dictionary <code>LC</code> (None - Uses local file storage) Local Redis <code>LR</code> <code>DB_CONNECTION_URI</code> (e.g., <code>redis://localhost:6379</code>) Remote Redis <code>RR</code> <code>DB_CONNECTION_URI</code> or <code>DB_USERNAME</code> &amp; <code>DB_PASSWORD</code> <p>Example <code>.env</code> file for using Local Redis: <pre><code>DB_MODE_KEY=LR\nDB_CONNECTION_URI=redis://localhost:6379\n</code></pre></p>"},{"location":"db_usage_guide/#switching-modes-programmatically","title":"Switching Modes Programmatically","text":"<p>You can switch the database mode at runtime, which is useful for testing or dynamic configuration.</p> <pre><code>from toolboxv2.mods.DB.types import DatabaseModes\n\ndb = app.get_mod(\"DB\")\n\n# Switch to Local Dictionary mode\nresult = db.edit_programmable(mode=DatabaseModes.LC)\n\nif result.is_ok():\n    print(\"Successfully switched DB mode to LOCAL_DICT\")\n\n# The module will automatically close the old connection \n# and initialize the new one.\n</code></pre> <p>This guide covers the primary functionalities of the ToolBoxV2 <code>DB</code> module. By leveraging its abstraction and flexible configuration, you can build robust applications with persistent data storage tailored to your specific needs.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#gei-isaa-redy-in-conda-with-cuda-conda-install-pytorch-torchvision-torchaudio-pytorch-cuda124-c-pytorch-c-nvidia","title":"Gei isaa redy in conda with cuda  # conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia","text":""},{"location":"faq/#errors","title":"Errors :","text":""},{"location":"faq/#modulenotfounderror-no-module-named-_cffi_backend-fix-pip-vvv-install-upgrade-force-reinstall-cffi","title":"ModuleNotFoundError: No module named '_cffi_backend' fix -&gt; pip -vvv install --upgrade --force-reinstall cffi","text":""},{"location":"faq/#extraes-langchain-experimental-astor-pyaudio-pebble-transformers-litellm-nltk-gpt4all-speechrecognition-chromadb-pydub-duckduckgo-search-langchain-groq-beautifulsoup4-langchain-huggingface-langchain-langchain-chroma-langchain-ollama-tiktoken","title":"extraes : langchain-experimental astor PyAudio Pebble transformers litellm nltk gpt4all SpeechRecognition chromadb pydub duckduckgo-search langchain-groq beautifulsoup4 langchain-huggingface langchain  langchain-chroma langchain-ollama tiktoken","text":""},{"location":"installation/","title":"ToolBoxV2: Installation Guide","text":"<p>This guide provides instructions for installing ToolBoxV2, whether you need just the core Python library or the full-stack application including the Rust server and Tauri/Web frontend.</p>"},{"location":"installation/#1-installing-the-core-python-library","title":"1. Installing the Core Python Library","text":"<p>This method is suitable if you primarily need to use ToolBoxV2 as a Python library within your own projects or want to develop Python-based modules for it.</p>"},{"location":"installation/#option-a-stable-release-from-pypi-recommended","title":"Option A: Stable Release from PyPI (Recommended)","text":"<p>This is the preferred method for installing the latest stable release of the ToolBoxV2 Python package.</p> <ol> <li> <p>Ensure you have Python and pip:     If you don't have Python and pip installed, this Python installation guide can help. We recommend Python 3.10 or newer.</p> </li> <li> <p>Install ToolBoxV2:     Open your terminal or command prompt and run:     <pre><code>pip install ToolBoxV2\n</code></pre> Consider using a virtual environment to manage project dependencies: <pre><code># Create a virtual environment (optional but recommended)\npython -m venv .venv\n# Activate it (Windows)\n# .venv\\Scripts\\activate\n# Activate it (macOS/Linux)\n# source .venv/bin/activate\n\npip install ToolBoxV2\n</code></pre></p> </li> </ol>"},{"location":"installation/#option-b-from-source-latest-development-version","title":"Option B: From Source (Latest Development Version)","text":"<p>This method allows you to get the very latest code from the GitHub repository, which might include new features or changes not yet in a stable release.</p> <ol> <li> <p>Clone the Repository: <pre><code>git clone https://github.com/MarkinHaus/ToolBoxV2.git\ncd ToolBoxV2\n</code></pre></p> </li> <li> <p>Install in Editable Mode:     This installs the package from your local clone, and any changes you make to the source code will be immediately reflected in your environment.</p> <ul> <li>Using pip: <pre><code># Recommended: Activate a virtual environment first\npip install -e .\n</code></pre></li> <li>Using <code>uv</code> (a fast Python package installer and resolver): <pre><code># Recommended: Activate a virtual environment first\nuv pip install -e .\n</code></pre></li> <li>Using the provided script (sets up environment):     This script creates a virtual environment and installs dependencies.     <pre><code>chmod +x install_python_env.sh\n./install_python_env.sh\n</code></pre></li> </ul> </li> </ol>"},{"location":"installation/#option-c-directly-from-github-with-pip","title":"Option C: Directly from GitHub with pip","text":"<p>You can also install directly from the GitHub repository without cloning it first: <pre><code>pip install git+https://github.com/MarkinHaus/ToolBoxV2.git\n</code></pre></p>"},{"location":"installation/#2-installing-the-full-stack-desktopweb-application","title":"2. Installing the Full Stack Desktop/Web Application","text":"<p>This setup is for developers who want to run or develop the complete ToolBoxV2 application, including the Python backend, Rust server (Actix), and the Tauri-based desktop application or <code>tbjs</code> web frontend.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed on your system:</p> <ul> <li>Python: Version 3.10 or higher.</li> <li>Rust and Cargo: Install from rust-lang.org.</li> <li>Node.js and npm/pnpm: Install from nodejs.org. We recommend <code>pnpm</code> for managing Node.js dependencies in this project.<ul> <li>Install <code>pnpm</code> globally: <code>npm install -g pnpm</code></li> </ul> </li> <li>Tauri CLI: Install using Cargo: <code>cargo install tauri-cli</code></li> </ul> <p>Ensure the virtual environment created by the script (or one you created manually) is activated for the subsequent steps.</p> <ol> <li>Install Node.js Dependencies and Build Rust Components:     From the root of the <code>ToolBoxV2</code> directory:     <pre><code>pnpm install  # Installs Node.js dependencies for tbjs and Tauri frontend\n</code></pre>     The Rust backend (<code>src-core/</code>) and Tauri components are typically built as part of the <code>pnpm</code> scripts defined in <code>package.json</code>. If you need to build the Rust core manually:     <pre><code># (Usually not needed if using pnpm scripts)\n# cargo build --release --manifest-path src-core/Cargo.toml\n</code></pre> the build step is Usually handled by the api flow</li> </ol>"},{"location":"installation/#running-the-application-in-cli","title":"Running the Application in CLI","text":"<ul> <li>Row python runner tb <pre><code>tb -c {MOD_NAME} {FUCTION_NAME} {AGRGS} --kwargs name:value\n</code></pre></li> <li>or run in ipython <pre><code>tb --ipy\n</code></pre></li> </ul>"},{"location":"installation/#running-the-application-in-server-mode-for-web-and-desktop","title":"Running the Application in Server mode for web and Desktop","text":"<p>Refer to the scripts in the <code>package.json</code> file for various ways to run and build the application. Common commands include:</p> <ul> <li> <p>Web Development Mode (tbjs frontend with hot-reloading): <pre><code>pnpm dev\n# or live\n</code></pre>     This typically starts the Rust server and the web frontend development server.</p> </li> <li> <p>Tauri Desktop Application (Development Mode): <pre><code>pnpm tauri dev\n</code></pre>     This will build and run the Tauri desktop application with hot-reloading for the frontend.</p> </li> <li> <p>Build Tauri Desktop Application (Production): <pre><code>pnpm tauri build # Or a custom script like `pnpm tauriB` if defined\n</code></pre>     This creates a distributable binary of the desktop application.</p> </li> </ul> <p>For more specific build and run commands, please consult the <code>scripts</code> section in the <code>package.json</code> file located in the <code>ToolBoxV2</code> repository root or use the CLI help: <pre><code>    tb --help\n    # or\n    python -m toolboxv2 --help\n</code></pre></p>"},{"location":"installation/#developing-tip-use-to-activate-all-hooks","title":"developing tip use to activate all hooks","text":"<pre><code>    bash .github/hooks/setup_hooks.sh\n</code></pre>"},{"location":"installation/#auto-version-commit-hook-add-to-the-commit-msg-and-for-auto-summary","title":"auto version commit hook add &lt;#&gt; to the commit msg and  for auto summary","text":""},{"location":"installation/#auto-tagging-of-version-dev-alpha-or-release-tagging-syntax-in-commit-msg","title":"auto tagging of version dev, alpha or release tagging syntax in commit msg <ul> <li>[t:d] for dev</li> <li>[t:a] for alpha and</li> <li>[t:r] for release</li> </ul> <p>all with auto versioning</p>","text":""},{"location":"installation/#pre-commit-hook","title":"pre-commit hook","text":"<p>runs Ruff Bandit Safety versions and on  in the commit msg auto summary of the changes crates an report in local-reports"},{"location":"installation/#_1","title":"????????? <p><pre><code>INSTALLER_URL=\"https://raw.githubusercontent.com/MarkinHaus/ToolBoxV2/refs/heads/master/installer.sh\"; (echo \"Fetching installer script...\" &amp;&amp; curl -sSL -o installer.sh \"$INSTALLER_URL\" &amp;&amp; echo \"Creating default 'init.config'...\" &amp;&amp; cat &lt;&lt;EOL &gt; init.config &amp;&amp; echo \"# ToolBoxV2 Installer Configuration\" &amp;&amp; echo \"# File will be located at: $(pwd)/init.config\" &amp;&amp; echo \"# Modify values below as needed before proceeding.\" &amp;&amp; echo \"# The installer (installer.sh) will use these if this file exists and no arguments are provided to it.\" &amp;&amp; echo \"# --- Example values (uncomment and change if needed): ---\" &amp;&amp; echo \"# TB_VERSION=latest\" &amp;&amp; echo \"# INSTALL_SOURCE=pip\" &amp;&amp; echo \"# PKG_MANAGER=pip\" &amp;&amp; echo \"# PYTHON_VERSION_TARGET=3.11\" &amp;&amp; echo \"# ISAA_EXTRA=false\" &amp;&amp; echo \"# DEV_EXTRA=false\" &amp;&amp; echo \"# INSTALL_LOCATION_TYPE=apps_folder\" &amp;&amp; EOL &amp;&amp; INIT_CONFIG_PATH=\"$(pwd)/init.config\" &amp;&amp; echo -e \"\\n\\033[0;32m\ud83d\udcc4 Default 'init.config' created at:\\033[0m \\033[1;33m$INIT_CONFIG_PATH\\033[0m\" &amp;&amp; echo -e \"   You can review or modify it now in another terminal if you wish.\" &amp;&amp; echo -e \"   The main script (installer.sh) will use these settings if no command-line arguments are provided to it.\" &amp;&amp; read -p \"\u23f3 Press [Enter] to make the installer executable and run it...\" REPLY &amp;&amp; chmod +x installer.sh &amp;&amp; echo \"\ud83d\ude80 Running installer...\" &amp;&amp; ./installer.sh) || echo -e \"\\033[0;31m\u274c An error occurred during the setup process. Please check messages above.\\033[0m\"\n</code></pre> onliner installer</p>","text":""},{"location":"isaa/","title":"ISAA (Intelligent System Agent Architecture) Module Documentation","text":"<p>Version: 0.2.0</p>"},{"location":"isaa/#1-overview","title":"1. Overview","text":"<p>The ISAA module provides a comprehensive framework for building, managing, and orchestrating AI agents. It leverages the <code>EnhancedAgent</code> and <code>EnhancedAgentBuilder</code> for creating sophisticated agents with capabilities like tool use, code execution, web interaction, and persistent memory. The module is designed to be extensible and configurable, allowing developers to create complex multi-agent systems and automated workflows.</p> <p>Key features include: *   Advanced Agent System: Based on <code>EnhancedAgent</code> for robust and production-ready agents. *   Flexible Agent Configuration: Uses <code>EnhancedAgentBuilder</code> for fluent and detailed agent setup. *   Task Chain Management: Define and execute sequences of agent actions or tool uses. *   Interactive Code Execution Pipelines: Stateful Python execution environments for agents. *   Semantic Memory: Persistent knowledge storage and retrieval using <code>AISemanticMemory</code>. *   Tool Integration: Supports ADK-compatible tools, custom functions, and has provisions for LangChain tools. *   Asynchronous Operations: Many core functionalities are <code>async</code> for better performance.</p>"},{"location":"isaa/#2-core-concepts","title":"2. Core Concepts","text":""},{"location":"isaa/#21-enhancedagent","title":"2.1. <code>EnhancedAgent</code>","text":"<p>The <code>EnhancedAgent</code> is the primary agent class in ISAA. It integrates: *   LiteLLM: For interaction with a wide range of LLMs. *   ADK (Agent Development Kit): For structured tool use, planning, and code execution (if ADK is available and configured). *   A2A (Agent-to-Agent): For inter-agent communication (if A2A is available). *   MCP (Model Context Protocol): For exposing agent capabilities (if MCP is available). *   World Model: A dictionary-like persistent state for the agent. *   Cost Tracking: Built-in user cost tracking. *   Callbacks: For streaming, progress, and post-run actions.</p>"},{"location":"isaa/#22-enhancedagentbuilder","title":"2.2. <code>EnhancedAgentBuilder</code>","text":"<p>The <code>EnhancedAgentBuilder</code> provides a fluent API to configure and construct <code>EnhancedAgent</code> instances. Key aspects: *   Configuration Model (<code>BuilderConfig</code>): A Pydantic model that holds all serializable configurations for an agent. This can be saved to and loaded from JSON. *   Model Configuration: Specify LLM model, API keys (via env vars), temperature, etc. *   Behavior: Streaming, logging, initial world model data. *   Framework Integrations: Enable and configure ADK, A2A, MCP. *   Tool Management: Add ADK tools (including wrapped functions). *   Cost Tracking: Configure persistence for user costs. *   Telemetry: Configure OpenTelemetry.</p>"},{"location":"isaa/#3-initialization-and-configuration-tools-class","title":"3. Initialization and Configuration (<code>Tools</code> Class)","text":"<p>The <code>Tools</code> class is the main entry point for interacting with the ISAA module.</p> <pre><code>from toolboxv2 import get_app\nfrom toolboxv2.mods.isaa.module import Tools\n\n# Get the application instance (if using toolboxv2 framework)\napp = get_app(\"my_application\")\nisaa = Tools(app=app) # or isaa = app.get_mod(\"isaa\") if registered\n\n# Initialize ISAA (loads configs, sets up defaults)\n# This is now an async operation if it involves building default agents\nasync def initialize_isaa():\n    await isaa.init_isaa()\n    print(\"ISAA initialized.\")\n\n# asyncio.run(initialize_isaa())\n</code></pre>"},{"location":"isaa/#31-configuration-isaaconfig","title":"3.1. Configuration (<code>isaa.config</code>)","text":"<p>The <code>isaa.config</code> dictionary holds various settings: *   <code>DEFAULTMODEL*</code>: Default LLM model identifiers for different agent types (e.g., <code>DEFAULTMODEL0</code>, <code>DEFAULTMODELCODE</code>). These can be overridden by environment variables. *   <code>agents-name-list</code>: A list of registered agent names. *   <code>controller_file</code>: Path to the JSON file for <code>ControllerManager</code> (LLM modes). *   Other internal states and paths.</p> <p>API keys (like <code>OPENAI_API_KEY</code>, <code>GEMINI_API_KEY</code>, etc.) are typically loaded from environment variables by LiteLLM or explicitly set in the <code>EnhancedAgentBuilder</code> via <code>with_api_key_from_env()</code>.</p>"},{"location":"isaa/#32-isaaon_exit","title":"3.2. <code>isaa.on_exit()</code>","text":"<p>Called when the application or module shuts down. It saves: *   Agent builder configurations (<code>BuilderConfig</code> dicts from <code>isaa.agent_data</code>). *   <code>ControllerManager</code> state. *   <code>AgentChain</code> definitions. *   <code>Scripts</code> definitions. *   ISAA <code>Tools</code> class configuration.</p>"},{"location":"isaa/#4-agent-management","title":"4. Agent Management","text":"<p>All agent management functions are now primarily <code>async</code>.</p>"},{"location":"isaa/#41-getting-an-agent-builder-async-get_agent_builder","title":"4.1. Getting an Agent Builder (<code>async get_agent_builder</code>)","text":"<p>This method returns a pre-configured <code>EnhancedAgentBuilder</code> instance.</p> <p><pre><code>async def manage_agent_builder():\n    # Get a default builder for an agent named \"coder_agent\"\n    coder_builder = await isaa.get_agent_builder(\"coder_agent\")\n\n    # Further configure the builder\n    coder_builder.with_model(\"anthropic/claude-3-haiku-20240229\")\n    coder_builder.with_system_message(\"You are a master Python programmer.\")\n    coder_builder.enable_adk_code_executor(\"adk_builtin\") # If ADK is available\n\n    # ... other configurations ...\n    return coder_builder\n</code></pre> Default builders come with common ISAA tools like <code>runAgent</code>, <code>memorySearch</code>, <code>searchWeb</code>, and <code>shell</code>.</p>"},{"location":"isaa/#42-registering-an-agent-async-register_agent","title":"4.2. Registering an Agent (<code>async register_agent</code>)","text":"<p>Once an <code>EnhancedAgentBuilder</code> is configured, its configuration can be registered with ISAA. The agent instance itself will be built on demand.</p> <p><pre><code>async def register_my_agent():\n    builder = await isaa.get_agent_builder(\"my_query_agent\")\n    builder.with_system_message(\"You answer questions based on internal memory.\")\n\n    await isaa.register_agent(builder)\n    print(\"Agent 'my_query_agent' configuration registered.\")\n</code></pre> This saves the builder's configuration to a JSON file (e.g., <code>.data/app_id/Agents/my_query_agent.agent.json</code>) and stores the config dictionary in <code>isaa.agent_data</code>.</p>"},{"location":"isaa/#43-retrieving-an-agent-instance-async-get_agent","title":"4.3. Retrieving an Agent Instance (<code>async get_agent</code>)","text":"<p>This <code>async</code> method retrieves (and builds if necessary) an <code>EnhancedAgent</code> instance.</p> <p><pre><code>async def retrieve_and_use_agent():\n    my_agent = await isaa.get_agent(\"my_query_agent\")\n    # my_agent is now an instance of EnhancedAgent\n\n    # If you need to override the model for this instance (will rebuild if different)\n    # my_agent_gpt4 = await isaa.get_agent(\"my_query_agent\", model_override=\"openai/gpt-4-turbo\")\n\n    response = await my_agent.a_run(\"What is the capital of France?\")\n    print(response)\n</code></pre> If an agent configuration exists, it's loaded. Otherwise, a default builder is used. The agent instance is cached in <code>isaa.config['agent-instance-{name}']</code>.</p>"},{"location":"isaa/#5-running-agents-and-tasks","title":"5. Running Agents and Tasks","text":""},{"location":"isaa/#51-running-an-agent-async-run_agent","title":"5.1. Running an Agent (<code>async run_agent</code>)","text":"<p>This is the primary method to interact with a registered agent.</p> <p><pre><code>async def run_specific_agent():\n    # Ensure agent is registered (e.g., during init_isaa or manually)\n    # For example, register a 'self' agent if not already present\n    if \"self\" not in isaa.config.get(\"agents-name-list\", []):\n        self_builder = await isaa.get_agent_builder(\"self\")\n        await isaa.register_agent(self_builder)\n\n    response = await isaa.run_agent(\"self\", \"Tell me a joke.\")\n    print(f\"Self agent says: {response}\")\n\n    # Example with session_id for persistent history with EnhancedAgent\n    session_id = \"user123_chat_session\"\n    response1 = await isaa.run_agent(\"self\", \"My name is Bob.\", session_id=session_id)\n    response2 = await isaa.run_agent(\"self\", \"What is my name?\", session_id=session_id)\n    print(f\"Agent remembers: {response2}\")\n</code></pre> The <code>run_agent</code> method now calls the <code>EnhancedAgent.a_run()</code> method, which supports features like session-based history, world model updates, ADK tool execution, etc.</p>"},{"location":"isaa/#52-mini-task-completion-async-mini_task_completion","title":"5.2. Mini Task Completion (<code>async mini_task_completion</code>)","text":"<p>For quick, one-off LLM calls without full agent capabilities.</p> <pre><code>async def run_mini_task():\n    translation = await isaa.mini_task_completion(\n        mini_task=\"Translate to German.\",\n        user_task=\"Hello, how are you?\"\n    )\n    print(f\"Translation: {translation}\")\n</code></pre>"},{"location":"isaa/#53-structured-output-async-format_class","title":"5.3. Structured Output (<code>async format_class</code>)","text":"<p>To get LLM output structured according to a Pydantic model.</p> <pre><code>from pydantic import BaseModel\n\nclass UserInfo(BaseModel):\n    name: str\n    age: int\n    city: Optional[str] = None\n\nasync def get_structured_info():\n    user_data_dict = await isaa.format_class(\n        UserInfo,\n        \"The user is Alice, 30 years old, from New York.\"\n    )\n    if user_data_dict:\n        user_info = UserInfo(**user_data_dict)\n        print(f\"Parsed User: {user_info.name}, Age: {user_info.age}\")\n</code></pre>"},{"location":"isaa/#6-task-chains","title":"6. Task Chains","text":"<p>Task chains allow defining a sequence of operations involving agents or tools.</p>"},{"location":"isaa/#61-defining-a-task-chain","title":"6.1. Defining a Task Chain","text":"<p>Task chains are defined as lists of dictionaries, where each dictionary represents a task. The <code>TaskChain</code> Pydantic model from <code>toolboxv2.mods.isaa.types</code> can be used for validation.</p> <p>Example structure for a task in the list: <pre><code>{\n  \"use\": \"agent\", // \"agent\", \"tool\", \"chain\"\n  \"name\": \"agent_name_or_tool_name_or_chain_name\",\n  \"args\": \"Prompt or arguments, can use $variable or $user-input\",\n  \"return_key\": \"my_result_variable\" // Result stored under this key\n}\n</code></pre></p>"},{"location":"isaa/#62-creating-a-task-chain-async-create_task_chain","title":"6.2. Creating a Task Chain (<code>async create_task_chain</code>)","text":"<p>Uses an LLM (typically \"TaskChainAgent\") to generate a task chain definition from a natural language prompt.</p> <pre><code>async def create_my_chain():\n    chain_name = await isaa.create_task_chain(\n        \"Create a plan to research a topic: first search the web, then summarize findings.\"\n    )\n    if chain_name:\n        print(f\"Task chain '{chain_name}' created.\")\n        isaa.save_task(chain_name) # Save it\n    else:\n        print(\"Failed to create task chain.\")\n</code></pre>"},{"location":"isaa/#63-managing-task-chains","title":"6.3. Managing Task Chains","text":"<ul> <li><code>isaa.add_task(chain_name, task_definition_list)</code>: Manually add/update a chain.</li> <li><code>isaa.get_task(chain_name)</code>: Get the definition of a chain.</li> <li><code>isaa.list_task()</code>: List names of all available chains.</li> <li><code>isaa.save_task(chain_name=None)</code>: Save one or all chains to file.</li> <li><code>isaa.load_task(chain_name=None)</code>: Load one or all chains from file.</li> </ul>"},{"location":"isaa/#64-running-a-task-chain-async-run_task","title":"6.4. Running a Task Chain (<code>async run_task</code>)","text":"<p>Executes a defined task chain.</p> <p><pre><code>async def execute_my_chain():\n    # Assume \"research_topic_chain\" was created and saved earlier\n    isaa.load_task(\"research_topic_chain\") # Load if not already in memory\n    results = await isaa.run_task(\n        task_input=\"Quantum computing advancements in 2024\",\n        chain_name=\"research_topic_chain\"\n    )\n    print(\"Task chain execution results:\", results)\n</code></pre> The <code>ChainTreeExecutor</code> handles variable injection (<code>$variable_name</code>, <code>$user-input</code>) and result passing between tasks.</p>"},{"location":"isaa/#7-pipelines-for-code-execution-pipeline-class","title":"7. Pipelines for Code Execution (<code>Pipeline</code> Class)","text":"<p>The <code>Pipeline</code> class (from <code>toolboxv2.mods.isaa.CodingAgent.live</code>) provides a stateful environment for agents to execute Python code iteratively. It uses a mock IPython interface.</p>"},{"location":"isaa/#71-getting-a-pipeline-instance-async-get_pipe","title":"7.1. Getting a Pipeline Instance (<code>async get_pipe</code>)","text":"<p>Retrieves or creates a <code>Pipeline</code> instance associated with a specific ISAA agent. The agent's context (variables, potentially its LLM for thinking within the pipeline) can influence the pipeline's behavior.</p> <pre><code>async def setup_pipeline():\n    # Get a pipeline associated with the 'coder_agent'\n    # Ensure 'coder_agent' is registered\n    coder_agent_builder = await isaa.get_agent_builder(\"coder_agent\")\n    coder_agent_builder.with_system_message(\"You write and execute Python code to solve problems.\")\n    await isaa.register_agent(coder_agent_builder)\n\n    coder_pipeline = await isaa.get_pipe(\"coder_agent\", verbose=True)\n    return coder_pipeline\n</code></pre>"},{"location":"isaa/#72-running-a-pipeline-async-run_pipe","title":"7.2. Running a Pipeline (<code>async run_pipe</code>)","text":"<p>Executes a task within the pipeline. The agent associated with the pipeline will \"think\" and generate code or actions to be executed by the pipeline's IPython environment.</p> <p><pre><code>async def execute_pipeline_task():\n    coder_pipeline = await isaa.get_pipe(\"coder_agent\") # Assumes \"coder_agent\" is set up\n\n    task_description = \"Define a function to calculate factorial and test it with n=5.\"\n    pipeline_result = await coder_pipeline.run(task_description)\n\n    print(f\"Pipeline Final Result: {pipeline_result.result}\")\n    print(\"Execution History:\")\n    for record in pipeline_result.execution_history:\n        print(f\"  Code: {record.code[:50]}... -&gt; Result: {record.result}, Error: {record.error}\")\n    print(\"Final Variables in Pipeline:\", pipeline_result.variables)\n</code></pre> The <code>Pipeline.run</code> method involves multiple turns of the agent thinking, generating code/actions, and the pipeline executing them, until the task is marked \"done\" or iterations are exhausted.</p>"},{"location":"isaa/#8-semantic-memory-aisemanticmemory","title":"8. Semantic Memory (<code>AISemanticMemory</code>)","text":"<p>ISAA uses <code>AISemanticMemory</code> for persistent, semantic storage and retrieval of information.</p>"},{"location":"isaa/#81-accessing-memory-isaaget_memory","title":"8.1. Accessing Memory (<code>isaa.get_memory</code>)","text":"<pre><code># Get the global AISemanticMemory instance\nsemantic_memory = isaa.get_memory()\n\n# AISemanticMemory can manage multiple named \"memory spaces\" (KnowledgeBase instances)\n# To get a specific KnowledgeBase instance (e.g., for an agent):\nagent_kb = isaa.get_memory(name=\"my_agent_context\") # This will create if not exists\n</code></pre>"},{"location":"isaa/#82-interacting-with-memory","title":"8.2. Interacting with Memory","text":"<p>The <code>AISemanticMemory</code> class (and its underlying <code>KnowledgeBase</code> instances) provides methods like: *   <code>async add_data(memory_name: str, data: ..., metadata: ...)</code>: Adds data to a specific memory space. *   <code>async query(query: str, memory_names: ..., to_str: bool)</code>: Queries one or more memory spaces. *   <code>async unified_retrieve(...)</code>: A more comprehensive retrieval method.</p> <p><pre><code>async def use_semantic_memory():\n    mem = isaa.get_memory()\n    agent_name = \"researcher\"\n\n    # Ensure the agent's memory space exists (usually handled by agent init)\n    await mem.create_memory(agent_name) # Or rely on get_memory(name=...)\n\n    # Add data\n    await mem.add_data(\n        memory_name=agent_name,\n        data=\"Photosynthesis is a process used by plants to convert light energy into chemical energy.\",\n        metadata={\"source\": \"biology_notes\"}\n    )\n\n    # Query data\n    results = await mem.query(\n        query=\"How do plants get energy?\",\n        memory_names=[agent_name],\n        to_str=True\n    )\n    print(f\"Memory search results: {results}\")\n</code></pre> <code>EnhancedAgent</code> instances often have their world model, but can also interact with <code>AISemanticMemory</code> via tools for broader knowledge. <code>ChatSession</code> (used by <code>Pipeline</code>) also uses <code>AISemanticMemory</code>.</p>"},{"location":"isaa/#9-tool-integration","title":"9. Tool Integration","text":""},{"location":"isaa/#91-default-isaa-tools","title":"9.1. Default ISAA Tools","text":"<p>Agents created with <code>get_agent_builder</code> automatically get several tools: *   <code>runAgent</code>: To call other registered ISAA agents. *   <code>memorySearch</code>: To query the <code>AISemanticMemory</code>. *   <code>saveDataToMemory</code>: To save data into the agent's context in <code>AISemanticMemory</code>. *   <code>searchWeb</code>: Uses <code>WebScraper</code> to search the internet. *   <code>shell</code>: Executes shell commands using <code>shell_tool_function</code>. *   <code>runCodePipeline</code> (for agents like \"self\", \"code\"): To invoke a <code>Pipeline</code> task.</p> <p>These are added as ADK-compatible function tools to the <code>EnhancedAgentBuilder</code>.</p>"},{"location":"isaa/#92-adding-custom-and-langchain-tools-async-init_tools","title":"9.2. Adding Custom and LangChain Tools (<code>async init_tools</code>)","text":"<p>The <code>init_tools</code> method is intended for adding external tools, particularly LangChain tools, to an agent builder.</p> <p><pre><code># This is a conceptual example, as init_tools itself needs to be fully async\n# and adapt to how EnhancedAgentBuilder handles external tool definitions.\n\nasync def add_external_tools():\n    builder = await isaa.get_agent_builder(\"tool_user_agent\")\n\n    # Configuration for tools (example)\n    tools_config = {\n        \"lagChinTools\": [\"wikipedia\", \"ddg-search\"], # Example LangChain tool names\n        # \"huggingTools\": [], # HF tools are also LC tools\n        # \"Plugins\": [] # AIPluginTool\n    }\n\n    # init_tools would modify the builder by adding wrapped LangChain tools\n    await isaa.init_tools(tools_config, builder) # Pass the builder instance\n\n    await isaa.register_agent(builder)\n\n    agent_with_tools = await isaa.get_agent(\"tool_user_agent\")\n    response = await agent_with_tools.a_run(\"Search Wikipedia for 'Large Language Models'\")\n    print(response)\n</code></pre> Note: Wrapping arbitrary LangChain tools (which can have complex Pydantic <code>args_schema</code>) into ADK <code>FunctionTool</code>s (which prefer simpler schemas or Pydantic models for arguments) can be non-trivial. <code>init_tools</code> will need careful implementation to handle schema mapping or require tools to be provided as simple callables.</p>"},{"location":"isaa/#10-example-usage-flow","title":"10. Example Usage Flow","text":"<pre><code>import asyncio\nfrom toolboxv2 import get_app\nfrom toolboxv2.mods.isaa.module import Tools\nfrom pydantic import BaseModel\n\n# --- Setup ---\napp = get_app(\"isaa_demo_app\")\nisaa = Tools(app=app)\n\n# --- Pydantic Model for Structured Output ---\nclass AnalysisResult(BaseModel):\n    topic: str\n    key_points: List[str]\n    sentiment: Optional[str] = None\n\nasync def main_demo():\n    # 1. Initialize ISAA (loads configs, ControllerManager, etc.)\n    await isaa.init_isaa()\n    print(\"ISAA Initialized.\")\n\n    # 2. Create and Register a \"Researcher\" Agent\n    researcher_builder = await isaa.get_agent_builder(\"Researcher\")\n    researcher_builder.with_system_message(\n        \"You are a research assistant. Your job is to find information using web search and summarize it.\"\n    )\n    researcher_builder.with_model(\"openai/gpt-3.5-turbo\") # Example model\n    # The default builder already adds searchWeb and memory tools.\n    await isaa.register_agent(researcher_builder)\n    print(\"Researcher agent registered.\")\n\n    # 3. Create and Register a \"Summarizer\" Agent for structured output\n    summarizer_builder = await isaa.get_agent_builder(\"Summarizer\")\n    summarizer_builder.with_system_message(\n        \"You summarize text and extract key information into a structured format.\"\n    )\n    summarizer_builder.with_model(\"openai/gpt-4-turbo\") # Example model, good for JSON\n    await isaa.register_agent(summarizer_builder)\n    print(\"Summarizer agent registered.\")\n\n    # 4. Define a Task Chain\n    chain_name = \"WebResearchAndAnalyze\"\n    research_tasks = [\n        {\n            \"use\": \"agent\",\n            \"name\": \"Researcher\",\n            \"args\": \"Find recent news about AI in healthcare. Focus on the top 3 articles. User input: $user-input\", # $user-input will be main query\n            \"return_key\": \"research_findings\"\n        },\n        {\n            \"use\": \"agent\",\n            \"name\": \"Summarizer\",\n            # The 'Summarizer' agent will use its format_class capability internally if prompted correctly\n            # For this example, we assume 'Summarizer' is prompted to produce AnalysisResult\n            # A more robust way is to have a specific tool/agent that *only* does formatting.\n            \"args\": \"Analyze the following research findings and structure them: $research_findings. Extract topic, key points, and sentiment.\",\n            \"return_key\": \"structured_analysis\"\n        }\n    ]\n    isaa.agent_chain.add(chain_name, research_tasks)\n    isaa.save_task(chain_name)\n    print(f\"Task chain '{chain_name}' created and saved.\")\n\n    # 5. Run the Task Chain\n    user_query = \"Latest breakthroughs in AI-driven drug discovery\"\n    print(f\"\\nRunning task chain '{chain_name}' for query: '{user_query}'\")\n    chain_results = await isaa.run_task(user_query, chain_name)\n    print(\"\\n--- Task Chain Results ---\")\n    if chain_results and \"structured_analysis\" in chain_results:\n        analysis_output = chain_results[\"structured_analysis\"]\n        # If the summarizer agent directly returned a dict matching AnalysisResult:\n        try:\n            # The result from an agent run is typically a string.\n            # If the 'Summarizer' was specifically designed to output JSON string for AnalysisResult:\n            analysis_dict = json.loads(analysis_output) # Agent must output valid JSON string\n            structured_data = AnalysisResult(**analysis_dict)\n            print(f\"Topic: {structured_data.topic}\")\n            print(\"Key Points:\")\n            for point in structured_data.key_points:\n                print(f\"  - {point}\")\n            if structured_data.sentiment:\n                print(f\"Sentiment: {structured_data.sentiment}\")\n        except Exception as e:\n            print(f\"Could not parse structured analysis: {e}\")\n            print(\"Raw analysis output:\", analysis_output)\n    else:\n        print(\"Chain did not produce expected 'structured_analysis'. Full results:\", chain_results)\n\n    # 6. Example of using a Pipeline with a \"Coder\" agent\n    coder_builder = await isaa.get_agent_builder(\"PyCoder\")\n    coder_builder.with_system_message(\"You are a Python coding assistant. You write and execute Python code to solve problems. Ensure your code prints results or returns values.\")\n    coder_builder.enable_adk_code_executor(\"unsafe_simple\") # Or \"adk_builtin\" if model supports\n    await isaa.register_agent(coder_builder)\n\n    py_coder_pipeline = await isaa.get_pipe(\"PyCoder\", verbose=True) # Get pipeline for this agent\n    pipeline_task = \"Write a Python function that takes a list of numbers and returns their sum. Then, call this function with the list [1, 2, 3, 4, 5] and print the result.\"\n    print(f\"\\nRunning Pipeline for task: '{pipeline_task}'\")\n    pipeline_result = await py_coder_pipeline.run(pipeline_task)\n    print(\"\\n--- Pipeline Final Output ---\")\n    print(pipeline_result.result)\n    print(\"\\n--- Pipeline Variables ---\")\n    # Filter out internal IPython variables for clarity\n    final_vars = {k: v for k, v in pipeline_result.variables.items() if not k.startswith('_') and k not in ['In', 'Out', 'exit', 'quit', 'get_ipython', 'open']}\n    print(json.dumps(final_vars, default=str, indent=2))\n\n    # 7. Clean up (optional, as on_exit handles saving)\n    # isaa.on_exit()\n\nif __name__ == \"__main__\":\n    asyncio.run(main_demo())\n</code></pre>"},{"location":"isaa/#11-important-notes","title":"11. Important Notes","text":"<ul> <li>Asynchronous Nature: Most core methods of the <code>Tools</code> class are now <code>async</code>. Ensure your calling code uses <code>await</code> appropriately or runs within an asyncio event loop.</li> <li>Agent Configuration: Agent capabilities are primarily defined by their system message, the tools provided to them via the <code>EnhancedAgentBuilder</code>, and their underlying LLM model.</li> <li>Error Handling: Robust error handling should be implemented around <code>async</code> calls, especially for network-dependent operations like LLM calls or web interactions.</li> <li>ADK Integration: Full ADK functionality (planning, advanced tool schemas, long-running operations) requires Google ADK to be installed and properly configured. The <code>EnhancedAgentBuilder</code> provides methods like <code>with_adk_code_executor</code>, <code>with_adk_tool_instance</code>, etc.</li> <li>Security: Be cautious when enabling code execution (<code>unsafe_simple</code> executor is for development only) or shell access.</li> </ul> <p>This documentation provides a starting point for using the refactored ISAA module. As the module evolves, further details on specific component interactions (e.g., advanced ADK planner configurations, A2A/MCP server setup via builder) will be crucial.</p>"},{"location":"module_creation_guide/","title":"ToolBoxV2 Module Creation Guide","text":"<p>This guide provides a comprehensive overview of how to create, structure, and integrate new modules within the ToolBoxV2 framework. It is intended for both human developers and AI agents.</p>"},{"location":"module_creation_guide/#1-core-concepts","title":"1. Core Concepts","text":"<p>A ToolBoxV2 module is a self-contained unit of functionality that plugs into the main application. It can expose functions to the system, provide API endpoints, and render user interfaces.</p>"},{"location":"module_creation_guide/#key-components-of-a-module","title":"Key Components of a Module:","text":"<ul> <li>Module File: A Python file (e.g., <code>MyModule.py</code>) located in the <code>toolboxv2/mods/</code> directory.</li> <li><code>get_app</code> and <code>export</code>: The entry point for connecting your module to the ToolBoxV2 application instance.</li> <li><code>@export</code> Decorator: The primary mechanism for registering functions and defining their behavior (e.g., as API endpoints, lifecycle hooks, etc.).</li> <li><code>Tools</code> Class (Optional but Recommended): A class that inherits from <code>MainTool</code> to organize your module's logic, state, and lifecycle methods (<code>on_start</code>, <code>on_exit</code>).</li> <li><code>RequestData</code> and <code>Result</code> Objects: Standardized Pydantic models for handling incoming requests and formatting outgoing responses, ensuring consistency across the framework.</li> </ul>"},{"location":"module_creation_guide/#2-module-structure-a-boilerplate","title":"2. Module Structure: A Boilerplate","text":"<p>Here is a standard boilerplate for a new module file (e.g., <code>toolboxv2/mods/MyNewModule.py</code>):</p> <pre><code># toolboxv2/mods/MyNewModule.py\n\nfrom toolboxv2 import App, Result, RequestData, get_app, MainTool\nfrom typing import Dict, Optional\n\n# -- Constants ---\nMOD_NAME = \"MyNewModule\"\nVERSION = \"1.0.0\"\n\n# -- Module Export ---\n# This makes the @export decorator available for this module.\nexport = get_app(f\"mods.{MOD_NAME}\").tb\n\n# -- Main Logic Class (Recommended) ---\nclass Tools(MainTool):\n    def __init__(self, app: App):\n        self.app = app\n        self.name = MOD_NAME\n        self.version = VERSION\n        # You can define CLI tools here if needed\n        self.tools = {\n            \"all\": [[\"show_version\", \"Displays the module version\"]],\n            \"name\": self.name,\n            \"show_version\": self.show_version,\n        }\n        super().__init__(\n            load=self.on_start, # Corresponds to @export(initial=True)\n            v=self.version,\n            tool=self.tools,\n            name=self.name,\n            on_exit=self.on_exit # Corresponds to @export(exit_f=True)\n        )\n\n    def on_start(self):\n        \"\"\"Called when the module is loaded.\"\"\"\n        self.app.logger.info(f\"{self.name} v{self.version} initialized.\")\n        # Example: Registering a UI component with another module\n        self.app.run_any((\"CloudM\", \"add_ui\"),\n                         name=self.name,\n                         title=self.name,\n                         path=f\"/api/{self.name}/ui\",\n                         description=\"A description of my module's UI.\",\n                         auth=True\n                         )\n\n    def on_exit(self):\n        \"\"\"Called when the application is shutting down.\"\"\"\n        self.app.logger.info(f\"Closing {self.name}. Goodbye!\")\n\n    def show_version(self):\n        return self.version\n\n# -- API Endpoints &amp; Functions ---\n\n@export(mod_name=MOD_NAME, name=\"ui\", api=True, api_methods=[\"GET\"])\nasync def get_main_ui(self) -&gt; Result:\n    \"\"\"Serves the main HTML UI for the module.\"\"\"\n    # The 'self' here will be the instance of the Tools class\n    html_content = \"&lt;h1&gt;Welcome to MyNewModule!&lt;/h1&gt;\"\n    return Result.html(data=html_content)\n\n@export(mod_name=MOD_NAME, name=\"get_data\", api=True, request_as_kwarg=True)\nasync def get_some_data(self, request: RequestData) -&gt; Result:\n    \"\"\"An example API endpoint to fetch data.\"\"\"\n    user = await self.app.run_any((\"WidgetsProvider\", \"get_user_from_request\"), request=request)\n    user_name = user.name if user else \"Guest\"\n    return Result.json(data={\"message\": f\"Hello, {user_name}!\", \"module\": self.name})\n</code></pre>"},{"location":"module_creation_guide/#3-the-export-decorator","title":"3. The <code>@export</code> Decorator","text":"<p>The <code>@export</code> decorator is the most critical part of creating a module. It tells ToolBoxV2 how to treat your function. Here are the key parameters:</p> <ul> <li><code>mod_name</code> (str): Required. The name of your module. Must match <code>MOD_NAME</code>.</li> <li><code>name</code> (str): The name to expose the function under. If omitted, the Python function name is used.</li> <li><code>api</code> (bool): If <code>True</code>, the function becomes an HTTP API endpoint accessible at <code>/api/MOD_NAME/function_name</code>.</li> <li><code>api_methods</code> (List[str]): A list of allowed HTTP methods (e.g., <code>[\"GET\", \"POST\"]</code>).</li> <li><code>request_as_kwarg</code> (bool): If <code>True</code>, the <code>RequestData</code> object will be passed as a keyword argument to your function.</li> <li><code>initial</code> (bool): If <code>True</code>, the function is an initialization hook and runs when the module is first loaded.</li> <li><code>exit_f</code> (bool): If <code>True</code>, the function is a cleanup hook and runs when the application exits.</li> <li><code>row</code> (bool): If <code>True</code>, the function's raw return value is sent as the response body, bypassing the <code>Result</code> object wrapper. Useful for serving files or raw text.</li> <li><code>level</code> (int): An integer indicating the privilege level required to execute the function.</li> </ul>"},{"location":"module_creation_guide/#4-handling-requests-and-responses","title":"4. Handling Requests and Responses","text":""},{"location":"module_creation_guide/#the-requestdata-object","title":"The <code>RequestData</code> Object","text":"<p>When a function is an API endpoint (<code>api=True</code>) and uses <code>request_as_kwarg=True</code>, it receives a <code>RequestData</code> object. This object contains all the information about the incoming HTTP request:</p> <ul> <li><code>request.method</code>: The HTTP method (e.g., 'GET', 'POST').</li> <li><code>request.path</code>: The request path.</li> <li><code>request.headers</code>: A Pydantic model of the request headers.</li> <li><code>request.query_params</code>: A dictionary of URL query parameters.</li> <li><code>request.form_data</code>: A dictionary of data from a submitted form.</li> <li><code>request.session</code>: A Pydantic model containing user session information, if the user is authenticated.</li> </ul>"},{"location":"module_creation_guide/#the-result-object","title":"The <code>Result</code> Object","text":"<p>API functions should almost always return a <code>Result</code> object. This standardizes responses and error handling.</p> <p>Success Responses:</p> <ul> <li><code>Result.ok(data, info)</code>: A generic success response.</li> <li><code>Result.json(data, info)</code>: For JSON API responses. Sets the <code>Content-Type</code> header to <code>application/json</code>.</li> <li><code>Result.html(data, info)</code>: For serving HTML content.</li> <li><code>Result.file(data, filename)</code>: For sending files to the user for download.</li> <li><code>Result.sse(stream_generator)</code>: For Server-Sent Events (event streaming).</li> </ul> <p>Error Responses:</p> <ul> <li><code>Result.default_user_error(info, exec_code)</code>: For client-side errors (e.g., bad input). Typically returns a 4xx status code.</li> <li><code>Result.default_internal_error(info, exec_code)</code>: For server-side errors. Typically returns a 5xx status code.</li> </ul>"},{"location":"module_creation_guide/#5-frontend-integration-with-tbjs","title":"5. Frontend Integration with <code>tbjs</code>","text":"<p>Modules often have a corresponding frontend component. The <code>tbjs</code> framework is designed to interact seamlessly with ToolBoxV2 modules.</p>"},{"location":"module_creation_guide/#making-api-calls-from-the-frontend","title":"Making API Calls from the Frontend","text":"<p>Use the <code>TB.api.request</code> function in your JavaScript to call your module's backend functions.</p> <pre><code>// In your frontend JavaScript file\n\nasync function fetchDataFromMyModule() {\n    try {\n        // Calls the 'get_data' function in the 'MyNewModule' module\n        const response = await TB.api.request('MyNewModule', 'get_data');\n\n        if (response.error === \"none\") {\n            const data = response.get(); // Helper to get response.result.data\n            console.log(\"Data from backend:\", data.message);\n            document.getElementById('my-element').innerText = data.message;\n        } else {\n            TB.ui.Toast.showError(`Error: ${response.info.help_text}`);\n        }\n    } catch (error) {\n        TB.logger.error(\"Network or API request failed\", error);\n        TB.ui.Toast.showError(\"Failed to connect to the server.\");\n    }\n}\n</code></pre>"},{"location":"module_creation_guide/#serving-a-ui","title":"Serving a UI","text":"<p>Your module can serve its entire UI as an HTML string from an API endpoint. This is a common pattern for \"widgets\" or self-contained applications.</p> <ol> <li> <p>Backend (<code>MyNewModule.py</code>): Create an endpoint that returns HTML.</p> <pre><code>@export(mod_name=MOD_NAME, name=\"ui\", api=True)\nasync def get_main_ui(self) -&gt; Result:\n    html_content = \"\"\"\n        &lt;div&gt;\n            &lt;h1&gt;My Module's UI&lt;/h1&gt;\n            &lt;button id=\"my-button\"&gt;Fetch Data&lt;/button&gt;\n            &lt;p id=\"my-element\"&gt;&lt;/p&gt;\n            &lt;script unSave=\"true\"&gt;\n                document.getElementById('my-button').addEventListener('click', async () =&gt; {\n                    const response = await TB.api.request('MyNewModule', 'get_data');\n                    if (response.get()) {\n                        document.getElementById('my-element').innerText = response.get().message;\n                    }\n                });\n            &lt;/script&gt;\n        &lt;/div&gt;\n    \"\"\"\n    return Result.html(data=html_content)\n</code></pre> </li> <li> <p>Integration (<code>on_start</code>): In your module's <code>on_start</code> method, register this UI with the <code>CloudM</code> module so it appears in the main application menu.</p> <pre><code>def on_start(self):\n    self.app.run_any((\"CloudM\", \"add_ui\"),\n                     name=self.name,\n                     title=\"My Awesome Module\",\n                     path=f\"/api/{self.name}/ui\",\n                     description=\"This is a module I built.\",\n                     auth=True # Requires user to be logged in\n                     )\n</code></pre> </li> </ol>"},{"location":"module_creation_guide/#6-inter-module-communication","title":"6. Inter-Module Communication","text":"<p>Modules can call functions in other modules using <code>app.run_any()</code>.</p> <pre><code># In MyNewModule.py\n\n@export(mod_name=MOD_NAME, name=\"process_and_store\", api=True, request_as_kwarg=True)\nasync def process_and_store(self, request: RequestData) -&gt; Result:\n    # 1. Get the user from the request using the WidgetsProvider module\n    user = await self.app.run_any((\"WidgetsProvider\", \"get_user_from_request\"), request=request)\n    if not user:\n        return Result.default_user_error(\"Authentication required.\")\n\n    # 2. Use the ISAA module to analyze some text\n    analysis = await self.app.run_any((\"isaa\", \"mini_task_completion\"),\n                                     mini_task=\"Summarize this text.\",\n                                     user_task=\"ToolBoxV2 is a modular framework...\")\n\n    # 3. Save the result to the user's file storage using FileWidget\n    storage = await self.app.run_any((\"FileWidget\", \"get_blob_storage\"), request=request)\n    # ... (code to save 'analysis' to blob storage) ...\n\n    return Result.ok(info=\"Data processed and saved.\")\n</code></pre> <p>This pattern allows for powerful, decoupled architectures where modules specialize in one area (AI, files, UI) and collaborate to build complex applications.</p>"},{"location":"p2p_rpc_protocol/","title":"P2P RPC Protocol Specification","text":"<p>This document defines the JSON-based Remote Procedure Call (RPC) protocol used for secure communication over the ToolBoxV2 P2P network.</p>"},{"location":"p2p_rpc_protocol/#overview","title":"Overview","text":"<p>The protocol is designed to be simple, extensible, and secure. All messages are JSON objects, which are then serialized into a string, encoded to UTF-8 bytes, and then E2E encrypted by the <code>tcm</code> peer before being sent over the P2P data channel.</p>"},{"location":"p2p_rpc_protocol/#message-structure","title":"Message Structure","text":"<p>There are two main types of messages: Request and Response.</p>"},{"location":"p2p_rpc_protocol/#request-message","title":"Request Message","text":"<p>A message sent from a Consumer to a Provider to execute a function.</p> <pre><code>{\n  \"type\": \"request\",\n  \"call_id\": \"unique-string-identifier-123\",\n  \"module\": \"MyModule\",\n  \"function\": \"my_function_name\",\n  \"args\": [\"positional_arg1\", 42],\n  \"kwargs\": {\n    \"keyword_arg1\": \"value1\",\n    \"optional_arg\": true\n  }\n}\n</code></pre> <p>Fields:</p> <ul> <li><code>type</code> (string, required): Must be <code>\"request\"</code>.</li> <li><code>call_id</code> (string, required): A unique identifier (e.g., a UUID) generated by the Consumer. This ID is used to correlate a Response with its original Request.</li> <li><code>module</code> (string, required): The name of the ToolBoxV2 module to be called (e.g., <code>\"CloudM\"</code>).</li> <li><code>function</code> (string, required): The name of the function to execute within the specified module.</li> <li><code>args</code> (array, optional): A list of positional arguments for the function. Defaults to <code>[]</code> if not provided.</li> <li><code>kwargs</code> (object, optional): A dictionary of keyword arguments for the function. Defaults to <code>{}</code> if not provided.</li> </ul>"},{"location":"p2p_rpc_protocol/#response-message","title":"Response Message","text":"<p>A message sent from a Provider back to the Consumer after a function has been executed.</p>"},{"location":"p2p_rpc_protocol/#success-response","title":"Success Response","text":"<pre><code>{\n  \"type\": \"response\",\n  \"call_id\": \"unique-string-identifier-123\",\n  \"result\": {\n    \"status\": \"ok\",\n    \"data\": { \"some_key\": \"some_value\" }\n  },\n  \"error\": null\n}\n</code></pre>"},{"location":"p2p_rpc_protocol/#error-response","title":"Error Response","text":"<pre><code>{\n  \"type\": \"response\",\n  \"call_id\": \"unique-string-identifier-123\",\n  \"result\": null,\n  \"error\": {\n    \"code\": 500,\n    \"message\": \"An internal error occurred.\",\n    \"details\": \"Traceback...\"\n  }\n}\n</code></pre> <p>Fields:</p> <ul> <li><code>type</code> (string, required): Must be <code>\"response\"</code>.</li> <li><code>call_id</code> (string, required): The <code>call_id</code> from the original Request message.</li> <li><code>result</code> (any, nullable): The data returned by the successful execution of the function. This should be JSON-serializable. It is <code>null</code> if an error occurred.</li> <li><code>error</code> (object, nullable): An object describing the error if the function execution failed. It is <code>null</code> on success.<ul> <li><code>code</code> (integer): An error code (e.g., 403 for Forbidden, 404 for Not Found, 500 for Internal Server Error).</li> <li><code>message</code> (string): A human-readable error message.</li> <li><code>details</code> (string, optional): Additional details, such as a traceback.</li> </ul> </li> </ul>"},{"location":"tbjs/","title":"TBjs","text":""},{"location":"tbjs/#tbjs-framework-comprehensive-guide-documentation","title":"tbjs Framework: Comprehensive Guide &amp; Documentation","text":"<p>Table of Contents</p> <ol> <li>Introduction<ul> <li>Key Design Principles &amp; Features</li> </ul> </li> <li>Getting Started<ul> <li>Prerequisites</li> <li>Installation</li> <li>HTML Setup</li> <li>Application Initialization (<code>TB.init</code>)</li> </ul> </li> <li>Core Modules (<code>TB.*</code>)<ul> <li><code>TB.config</code>: Configuration Management</li> <li><code>TB.logger</code>: Logging Utility</li> <li><code>TB.state</code>: Global State Management</li> <li><code>TB.events</code>: Event Bus / Pub/Sub</li> <li><code>TB.env</code>: Environment Detection</li> <li><code>TB.api</code>: Backend Communication</li> <li><code>TB.router</code>: SPA Routing</li> <li><code>TB.crypto</code>: Cryptographic Utilities</li> <li><code>TB.user</code>: User Session &amp; Authentication</li> <li><code>TB.sse</code>: Server-Sent Events</li> <li><code>TB.sw</code>: Service Worker Management</li> <li><code>TB.utils</code>: General Utilities</li> <li><code>TB.graphics</code>: 3D Graphics (THREE.js)</li> </ul> </li> <li>UI System (<code>TB.ui.*</code>)<ul> <li><code>TB.ui.theme</code>: Theming (Light/Dark Mode, Backgrounds)</li> <li><code>TB.ui.htmxIntegration</code>: HTMX Event Handling</li> <li><code>TB.ui.processDynamicContent</code>: Handling New DOM Content</li> <li>UI Components:<ul> <li><code>TB.ui.Modal</code></li> <li><code>TB.ui.Toast</code></li> <li><code>TB.ui.Loader</code></li> <li><code>TB.ui.Button</code></li> <li><code>TB.ui.DarkModeToggle</code></li> <li><code>TB.ui.CookieBanner</code></li> <li><code>TB.ui.MarkdownRenderer</code></li> <li><code>TB.ui.NavMenu</code></li> <li><code>TB.ui.AutocompleteWidget</code></li> </ul> </li> </ul> </li> <li>Styling with Tailwind CSS<ul> <li>Prefixing and CSS Variables</li> <li>Using <code>tbjs</code> Tailwind Config in Your Project</li> </ul> </li> <li>Advanced Topics<ul> <li>Tauri Integration</li> <li>Working with 3D Graphics</li> </ul> </li> <li>Example: Login Flow Walkthrough</li> <li>Building <code>tbjs</code> (For Developers)</li> </ol>"},{"location":"tbjs/#1-introduction","title":"1. Introduction","text":"<p><code>tbjs</code> is a modular frontend framework designed for building modern web applications, with special consideration for integration with Tauri for desktop applications and tools like HTMX and Three.js. It provides a comprehensive set of tools for managing configuration, state, API communication, routing, UI components, user authentication, and more.</p> <p>Key Design Principles &amp; Features:</p> <ul> <li>Modularity: Clear separation of concerns into <code>core</code> and <code>ui</code> modules. You can use only the parts you need.</li> <li>Event-Driven: Facilitates decoupled communication between modules via an event bus.</li> <li>Configuration-Centric: Application behavior is heavily influenced by a central configuration object.</li> <li>State Management: Centralized application state with optional persistence.</li> <li>SPA Router: Handles client-side navigation and view loading.</li> <li>API Abstraction: Simplifies backend communication, supporting both HTTP and Tauri <code>invoke</code> calls.</li> <li>UI System: Includes theme management (light/dark mode), dynamic backgrounds, and reusable UI components.</li> <li>3D Graphics Integration: Built-in support for THREE.js for dynamic backgrounds or scenes, managed by <code>TB.graphics</code>.</li> <li>User Authentication: Robust support for various authentication flows, including device key (asymmetric crypto) and WebAuthn (passkeys).</li> <li>HTMX Friendly: Designed to work seamlessly alongside HTMX for enhancing HTML with dynamic behaviors.</li> <li>Tauri-Aware: Core functionalities can adapt to run optimally in a Tauri environment.</li> <li>Modern Tooling: Built with Webpack, Babel, PostCSS, and Tailwind CSS.</li> </ul>"},{"location":"tbjs/#2-getting-started","title":"2. Getting Started","text":""},{"location":"tbjs/#prerequisites","title":"Prerequisites","text":"<p>Before using <code>tbjs</code>, ensure you have the following (or plan to include them if using related features):</p> <ol> <li>HTMX (Recommended): <code>tbjs</code> integrates well with HTMX for server-rendered partials and dynamic updates.     <pre><code>&lt;script defer src=\"https://unpkg.com/htmx.org@2.0.2/dist/htmx.min.js\"&gt;&lt;/script&gt;\n</code></pre></li> <li>Three.js (Optional, if using <code>TB.graphics</code>):     <pre><code>&lt;script defer src=\"https://cdnjs.cloudflare.com/ajax/libs/three.js/0.153.0/three.min.js\"&gt;&lt;/script&gt;\n</code></pre></li> <li> <p>Marked &amp; Highlight.js (Optional, if using <code>TB.ui.MarkdownRenderer</code>): For rendering Markdown to HTML with syntax highlighting.     <pre><code>&lt;script defer src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"&gt;&lt;/script&gt;\n&lt;script defer src=\"https://cdn.jsdelivr.net/npm/marked-highlight/lib/index.umd.min.js\"&gt;&lt;/script&gt;\n&lt;script defer src=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js\"&gt;&lt;/script&gt;\n&lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css\"&gt;\n</code></pre> Ensure <code>window.marked</code>, <code>window.markedHighlight</code>, and <code>window.hljs</code> are available before <code>TB.ui.MarkdownRenderer</code> is used.</p> </li> <li> <p>Material Symbols (Optional, used by some default UI components):     <pre><code>&lt;link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200\" /&gt;\n</code></pre></p> </li> </ol>"},{"location":"tbjs/#installation","title":"Installation","text":"<ol> <li> <p>Add <code>tbjs</code> to your project:     If <code>tbjs</code> were published to npm:     <pre><code>npm install tbjs\n# or\nyarn add tbjs\n</code></pre>     Since it's often used locally or as part of a larger monorepo, you'd typically:</p> <ul> <li>Build <code>tbjs</code> from source (see Building <code>tbjs</code>) to get <code>dist/tbjs.js</code> and <code>dist/tbjs.css</code>.</li> <li>Or, if integrating into a build system, import directly from its source path (e.g., <code>import TB from 'path/to/tbjs/src/index.js';</code>).</li> </ul> </li> <li> <p>Include files in your HTML (if using pre-built dist files): <pre><code>&lt;link rel=\"stylesheet\" href=\"path/to/your/tbjs/dist/tbjs.css\"&gt;\n&lt;!-- Load tbjs.js as a module or a global script depending on its build --&gt;\n&lt;script defer type=\"module\" src=\"path/to/your/tbjs/dist/tbjs.js\"&gt;&lt;/script&gt; &lt;!-- If ES Module build --&gt;\n&lt;!-- &lt;script defer src=\"path/to/your/tbjs/dist/tbjs.js\"&gt;&lt;/script&gt; --&gt; &lt;!-- If UMD build --&gt;\n</code></pre></p> </li> <li> <p>Peer Dependencies (Reminder):     Ensure you have <code>htmx.org</code> and <code>three</code> installed/included if you plan to use features that depend on them.     <pre><code>npm install htmx.org three # Or yarn add\n</code></pre></p> </li> </ol>"},{"location":"tbjs/#3-core-modules-tb","title":"3. Core Modules (<code>TB.*</code>)","text":""},{"location":"tbjs/#tbconfig-configuration-management","title":"<code>TB.config</code>: Configuration Management","text":"<p>Manages application-wide settings. It's initialized by <code>TB.init()</code> with default values merged with your provided configuration.</p> <ul> <li>Initialization: <code>TB.config.init(userAppConfig)</code> is called by <code>TB.init</code>.<ul> <li><code>userAppConfig</code> options:<ul> <li><code>appRootId</code> (string): ID of the main DOM element for router views. Default: <code>app-root</code>.</li> <li><code>baseApiUrl</code> (string): Base URL for API calls. Default: <code>/api</code>. Normalized to be absolute (e.g., <code>/api</code> becomes <code>window.location.origin/api</code>).</li> <li><code>baseFileUrl</code> (string): Base URL for fetching static HTML files for routing. Default: <code>window.location.origin</code>. Normalized to end with <code>/</code> if it has a path, and ensures it doesn't include file names.</li> <li><code>initialState</code> (object): Initial state for <code>TB.state</code>.</li> <li><code>themeSettings</code> (object): See <code>TB.ui.theme</code> section.</li> <li><code>routes</code> (array): Predefined routes for <code>TB.router</code> (currently for reference/future use).</li> <li><code>logLevel</code> (string): <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code>, <code>none</code>. Default: <code>info</code>.</li> <li><code>isProduction</code> (boolean): Inferred based on hostname (<code>localhost</code>, <code>127.0.0.1</code>) if not explicitly set.</li> <li><code>serviceWorker</code> (object): <code>{ enabled: boolean, url: string, scope: string }</code>.</li> </ul> </li> </ul> </li> <li>Getting Configuration: <pre><code>const apiUrl = TB.config.get('baseApiUrl');\nconst logLevel = TB.config.get('logLevel');\nconst themePref = TB.config.get('themeSettings.defaultPreference'); // Dot notation for nested\nconst allConfig = TB.config.getAll(); // Returns a copy of the entire config\n</code></pre></li> <li>Setting Configuration (dynamically, use with caution after init): <pre><code>TB.config.set('myCustomSetting', 'someValue');\nTB.config.set('featureFlags.newFeature', true);\n</code></pre></li> </ul>"},{"location":"tbjs/#tblogger-logging-utility","title":"<code>TB.logger</code>: Logging Utility","text":"<p>Provides leveled, prefixed, and timestamped logging to the console.</p> <ul> <li>Initialization: <code>TB.logger.init({ logLevel: '...' })</code> is called by <code>TB.init</code> based on <code>TB.config</code>.</li> <li>Setting Log Level: <pre><code>TB.logger.setLevel('debug'); // 'debug', 'info', 'warn', 'error', 'none'\n</code></pre></li> <li>Logging Messages: <pre><code>TB.logger.debug('Detailed debug message:', { data: 123 });\nTB.logger.info('Informational message.'); // Alias: TB.logger.log()\nTB.logger.warn('Potential issue warning.');\nTB.logger.error('An error occurred:', new Error('Something went wrong'));\n</code></pre>     Output includes a timestamp, <code>[tbjs]</code>, and the log level (e.g., <code>[DEBUG]</code>).</li> </ul>"},{"location":"tbjs/#tbstate-global-state-management","title":"<code>TB.state</code>: Global State Management","text":"<p>A simple key-value store for global application state with optional persistence to <code>localStorage</code>.</p> <ul> <li>Initialization: <code>TB.state.init(initialState)</code> is called by <code>TB.init</code> with <code>TB.config.get('initialState')</code>. Loads any persisted state.</li> <li>Getting State: <pre><code>const username = TB.state.get('user.username'); // Dot notation for nested\nconst allState = TB.state.get(); // Returns a copy of the entire state\n</code></pre></li> <li>Setting State: <pre><code>// Set a simple value\nTB.state.set('ui.darkMode', true);\n\n// Set a nested value, creating intermediate objects if they don't exist\nTB.state.set('user.profile.avatarUrl', '/path/to/avatar.png');\n\n// Persist the top-level key 'user' to localStorage\nTB.state.set('user.isLoggedIn', true, { persist: true });\n// Any change under 'user' (e.g., 'user.settings.notifications') will now persist 'user'.\n</code></pre><ul> <li>Emits <code>state:changed</code> event with <code>{ key, value, fullState }</code>.</li> <li>Emits specific event like <code>state:changed:user:profile:avatarUrl</code> with the <code>value</code>.</li> </ul> </li> <li>Deleting State: <pre><code>TB.state.delete('user.profile.temporaryToken');\nTB.state.delete('featureFlags.oldFlag', { persist: true }); // Will update persisted 'featureFlags'\n</code></pre></li> <li>Legacy \"Var\" Methods (for simple key-value persistence, prefer structured state with <code>persist</code> option):<ul> <li><code>TB.state.initVar('myVar', 'defaultValue')</code>: Sets if not already defined, persists.</li> <li><code>TB.state.delVar('myVar')</code>: Deletes and updates persisted state.</li> <li><code>TB.state.getVar('myVar')</code>: Gets value.</li> <li><code>TB.state.setVar('myVar', 'newValue')</code>: Sets value, persists.</li> </ul> </li> </ul>"},{"location":"tbjs/#tbevents-event-bus-pubsub","title":"<code>TB.events</code>: Event Bus / Pub/Sub","text":"<p>Allows modules to communicate without direct dependencies.</p> <ul> <li>Subscribing to Events: <pre><code>function handleThemeChange(eventData) {\n    console.log('Theme changed to:', eventData.mode);\n}\nTB.events.on('theme:changed', handleThemeChange);\n\n// Subscribe only once\nTB.events.once('app:firstLogin', (userData) =&gt; { /* ... */ });\n</code></pre></li> <li>Unsubscribing from Events: <pre><code>TB.events.off('theme:changed', handleThemeChange);\n</code></pre></li> <li>Emitting Events: <pre><code>TB.events.emit('user:loggedIn', { userId: 123, username: 'testuser' });\n</code></pre>     If a listener throws an error, <code>TB.logger.error</code> is called, and other listeners still execute.</li> <li>Common Framework Events: <code>tbjs:initialized</code>, <code>state:changed</code>, <code>router:navigationSuccess</code>, <code>theme:changed</code>, <code>api:networkError</code>, <code>graphics:initialized</code>, <code>user:loggedOut</code>, etc.</li> </ul>"},{"location":"tbjs/#tbenv-environment-detection","title":"<code>TB.env</code>: Environment Detection","text":"<p>Provides information about the runtime environment.</p> <ul> <li>Initialization: <code>TB.env.detect()</code> is called by <code>TB.init</code>.</li> <li>Checking Environment: <pre><code>if (TB.env.isTauri()) {\n    console.log('Running in Tauri environment.');\n} else if (TB.env.isWeb()) {\n    console.log('Running in a web browser.');\n}\nif (TB.env.isMobile()) { // Currently implies Tauri mobile if detected\n    console.log('Running on a mobile platform (Tauri).');\n}\n</code></pre></li> </ul>"},{"location":"tbjs/#tbapi-backend-communication","title":"<code>TB.api</code>: Backend Communication","text":"<p>Handles all HTTP and Tauri <code>invoke</code> calls, standardizing responses.</p> <ul> <li>Core <code>Result</code> Object: All <code>TB.api</code> methods (and Tauri invokes) aim to return or be wrapped into a <code>Result</code> object:     <pre><code>// Structure of a Result object (simplified)\n// const result = {\n//   origin: Array&lt;string&gt;,    // e.g., ['http'], ['tauri']\n//   error: string,           // From TB.ToolBoxError (e.g., 'none', 'InternalError')\n//   result: {                // Instance of ToolBoxResult\n//     data_to: string,       // From TB.ToolBoxInterfaces (e.g., 'API', 'NATIVE')\n//     data_info: string|null,// Additional info\n//     data: any              // The actual payload\n//   },\n//   info: {                  // Instance of ToolBoxInfo\n//     exec_code: number,     // HTTP status or custom code (0 for success)\n//     help_text: string      // Descriptive message\n//   },\n//   get: function() { return this.result.data; }, // Helper to get payload\n//   log: function() { /* console logs details */ },\n//   html: function() { /* returns an HTML representation for debugging */ }\n// };\n</code></pre></li> <li> <p><code>TB.api.request(moduleName, functionName, payload, method, useTauri, isSpecialAuthRoute)</code>:     The primary method for making backend requests.</p> <ul> <li><code>moduleName</code> (string): Backend module/class OR full path (e.g., <code>/validateSession</code>).</li> <li><code>functionName</code> (string|object): Backend function/method OR query params object if <code>moduleName</code> is a full path (for GET/DELETE).</li> <li><code>payload</code> (object|string|null): Data to send. Object for JSON POST/PUT; string can be form-urlencoded or query params.</li> <li><code>method</code> (string): HTTP method (<code>GET</code>, <code>POST</code>, etc.). Default: <code>POST</code>.</li> <li><code>useTauri</code> (string): <code>auto</code> (default), <code>force</code> (Tauri only), <code>never</code> (HTTP only).</li> <li><code>isSpecialAuthRoute</code> (boolean): If <code>true</code>, influences token handling (rarely needed).</li> </ul> <p>URL Construction (HTTP): *   Standard: <code>baseApiUrl/moduleName/functionName</code> *   Full path: <code>baseApiUrl</code> + <code>moduleName</code> (where <code>moduleName</code> starts with <code>/</code>, e.g., <code>/custom/endpoint</code>)</p> <p><pre><code>// POST request (HTTP or Tauri if available and 'auto')\nconst userData = { name: 'John Doe', email: 'john@example.com' };\nlet response = await TB.api.request('UserModule', 'createUser', userData); // Defaults to POST\n\nif (response.error === TB.ToolBoxError.none) {\n    console.log('User created:', response.get());\n} else {\n    TB.logger.error('Failed to create user:', response.info.help_text);\n}\n\n// GET request with query parameters from an object\nresponse = await TB.api.request('ProductModule', 'getProduct', { id: 123 }, 'GET');\n// URL: /api/ProductModule/getProduct?id=123\n\n// Full path GET (functionName is query params object)\nresponse = await TB.api.request('/custom/data', { type: 'summary' }, null, 'GET');\n// URL: /api/custom/data?type=summary (if baseApiUrl is /api)\n</code></pre> *   <code>TB.api.fetchHtml(path)</code>: Fetches HTML content, typically for router views. Path is relative to <code>baseFileUrl</code>. <pre><code>const htmlResult = await TB.api.fetchHtml('/about.html'); // Fetches /web/pages/about.html\nif (!htmlResult.startsWith('HTTP error!')) { /* ... */ }\n</code></pre> *   <code>TB.api.AuthHttpPostData(username)</code>: Specific method for validating a session. Calls <code>/validateSession</code>. *   <code>TB.api.logoutServer()</code>: Notifies the backend to invalidate the current user's session token (calls <code>/web/logoutS</code>). *   Events: *   <code>api:networkError</code>: Emitted on fetch network failures. Payload: <code>{ url, error }</code>.</p> </li> </ul>"},{"location":"tbjs/#tbrouter-spa-routing","title":"<code>TB.router</code>: SPA Routing","text":"<p>Manages client-side navigation and view rendering.</p> <ul> <li>Initialization: <code>TB.router.init(rootElement, predefinedRoutes)</code> called by <code>TB.init</code>.<ul> <li><code>rootElement</code>: The DOM element where views will be rendered (from <code>TB.config.appRootId</code>).</li> <li>Automatically navigates to the initial URL (or <code>/index.html</code>).</li> </ul> </li> <li>Navigating: <pre><code>// Navigate to a new path, updating browser history\nTB.router.navigateTo('/products/123'); // Fetches baseFileUrl + /products/123.html\n\n// Navigate and replace current history entry\nTB.router.navigateTo('/profile/settings', true);\n</code></pre><ul> <li>Fetches HTML from <code>TB.config.get('baseFileUrl') + path + '.html'</code> (by default, unless path includes an extension).</li> <li>Handles script loading within new views (external once, inline executed, <code>unsave</code> attribute for fresh execution, <code>global=\"true\"</code> for potential preservation).</li> <li>Updates <code>appRootElement.innerHTML</code> with fetched content.</li> <li>Calls <code>TB.ui.processDynamicContent()</code> on the new content.</li> <li>Handles 404 errors by trying to navigate to <code>/web/assets/404.html</code>.</li> <li>Handles 401 errors by trying to navigate to <code>/web/assets/401.html</code>.</li> </ul> </li> <li>Getting Current Path: <code>TB.router.getCurrentPath()</code></li> <li>Cache Management:<ul> <li><code>TB.router.clearCache(path)</code>: Clears HTML cache for a specific path or all if <code>path</code> is omitted (uses <code>sessionStorage</code> if <code>USE_SESSION_CACHE</code> is true in router.js).</li> <li><code>scriptCache</code> (Set of script <code>src</code> URLs) prevents re-fetching external scripts.</li> </ul> </li> <li>Events:<ul> <li><code>router:beforeNavigation</code>: <code>{ from, to }</code></li> <li><code>router:navigationSuccess</code>: <code>{ path, contentSource }</code> ('cache' or 'fetched')</li> <li><code>router:navigationError</code>: <code>{ path, error }</code></li> <li><code>router:contentProcessed</code>: <code>{ path, element }</code></li> </ul> </li> </ul>"},{"location":"tbjs/#tbcrypto-cryptographic-utilities","title":"<code>TB.crypto</code>: Cryptographic Utilities","text":"<p>Provides functions for various cryptographic operations, including WebAuthn. Relies on browser's Web Crypto API.</p> <ul> <li>Key Management &amp; Signing:<ul> <li><code>TB.crypto.generateAsymmetricKeys()</code>: Generates RSA-OAEP key pair (PEM &amp; Base64).</li> <li><code>TB.crypto.decryptAsymmetric(encryptedBase64Data, privateKeyBase64, convertHex = false)</code>: Decrypts RSA-OAEP encrypted data.</li> <li><code>TB.crypto.signMessage(privateKeyBase64, message)</code>: Signs a message using RSA-PSS.</li> <li><code>TB.crypto.storePrivateKey(privateKeyBase64, username)</code>: Stores private key in <code>localStorage</code>.</li> <li><code>TB.crypto.retrievePrivateKey(username)</code>: Retrieves private key.</li> </ul> </li> <li>Symmetric Encryption/Decryption:<ul> <li><code>TB.crypto.generateSymmetricKey()</code>: Generates an AES-GCM key (Base64 of raw key).</li> <li><code>TB.crypto.decryptSymmetric(encryptedDataB64, password)</code>: Decrypts AES-GCM data (assumes IV is prepended to ciphertext, password used for key derivation via PBKDF2).</li> </ul> </li> <li>WebAuthn (Passkeys):<ul> <li>The Relying Party ID (<code>rpId</code>) is determined from <code>window.location.hostname</code> (or \"localhost\").</li> <li><code>TB.crypto.registerWebAuthnCredential(registrationData, singData)</code>:<ul> <li><code>registrationData</code>: <code>{ challenge, userId, username }</code> from server.</li> <li><code>singData</code>: Additional data (e.g., session token) to associate.</li> <li>Calls <code>navigator.credentials.create()</code>. Returns payload for server verification.</li> </ul> </li> <li><code>TB.crypto.authorizeWebAuthnCredential(rawIdAsBase64, challenge, username)</code>:<ul> <li><code>rawIdAsBase64</code>, <code>challenge</code>, <code>username</code> from server.</li> <li>Calls <code>navigator.credentials.get()</code>. Returns assertion payload for server verification.</li> </ul> </li> </ul> </li> <li>Data Conversions: <code>arrayBufferToBase64</code>, <code>base64ToArrayBuffer</code>, <code>strToBase64</code>, etc.</li> </ul>"},{"location":"tbjs/#tbuser-user-session-authentication","title":"<code>TB.user</code>: User Session &amp; Authentication","text":"<p>Manages user state, authentication flows, and user-specific data. User state is stored under <code>TB.state.get('user')</code>.</p> <ul> <li>Initialization: <code>TB.user.init(forceServerFetch = false)</code>:<ul> <li>Called by <code>TB.init</code>. Loads session, validates with backend, synchronizes user data.</li> </ul> </li> <li>Authentication State: <code>TB.user.isAuthenticated()</code>, <code>TB.user.getUsername()</code>, <code>TB.user.getToken()</code>, etc.</li> <li>Login Methods:<ul> <li><code>async TB.user.signup(username, email, initiationKey, registerAsPersona = false)</code>: Initiates user creation.</li> <li><code>async TB.user.loginWithDeviceKey(username)</code>: Login using locally stored asymmetric key.</li> <li><code>async TB.user.loginWithWebAuthn(username)</code>: Login using WebAuthn (passkey).</li> <li><code>async TB.user.requestMagicLink(username)</code>: Requests a magic link email.</li> <li><code>async TB.user.registerDeviceWithInvitation(username, invitationKey)</code>: Registers a new device.</li> <li><code>async TB.user.registerWebAuthnForCurrentUser(username)</code>: Adds a WebAuthn credential for an authenticated user.</li> </ul> </li> <li>Session Management:<ul> <li><code>async TB.user.checkSessionValidity()</code>: Validates current token with server.</li> <li><code>async TB.user.logout(notifyServer = true)</code>: Clears local session, notifies server.</li> </ul> </li> <li>User-Specific Data: <code>TB.user.getUserData(key)</code>, <code>TB.user.setUserData(keyOrObject, value, syncToServer = false)</code>, <code>async TB.user.syncUserData()</code>, <code>async TB.user.fetchUserData()</code>.</li> <li>Events: <code>user:stateChanged</code>, <code>user:loggedOut</code>.</li> </ul>"},{"location":"tbjs/#tbsse-server-sent-events","title":"<code>TB.sse</code>: Server-Sent Events","text":"<p>Manages connections to Server-Sent Event streams.</p> <ul> <li>Connecting: <code>TB.sse.connect(url, options = {})</code><ul> <li><code>options</code>: <code>{ onOpen, onError, onMessage, listeners: { eventName: handler }, eventSourceOptions }</code>. <pre><code>TB.sse.connect('/api/sse/updates', {\n    listeners: {\n        'user-update': (data) =&gt; TB.state.set('user.profile', data),\n        'new-notification': (data) =&gt; TB.ui.Toast.showInfo(data.message)\n    }\n});\n</code></pre></li> </ul> </li> <li>Disconnecting: <code>TB.sse.disconnect(url)</code>, <code>TB.sse.disconnectAll()</code></li> <li>Getting Connection: <code>TB.sse.getConnection(url)</code></li> <li>Events Emitted: <code>sse:open:&lt;url&gt;</code>, <code>sse:error:&lt;url&gt;</code>, <code>sse:event:&lt;url&gt;:&lt;eventName&gt;</code>, etc.</li> </ul>"},{"location":"tbjs/#tbsw-service-worker-management","title":"<code>TB.sw</code>: Service Worker Management","text":"<p>Handles registration and communication with your application's service worker.</p> <ul> <li>Configuration (<code>TB.config.get('serviceWorker')</code>): <code>enabled</code>, <code>url</code>, <code>scope</code>.</li> <li>Registration: Called automatically by <code>TB.init</code> if enabled. Manual: <code>await TB.sw.register()</code>.</li> <li>Unregistration: <code>await TB.sw.unregister()</code></li> <li>Sending Messages: <code>await TB.sw.sendMessage({ type: 'GET_VERSION' })</code></li> <li>Events Emitted: <code>sw:updateAvailable</code>, <code>sw:contentCached</code>.     <pre><code>TB.events.on('sw:updateAvailable', ({ registration }) =&gt; {\n    if (confirm('New version available. Reload?')) {\n        registration.waiting.postMessage({ type: 'SKIP_WAITING' });\n        // Listen for controllerchange to reload\n        navigator.serviceWorker.addEventListener('controllerchange', () =&gt; window.location.reload());\n    }\n});\n</code></pre></li> </ul>"},{"location":"tbjs/#tbutils-general-utilities","title":"<code>TB.utils</code>: General Utilities","text":"<p>A collection of helper functions.</p> <ul> <li><code>TB.utils.autocomplete(inputElement, arrayOrFunctionSource)</code>: Basic autocomplete (prefer <code>TB.ui.AutocompleteWidget</code>).</li> <li><code>TB.utils.debounce(func, delay)</code></li> <li><code>TB.utils.throttle(func, limit)</code></li> <li><code>TB.utils.uniqueId(prefix = 'id-')</code></li> <li><code>TB.utils.deepClone(obj)</code></li> <li><code>TB.utils.cleanUrl(url)</code>: Basic URL cleaning.</li> </ul>"},{"location":"tbjs/#tbgraphics-3d-graphics-threejs","title":"<code>TB.graphics</code>: 3D Graphics (THREE.js)","text":"<p>Manages a THREE.js scene, typically for background effects.</p> <ul> <li>Initialization: <code>TB.graphics.init(canvasContainerSelector, options = {})</code><ul> <li><code>canvasContainerSelector</code>: CSS selector for the DOM element (e.g., <code>'#threeDScene'</code>).</li> <li><code>options</code>: <code>{ cameraY, cameraZ, sierpinskiDepth, loaderHideDelay }</code>.</li> <li>Typically called if <code>themeSettings.background.type</code> is <code>'3d'</code>.</li> </ul> </li> <li>Control Methods:<ul> <li><code>TB.graphics.dispose()</code>, <code>TB.graphics.pause()</code>, <code>TB.graphics.resume()</code>.</li> <li><code>TB.graphics.updateTheme(themeMode)</code>: Called by <code>TB.ui.theme</code>.</li> <li><code>TB.graphics.setSierpinskiDepth(newDepth)</code>.</li> <li><code>TB.graphics.setAnimationSpeed(x, y, z, factor)</code>.</li> <li><code>TB.graphics.adjustCameraZoom(delta)</code>, <code>TB.graphics.setCameraZoom(absoluteZoomValue)</code>.</li> </ul> </li> <li>Programmed Animation Sequences:<ul> <li><code>TB.graphics.playAnimationSequence(sequenceString, onCompleteCallback, baseSpeedOverride, speedFactorOverride)</code><ul> <li><code>sequenceString</code>: e.g., <code>\"R1+32:P2-14\"</code> (Type, Repeat, Direction, Speed, Complexity).</li> </ul> </li> <li><code>TB.graphics.stopAnimationSequence()</code>.</li> </ul> </li> <li>Events: <code>graphics:initialized</code>, <code>graphics:disposed</code>.</li> </ul>"},{"location":"tbjs/#4-ui-system-tbui","title":"4. UI System (<code>TB.ui.*</code>)","text":""},{"location":"tbjs/#tbuitheme-theming","title":"<code>TB.ui.theme</code>: Theming","text":"<p>Manages light/dark mode and application background.</p> <ul> <li>Initialization: <code>TB.ui.theme.init(themeSettings)</code> called by <code>TB.init</code>.<ul> <li><code>themeSettings</code>: <code>{ defaultPreference ('light'|'dark'|'system'), background: { type, light, dark, placeholder } }</code>.</li> <li><code>background.type</code>: <code>'3d'</code>, <code>'image'</code>, <code>'color'</code>, <code>'none'</code>.</li> <li><code>background.light/dark</code>: <code>{ color: string, image: string|null }</code>.</li> <li><code>background.placeholder</code>: <code>{ image_light, image_dark, displayUntil3DReady }</code>.</li> </ul> </li> <li>Interacting with Theme:<ul> <li><code>TB.ui.theme.setPreference('dark')</code>, <code>TB.ui.theme.togglePreference()</code>.</li> <li><code>TB.ui.theme.getCurrentMode()</code> ('light' or 'dark').</li> <li><code>TB.ui.theme.getPreference()</code> ('light', 'dark', or 'system').</li> </ul> </li> <li>Background Management:<ul> <li>Uses <code>#appBackgroundContainer</code> for image/color and <code>#threeDScene</code> for 3D.</li> </ul> </li> <li>Events: <code>theme:changed</code> (payload: <code>{ mode: 'light' | 'dark' }</code>).</li> </ul>"},{"location":"tbjs/#tbuihtmxintegration-htmx-event-handling","title":"<code>TB.ui.htmxIntegration</code>: HTMX Event Handling","text":"<p>Listens to HTMX events to integrate <code>tbjs</code> functionalities.</p> <ul> <li>Initialization: <code>TB.ui.htmxIntegration.init()</code> is called by <code>TB.init</code>.</li> <li><code>htmx:afterSwap</code>: Calls <code>TB.ui.processDynamicContent</code> on the new HTMX target element.</li> <li><code>htmx:afterRequest</code>:<ul> <li>Inspects XHR response. If JSON, wraps in <code>TB.api.Result</code>, shows toasts for errors.</li> <li>Handles <code>REMOTE</code> render commands.</li> <li>Emits <code>htmx:jsonResponse</code>.</li> </ul> </li> </ul>"},{"location":"tbjs/#tbuiprocessdynamiccontentparentelement-options","title":"<code>TB.ui.processDynamicContent(parentElement, options = {})</code>","text":"<p>Initializes <code>tbjs</code> features/components within newly added DOM content.</p> <ul> <li><code>parentElement</code>: The container of the new content.</li> <li><code>options</code>: <code>{ addScripts (default true), scriptCache }</code>.</li> <li>Actions: Calls <code>window.htmx.process()</code>, handles scripts, calls <code>TB.ui.MarkdownRenderer.renderAllIn()</code>, initializes data-attribute driven components like <code>AutocompleteWidget</code>.</li> </ul>"},{"location":"tbjs/#ui-components","title":"UI Components","text":""},{"location":"tbjs/#tbuimodal","title":"<code>TB.ui.Modal</code>","text":"<p>Displays modal dialogs.</p> <ul> <li>Static Usage: <code>TB.ui.Modal.show({ title, content, maxWidth, buttons, onOpen, onClose, ... })</code><ul> <li><code>buttons</code>: <code>[{ text, action: (modalInstance) =&gt; {}, variant, className }]</code></li> </ul> </li> <li>Styling: Uses Tailwind CSS, \"milk glass\" effect.</li> <li> <p>Events: <code>modal:shown</code>, <code>modal:closed</code>.</p> </li> <li> <p>Static Usage: `TB.ui.Modal.confirm({         title,         content,         confirmButtonText = 'OK',         cancelButtonText = 'Cancel',         confirmButtonVariant = 'primary',         cancelButtonVariant = 'secondary',         confirmButtonClass = '',         cancelButtonClass = '',         hideCancelButton = false,         resolveOnClose = false,         ...extraModalOptions // Collects any other options passed to confirm</p> </li> </ul>"},{"location":"tbjs/#tbuitoast","title":"<code>TB.ui.Toast</code>","text":"<p>Displays \"speech balloon\" style toast notifications.</p> <ul> <li>Static Usage:<ul> <li><code>TB.ui.Toast.showInfo(message, options)</code></li> <li><code>TB.ui.Toast.showSuccess(message, options)</code></li> <li><code>TB.ui.Toast.showWarning(message, options)</code></li> <li><code>TB.ui.Toast.showError(message, options)</code></li> </ul> </li> <li>Options: <code>{ title, duration, position, actions, icon, closable, showDotOnHide, dotDuration }</code>.</li> <li>Events: <code>toast:shown</code>, <code>toast:hidden</code>.</li> </ul>"},{"location":"tbjs/#tbuiloader","title":"<code>TB.ui.Loader</code>","text":"<p>Displays a loading indicator.</p> <ul> <li>Static Usage (Global Page Loader):<ul> <li><code>const loaderElement = TB.ui.Loader.show('Processing...');</code></li> <li><code>TB.ui.Loader.hide(loaderElement);</code> (or <code>TB.ui.Loader.hide()</code> for default).</li> </ul> </li> <li>Options: <code>{ text, fullscreen, customSpinnerHtml }</code>.</li> </ul>"},{"location":"tbjs/#tbuibutton","title":"<code>TB.ui.Button</code>","text":"<p>Creates styled button elements programmatically.</p> <ul> <li>Static Usage: <code>const myButtonElement = TB.ui.Button.create(text, onClickCallback, options)</code></li> <li>Options: <code>{ variant, size, iconLeft, iconRight, type, disabled, isLoading, ... }</code>.</li> <li>Instance Methods: <code>setLoading(true)</code>, <code>setDisabled(true)</code>.</li> </ul>"},{"location":"tbjs/#tbuidarkmodetoggle","title":"<code>TB.ui.DarkModeToggle</code>","text":"<p>UI component for switching themes, syncing with <code>TB.ui.theme</code>.</p> <ul> <li>HTML (Example): <pre><code>&lt;div id=\"darkModeToggleContainer\"&gt;\n    &lt;label for=\"darkModeSwitch\"&gt;&lt;span class=\"tb-toggle-icon material-symbols-outlined\"&gt;light_mode&lt;/span&gt;&lt;/label&gt;\n    &lt;input type=\"checkbox\" id=\"darkModeSwitch\" class=\"tb-sr-only\"&gt;\n&lt;/div&gt;\n</code></pre></li> <li>Initialization: <code>TB.ui.DarkModeToggle.init({ containerSelector, iconSelector, checkboxSelector, ... })</code>. Default init uses common selectors.</li> <li>Updates icon and checkbox based on <code>theme:changed</code> event.</li> </ul>"},{"location":"tbjs/#tbuicookiebanner","title":"<code>TB.ui.CookieBanner</code>","text":"<p>Displays a cookie consent banner and settings modal.</p> <ul> <li>Static Usage: <code>TB.ui.CookieBanner.show({ title, message, termsLink, onConsent, ... })</code></li> <li><code>onConsent</code> callback receives <code>{ essential, preferences, analytics, source }</code>.</li> <li>Methods: <code>CookieBanner.getConsent()</code>, <code>CookieBanner.clearConsent()</code>.</li> <li>Events: <code>cookieConsent:updated</code>, <code>cookieBanner:shown</code>/<code>hidden</code>.</li> </ul>"},{"location":"tbjs/#tbuimarkdownrenderer","title":"<code>TB.ui.MarkdownRenderer</code>","text":"<p>Renders Markdown to HTML, with optional <code>highlight.js</code> syntax highlighting.</p> <ul> <li>Dependencies: <code>marked</code>, <code>highlight.js</code>, <code>marked-highlight</code> (global or loaded).</li> <li>Methods:<ul> <li><code>TB.ui.MarkdownRenderer.render(markdownString)</code></li> <li><code>TB.ui.MarkdownRenderer.renderAllIn(parentElement)</code> (for elements with <code>.markdown</code> class)</li> <li><code>TB.ui.MarkdownRenderer.renderElement(element)</code></li> </ul> </li> <li>Adds Tailwind Prose classes (<code>prose dark:prose-invert</code>) for styling.</li> </ul>"},{"location":"tbjs/#tbuinavmenu","title":"<code>TB.ui.NavMenu</code>","text":"<p>A slide-in (or modal-style) navigation menu.</p> <ul> <li>HTML Trigger (Example): <pre><code>&lt;div id=\"Nav-Main\"&gt; &lt;!-- Menu is appended here --&gt;\n    &lt;div id=\"links\"&gt;&lt;span class=\"material-symbols-outlined\"&gt;menu&lt;/span&gt;&lt;/div&gt;\n&lt;/div&gt;\n</code></pre></li> <li>Initialization: <code>TB.ui.NavMenu.init({ triggerSelector, menuContentHtml, ... })</code>.</li> <li>Events: <code>navMenu:opened</code>, <code>navMenu:closed</code>.</li> </ul>"},{"location":"tbjs/#tbuiautocompletewidget","title":"<code>TB.ui.AutocompleteWidget</code>","text":"<p>Provides autocomplete suggestions for input fields.</p> <ul> <li>HTML (Declarative): <pre><code>&lt;input type=\"text\" data-tb-autocomplete data-tb-autocomplete-source='[\"Apple\", \"Banana\"]'&gt;\n&lt;!-- Or data-tb-autocomplete-source=\"myGlobalFunctionName\" --&gt;\n</code></pre></li> <li>Initialization:<ul> <li>Automatic: <code>TB.ui.AutocompleteWidget.initAll()</code> (called by <code>processDynamicContent</code>).</li> <li>Manual: <code>new TB.ui.AutocompleteWidget(inputEl, { source, minLength, onSelect, ... })</code>.</li> </ul> </li> <li>Features: Keyboard navigation, ARIA attributes.</li> </ul>"},{"location":"tbjs/#5-styling-with-tailwind-css","title":"5. Styling with Tailwind CSS","text":"<p><code>tbjs</code> components are primarily styled using Tailwind CSS utility classes.</p>"},{"location":"tbjs/#prefixing-and-css-variables","title":"Prefixing and CSS Variables","text":"<ul> <li>Prefix: <code>tbjs</code>'s internal Tailwind configuration uses a <code>tb-</code> prefix (e.g., <code>tb-bg-primary-500</code>, <code>tb-text-lg</code>). This is crucial to avoid conflicts if your main application also uses Tailwind without a prefix or with a different one.</li> <li>Main CSS (<code>tbjs.css</code> or <code>tbjs-main.css</code>):<ul> <li>Imports Tailwind utilities generated with the <code>tb-</code> prefix.</li> <li>Defines CSS custom properties (variables) for theming (e.g., <code>--tb-color-primary-500</code>, <code>--theme-bg</code>, <code>--glass-bg</code>). These are used by the prefixed Tailwind classes.</li> <li>Includes light and dark theme definitions typically applied to <code>body[data-theme=\"dark\"]</code> or <code>body.dark-mode</code>.</li> <li>Provides base styles and some component-specific styles hard to achieve with utilities alone (e.g., toast speech balloon tail).</li> </ul> </li> <li>Customization:<ul> <li>Applications can override the CSS variables defined in <code>tbjs.css</code> in their own stylesheets to customize the look and feel.</li> <li>For deeper Tailwind customization (new colors, variants specific to <code>tbjs</code>), you would edit <code>tbjs/tailwind.config.js</code> and rebuild <code>tbjs</code>.</li> </ul> </li> </ul>"},{"location":"tbjs/#using-tbjs-tailwind-config-in-your-project","title":"Using <code>tbjs</code> Tailwind Config in Your Project","text":"<p>If your project also uses Tailwind CSS, you have a few options:</p> <ol> <li> <p>Separate Builds (Recommended for Isolation):</p> <ul> <li>Build <code>tbjs.css</code> using its own Tailwind configuration (with the <code>tb-</code> prefix).</li> <li>Build your application's CSS using its Tailwind configuration.</li> <li>Include both CSS files in your HTML. The <code>tb-</code> prefix prevents most conflicts.</li> </ul> </li> <li> <p>Merging Configurations (Advanced):     If you want a single Tailwind build process, you might try to merge configurations. This can be complex due to prefixing and potential conflicts.</p> <ul> <li>You would need to ensure your main <code>tailwind.config.js</code> includes the <code>content</code> paths for <code>tbjs</code> source files.</li> <li>You'd also need to decide how to handle the <code>tb-</code> prefix. If your app doesn't use a prefix, <code>tbjs</code> components might not be styled correctly unless you manually adapt their classes or adjust the <code>tbjs</code> source.</li> <li>A simpler merge might involve including <code>tbjs</code>'s Tailwind plugin or preset if it were structured that way, but this is not the default.</li> </ul> <p>Example (Conceptual - requires careful setup): <pre><code>// your-app/tailwind.config.js\n// const tbjsTailwindConfig = require('path/to/tbjs/tailwind.config.js'); // If CJS\n\nexport default {\n  content: [\n    './src/**/*.{html,js,svelte,vue,jsx,tsx}', // Your app's content\n    './node_modules/tbjs/src/**/*.{html,js}', // Or path to tbjs source\n  ],\n  // If your app uses a prefix, it might conflict or work alongside tb-\n  // prefix: 'app-',\n  theme: {\n    extend: {\n      // You might try to extend with tbjs colors if they are defined without prefix in its config\n      // This part is tricky due to the 'tb-' prefix baked into tbjs's own build\n    },\n  },\n  plugins: [],\n};\n</code></pre> Generally, keeping <code>tbjs.css</code> separate with its <code>tb-</code> prefix is the most straightforward way to avoid styling conflicts.</p> </li> </ol>"},{"location":"tbjs/#6-advanced-topics","title":"6. Advanced Topics","text":""},{"location":"tbjs/#tauri-integration","title":"Tauri Integration","text":"<ul> <li>Environment Check: Use <code>TB.env.isTauri()</code> to execute Tauri-specific code.</li> <li>API Calls: <code>TB.api.request()</code> automatically uses <code>window.__TAURI__.invoke</code> if <code>useTauri</code> is <code>'auto'</code> (default) or <code>'force'</code> and the environment is Tauri.<ul> <li>The Tauri command invoked is typically <code>moduleName.functionName</code> (e.g., <code>MyRustModule.my_function</code>). <pre><code>if (TB.env.isTauri()) {\n    const result = await TB.api.request('my_rust_command', 'sub_command_or_payload_key', { data: 'payload' });\n    // Effective Tauri invoke: window.__TAURI__.invoke('my_rust_command.sub_command_or_payload_key', { data: 'payload' });\n}\n</code></pre></li> </ul> </li> <li>Platform-Specific Features: The <code>initializeApp</code> function shows a pattern for loading Tauri-specific listeners or UI adjustments.</li> </ul>"},{"location":"tbjs/#working-with-3d-graphics","title":"Working with 3D Graphics","text":"<ul> <li>The <code>TB.graphics</code> module manages a THREE.js scene, typically for background effects.</li> <li>Integration with Theme: If <code>themeSettings.background.type</code> is <code>'3d'</code>, <code>TB.ui.theme</code> will initialize <code>TB.graphics</code> (targeting <code>#threeDScene</code>) and call <code>TB.graphics.updateTheme()</code> on light/dark mode changes.</li> <li>Manual Control: <pre><code>TB.graphics.setSierpinskiDepth(3);\nTB.graphics.playAnimationSequence(\"R2+52:P1-31\", () =&gt; console.log(\"3D Animation done!\"));\n// Mouse/touch drag for interaction is usually enabled by default.\n</code></pre></li> </ul>"},{"location":"tbjs/#7-example-login-flow-walkthrough","title":"7. Example: Login Flow Walkthrough","text":"<p>This conceptual example (based on a typical <code>login.js</code> implementation with <code>tbjs</code>) demonstrates how various modules work together:</p> <ol> <li> <p>Initialization (e.g., in a <code>setupLogin</code> function called when the login page loads):</p> <ul> <li>Wait for <code>tbjs:initialized</code> or check <code>TB.isInitialized</code>.</li> <li>Optionally, play an initial graphics animation: <code>TB.graphics.playAnimationSequence(\"Z0+12\")</code>.</li> <li>Check session validity: <code>TB.user.checkSessionValidity()</code>. If valid, show a toast and offer navigation to a dashboard.</li> </ul> </li> <li> <p>Form Submission (e.g., on login button click):</p> <ul> <li>Prevent default form submission.</li> <li>Get username from input. Validate locally (show info/toast on error).</li> <li>Play a \"login attempt\" graphics animation: <code>TB.graphics.playAnimationSequence(\"R1+11:P1-11\")</code>.</li> <li>Show a global loader: <code>TB.ui.Loader.show('Attempting login...')</code>.</li> <li>Authentication Logic (Conditional):<ul> <li>If user opts for WebAuthn/Passkey: <code>await TB.user.loginWithWebAuthn(username)</code>.</li> <li>Else (e.g., device key): <code>await TB.user.loginWithDeviceKey(username)</code>.<ul> <li>If <code>loginWithDeviceKey</code> fails due to no key: Show a sticky error toast with actions:<ul> <li>\"Try Passkey/WebAuthn\": Calls <code>TB.user.loginWithWebAuthn()</code>.</li> <li>\"Register with Invitation\": Prompts for key, calls <code>TB.user.registerDeviceWithInvitation()</code>.</li> <li>\"Send Magic Link\": Calls <code>TB.user.requestMagicLink()</code>.</li> <li>Each action would have its own loader management and graphics animations.</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Result Handling:</p> <ul> <li>Based on <code>result.success</code> from <code>TB.user</code> login methods:<ul> <li>Success:<ul> <li>Show success toast: <code>TB.ui.Toast.showSuccess('Login successful!')</code>.</li> <li>Play success animation: <code>TB.graphics.playAnimationSequence(\"Z1+32:R0+50\")</code>.</li> <li>Navigate: <code>TB.router.navigateTo('/dashboard')</code>.</li> </ul> </li> <li>Failure:<ul> <li>Show error toast: <code>TB.ui.Toast.showError(result.message)</code>.</li> <li>Play failure animation: <code>TB.graphics.playAnimationSequence(\"P2-42\")</code>.</li> </ul> </li> </ul> </li> <li>Use <code>TB.logger</code> for detailed console logging throughout the process.</li> <li>Hide loader (<code>TB.ui.Loader.hide()</code>) and stop animations (<code>TB.graphics.stopAnimationSequence()</code>) in a <code>finally</code> block or after completion.</li> </ul> </li> </ol> <p>This flow showcases: *   Event-driven UI: Graphics and toasts respond to login states. *   Module Orchestration: <code>TB.user</code>, <code>TB.graphics</code>, <code>TB.ui.Toast</code>, <code>TB.ui.Loader</code>, <code>TB.router</code>, <code>TB.logger</code> working in concert. *   User Feedback: Clear messages and visual cues for different scenarios.</p>"},{"location":"tbjs/#8-building-tbjs-for-developers","title":"8. Building <code>tbjs</code> (For Developers)","text":"<p>If you are modifying the <code>tbjs</code> framework itself or need to build it from source:</p> <ol> <li>Prerequisites:<ul> <li>Node.js and npm (or yarn) installed.</li> </ul> </li> <li>Install Dependencies:     Navigate to the <code>tbjs</code> root directory in your terminal and run:     <pre><code>npm install\n# or\n# yarn install\n</code></pre></li> <li>Build Scripts (examples from a typical <code>package.json</code>):<ul> <li>Production Build: <pre><code>npm run build\n</code></pre>     This usually creates optimized, minified files in a <code>dist/</code> directory (e.g., <code>dist/tbjs.js</code> and <code>dist/tbjs.css</code>). The build process uses Webpack, configured in <code>webpack.config.js</code>.</li> <li>Development Watch Mode: <pre><code>npm run watch\n# or npm run dev\n</code></pre>     This watches source files for changes and automatically rebuilds, often in a non-minified format for easier debugging.</li> <li>Linting: <pre><code>npm run lint\n</code></pre>     Checks the JavaScript code for style consistency and potential errors using a linter like ESLint.</li> </ul> </li> </ol>"},{"location":"toolboxv2/","title":"toolboxv2 API Reference","text":"<p>This section provides an API reference for key components directly available from the <code>toolboxv2</code> package.</p>"},{"location":"toolboxv2/#core-application-tooling","title":"Core Application &amp; Tooling","text":""},{"location":"toolboxv2/#toolboxv2.AppType","title":"<code>toolboxv2.AppType</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class AppType:\n    prefix: str\n    id: str\n    globals: dict[str, Any] = {\"root\": dict, }\n    locals: dict[str, Any] = {\"user\": {'app': \"self\"}, }\n\n    local_test: bool = False\n    start_dir: str\n    data_dir: str\n    config_dir: str\n    info_dir: str\n\n    logger: logging.Logger\n    logging_filename: str\n\n    api_allowed_mods_list: list[str] = []\n\n    version: str\n    loop: asyncio.AbstractEventLoop\n\n    keys: dict[str, str] = {\n        \"MACRO\": \"macro~~~~:\",\n        \"MACRO_C\": \"m_color~~:\",\n        \"HELPER\": \"helper~~~:\",\n        \"debug\": \"debug~~~~:\",\n        \"id\": \"name-spa~:\",\n        \"st-load\": \"mute~load:\",\n        \"comm-his\": \"comm-his~:\",\n        \"develop-mode\": \"dev~mode~:\",\n        \"provider::\": \"provider::\",\n    }\n\n    defaults: dict[str, (bool or dict or dict[str, dict[str, str]] or str or list[str] or list[list]) | None] = {\n        \"MACRO\": list[str],\n        \"MACRO_C\": dict,\n        \"HELPER\": dict,\n        \"debug\": str,\n        \"id\": str,\n        \"st-load\": False,\n        \"comm-his\": list[list],\n        \"develop-mode\": bool,\n    }\n\n    cluster_manager: ClusterManager\n    root_blob_storage: BlobStorage\n    config_fh: FileHandler\n    _debug: bool\n    flows: dict[str, Callable]\n    dev_modi: bool\n    functions: dict[str, Any]\n    modules: dict[str, Any]\n\n    interface_type: ToolBoxInterfaces\n    REFIX: str\n\n    alive: bool\n    called_exit: tuple[bool, float]\n    args_sto: AppArgs\n    system_flag = None\n    session = None\n    appdata = None\n    exit_tasks = []\n\n    enable_profiling: bool = False\n    sto = None\n\n    def __init__(self, prefix: None | str= None, args: AppArgs | None = None):\n        self.args_sto = args\n        self.prefix = prefix\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def exit_main(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def hide_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def show_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def disconnect(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def set_logger(self, debug=False):\n        \"\"\"proxi attr\"\"\"\n\n    @property\n    def debug(self):\n        \"\"\"proxi attr\"\"\"\n        return self._debug\n\n    def debug_rains(self, e):\n        \"\"\"proxi attr\"\"\"\n\n    def set_flows(self, r):\n        \"\"\"proxi attr\"\"\"\n\n    def run_flows(self, name, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def rrun_flows(self, name, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def idle(self):\n        import time\n        self.print(\"idle\")\n        try:\n            while self.alive:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            pass\n        self.print(\"idle done\")\n\n    async def a_idle(self):\n        self.print(\"a idle\")\n        try:\n            if hasattr(self, 'daemon_app'):\n                self.print(\"serving daemon\")\n                await self.daemon_app.connect(self)\n            else:\n                self.print(\"serving default\")\n                while self.alive:\n                    await asyncio.sleep(1)\n        except KeyboardInterrupt:\n            pass\n        self.print(\"a idle done\")\n\n    @debug.setter\n    def debug(self, value):\n        \"\"\"proxi attr\"\"\"\n\n    def _coppy_mod(self, content, new_mod_dir, mod_name, file_type='py'):\n        \"\"\"proxi attr\"\"\"\n\n    def _pre_lib_mod(self, mod_name, path_to=\"./runtime\", file_type='py'):\n        \"\"\"proxi attr\"\"\"\n\n    def _copy_load(self, mod_name, file_type='py', **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True):\n        \"\"\"proxi attr\"\"\"\n\n    def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n        \"\"\"proxi attr\"\"\"\n\n    def save_initialized_module(self, tools_class, spec):\n        \"\"\"proxi attr\"\"\"\n\n    def mod_online(self, mod_name, installed=False):\n        \"\"\"proxi attr\"\"\"\n\n    def _get_function(self,\n                      name: Enum or None,\n                      state: bool = True,\n                      specification: str = \"app\",\n                      metadata=False, as_str: tuple or None = None, r=0):\n        \"\"\"proxi attr\"\"\"\n\n    def save_exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def load_mod(self, mod_name: str, mlm='I', **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    async def init_module(self, modular):\n        return await self.load_mod(modular)\n\n    async def load_all_mods_in_file(self, working_dir=\"mods\"):\n        \"\"\"proxi attr\"\"\"\n\n    def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\"):\n        \"\"\"proxi attr\"\"\"\n\n    def remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            self.remove_mod(mod, delete=delete)\n\n    async def a_remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            await self.a_remove_mod(mod, delete=delete)\n\n    def print_ok(self):\n        \"\"\"proxi attr\"\"\"\n        self.logger.info(\"OK\")\n\n    def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n        \"\"\"proxi attr\"\"\"\n\n    def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None):\n        \"\"\"proxi attr\"\"\"\n\n    def remove_mod(self, mod_name, spec='app', delete=True):\n        \"\"\"proxi attr\"\"\"\n\n    async def a_remove_mod(self, mod_name, spec='app', delete=True):\n        \"\"\"proxi attr\"\"\"\n\n    def exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def web_context(self) -&gt; str:\n        \"\"\"returns the build index ( toolbox web component )\"\"\"\n\n    async def a_exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def save_load(self, modname, spec='app'):\n        \"\"\"proxi attr\"\"\"\n\n    def get_function(self, name: Enum or tuple, **kwargs):\n        \"\"\"\n        Kwargs for _get_function\n            metadata:: return the registered function dictionary\n                stateless: (function_data, None), 0\n                stateful: (function_data, higher_order_function), 0\n            state::boolean\n                specification::str default app\n        \"\"\"\n\n    def run_a_from_sync(self, function, *args):\n        \"\"\"\n        run a async fuction\n        \"\"\"\n\n    def run_bg_task_advanced(self, task, *args, **kwargs):\n        \"\"\"\n        proxi attr\n        \"\"\"\n\n    def wait_for_bg_tasks(self, timeout=None):\n        \"\"\"\n        proxi attr\n        \"\"\"\n\n    def run_bg_task(self, task):\n        \"\"\"\n                run a async fuction\n                \"\"\"\n    def run_function(self, mod_function_name: Enum or tuple,\n                     tb_run_function_with_state=True,\n                     tb_run_with_specification='app',\n                     args_=None,\n                     kwargs_=None,\n                     *args,\n                     **kwargs) -&gt; Result:\n\n        \"\"\"proxi attr\"\"\"\n\n    async def a_run_function(self, mod_function_name: Enum or tuple,\n                             tb_run_function_with_state=True,\n                             tb_run_with_specification='app',\n                             args_=None,\n                             kwargs_=None,\n                             *args,\n                             **kwargs) -&gt; Result:\n\n        \"\"\"proxi attr\"\"\"\n\n    def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n        \"\"\"\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        proxi attr\n        \"\"\"\n\n    async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict):\n        \"\"\"\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        proxi attr\n        \"\"\"\n\n    async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None, method=\"GET\",\n                       args_=None,\n                       kwargs_=None,\n                       *args, **kwargs):\n        \"\"\"run a function remote via http / https\"\"\"\n\n    def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n                get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                kwargs_=None,\n                *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                        backwords_compability_variabel_string_holder=None,\n                        get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                        kwargs_=None,\n                        *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def print(text, *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def sprint(text, *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    # ----------------------------------------------------------------\n    # Decorators for the toolbox\n\n    def _register_function(self, module_name, func_name, data):\n        \"\"\"proxi attr\"\"\"\n\n    def _create_decorator(self, type_: str,\n                          name: str = \"\",\n                          mod_name: str = \"\",\n                          level: int = -1,\n                          restrict_in_virtual_mode: bool = False,\n                          api: bool = False,\n                          helper: str = \"\",\n                          version: str or None = None,\n                          initial=False,\n                          exit_f=False,\n                          test=True,\n                          samples=None,\n                          state=None,\n                          pre_compute=None,\n                          post_compute=None,\n                          memory_cache=False,\n                          file_cache=False,\n                          row=False,\n                          request_as_kwarg=False,\n                          memory_cache_max_size=100,\n                          memory_cache_ttl=300):\n        \"\"\"proxi attr\"\"\"\n\n        # data = {\n        #     \"type\": type_,\n        #     \"module_name\": module_name,\n        #     \"func_name\": func_name,\n        #     \"level\": level,\n        #     \"restrict_in_virtual_mode\": restrict_in_virtual_mode,\n        #     \"func\": func,\n        #     \"api\": api,\n        #     \"helper\": helper,\n        #     \"version\": version,\n        #     \"initial\": initial,\n        #     \"exit_f\": exit_f,\n        #     \"__module__\": func.__module__,\n        #     \"signature\": sig,\n        #     \"params\": params,\n        #     \"state\": (\n        #         False if len(params) == 0 else params[0] in ['self', 'state', 'app']) if state is None else state,\n        #     \"do_test\": test,\n        #     \"samples\": samples,\n        #     \"request_as_kwarg\": request_as_kwarg,\n\n    def tb(self, name=None,\n           mod_name: str = \"\",\n           helper: str = \"\",\n           version: str or None = None,\n           test: bool = True,\n           restrict_in_virtual_mode: bool = False,\n           api: bool = False,\n           initial: bool = False,\n           exit_f: bool = False,\n           test_only: bool = False,\n           memory_cache: bool = False,\n           file_cache: bool = False,\n           row=False,\n           request_as_kwarg: bool = False,\n           state: bool or None = None,\n           level: int = 0,\n           memory_cache_max_size: int = 100,\n           memory_cache_ttl: int = 300,\n           samples: list or dict or None = None,\n           interface: ToolBoxInterfaces or None or str = None,\n           pre_compute=None,\n           post_compute=None,\n           api_methods=None,\n           ):\n        \"\"\"\n    A decorator for registering and configuring functions within a module.\n\n    This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\n    Args:\n        name (str, optional): The name to register the function under. Defaults to the function's own name.\n        mod_name (str, optional): The name of the module the function belongs to.\n        helper (str, optional): A helper string providing additional information about the function.\n        version (str or None, optional): The version of the function or module.\n        test (bool, optional): Flag to indicate if the function is for testing purposes.\n        restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n        api (bool, optional): Flag to indicate if the function is part of an API.\n        initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n        exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n        test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n        memory_cache (bool, optional): Flag to enable memory caching for the function.\n        request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n        file_cache (bool, optional): Flag to enable file caching for the function.\n        row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n        state (bool or None, optional): Flag to indicate if the function maintains state.\n        level (int, optional): The level of the function, used for prioritization or categorization.\n        memory_cache_max_size (int, optional): Maximum size of the memory cache.\n        memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n        samples (list or dict or None, optional): Samples or examples of function usage.\n        interface (str, optional): The interface type for the function.\n        pre_compute (callable, optional): A function to be called before the main function.\n        post_compute (callable, optional): A function to be called after the main function.\n        api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\n    Returns:\n        function: The decorated function with additional processing and registration capabilities.\n    \"\"\"\n        if interface is None:\n            interface = \"tb\"\n        if test_only and 'test' not in self.id:\n            return lambda *args, **kwargs: args\n        return self._create_decorator(interface,\n                                      name,\n                                      mod_name,\n                                      level=level,\n                                      restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                      helper=helper,\n                                      api=api,\n                                      version=version,\n                                      initial=initial,\n                                      exit_f=exit_f,\n                                      test=test,\n                                      samples=samples,\n                                      state=state,\n                                      pre_compute=pre_compute,\n                                      post_compute=post_compute,\n                                      memory_cache=memory_cache,\n                                      file_cache=file_cache,\n                                      row=row,\n                                      request_as_kwarg=request_as_kwarg,\n                                      memory_cache_max_size=memory_cache_max_size,\n                                      memory_cache_ttl=memory_cache_ttl)\n\n    def print_functions(self, name=None):\n\n\n        if not self.functions:\n            print(\"Nothing to see\")\n            return\n\n        def helper(_functions):\n            for func_name, data in _functions.items():\n                if not isinstance(data, dict):\n                    continue\n\n                func_type = data.get('type', 'Unknown')\n                func_level = 'r' if data['level'] == -1 else data['level']\n                api_status = 'Api' if data.get('api', False) else 'Non-Api'\n\n                print(f\"  Function: {func_name}{data.get('signature', '()')}; \"\n                      f\"Type: {func_type}, Level: {func_level}, {api_status}\")\n\n        if name is not None:\n            functions = self.functions.get(name)\n            if functions is not None:\n                print(f\"\\nModule: {name}; Type: {functions.get('app_instance_type', 'Unknown')}\")\n                helper(functions)\n                return\n        for module, functions in self.functions.items():\n            print(f\"\\nModule: {module}; Type: {functions.get('app_instance_type', 'Unknown')}\")\n            helper(functions)\n\n    def save_autocompletion_dict(self):\n        \"\"\"proxi attr\"\"\"\n\n    def get_autocompletion_dict(self):\n        \"\"\"proxi attr\"\"\"\n\n    def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n        \"\"\"proxi attr\"\"\"\n\n    def save_registry_as_enums(self, directory: str, filename: str):\n        \"\"\"proxi attr\"\"\"\n\n    async def execute_all_functions_(self, m_query='', f_query=''):\n        print(\"Executing all functions\")\n        from ..extras import generate_test_cases\n        all_data = {\n            \"modular_run\": 0,\n            \"modular_fatal_error\": 0,\n            \"errors\": 0,\n            \"modular_sug\": 0,\n            \"coverage\": [],\n            \"total_coverage\": {},\n        }\n        items = list(self.functions.items()).copy()\n        for module_name, functions in items:\n            infos = {\n                \"functions_run\": 0,\n                \"functions_fatal_error\": 0,\n                \"error\": 0,\n                \"functions_sug\": 0,\n                'calls': {},\n                'callse': {},\n                \"coverage\": [0, 0],\n            }\n            all_data['modular_run'] += 1\n            if not module_name.startswith(m_query):\n                all_data['modular_sug'] += 1\n                continue\n\n            with Spinner(message=f\"In {module_name}| \"):\n                f_items = list(functions.items()).copy()\n                for function_name, function_data in f_items:\n                    if not isinstance(function_data, dict):\n                        continue\n                    if not function_name.startswith(f_query):\n                        continue\n                    test: list = function_data.get('do_test')\n                    # print(test, module_name, function_name, function_data)\n                    infos[\"coverage\"][0] += 1\n                    if test is False:\n                        continue\n\n                    with Spinner(message=f\"\\t\\t\\t\\t\\t\\tfuction {function_name}...\"):\n                        params: list = function_data.get('params')\n                        sig: signature = function_data.get('signature')\n                        state: bool = function_data.get('state')\n                        samples: bool = function_data.get('samples')\n\n                        test_kwargs_list = [{}]\n\n                        if params is not None:\n                            test_kwargs_list = samples if samples is not None else generate_test_cases(sig=sig)\n                            # print(test_kwargs)\n                            # print(test_kwargs[0])\n                            # test_kwargs = test_kwargs_list[0]\n                        # print(module_name, function_name, test_kwargs_list)\n                        infos[\"coverage\"][1] += 1\n                        for test_kwargs in test_kwargs_list:\n                            try:\n                                # print(f\"test Running {state=} |{module_name}.{function_name}\")\n                                result = await self.a_run_function((module_name, function_name),\n                                                                   tb_run_function_with_state=state,\n                                                                   **test_kwargs)\n                                if not isinstance(result, Result):\n                                    result = Result.ok(result)\n                                if result.info.exec_code == 0:\n                                    infos['calls'][function_name] = [test_kwargs, str(result)]\n                                    infos['functions_sug'] += 1\n                                else:\n                                    infos['functions_sug'] += 1\n                                    infos['error'] += 1\n                                    infos['callse'][function_name] = [test_kwargs, str(result)]\n                            except Exception as e:\n                                infos['functions_fatal_error'] += 1\n                                infos['callse'][function_name] = [test_kwargs, str(e)]\n                            finally:\n                                infos['functions_run'] += 1\n\n                if infos['functions_run'] == infos['functions_sug']:\n                    all_data['modular_sug'] += 1\n                else:\n                    all_data['modular_fatal_error'] += 1\n                if infos['error'] &gt; 0:\n                    all_data['errors'] += infos['error']\n\n                all_data[module_name] = infos\n                if infos['coverage'][0] == 0:\n                    c = 0\n                else:\n                    c = infos['coverage'][1] / infos['coverage'][0]\n                all_data[\"coverage\"].append(f\"{module_name}:{c:.2f}\\n\")\n        total_coverage = sum([float(t.split(\":\")[-1]) for t in all_data[\"coverage\"]]) / len(all_data[\"coverage\"])\n        print(\n            f\"\\n{all_data['modular_run']=}\\n{all_data['modular_sug']=}\\n{all_data['modular_fatal_error']=}\\n{total_coverage=}\")\n        d = analyze_data(all_data)\n        return Result.ok(data=all_data, data_info=d)\n\n    @staticmethod\n    def calculate_complexity(filename_or_code):\n        from radon.complexity import cc_rank, cc_visit\n        if os.path.exists(filename_or_code):\n            with open(filename_or_code) as file:\n                code = file.read()\n        else:\n            code = filename_or_code\n\n        # Calculate and print Cyclomatic Complexity\n        complexity_results = cc_visit(code)\n        i = -1\n        avg_complexity = 0\n        for block in complexity_results:\n            complexity = block.complexity\n            i += 1\n            print(f\"block: {block.name} {i} Class/Fuction/Methode : {block.letter}\")\n            print(f\"    fullname: {block.fullname}\")\n            print(f\"    Cyclomatic Complexity: {complexity}\")\n            # Optional: Get complexity rank\n            avg_complexity += complexity\n            rank = cc_rank(complexity)\n            print(f\"    Complexity Rank: {rank}\")\n            # print(f\"    lineno: {block.lineno}\")\n            print(f\"    endline: {block.endline}\")\n            print(f\"    col_offset: {block.col_offset}\\n\")\n        if i &lt;= 0:\n            i += 2\n        avg_complexity = avg_complexity / i\n        print(f\"\\nAVG Complexity: {avg_complexity:.2f}\")\n        print(f\"Total Rank: {cc_rank(int(avg_complexity + i // 10))}\")\n\n    async def execute_function_test(self, module_name: str, function_name: str,\n                                    function_data: dict, test_kwargs: dict,\n                                    profiler: cProfile.Profile) -&gt; tuple[bool, str, dict, float]:\n        start_time = time.time()\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            try:\n                result = await self.a_run_function(\n                    (module_name, function_name),\n                    tb_run_function_with_state=function_data.get('state'),\n                    **test_kwargs\n                )\n\n                if not isinstance(result, Result):\n                    result = Result.ok(result)\n\n                success = result.info.exec_code == 0\n                execution_time = time.time() - start_time\n                return success, str(result), test_kwargs, execution_time\n            except Exception as e:\n                execution_time = time.time() - start_time\n                return False, str(e), test_kwargs, execution_time\n\n    async def process_function(self, module_name: str, function_name: str,\n                               function_data: dict, profiler: cProfile.Profile) -&gt; tuple[str, ModuleInfo]:\n        start_time = time.time()\n        info = ModuleInfo()\n\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            if not isinstance(function_data, dict):\n                return function_name, info\n\n            test = function_data.get('do_test')\n            info.coverage[0] += 1\n\n            if test is False:\n                return function_name, info\n\n            params = function_data.get('params')\n            sig = function_data.get('signature')\n            samples = function_data.get('samples')\n\n            test_kwargs_list = [{}] if params is None else (\n                samples if samples is not None else generate_test_cases(sig=sig)\n            )\n\n            info.coverage[1] += 1\n\n            # Create tasks for all test cases\n            tasks = [\n                self.execute_function_test(module_name, function_name, function_data, test_kwargs, profiler)\n                for test_kwargs in test_kwargs_list\n            ]\n\n            # Execute all tests concurrently\n            results = await asyncio.gather(*tasks)\n\n            total_execution_time = 0\n            for success, result_str, test_kwargs, execution_time in results:\n                info.functions_run += 1\n                total_execution_time += execution_time\n\n                if success:\n                    info.functions_sug += 1\n                    info.calls[function_name] = [test_kwargs, result_str]\n                else:\n                    info.functions_sug += 1\n                    info.error += 1\n                    info.callse[function_name] = [test_kwargs, result_str]\n\n            info.execution_time = time.time() - start_time\n            return function_name, info\n\n    async def process_module(self, module_name: str, functions: dict,\n                             f_query: str, profiler: cProfile.Profile) -&gt; tuple[str, ModuleInfo]:\n        start_time = time.time()\n\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            async with asyncio.Semaphore(mp.cpu_count()):\n                tasks = [\n                    self.process_function(module_name, fname, fdata, profiler)\n                    for fname, fdata in functions.items()\n                    if fname.startswith(f_query)\n                ]\n\n                if not tasks:\n                    return module_name, ModuleInfo()\n\n                results = await asyncio.gather(*tasks)\n\n                # Combine results from all functions in the module\n                combined_info = ModuleInfo()\n                total_execution_time = 0\n\n                for _, info in results:\n                    combined_info.functions_run += info.functions_run\n                    combined_info.functions_fatal_error += info.functions_fatal_error\n                    combined_info.error += info.error\n                    combined_info.functions_sug += info.functions_sug\n                    combined_info.calls.update(info.calls)\n                    combined_info.callse.update(info.callse)\n                    combined_info.coverage[0] += info.coverage[0]\n                    combined_info.coverage[1] += info.coverage[1]\n                    total_execution_time += info.execution_time\n\n                combined_info.execution_time = time.time() - start_time\n                return module_name, combined_info\n\n    async def execute_all_functions(self, m_query='', f_query='', enable_profiling=True):\n        \"\"\"\n        Execute all functions with parallel processing and optional profiling.\n\n        Args:\n            m_query (str): Module name query filter\n            f_query (str): Function name query filter\n            enable_profiling (bool): Enable detailed profiling information\n        \"\"\"\n        print(\"Executing all functions in parallel\" + (\" with profiling\" if enable_profiling else \"\"))\n\n        start_time = time.time()\n        stats = ExecutionStats()\n        items = list(self.functions.items()).copy()\n\n        # Set up profiling\n        self.enable_profiling = enable_profiling\n        profiler = cProfile.Profile()\n\n        with profile_section(profiler, enable_profiling):\n            # Filter modules based on query\n            filtered_modules = [\n                (mname, mfuncs) for mname, mfuncs in items\n                if mname.startswith(m_query)\n            ]\n\n            stats.modular_run = len(filtered_modules)\n\n            # Process all modules concurrently\n            async with asyncio.Semaphore(mp.cpu_count()):\n                tasks = [\n                    self.process_module(mname, mfuncs, f_query, profiler)\n                    for mname, mfuncs in filtered_modules\n                ]\n\n                results = await asyncio.gather(*tasks)\n\n            # Combine results and calculate statistics\n            for module_name, info in results:\n                if info.functions_run == info.functions_sug:\n                    stats.modular_sug += 1\n                else:\n                    stats.modular_fatal_error += 1\n\n                stats.errors += info.error\n\n                # Calculate coverage\n                coverage = (info.coverage[1] / info.coverage[0]) if info.coverage[0] &gt; 0 else 0\n                stats.coverage.append(f\"{module_name}:{coverage:.2f}\\n\")\n\n                # Store module info\n                stats.__dict__[module_name] = info\n\n            # Calculate total coverage\n            total_coverage = (\n                sum(float(t.split(\":\")[-1]) for t in stats.coverage) / len(stats.coverage)\n                if stats.coverage else 0\n            )\n\n            stats.total_execution_time = time.time() - start_time\n\n            # Generate profiling stats if enabled\n            if enable_profiling:\n                s = io.StringIO()\n                ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n                ps.print_stats()\n                stats.profiling_data = {\n                    'detailed_stats': s.getvalue(),\n                    'total_time': stats.total_execution_time,\n                    'function_count': stats.modular_run,\n                    'successful_functions': stats.modular_sug\n                }\n\n            print(\n                f\"\\n{stats.modular_run=}\"\n                f\"\\n{stats.modular_sug=}\"\n                f\"\\n{stats.modular_fatal_error=}\"\n                f\"\\n{total_coverage=}\"\n                f\"\\nTotal execution time: {stats.total_execution_time:.2f}s\"\n            )\n\n            if enable_profiling:\n                print(\"\\nProfiling Summary:\")\n                print(f\"{'=' * 50}\")\n                print(\"Top 10 time-consuming functions:\")\n                ps.print_stats(10)\n\n            analyzed_data = analyze_data(stats.__dict__)\n            return Result.ok(data=stats.__dict__, data_info=analyzed_data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.debug","title":"<code>debug</code>  <code>property</code> <code>writable</code>","text":"<p>proxi attr</p>"},{"location":"toolboxv2/#toolboxv2.AppType.prefix","title":"<code>prefix = prefix</code>  <code>instance-attribute</code>","text":"<p>proxi attr</p>"},{"location":"toolboxv2/#toolboxv2.AppType.a_exit","title":"<code>a_exit()</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.a_fuction_runner","title":"<code>a_fuction_runner(function, function_data, args, kwargs)</code>  <code>async</code>","text":"<p>parameters = function_data.get('params') modular_name = function_data.get('module_name') function_name = function_data.get('func_name') mod_function_name = f\"{modular_name}.{function_name}\"</p> <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict):\n    \"\"\"\n    parameters = function_data.get('params')\n    modular_name = function_data.get('module_name')\n    function_name = function_data.get('func_name')\n    mod_function_name = f\"{modular_name}.{function_name}\"\n\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.a_remove_mod","title":"<code>a_remove_mod(mod_name, spec='app', delete=True)</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_remove_mod(self, mod_name, spec='app', delete=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.a_run_any","title":"<code>a_run_any(mod_function_name, backwords_compability_variabel_string_holder=None, get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                    backwords_compability_variabel_string_holder=None,\n                    get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                    kwargs_=None,\n                    *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.a_run_function","title":"<code>a_run_function(mod_function_name, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_run_function(self, mod_function_name: Enum or tuple,\n                         tb_run_function_with_state=True,\n                         tb_run_with_specification='app',\n                         args_=None,\n                         kwargs_=None,\n                         *args,\n                         **kwargs) -&gt; Result:\n\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.debug_rains","title":"<code>debug_rains(e)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def debug_rains(self, e):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.disconnect","title":"<code>disconnect(*args, **kwargs)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def disconnect(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.execute_all_functions","title":"<code>execute_all_functions(m_query='', f_query='', enable_profiling=True)</code>  <code>async</code>","text":"<p>Execute all functions with parallel processing and optional profiling.</p> <p>Parameters:</p> Name Type Description Default <code>m_query</code> <code>str</code> <p>Module name query filter</p> <code>''</code> <code>f_query</code> <code>str</code> <p>Function name query filter</p> <code>''</code> <code>enable_profiling</code> <code>bool</code> <p>Enable detailed profiling information</p> <code>True</code> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def execute_all_functions(self, m_query='', f_query='', enable_profiling=True):\n    \"\"\"\n    Execute all functions with parallel processing and optional profiling.\n\n    Args:\n        m_query (str): Module name query filter\n        f_query (str): Function name query filter\n        enable_profiling (bool): Enable detailed profiling information\n    \"\"\"\n    print(\"Executing all functions in parallel\" + (\" with profiling\" if enable_profiling else \"\"))\n\n    start_time = time.time()\n    stats = ExecutionStats()\n    items = list(self.functions.items()).copy()\n\n    # Set up profiling\n    self.enable_profiling = enable_profiling\n    profiler = cProfile.Profile()\n\n    with profile_section(profiler, enable_profiling):\n        # Filter modules based on query\n        filtered_modules = [\n            (mname, mfuncs) for mname, mfuncs in items\n            if mname.startswith(m_query)\n        ]\n\n        stats.modular_run = len(filtered_modules)\n\n        # Process all modules concurrently\n        async with asyncio.Semaphore(mp.cpu_count()):\n            tasks = [\n                self.process_module(mname, mfuncs, f_query, profiler)\n                for mname, mfuncs in filtered_modules\n            ]\n\n            results = await asyncio.gather(*tasks)\n\n        # Combine results and calculate statistics\n        for module_name, info in results:\n            if info.functions_run == info.functions_sug:\n                stats.modular_sug += 1\n            else:\n                stats.modular_fatal_error += 1\n\n            stats.errors += info.error\n\n            # Calculate coverage\n            coverage = (info.coverage[1] / info.coverage[0]) if info.coverage[0] &gt; 0 else 0\n            stats.coverage.append(f\"{module_name}:{coverage:.2f}\\n\")\n\n            # Store module info\n            stats.__dict__[module_name] = info\n\n        # Calculate total coverage\n        total_coverage = (\n            sum(float(t.split(\":\")[-1]) for t in stats.coverage) / len(stats.coverage)\n            if stats.coverage else 0\n        )\n\n        stats.total_execution_time = time.time() - start_time\n\n        # Generate profiling stats if enabled\n        if enable_profiling:\n            s = io.StringIO()\n            ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n            ps.print_stats()\n            stats.profiling_data = {\n                'detailed_stats': s.getvalue(),\n                'total_time': stats.total_execution_time,\n                'function_count': stats.modular_run,\n                'successful_functions': stats.modular_sug\n            }\n\n        print(\n            f\"\\n{stats.modular_run=}\"\n            f\"\\n{stats.modular_sug=}\"\n            f\"\\n{stats.modular_fatal_error=}\"\n            f\"\\n{total_coverage=}\"\n            f\"\\nTotal execution time: {stats.total_execution_time:.2f}s\"\n        )\n\n        if enable_profiling:\n            print(\"\\nProfiling Summary:\")\n            print(f\"{'=' * 50}\")\n            print(\"Top 10 time-consuming functions:\")\n            ps.print_stats(10)\n\n        analyzed_data = analyze_data(stats.__dict__)\n        return Result.ok(data=stats.__dict__, data_info=analyzed_data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.exit","title":"<code>exit()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.exit_main","title":"<code>exit_main(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef exit_main(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.fuction_runner","title":"<code>fuction_runner(function, function_data, args, kwargs, t0=0.0)</code>","text":"<p>parameters = function_data.get('params') modular_name = function_data.get('module_name') function_name = function_data.get('func_name') mod_function_name = f\"{modular_name}.{function_name}\"</p> <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n    \"\"\"\n    parameters = function_data.get('params')\n    modular_name = function_data.get('module_name')\n    function_name = function_data.get('func_name')\n    mod_function_name = f\"{modular_name}.{function_name}\"\n\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.get_all_mods","title":"<code>get_all_mods(working_dir='mods', path_to='./runtime')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.get_autocompletion_dict","title":"<code>get_autocompletion_dict()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_autocompletion_dict(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.get_function","title":"<code>get_function(name, **kwargs)</code>","text":"<p>Kwargs for _get_function     metadata:: return the registered function dictionary         stateless: (function_data, None), 0         stateful: (function_data, higher_order_function), 0     state::boolean         specification::str default app</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_function(self, name: Enum or tuple, **kwargs):\n    \"\"\"\n    Kwargs for _get_function\n        metadata:: return the registered function dictionary\n            stateless: (function_data, None), 0\n            stateful: (function_data, higher_order_function), 0\n        state::boolean\n            specification::str default app\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.get_mod","title":"<code>get_mod(name, spec='app')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.get_username","title":"<code>get_username(get_input=False, default='loot')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.hide_console","title":"<code>hide_console(*args, **kwargs)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def hide_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.inplace_load_instance","title":"<code>inplace_load_instance(mod_name, loc='toolboxv2.mods.', spec='app', save=True)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.load_all_mods_in_file","title":"<code>load_all_mods_in_file(working_dir='mods')</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def load_all_mods_in_file(self, working_dir=\"mods\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.load_mod","title":"<code>load_mod(mod_name, mlm='I', **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def load_mod(self, mod_name: str, mlm='I', **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.mod_online","title":"<code>mod_online(mod_name, installed=False)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def mod_online(self, mod_name, installed=False):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.print","title":"<code>print(text, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef print(text, *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.print_ok","title":"<code>print_ok()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def print_ok(self):\n    \"\"\"proxi attr\"\"\"\n    self.logger.info(\"OK\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.reload_mod","title":"<code>reload_mod(mod_name, spec='app', is_file=True, loc='toolboxv2.mods.')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.remove_mod","title":"<code>remove_mod(mod_name, spec='app', delete=True)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def remove_mod(self, mod_name, spec='app', delete=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.rrun_flows","title":"<code>rrun_flows(name, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def rrun_flows(self, name, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_a_from_sync","title":"<code>run_a_from_sync(function, *args)</code>","text":"<p>run a async fuction</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_a_from_sync(self, function, *args):\n    \"\"\"\n    run a async fuction\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_any","title":"<code>run_any(mod_function_name, backwords_compability_variabel_string_holder=None, get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n            get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n            kwargs_=None,\n            *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_bg_task","title":"<code>run_bg_task(task)</code>","text":"<p>run a async fuction</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_bg_task(self, task):\n    \"\"\"\n            run a async fuction\n            \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_bg_task_advanced","title":"<code>run_bg_task_advanced(task, *args, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_bg_task_advanced(self, task, *args, **kwargs):\n    \"\"\"\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_flows","title":"<code>run_flows(name, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_flows(self, name, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_function","title":"<code>run_function(mod_function_name, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_function(self, mod_function_name: Enum or tuple,\n                 tb_run_function_with_state=True,\n                 tb_run_with_specification='app',\n                 args_=None,\n                 kwargs_=None,\n                 *args,\n                 **kwargs) -&gt; Result:\n\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.run_http","title":"<code>run_http(mod_function_name, function_name=None, method='GET', args_=None, kwargs_=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>run a function remote via http / https</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None, method=\"GET\",\n                   args_=None,\n                   kwargs_=None,\n                   *args, **kwargs):\n    \"\"\"run a function remote via http / https\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.save_autocompletion_dict","title":"<code>save_autocompletion_dict()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_autocompletion_dict(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.save_exit","title":"<code>save_exit()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.save_initialized_module","title":"<code>save_initialized_module(tools_class, spec)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_initialized_module(self, tools_class, spec):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.save_instance","title":"<code>save_instance(instance, modular_id, spec='app', instance_type='file/application', tools_class=None)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.save_load","title":"<code>save_load(modname, spec='app')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_load(self, modname, spec='app'):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.save_registry_as_enums","title":"<code>save_registry_as_enums(directory, filename)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_registry_as_enums(self, directory: str, filename: str):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.set_flows","title":"<code>set_flows(r)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def set_flows(self, r):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.set_logger","title":"<code>set_logger(debug=False)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def set_logger(self, debug=False):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.show_console","title":"<code>show_console(*args, **kwargs)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def show_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.sprint","title":"<code>sprint(text, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef sprint(text, *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.tb","title":"<code>tb(name=None, mod_name='', helper='', version=None, test=True, restrict_in_virtual_mode=False, api=False, initial=False, exit_f=False, test_only=False, memory_cache=False, file_cache=False, row=False, request_as_kwarg=False, state=None, level=0, memory_cache_max_size=100, memory_cache_ttl=300, samples=None, interface=None, pre_compute=None, post_compute=None, api_methods=None)</code>","text":"<p>A decorator for registering and configuring functions within a module.</p> <p>This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to register the function under. Defaults to the function's own name.</p> <code>None</code> <code>mod_name</code> <code>str</code> <p>The name of the module the function belongs to.</p> <code>''</code> <code>helper</code> <code>str</code> <p>A helper string providing additional information about the function.</p> <code>''</code> <code>version</code> <code>str or None</code> <p>The version of the function or module.</p> <code>None</code> <code>test</code> <code>bool</code> <p>Flag to indicate if the function is for testing purposes.</p> <code>True</code> <code>restrict_in_virtual_mode</code> <code>bool</code> <p>Flag to restrict the function in virtual mode.</p> <code>False</code> <code>api</code> <code>bool</code> <p>Flag to indicate if the function is part of an API.</p> <code>False</code> <code>initial</code> <code>bool</code> <p>Flag to indicate if the function should be executed at initialization.</p> <code>False</code> <code>exit_f</code> <code>bool</code> <p>Flag to indicate if the function should be executed at exit.</p> <code>False</code> <code>test_only</code> <code>bool</code> <p>Flag to indicate if the function should only be used for testing.</p> <code>False</code> <code>memory_cache</code> <code>bool</code> <p>Flag to enable memory caching for the function.</p> <code>False</code> <code>request_as_kwarg</code> <code>bool</code> <p>Flag to get request if the fuction is calld from api.</p> <code>False</code> <code>file_cache</code> <code>bool</code> <p>Flag to enable file caching for the function.</p> <code>False</code> <code>row</code> <code>bool</code> <p>rather to auto wrap the result in Result type default False means no row data aka result type</p> <code>False</code> <code>state</code> <code>bool or None</code> <p>Flag to indicate if the function maintains state.</p> <code>None</code> <code>level</code> <code>int</code> <p>The level of the function, used for prioritization or categorization.</p> <code>0</code> <code>memory_cache_max_size</code> <code>int</code> <p>Maximum size of the memory cache.</p> <code>100</code> <code>memory_cache_ttl</code> <code>int</code> <p>Time-to-live for the memory cache entries.</p> <code>300</code> <code>samples</code> <code>list or dict or None</code> <p>Samples or examples of function usage.</p> <code>None</code> <code>interface</code> <code>str</code> <p>The interface type for the function.</p> <code>None</code> <code>pre_compute</code> <code>callable</code> <p>A function to be called before the main function.</p> <code>None</code> <code>post_compute</code> <code>callable</code> <p>A function to be called after the main function.</p> <code>None</code> <code>api_methods</code> <code>list[str]</code> <p>default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>function</code> <p>The decorated function with additional processing and registration capabilities.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def tb(self, name=None,\n       mod_name: str = \"\",\n       helper: str = \"\",\n       version: str or None = None,\n       test: bool = True,\n       restrict_in_virtual_mode: bool = False,\n       api: bool = False,\n       initial: bool = False,\n       exit_f: bool = False,\n       test_only: bool = False,\n       memory_cache: bool = False,\n       file_cache: bool = False,\n       row=False,\n       request_as_kwarg: bool = False,\n       state: bool or None = None,\n       level: int = 0,\n       memory_cache_max_size: int = 100,\n       memory_cache_ttl: int = 300,\n       samples: list or dict or None = None,\n       interface: ToolBoxInterfaces or None or str = None,\n       pre_compute=None,\n       post_compute=None,\n       api_methods=None,\n       ):\n    \"\"\"\nA decorator for registering and configuring functions within a module.\n\nThis decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\nArgs:\n    name (str, optional): The name to register the function under. Defaults to the function's own name.\n    mod_name (str, optional): The name of the module the function belongs to.\n    helper (str, optional): A helper string providing additional information about the function.\n    version (str or None, optional): The version of the function or module.\n    test (bool, optional): Flag to indicate if the function is for testing purposes.\n    restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n    api (bool, optional): Flag to indicate if the function is part of an API.\n    initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n    exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n    test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n    memory_cache (bool, optional): Flag to enable memory caching for the function.\n    request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n    file_cache (bool, optional): Flag to enable file caching for the function.\n    row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n    state (bool or None, optional): Flag to indicate if the function maintains state.\n    level (int, optional): The level of the function, used for prioritization or categorization.\n    memory_cache_max_size (int, optional): Maximum size of the memory cache.\n    memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n    samples (list or dict or None, optional): Samples or examples of function usage.\n    interface (str, optional): The interface type for the function.\n    pre_compute (callable, optional): A function to be called before the main function.\n    post_compute (callable, optional): A function to be called after the main function.\n    api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\nReturns:\n    function: The decorated function with additional processing and registration capabilities.\n\"\"\"\n    if interface is None:\n        interface = \"tb\"\n    if test_only and 'test' not in self.id:\n        return lambda *args, **kwargs: args\n    return self._create_decorator(interface,\n                                  name,\n                                  mod_name,\n                                  level=level,\n                                  restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                  helper=helper,\n                                  api=api,\n                                  version=version,\n                                  initial=initial,\n                                  exit_f=exit_f,\n                                  test=test,\n                                  samples=samples,\n                                  state=state,\n                                  pre_compute=pre_compute,\n                                  post_compute=post_compute,\n                                  memory_cache=memory_cache,\n                                  file_cache=file_cache,\n                                  row=row,\n                                  request_as_kwarg=request_as_kwarg,\n                                  memory_cache_max_size=memory_cache_max_size,\n                                  memory_cache_ttl=memory_cache_ttl)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.wait_for_bg_tasks","title":"<code>wait_for_bg_tasks(timeout=None)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def wait_for_bg_tasks(self, timeout=None):\n    \"\"\"\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.watch_mod","title":"<code>watch_mod(mod_name, spec='app', loc='toolboxv2.mods.', use_thread=True, path_name=None)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.AppType.web_context","title":"<code>web_context()</code>","text":"<p>returns the build index ( toolbox web component )</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def web_context(self) -&gt; str:\n    \"\"\"returns the build index ( toolbox web component )\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.MainTool","title":"<code>toolboxv2.MainTool</code>","text":"Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>class MainTool:\n    toolID: str = \"\"\n    # app = None\n    interface = None\n    spec = \"app\"\n    name = \"\"\n    color = \"Bold\"\n    stuf = False\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.__storedargs = args, kwargs\n        self.tools = kwargs.get(\"tool\", {})\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n        if \"on_exit\" in kwargs and isinstance(kwargs.get(\"on_exit\"), Callable):\n            self.on_exit =self.app.tb(\n                mod_name=self.name,\n                name=kwargs.get(\"on_exit\").__name__,\n                version=self.version if hasattr(self, 'version') else \"0.0.0\",\n            )(kwargs.get(\"on_exit\"))\n        self.async_initialized = False\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    pass\n                else:\n                    self.todo()\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\"Error loading mod {self.name} {e}\")\n                if self.app.debug:\n                    import traceback\n                    traceback.print_exc()\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n\n    async def __ainit__(self, *args, **kwargs):\n        self.version = kwargs[\"v\"]\n        self.tools = kwargs.get(\"tool\", {})\n        self.name = kwargs[\"name\"]\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", None))\n        if not hasattr(self, 'config'):\n            self.config = {}\n        self.user = None\n        self.description = \"A toolbox mod\" if kwargs.get(\"description\") is None else kwargs.get(\"description\")\n        if MainTool.interface is None:\n            MainTool.interface = self.app.interface_type\n        # Result.default(self.app.interface)\n\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    await self.todo()\n                else:\n                    pass\n                await asyncio.sleep(0.1)\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\"Error loading mod {self.name} {e}\")\n                if self.app.debug:\n                    import traceback\n                    traceback.print_exc()\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n        self.app.print(f\"TOOL : {self.spec}.{self.name} online\")\n\n\n\n    @property\n    def app(self):\n        return get_app(\n            from_=f\"{self.spec}.{self.name}|{self.toolID if self.toolID else '*' + MainTool.toolID} {self.interface if self.interface else MainTool.interface}\")\n\n    @app.setter\n    def app(self, v):\n        raise PermissionError(f\"You cannot set the App Instance! {v=}\")\n\n    @staticmethod\n    def return_result(error: ToolBoxError = ToolBoxError.none,\n                      exec_code: int = 0,\n                      help_text: str = \"\",\n                      data_info=None,\n                      data=None,\n                      data_to=None):\n\n        if data_to is None:\n            data_to = MainTool.interface if MainTool.interface is not None else ToolBoxInterfaces.cli\n\n        if data is None:\n            data = {}\n\n        if data_info is None:\n            data_info = {}\n\n        return Result(\n            error,\n            ToolBoxResult(data_info=data_info, data=data, data_to=data_to),\n            ToolBoxInfo(exec_code=exec_code, help_text=help_text)\n        )\n\n    def print(self, message, end=\"\\n\", **kwargs):\n        if self.stuf:\n            return\n\n        self.app.print(Style.style_dic[self.color] + self.name + Style.style_dic[\"END\"] + \":\", message, end=end,\n                       **kwargs)\n\n    def add_str_to_config(self, command):\n        if len(command) != 2:\n            self.logger.error('Invalid command must be key value')\n            return False\n        self.config[command[0]] = command[1]\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n\n    def get_version(self) -&gt; str:\n        \"\"\"\"Returns the version\"\"\"\n        return self.version\n\n    async def get_user(self, username: str) -&gt; Result:\n        return await self.app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME, username=username, get_results=True)\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.MainTool.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.__storedargs = args, kwargs\n    self.tools = kwargs.get(\"tool\", {})\n    self.logger = kwargs.get(\"logs\", get_logger())\n    self.color = kwargs.get(\"color\", \"WHITE\")\n    self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n    if \"on_exit\" in kwargs and isinstance(kwargs.get(\"on_exit\"), Callable):\n        self.on_exit =self.app.tb(\n            mod_name=self.name,\n            name=kwargs.get(\"on_exit\").__name__,\n            version=self.version if hasattr(self, 'version') else \"0.0.0\",\n        )(kwargs.get(\"on_exit\"))\n    self.async_initialized = False\n    if self.todo:\n        try:\n            if inspect.iscoroutinefunction(self.todo):\n                pass\n            else:\n                self.todo()\n            get_logger().info(f\"{self.name} on load suspended\")\n        except Exception as e:\n            get_logger().error(f\"Error loading mod {self.name} {e}\")\n            if self.app.debug:\n                import traceback\n                traceback.print_exc()\n    else:\n        get_logger().info(f\"{self.name} no load require\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.MainTool.__initobj","title":"<code>__initobj()</code>  <code>async</code>","text":"<p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.MainTool.get_version","title":"<code>get_version()</code>","text":"<p>\"Returns the version</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\"Returns the version\"\"\"\n    return self.version\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.MainTool.webInstall","title":"<code>webInstall(user_instance, construct_render)</code>","text":"<p>\"Returns a web installer for the given user instance and construct render template</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def webInstall(self, user_instance, construct_render) -&gt; str:\n    \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.get_app","title":"<code>toolboxv2.get_app(from_=None, name=None, args=AppArgs().default(), app_con=None, sync=False)</code>","text":"Source code in <code>toolboxv2/utils/system/getting_and_closing_app.py</code> <pre><code>def get_app(from_=None, name=None, args=AppArgs().default(), app_con=None, sync=False) -&gt; AppType:\n    global registered_apps\n    # name = None\n    # print(f\"get app requested from: {from_} withe name: {name}\")\n    logger = get_logger()\n    logger.info(Style.GREYBG(f\"get app requested from: {from_}\"))\n    if registered_apps[0] is not None:\n        return registered_apps[0]\n\n    if app_con is None:\n        from ... import App\n        app_con = App\n    app = app_con(name, args=args) if name else app_con()\n    logger.info(Style.Bold(f\"App instance, returned ID: {app.id}\"))\n\n    registered_apps[0] = app\n    return app\n</code></pre>"},{"location":"toolboxv2/#system-utilities-configuration","title":"System Utilities &amp; Configuration","text":""},{"location":"toolboxv2/#toolboxv2.FileHandler","title":"<code>toolboxv2.FileHandler</code>","text":"<p>               Bases: <code>Code</code></p> Source code in <code>toolboxv2/utils/system/file_handler.py</code> <pre><code>class FileHandler(Code):\n\n    def __init__(self, filename, name='mainTool', keys=None, defaults=None):\n        if defaults is None:\n            defaults = {}\n        if keys is None:\n            keys = {}\n        assert filename.endswith(\".config\") or filename.endswith(\".data\"), \\\n            f\"filename must end with .config or .data {filename=}\"\n        self.file_handler_save = {}\n        self.file_handler_load = {}\n        self.file_handler_key_mapper = {}\n        self.file_handler_filename = filename\n        self.file_handler_storage = None\n        self.file_handler_max_loaded_index_ = 0\n        self.file_handler_file_prefix = (f\".{filename.split('.')[1]}/\"\n                                         f\"{name.replace('.', '-')}/\")\n        # self.load_file_handler()\n        self.set_defaults_keys_file_handler(keys, defaults)\n\n    def _open_file_handler(self, mode: str, rdu):\n        logger = get_logger()\n        logger.info(Style.Bold(Style.YELLOW(f\"Opening file in mode : {mode}\")))\n        if self.file_handler_storage:\n            self.file_handler_storage.close()\n            self.file_handler_storage = None\n        try:\n            self.file_handler_storage = open(self.file_handler_file_prefix + self.file_handler_filename, mode)\n            self.file_handler_max_loaded_index_ += 1\n        except FileNotFoundError:\n            if self.file_handler_max_loaded_index_ == 2:\n                os.makedirs(self.file_handler_file_prefix, exist_ok=True)\n            if self.file_handler_max_loaded_index_ == 3:\n                os.makedirs(\".config/mainTool\", exist_ok=True)\n            if self.file_handler_max_loaded_index_ &gt;= 5:\n                print(Style.RED(f\"pleas create this file to prosed : {self.file_handler_file_prefix}\"\n                                f\"{self.file_handler_filename}\"))\n                logger.critical(f\"{self.file_handler_file_prefix} {self.file_handler_filename} FileNotFoundError cannot\"\n                                f\" be Created\")\n                exit(0)\n            self.file_handler_max_loaded_index_ += 1\n            logger.info(Style.YELLOW(f\"Try Creating File: {self.file_handler_file_prefix}{self.file_handler_filename}\"))\n\n            if not os.path.exists(f\"{self.file_handler_file_prefix}\"):\n                os.makedirs(f\"{self.file_handler_file_prefix}\")\n\n            with open(self.file_handler_file_prefix + self.file_handler_filename, 'a'):\n                logger.info(Style.GREEN(\"File created successfully\"))\n                self.file_handler_max_loaded_index_ = -1\n            rdu()\n        except OSError and PermissionError as e:\n            raise e\n\n    def open_s_file_handler(self):\n        self._open_file_handler('w+', self.open_s_file_handler)\n        return self\n\n    def open_l_file_handler(self):\n        self._open_file_handler('r+', self.open_l_file_handler)\n        return self\n\n    def save_file_handler(self):\n        get_logger().info(\n            Style.BLUE(\n                f\"init Saving (S) {self.file_handler_filename} \"\n            )\n        )\n        if self.file_handler_storage:\n            get_logger().warning(\n                f\"WARNING file is already open (S): {self.file_handler_filename} {self.file_handler_storage}\")\n\n        self.open_s_file_handler()\n\n        get_logger().info(\n            Style.BLUE(\n                f\"Elements to save : ({len(self.file_handler_save.keys())})\"\n            )\n        )\n\n        self.file_handler_storage.write(json.dumps(self.file_handler_save))\n\n        self.file_handler_storage.close()\n        self.file_handler_storage = None\n\n        get_logger().info(\n            Style.BLUE(\n                f\"closing file : {self.file_handler_filename} \"\n            )\n        )\n\n        return self\n\n    def add_to_save_file_handler(self, key: str, value: str):\n        if len(key) != 10:\n            get_logger(). \\\n                warning(\n                Style.YELLOW(\n                    'WARNING: key length is not 10 characters'\n                )\n            )\n            return False\n        if key not in self.file_handler_load:\n            if key in self.file_handler_key_mapper:\n                key = self.file_handler_key_mapper[key]\n\n        self.file_handler_load[key] = value\n        self.file_handler_save[key] = self.encode_code(value)\n        return True\n\n    def remove_key_file_handler(self, key: str):\n        if key == 'Pka7237327':\n            print(\"Cant remove Root Key\")\n            return\n        if key in self.file_handler_load:\n            del self.file_handler_load[key]\n        if key in self.file_handler_save:\n            del self.file_handler_save[key]\n\n    def load_file_handler(self):\n        get_logger().info(\n            Style.BLUE(\n                f\"loading {self.file_handler_filename} \"\n            )\n        )\n        if self.file_handler_storage:\n            get_logger().warning(\n                Style.YELLOW(\n                    f\"WARNING file is already open (L) {self.file_handler_filename}\"\n                )\n            )\n        self.open_l_file_handler()\n\n        try:\n\n            self.file_handler_save = json.load(self.file_handler_storage)\n            for key, line in self.file_handler_save.items():\n                self.file_handler_load[key] = self.decode_code(line)\n\n        except json.decoder.JSONDecodeError and Exception:\n\n            for line in self.file_handler_storage:\n                line = line[:-1]\n                heda = line[:10]\n                self.file_handler_save[heda] = line[10:]\n                enc = self.decode_code(line[10:])\n                self.file_handler_load[heda] = enc\n\n            self.file_handler_save = {}\n\n        self.file_handler_storage.close()\n        self.file_handler_storage = None\n\n        return self\n\n    def get_file_handler(self, obj: str, default=None) -&gt; str or None:\n        logger = get_logger()\n        if obj not in self.file_handler_load:\n            if obj in self.file_handler_key_mapper:\n                obj = self.file_handler_key_mapper[obj]\n        logger.info(Style.ITALIC(Style.GREY(f\"Collecting data from storage key : {obj}\")))\n        self.file_handler_max_loaded_index_ = -1\n        for objects in self.file_handler_load.items():\n            self.file_handler_max_loaded_index_ += 1\n            if obj == objects[0]:\n\n                try:\n                    if len(objects[1]) &gt; 0:\n                        return ast.literal_eval(objects[1]) if isinstance(objects[1], str) else objects[1]\n                    logger.warning(\n                        Style.YELLOW(\n                            f\"No data  {obj}  ; {self.file_handler_filename}\"\n                        )\n                    )\n                except ValueError:\n                    logger.error(f\"ValueError Loading {obj} ; {self.file_handler_filename}\")\n                except SyntaxError:\n                    if isinstance(objects[1], str):\n                        return objects[1]\n                    logger.warning(\n                        Style.YELLOW(\n                            f\"Possible SyntaxError Loading {obj} ; {self.file_handler_filename}\"\n                            f\" {len(objects[1])} {type(objects[1])}\"\n                        )\n                    )\n                    return objects[1]\n                except NameError:\n                    return str(objects[1])\n\n        if obj in list(self.file_handler_save.keys()):\n            r = self.decode_code(self.file_handler_save[obj])\n            logger.info(f\"returning Default for {obj}\")\n            return r\n\n        if default is None:\n            default = self.file_handler_load.get(obj)\n\n        logger.info(\"no data found\")\n        return default\n\n    def set_defaults_keys_file_handler(self, keys: dict, defaults: dict):\n        list_keys = iter(list(keys.keys()))\n        df_keys = defaults.keys()\n        for key in list_keys:\n            self.file_handler_key_mapper[key] = keys[key]\n            self.file_handler_key_mapper[keys[key]] = key\n            if key in df_keys:\n                self.file_handler_load[keys[key]] = str(defaults[key])\n                self.file_handler_save[keys[key]] = self.encode_code(defaults[key])\n            else:\n                self.file_handler_load[keys[key]] = \"None\"\n\n    def delete_file(self):\n        os.remove(self.file_handler_file_prefix + self.file_handler_filename)\n        get_logger().warning(Style.GREEN(f\"File deleted {self.file_handler_file_prefix + self.file_handler_filename}\"))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils","title":"<code>toolboxv2.utils</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.App","title":"<code>App</code>","text":"Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>class App(AppType, metaclass=Singleton):\n\n    def __init__(self, prefix: str = \"\", args=AppArgs().default()):\n        super().__init__(prefix, args)\n        self._web_context = None\n        t0 = time.perf_counter()\n        abspath = os.path.abspath(__file__)\n        self.system_flag = system()  # Linux: Linux Mac: Darwin Windows: Windows\n\n        self.appdata = os.getenv('APPDATA') if os.name == 'nt' else os.getenv('XDG_CONFIG_HOME') or os.path.expanduser(\n                '~/.config') if os.name == 'posix' else None\n\n        if self.system_flag == \"Darwin\" or self.system_flag == \"Linux\":\n            dir_name = os.path.dirname(abspath).replace(\"/utils\", \"\")\n        else:\n            dir_name = os.path.dirname(abspath).replace(\"\\\\utils\", \"\")\n\n        self.start_dir = str(dir_name)\n\n        self.bg_tasks = []\n\n        lapp = dir_name + '\\\\.data\\\\'\n\n        if not prefix:\n            if not os.path.exists(f\"{lapp}last-app-prefix.txt\"):\n                os.makedirs(lapp, exist_ok=True)\n                open(f\"{lapp}last-app-prefix.txt\", \"a\").close()\n            with open(f\"{lapp}last-app-prefix.txt\") as prefix_file:\n                cont = prefix_file.read()\n                if cont:\n                    prefix = cont.rstrip()\n        else:\n            if not os.path.exists(f\"{lapp}last-app-prefix.txt\"):\n                os.makedirs(lapp, exist_ok=True)\n                open(f\"{lapp}last-app-prefix.txt\", \"a\").close()\n            with open(f\"{lapp}last-app-prefix.txt\", \"w\") as prefix_file:\n                prefix_file.write(prefix)\n\n        self.prefix = prefix\n\n        node_ = node()\n\n        if 'localhost' in node_ and (host := os.getenv('HOSTNAME', 'localhost')) != 'localhost':\n            node_ = node_.replace('localhost', host)\n        self.id = prefix + '-' + node_\n        self.globals = {\n            \"root\": {**globals()},\n        }\n        self.locals = {\n            \"user\": {'app': self, **locals()},\n        }\n\n        identification = self.id\n        collective_identification = self.id\n        if \"test\" in prefix:\n            if self.system_flag == \"Darwin\" or self.system_flag == \"Linux\":\n                start_dir = self.start_dir.replace(\"ToolBoxV2/toolboxv2\", \"toolboxv2\")\n            else:\n                start_dir = self.start_dir.replace(\"ToolBoxV2\\\\toolboxv2\", \"toolboxv2\")\n            self.data_dir = start_dir + '\\\\.data\\\\' + \"test\"\n            self.config_dir = start_dir + '\\\\.config\\\\' + \"test\"\n            self.info_dir = start_dir + '\\\\.info\\\\' + \"test\"\n        elif identification.startswith('collective-'):\n            collective_identification = identification.split('-')[1]\n            self.data_dir = self.start_dir + '\\\\.data\\\\' + collective_identification\n            self.config_dir = self.start_dir + '\\\\.config\\\\' + collective_identification\n            self.info_dir = self.start_dir + '\\\\.info\\\\' + collective_identification\n            self.id = collective_identification\n        else:\n            self.data_dir = self.start_dir + '\\\\.data\\\\' + identification\n            self.config_dir = self.start_dir + '\\\\.config\\\\' + identification\n            self.info_dir = self.start_dir + '\\\\.info\\\\' + identification\n\n        if self.appdata is None:\n            self.appdata = self.data_dir\n        else:\n            self.appdata += \"/ToolBoxV2\"\n\n        if not os.path.exists(self.appdata):\n            os.makedirs(self.appdata, exist_ok=True)\n        if not os.path.exists(self.data_dir):\n            os.makedirs(self.data_dir, exist_ok=True)\n        if not os.path.exists(self.config_dir):\n            os.makedirs(self.config_dir, exist_ok=True)\n        if not os.path.exists(self.info_dir):\n            os.makedirs(self.info_dir, exist_ok=True)\n\n        print(f\"Starting ToolBox as {prefix} from :\", Style.Bold(Style.CYAN(f\"{os.getcwd()}\")))\n\n        logger_info_str, self.logger, self.logging_filename = self.set_logger(args.debug)\n\n        print(\"Logger \" + logger_info_str)\n        print(\"================================\")\n        self.logger.info(\"Logger initialized\")\n        get_logger().info(Style.GREEN(\"Starting Application instance\"))\n        if args.init and args.init is not None and self.start_dir not in sys.path:\n            sys.path.append(self.start_dir)\n\n        __version__ = get_version_from_pyproject()\n        self.version = __version__\n\n        self.keys = {\n            \"MACRO\": \"macro~~~~:\",\n            \"MACRO_C\": \"m_color~~:\",\n            \"HELPER\": \"helper~~~:\",\n            \"debug\": \"debug~~~~:\",\n            \"id\": \"name-spa~:\",\n            \"st-load\": \"mute~load:\",\n            \"comm-his\": \"comm-his~:\",\n            \"develop-mode\": \"dev~mode~:\",\n            \"provider::\": \"provider::\",\n        }\n\n        defaults = {\n            \"MACRO\": ['Exit'],\n            \"MACRO_C\": {},\n            \"HELPER\": {},\n            \"debug\": args.debug,\n            \"id\": self.id,\n            \"st-load\": False,\n            \"comm-his\": [[]],\n            \"develop-mode\": False,\n        }\n        self.config_fh = FileHandler(collective_identification + \".config\", keys=self.keys, defaults=defaults)\n        self.config_fh.load_file_handler()\n        self._debug = args.debug\n        self.flows = {}\n        self.dev_modi = self.config_fh.get_file_handler(self.keys[\"develop-mode\"])\n        if self.config_fh.get_file_handler(\"provider::\") is None:\n            self.config_fh.add_to_save_file_handler(\"provider::\", \"http://localhost:\" + str(\n                self.args_sto.port) if os.environ.get(\"HOSTNAME\",\"localhost\") == \"localhost\" else \"https://simplecore.app\")\n        self.functions = {}\n        self.modules = {}\n\n        self.interface_type = ToolBoxInterfaces.native\n        self.PREFIX = Style.CYAN(f\"~{node()}@&gt;\")\n        self.alive = True\n        self.called_exit = False, time.time()\n\n        self.print(f\"Infos:\\n  {'Name':&lt;8} -&gt; {node()}\\n  {'ID':&lt;8} -&gt; {self.id}\\n  {'Version':&lt;8} -&gt; {self.version}\\n\")\n\n        self.logger.info(\n            Style.GREEN(\n                f\"Finish init up in {time.perf_counter() - t0:.2f}s\"\n            )\n        )\n\n        self.args_sto = args\n        self.loop = None\n\n        from .system.session import Session\n        self.session: Session = Session(self.get_username())\n        if len(sys.argv) &gt; 2 and \"db\" == sys.argv[1]:\n            return\n        from .system.db_cli_manager import ClusterManager, get_executable_path\n        self.cluster_manager = ClusterManager()\n        online_list, server_list = self.cluster_manager.status_all(silent=True)\n        if not server_list:\n            self.cluster_manager.start_all(get_executable_path(), self.version)\n            _, server_list = self.cluster_manager.status_all()\n        from .extras.blobs import BlobStorage\n        self.root_blob_storage = BlobStorage(servers=server_list, storage_directory=self.data_dir+ '\\\\blob_cache\\\\')\n\n    def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n        user_name = self.config_fh.get_file_handler(\"ac_user:::\")\n        if get_input and user_name is None:\n            user_name = input(\"Input your username: \")\n            self.config_fh.add_to_save_file_handler(\"ac_user:::\", user_name)\n        if user_name is None:\n            user_name = default\n            self.config_fh.add_to_save_file_handler(\"ac_user:::\", user_name)\n        return user_name\n\n    def set_username(self, username):\n        return self.config_fh.add_to_save_file_handler(\"ac_user:::\", username)\n\n    @staticmethod\n    def exit_main(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def hide_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def show_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def disconnect(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def set_logger(self, debug=False):\n        if \"test\" in self.prefix and not debug:\n            logger, logging_filename = setup_logging(logging.NOTSET, name=\"toolbox-test\", interminal=True,\n                                                     file_level=logging.NOTSET, app_name=self.id)\n            logger_info_str = \"in Test Mode\"\n        elif \"live\" in self.prefix and not debug:\n            logger, logging_filename = setup_logging(logging.DEBUG, name=\"toolbox-live\", interminal=False,\n                                                     file_level=logging.WARNING, app_name=self.id)\n            logger_info_str = \"in Live Mode\"\n            # setup_logging(logging.WARNING, name=\"toolbox-live\", is_online=True\n            #              , online_level=logging.WARNING).info(\"Logger initialized\")\n        elif \"debug\" in self.prefix or self.prefix.endswith(\"D\"):\n            self.prefix = self.prefix.replace(\"-debug\", '').replace(\"debug\", '')\n            logger, logging_filename = setup_logging(logging.DEBUG, name=\"toolbox-debug\", interminal=True,\n                                                     file_level=logging.WARNING, app_name=self.id)\n            logger_info_str = \"in debug Mode\"\n            self.debug = True\n        elif debug:\n            logger, logging_filename = setup_logging(logging.DEBUG, name=f\"toolbox-{self.prefix}-debug\",\n                                                     interminal=True,\n                                                     file_level=logging.DEBUG, app_name=self.id)\n            logger_info_str = \"in args debug Mode\"\n        else:\n            logger, logging_filename = setup_logging(logging.ERROR, name=f\"toolbox-{self.prefix}\", app_name=self.id)\n            logger_info_str = \"in Default\"\n\n        return logger_info_str, logger, logging_filename\n\n    @property\n    def debug(self):\n        return self._debug\n\n    @debug.setter\n    def debug(self, value):\n        if not isinstance(value, bool):\n            self.logger.debug(f\"Value must be an boolean. is : {value} type of {type(value)}\")\n            raise ValueError(\"Value must be an boolean.\")\n\n        # self.logger.info(f\"Setting debug {value}\")\n        self._debug = value\n\n    def debug_rains(self, e):\n        if self.debug:\n            import traceback\n            x = \"=\"*5\n            x += \" DEBUG \"\n            x += \"=\"*5\n            self.print(x)\n            self.print(traceback.format_exc())\n            self.print(x)\n            raise e\n        else:\n            self.logger.error(f\"Error: {e}\")\n            import traceback\n            x = \"=\"*5\n            x += \" DEBUG \"\n            x += \"=\"*5\n            self.print(x)\n            self.print(traceback.format_exc())\n            self.print(x)\n\n    def set_flows(self, r):\n        self.flows = r\n\n    async def run_flows(self, name, **kwargs):\n        from ..flows import flows_dict as flows_dict_func\n        if name not in self.flows:\n            self.flows = {**self.flows, **flows_dict_func(s=name, remote=True)}\n        if name in self.flows:\n            if asyncio.iscoroutinefunction(self.flows[name]):\n                return await self.flows[name](get_app(from_=\"runner\"), self.args_sto, **kwargs)\n            else:\n                return self.flows[name](get_app(from_=\"runner\"), self.args_sto, **kwargs)\n        else:\n            print(\"Flow not found, active flows:\", len(self.flows.keys()))\n\n    def _coppy_mod(self, content, new_mod_dir, mod_name, file_type='py'):\n\n        mode = 'xb'\n        self.logger.info(f\" coppy mod {mod_name} to {new_mod_dir} size : {sys.getsizeof(content) / 8388608:.3f} mb\")\n\n        if not os.path.exists(new_mod_dir):\n            os.makedirs(new_mod_dir)\n            with open(f\"{new_mod_dir}/__init__.py\", \"w\") as nmd:\n                nmd.write(f\"__version__ = '{self.version}'\")\n\n        if os.path.exists(f\"{new_mod_dir}/{mod_name}.{file_type}\"):\n            mode = False\n\n            with open(f\"{new_mod_dir}/{mod_name}.{file_type}\", 'rb') as d:\n                runtime_mod = d.read()  # Testing version but not efficient\n\n            if len(content) != len(runtime_mod):\n                mode = 'wb'\n\n        if mode:\n            with open(f\"{new_mod_dir}/{mod_name}.{file_type}\", mode) as f:\n                f.write(content)\n\n    def _pre_lib_mod(self, mod_name, path_to=\"./runtime\", file_type='py'):\n        working_dir = self.id.replace(\".\", \"_\")\n        lib_mod_dir = f\"toolboxv2.runtime.{working_dir}.mod_lib.\"\n\n        self.logger.info(f\"pre_lib_mod {mod_name} from {lib_mod_dir}\")\n\n        postfix = \"_dev\" if self.dev_modi else \"\"\n        mod_file_dir = f\"./mods{postfix}/{mod_name}.{file_type}\"\n        new_mod_dir = f\"{path_to}/{working_dir}/mod_lib\"\n        with open(mod_file_dir, \"rb\") as c:\n            content = c.read()\n        self._coppy_mod(content, new_mod_dir, mod_name, file_type=file_type)\n        return lib_mod_dir\n\n    def _copy_load(self, mod_name, file_type='py', **kwargs):\n        loc = self._pre_lib_mod(mod_name, file_type)\n        return self.inplace_load_instance(mod_name, loc=loc, **kwargs)\n\n    def helper_install_pip_module(self, module_name):\n        if 'main' in self.id:\n            return\n        self.print(f\"Installing {module_name} GREEDY\")\n        os.system(f\"{sys.executable} -m pip install {module_name}\")\n\n    def python_module_import_classifier(self, mod_name, error_message):\n\n        if error_message.startswith(\"No module named 'toolboxv2.utils\"):\n            return Result.default_internal_error(f\"404 {error_message.split('utils')[1]} not found\")\n        if error_message.startswith(\"No module named 'toolboxv2.mods\"):\n            if mod_name.startswith('.'):\n                return\n            return self.run_a_from_sync(self.a_run_any, (\"CloudM\", \"install\"), module_name=mod_name)\n        if error_message.startswith(\"No module named '\"):\n            pip_requ = error_message.split(\"'\")[1].replace(\"'\", \"\").strip()\n            # if 'y' in input(f\"\\t\\t\\tAuto install {pip_requ} Y/n\").lower:\n            return self.helper_install_pip_module(pip_requ)\n            # return Result.default_internal_error(f\"404 {pip_requ} not found\")\n\n    def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True, mfo=None):\n        if self.dev_modi and loc == \"toolboxv2.mods.\":\n            loc = \"toolboxv2.mods_dev.\"\n        if spec=='app' and self.mod_online(mod_name):\n            self.logger.info(f\"Reloading mod from : {loc + mod_name}\")\n            self.remove_mod(mod_name, spec=spec, delete=False)\n\n        if (os.path.exists(self.start_dir + '/mods/' + mod_name) or os.path.exists(\n            self.start_dir + '/mods/' + mod_name + '.py')) and (\n            os.path.isdir(self.start_dir + '/mods/' + mod_name) or os.path.isfile(\n            self.start_dir + '/mods/' + mod_name + '.py')):\n            try:\n                if mfo is None:\n                    modular_file_object = import_module(loc + mod_name)\n                else:\n                    modular_file_object = mfo\n                self.modules[mod_name] = modular_file_object\n            except ModuleNotFoundError as e:\n                self.logger.error(Style.RED(f\"module {loc + mod_name} not found is type sensitive {e}\"))\n                self.print(Style.RED(f\"module {loc + mod_name} not found is type sensitive {e}\"))\n                if self.debug or self.args_sto.sysPrint:\n                    self.python_module_import_classifier(mod_name, str(e))\n                self.debug_rains(e)\n                return None\n        else:\n            self.print(f\"module {loc + mod_name} is not valid\")\n            return None\n        if hasattr(modular_file_object, \"Tools\"):\n            tools_class = modular_file_object.Tools\n        else:\n            if hasattr(modular_file_object, \"name\"):\n                tools_class = modular_file_object\n                modular_file_object = import_module(loc + mod_name)\n            else:\n                tools_class = None\n\n        modular_id = None\n        instance = modular_file_object\n        app_instance_type = \"file/application\"\n\n        if tools_class is None:\n            modular_id = modular_file_object.Name if hasattr(modular_file_object, \"Name\") else mod_name\n\n        if tools_class is None and modular_id is None:\n            modular_id = str(modular_file_object.__name__)\n            self.logger.warning(f\"Unknown instance loaded {mod_name}\")\n            return modular_file_object\n\n        if tools_class is not None:\n            tools_class = self.save_initialized_module(tools_class, spec)\n            modular_id = tools_class.name\n            app_instance_type = \"functions/class\"\n        else:\n            instance.spec = spec\n        # if private:\n        #     self.functions[modular_id][f\"{spec}_private\"] = private\n\n        if not save:\n            return instance if tools_class is None else tools_class\n\n        return self.save_instance(instance, modular_id, spec, app_instance_type, tools_class=tools_class)\n\n    def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n\n        if modular_id in self.functions and tools_class is None:\n            if self.functions[modular_id].get(f\"{spec}_instance\", None) is None:\n                self.functions[modular_id][f\"{spec}_instance\"] = instance\n                self.functions[modular_id][f\"{spec}_instance_type\"] = instance_type\n            else:\n                self.print(\"ERROR OVERRIDE\")\n                raise ImportError(f\"Module already known {modular_id}\")\n\n        elif tools_class is not None:\n            if modular_id not in self.functions:\n                self.functions[modular_id] = {}\n            self.functions[modular_id][f\"{spec}_instance\"] = tools_class\n            self.functions[modular_id][f\"{spec}_instance_type\"] = instance_type\n\n            try:\n                if not hasattr(tools_class, 'tools'):\n                    tools_class.tools = {\"Version\": tools_class.get_version, 'name': tools_class.name}\n                for function_name in list(tools_class.tools.keys()):\n                    t_function_name = function_name.lower()\n                    if t_function_name != \"all\" and t_function_name != \"name\":\n                        self.tb(function_name, mod_name=modular_id)(tools_class.tools.get(function_name))\n                self.functions[modular_id][f\"{spec}_instance_type\"] += \"/BC\"\n                if hasattr(tools_class, 'on_exit'):\n                    if \"on_exit\" in self.functions[modular_id]:\n                        self.functions[modular_id][\"on_exit\"].append(tools_class.on_exit)\n                    else:\n                        self.functions[modular_id][\"on_exit\"] = [tools_class.on_exit]\n            except Exception as e:\n                self.logger.error(f\"Starting Module {modular_id} compatibility failed with : {e}\")\n                pass\n        elif modular_id not in self.functions and tools_class is None:\n            self.functions[modular_id] = {}\n            self.functions[modular_id][f\"{spec}_instance\"] = instance\n            self.functions[modular_id][f\"{spec}_instance_type\"] = instance_type\n\n        else:\n            raise ImportError(f\"Modular {modular_id} is not a valid mod\")\n        on_start = self.functions[modular_id].get(\"on_start\")\n        if on_start is not None:\n            i = 1\n            for f in on_start:\n                try:\n                    f_, e = self.get_function((modular_id, f), state=True, specification=spec)\n                    if e == 0:\n                        self.logger.info(Style.GREY(f\"Running On start {f} {i}/{len(on_start)}\"))\n                        if asyncio.iscoroutinefunction(f_):\n                            self.print(f\"Async on start is only in Tool claas supported for {modular_id}.{f}\" if tools_class is None else f\"initialization starting soon for {modular_id}.{f}\")\n                            self.run_bg_task_advanced(f_)\n                        else:\n                            o = f_()\n                            if o is not None:\n                                self.print(f\"Function {modular_id} On start result: {o}\")\n                    else:\n                        self.logger.warning(f\"starting function not found {e}\")\n                except Exception as e:\n                    self.logger.debug(Style.YELLOW(\n                        Style.Bold(f\"modular:{modular_id}.{f} on_start error {i}/{len(on_start)} -&gt; {e}\")))\n                    self.debug_rains(e)\n                finally:\n                    i += 1\n        return instance if tools_class is None else tools_class\n\n    def save_initialized_module(self, tools_class, spec):\n        tools_class.spec = spec\n        live_tools_class = tools_class(app=self)\n        return live_tools_class\n\n    def mod_online(self, mod_name, installed=False):\n        if installed and mod_name not in self.functions:\n            self.save_load(mod_name)\n        return mod_name in self.functions\n\n    def _get_function(self,\n                      name: Enum or None,\n                      state: bool = True,\n                      specification: str = \"app\",\n                      metadata=False, as_str: tuple or None = None, r=0, **kwargs):\n\n        if as_str is None and isinstance(name, Enum):\n            modular_id = str(name.NAME.value)\n            function_id = str(name.value)\n        elif as_str is None and isinstance(name, list):\n            modular_id, function_id = name[0], name[1]\n        else:\n            modular_id, function_id = as_str\n\n        self.logger.info(f\"getting function : {specification}.{modular_id}.{function_id}\")\n\n        if modular_id not in self.functions:\n            if r == 0:\n                self.save_load(modular_id, spec=specification)\n                return self.get_function(name=(modular_id, function_id),\n                                         state=state,\n                                         specification=specification,\n                                         metadata=metadata,\n                                         r=1)\n            self.logger.warning(f\"function modular not found {modular_id} 404\")\n            return \"404\", 404\n\n        if function_id not in self.functions[modular_id]:\n            self.logger.warning(f\"function data not found {modular_id}.{function_id} 404\")\n            return \"404\", 404\n\n        function_data = self.functions[modular_id][function_id]\n\n        if isinstance(function_data, list):\n            print(f\"functions {function_id} : {function_data}\")\n            function_data = self.functions[modular_id][function_data[kwargs.get('i', -1)]]\n            print(f\"functions {modular_id} : {function_data}\")\n        function = function_data.get(\"func\")\n        params = function_data.get(\"params\")\n\n        state_ = function_data.get(\"state\")\n        if state_ is not None and state != state_:\n            state = state_\n\n        if function is None:\n            self.logger.warning(\"No function found\")\n            return \"404\", 404\n\n        if params is None:\n            self.logger.warning(\"No function (params) found\")\n            return \"404\", 301\n\n        if metadata and not state:\n            self.logger.info(\"returning metadata stateless\")\n            return (function_data, function), 0\n\n        if not state:  # mens a stateless function\n            self.logger.info(\"returning stateless function\")\n            return function, 0\n\n        instance = self.functions[modular_id].get(f\"{specification}_instance\")\n\n        # instance_type = self.functions[modular_id].get(f\"{specification}_instance_type\", \"functions/class\")\n\n        if params[0] == 'app':\n            instance = get_app(from_=f\"fuction {specification}.{modular_id}.{function_id}\")\n\n        if instance is None and self.alive:\n            self.inplace_load_instance(modular_id, spec=specification)\n            instance = self.functions[modular_id].get(f\"{specification}_instance\")\n\n        if instance is None:\n            self.logger.warning(\"No live Instance found\")\n            return \"404\", 400\n\n        # if instance_type.endswith(\"/BC\"):  # for backwards compatibility  functions/class/BC old modules\n        #     # returning as stateless\n        #     # return \"422\", -1\n        #     self.logger.info(\n        #         f\"returning stateless function, cant find tools class for state handling found {instance_type}\")\n        #     if metadata:\n        #         self.logger.info(f\"returning metadata stateless\")\n        #         return (function_data, function), 0\n        #     return function, 0\n\n        self.logger.info(\"wrapping in higher_order_function\")\n\n        self.logger.info(f\"returned fuction {specification}.{modular_id}.{function_id}\")\n        higher_order_function = partial(function, instance)\n\n        if metadata:\n            self.logger.info(\"returning metadata stateful\")\n            return (function_data, higher_order_function), 0\n\n        self.logger.info(\"returning stateful function\")\n        return higher_order_function, 0\n\n    def save_exit(self):\n        self.logger.info(f\"save exiting saving data to {self.config_fh.file_handler_filename} states of {self.debug=}\")\n        self.config_fh.add_to_save_file_handler(self.keys[\"debug\"], str(self.debug))\n\n    def init_mod(self, mod_name, spec='app'):\n        if '.' in mod_name:\n            mod_name = mod_name.split('.')[0]\n        return self.loop_gard().run_until_complete(self.a_init_mod(mod_name, spec))\n\n    def run_bg_task(self, task: Callable, *args, **kwargs) -&gt; Optional[asyncio.Task]:\n        \"\"\"\n        Runs a coroutine in the background without blocking the caller.\n\n        This is the primary method for \"fire-and-forget\" async tasks. It schedules\n        the coroutine to run on the application's main event loop.\n\n        Args:\n            task: The coroutine function to run.\n            *args: Arguments to pass to the coroutine function.\n            **kwargs: Keyword arguments to pass to the coroutine function.\n\n        Returns:\n            An asyncio.Task object representing the scheduled task, or None if\n            the task could not be scheduled.\n        \"\"\"\n        if not callable(task):\n            self.logger.warning(\"Task passed to run_bg_task is not callable!\")\n            return None\n\n        if not asyncio.iscoroutinefunction(task) and not asyncio.iscoroutine(task):\n            self.logger.warning(f\"Task '{getattr(task, '__name__', 'unknown')}' is not a coroutine. \"\n                                f\"Use run_bg_task_advanced for synchronous functions.\")\n            # Fallback to advanced runner for convenience\n            self.run_bg_task_advanced(task, *args, **kwargs)\n            return None\n\n        try:\n            loop = self.loop_gard()\n            if not loop.is_running():\n                # If the main loop isn't running, we can't create a task on it.\n                # This scenario is handled by run_bg_task_advanced.\n                self.logger.info(\"Main event loop not running. Delegating to advanced background runner.\")\n                return self.run_bg_task_advanced(task, *args, **kwargs)\n\n            # Create the coroutine if it's a function\n            coro = task(*args, **kwargs) if asyncio.iscoroutinefunction(task) else task\n\n            # Create a task on the running event loop\n            bg_task = loop.create_task(coro)\n\n            # Add a callback to log exceptions from the background task\n            def _log_exception(the_task: asyncio.Task):\n                if not the_task.cancelled() and the_task.exception():\n                    self.logger.error(f\"Exception in background task '{the_task.get_name()}':\",\n                                      exc_info=the_task.exception())\n\n            bg_task.add_done_callback(_log_exception)\n            self.bg_tasks.append(bg_task)\n            return bg_task\n\n        except Exception as e:\n            self.logger.error(f\"Failed to schedule background task: {e}\", exc_info=True)\n            return None\n\n    def run_bg_task_advanced(self, task: Callable, *args, **kwargs) -&gt; threading.Thread:\n        \"\"\"\n        Runs a task in a separate, dedicated background thread with its own event loop.\n\n        This is ideal for:\n        1. Running an async task from a synchronous context.\n        2. Launching a long-running, independent operation that should not\n           interfere with the main application's event loop.\n\n        Args:\n            task: The function to run (can be sync or async).\n            *args: Arguments for the task.\n            **kwargs: Keyword arguments for the task.\n\n        Returns:\n            The threading.Thread object managing the background execution.\n        \"\"\"\n        if not callable(task):\n            self.logger.warning(\"Task for run_bg_task_advanced is not callable!\")\n            return None\n\n        def thread_target():\n            # Each thread gets its own event loop.\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n\n            try:\n                # Prepare the coroutine we need to run\n                if asyncio.iscoroutinefunction(task):\n                    coro = task(*args, **kwargs)\n                elif asyncio.iscoroutine(task):\n                    # It's already a coroutine object\n                    coro = task\n                else:\n                    # It's a synchronous function, run it in an executor\n                    # to avoid blocking the new event loop.\n                    coro = loop.run_in_executor(None, lambda: task(*args, **kwargs))\n\n                # Run the coroutine to completion\n                result = loop.run_until_complete(coro)\n                self.logger.debug(f\"Advanced background task '{getattr(task, '__name__', 'unknown')}' completed.\")\n                if result is not None:\n                    self.logger.debug(f\"Task result: {str(result)[:100]}\")\n\n            except Exception as e:\n                self.logger.error(f\"Error in advanced background task '{getattr(task, '__name__', 'unknown')}':\",\n                                  exc_info=e)\n            finally:\n                # Cleanly shut down the event loop in this thread.\n                try:\n                    all_tasks = asyncio.all_tasks(loop=loop)\n                    if all_tasks:\n                        for t in all_tasks:\n                            t.cancel()\n                        loop.run_until_complete(asyncio.gather(*all_tasks, return_exceptions=True))\n                finally:\n                    loop.close()\n                    asyncio.set_event_loop(None)\n\n        # Create, start, and return the thread.\n        # It's a daemon thread so it won't prevent the main app from exiting.\n        t = threading.Thread(target=thread_target, daemon=True, name=f\"BGTask-{getattr(task, '__name__', 'unknown')}\")\n        self.bg_tasks.append(t)\n        t.start()\n        return t\n\n    # Helper method to wait for background tasks to complete (optional)\n    def wait_for_bg_tasks(self, timeout=None):\n        \"\"\"\n        Wait for all background tasks to complete.\n\n        Args:\n            timeout: Maximum time to wait (in seconds) for all tasks to complete.\n                     None means wait indefinitely.\n\n        Returns:\n            bool: True if all tasks completed, False if timeout occurred\n        \"\"\"\n        active_tasks = [t for t in self.bg_tasks if t.is_alive()]\n\n        for task in active_tasks:\n            task.join(timeout=timeout)\n            if task.is_alive():\n                return False\n\n        return True\n\n    def __call__(self, *args, **kwargs):\n        return self.run(*args, **kwargs)\n\n    def run(self, *args, request=None, running_function_coro=None, **kwargs):\n        \"\"\"\n        Run a function with support for SSE streaming in both\n        threaded and non-threaded contexts.\n        \"\"\"\n        if running_function_coro is None:\n            mn, fn = args[0]\n            if self.functions.get(mn, {}).get(fn, {}).get('request_as_kwarg', False):\n                kwargs[\"request\"] = RequestData.from_dict(request)\n                if 'data' in kwargs and 'data' not in self.functions.get(mn, {}).get(fn, {}).get('params', []):\n                    kwargs[\"request\"].data = kwargs[\"request\"].body = kwargs['data']\n                    del kwargs['data']\n                if 'form_data' in kwargs and 'form_data' not in self.functions.get(mn, {}).get(fn, {}).get('params',\n                                                                                                           []):\n                    kwargs[\"request\"].form_data = kwargs[\"request\"].body = kwargs['form_data']\n                    del kwargs['form_data']\n\n        # Create the coroutine\n        coro = running_function_coro or self.a_run_any(*args, **kwargs)\n\n        # Get or create an event loop\n        try:\n            loop = asyncio.get_event_loop()\n            is_running = loop.is_running()\n        except RuntimeError:\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n            is_running = False\n\n        # If the loop is already running, run in a separate thread\n        if is_running:\n            # Create thread pool executor as needed\n            if not hasattr(self.__class__, '_executor'):\n                self.__class__._executor = ThreadPoolExecutor(max_workers=4)\n\n            def run_in_new_thread():\n                # Set up a new loop in this thread\n                new_loop = asyncio.new_event_loop()\n                asyncio.set_event_loop(new_loop)\n\n                try:\n                    # Run the coroutine\n                    return new_loop.run_until_complete(coro)\n                finally:\n                    new_loop.close()\n\n            # Run in thread and get result\n            thread_result = self.__class__._executor.submit(run_in_new_thread).result()\n\n            # Handle streaming results from thread\n            if isinstance(thread_result, dict) and thread_result.get(\"is_stream\"):\n                # Create a new SSE stream in the main thread\n                async def stream_from_function():\n                    # Re-run the function with direct async access\n                    stream_result = await self.a_run_any(*args, **kwargs)\n\n                    if (isinstance(stream_result, Result) and\n                        getattr(stream_result.result, 'data_type', None) == \"stream\"):\n                        # Get and forward data from the original generator\n                        original_gen = stream_result.result.data.get(\"generator\")\n                        if inspect.isasyncgen(original_gen):\n                            async for item in original_gen:\n                                yield item\n\n                # Return a new streaming Result\n                return Result.stream(\n                    stream_generator=stream_from_function(),\n                    headers=thread_result.get(\"headers\", {})\n                )\n\n            result = thread_result\n        else:\n            # Direct execution when loop is not running\n            result = loop.run_until_complete(coro)\n\n        # Process the final result\n        if isinstance(result, Result):\n            if 'debug' in self.id:\n                result.print()\n            if getattr(result.result, 'data_type', None) == \"stream\":\n                return result\n            return result.to_api_result().model_dump(mode='json')\n\n        return result\n\n    def loop_gard(self):\n        if self.loop is None:\n            self.loop = asyncio.get_event_loop()\n        if self.loop.is_closed():\n            self.loop = asyncio.get_event_loop()\n        return self.loop\n\n    async def a_init_mod(self, mod_name, spec='app'):\n        mod = self.save_load(mod_name, spec=spec)\n        if hasattr(mod, \"__initobj\") and not mod.async_initialized:\n            await mod\n        return mod\n\n\n    def load_mod(self, mod_name: str, mlm='I', **kwargs):\n\n        action_list_helper = ['I (inplace load dill on error python)',\n                              # 'C (coppy py file to runtime dir)',\n                              # 'S (save py file to dill)',\n                              # 'CS (coppy and save py file)',\n                              # 'D (development mode, inplace load py file)'\n                              ]\n        action_list = {\"I\": lambda: self.inplace_load_instance(mod_name, **kwargs),\n                       \"C\": lambda: self._copy_load(mod_name, **kwargs)\n                       }\n\n        try:\n            if mlm in action_list:\n\n                return action_list.get(mlm)()\n            else:\n                self.logger.critical(\n                    f\"config mlm must be {' or '.join(action_list_helper)} is {mlm=}\")\n                raise ValueError(f\"config mlm must be {' or '.join(action_list_helper)} is {mlm=}\")\n        except ValueError as e:\n            self.logger.warning(Style.YELLOW(f\"Error Loading Module '{mod_name}', with error :{e}\"))\n            self.debug_rains(e)\n        except ImportError as e:\n            self.logger.error(Style.YELLOW(f\"Error Loading Module '{mod_name}', with error :{e}\"))\n            self.debug_rains(e)\n        except Exception as e:\n            self.logger.critical(Style.RED(f\"Error Loading Module '{mod_name}', with critical error :{e}\"))\n            print(Style.RED(f\"Error Loading Module '{mod_name}'\"))\n            self.debug_rains(e)\n\n        return Result.default_internal_error(info=\"info's in logs.\")\n\n    async def load_all_mods_in_file(self, working_dir=\"mods\"):\n        print(f\"LOADING ALL MODS FROM FOLDER : {working_dir}\")\n        t0 = time.perf_counter()\n        # Get the list of all modules\n        module_list = self.get_all_mods(working_dir)\n        open_modules = self.functions.keys()\n        start_len = len(open_modules)\n\n        for om in open_modules:\n            if om in module_list:\n                module_list.remove(om)\n\n        tasks: set[Task] = set()\n\n        _ = {tasks.add(asyncio.create_task(asyncio.to_thread(self.save_load, mod, 'app'))) for mod in module_list}\n        for t in asyncio.as_completed(tasks):\n            try:\n                result = await t\n                if hasattr(result, 'Name'):\n                    print('Opened :', result.Name)\n                elif hasattr(result, 'name'):\n                    if hasattr(result, 'async_initialized'):\n                        if not result.async_initialized:\n                            async def _():\n                                try:\n                                    if asyncio.iscoroutine(result):\n                                        await result\n                                    if hasattr(result, 'Name'):\n                                        print('Opened :', result.Name)\n                                    elif hasattr(result, 'name'):\n                                        print('Opened :', result.name)\n                                except Exception as e:\n                                    self.debug_rains(e)\n                                    if hasattr(result, 'Name'):\n                                        print('Error opening :', result.Name)\n                                    elif hasattr(result, 'name'):\n                                        print('Error opening :', result.name)\n                            asyncio.create_task(_())\n                        else:\n                            print('Opened :', result.name)\n                else:\n                    print('Opened :', result)\n            except Exception as e:\n                self.logger.error(Style.RED(f\"An Error occurred while opening all modules error: {str(e)}\"))\n                self.debug_rains(e)\n        opened = len(self.functions.keys()) - start_len\n\n        self.logger.info(f\"Opened {opened} modules in {time.perf_counter() - t0:.2f}s\")\n        return f\"Opened {opened} modules in {time.perf_counter() - t0:.2f}s\"\n\n    def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\", use_wd=True):\n        self.logger.info(f\"collating all mods in working directory {working_dir}\")\n\n        pr = \"_dev\" if self.dev_modi else \"\"\n        if working_dir == \"mods\" and use_wd:\n            working_dir = f\"{self.start_dir}/mods{pr}\"\n        elif use_wd:\n            pass\n        else:\n            w_dir = self.id.replace(\".\", \"_\")\n            working_dir = f\"{path_to}/{w_dir}/mod_lib{pr}/\"\n        res = os.listdir(working_dir)\n\n        self.logger.info(f\"found : {len(res)} files\")\n\n        def do_helper(_mod):\n            if \"mainTool\" in _mod:\n                return False\n            # if not _mod.endswith(\".py\"):\n            #     return False\n            if _mod.startswith(\"__\"):\n                return False\n            if _mod.startswith(\".\"):\n                return False\n            return not _mod.startswith(\"test_\")\n\n        def r_endings(word: str):\n            if word.endswith(\".py\"):\n                return word[:-3]\n            return word\n\n        mods_list = list(map(r_endings, filter(do_helper, res)))\n\n        self.logger.info(f\"found : {len(mods_list)} Modules\")\n        return mods_list\n\n    def remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            self.remove_mod(mod, delete=delete)\n\n    def remove_mod(self, mod_name, spec='app', delete=True):\n        if mod_name not in self.functions:\n            self.logger.info(f\"mod not active {mod_name}\")\n            return\n\n        on_exit = self.functions[mod_name].get(\"on_exit\")\n        self.logger.info(f\"closing: {on_exit}\")\n        def helper():\n            if f\"{spec}_instance\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance\"]\n            if f\"{spec}_instance_type\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance_type\"]\n\n        if on_exit is None and self.functions[mod_name].get(f\"{spec}_instance_type\", \"\").endswith(\"/BC\"):\n            instance = self.functions[mod_name].get(f\"{spec}_instance\", None)\n            if instance is not None and hasattr(instance, 'on_exit'):\n                if asyncio.iscoroutinefunction(instance.on_exit):\n                    self.exit_tasks.append(instance.on_exit)\n                else:\n                    instance.on_exit()\n\n        if on_exit is None and delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n            return\n        if on_exit is None:\n            helper()\n            return\n\n        i = 1\n\n        for j, f in enumerate(on_exit):\n            try:\n                f_, e = self.get_function((mod_name, f), state=True, specification=spec, i=j)\n                if e == 0:\n                    self.logger.info(Style.GREY(f\"Running On exit {f} {i}/{len(on_exit)}\"))\n                    if asyncio.iscoroutinefunction(f_):\n                        self.exit_tasks.append(f_)\n                        o = None\n                    else:\n                        o = f_()\n                    if o is not None:\n                        self.print(f\"Function On Exit result: {o}\")\n                else:\n                    self.logger.warning(\"closing function not found\")\n            except Exception as e:\n                self.logger.debug(\n                    Style.YELLOW(Style.Bold(f\"modular:{mod_name}.{f} on_exit error {i}/{len(on_exit)} -&gt; {e}\")))\n\n                self.debug_rains(e)\n            finally:\n                i += 1\n\n        helper()\n\n        if delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n\n    async def a_remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            await self.a_remove_mod(mod, delete=delete)\n\n    async def a_remove_mod(self, mod_name, spec='app', delete=True):\n        if mod_name not in self.functions:\n            self.logger.info(f\"mod not active {mod_name}\")\n            return\n        on_exit = self.functions[mod_name].get(\"on_exit\")\n        self.logger.info(f\"closing: {on_exit}\")\n        def helper():\n            if f\"{spec}_instance\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance\"]\n            if f\"{spec}_instance_type\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance_type\"]\n\n        if on_exit is None and self.functions[mod_name].get(f\"{spec}_instance_type\", \"\").endswith(\"/BC\"):\n            instance = self.functions[mod_name].get(f\"{spec}_instance\", None)\n            if instance is not None and hasattr(instance, 'on_exit'):\n                if asyncio.iscoroutinefunction(instance.on_exit):\n                    await instance.on_exit()\n                else:\n                    instance.on_exit()\n\n        if on_exit is None and delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n            return\n        if on_exit is None:\n            helper()\n            return\n\n        i = 1\n        for f in on_exit:\n            try:\n                e = 1\n                if isinstance(f, str):\n                    f_, e = self.get_function((mod_name, f), state=True, specification=spec)\n                elif isinstance(f, Callable):\n                    f_, e, f  = f, 0, f.__name__\n                if e == 0:\n                    self.logger.info(Style.GREY(f\"Running On exit {f} {i}/{len(on_exit)}\"))\n                    if asyncio.iscoroutinefunction(f_):\n                        o = await f_()\n                    else:\n                        o = f_()\n                    if o is not None:\n                        self.print(f\"Function On Exit result: {o}\")\n                else:\n                    self.logger.warning(\"closing function not found\")\n            except Exception as e:\n                self.logger.debug(\n                    Style.YELLOW(Style.Bold(f\"modular:{mod_name}.{f} on_exit error {i}/{len(on_exit)} -&gt; {e}\")))\n                self.debug_rains(e)\n            finally:\n                i += 1\n\n        helper()\n\n        if delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n\n    def exit(self, remove_all=True):\n        if not self.alive:\n            return\n        if self.args_sto.debug:\n            self.hide_console()\n        self.disconnect()\n        if remove_all:\n            self.remove_all_modules()\n        self.logger.info(\"Exiting ToolBox interface\")\n        self.alive = False\n        self.called_exit = True, time.time()\n        self.save_exit()\n        if hasattr(self, 'root_blob_storage') and self.root_blob_storage:\n            self.root_blob_storage.exit()\n        try:\n            self.config_fh.save_file_handler()\n        except SystemExit:\n            print(\"If u ar testing this is fine else ...\")\n\n        if hasattr(self, 'daemon_app'):\n            import threading\n\n            for thread in threading.enumerate()[::-1]:\n                if thread.name == \"MainThread\":\n                    continue\n                try:\n                    with Spinner(f\"closing Thread {thread.name:^50}|\", symbols=\"s\", count_down=True,\n                                 time_in_s=0.751 if not self.debug else 0.6):\n                        thread.join(timeout=0.751 if not self.debug else 0.6)\n                except TimeoutError as e:\n                    self.logger.error(f\"Timeout error on exit {thread.name} {str(e)}\")\n                    print(str(e), f\"Timeout {thread.name}\")\n                except KeyboardInterrupt:\n                    print(\"Unsave Exit\")\n                    break\n        if hasattr(self, 'loop') and self.loop is not None:\n            with Spinner(\"closing Event loop:\", symbols=\"+\"):\n                self.loop.stop()\n\n    async def a_exit(self):\n        await self.a_remove_all_modules(delete=True)\n        results = await asyncio.gather(\n            *[asyncio.create_task(f()) for f in self.exit_tasks if asyncio.iscoroutinefunction(f)])\n        for result in results:\n            self.print(f\"Function On Exit result: {result}\")\n        self.exit(remove_all=False)\n\n    def save_load(self, modname, spec='app'):\n        self.logger.debug(f\"Save load module {modname}\")\n        if not modname:\n            self.logger.warning(\"no filename specified\")\n            return False\n        try:\n            return self.load_mod(modname, spec=spec)\n        except ModuleNotFoundError as e:\n            self.logger.error(Style.RED(f\"Module {modname} not found\"))\n            self.debug_rains(e)\n\n        return False\n\n    def get_function(self, name: Enum or tuple, **kwargs):\n        \"\"\"\n        Kwargs for _get_function\n            metadata:: return the registered function dictionary\n                stateless: (function_data, None), 0\n                stateful: (function_data, higher_order_function), 0\n            state::boolean\n                specification::str default app\n        \"\"\"\n        if isinstance(name, tuple):\n            return self._get_function(None, as_str=name, **kwargs)\n        else:\n            return self._get_function(name, **kwargs)\n\n    async def a_run_function(self, mod_function_name: Enum or tuple,\n                             tb_run_function_with_state=True,\n                             tb_run_with_specification='app',\n                             args_=None,\n                             kwargs_=None,\n                             *args,\n                             **kwargs) -&gt; Result:\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n        if isinstance(mod_function_name, tuple):\n            modular_name, function_name = mod_function_name\n        elif isinstance(mod_function_name, list):\n            modular_name, function_name = mod_function_name[0], mod_function_name[1]\n        elif isinstance(mod_function_name, Enum):\n            modular_name, function_name = mod_function_name.__class__.NAME.value, mod_function_name.value\n        else:\n            raise TypeError(\"Unknown function type\")\n\n        if not self.mod_online(modular_name, installed=True):\n            self.get_mod(modular_name)\n\n        function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                      metadata=True, specification=tb_run_with_specification)\n        self.logger.info(f\"Received fuction : {mod_function_name}, with execode: {error_code}\")\n        if error_code == 404:\n            mod = self.get_mod(modular_name)\n            if hasattr(mod, \"async_initialized\") and not mod.async_initialized:\n                await mod\n            function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                          metadata=True, specification=tb_run_with_specification)\n\n        if error_code == 404:\n            self.logger.warning(Style.RED(\"Function Not Found\"))\n            return (Result.default_user_error(interface=self.interface_type,\n                                              exec_code=404,\n                                              info=\"function not found function is not decorated\").\n                    set_origin(mod_function_name))\n\n        if error_code == 300:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 info=f\"module {modular_name}\"\n                                                      f\" has no state (instance)\").set_origin(mod_function_name)\n\n        if error_code != 0:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=error_code,\n                                                 info=f\"Internal error\"\n                                                      f\" {modular_name}.\"\n                                                      f\"{function_name}\").set_origin(mod_function_name)\n\n        if not tb_run_function_with_state:\n            function_data, _ = function_data\n            function = function_data.get('func')\n        else:\n            function_data, function = function_data\n\n        if not function:\n            self.logger.warning(Style.RED(f\"Function {function_name} not found\"))\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=404,\n                                                 info=\"function not found function\").set_origin(mod_function_name)\n\n        self.logger.info(\"Profiling function\")\n        t0 = time.perf_counter()\n        if asyncio.iscoroutinefunction(function):\n            return await self.a_fuction_runner(function, function_data, args, kwargs, t0)\n        else:\n            return self.fuction_runner(function, function_data, args, kwargs, t0)\n\n    def run_function(self, mod_function_name: Enum or tuple,\n                     tb_run_function_with_state=True,\n                     tb_run_with_specification='app',\n                     args_=None,\n                     kwargs_=None,\n                     *args,\n                     **kwargs) -&gt; Result:\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n        if isinstance(mod_function_name, tuple):\n            modular_name, function_name = mod_function_name\n        elif isinstance(mod_function_name, list):\n            modular_name, function_name = mod_function_name[0], mod_function_name[1]\n        elif isinstance(mod_function_name, Enum):\n            modular_name, function_name = mod_function_name.__class__.NAME.value, mod_function_name.value\n        else:\n            raise TypeError(\"Unknown function type\")\n\n        if not self.mod_online(modular_name, installed=True):\n            self.get_mod(modular_name)\n\n        function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                      metadata=True, specification=tb_run_with_specification)\n        self.logger.info(f\"Received fuction : {mod_function_name}, with execode: {error_code}\")\n        if error_code == 1 or error_code == 3 or error_code == 400:\n            self.get_mod(modular_name)\n            function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                          metadata=True, specification=tb_run_with_specification)\n\n        if error_code == 2:\n            self.logger.warning(Style.RED(\"Function Not Found\"))\n            return (Result.default_user_error(interface=self.interface_type,\n                                              exec_code=404,\n                                              info=\"function not found function is not decorated\").\n                    set_origin(mod_function_name))\n\n        if error_code == -1:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 info=f\"module {modular_name}\"\n                                                      f\" has no state (instance)\").set_origin(mod_function_name)\n\n        if error_code != 0:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=error_code,\n                                                 info=f\"Internal error\"\n                                                      f\" {modular_name}.\"\n                                                      f\"{function_name}\").set_origin(mod_function_name)\n\n        if not tb_run_function_with_state:\n            function_data, _ = function_data\n            function = function_data.get('func')\n        else:\n            function_data, function = function_data\n\n        if not function:\n            self.logger.warning(Style.RED(f\"Function {function_name} not found\"))\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=404,\n                                                 info=\"function not found function\").set_origin(mod_function_name)\n\n        self.logger.info(\"Profiling function\")\n        t0 = time.perf_counter()\n        if asyncio.iscoroutinefunction(function):\n            raise ValueError(f\"Fuction {function_name} is Async use a_run_any\")\n        else:\n            return self.fuction_runner(function, function_data, args, kwargs, t0)\n\n    def run_a_from_sync(self, function, *args, **kwargs):\n        # Initialize self.loop if not already set.\n        if self.loop is None:\n            try:\n                self.loop = asyncio.get_running_loop()\n            except RuntimeError:\n                self.loop = asyncio.new_event_loop()\n\n        # If the loop is running, offload the coroutine to a new thread.\n        if self.loop.is_running():\n            result_future = Future()\n\n            def run_in_new_loop():\n                new_loop = asyncio.new_event_loop()\n                asyncio.set_event_loop(new_loop)\n                try:\n                    result = new_loop.run_until_complete(function(*args, **kwargs))\n                    result_future.set_result(result)\n                except Exception as e:\n                    result_future.set_exception(e)\n                finally:\n                    new_loop.close()\n\n            thread = threading.Thread(target=run_in_new_loop)\n            thread.start()\n            thread.join()  # Block until the thread completes.\n            return result_future.result()\n        else:\n            # If the loop is not running, schedule and run the coroutine directly.\n            future = self.loop.create_task(function(*args, **kwargs))\n            return self.loop.run_until_complete(future)\n\n    def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        row = function_data.get('row')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        if_self_state = 1 if 'self' in parameters else 0\n\n        try:\n            if len(parameters) == 0:\n                res = function()\n            elif len(parameters) == len(args) + if_self_state:\n                res = function(*args)\n            elif len(parameters) == len(kwargs.keys()) + if_self_state:\n                res = function(**kwargs)\n            else:\n                res = function(*args, **kwargs)\n            self.logger.info(f\"Execution done in {time.perf_counter()-t0:.4f}\")\n            if isinstance(res, Result):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.set_origin(mod_function_name)\n            elif isinstance(res, ApiResult):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.as_result().set_origin(mod_function_name).to_api_result()\n            elif row:\n                formatted_result = res\n            else:\n                # Wrap the result in a Result object\n                formatted_result = Result.ok(\n                    interface=self.interface_type,\n                    data_info=\"Auto generated result\",\n                    data=res,\n                    info=\"Function executed successfully\"\n                ).set_origin(mod_function_name)\n            if not row:\n                self.logger.info(\n                    f\"Function Exec code: {formatted_result.info.exec_code} Info's: {formatted_result.info.help_text}\")\n            else:\n                self.logger.info(\n                    f\"Function Exec data: {formatted_result}\")\n        except Exception as e:\n            self.logger.error(\n                Style.YELLOW(Style.Bold(\n                    f\"! Function ERROR: in {modular_name}.{function_name}\")))\n            # Wrap the exception in a Result object\n            formatted_result = Result.default_internal_error(info=str(e)).set_origin(mod_function_name)\n            # res = formatted_result\n            self.logger.error(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed wit an error {str(e)}, {type(e)}\")\n            self.debug_rains(e)\n            self.print(f\"! Function ERROR: in {modular_name}.{function_name} \")\n\n\n\n        else:\n            self.print_ok()\n\n            self.logger.info(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed successfully\")\n\n        return formatted_result\n\n    async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        row = function_data.get('row')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        if_self_state = 1 if 'self' in parameters else 0\n\n        try:\n            if len(parameters) == 0:\n                res = await function()\n            elif len(parameters) == len(args) + if_self_state:\n                res = await function(*args)\n            elif len(parameters) == len(kwargs.keys()) + if_self_state:\n                res = await function(**kwargs)\n            else:\n                res = await function(*args, **kwargs)\n            self.logger.info(f\"Execution done in {time.perf_counter()-t0:.4f}\")\n            if isinstance(res, Result):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.set_origin(mod_function_name)\n            elif isinstance(res, ApiResult):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.as_result().set_origin(mod_function_name).to_api_result()\n            elif row:\n                formatted_result = res\n            else:\n                # Wrap the result in a Result object\n                formatted_result = Result.ok(\n                    interface=self.interface_type,\n                    data_info=\"Auto generated result\",\n                    data=res,\n                    info=\"Function executed successfully\"\n                ).set_origin(mod_function_name)\n            if not row:\n                self.logger.info(\n                    f\"Function Exec code: {formatted_result.info.exec_code} Info's: {formatted_result.info.help_text}\")\n            else:\n                self.logger.info(\n                    f\"Function Exec data: {formatted_result}\")\n        except Exception as e:\n            self.logger.error(\n                Style.YELLOW(Style.Bold(\n                    f\"! Function ERROR: in {modular_name}.{function_name}\")))\n            # Wrap the exception in a Result object\n            formatted_result = Result.default_internal_error(info=str(e)).set_origin(mod_function_name)\n            # res = formatted_result\n            self.logger.error(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed wit an error {str(e)}, {type(e)}\")\n            self.debug_rains(e)\n\n        else:\n            self.print_ok()\n\n            self.logger.info(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed successfully\")\n\n        return formatted_result\n\n    async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None,\n                       args_=None,\n                       kwargs_=None, method=\"GET\",\n                       *args, **kwargs):\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n\n        modular_name = mod_function_name\n        function_name = function_name\n\n        if isinstance(mod_function_name, str) and isinstance(function_name, str):\n            mod_function_name = (mod_function_name, function_name)\n\n        if isinstance(mod_function_name, tuple):\n            modular_name, function_name = mod_function_name\n        elif isinstance(mod_function_name, list):\n            modular_name, function_name = mod_function_name[0], mod_function_name[1]\n        elif isinstance(mod_function_name, Enum):\n            modular_name, function_name = mod_function_name.__class__.NAME.value, mod_function_name.value\n\n        self.logger.info(f\"getting function : {modular_name}.{function_name} from http {self.session.base}\")\n        r = await self.session.fetch(f\"/api/{modular_name}/{function_name}{'?' + args_ if args_ is not None else ''}\",\n                                     data=kwargs, method=method)\n        try:\n            if not r:\n                print(\"\u00a7 Session server Offline!\", self.session.base)\n                return Result.default_internal_error(info=\"Session fetch failed\").as_dict()\n\n            content_type = r.headers.get('Content-Type', '').lower()\n\n            if 'application/json' in content_type:\n                try:\n                    return r.json()\n                except Exception as e:\n                    print(f\"\u26a0 JSON decode error: {e}\")\n                    # Fallback to text if JSON decoding fails\n                    text = r.text\n            else:\n                text = r.text\n\n\n            # Attempt YAML\n            if 'yaml' in content_type or text.strip().startswith('---'):\n                try:\n                    import yaml\n                    return yaml.safe_load(text)\n                except Exception as e:\n                    print(f\"\u26a0 YAML decode error: {e}\")\n\n            # Attempt XML\n            if 'xml' in content_type or text.strip().startswith('&lt;?xml'):\n                try:\n                    import xmltodict\n                    return xmltodict.parse(text)\n                except Exception as e:\n                    print(f\"\u26a0 XML decode error: {e}\")\n\n            # Fallback: return plain text\n            return Result.default_internal_error(data={'raw_text': text, 'content_type': content_type}).as_dict()\n\n        except Exception as e:\n            print(\"\u274c Fatal error during API call:\", e)\n            self.debug_rains(e)\n            return Result.default_internal_error(str(e)).as_dict()\n\n    def run_local(self, *args, **kwargs):\n        return self.run_any(*args, **kwargs)\n\n    async def a_run_local(self, *args, **kwargs):\n        return await self.a_run_any(*args, **kwargs)\n\n    def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n                get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                kwargs_=None,\n                *args, **kwargs):\n\n        # if self.debug:\n        #     self.logger.info(f'Called from: {getouterframes(currentframe(), 2)}')\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n\n        if isinstance(mod_function_name, str) and backwords_compability_variabel_string_holder is None:\n            backwords_compability_variabel_string_holder = mod_function_name.split('.')[-1]\n            mod_function_name = mod_function_name.replace(f\".{backwords_compability_variabel_string_holder}\", \"\")\n\n        if isinstance(mod_function_name, str) and isinstance(backwords_compability_variabel_string_holder, str):\n            mod_function_name = (mod_function_name, backwords_compability_variabel_string_holder)\n\n        res: Result = self.run_function(mod_function_name,\n                                        tb_run_function_with_state=tb_run_function_with_state,\n                                        tb_run_with_specification=tb_run_with_specification,\n                                        args_=args, kwargs_=kwargs).as_result()\n        if isinstance(res, ApiResult):\n            res = res.as_result()\n\n        if isinstance(res, Result) and res.bg_task is not None:\n            self.run_bg_task(res.bg_task)\n\n        if self.debug:\n            res.log(show_data=False)\n\n        if not get_results and isinstance(res, Result):\n            return res.get()\n\n        if get_results and not isinstance(res, Result):\n            return Result.ok(data=res)\n\n        return res\n\n    async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                        backwords_compability_variabel_string_holder=None,\n                        get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                        kwargs_=None,\n                        *args, **kwargs):\n\n        # if self.debug:\n        #     self.logger.info(f'Called from: {getouterframes(currentframe(), 2)}')\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n\n        if isinstance(mod_function_name, str) and backwords_compability_variabel_string_holder is None:\n            backwords_compability_variabel_string_holder = mod_function_name.split('.')[-1]\n            mod_function_name = mod_function_name.replace(f\".{backwords_compability_variabel_string_holder}\", \"\")\n\n        if isinstance(mod_function_name, str) and isinstance(backwords_compability_variabel_string_holder, str):\n            mod_function_name = (mod_function_name, backwords_compability_variabel_string_holder)\n\n        res: Result = await self.a_run_function(mod_function_name,\n                                                tb_run_function_with_state=tb_run_function_with_state,\n                                                tb_run_with_specification=tb_run_with_specification,\n                                                args_=args, kwargs_=kwargs)\n        if isinstance(res, ApiResult):\n            res = res.as_result()\n\n        if isinstance(res, Result) and res.bg_task is not None:\n            self.run_bg_task(res.bg_task)\n\n        if self.debug:\n            res.print()\n            res.log(show_data=False) if isinstance(res, Result) else self.logger.debug(res)\n        if not get_results and isinstance(res, Result):\n            return res.get()\n\n        if get_results and not isinstance(res, Result):\n            return Result.ok(data=res)\n\n        return res\n\n\n    def web_context(self):\n        if self._web_context is None:\n            try:\n                self._web_context = open(\"./dist/helper.html\", encoding=\"utf-8\").read()\n            except Exception as e:\n                self.logger.error(f\"Could not load web context: {e}\")\n                self._web_context = \"&lt;div&gt;&lt;h1&gt;Web Context not found&lt;/h1&gt;&lt;/div&gt;\"\n        return self._web_context\n\n    def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n        if spec != \"app\":\n            self.print(f\"Getting Module {name} spec: {spec}\")\n        if name not in self.functions:\n            mod = self.save_load(name, spec=spec)\n            if mod is False or (isinstance(mod, Result) and mod.is_error()):\n                self.logger.warning(f\"Could not find {name} in {list(self.functions.keys())}\")\n                raise ValueError(f\"Could not find {name} in {list(self.functions.keys())} pleas install the module, or its posibly broken use --debug for infos\")\n        # private = self.functions[name].get(f\"{spec}_private\")\n        # if private is not None:\n        #     if private and spec != 'app':\n        #         raise ValueError(\"Module is private\")\n        if name not in self.functions:\n            self.logger.warning(f\"Module '{name}' is not found\")\n            return None\n        instance = self.functions[name].get(f\"{spec}_instance\")\n        if instance is None:\n            return self.load_mod(name, spec=spec)\n        return self.functions[name].get(f\"{spec}_instance\")\n\n    def print(self, text=\"\", *args, **kwargs):\n        # self.logger.info(f\"Output : {text}\")\n        if 'live' in self.id:\n            return\n\n        flush = kwargs.pop('flush', True)\n        if self.sprint(None):\n            print(Style.CYAN(f\"System${self.id}:\"), end=\" \", flush=flush)\n        print(text, *args, **kwargs, flush=flush)\n\n    def sprint(self, text=\"\", *args, **kwargs):\n        if text is None:\n            return True\n        if 'live' in self.id:\n            return\n        flush = kwargs.pop('flush', True)\n        # self.logger.info(f\"Output : {text}\")\n        print(Style.CYAN(f\"System${self.id}:\"), end=\" \", flush=flush)\n        if isinstance(text, str) and kwargs == {} and text:\n            stram_print(text + ' '.join(args))\n            print()\n        else:\n            print(text, *args, **kwargs, flush=flush)\n\n    # ----------------------------------------------------------------\n    # Decorators for the toolbox\n\n    def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n        self.remove_mod(mod_name, delete=True)\n        if mod_name not in self.modules:\n            self.logger.warning(f\"Module '{mod_name}' is not found\")\n            return\n        if hasattr(self.modules[mod_name], 'reload_save') and self.modules[mod_name].reload_save:\n            def reexecute_module_code(x):\n                return x\n        else:\n            def reexecute_module_code(module_name):\n                if isinstance(module_name, str):\n                    module = import_module(module_name)\n                else:\n                    module = module_name\n                # Get the source code of the module\n                try:\n                    source = inspect.getsource(module)\n                except Exception:\n                    # print(f\"No source for {str(module_name).split('from')[0]}: {e}\")\n                    return module\n                # Compile the source code\n                try:\n                    code = compile(source, module.__file__, 'exec')\n                    # Execute the code in the module's namespace\n                    exec(code, module.__dict__)\n                except Exception:\n                    # print(f\"No source for {str(module_name).split('from')[0]}: {e}\")\n                    pass\n                return module\n\n        if not is_file:\n            mods = self.get_all_mods(\"./mods/\" + mod_name)\n            def recursive_reload(package_name):\n                package = import_module(package_name)\n\n                # First, reload all submodules\n                if hasattr(package, '__path__'):\n                    for _finder, name, _ispkg in pkgutil.walk_packages(package.__path__, package.__name__ + \".\"):\n                        try:\n                            mod = import_module(name)\n                            reexecute_module_code(mod)\n                            reload(mod)\n                        except Exception as e:\n                            print(f\"Error reloading module {name}: {e}\")\n                            break\n\n                # Finally, reload the package itself\n                reexecute_module_code(package)\n                reload(package)\n\n            for mod in mods:\n                if mod.endswith(\".txt\") or mod.endswith(\".yaml\"):\n                    continue\n                try:\n                    recursive_reload(loc + mod_name + '.' + mod)\n                    self.print(f\"Reloaded {mod_name}.{mod}\")\n                except ImportError:\n                    self.print(f\"Could not load {mod_name}.{mod}\")\n        reexecute_module_code(self.modules[mod_name])\n        if mod_name in self.functions:\n            if \"on_exit\" in self.functions[mod_name]:\n                self.functions[mod_name][\"on_exit\"] = []\n            if \"on_start\" in self.functions[mod_name]:\n                self.functions[mod_name][\"on_start\"] = []\n        self.inplace_load_instance(mod_name, spec=spec, mfo=reload(self.modules[mod_name]) if mod_name in self.modules else None)\n\n    def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None, on_reload=None):\n        if path_name is None:\n            path_name = mod_name\n        is_file = os.path.isfile(self.start_dir + '/mods/' + path_name + '.py')\n        import watchfiles\n        def helper():\n            paths = f'mods/{path_name}' + ('.py' if is_file else '')\n            self.logger.info(f'Watching Path: {paths}')\n            try:\n                for changes in watchfiles.watch(paths):\n                    if not changes:\n                        continue\n                    self.reload_mod(mod_name, spec, is_file, loc)\n                    if on_reload:\n                        on_reload()\n            except FileNotFoundError:\n                self.logger.warning(f\"Path {paths} not found\")\n\n        if not use_thread:\n            helper()\n        else:\n            threading.Thread(target=helper, daemon=True).start()\n\n    def _register_function(self, module_name, func_name, data):\n        if module_name not in self.functions:\n            self.functions[module_name] = {}\n        if func_name in self.functions[module_name]:\n            self.print(f\"Overriding function {func_name} from {module_name}\", end=\"\\r\")\n            self.functions[module_name][func_name] = data\n        else:\n            self.functions[module_name][func_name] = data\n\n    def _create_decorator(self, type_: str,\n                          name: str = \"\",\n                          mod_name: str = \"\",\n                          level: int = -1,\n                          restrict_in_virtual_mode: bool = False,\n                          api: bool = False,\n                          helper: str = \"\",\n                          version: str or None = None,\n                          initial: bool=False,\n                          exit_f: bool=False,\n                          test: bool=True,\n                          samples:list[dict[str, Any]] | None=None,\n                          state:bool | None=None,\n                          pre_compute:Callable | None=None,\n                          post_compute:Callable[[], Result] | None=None,\n                          api_methods:list[str] | None=None,\n                          memory_cache: bool=False,\n                          file_cache: bool=False,\n                          request_as_kwarg: bool=False,\n                          row: bool=False,\n                          memory_cache_max_size:int=100,\n                          memory_cache_ttl:int=300):\n\n        if isinstance(type_, Enum):\n            type_ = type_.value\n\n        if memory_cache and file_cache:\n            raise ValueError(\"Don't use both cash at the same time for the same fuction\")\n\n        use_cache = memory_cache or file_cache\n        cache = {}\n        if file_cache:\n            cache = FileCache(folder=self.data_dir + f'\\\\cache\\\\{mod_name}\\\\',\n                              filename=self.data_dir + f'\\\\cache\\\\{mod_name}\\\\{name}cache.db')\n        if memory_cache:\n            cache = MemoryCache(maxsize=memory_cache_max_size, ttl=memory_cache_ttl)\n\n        version = self.version if version is None else self.version + ':' + version\n\n        def a_additional_process(func):\n\n            async def executor(*args, **kwargs):\n\n                if pre_compute is not None:\n                    args, kwargs = await pre_compute(*args, **kwargs)\n                if asyncio.iscoroutinefunction(func):\n                    result = await func(*args, **kwargs)\n                else:\n                    result = func(*args, **kwargs)\n                if post_compute is not None:\n                    result = await post_compute(result)\n                if row:\n                    return result\n                if not isinstance(result, Result):\n                    result = Result.ok(data=result)\n                if result.origin is None:\n                    result.set_origin((mod_name if mod_name else func.__module__.split('.')[-1]\n                                       , name if name else func.__name__\n                                       , type_))\n                if result.result.data_to == ToolBoxInterfaces.native.name:\n                    result.result.data_to = ToolBoxInterfaces.remote if api else ToolBoxInterfaces.native\n                # Wenden Sie die to_api_result Methode auf das Ergebnis an, falls verf\u00fcgbar\n                if api and hasattr(result, 'to_api_result'):\n                    return result.to_api_result()\n                return result\n\n            @wraps(func)\n            async def wrapper(*args, **kwargs):\n\n                if not use_cache:\n                    return await executor(*args, **kwargs)\n\n                try:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{str(args)},{str(kwargs.items())}\")\n                except ValueError:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{bytes(args)},{str(kwargs.items())}\")\n\n                result = cache.get(cache_key)\n                if result is not None:\n                    return result\n\n                result = await executor(*args, **kwargs)\n\n                cache.set(cache_key, result)\n\n                return result\n\n            return wrapper\n\n        def additional_process(func):\n\n            def executor(*args, **kwargs):\n\n                if pre_compute is not None:\n                    args, kwargs = pre_compute(*args, **kwargs)\n                if asyncio.iscoroutinefunction(func):\n                    result = func(*args, **kwargs)\n                else:\n                    result = func(*args, **kwargs)\n                if post_compute is not None:\n                    result = post_compute(result)\n                if row:\n                    return result\n                if not isinstance(result, Result):\n                    result = Result.ok(data=result)\n                if result.origin is None:\n                    result.set_origin((mod_name if mod_name else func.__module__.split('.')[-1]\n                                       , name if name else func.__name__\n                                       , type_))\n                if result.result.data_to == ToolBoxInterfaces.native.name:\n                    result.result.data_to = ToolBoxInterfaces.remote if api else ToolBoxInterfaces.native\n                # Wenden Sie die to_api_result Methode auf das Ergebnis an, falls verf\u00fcgbar\n                if api and hasattr(result, 'to_api_result'):\n                    return result.to_api_result()\n                return result\n\n            @wraps(func)\n            def wrapper(*args, **kwargs):\n\n                if not use_cache:\n                    return executor(*args, **kwargs)\n\n                try:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{str(args)},{str(kwargs.items())}\")\n                except ValueError:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{bytes(args)},{str(kwargs.items())}\")\n\n                result = cache.get(cache_key)\n                if result is not None:\n                    return result\n\n                result = executor(*args, **kwargs)\n\n                cache.set(cache_key, result)\n\n                return result\n\n            return wrapper\n\n        def decorator(func):\n            sig = signature(func)\n            params = list(sig.parameters)\n            module_name = mod_name if mod_name else func.__module__.split('.')[-1]\n            func_name = name if name else func.__name__\n            if func_name == 'on_start':\n                func_name = 'on_startup'\n            if func_name == 'on_exit':\n                func_name = 'on_close'\n            if api or pre_compute is not None or post_compute is not None or memory_cache or file_cache:\n                if asyncio.iscoroutinefunction(func):\n                    func = a_additional_process(func)\n                else:\n                    func = additional_process(func)\n            if api and str(sig.return_annotation) == 'Result':\n                raise ValueError(f\"Fuction {module_name}.{func_name} registered as \"\n                                 f\"Api fuction but uses {str(sig.return_annotation)}\\n\"\n                                 f\"Please change the sig from ..)-&gt; Result to ..)-&gt; ApiResult\")\n            data = {\n                \"type\": type_,\n                \"module_name\": module_name,\n                \"func_name\": func_name,\n                \"level\": level,\n                \"restrict_in_virtual_mode\": restrict_in_virtual_mode,\n                \"func\": func,\n                \"api\": api,\n                \"helper\": helper,\n                \"version\": version,\n                \"initial\": initial,\n                \"exit_f\": exit_f,\n                \"api_methods\": api_methods if api_methods is not None else [\"AUTO\"],\n                \"__module__\": func.__module__,\n                \"signature\": sig,\n                \"params\": params,\n                \"row\": row,\n                \"state\": (\n                    False if len(params) == 0 else params[0] in ['self', 'state', 'app']) if state is None else state,\n                \"do_test\": test,\n                \"samples\": samples,\n                \"request_as_kwarg\": request_as_kwarg,\n\n            }\n            self._register_function(module_name, func_name, data)\n            if exit_f:\n                if \"on_exit\" not in self.functions[module_name]:\n                    self.functions[module_name][\"on_exit\"] = []\n                self.functions[module_name][\"on_exit\"].append(func_name)\n            if initial:\n                if \"on_start\" not in self.functions[module_name]:\n                    self.functions[module_name][\"on_start\"] = []\n                self.functions[module_name][\"on_start\"].append(func_name)\n\n            return func\n\n        decorator.tb_init = True\n\n        return decorator\n\n    def tb(self, name=None,\n           mod_name: str = \"\",\n           helper: str = \"\",\n           version: str | None = None,\n           test: bool = True,\n           restrict_in_virtual_mode: bool = False,\n           api: bool = False,\n           initial: bool = False,\n           exit_f: bool = False,\n           test_only: bool = False,\n           memory_cache: bool = False,\n           file_cache: bool = False,\n           request_as_kwarg: bool = False,\n           row: bool = False,\n           state: bool | None = None,\n           level: int = -1,\n           memory_cache_max_size: int = 100,\n           memory_cache_ttl: int = 300,\n           samples: list or dict or None = None,\n           interface: ToolBoxInterfaces or None or str = None,\n           pre_compute=None,\n           post_compute=None,\n           api_methods=None,\n           ):\n        \"\"\"\n    A decorator for registering and configuring functions within a module.\n\n    This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\n    Args:\n        name (str, optional): The name to register the function under. Defaults to the function's own name.\n        mod_name (str, optional): The name of the module the function belongs to.\n        helper (str, optional): A helper string providing additional information about the function.\n        version (str or None, optional): The version of the function or module.\n        test (bool, optional): Flag to indicate if the function is for testing purposes.\n        restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n        api (bool, optional): Flag to indicate if the function is part of an API.\n        initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n        exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n        test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n        memory_cache (bool, optional): Flag to enable memory caching for the function.\n        request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n        file_cache (bool, optional): Flag to enable file caching for the function.\n        row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n        state (bool or None, optional): Flag to indicate if the function maintains state.\n        level (int, optional): The level of the function, used for prioritization or categorization.\n        memory_cache_max_size (int, optional): Maximum size of the memory cache.\n        memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n        samples (list or dict or None, optional): Samples or examples of function usage.\n        interface (str, optional): The interface type for the function.\n        pre_compute (callable, optional): A function to be called before the main function.\n        post_compute (callable, optional): A function to be called after the main function.\n        api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\n    Returns:\n        function: The decorated function with additional processing and registration capabilities.\n    \"\"\"\n        if interface is None:\n            interface = \"tb\"\n        if test_only and 'test' not in self.id:\n            return lambda *args, **kwargs: args\n        return self._create_decorator(interface,\n                                      name,\n                                      mod_name,\n                                      level=level,\n                                      restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                      helper=helper,\n                                      api=api,\n                                      version=version,\n                                      initial=initial,\n                                      exit_f=exit_f,\n                                      test=test,\n                                      samples=samples,\n                                      state=state,\n                                      pre_compute=pre_compute,\n                                      post_compute=post_compute,\n                                      memory_cache=memory_cache,\n                                      file_cache=file_cache,\n                                      request_as_kwarg=request_as_kwarg,\n                                      row=row,\n                                      api_methods=api_methods,\n                                      memory_cache_max_size=memory_cache_max_size,\n                                      memory_cache_ttl=memory_cache_ttl)\n\n    def save_autocompletion_dict(self):\n        autocompletion_dict = {}\n        for module_name, _module in self.functions.items():\n            data = {}\n            for function_name, function_data in self.functions[module_name].items():\n                if not isinstance(function_data, dict):\n                    continue\n                data[function_name] = {arg: None for arg in\n                                       function_data.get(\"params\", [])}\n                if len(data[function_name].keys()) == 0:\n                    data[function_name] = None\n            autocompletion_dict[module_name] = data if len(data.keys()) &gt; 0 else None\n        self.config_fh.add_to_save_file_handler(\"auto~~~~~~\", str(autocompletion_dict))\n\n    def get_autocompletion_dict(self):\n        return self.config_fh.get_file_handler(\"auto~~~~~~\")\n\n    def save_registry_as_enums(self, directory: str, filename: str):\n        # Ordner erstellen, falls nicht vorhanden\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n        # Dateipfad vorbereiten\n        filepath = os.path.join(directory, filename)\n\n        # Enum-Klassen als Strings generieren\n        enum_classes = [f'\"\"\"Automatic generated by ToolBox v = {self.version}\"\"\"'\n                        f'\\nfrom enum import Enum\\nfrom dataclasses import dataclass'\n                        f'\\n\\n\\n']\n        for module, functions in self.functions.items():\n            if module.startswith(\"APP_INSTANCE\"):\n                continue\n            class_name = module\n            enum_members = \"\\n    \".join(\n                [\n                    f\"{func_name.upper().replace('-', '')}\"\n                    f\" = '{func_name}' \"\n                    f\"# Input: ({fuction_data['params'] if isinstance(fuction_data, dict) else ''}),\"\n                    f\" Output: {fuction_data['signature'].return_annotation if isinstance(fuction_data, dict) else 'None'}\"\n                    for func_name, fuction_data in functions.items()])\n            enum_class = (f'@dataclass\\nclass {class_name.upper().replace(\".\", \"_\").replace(\"-\", \"\")}(Enum):'\n                          f\"\\n    NAME = '{class_name}'\\n    {enum_members}\")\n            enum_classes.append(enum_class)\n\n        # Enums in die Datei schreiben\n        data = \"\\n\\n\\n\".join(enum_classes)\n        if len(data) &lt; 12:\n            raise ValueError(\n                \"Invalid Enums Loosing content pleas delete it ur self in the (utils/system/all_functions_enums.py) or add mor new stuff :}\")\n        with open(filepath, 'w') as file:\n            file.write(data)\n\n        print(Style.Bold(Style.BLUE(f\"Enums gespeichert in {filepath}\")))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.disconnect","title":"<code>disconnect(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef disconnect(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.exit_main","title":"<code>exit_main(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef exit_main(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.get_function","title":"<code>get_function(name, **kwargs)</code>","text":"<p>Kwargs for _get_function     metadata:: return the registered function dictionary         stateless: (function_data, None), 0         stateful: (function_data, higher_order_function), 0     state::boolean         specification::str default app</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def get_function(self, name: Enum or tuple, **kwargs):\n    \"\"\"\n    Kwargs for _get_function\n        metadata:: return the registered function dictionary\n            stateless: (function_data, None), 0\n            stateful: (function_data, higher_order_function), 0\n        state::boolean\n            specification::str default app\n    \"\"\"\n    if isinstance(name, tuple):\n        return self._get_function(None, as_str=name, **kwargs)\n    else:\n        return self._get_function(name, **kwargs)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.hide_console","title":"<code>hide_console(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef hide_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.run","title":"<code>run(*args, request=None, running_function_coro=None, **kwargs)</code>","text":"<p>Run a function with support for SSE streaming in both threaded and non-threaded contexts.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def run(self, *args, request=None, running_function_coro=None, **kwargs):\n    \"\"\"\n    Run a function with support for SSE streaming in both\n    threaded and non-threaded contexts.\n    \"\"\"\n    if running_function_coro is None:\n        mn, fn = args[0]\n        if self.functions.get(mn, {}).get(fn, {}).get('request_as_kwarg', False):\n            kwargs[\"request\"] = RequestData.from_dict(request)\n            if 'data' in kwargs and 'data' not in self.functions.get(mn, {}).get(fn, {}).get('params', []):\n                kwargs[\"request\"].data = kwargs[\"request\"].body = kwargs['data']\n                del kwargs['data']\n            if 'form_data' in kwargs and 'form_data' not in self.functions.get(mn, {}).get(fn, {}).get('params',\n                                                                                                       []):\n                kwargs[\"request\"].form_data = kwargs[\"request\"].body = kwargs['form_data']\n                del kwargs['form_data']\n\n    # Create the coroutine\n    coro = running_function_coro or self.a_run_any(*args, **kwargs)\n\n    # Get or create an event loop\n    try:\n        loop = asyncio.get_event_loop()\n        is_running = loop.is_running()\n    except RuntimeError:\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        is_running = False\n\n    # If the loop is already running, run in a separate thread\n    if is_running:\n        # Create thread pool executor as needed\n        if not hasattr(self.__class__, '_executor'):\n            self.__class__._executor = ThreadPoolExecutor(max_workers=4)\n\n        def run_in_new_thread():\n            # Set up a new loop in this thread\n            new_loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(new_loop)\n\n            try:\n                # Run the coroutine\n                return new_loop.run_until_complete(coro)\n            finally:\n                new_loop.close()\n\n        # Run in thread and get result\n        thread_result = self.__class__._executor.submit(run_in_new_thread).result()\n\n        # Handle streaming results from thread\n        if isinstance(thread_result, dict) and thread_result.get(\"is_stream\"):\n            # Create a new SSE stream in the main thread\n            async def stream_from_function():\n                # Re-run the function with direct async access\n                stream_result = await self.a_run_any(*args, **kwargs)\n\n                if (isinstance(stream_result, Result) and\n                    getattr(stream_result.result, 'data_type', None) == \"stream\"):\n                    # Get and forward data from the original generator\n                    original_gen = stream_result.result.data.get(\"generator\")\n                    if inspect.isasyncgen(original_gen):\n                        async for item in original_gen:\n                            yield item\n\n            # Return a new streaming Result\n            return Result.stream(\n                stream_generator=stream_from_function(),\n                headers=thread_result.get(\"headers\", {})\n            )\n\n        result = thread_result\n    else:\n        # Direct execution when loop is not running\n        result = loop.run_until_complete(coro)\n\n    # Process the final result\n    if isinstance(result, Result):\n        if 'debug' in self.id:\n            result.print()\n        if getattr(result.result, 'data_type', None) == \"stream\":\n            return result\n        return result.to_api_result().model_dump(mode='json')\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.run_bg_task","title":"<code>run_bg_task(task, *args, **kwargs)</code>","text":"<p>Runs a coroutine in the background without blocking the caller.</p> <p>This is the primary method for \"fire-and-forget\" async tasks. It schedules the coroutine to run on the application's main event loop.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Callable</code> <p>The coroutine function to run.</p> required <code>*args</code> <p>Arguments to pass to the coroutine function.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the coroutine function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Task]</code> <p>An asyncio.Task object representing the scheduled task, or None if</p> <code>Optional[Task]</code> <p>the task could not be scheduled.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def run_bg_task(self, task: Callable, *args, **kwargs) -&gt; Optional[asyncio.Task]:\n    \"\"\"\n    Runs a coroutine in the background without blocking the caller.\n\n    This is the primary method for \"fire-and-forget\" async tasks. It schedules\n    the coroutine to run on the application's main event loop.\n\n    Args:\n        task: The coroutine function to run.\n        *args: Arguments to pass to the coroutine function.\n        **kwargs: Keyword arguments to pass to the coroutine function.\n\n    Returns:\n        An asyncio.Task object representing the scheduled task, or None if\n        the task could not be scheduled.\n    \"\"\"\n    if not callable(task):\n        self.logger.warning(\"Task passed to run_bg_task is not callable!\")\n        return None\n\n    if not asyncio.iscoroutinefunction(task) and not asyncio.iscoroutine(task):\n        self.logger.warning(f\"Task '{getattr(task, '__name__', 'unknown')}' is not a coroutine. \"\n                            f\"Use run_bg_task_advanced for synchronous functions.\")\n        # Fallback to advanced runner for convenience\n        self.run_bg_task_advanced(task, *args, **kwargs)\n        return None\n\n    try:\n        loop = self.loop_gard()\n        if not loop.is_running():\n            # If the main loop isn't running, we can't create a task on it.\n            # This scenario is handled by run_bg_task_advanced.\n            self.logger.info(\"Main event loop not running. Delegating to advanced background runner.\")\n            return self.run_bg_task_advanced(task, *args, **kwargs)\n\n        # Create the coroutine if it's a function\n        coro = task(*args, **kwargs) if asyncio.iscoroutinefunction(task) else task\n\n        # Create a task on the running event loop\n        bg_task = loop.create_task(coro)\n\n        # Add a callback to log exceptions from the background task\n        def _log_exception(the_task: asyncio.Task):\n            if not the_task.cancelled() and the_task.exception():\n                self.logger.error(f\"Exception in background task '{the_task.get_name()}':\",\n                                  exc_info=the_task.exception())\n\n        bg_task.add_done_callback(_log_exception)\n        self.bg_tasks.append(bg_task)\n        return bg_task\n\n    except Exception as e:\n        self.logger.error(f\"Failed to schedule background task: {e}\", exc_info=True)\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.run_bg_task_advanced","title":"<code>run_bg_task_advanced(task, *args, **kwargs)</code>","text":"<p>Runs a task in a separate, dedicated background thread with its own event loop.</p> <p>This is ideal for: 1. Running an async task from a synchronous context. 2. Launching a long-running, independent operation that should not    interfere with the main application's event loop.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Callable</code> <p>The function to run (can be sync or async).</p> required <code>*args</code> <p>Arguments for the task.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments for the task.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Thread</code> <p>The threading.Thread object managing the background execution.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def run_bg_task_advanced(self, task: Callable, *args, **kwargs) -&gt; threading.Thread:\n    \"\"\"\n    Runs a task in a separate, dedicated background thread with its own event loop.\n\n    This is ideal for:\n    1. Running an async task from a synchronous context.\n    2. Launching a long-running, independent operation that should not\n       interfere with the main application's event loop.\n\n    Args:\n        task: The function to run (can be sync or async).\n        *args: Arguments for the task.\n        **kwargs: Keyword arguments for the task.\n\n    Returns:\n        The threading.Thread object managing the background execution.\n    \"\"\"\n    if not callable(task):\n        self.logger.warning(\"Task for run_bg_task_advanced is not callable!\")\n        return None\n\n    def thread_target():\n        # Each thread gets its own event loop.\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n\n        try:\n            # Prepare the coroutine we need to run\n            if asyncio.iscoroutinefunction(task):\n                coro = task(*args, **kwargs)\n            elif asyncio.iscoroutine(task):\n                # It's already a coroutine object\n                coro = task\n            else:\n                # It's a synchronous function, run it in an executor\n                # to avoid blocking the new event loop.\n                coro = loop.run_in_executor(None, lambda: task(*args, **kwargs))\n\n            # Run the coroutine to completion\n            result = loop.run_until_complete(coro)\n            self.logger.debug(f\"Advanced background task '{getattr(task, '__name__', 'unknown')}' completed.\")\n            if result is not None:\n                self.logger.debug(f\"Task result: {str(result)[:100]}\")\n\n        except Exception as e:\n            self.logger.error(f\"Error in advanced background task '{getattr(task, '__name__', 'unknown')}':\",\n                              exc_info=e)\n        finally:\n            # Cleanly shut down the event loop in this thread.\n            try:\n                all_tasks = asyncio.all_tasks(loop=loop)\n                if all_tasks:\n                    for t in all_tasks:\n                        t.cancel()\n                    loop.run_until_complete(asyncio.gather(*all_tasks, return_exceptions=True))\n            finally:\n                loop.close()\n                asyncio.set_event_loop(None)\n\n    # Create, start, and return the thread.\n    # It's a daemon thread so it won't prevent the main app from exiting.\n    t = threading.Thread(target=thread_target, daemon=True, name=f\"BGTask-{getattr(task, '__name__', 'unknown')}\")\n    self.bg_tasks.append(t)\n    t.start()\n    return t\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.show_console","title":"<code>show_console(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef show_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.tb","title":"<code>tb(name=None, mod_name='', helper='', version=None, test=True, restrict_in_virtual_mode=False, api=False, initial=False, exit_f=False, test_only=False, memory_cache=False, file_cache=False, request_as_kwarg=False, row=False, state=None, level=-1, memory_cache_max_size=100, memory_cache_ttl=300, samples=None, interface=None, pre_compute=None, post_compute=None, api_methods=None)</code>","text":"<p>A decorator for registering and configuring functions within a module.</p> <p>This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to register the function under. Defaults to the function's own name.</p> <code>None</code> <code>mod_name</code> <code>str</code> <p>The name of the module the function belongs to.</p> <code>''</code> <code>helper</code> <code>str</code> <p>A helper string providing additional information about the function.</p> <code>''</code> <code>version</code> <code>str or None</code> <p>The version of the function or module.</p> <code>None</code> <code>test</code> <code>bool</code> <p>Flag to indicate if the function is for testing purposes.</p> <code>True</code> <code>restrict_in_virtual_mode</code> <code>bool</code> <p>Flag to restrict the function in virtual mode.</p> <code>False</code> <code>api</code> <code>bool</code> <p>Flag to indicate if the function is part of an API.</p> <code>False</code> <code>initial</code> <code>bool</code> <p>Flag to indicate if the function should be executed at initialization.</p> <code>False</code> <code>exit_f</code> <code>bool</code> <p>Flag to indicate if the function should be executed at exit.</p> <code>False</code> <code>test_only</code> <code>bool</code> <p>Flag to indicate if the function should only be used for testing.</p> <code>False</code> <code>memory_cache</code> <code>bool</code> <p>Flag to enable memory caching for the function.</p> <code>False</code> <code>request_as_kwarg</code> <code>bool</code> <p>Flag to get request if the fuction is calld from api.</p> <code>False</code> <code>file_cache</code> <code>bool</code> <p>Flag to enable file caching for the function.</p> <code>False</code> <code>row</code> <code>bool</code> <p>rather to auto wrap the result in Result type default False means no row data aka result type</p> <code>False</code> <code>state</code> <code>bool or None</code> <p>Flag to indicate if the function maintains state.</p> <code>None</code> <code>level</code> <code>int</code> <p>The level of the function, used for prioritization or categorization.</p> <code>-1</code> <code>memory_cache_max_size</code> <code>int</code> <p>Maximum size of the memory cache.</p> <code>100</code> <code>memory_cache_ttl</code> <code>int</code> <p>Time-to-live for the memory cache entries.</p> <code>300</code> <code>samples</code> <code>list or dict or None</code> <p>Samples or examples of function usage.</p> <code>None</code> <code>interface</code> <code>str</code> <p>The interface type for the function.</p> <code>None</code> <code>pre_compute</code> <code>callable</code> <p>A function to be called before the main function.</p> <code>None</code> <code>post_compute</code> <code>callable</code> <p>A function to be called after the main function.</p> <code>None</code> <code>api_methods</code> <code>list[str]</code> <p>default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>function</code> <p>The decorated function with additional processing and registration capabilities.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def tb(self, name=None,\n       mod_name: str = \"\",\n       helper: str = \"\",\n       version: str | None = None,\n       test: bool = True,\n       restrict_in_virtual_mode: bool = False,\n       api: bool = False,\n       initial: bool = False,\n       exit_f: bool = False,\n       test_only: bool = False,\n       memory_cache: bool = False,\n       file_cache: bool = False,\n       request_as_kwarg: bool = False,\n       row: bool = False,\n       state: bool | None = None,\n       level: int = -1,\n       memory_cache_max_size: int = 100,\n       memory_cache_ttl: int = 300,\n       samples: list or dict or None = None,\n       interface: ToolBoxInterfaces or None or str = None,\n       pre_compute=None,\n       post_compute=None,\n       api_methods=None,\n       ):\n    \"\"\"\nA decorator for registering and configuring functions within a module.\n\nThis decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\nArgs:\n    name (str, optional): The name to register the function under. Defaults to the function's own name.\n    mod_name (str, optional): The name of the module the function belongs to.\n    helper (str, optional): A helper string providing additional information about the function.\n    version (str or None, optional): The version of the function or module.\n    test (bool, optional): Flag to indicate if the function is for testing purposes.\n    restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n    api (bool, optional): Flag to indicate if the function is part of an API.\n    initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n    exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n    test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n    memory_cache (bool, optional): Flag to enable memory caching for the function.\n    request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n    file_cache (bool, optional): Flag to enable file caching for the function.\n    row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n    state (bool or None, optional): Flag to indicate if the function maintains state.\n    level (int, optional): The level of the function, used for prioritization or categorization.\n    memory_cache_max_size (int, optional): Maximum size of the memory cache.\n    memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n    samples (list or dict or None, optional): Samples or examples of function usage.\n    interface (str, optional): The interface type for the function.\n    pre_compute (callable, optional): A function to be called before the main function.\n    post_compute (callable, optional): A function to be called after the main function.\n    api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\nReturns:\n    function: The decorated function with additional processing and registration capabilities.\n\"\"\"\n    if interface is None:\n        interface = \"tb\"\n    if test_only and 'test' not in self.id:\n        return lambda *args, **kwargs: args\n    return self._create_decorator(interface,\n                                  name,\n                                  mod_name,\n                                  level=level,\n                                  restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                  helper=helper,\n                                  api=api,\n                                  version=version,\n                                  initial=initial,\n                                  exit_f=exit_f,\n                                  test=test,\n                                  samples=samples,\n                                  state=state,\n                                  pre_compute=pre_compute,\n                                  post_compute=post_compute,\n                                  memory_cache=memory_cache,\n                                  file_cache=file_cache,\n                                  request_as_kwarg=request_as_kwarg,\n                                  row=row,\n                                  api_methods=api_methods,\n                                  memory_cache_max_size=memory_cache_max_size,\n                                  memory_cache_ttl=memory_cache_ttl)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.App.wait_for_bg_tasks","title":"<code>wait_for_bg_tasks(timeout=None)</code>","text":"<p>Wait for all background tasks to complete.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <p>Maximum time to wait (in seconds) for all tasks to complete.      None means wait indefinitely.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if all tasks completed, False if timeout occurred</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def wait_for_bg_tasks(self, timeout=None):\n    \"\"\"\n    Wait for all background tasks to complete.\n\n    Args:\n        timeout: Maximum time to wait (in seconds) for all tasks to complete.\n                 None means wait indefinitely.\n\n    Returns:\n        bool: True if all tasks completed, False if timeout occurred\n    \"\"\"\n    active_tasks = [t for t in self.bg_tasks if t.is_alive()]\n\n    for task in active_tasks:\n        task.join(timeout=timeout)\n        if task.is_alive():\n            return False\n\n    return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code","title":"<code>Code</code>","text":"Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>class Code:\n\n    @staticmethod\n    def DK():\n        return DEVICE_KEY\n\n    def decode_code(self, encrypted_data, key=None):\n\n        if not isinstance(encrypted_data, str):\n            encrypted_data = str(encrypted_data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.decrypt_symmetric(encrypted_data, key)\n\n    def encode_code(self, data, key=None):\n\n        if not isinstance(data, str):\n            data = str(data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.encrypt_symmetric(data, key)\n\n    @staticmethod\n    def generate_seed() -&gt; int:\n        \"\"\"\n        Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n        Returns:\n            int: Eine zuf\u00e4llige Zahl.\n        \"\"\"\n        return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n\n    @staticmethod\n    def one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n        \"\"\"\n        Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n        Args:\n            text (str): Der zu hashende Text.\n            salt (str): Der Salt-Wert.\n            pepper (str): Der Pepper-Wert.\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            str: Der resultierende Hash-Wert.\n        \"\"\"\n        return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n\n    @staticmethod\n    def generate_symmetric_key(as_str=True) -&gt; str or bytes:\n        \"\"\"\n        Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n        Returns:\n            str: Der generierte Schl\u00fcssel.\n        \"\"\"\n        key = Fernet.generate_key()\n        if as_str:\n            key = key.decode()\n        return key\n\n    @staticmethod\n    def encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        if isinstance(text, str):\n            text = text.encode()\n\n        try:\n            fernet = Fernet(key.encode())\n            return fernet.encrypt(text).decode()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n            return \"Error encrypt\"\n\n    @staticmethod\n    def decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n            to_str (bool): default true returns str if false returns bytes\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n\n        if isinstance(key, str):\n            key = key.encode()\n\n        #try:\n        fernet = Fernet(key)\n        text_b = fernet.decrypt(encrypted_text)\n        if not to_str:\n            return text_b\n        return text_b.decode()\n        # except Exception as e:\n        #     get_logger().error(f\"Error decrypt_symmetric {e}\")\n        #     if not mute:\n        #         raise e\n        #     if not to_str:\n        #         return f\"Error decoding\".encode()\n        #     return f\"Error decoding\"\n\n    @staticmethod\n    def generate_asymmetric_keys() -&gt; (str, str):\n        \"\"\"\n        Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n        Args:\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n        \"\"\"\n        private_key = rsa.generate_private_key(\n            public_exponent=65537,\n            key_size=2048 * 3,\n        )\n        public_key = private_key.public_key()\n\n        # Serialisieren der Schl\u00fcssel\n        pem_private_key = private_key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.NoEncryption()\n        ).decode()\n\n        pem_public_key = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        ).decode()\n\n        return pem_public_key, pem_private_key\n\n    @staticmethod\n    def save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n        \"\"\"\n        Speichert die generierten Schl\u00fcssel in separate Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n        Args:\n            public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n            private_key (str): Der private Schl\u00fcssel im PEM-Format\n            directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n        \"\"\"\n        # Erstelle das Verzeichnis, falls es nicht existiert\n        os.makedirs(directory, exist_ok=True)\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n        encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n        # Speichere den \u00f6ffentlichen Schl\u00fcssel\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        with open(public_key_path, \"w\") as f:\n            f.write(public_key)\n\n        # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n        with open(private_key_path, \"w\") as f:\n            f.write(encrypted_private_key)\n\n        print(\"Saved keys in \", public_key_path)\n\n    @staticmethod\n    def load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n        \"\"\"\n        L\u00e4dt die Schl\u00fcssel aus den Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n        Args:\n            directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n        Raises:\n            FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n        \"\"\"\n        # Pfade zu den Schl\u00fcsseldateien\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n\n        # Pr\u00fcfe ob die Dateien existieren\n        if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n            return \"\", \"\"\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Lade den \u00f6ffentlichen Schl\u00fcssel\n        with open(public_key_path) as f:\n            public_key = f.read()\n\n        # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n        with open(private_key_path) as f:\n            encrypted_private_key = f.read()\n            private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n        return public_key, private_key\n\n    @staticmethod\n    def encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        # try:\n        #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        #  except Exception as e:\n        #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            encrypted = public_key.encrypt(\n                text.encode(),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return encrypted.hex()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_asymmetric {e}\")\n            return \"Invalid\"\n\n    @staticmethod\n    def decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n        Args:\n            encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n            private_key_str (str): Der private Schl\u00fcssel als String.\n\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            decrypted = private_key.decrypt(\n                bytes.fromhex(encrypted_text_hex),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return decrypted.decode()\n\n        except Exception as e:\n            get_logger().error(f\"Error decrypt_asymmetric {e}\")\n        return \"Invalid\"\n\n    @staticmethod\n    def verify_signature(signature: str or bytes, message: str or bytes, public_key_str: str,\n                         salt_length=padding.PSS.MAX_LENGTH) -&gt; bool:\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                padding=padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                algorithm=hashes.SHA512()\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def verify_signature_web_algo(signature: str or bytes, message: str or bytes, public_key_str: str,\n                                  algo: int = -512) -&gt; bool:\n        signature_algorithm = ECDSA(hashes.SHA512())\n        if algo != -512:\n            signature_algorithm = ECDSA(hashes.SHA256())\n\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                # padding=padding.PSS(\n                #    mgf=padding.MGF1(hashes.SHA512()),\n                #    salt_length=padding.PSS.MAX_LENGTH\n                # ),\n                signature_algorithm=signature_algorithm\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def create_signature(message: str, private_key_str: str, salt_length=padding.PSS.MAX_LENGTH,\n                         row=False) -&gt; str or bytes:\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            signature = private_key.sign(\n                message.encode(),\n                padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                hashes.SHA512()\n            )\n            if row:\n                return signature\n            return base64.b64encode(signature).decode()\n        except Exception as e:\n            get_logger().error(f\"Error create_signature {e}\")\n            print(e)\n        return \"Invalid Key\"\n\n    @staticmethod\n    def pem_to_public_key(pem_key: str):\n        \"\"\"\n        Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n        Args:\n            pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n        Returns:\n            PublicKey: Das PublicKey-Objekt.\n        \"\"\"\n        public_key = serialization.load_pem_public_key(pem_key.encode())\n        return public_key\n\n    @staticmethod\n    def public_key_to_pem(public_key: RSAPublicKey):\n        \"\"\"\n        Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n        Args:\n            public_key (PublicKey): Das PublicKey-Objekt.\n\n        Returns:\n            str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n        \"\"\"\n        pem = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n        return pem.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.decrypt_asymmetric","title":"<code>decrypt_asymmetric(encrypted_text_hex, private_key_str)</code>  <code>staticmethod</code>","text":"<p>Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text_hex</code> <code>str</code> <p>Der verschl\u00fcsselte Text als Hex-String.</p> required <code>private_key_str</code> <code>str</code> <p>Der private Schl\u00fcssel als String.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n    Args:\n        encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n        private_key_str (str): Der private Schl\u00fcssel als String.\n\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n    try:\n        private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n        decrypted = private_key.decrypt(\n            bytes.fromhex(encrypted_text_hex),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return decrypted.decode()\n\n    except Exception as e:\n        get_logger().error(f\"Error decrypt_asymmetric {e}\")\n    return \"Invalid\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.decrypt_symmetric","title":"<code>decrypt_symmetric(encrypted_text, key, to_str=True, mute=False)</code>  <code>staticmethod</code>","text":"<p>Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text</code> <code>str</code> <p>Der zu entschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <code>to_str</code> <code>bool</code> <p>default true returns str if false returns bytes</p> <code>True</code> <p>Returns:     str: Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n        to_str (bool): default true returns str if false returns bytes\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n\n    if isinstance(key, str):\n        key = key.encode()\n\n    #try:\n    fernet = Fernet(key)\n    text_b = fernet.decrypt(encrypted_text)\n    if not to_str:\n        return text_b\n    return text_b.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.encrypt_asymmetric","title":"<code>encrypt_asymmetric(text, public_key_str)</code>  <code>staticmethod</code>","text":"<p>Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>public_key_str</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    # try:\n    #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n    #  except Exception as e:\n    #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n    try:\n        public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        encrypted = public_key.encrypt(\n            text.encode(),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return encrypted.hex()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        return \"Invalid\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.encrypt_symmetric","title":"<code>encrypt_symmetric(text, key)</code>  <code>staticmethod</code>","text":"<p>Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    if isinstance(text, str):\n        text = text.encode()\n\n    try:\n        fernet = Fernet(key.encode())\n        return fernet.encrypt(text).decode()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n        return \"Error encrypt\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.generate_asymmetric_keys","title":"<code>generate_asymmetric_keys()</code>  <code>staticmethod</code>","text":"<p>Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_asymmetric_keys() -&gt; (str, str):\n    \"\"\"\n    Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n    Args:\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n    \"\"\"\n    private_key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048 * 3,\n    )\n    public_key = private_key.public_key()\n\n    # Serialisieren der Schl\u00fcssel\n    pem_private_key = private_key.private_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PrivateFormat.PKCS8,\n        encryption_algorithm=serialization.NoEncryption()\n    ).decode()\n\n    pem_public_key = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    ).decode()\n\n    return pem_public_key, pem_private_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.generate_seed","title":"<code>generate_seed()</code>  <code>staticmethod</code>","text":"<p>Erzeugt eine zuf\u00e4llige Zahl als Seed.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Eine zuf\u00e4llige Zahl.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_seed() -&gt; int:\n    \"\"\"\n    Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n    Returns:\n        int: Eine zuf\u00e4llige Zahl.\n    \"\"\"\n    return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.generate_symmetric_key","title":"<code>generate_symmetric_key(as_str=True)</code>  <code>staticmethod</code>","text":"<p>Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str or bytes</code> <p>Der generierte Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_symmetric_key(as_str=True) -&gt; str or bytes:\n    \"\"\"\n    Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n    Returns:\n        str: Der generierte Schl\u00fcssel.\n    \"\"\"\n    key = Fernet.generate_key()\n    if as_str:\n        key = key.decode()\n    return key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.load_keys_from_files","title":"<code>load_keys_from_files(directory='keys')</code>  <code>staticmethod</code>","text":"<p>L\u00e4dt die Schl\u00fcssel aus den Dateien. Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen</p> <code>'keys'</code> <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n    \"\"\"\n    L\u00e4dt die Schl\u00fcssel aus den Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n    Args:\n        directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n    Raises:\n        FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n    \"\"\"\n    # Pfade zu den Schl\u00fcsseldateien\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n\n    # Pr\u00fcfe ob die Dateien existieren\n    if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n        return \"\", \"\"\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Lade den \u00f6ffentlichen Schl\u00fcssel\n    with open(public_key_path) as f:\n        public_key = f.read()\n\n    # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n    with open(private_key_path) as f:\n        encrypted_private_key = f.read()\n        private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n    return public_key, private_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.one_way_hash","title":"<code>one_way_hash(text, salt='', pepper='')</code>  <code>staticmethod</code>","text":"<p>Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu hashende Text.</p> required <code>salt</code> <code>str</code> <p>Der Salt-Wert.</p> <code>''</code> <code>pepper</code> <code>str</code> <p>Der Pepper-Wert.</p> <code>''</code> <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der resultierende Hash-Wert.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n    \"\"\"\n    Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n    Args:\n        text (str): Der zu hashende Text.\n        salt (str): Der Salt-Wert.\n        pepper (str): Der Pepper-Wert.\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        str: Der resultierende Hash-Wert.\n    \"\"\"\n    return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.pem_to_public_key","title":"<code>pem_to_public_key(pem_key)</code>  <code>staticmethod</code>","text":"<p>Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.</p> <p>Parameters:</p> Name Type Description Default <code>pem_key</code> <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef pem_to_public_key(pem_key: str):\n    \"\"\"\n    Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n    Args:\n        pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n    Returns:\n        PublicKey: Das PublicKey-Objekt.\n    \"\"\"\n    public_key = serialization.load_pem_public_key(pem_key.encode())\n    return public_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.public_key_to_pem","title":"<code>public_key_to_pem(public_key)</code>  <code>staticmethod</code>","text":"<p>Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef public_key_to_pem(public_key: RSAPublicKey):\n    \"\"\"\n    Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n    Args:\n        public_key (PublicKey): Das PublicKey-Objekt.\n\n    Returns:\n        str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n    \"\"\"\n    pem = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    )\n    return pem.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Code.save_keys_to_files","title":"<code>save_keys_to_files(public_key, private_key, directory='keys')</code>  <code>staticmethod</code>","text":"<p>Speichert die generierten Schl\u00fcssel in separate Dateien. Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel im PEM-Format</p> required <code>private_key</code> <code>str</code> <p>Der private Schl\u00fcssel im PEM-Format</p> required <code>directory</code> <code>str</code> <p>Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen</p> <code>'keys'</code> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n    \"\"\"\n    Speichert die generierten Schl\u00fcssel in separate Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n    Args:\n        public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n        private_key (str): Der private Schl\u00fcssel im PEM-Format\n        directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n    \"\"\"\n    # Erstelle das Verzeichnis, falls es nicht existiert\n    os.makedirs(directory, exist_ok=True)\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n    encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n    # Speichere den \u00f6ffentlichen Schl\u00fcssel\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    with open(public_key_path, \"w\") as f:\n        f.write(public_key)\n\n    # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n    with open(private_key_path, \"w\") as f:\n        f.write(encrypted_private_key)\n\n    print(\"Saved keys in \", public_key_path)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.MainTool","title":"<code>MainTool</code>","text":"Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>class MainTool:\n    toolID: str = \"\"\n    # app = None\n    interface = None\n    spec = \"app\"\n    name = \"\"\n    color = \"Bold\"\n    stuf = False\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.__storedargs = args, kwargs\n        self.tools = kwargs.get(\"tool\", {})\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n        if \"on_exit\" in kwargs and isinstance(kwargs.get(\"on_exit\"), Callable):\n            self.on_exit =self.app.tb(\n                mod_name=self.name,\n                name=kwargs.get(\"on_exit\").__name__,\n                version=self.version if hasattr(self, 'version') else \"0.0.0\",\n            )(kwargs.get(\"on_exit\"))\n        self.async_initialized = False\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    pass\n                else:\n                    self.todo()\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\"Error loading mod {self.name} {e}\")\n                if self.app.debug:\n                    import traceback\n                    traceback.print_exc()\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n\n    async def __ainit__(self, *args, **kwargs):\n        self.version = kwargs[\"v\"]\n        self.tools = kwargs.get(\"tool\", {})\n        self.name = kwargs[\"name\"]\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", None))\n        if not hasattr(self, 'config'):\n            self.config = {}\n        self.user = None\n        self.description = \"A toolbox mod\" if kwargs.get(\"description\") is None else kwargs.get(\"description\")\n        if MainTool.interface is None:\n            MainTool.interface = self.app.interface_type\n        # Result.default(self.app.interface)\n\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    await self.todo()\n                else:\n                    pass\n                await asyncio.sleep(0.1)\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\"Error loading mod {self.name} {e}\")\n                if self.app.debug:\n                    import traceback\n                    traceback.print_exc()\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n        self.app.print(f\"TOOL : {self.spec}.{self.name} online\")\n\n\n\n    @property\n    def app(self):\n        return get_app(\n            from_=f\"{self.spec}.{self.name}|{self.toolID if self.toolID else '*' + MainTool.toolID} {self.interface if self.interface else MainTool.interface}\")\n\n    @app.setter\n    def app(self, v):\n        raise PermissionError(f\"You cannot set the App Instance! {v=}\")\n\n    @staticmethod\n    def return_result(error: ToolBoxError = ToolBoxError.none,\n                      exec_code: int = 0,\n                      help_text: str = \"\",\n                      data_info=None,\n                      data=None,\n                      data_to=None):\n\n        if data_to is None:\n            data_to = MainTool.interface if MainTool.interface is not None else ToolBoxInterfaces.cli\n\n        if data is None:\n            data = {}\n\n        if data_info is None:\n            data_info = {}\n\n        return Result(\n            error,\n            ToolBoxResult(data_info=data_info, data=data, data_to=data_to),\n            ToolBoxInfo(exec_code=exec_code, help_text=help_text)\n        )\n\n    def print(self, message, end=\"\\n\", **kwargs):\n        if self.stuf:\n            return\n\n        self.app.print(Style.style_dic[self.color] + self.name + Style.style_dic[\"END\"] + \":\", message, end=end,\n                       **kwargs)\n\n    def add_str_to_config(self, command):\n        if len(command) != 2:\n            self.logger.error('Invalid command must be key value')\n            return False\n        self.config[command[0]] = command[1]\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n\n    def get_version(self) -&gt; str:\n        \"\"\"\"Returns the version\"\"\"\n        return self.version\n\n    async def get_user(self, username: str) -&gt; Result:\n        return await self.app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME, username=username, get_results=True)\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.MainTool.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.__storedargs = args, kwargs\n    self.tools = kwargs.get(\"tool\", {})\n    self.logger = kwargs.get(\"logs\", get_logger())\n    self.color = kwargs.get(\"color\", \"WHITE\")\n    self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n    if \"on_exit\" in kwargs and isinstance(kwargs.get(\"on_exit\"), Callable):\n        self.on_exit =self.app.tb(\n            mod_name=self.name,\n            name=kwargs.get(\"on_exit\").__name__,\n            version=self.version if hasattr(self, 'version') else \"0.0.0\",\n        )(kwargs.get(\"on_exit\"))\n    self.async_initialized = False\n    if self.todo:\n        try:\n            if inspect.iscoroutinefunction(self.todo):\n                pass\n            else:\n                self.todo()\n            get_logger().info(f\"{self.name} on load suspended\")\n        except Exception as e:\n            get_logger().error(f\"Error loading mod {self.name} {e}\")\n            if self.app.debug:\n                import traceback\n                traceback.print_exc()\n    else:\n        get_logger().info(f\"{self.name} no load require\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.MainTool.__initobj","title":"<code>__initobj()</code>  <code>async</code>","text":"<p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.MainTool.get_version","title":"<code>get_version()</code>","text":"<p>\"Returns the version</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\"Returns the version\"\"\"\n    return self.version\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.MainTool.webInstall","title":"<code>webInstall(user_instance, construct_render)</code>","text":"<p>\"Returns a web installer for the given user instance and construct render template</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def webInstall(self, user_instance, construct_render) -&gt; str:\n    \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result","title":"<code>Result</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class Result:\n    _task = None\n    def __init__(self,\n                 error: ToolBoxError,\n                 result: ToolBoxResult,\n                 info: ToolBoxInfo,\n                 origin: Any | None = None,\n                 ):\n        self.error: ToolBoxError = error\n        self.result: ToolBoxResult = result\n        self.info: ToolBoxInfo = info\n        self.origin = origin\n\n    def as_result(self):\n        return self\n\n    def as_dict(self):\n        return {\n            \"error\":self.error.value if isinstance(self.error, Enum) else self.error,\n        \"result\" : {\n            \"data_to\":self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n            \"data_info\":self.result.data_info,\n            \"data\":self.result.data,\n            \"data_type\":self.result.data_type\n        } if self.result else None,\n        \"info\" : {\n            \"exec_code\" : self.info.exec_code,  # exec_code umwandel in http resposn codes\n        \"help_text\" : self.info.help_text\n        } if self.info else None,\n        \"origin\" : self.origin\n        }\n\n    def set_origin(self, origin):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = origin\n        return self\n\n    def set_dir_origin(self, name, extras=\"assets/\"):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = f\"mods/{name}/{extras}\"\n        return self\n\n    def is_error(self):\n        if _test_is_result(self.result.data):\n            return self.result.data.is_error()\n        if self.error == ToolBoxError.none:\n            return False\n        if self.info.exec_code == 0:\n            return False\n        if self.info.exec_code == 200:\n            return False\n        return True\n\n    def is_ok(self):\n        return not self.is_error()\n\n    def is_data(self):\n        return self.result.data is not None\n\n    def to_api_result(self):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=self.error.value if isinstance(self.error, Enum) else self.error,\n            result=ToolBoxResultBM(\n                data_to=self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n                data_info=self.result.data_info,\n                data=self.result.data,\n                data_type=self.result.data_type\n            ) if self.result else None,\n            info=ToolBoxInfoBM(\n                exec_code=self.info.exec_code,  # exec_code umwandel in http resposn codes\n                help_text=self.info.help_text\n            ) if self.info else None,\n            origin=self.origin\n        )\n\n    def task(self, task):\n        self._task = task\n        return self\n\n    @staticmethod\n    def result_from_dict(error: str, result: dict, info: dict, origin: list or None or str):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=error if isinstance(error, Enum) else error,\n            result=ToolBoxResultBM(\n                data_to=result.get('data_to') if isinstance(result.get('data_to'), Enum) else result.get('data_to'),\n                data_info=result.get('data_info', '404'),\n                data=result.get('data'),\n                data_type=result.get('data_type', '404'),\n            ) if result else ToolBoxResultBM(\n                data_to=ToolBoxInterfaces.cli.value,\n                data_info='',\n                data='404',\n                data_type='404',\n            ),\n            info=ToolBoxInfoBM(\n                exec_code=info.get('exec_code', 404),\n                help_text=info.get('help_text', '404')\n            ) if info else ToolBoxInfoBM(\n                exec_code=404,\n                help_text='404'\n            ),\n            origin=origin\n        ).as_result()\n\n    @classmethod\n    def stream(cls,\n               stream_generator: Any,  # Renamed from source for clarity\n               content_type: str = \"text/event-stream\",  # Default to SSE\n               headers: Union[dict, None] = None,\n               info: str = \"OK\",\n               interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n               cleanup_func: Union[\n                   Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None):\n        \"\"\"\n        Create a streaming response Result. Handles SSE and other stream types.\n\n        Args:\n            stream_generator: Any stream source (async generator, sync generator, iterable, or single item).\n            content_type: Content-Type header (default: text/event-stream for SSE).\n            headers: Additional HTTP headers for the response.\n            info: Help text for the result.\n            interface: Interface to send data to.\n            cleanup_func: Optional function for cleanup.\n\n        Returns:\n            A Result object configured for streaming.\n        \"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        final_generator: AsyncGenerator[str, None]\n\n        if content_type == \"text/event-stream\":\n            # For SSE, always use SSEGenerator.create_sse_stream to wrap the source.\n            # SSEGenerator.create_sse_stream handles various types of stream_generator internally.\n            final_generator = SSEGenerator.create_sse_stream(source=stream_generator, cleanup_func=cleanup_func)\n\n            # Standard SSE headers for the HTTP response itself\n            # These will be stored in the Result object. Rust side decides how to use them.\n            standard_sse_headers = {\n                \"Cache-Control\": \"no-cache\",  # SSE specific\n                \"Connection\": \"keep-alive\",  # SSE specific\n                \"X-Accel-Buffering\": \"no\",  # Useful for proxies with SSE\n                # Content-Type is implicitly text/event-stream, will be in streaming_data below\n            }\n            all_response_headers = standard_sse_headers.copy()\n            if headers:\n                all_response_headers.update(headers)\n        else:\n            # For non-SSE streams.\n            # If stream_generator is sync, wrap it to be async.\n            # If already async or single item, it will be handled.\n            # Rust's stream_generator in ToolboxClient seems to handle both sync/async Python generators.\n            # For consistency with how SSEGenerator does it, we can wrap sync ones.\n            if inspect.isgenerator(stream_generator) or \\\n                (not isinstance(stream_generator, str) and hasattr(stream_generator, '__iter__')):\n                final_generator = SSEGenerator.wrap_sync_generator(stream_generator)  # Simple async wrapper\n            elif inspect.isasyncgen(stream_generator):\n                final_generator = stream_generator\n            else:  # Single item or string\n                async def _single_item_gen():\n                    yield stream_generator\n\n                final_generator = _single_item_gen()\n            all_response_headers = headers if headers else {}\n\n        # Prepare streaming data to be stored in the Result object\n        streaming_data = {\n            \"type\": \"stream\",  # Indicator for Rust side\n            \"generator\": final_generator,\n            \"content_type\": content_type,  # Let Rust know the intended content type\n            \"headers\": all_response_headers  # Intended HTTP headers for the overall response\n        }\n\n        result_payload = ToolBoxResult(\n            data_to=interface,\n            data=streaming_data,\n            data_info=\"Streaming response\" if content_type != \"text/event-stream\" else \"SSE Event Stream\",\n            data_type=\"stream\"  # Generic type for Rust to identify it needs to stream from 'generator'\n        )\n\n        return cls(error=error, info=info_obj, result=result_payload)\n\n    @classmethod\n    def sse(cls,\n            stream_generator: Any,\n            info: str = \"OK\",\n            interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n            cleanup_func: Union[\n                Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None,\n            # http_headers: Optional[dict] = None # If we want to allow overriding default SSE HTTP headers\n            ):\n        \"\"\"\n        Create an Server-Sent Events (SSE) streaming response Result.\n\n        Args:\n            stream_generator: A source yielding individual data items. This can be an\n                              async generator, sync generator, iterable, or a single item.\n                              Each item will be formatted as an SSE event.\n            info: Optional help text for the Result.\n            interface: Optional ToolBoxInterface to target.\n            cleanup_func: Optional cleanup function to run when the stream ends or is cancelled.\n            #http_headers: Optional dictionary of custom HTTP headers for the SSE response.\n\n        Returns:\n            A Result object configured for SSE streaming.\n        \"\"\"\n        # Result.stream will handle calling SSEGenerator.create_sse_stream\n        # and setting appropriate default headers for SSE when content_type is \"text/event-stream\".\n        return cls.stream(\n            stream_generator=stream_generator,\n            content_type=\"text/event-stream\",\n            # headers=http_headers, # Pass if we add http_headers param\n            info=info,\n            interface=interface,\n            cleanup_func=cleanup_func\n        )\n\n    @classmethod\n    def default(cls, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=-1, help_text=\"\")\n        result = ToolBoxResult(data_to=interface)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None):\n        \"\"\"Create a JSON response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code or exec_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=data,\n            data_info=\"JSON response\",\n            data_type=\"json\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n        \"\"\"Create a text response Result with specific content type.\"\"\"\n        if headers is not None:\n            return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=text_data,\n            data_info=\"Text response\",\n            data_type=content_type\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n               interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a binary data response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        # Create a dictionary with binary data and metadata\n        binary_data = {\n            \"data\": data,\n            \"content_type\": content_type,\n            \"filename\": download_name\n        }\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=binary_data,\n            data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n            data_type=\"binary\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def file(cls, data, filename, content_type=None, info=\"OK\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a file download response Result.\n\n        Args:\n            data: File data as bytes or base64 string\n            filename: Name of the file for download\n            content_type: MIME type of the file (auto-detected if None)\n            info: Response info text\n            interface: Target interface\n\n        Returns:\n            Result object configured for file download\n        \"\"\"\n        import base64\n        import mimetypes\n\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=200, help_text=info)\n\n        # Auto-detect content type if not provided\n        if content_type is None:\n            content_type, _ = mimetypes.guess_type(filename)\n            if content_type is None:\n                content_type = \"application/octet-stream\"\n\n        # Ensure data is base64 encoded string (as expected by Rust server)\n        if isinstance(data, bytes):\n            base64_data = base64.b64encode(data).decode('utf-8')\n        elif isinstance(data, str):\n            # Assume it's already base64 encoded\n            base64_data = data\n        else:\n            raise ValueError(\"File data must be bytes or base64 string\")\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=base64_data,  # Rust expects base64 string for \"file\" type\n            data_info=f\"File download: {filename}\",\n            data_type=\"file\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a redirect response.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=url,\n            data_info=\"Redirect response\",\n            data_type=\"redirect\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def ok(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def html(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.remote, data_type=\"html\",status=200, headers=None, row=False):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=status, help_text=info)\n        from ...utils.system.getting_and_closing_app import get_app\n\n        if not row and not '\"&lt;div class=\"main-content\"\"' in data:\n            data = f'&lt;div class=\"main-content frosted-glass\"&gt;{data}&lt;div&gt;'\n        if not row and not get_app().web_context() in data:\n            data = get_app().web_context() + data\n\n        if isinstance(headers, dict):\n            result = ToolBoxResult(data_to=interface, data={'html':data,'headers':headers}, data_info=data_info,\n                                   data_type=\"special_html\")\n        else:\n            result = ToolBoxResult(data_to=interface, data=data, data_info=data_info,\n                                   data_type=data_type if data_type is not None else type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def future(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.future):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=\"future\")\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def custom_error(cls, data=None, data_info=\"\", info=\"\", exec_code=-1, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def error(cls, data=None, data_info=\"\", info=\"\", exec_code=450, interface=ToolBoxInterfaces.remote):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_user_error(cls, info=\"\", exec_code=-3, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.input_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_internal_error(cls, info=\"\", exec_code=-2, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.internal_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    def print(self, show=True, show_data=True, prifix=\"\"):\n        data = '\\n' + f\"{((prifix + 'Data: ' + str(self.result.data) if self.result.data is not None else 'NO Data') if not isinstance(self.result.data, Result) else self.result.data.print(show=False, show_data=show_data, prifix=prifix + '-')) if show_data else 'Data: private'}\"\n        origin = '\\n' + f\"{prifix + 'Origin: ' + str(self.origin) if self.origin is not None else 'NO Origin'}\"\n        text = (f\"Function Exec code: {self.info.exec_code}\"\n                f\"\\n{prifix}Info's:\"\n                f\" {self.info.help_text} {'&lt;|&gt; ' + str(self.result.data_info) if self.result.data_info is not None else ''}\"\n                f\"{origin}{data if not data.endswith('NO Data') else ''}\")\n        if not show:\n            return text\n        print(\"\\n======== Result ========\\n\" + text + \"\\n------- EndOfD -------\")\n        return self\n\n    def log(self, show_data=True, prifix=\"\"):\n        from toolboxv2 import get_logger\n        get_logger().debug(self.print(show=False, show_data=show_data, prifix=prifix).replace(\"\\n\", \" - \"))\n        return self\n\n    def __str__(self):\n        return self.print(show=False, show_data=True)\n\n    def get(self, key=None, default=None):\n        data = self.result.data\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    async def aget(self, key=None, default=None):\n        if asyncio.isfuture(self.result.data) or asyncio.iscoroutine(self.result.data) or (\n            isinstance(self.result.data_to, Enum) and self.result.data_to.name == ToolBoxInterfaces.future.name):\n            data = await self.result.data\n        else:\n            data = self.get(key=None, default=None)\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    def lazy_return(self, _=0, data=None, **kwargs):\n        flags = ['raise', 'logg', 'user', 'intern']\n        flag = flags[_] if isinstance(_, int) else _\n        if self.info.exec_code == 0:\n            return self if data is None else data if _test_is_result(data) else self.ok(data=data, **kwargs)\n        if flag == 'raise':\n            raise ValueError(self.print(show=False))\n        if flag == 'logg':\n            from .. import get_logger\n            get_logger().error(self.print(show=False))\n\n        if flag == 'user':\n            return self if data is None else data if _test_is_result(data) else self.default_user_error(data=data,\n                                                                                                        **kwargs)\n        if flag == 'intern':\n            return self if data is None else data if _test_is_result(data) else self.default_internal_error(data=data,\n                                                                                                            **kwargs)\n\n        return self if data is None else data if _test_is_result(data) else self.custom_error(data=data, **kwargs)\n\n    @property\n    def bg_task(self):\n        return self._task\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.binary","title":"<code>binary(data, content_type='application/octet-stream', download_name=None, info='OK', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a binary data response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n           interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a binary data response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    # Create a dictionary with binary data and metadata\n    binary_data = {\n        \"data\": data,\n        \"content_type\": content_type,\n        \"filename\": download_name\n    }\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=binary_data,\n        data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n        data_type=\"binary\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.file","title":"<code>file(data, filename, content_type=None, info='OK', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a file download response Result.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>File data as bytes or base64 string</p> required <code>filename</code> <p>Name of the file for download</p> required <code>content_type</code> <p>MIME type of the file (auto-detected if None)</p> <code>None</code> <code>info</code> <p>Response info text</p> <code>'OK'</code> <code>interface</code> <p>Target interface</p> <code>remote</code> <p>Returns:</p> Type Description <p>Result object configured for file download</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef file(cls, data, filename, content_type=None, info=\"OK\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a file download response Result.\n\n    Args:\n        data: File data as bytes or base64 string\n        filename: Name of the file for download\n        content_type: MIME type of the file (auto-detected if None)\n        info: Response info text\n        interface: Target interface\n\n    Returns:\n        Result object configured for file download\n    \"\"\"\n    import base64\n    import mimetypes\n\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=200, help_text=info)\n\n    # Auto-detect content type if not provided\n    if content_type is None:\n        content_type, _ = mimetypes.guess_type(filename)\n        if content_type is None:\n            content_type = \"application/octet-stream\"\n\n    # Ensure data is base64 encoded string (as expected by Rust server)\n    if isinstance(data, bytes):\n        base64_data = base64.b64encode(data).decode('utf-8')\n    elif isinstance(data, str):\n        # Assume it's already base64 encoded\n        base64_data = data\n    else:\n        raise ValueError(\"File data must be bytes or base64 string\")\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=base64_data,  # Rust expects base64 string for \"file\" type\n        data_info=f\"File download: {filename}\",\n        data_type=\"file\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.json","title":"<code>json(data, info='OK', interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None)</code>  <code>classmethod</code>","text":"<p>Create a JSON response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None):\n    \"\"\"Create a JSON response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code or exec_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=data,\n        data_info=\"JSON response\",\n        data_type=\"json\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.redirect","title":"<code>redirect(url, status_code=302, info='Redirect', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a redirect response.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a redirect response.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=url,\n        data_info=\"Redirect response\",\n        data_type=\"redirect\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.sse","title":"<code>sse(stream_generator, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code>  <code>classmethod</code>","text":"<p>Create an Server-Sent Events (SSE) streaming response Result.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <code>Any</code> <p>A source yielding individual data items. This can be an               async generator, sync generator, iterable, or a single item.               Each item will be formatted as an SSE event.</p> required <code>info</code> <code>str</code> <p>Optional help text for the Result.</p> <code>'OK'</code> <code>interface</code> <code>ToolBoxInterfaces</code> <p>Optional ToolBoxInterface to target.</p> <code>remote</code> <code>cleanup_func</code> <code>Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None]</code> <p>Optional cleanup function to run when the stream ends or is cancelled.</p> <code>None</code> <code>#http_headers</code> <p>Optional dictionary of custom HTTP headers for the SSE response.</p> required <p>Returns:</p> Type Description <p>A Result object configured for SSE streaming.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef sse(cls,\n        stream_generator: Any,\n        info: str = \"OK\",\n        interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n        cleanup_func: Union[\n            Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None,\n        # http_headers: Optional[dict] = None # If we want to allow overriding default SSE HTTP headers\n        ):\n    \"\"\"\n    Create an Server-Sent Events (SSE) streaming response Result.\n\n    Args:\n        stream_generator: A source yielding individual data items. This can be an\n                          async generator, sync generator, iterable, or a single item.\n                          Each item will be formatted as an SSE event.\n        info: Optional help text for the Result.\n        interface: Optional ToolBoxInterface to target.\n        cleanup_func: Optional cleanup function to run when the stream ends or is cancelled.\n        #http_headers: Optional dictionary of custom HTTP headers for the SSE response.\n\n    Returns:\n        A Result object configured for SSE streaming.\n    \"\"\"\n    # Result.stream will handle calling SSEGenerator.create_sse_stream\n    # and setting appropriate default headers for SSE when content_type is \"text/event-stream\".\n    return cls.stream(\n        stream_generator=stream_generator,\n        content_type=\"text/event-stream\",\n        # headers=http_headers, # Pass if we add http_headers param\n        info=info,\n        interface=interface,\n        cleanup_func=cleanup_func\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.stream","title":"<code>stream(stream_generator, content_type='text/event-stream', headers=None, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code>  <code>classmethod</code>","text":"<p>Create a streaming response Result. Handles SSE and other stream types.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <code>Any</code> <p>Any stream source (async generator, sync generator, iterable, or single item).</p> required <code>content_type</code> <code>str</code> <p>Content-Type header (default: text/event-stream for SSE).</p> <code>'text/event-stream'</code> <code>headers</code> <code>Union[dict, None]</code> <p>Additional HTTP headers for the response.</p> <code>None</code> <code>info</code> <code>str</code> <p>Help text for the result.</p> <code>'OK'</code> <code>interface</code> <code>ToolBoxInterfaces</code> <p>Interface to send data to.</p> <code>remote</code> <code>cleanup_func</code> <code>Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None]</code> <p>Optional function for cleanup.</p> <code>None</code> <p>Returns:</p> Type Description <p>A Result object configured for streaming.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef stream(cls,\n           stream_generator: Any,  # Renamed from source for clarity\n           content_type: str = \"text/event-stream\",  # Default to SSE\n           headers: Union[dict, None] = None,\n           info: str = \"OK\",\n           interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n           cleanup_func: Union[\n               Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None):\n    \"\"\"\n    Create a streaming response Result. Handles SSE and other stream types.\n\n    Args:\n        stream_generator: Any stream source (async generator, sync generator, iterable, or single item).\n        content_type: Content-Type header (default: text/event-stream for SSE).\n        headers: Additional HTTP headers for the response.\n        info: Help text for the result.\n        interface: Interface to send data to.\n        cleanup_func: Optional function for cleanup.\n\n    Returns:\n        A Result object configured for streaming.\n    \"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    final_generator: AsyncGenerator[str, None]\n\n    if content_type == \"text/event-stream\":\n        # For SSE, always use SSEGenerator.create_sse_stream to wrap the source.\n        # SSEGenerator.create_sse_stream handles various types of stream_generator internally.\n        final_generator = SSEGenerator.create_sse_stream(source=stream_generator, cleanup_func=cleanup_func)\n\n        # Standard SSE headers for the HTTP response itself\n        # These will be stored in the Result object. Rust side decides how to use them.\n        standard_sse_headers = {\n            \"Cache-Control\": \"no-cache\",  # SSE specific\n            \"Connection\": \"keep-alive\",  # SSE specific\n            \"X-Accel-Buffering\": \"no\",  # Useful for proxies with SSE\n            # Content-Type is implicitly text/event-stream, will be in streaming_data below\n        }\n        all_response_headers = standard_sse_headers.copy()\n        if headers:\n            all_response_headers.update(headers)\n    else:\n        # For non-SSE streams.\n        # If stream_generator is sync, wrap it to be async.\n        # If already async or single item, it will be handled.\n        # Rust's stream_generator in ToolboxClient seems to handle both sync/async Python generators.\n        # For consistency with how SSEGenerator does it, we can wrap sync ones.\n        if inspect.isgenerator(stream_generator) or \\\n            (not isinstance(stream_generator, str) and hasattr(stream_generator, '__iter__')):\n            final_generator = SSEGenerator.wrap_sync_generator(stream_generator)  # Simple async wrapper\n        elif inspect.isasyncgen(stream_generator):\n            final_generator = stream_generator\n        else:  # Single item or string\n            async def _single_item_gen():\n                yield stream_generator\n\n            final_generator = _single_item_gen()\n        all_response_headers = headers if headers else {}\n\n    # Prepare streaming data to be stored in the Result object\n    streaming_data = {\n        \"type\": \"stream\",  # Indicator for Rust side\n        \"generator\": final_generator,\n        \"content_type\": content_type,  # Let Rust know the intended content type\n        \"headers\": all_response_headers  # Intended HTTP headers for the overall response\n    }\n\n    result_payload = ToolBoxResult(\n        data_to=interface,\n        data=streaming_data,\n        data_info=\"Streaming response\" if content_type != \"text/event-stream\" else \"SSE Event Stream\",\n        data_type=\"stream\"  # Generic type for Rust to identify it needs to stream from 'generator'\n    )\n\n    return cls(error=error, info=info_obj, result=result_payload)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Result.text","title":"<code>text(text_data, content_type='text/plain', exec_code=None, status=200, info='OK', interface=ToolBoxInterfaces.remote, headers=None)</code>  <code>classmethod</code>","text":"<p>Create a text response Result with specific content type.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n    \"\"\"Create a text response Result with specific content type.\"\"\"\n    if headers is not None:\n        return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=text_data,\n        data_info=\"Text response\",\n        data_type=content_type\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Singleton","title":"<code>Singleton</code>","text":"<p>Singleton metaclass for ensuring only one instance of a class.</p> Source code in <code>toolboxv2/utils/singelton_class.py</code> <pre><code>class Singleton(type):\n    \"\"\"\n    Singleton metaclass for ensuring only one instance of a class.\n    \"\"\"\n\n    _instances = {}\n    _kwargs = {}\n    _args = {}\n\n    def __call__(cls, *args, **kwargs):\n        if cls not in cls._instances:\n            cls._instances[cls] = super().__call__(*args, **kwargs)\n            cls._args[cls] = args\n            cls._kwargs[cls] = kwargs\n        return cls._instances[cls]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Spinner","title":"<code>Spinner</code>","text":"<p>Enhanced Spinner with tqdm-like line rendering.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>class Spinner:\n    \"\"\"\n    Enhanced Spinner with tqdm-like line rendering.\n    \"\"\"\n    SYMBOL_SETS = {\n        \"c\": [\"\u25d0\", \"\u25d3\", \"\u25d1\", \"\u25d2\"],\n        \"b\": [\"\u2581\", \"\u2583\", \"\u2584\", \"\u2585\", \"\u2586\", \"\u2587\", \"\u2588\", \"\u2587\", \"\u2586\", \"\u2585\", \"\u2584\", \"\u2583\"],\n        \"d\": [\"\u28fe\", \"\u28fd\", \"\u28fb\", \"\u28bf\", \"\u287f\", \"\u28df\", \"\u28ef\", \"\u28f7\"],\n        \"w\": [\"\ud83c\udf0d\", \"\ud83c\udf0e\", \"\ud83c\udf0f\"],\n        \"s\": [\"\ud83c\udf00   \", \" \ud83c\udf00  \", \"  \ud83c\udf00 \", \"   \ud83c\udf00\", \"  \ud83c\udf00 \", \" \ud83c\udf00  \"],\n        \"+\": [\"+\", \"x\"],\n        \"t\": [\"\u2736\", \"\u2738\", \"\u2739\", \"\u273a\", \"\u2739\", \"\u2737\"]\n    }\n\n    def __init__(\n        self,\n        message: str = \"Loading...\",\n        delay: float = 0.1,\n        symbols=None,\n        count_down: bool = False,\n        time_in_s: float = 0\n    ):\n        \"\"\"Initialize spinner with flexible configuration.\"\"\"\n        # Resolve symbol set.\n        if isinstance(symbols, str):\n            symbols = self.SYMBOL_SETS.get(symbols, None)\n\n        # Default symbols if not provided.\n        if symbols is None:\n            symbols = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n\n        # Test mode symbol set.\n        if 'unittest' in sys.argv[0]:\n            symbols = ['#', '=', '-']\n\n        self.spinner = itertools.cycle(symbols)\n        self.delay = delay\n        self.message = message\n        self.running = False\n        self.spinner_thread = None\n        self.max_t = time_in_s\n        self.contd = count_down\n\n        # Rendering management.\n        self._is_primary = False\n        self._start_time = 0\n\n        # Central manager.\n        self.manager = SpinnerManager()\n\n    def _generate_render_line(self):\n        \"\"\"Generate the primary render line.\"\"\"\n        current_time = time.time()\n        if self.contd:\n            remaining = max(0, self.max_t - (current_time - self._start_time))\n            time_display = f\"{remaining:.2f}\"\n        else:\n            time_display = f\"{current_time - self._start_time:.2f}\"\n\n        symbol = next(self.spinner)\n        return f\"{symbol} {self.message} | {time_display}\"\n\n    def _generate_secondary_info(self):\n        \"\"\"Generate secondary spinner info for additional spinners.\"\"\"\n        return f\"{self.message}\"\n\n    def __enter__(self):\n        \"\"\"Start the spinner.\"\"\"\n        self.running = True\n        self._start_time = time.time()\n        self.manager.register_spinner(self)\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        \"\"\"Stop the spinner.\"\"\"\n        self.running = False\n        self.manager.unregister_spinner(self)\n        # Clear the spinner's line if it was the primary spinner.\n        if self._is_primary:\n            sys.stdout.write(\"\\r\\033[K\")\n            sys.stdout.flush()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Spinner.__enter__","title":"<code>__enter__()</code>","text":"<p>Start the spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __enter__(self):\n    \"\"\"Start the spinner.\"\"\"\n    self.running = True\n    self._start_time = time.time()\n    self.manager.register_spinner(self)\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Spinner.__exit__","title":"<code>__exit__(exc_type, exc_value, exc_traceback)</code>","text":"<p>Stop the spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __exit__(self, exc_type, exc_value, exc_traceback):\n    \"\"\"Stop the spinner.\"\"\"\n    self.running = False\n    self.manager.unregister_spinner(self)\n    # Clear the spinner's line if it was the primary spinner.\n    if self._is_primary:\n        sys.stdout.write(\"\\r\\033[K\")\n        sys.stdout.flush()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.Spinner.__init__","title":"<code>__init__(message='Loading...', delay=0.1, symbols=None, count_down=False, time_in_s=0)</code>","text":"<p>Initialize spinner with flexible configuration.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __init__(\n    self,\n    message: str = \"Loading...\",\n    delay: float = 0.1,\n    symbols=None,\n    count_down: bool = False,\n    time_in_s: float = 0\n):\n    \"\"\"Initialize spinner with flexible configuration.\"\"\"\n    # Resolve symbol set.\n    if isinstance(symbols, str):\n        symbols = self.SYMBOL_SETS.get(symbols, None)\n\n    # Default symbols if not provided.\n    if symbols is None:\n        symbols = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n\n    # Test mode symbol set.\n    if 'unittest' in sys.argv[0]:\n        symbols = ['#', '=', '-']\n\n    self.spinner = itertools.cycle(symbols)\n    self.delay = delay\n    self.message = message\n    self.running = False\n    self.spinner_thread = None\n    self.max_t = time_in_s\n    self.contd = count_down\n\n    # Rendering management.\n    self._is_primary = False\n    self._start_time = 0\n\n    # Central manager.\n    self.manager = SpinnerManager()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.TBEF","title":"<code>TBEF</code>","text":"<p>Automatic generated by ToolBox v = 0.1.21</p>"},{"location":"toolboxv2/#toolboxv2.utils.daemon","title":"<code>daemon</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.daemon.DaemonUtil","title":"<code>DaemonUtil</code>","text":"Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>class DaemonUtil:\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.server = None\n        self.alive = False\n        self.__storedargs = args, kwargs\n        self.async_initialized = False\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n\n    async def __ainit__(self, class_instance: Any, host='0.0.0.0', port=6587, t=False,\n                        app: (App or AppType) | None = None,\n                        peer=False, name='daemonApp-server', on_register=None, on_client_exit=None, on_server_exit=None,\n                        unix_socket=False, test_override=False):\n        from toolboxv2.mods.SocketManager import SocketType\n        self.class_instance = class_instance\n        self.server = None\n        self.port = port\n        self.host = host\n        self.alive = False\n        self.test_override = test_override\n        self._name = name\n        if on_register is None:\n            def on_register(*args):\n                return None\n        self._on_register = on_register\n        if on_client_exit is None:\n            def on_client_exit(*args):\n                return None\n        self.on_client_exit = on_client_exit\n        if on_server_exit is None:\n            def on_server_exit():\n                return None\n        self.on_server_exit = on_server_exit\n        self.unix_socket = unix_socket\n        self.online = None\n        connection_type = SocketType.server\n        if peer:\n            connection_type = SocketType.peer\n\n        await self.start_server(connection_type)\n        app = app if app is not None else get_app(from_=f\"DaemonUtil.{self._name}\")\n        self.online = await asyncio.to_thread(self.connect, app)\n        if t:\n            await self.online\n\n    async def start_server(self, connection_type=None):\n        \"\"\"Start the server using app and the socket manager\"\"\"\n        from toolboxv2.mods.SocketManager import SocketType\n        if connection_type is None:\n            connection_type = SocketType.server\n        app = get_app(from_=\"Starting.Daemon\")\n        print(app.mod_online(\"SocketManager\"), \"SocketManager\")\n        if not app.mod_online(\"SocketManager\"):\n            await app.load_mod(\"SocketManager\")\n        server_result = await app.a_run_any(SOCKETMANAGER.CREATE_SOCKET,\n                                            get_results=True,\n                                            name=self._name,\n                                            host=self.host,\n                                            port=self.port,\n                                            type_id=connection_type,\n                                            max_connections=-1,\n                                            return_full_object=True,\n                                            test_override=self.test_override,\n                                            unix_file=self.unix_socket)\n        if server_result.is_error():\n            raise Exception(f\"Server error: {server_result.print(False)}\")\n        if not server_result.is_data():\n            raise Exception(f\"Server error: {server_result.print(False)}\")\n        self.alive = True\n        self.server = server_result\n        # 'socket': socket,\n        # 'receiver_socket': r_socket,\n        # 'host': host,\n        # 'port': port,\n        # 'p2p-port': endpoint_port,\n        # 'sender': send,\n        # 'receiver_queue': receiver_queue,\n        # 'connection_error': connection_error,\n        # 'receiver_thread': s_thread,\n        # 'keepalive_thread': keep_alive_thread,\n        # 'running_dict': running_dict,\n        # 'client_to_receiver_thread': to_receive,\n        # 'client_receiver_threads': threeds,\n\n    async def send(self, data: dict or bytes or str, identifier: tuple[str, int] or str = \"main\"):\n        result = await self.server.aget()\n        sender = result.get('sender')\n        await sender(data, identifier)\n        return \"Data Transmitted\"\n\n    @staticmethod\n    async def runner_co(fuction, *args, **kwargs):\n        if asyncio.iscoroutinefunction(fuction):\n            return await fuction(*args, **kwargs)\n        return fuction(*args, **kwargs)\n\n    async def connect(self, app):\n        result = await self.server.aget()\n        if not isinstance(result, dict) or result.get('connection_error') != 0:\n            raise Exception(f\"Server error: {result}\")\n        self.server = Result.ok(result)\n        receiver_queue: queue.Queue = self.server.get('receiver_queue')\n        client_to_receiver_thread = self.server.get('client_to_receiver_thread')\n        running_dict = self.server.get('running_dict')\n        sender = self.server.get('sender')\n        known_clients = {}\n        valid_clients = {}\n        app.print(f\"Starting Demon {self._name}\")\n\n        while self.alive:\n\n            if not receiver_queue.empty():\n                data = receiver_queue.get()\n                if not data:\n                    continue\n                if 'identifier' not in data:\n                    continue\n\n                identifier = data.get('identifier', 'unknown')\n                try:\n                    if identifier == \"new_con\":\n                        client, address = data.get('data')\n                        get_logger().info(f\"New connection: {address}\")\n                        known_clients[str(address)] = client\n                        await client_to_receiver_thread(client, str(address))\n\n                        await self.runner_co(self._on_register, identifier, address)\n                        identifier = str(address)\n                        # await sender({'ok': 0}, identifier)\n\n                    print(\"Receiver queue\", identifier, identifier in known_clients, identifier in valid_clients)\n                    # validation\n                    if identifier in known_clients:\n                        get_logger().info(identifier)\n                        if identifier.startswith(\"('127.0.0.1'\"):\n                            valid_clients[identifier] = known_clients[identifier]\n                            await self.runner_co(self._on_register, identifier, data)\n                        elif data.get(\"claim\", False):\n                            do = app.run_any((\"CloudM.UserInstances\", \"validate_ws_id\"),\n                                             ws_id=data.get(\"claim\"))[0]\n                            get_logger().info(do)\n                            if do:\n                                valid_clients[identifier] = known_clients[identifier]\n                                await self.runner_co(self._on_register, identifier, data)\n                        elif data.get(\"key\", False) == os.getenv(\"TB_R_KEY\"):\n                            valid_clients[identifier] = known_clients[identifier]\n                            await self.runner_co(self._on_register, identifier, data)\n                        else:\n                            get_logger().warning(f\"Validating Failed: {identifier}\")\n                            # sender({'Validating Failed': -1}, eval(identifier))\n                        get_logger().info(f\"Validating New: {identifier}\")\n                        del known_clients[identifier]\n\n                    elif identifier in valid_clients:\n                        get_logger().info(f\"New valid Request: {identifier}\")\n                        name = data.get('name')\n                        args = data.get('args')\n                        kwargs = data.get('kwargs')\n\n                        get_logger().info(f\"Request data: {name=}{args=}{kwargs=}{identifier=}\")\n\n                        if name == 'exit_main':\n                            self.alive = False\n                            break\n\n                        if name == 'show_console':\n                            show_console(True)\n                            await sender({'ok': 0}, identifier)\n                            continue\n\n                        if name == 'hide_console':\n                            show_console(False)\n                            await sender({'ok': 0}, identifier)\n                            continue\n\n                        if name == 'rrun_flow':\n                            show_console(True)\n                            runnner = self.class_instance.run_flow\n                            threading.Thread(target=runnner, args=args, kwargs=kwargs, daemon=True).start()\n                            await sender({'ok': 0}, identifier)\n                            show_console(False)\n                            continue\n\n                        async def _helper_runner():\n                            try:\n                                attr_f = getattr(self.class_instance, name)\n\n                                if asyncio.iscoroutinefunction(attr_f):\n                                    res = await attr_f(*args, **kwargs)\n                                else:\n                                    res = attr_f(*args, **kwargs)\n\n                                if res is None:\n                                    res = {'data': res}\n                                elif isinstance(res, Result):\n                                    if asyncio.iscoroutine(res.get()) or isinstance(res.get(), asyncio.Task):\n                                        res_ = await res.aget()\n                                        res.result.data = res_\n                                    res = json.loads(res.to_api_result().json())\n                                elif isinstance(res, bytes | dict):\n                                    pass\n                                else:\n                                    res = {'data': 'unsupported type', 'type': str(type(res))}\n\n                                get_logger().info(f\"sending response {res} {type(res)}\")\n\n                                await sender(res, identifier)\n                            except Exception as e:\n                                await sender({\"data\": str(e)}, identifier)\n\n                        await _helper_runner()\n                    else:\n                        print(\"Unknown connection data:\", data)\n\n                except Exception as e:\n                    get_logger().warning(Style.RED(f\"An error occurred on {identifier} {str(e)}\"))\n                    if identifier != \"unknown\":\n                        running_dict[\"receive\"][str(identifier)] = False\n                        await self.runner_co(self.on_client_exit,  identifier)\n            await asyncio.sleep(0.1)\n        running_dict[\"server_receiver\"] = False\n        for x in running_dict[\"receive\"]:\n            running_dict[\"receive\"][x] = False\n        running_dict[\"keep_alive_var\"] = False\n        await self.runner_co(self.on_server_exit)\n        app.print(f\"Closing Demon {self._name}\")\n        return Result.ok()\n\n    async def a_exit(self):\n        result = await self.server.aget()\n        await result.get(\"close\")()\n        self.alive = False\n        if asyncio.iscoroutine(self.online):\n            await self.online\n        print(\"Connection result :\", result.get(\"host\"), result.get(\"port\"),\n              \"total connections:\", result.get(\"connections\"))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.daemon.DaemonUtil.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.server = None\n    self.alive = False\n    self.__storedargs = args, kwargs\n    self.async_initialized = False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.daemon.DaemonUtil.__initobj","title":"<code>__initobj()</code>  <code>async</code>","text":"<p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.daemon.DaemonUtil.start_server","title":"<code>start_server(connection_type=None)</code>  <code>async</code>","text":"<p>Start the server using app and the socket manager</p> Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>async def start_server(self, connection_type=None):\n    \"\"\"Start the server using app and the socket manager\"\"\"\n    from toolboxv2.mods.SocketManager import SocketType\n    if connection_type is None:\n        connection_type = SocketType.server\n    app = get_app(from_=\"Starting.Daemon\")\n    print(app.mod_online(\"SocketManager\"), \"SocketManager\")\n    if not app.mod_online(\"SocketManager\"):\n        await app.load_mod(\"SocketManager\")\n    server_result = await app.a_run_any(SOCKETMANAGER.CREATE_SOCKET,\n                                        get_results=True,\n                                        name=self._name,\n                                        host=self.host,\n                                        port=self.port,\n                                        type_id=connection_type,\n                                        max_connections=-1,\n                                        return_full_object=True,\n                                        test_override=self.test_override,\n                                        unix_file=self.unix_socket)\n    if server_result.is_error():\n        raise Exception(f\"Server error: {server_result.print(False)}\")\n    if not server_result.is_data():\n        raise Exception(f\"Server error: {server_result.print(False)}\")\n    self.alive = True\n    self.server = server_result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.daemon.daemon_util","title":"<code>daemon_util</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.daemon.daemon_util.DaemonUtil","title":"<code>DaemonUtil</code>","text":"Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>class DaemonUtil:\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.server = None\n        self.alive = False\n        self.__storedargs = args, kwargs\n        self.async_initialized = False\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n\n    async def __ainit__(self, class_instance: Any, host='0.0.0.0', port=6587, t=False,\n                        app: (App or AppType) | None = None,\n                        peer=False, name='daemonApp-server', on_register=None, on_client_exit=None, on_server_exit=None,\n                        unix_socket=False, test_override=False):\n        from toolboxv2.mods.SocketManager import SocketType\n        self.class_instance = class_instance\n        self.server = None\n        self.port = port\n        self.host = host\n        self.alive = False\n        self.test_override = test_override\n        self._name = name\n        if on_register is None:\n            def on_register(*args):\n                return None\n        self._on_register = on_register\n        if on_client_exit is None:\n            def on_client_exit(*args):\n                return None\n        self.on_client_exit = on_client_exit\n        if on_server_exit is None:\n            def on_server_exit():\n                return None\n        self.on_server_exit = on_server_exit\n        self.unix_socket = unix_socket\n        self.online = None\n        connection_type = SocketType.server\n        if peer:\n            connection_type = SocketType.peer\n\n        await self.start_server(connection_type)\n        app = app if app is not None else get_app(from_=f\"DaemonUtil.{self._name}\")\n        self.online = await asyncio.to_thread(self.connect, app)\n        if t:\n            await self.online\n\n    async def start_server(self, connection_type=None):\n        \"\"\"Start the server using app and the socket manager\"\"\"\n        from toolboxv2.mods.SocketManager import SocketType\n        if connection_type is None:\n            connection_type = SocketType.server\n        app = get_app(from_=\"Starting.Daemon\")\n        print(app.mod_online(\"SocketManager\"), \"SocketManager\")\n        if not app.mod_online(\"SocketManager\"):\n            await app.load_mod(\"SocketManager\")\n        server_result = await app.a_run_any(SOCKETMANAGER.CREATE_SOCKET,\n                                            get_results=True,\n                                            name=self._name,\n                                            host=self.host,\n                                            port=self.port,\n                                            type_id=connection_type,\n                                            max_connections=-1,\n                                            return_full_object=True,\n                                            test_override=self.test_override,\n                                            unix_file=self.unix_socket)\n        if server_result.is_error():\n            raise Exception(f\"Server error: {server_result.print(False)}\")\n        if not server_result.is_data():\n            raise Exception(f\"Server error: {server_result.print(False)}\")\n        self.alive = True\n        self.server = server_result\n        # 'socket': socket,\n        # 'receiver_socket': r_socket,\n        # 'host': host,\n        # 'port': port,\n        # 'p2p-port': endpoint_port,\n        # 'sender': send,\n        # 'receiver_queue': receiver_queue,\n        # 'connection_error': connection_error,\n        # 'receiver_thread': s_thread,\n        # 'keepalive_thread': keep_alive_thread,\n        # 'running_dict': running_dict,\n        # 'client_to_receiver_thread': to_receive,\n        # 'client_receiver_threads': threeds,\n\n    async def send(self, data: dict or bytes or str, identifier: tuple[str, int] or str = \"main\"):\n        result = await self.server.aget()\n        sender = result.get('sender')\n        await sender(data, identifier)\n        return \"Data Transmitted\"\n\n    @staticmethod\n    async def runner_co(fuction, *args, **kwargs):\n        if asyncio.iscoroutinefunction(fuction):\n            return await fuction(*args, **kwargs)\n        return fuction(*args, **kwargs)\n\n    async def connect(self, app):\n        result = await self.server.aget()\n        if not isinstance(result, dict) or result.get('connection_error') != 0:\n            raise Exception(f\"Server error: {result}\")\n        self.server = Result.ok(result)\n        receiver_queue: queue.Queue = self.server.get('receiver_queue')\n        client_to_receiver_thread = self.server.get('client_to_receiver_thread')\n        running_dict = self.server.get('running_dict')\n        sender = self.server.get('sender')\n        known_clients = {}\n        valid_clients = {}\n        app.print(f\"Starting Demon {self._name}\")\n\n        while self.alive:\n\n            if not receiver_queue.empty():\n                data = receiver_queue.get()\n                if not data:\n                    continue\n                if 'identifier' not in data:\n                    continue\n\n                identifier = data.get('identifier', 'unknown')\n                try:\n                    if identifier == \"new_con\":\n                        client, address = data.get('data')\n                        get_logger().info(f\"New connection: {address}\")\n                        known_clients[str(address)] = client\n                        await client_to_receiver_thread(client, str(address))\n\n                        await self.runner_co(self._on_register, identifier, address)\n                        identifier = str(address)\n                        # await sender({'ok': 0}, identifier)\n\n                    print(\"Receiver queue\", identifier, identifier in known_clients, identifier in valid_clients)\n                    # validation\n                    if identifier in known_clients:\n                        get_logger().info(identifier)\n                        if identifier.startswith(\"('127.0.0.1'\"):\n                            valid_clients[identifier] = known_clients[identifier]\n                            await self.runner_co(self._on_register, identifier, data)\n                        elif data.get(\"claim\", False):\n                            do = app.run_any((\"CloudM.UserInstances\", \"validate_ws_id\"),\n                                             ws_id=data.get(\"claim\"))[0]\n                            get_logger().info(do)\n                            if do:\n                                valid_clients[identifier] = known_clients[identifier]\n                                await self.runner_co(self._on_register, identifier, data)\n                        elif data.get(\"key\", False) == os.getenv(\"TB_R_KEY\"):\n                            valid_clients[identifier] = known_clients[identifier]\n                            await self.runner_co(self._on_register, identifier, data)\n                        else:\n                            get_logger().warning(f\"Validating Failed: {identifier}\")\n                            # sender({'Validating Failed': -1}, eval(identifier))\n                        get_logger().info(f\"Validating New: {identifier}\")\n                        del known_clients[identifier]\n\n                    elif identifier in valid_clients:\n                        get_logger().info(f\"New valid Request: {identifier}\")\n                        name = data.get('name')\n                        args = data.get('args')\n                        kwargs = data.get('kwargs')\n\n                        get_logger().info(f\"Request data: {name=}{args=}{kwargs=}{identifier=}\")\n\n                        if name == 'exit_main':\n                            self.alive = False\n                            break\n\n                        if name == 'show_console':\n                            show_console(True)\n                            await sender({'ok': 0}, identifier)\n                            continue\n\n                        if name == 'hide_console':\n                            show_console(False)\n                            await sender({'ok': 0}, identifier)\n                            continue\n\n                        if name == 'rrun_flow':\n                            show_console(True)\n                            runnner = self.class_instance.run_flow\n                            threading.Thread(target=runnner, args=args, kwargs=kwargs, daemon=True).start()\n                            await sender({'ok': 0}, identifier)\n                            show_console(False)\n                            continue\n\n                        async def _helper_runner():\n                            try:\n                                attr_f = getattr(self.class_instance, name)\n\n                                if asyncio.iscoroutinefunction(attr_f):\n                                    res = await attr_f(*args, **kwargs)\n                                else:\n                                    res = attr_f(*args, **kwargs)\n\n                                if res is None:\n                                    res = {'data': res}\n                                elif isinstance(res, Result):\n                                    if asyncio.iscoroutine(res.get()) or isinstance(res.get(), asyncio.Task):\n                                        res_ = await res.aget()\n                                        res.result.data = res_\n                                    res = json.loads(res.to_api_result().json())\n                                elif isinstance(res, bytes | dict):\n                                    pass\n                                else:\n                                    res = {'data': 'unsupported type', 'type': str(type(res))}\n\n                                get_logger().info(f\"sending response {res} {type(res)}\")\n\n                                await sender(res, identifier)\n                            except Exception as e:\n                                await sender({\"data\": str(e)}, identifier)\n\n                        await _helper_runner()\n                    else:\n                        print(\"Unknown connection data:\", data)\n\n                except Exception as e:\n                    get_logger().warning(Style.RED(f\"An error occurred on {identifier} {str(e)}\"))\n                    if identifier != \"unknown\":\n                        running_dict[\"receive\"][str(identifier)] = False\n                        await self.runner_co(self.on_client_exit,  identifier)\n            await asyncio.sleep(0.1)\n        running_dict[\"server_receiver\"] = False\n        for x in running_dict[\"receive\"]:\n            running_dict[\"receive\"][x] = False\n        running_dict[\"keep_alive_var\"] = False\n        await self.runner_co(self.on_server_exit)\n        app.print(f\"Closing Demon {self._name}\")\n        return Result.ok()\n\n    async def a_exit(self):\n        result = await self.server.aget()\n        await result.get(\"close\")()\n        self.alive = False\n        if asyncio.iscoroutine(self.online):\n            await self.online\n        print(\"Connection result :\", result.get(\"host\"), result.get(\"port\"),\n              \"total connections:\", result.get(\"connections\"))\n</code></pre> <code>__init__(*args, **kwargs)</code> \u00b6 <p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.server = None\n    self.alive = False\n    self.__storedargs = args, kwargs\n    self.async_initialized = False\n</code></pre> <code>__initobj()</code> <code>async</code> \u00b6 <p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre> <code>start_server(connection_type=None)</code> <code>async</code> \u00b6 <p>Start the server using app and the socket manager</p> Source code in <code>toolboxv2/utils/daemon/daemon_util.py</code> <pre><code>async def start_server(self, connection_type=None):\n    \"\"\"Start the server using app and the socket manager\"\"\"\n    from toolboxv2.mods.SocketManager import SocketType\n    if connection_type is None:\n        connection_type = SocketType.server\n    app = get_app(from_=\"Starting.Daemon\")\n    print(app.mod_online(\"SocketManager\"), \"SocketManager\")\n    if not app.mod_online(\"SocketManager\"):\n        await app.load_mod(\"SocketManager\")\n    server_result = await app.a_run_any(SOCKETMANAGER.CREATE_SOCKET,\n                                        get_results=True,\n                                        name=self._name,\n                                        host=self.host,\n                                        port=self.port,\n                                        type_id=connection_type,\n                                        max_connections=-1,\n                                        return_full_object=True,\n                                        test_override=self.test_override,\n                                        unix_file=self.unix_socket)\n    if server_result.is_error():\n        raise Exception(f\"Server error: {server_result.print(False)}\")\n    if not server_result.is_data():\n        raise Exception(f\"Server error: {server_result.print(False)}\")\n    self.alive = True\n    self.server = server_result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras","title":"<code>extras</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.BaseWidget","title":"<code>BaseWidget</code>","text":"Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>class BaseWidget:\n    def __init__(self, name: str):\n        self.name = name\n        self.openWidgetsIDs = {}\n        self.onReload = []\n        self.iframes = {}\n\n    def register(self, app, fuction, version=None, name=\"get_widget\", level=1, **kwargs):\n        if version is None:\n            version = app.version\n        app.tb(mod_name=self.name, version=version, request_as_kwarg=True, level=level, api=True, name=name, **kwargs)(\n            fuction)\n\n    def modify_iterator(self, iterator, replace):\n        \"\"\"\n        ['a', 'b'] -&gt; [{replace[0]: 'a',..., replace[len(replace)-1]: 'a'},\n        {replace[0]: 'b',..., replace[len(replace)-1]: 'b'}, ]\n        \"\"\"\n\n        for item in iterator:\n            modified_item = {replace[i]: (self.name if replace[i] == \"name\" else '') + item for i in\n                             range(len(replace))}\n            yield modified_item\n\n    def register2reload(self, *functions):\n        for fuction in functions:\n            def x(r):\n                return fuction(request=r)\n            self.onReload.append(x)\n\n    def reload_guard(self, function):\n        c = None\n        if len(self.onReload) == 0:\n            c = function()\n        return c\n\n    async def oa_reload_guard(self, function):\n        c = None\n        if len(self.onReload) == 0:\n            c = await function() if asyncio.iscoroutinefunction(function) else function()\n        return c\n\n    @staticmethod\n    def get_a_group(asset_name, template=None, file_path=None, a_kwargs=None):\n        if a_kwargs is None:\n            raise ValueError(\"a_kwargs must be specified\")\n        return [{'name': asset_name,\n                 'file_path': file_path,\n                 'kwargs': a_kwargs\n                 } if file_path is not None else {'name': asset_name,\n                                                  'template': template,\n                                                  'kwargs': a_kwargs\n                                                  }]\n\n    def group_generator(self, asset_name: str, iterator: iter, template=None, file_path=None, a_kwargs=None):\n        groups = []\n        work_kwargs = a_kwargs\n        for _i, data in enumerate(iterator):\n            if isinstance(data, dict):\n                work_kwargs = {**a_kwargs, **data}\n            groups.append(self.get_a_group(asset_name, template=template, file_path=file_path, a_kwargs=work_kwargs))\n        return groups\n\n    def asset_loder(self, app, name, asset_id, file_path=None, template=None, iterator=None, **kwargs):\n        a_kwargs = {**{\n            'root': f\"/api/{self.name}\",\n            'WidgetID': asset_id},\n                    **kwargs}\n        asset_name = f\"{name}-{asset_id}\"\n        if iterator is None:\n            group = self.get_a_group(asset_name,\n                                     template=template,\n                                     file_path=file_path,\n                                     a_kwargs=a_kwargs)\n        else:\n            group = self.group_generator(asset_name,\n                                         iterator=iterator,\n                                         template=template,\n                                         file_path=file_path,\n                                         a_kwargs=a_kwargs)\n\n        asset = app.run_any(MINIMALHTML.ADD_COLLECTION_TO_GROUP,\n                            group_name=self.name,\n                            collection={'name': f\"{asset_name}\",\n                                        'group': group},\n                            get_results=True)\n        if asset.is_error():\n            app.run_any(MINIMALHTML.ADD_GROUP, command=self.name)\n            asset = app.run_any(MINIMALHTML.ADD_COLLECTION_TO_GROUP,\n                                group_name=self.name,\n                                collection={'name': f\"{self.name}-{asset_name}\",\n                                            'group': group},\n                                get_results=True)\n        return asset\n\n    def generate_html(self, app, name=\"MainWidget\", asset_id=str(uuid.uuid4())[:4]):\n        return app.run_any(MINIMALHTML.GENERATE_HTML,\n                           group_name=self.name,\n                           collection_name=f\"{name}-{asset_id}\")\n\n    def load_widget(self, app, request, name=\"MainWidget\", asset_id=str(uuid.uuid4())[:4]):\n        app.run_any(MINIMALHTML.ADD_GROUP, command=self.name)\n        self.reload(request)\n        html_widget = self.generate_html(app, name, asset_id)\n        return html_widget[0]['html_element']\n\n    @staticmethod\n    async def get_user_from_request(app, request):\n        from toolboxv2.mods.CloudM import User\n        if request is None:\n            return User()\n        return await get_current_user_from_request(app, request)\n\n    @staticmethod\n    def get_s_id(request):\n        from ..system.types import Result\n        if request is None:\n            return Result.default_internal_error(\"No request specified\")\n        return Result.ok(request.session.get('ID', ''))\n\n    def reload(self, request):\n        [_(request) for _ in self.onReload]\n\n    async def oa_reload(self, request):\n        [_(request) if not asyncio.iscoroutinefunction(_) else await _(request) for _ in self.onReload]\n\n    async def get_widget(self, request, **kwargs):\n        raise NotImplementedError\n\n    def hash_wrapper(self, _id, _salt=''):\n        from ..security.cryp import Code\n        return Code.one_way_hash(text=_id, salt=_salt, pepper=self.name)\n\n    def register_iframe(self, iframe_id: str, src: str, width: str = \"100%\", height: str = \"500px\", **kwargs):\n        \"\"\"\n        Registriert einen iframe mit gegebener ID und Quelle\n\n        Args:\n            iframe_id: Eindeutige ID f\u00fcr den iframe\n            src: URL oder Pfad zur Quelle des iframes\n            width: Breite des iframes (default: \"100%\")\n            height: H\u00f6he des iframes (default: \"500px\")\n            **kwargs: Weitere iframe-Attribute\n        \"\"\"\n        iframe_config = {\n            'src': src,\n            'width': width,\n            'height': height,\n            **kwargs\n        }\n        self.iframes[iframe_id] = iframe_config\n\n    def create_iframe_asset(self, app, iframe_id: str, asset_id: str = None):\n        \"\"\"\n        Erstellt ein Asset f\u00fcr einen registrierten iframe\n\n        Args:\n            app: App-Instanz\n            iframe_id: ID des registrierten iframes\n            asset_id: Optional, spezifische Asset-ID\n        \"\"\"\n        if iframe_id not in self.iframes:\n            raise ValueError(f\"iframe mit ID {iframe_id} nicht registriert\")\n\n        if asset_id is None:\n            asset_id = str(uuid.uuid4())[:4]\n\n        iframe_config = self.iframes[iframe_id]\n        iframe_template = \"\"\"\n        &lt;iframe id=\"{iframe_id}\"\n                src=\"{src}\"\n                width=\"{width}\"\n                height=\"{height}\"\n                frameborder=\"0\"\n                {additional_attrs}&gt;&lt;/iframe&gt;\n        \"\"\".strip()\n\n        # Filtere bekannte Attribute heraus und erstelle String f\u00fcr zus\u00e4tzliche Attribute\n        known_attrs = {'src', 'width', 'height'}\n        additional_attrs = ' '.join(\n            f'{k}=\"{v}\"' for k, v in iframe_config.items()\n            if k not in known_attrs\n        )\n\n        iframe_html = iframe_template.format(\n            iframe_id=iframe_id,\n            src=iframe_config['src'],\n            width=iframe_config['width'],\n            height=iframe_config['height'],\n            additional_attrs=additional_attrs\n        )\n\n        return self.asset_loder(\n            app=app,\n            name=f\"iframe-{iframe_id}\",\n            asset_id=asset_id,\n            template=iframe_html\n        )\n\n    def load_iframe(self, app, iframe_id: str, asset_id: str = None):\n        \"\"\"\n        L\u00e4dt einen registrierten iframe und gibt das HTML-Element zur\u00fcck\n\n        Args:\n            app: App-Instanz\n            iframe_id: ID des registrierten iframes\n            asset_id: Optional, spezifische Asset-ID\n        \"\"\"\n        self.create_iframe_asset(app, iframe_id, asset_id)\n        return self.generate_html(app, f\"iframe-{iframe_id}\", asset_id)[0]['html_element']\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.BaseWidget.create_iframe_asset","title":"<code>create_iframe_asset(app, iframe_id, asset_id=None)</code>","text":"<p>Erstellt ein Asset f\u00fcr einen registrierten iframe</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <p>App-Instanz</p> required <code>iframe_id</code> <code>str</code> <p>ID des registrierten iframes</p> required <code>asset_id</code> <code>str</code> <p>Optional, spezifische Asset-ID</p> <code>None</code> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def create_iframe_asset(self, app, iframe_id: str, asset_id: str = None):\n    \"\"\"\n    Erstellt ein Asset f\u00fcr einen registrierten iframe\n\n    Args:\n        app: App-Instanz\n        iframe_id: ID des registrierten iframes\n        asset_id: Optional, spezifische Asset-ID\n    \"\"\"\n    if iframe_id not in self.iframes:\n        raise ValueError(f\"iframe mit ID {iframe_id} nicht registriert\")\n\n    if asset_id is None:\n        asset_id = str(uuid.uuid4())[:4]\n\n    iframe_config = self.iframes[iframe_id]\n    iframe_template = \"\"\"\n    &lt;iframe id=\"{iframe_id}\"\n            src=\"{src}\"\n            width=\"{width}\"\n            height=\"{height}\"\n            frameborder=\"0\"\n            {additional_attrs}&gt;&lt;/iframe&gt;\n    \"\"\".strip()\n\n    # Filtere bekannte Attribute heraus und erstelle String f\u00fcr zus\u00e4tzliche Attribute\n    known_attrs = {'src', 'width', 'height'}\n    additional_attrs = ' '.join(\n        f'{k}=\"{v}\"' for k, v in iframe_config.items()\n        if k not in known_attrs\n    )\n\n    iframe_html = iframe_template.format(\n        iframe_id=iframe_id,\n        src=iframe_config['src'],\n        width=iframe_config['width'],\n        height=iframe_config['height'],\n        additional_attrs=additional_attrs\n    )\n\n    return self.asset_loder(\n        app=app,\n        name=f\"iframe-{iframe_id}\",\n        asset_id=asset_id,\n        template=iframe_html\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.BaseWidget.load_iframe","title":"<code>load_iframe(app, iframe_id, asset_id=None)</code>","text":"<p>L\u00e4dt einen registrierten iframe und gibt das HTML-Element zur\u00fcck</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <p>App-Instanz</p> required <code>iframe_id</code> <code>str</code> <p>ID des registrierten iframes</p> required <code>asset_id</code> <code>str</code> <p>Optional, spezifische Asset-ID</p> <code>None</code> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def load_iframe(self, app, iframe_id: str, asset_id: str = None):\n    \"\"\"\n    L\u00e4dt einen registrierten iframe und gibt das HTML-Element zur\u00fcck\n\n    Args:\n        app: App-Instanz\n        iframe_id: ID des registrierten iframes\n        asset_id: Optional, spezifische Asset-ID\n    \"\"\"\n    self.create_iframe_asset(app, iframe_id, asset_id)\n    return self.generate_html(app, f\"iframe-{iframe_id}\", asset_id)[0]['html_element']\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.BaseWidget.modify_iterator","title":"<code>modify_iterator(iterator, replace)</code>","text":"<p>['a', 'b'] -&gt; [{replace[0]: 'a',..., replace[len(replace)-1]: 'a'}, {replace[0]: 'b',..., replace[len(replace)-1]: 'b'}, ]</p> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def modify_iterator(self, iterator, replace):\n    \"\"\"\n    ['a', 'b'] -&gt; [{replace[0]: 'a',..., replace[len(replace)-1]: 'a'},\n    {replace[0]: 'b',..., replace[len(replace)-1]: 'b'}, ]\n    \"\"\"\n\n    for item in iterator:\n        modified_item = {replace[i]: (self.name if replace[i] == \"name\" else '') + item for i in\n                         range(len(replace))}\n        yield modified_item\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.BaseWidget.register_iframe","title":"<code>register_iframe(iframe_id, src, width='100%', height='500px', **kwargs)</code>","text":"<p>Registriert einen iframe mit gegebener ID und Quelle</p> <p>Parameters:</p> Name Type Description Default <code>iframe_id</code> <code>str</code> <p>Eindeutige ID f\u00fcr den iframe</p> required <code>src</code> <code>str</code> <p>URL oder Pfad zur Quelle des iframes</p> required <code>width</code> <code>str</code> <p>Breite des iframes (default: \"100%\")</p> <code>'100%'</code> <code>height</code> <code>str</code> <p>H\u00f6he des iframes (default: \"500px\")</p> <code>'500px'</code> <code>**kwargs</code> <p>Weitere iframe-Attribute</p> <code>{}</code> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def register_iframe(self, iframe_id: str, src: str, width: str = \"100%\", height: str = \"500px\", **kwargs):\n    \"\"\"\n    Registriert einen iframe mit gegebener ID und Quelle\n\n    Args:\n        iframe_id: Eindeutige ID f\u00fcr den iframe\n        src: URL oder Pfad zur Quelle des iframes\n        width: Breite des iframes (default: \"100%\")\n        height: H\u00f6he des iframes (default: \"500px\")\n        **kwargs: Weitere iframe-Attribute\n    \"\"\"\n    iframe_config = {\n        'src': src,\n        'width': width,\n        'height': height,\n        **kwargs\n    }\n    self.iframes[iframe_id] = iframe_config\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.Style","title":"<code>Style</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.Style.Spinner","title":"<code>Spinner</code>","text":"<p>Enhanced Spinner with tqdm-like line rendering.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>class Spinner:\n    \"\"\"\n    Enhanced Spinner with tqdm-like line rendering.\n    \"\"\"\n    SYMBOL_SETS = {\n        \"c\": [\"\u25d0\", \"\u25d3\", \"\u25d1\", \"\u25d2\"],\n        \"b\": [\"\u2581\", \"\u2583\", \"\u2584\", \"\u2585\", \"\u2586\", \"\u2587\", \"\u2588\", \"\u2587\", \"\u2586\", \"\u2585\", \"\u2584\", \"\u2583\"],\n        \"d\": [\"\u28fe\", \"\u28fd\", \"\u28fb\", \"\u28bf\", \"\u287f\", \"\u28df\", \"\u28ef\", \"\u28f7\"],\n        \"w\": [\"\ud83c\udf0d\", \"\ud83c\udf0e\", \"\ud83c\udf0f\"],\n        \"s\": [\"\ud83c\udf00   \", \" \ud83c\udf00  \", \"  \ud83c\udf00 \", \"   \ud83c\udf00\", \"  \ud83c\udf00 \", \" \ud83c\udf00  \"],\n        \"+\": [\"+\", \"x\"],\n        \"t\": [\"\u2736\", \"\u2738\", \"\u2739\", \"\u273a\", \"\u2739\", \"\u2737\"]\n    }\n\n    def __init__(\n        self,\n        message: str = \"Loading...\",\n        delay: float = 0.1,\n        symbols=None,\n        count_down: bool = False,\n        time_in_s: float = 0\n    ):\n        \"\"\"Initialize spinner with flexible configuration.\"\"\"\n        # Resolve symbol set.\n        if isinstance(symbols, str):\n            symbols = self.SYMBOL_SETS.get(symbols, None)\n\n        # Default symbols if not provided.\n        if symbols is None:\n            symbols = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n\n        # Test mode symbol set.\n        if 'unittest' in sys.argv[0]:\n            symbols = ['#', '=', '-']\n\n        self.spinner = itertools.cycle(symbols)\n        self.delay = delay\n        self.message = message\n        self.running = False\n        self.spinner_thread = None\n        self.max_t = time_in_s\n        self.contd = count_down\n\n        # Rendering management.\n        self._is_primary = False\n        self._start_time = 0\n\n        # Central manager.\n        self.manager = SpinnerManager()\n\n    def _generate_render_line(self):\n        \"\"\"Generate the primary render line.\"\"\"\n        current_time = time.time()\n        if self.contd:\n            remaining = max(0, self.max_t - (current_time - self._start_time))\n            time_display = f\"{remaining:.2f}\"\n        else:\n            time_display = f\"{current_time - self._start_time:.2f}\"\n\n        symbol = next(self.spinner)\n        return f\"{symbol} {self.message} | {time_display}\"\n\n    def _generate_secondary_info(self):\n        \"\"\"Generate secondary spinner info for additional spinners.\"\"\"\n        return f\"{self.message}\"\n\n    def __enter__(self):\n        \"\"\"Start the spinner.\"\"\"\n        self.running = True\n        self._start_time = time.time()\n        self.manager.register_spinner(self)\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        \"\"\"Stop the spinner.\"\"\"\n        self.running = False\n        self.manager.unregister_spinner(self)\n        # Clear the spinner's line if it was the primary spinner.\n        if self._is_primary:\n            sys.stdout.write(\"\\r\\033[K\")\n            sys.stdout.flush()\n</code></pre> <code>__enter__()</code> \u00b6 <p>Start the spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __enter__(self):\n    \"\"\"Start the spinner.\"\"\"\n    self.running = True\n    self._start_time = time.time()\n    self.manager.register_spinner(self)\n    return self\n</code></pre> <code>__exit__(exc_type, exc_value, exc_traceback)</code> \u00b6 <p>Stop the spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __exit__(self, exc_type, exc_value, exc_traceback):\n    \"\"\"Stop the spinner.\"\"\"\n    self.running = False\n    self.manager.unregister_spinner(self)\n    # Clear the spinner's line if it was the primary spinner.\n    if self._is_primary:\n        sys.stdout.write(\"\\r\\033[K\")\n        sys.stdout.flush()\n</code></pre> <code>__init__(message='Loading...', delay=0.1, symbols=None, count_down=False, time_in_s=0)</code> \u00b6 <p>Initialize spinner with flexible configuration.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __init__(\n    self,\n    message: str = \"Loading...\",\n    delay: float = 0.1,\n    symbols=None,\n    count_down: bool = False,\n    time_in_s: float = 0\n):\n    \"\"\"Initialize spinner with flexible configuration.\"\"\"\n    # Resolve symbol set.\n    if isinstance(symbols, str):\n        symbols = self.SYMBOL_SETS.get(symbols, None)\n\n    # Default symbols if not provided.\n    if symbols is None:\n        symbols = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n\n    # Test mode symbol set.\n    if 'unittest' in sys.argv[0]:\n        symbols = ['#', '=', '-']\n\n    self.spinner = itertools.cycle(symbols)\n    self.delay = delay\n    self.message = message\n    self.running = False\n    self.spinner_thread = None\n    self.max_t = time_in_s\n    self.contd = count_down\n\n    # Rendering management.\n    self._is_primary = False\n    self._start_time = 0\n\n    # Central manager.\n    self.manager = SpinnerManager()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.Style.SpinnerManager","title":"<code>SpinnerManager</code>","text":"<p>Manages multiple spinners to ensure tqdm-like line rendering. Automatically captures SIGINT (Ctrl+C) to stop all spinners.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>class SpinnerManager(metaclass=Singleton):\n    \"\"\"\n    Manages multiple spinners to ensure tqdm-like line rendering.\n    Automatically captures SIGINT (Ctrl+C) to stop all spinners.\n    \"\"\"\n    _instance = None\n\n    def __new__(cls):\n        if not cls._instance:\n            cls._instance = super().__new__(cls)\n            cls._instance._init_manager()\n        return cls._instance\n\n    def _init_manager(self):\n        \"\"\"Initialize spinner management resources and register SIGINT handler.\"\"\"\n        self._spinners = []\n        self._lock = threading.Lock()\n        self._render_thread = None\n        self._should_run = False\n        try:\n            signal.signal(signal.SIGINT, self._signal_handler)\n        except ValueError:\n            print(\"Spinner Manager not in the min Thread no signal possible\")\n            pass\n\n    def _signal_handler(self, signum, frame):\n        \"\"\"Handle SIGINT by stopping all spinners gracefully.\"\"\"\n        with self._lock:\n            for spinner in self._spinners:\n                spinner.running = False\n            self._spinners.clear()\n        self._should_run = False\n        sys.stdout.write(\"\\r\\033[K\")  # Clear the spinner's line.\n        sys.stdout.flush()\n        sys.exit(0)\n\n    def register_spinner(self, spinner):\n        \"\"\"Register a new spinner.\"\"\"\n        with self._lock:\n            # First spinner defines the rendering line.\n            if not self._spinners:\n                spinner._is_primary = True\n            self._spinners.append(spinner)\n            # Start rendering if not already running.\n            if not self._should_run:\n                self._should_run = True\n                self._render_thread = threading.Thread(\n                    target=self._render_loop,\n                    daemon=True\n                )\n                self._render_thread.start()\n\n    def unregister_spinner(self, spinner):\n        \"\"\"Unregister a completed spinner.\"\"\"\n        with self._lock:\n            if spinner in self._spinners:\n                self._spinners.remove(spinner)\n\n    def _render_loop(self):\n        \"\"\"Continuous rendering loop for all active spinners.\"\"\"\n        while self._should_run:\n            if not self._spinners:\n                self._should_run = False\n                break\n\n            with self._lock:\n                # Find primary spinner (first registered).\n                primary_spinner = next((s for s in self._spinners if s._is_primary), None)\n\n                if primary_spinner and primary_spinner.running:\n                    # Render in the same line.\n                    render_line = primary_spinner._generate_render_line()\n\n                    # Append additional spinner info if multiple exist.\n                    if len(self._spinners) &gt; 1:\n                        secondary_info = \" | \".join(\n                            s._generate_secondary_info()\n                            for s in self._spinners\n                            if s is not primary_spinner and s.running\n                        )\n                        render_line += f\" [{secondary_info}]\"\n\n                    # Clear line and write.\n                    try:\n                        sys.stdout.write(\"\\r\" + render_line + \"\\033[K\")\n                        sys.stdout.flush()\n                    except Exception:\n                        self._should_run = False\n\n            time.sleep(0.1)  # Render interval.\n</code></pre> <code>register_spinner(spinner)</code> \u00b6 <p>Register a new spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def register_spinner(self, spinner):\n    \"\"\"Register a new spinner.\"\"\"\n    with self._lock:\n        # First spinner defines the rendering line.\n        if not self._spinners:\n            spinner._is_primary = True\n        self._spinners.append(spinner)\n        # Start rendering if not already running.\n        if not self._should_run:\n            self._should_run = True\n            self._render_thread = threading.Thread(\n                target=self._render_loop,\n                daemon=True\n            )\n            self._render_thread.start()\n</code></pre> <code>unregister_spinner(spinner)</code> \u00b6 <p>Unregister a completed spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def unregister_spinner(self, spinner):\n    \"\"\"Unregister a completed spinner.\"\"\"\n    with self._lock:\n        if spinner in self._spinners:\n            self._spinners.remove(spinner)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.base_widget","title":"<code>base_widget</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.base_widget.BaseWidget","title":"<code>BaseWidget</code>","text":"Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>class BaseWidget:\n    def __init__(self, name: str):\n        self.name = name\n        self.openWidgetsIDs = {}\n        self.onReload = []\n        self.iframes = {}\n\n    def register(self, app, fuction, version=None, name=\"get_widget\", level=1, **kwargs):\n        if version is None:\n            version = app.version\n        app.tb(mod_name=self.name, version=version, request_as_kwarg=True, level=level, api=True, name=name, **kwargs)(\n            fuction)\n\n    def modify_iterator(self, iterator, replace):\n        \"\"\"\n        ['a', 'b'] -&gt; [{replace[0]: 'a',..., replace[len(replace)-1]: 'a'},\n        {replace[0]: 'b',..., replace[len(replace)-1]: 'b'}, ]\n        \"\"\"\n\n        for item in iterator:\n            modified_item = {replace[i]: (self.name if replace[i] == \"name\" else '') + item for i in\n                             range(len(replace))}\n            yield modified_item\n\n    def register2reload(self, *functions):\n        for fuction in functions:\n            def x(r):\n                return fuction(request=r)\n            self.onReload.append(x)\n\n    def reload_guard(self, function):\n        c = None\n        if len(self.onReload) == 0:\n            c = function()\n        return c\n\n    async def oa_reload_guard(self, function):\n        c = None\n        if len(self.onReload) == 0:\n            c = await function() if asyncio.iscoroutinefunction(function) else function()\n        return c\n\n    @staticmethod\n    def get_a_group(asset_name, template=None, file_path=None, a_kwargs=None):\n        if a_kwargs is None:\n            raise ValueError(\"a_kwargs must be specified\")\n        return [{'name': asset_name,\n                 'file_path': file_path,\n                 'kwargs': a_kwargs\n                 } if file_path is not None else {'name': asset_name,\n                                                  'template': template,\n                                                  'kwargs': a_kwargs\n                                                  }]\n\n    def group_generator(self, asset_name: str, iterator: iter, template=None, file_path=None, a_kwargs=None):\n        groups = []\n        work_kwargs = a_kwargs\n        for _i, data in enumerate(iterator):\n            if isinstance(data, dict):\n                work_kwargs = {**a_kwargs, **data}\n            groups.append(self.get_a_group(asset_name, template=template, file_path=file_path, a_kwargs=work_kwargs))\n        return groups\n\n    def asset_loder(self, app, name, asset_id, file_path=None, template=None, iterator=None, **kwargs):\n        a_kwargs = {**{\n            'root': f\"/api/{self.name}\",\n            'WidgetID': asset_id},\n                    **kwargs}\n        asset_name = f\"{name}-{asset_id}\"\n        if iterator is None:\n            group = self.get_a_group(asset_name,\n                                     template=template,\n                                     file_path=file_path,\n                                     a_kwargs=a_kwargs)\n        else:\n            group = self.group_generator(asset_name,\n                                         iterator=iterator,\n                                         template=template,\n                                         file_path=file_path,\n                                         a_kwargs=a_kwargs)\n\n        asset = app.run_any(MINIMALHTML.ADD_COLLECTION_TO_GROUP,\n                            group_name=self.name,\n                            collection={'name': f\"{asset_name}\",\n                                        'group': group},\n                            get_results=True)\n        if asset.is_error():\n            app.run_any(MINIMALHTML.ADD_GROUP, command=self.name)\n            asset = app.run_any(MINIMALHTML.ADD_COLLECTION_TO_GROUP,\n                                group_name=self.name,\n                                collection={'name': f\"{self.name}-{asset_name}\",\n                                            'group': group},\n                                get_results=True)\n        return asset\n\n    def generate_html(self, app, name=\"MainWidget\", asset_id=str(uuid.uuid4())[:4]):\n        return app.run_any(MINIMALHTML.GENERATE_HTML,\n                           group_name=self.name,\n                           collection_name=f\"{name}-{asset_id}\")\n\n    def load_widget(self, app, request, name=\"MainWidget\", asset_id=str(uuid.uuid4())[:4]):\n        app.run_any(MINIMALHTML.ADD_GROUP, command=self.name)\n        self.reload(request)\n        html_widget = self.generate_html(app, name, asset_id)\n        return html_widget[0]['html_element']\n\n    @staticmethod\n    async def get_user_from_request(app, request):\n        from toolboxv2.mods.CloudM import User\n        if request is None:\n            return User()\n        return await get_current_user_from_request(app, request)\n\n    @staticmethod\n    def get_s_id(request):\n        from ..system.types import Result\n        if request is None:\n            return Result.default_internal_error(\"No request specified\")\n        return Result.ok(request.session.get('ID', ''))\n\n    def reload(self, request):\n        [_(request) for _ in self.onReload]\n\n    async def oa_reload(self, request):\n        [_(request) if not asyncio.iscoroutinefunction(_) else await _(request) for _ in self.onReload]\n\n    async def get_widget(self, request, **kwargs):\n        raise NotImplementedError\n\n    def hash_wrapper(self, _id, _salt=''):\n        from ..security.cryp import Code\n        return Code.one_way_hash(text=_id, salt=_salt, pepper=self.name)\n\n    def register_iframe(self, iframe_id: str, src: str, width: str = \"100%\", height: str = \"500px\", **kwargs):\n        \"\"\"\n        Registriert einen iframe mit gegebener ID und Quelle\n\n        Args:\n            iframe_id: Eindeutige ID f\u00fcr den iframe\n            src: URL oder Pfad zur Quelle des iframes\n            width: Breite des iframes (default: \"100%\")\n            height: H\u00f6he des iframes (default: \"500px\")\n            **kwargs: Weitere iframe-Attribute\n        \"\"\"\n        iframe_config = {\n            'src': src,\n            'width': width,\n            'height': height,\n            **kwargs\n        }\n        self.iframes[iframe_id] = iframe_config\n\n    def create_iframe_asset(self, app, iframe_id: str, asset_id: str = None):\n        \"\"\"\n        Erstellt ein Asset f\u00fcr einen registrierten iframe\n\n        Args:\n            app: App-Instanz\n            iframe_id: ID des registrierten iframes\n            asset_id: Optional, spezifische Asset-ID\n        \"\"\"\n        if iframe_id not in self.iframes:\n            raise ValueError(f\"iframe mit ID {iframe_id} nicht registriert\")\n\n        if asset_id is None:\n            asset_id = str(uuid.uuid4())[:4]\n\n        iframe_config = self.iframes[iframe_id]\n        iframe_template = \"\"\"\n        &lt;iframe id=\"{iframe_id}\"\n                src=\"{src}\"\n                width=\"{width}\"\n                height=\"{height}\"\n                frameborder=\"0\"\n                {additional_attrs}&gt;&lt;/iframe&gt;\n        \"\"\".strip()\n\n        # Filtere bekannte Attribute heraus und erstelle String f\u00fcr zus\u00e4tzliche Attribute\n        known_attrs = {'src', 'width', 'height'}\n        additional_attrs = ' '.join(\n            f'{k}=\"{v}\"' for k, v in iframe_config.items()\n            if k not in known_attrs\n        )\n\n        iframe_html = iframe_template.format(\n            iframe_id=iframe_id,\n            src=iframe_config['src'],\n            width=iframe_config['width'],\n            height=iframe_config['height'],\n            additional_attrs=additional_attrs\n        )\n\n        return self.asset_loder(\n            app=app,\n            name=f\"iframe-{iframe_id}\",\n            asset_id=asset_id,\n            template=iframe_html\n        )\n\n    def load_iframe(self, app, iframe_id: str, asset_id: str = None):\n        \"\"\"\n        L\u00e4dt einen registrierten iframe und gibt das HTML-Element zur\u00fcck\n\n        Args:\n            app: App-Instanz\n            iframe_id: ID des registrierten iframes\n            asset_id: Optional, spezifische Asset-ID\n        \"\"\"\n        self.create_iframe_asset(app, iframe_id, asset_id)\n        return self.generate_html(app, f\"iframe-{iframe_id}\", asset_id)[0]['html_element']\n</code></pre> <code>create_iframe_asset(app, iframe_id, asset_id=None)</code> \u00b6 <p>Erstellt ein Asset f\u00fcr einen registrierten iframe</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <p>App-Instanz</p> required <code>iframe_id</code> <code>str</code> <p>ID des registrierten iframes</p> required <code>asset_id</code> <code>str</code> <p>Optional, spezifische Asset-ID</p> <code>None</code> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def create_iframe_asset(self, app, iframe_id: str, asset_id: str = None):\n    \"\"\"\n    Erstellt ein Asset f\u00fcr einen registrierten iframe\n\n    Args:\n        app: App-Instanz\n        iframe_id: ID des registrierten iframes\n        asset_id: Optional, spezifische Asset-ID\n    \"\"\"\n    if iframe_id not in self.iframes:\n        raise ValueError(f\"iframe mit ID {iframe_id} nicht registriert\")\n\n    if asset_id is None:\n        asset_id = str(uuid.uuid4())[:4]\n\n    iframe_config = self.iframes[iframe_id]\n    iframe_template = \"\"\"\n    &lt;iframe id=\"{iframe_id}\"\n            src=\"{src}\"\n            width=\"{width}\"\n            height=\"{height}\"\n            frameborder=\"0\"\n            {additional_attrs}&gt;&lt;/iframe&gt;\n    \"\"\".strip()\n\n    # Filtere bekannte Attribute heraus und erstelle String f\u00fcr zus\u00e4tzliche Attribute\n    known_attrs = {'src', 'width', 'height'}\n    additional_attrs = ' '.join(\n        f'{k}=\"{v}\"' for k, v in iframe_config.items()\n        if k not in known_attrs\n    )\n\n    iframe_html = iframe_template.format(\n        iframe_id=iframe_id,\n        src=iframe_config['src'],\n        width=iframe_config['width'],\n        height=iframe_config['height'],\n        additional_attrs=additional_attrs\n    )\n\n    return self.asset_loder(\n        app=app,\n        name=f\"iframe-{iframe_id}\",\n        asset_id=asset_id,\n        template=iframe_html\n    )\n</code></pre> <code>load_iframe(app, iframe_id, asset_id=None)</code> \u00b6 <p>L\u00e4dt einen registrierten iframe und gibt das HTML-Element zur\u00fcck</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <p>App-Instanz</p> required <code>iframe_id</code> <code>str</code> <p>ID des registrierten iframes</p> required <code>asset_id</code> <code>str</code> <p>Optional, spezifische Asset-ID</p> <code>None</code> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def load_iframe(self, app, iframe_id: str, asset_id: str = None):\n    \"\"\"\n    L\u00e4dt einen registrierten iframe und gibt das HTML-Element zur\u00fcck\n\n    Args:\n        app: App-Instanz\n        iframe_id: ID des registrierten iframes\n        asset_id: Optional, spezifische Asset-ID\n    \"\"\"\n    self.create_iframe_asset(app, iframe_id, asset_id)\n    return self.generate_html(app, f\"iframe-{iframe_id}\", asset_id)[0]['html_element']\n</code></pre> <code>modify_iterator(iterator, replace)</code> \u00b6 <p>['a', 'b'] -&gt; [{replace[0]: 'a',..., replace[len(replace)-1]: 'a'}, {replace[0]: 'b',..., replace[len(replace)-1]: 'b'}, ]</p> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def modify_iterator(self, iterator, replace):\n    \"\"\"\n    ['a', 'b'] -&gt; [{replace[0]: 'a',..., replace[len(replace)-1]: 'a'},\n    {replace[0]: 'b',..., replace[len(replace)-1]: 'b'}, ]\n    \"\"\"\n\n    for item in iterator:\n        modified_item = {replace[i]: (self.name if replace[i] == \"name\" else '') + item for i in\n                         range(len(replace))}\n        yield modified_item\n</code></pre> <code>register_iframe(iframe_id, src, width='100%', height='500px', **kwargs)</code> \u00b6 <p>Registriert einen iframe mit gegebener ID und Quelle</p> <p>Parameters:</p> Name Type Description Default <code>iframe_id</code> <code>str</code> <p>Eindeutige ID f\u00fcr den iframe</p> required <code>src</code> <code>str</code> <p>URL oder Pfad zur Quelle des iframes</p> required <code>width</code> <code>str</code> <p>Breite des iframes (default: \"100%\")</p> <code>'100%'</code> <code>height</code> <code>str</code> <p>H\u00f6he des iframes (default: \"500px\")</p> <code>'500px'</code> <code>**kwargs</code> <p>Weitere iframe-Attribute</p> <code>{}</code> Source code in <code>toolboxv2/utils/extras/base_widget.py</code> <pre><code>def register_iframe(self, iframe_id: str, src: str, width: str = \"100%\", height: str = \"500px\", **kwargs):\n    \"\"\"\n    Registriert einen iframe mit gegebener ID und Quelle\n\n    Args:\n        iframe_id: Eindeutige ID f\u00fcr den iframe\n        src: URL oder Pfad zur Quelle des iframes\n        width: Breite des iframes (default: \"100%\")\n        height: H\u00f6he des iframes (default: \"500px\")\n        **kwargs: Weitere iframe-Attribute\n    \"\"\"\n    iframe_config = {\n        'src': src,\n        'width': width,\n        'height': height,\n        **kwargs\n    }\n    self.iframes[iframe_id] = iframe_config\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.blobs","title":"<code>blobs</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.blobs.BlobFile","title":"<code>BlobFile</code>","text":"Source code in <code>toolboxv2/utils/extras/blobs.py</code> <pre><code>class BlobFile(io.IOBase):\n    def __init__(self, filename: str, mode: str = 'r', storage: BlobStorage = None, key: str = None,\n                 servers: list[str] = None):\n        if not isinstance(filename, str) or not filename:\n            raise ValueError(\"Filename must be a non-empty string.\")\n        if not filename.startswith('/'): filename = '/' + filename\n        self.filename = filename.lstrip('/\\\\')\n        self.blob_id, self.folder, self.datei = self._path_splitter(self.filename)\n        self.mode = mode\n\n        if storage is None:\n            # In a real app, dependency injection or a global factory would be better\n            # but this provides a fallback for simple scripts.\n            if not servers:\n                from toolboxv2 import get_app\n                storage = get_app(from_=\"BlobStorage\").root_blob_storage\n            else:\n                storage = BlobStorage(servers=servers)\n\n        self.storage = storage\n        self.data_buffer = b\"\"\n        self.key = key\n        if key:\n            try:\n                assert Code.decrypt_symmetric(Code.encrypt_symmetric(b\"test\", key), key, to_str=False) == b\"test\"\n            except Exception:\n                raise ValueError(\"Invalid symmetric key provided.\")\n\n    @staticmethod\n    def _path_splitter(filename):\n        parts = Path(filename).parts\n        if not parts: raise ValueError(\"Filename cannot be empty.\")\n        blob_id = parts[0]\n        if len(parts) == 1: raise ValueError(\"Filename must include a path within the blob, e.g., 'blob_id/file.txt'\")\n        datei = parts[-1]\n        folder = '|'.join(parts[1:-1])\n        return blob_id, folder, datei\n\n    def create(self):\n        self.storage.create_blob(pickle.dumps({}), self.blob_id)\n        return self\n\n    def __enter__(self):\n        try:\n            raw_blob_data = self.storage.read_blob(self.blob_id)\n            blob_content = pickle.loads(raw_blob_data)\n        except (requests.exceptions.HTTPError, EOFError, pickle.UnpicklingError, ConnectionError) as e:\n            if isinstance(e, requests.exceptions.HTTPError) and e.response.status_code == 404:\n                blob_content = {}  # Blob doesn't exist yet, treat as empty\n            elif isinstance(e, (EOFError, pickle.UnpicklingError)):\n                blob_content = {}  # Blob is empty or corrupt, treat as empty for writing\n            else:\n                self.storage.create_blob(blob_id=self.blob_id, data=pickle.dumps({}))\n                blob_content = {}\n\n        if 'r' in self.mode:\n            path_key = self.folder if self.folder else self.datei\n            if self.folder:\n                file_data = blob_content.get(self.folder, {}).get(self.datei)\n            else:\n                file_data = blob_content.get(self.datei)\n\n            if file_data:\n                self.data_buffer = file_data\n                if self.key:\n                    self.data_buffer = Code.decrypt_symmetric(self.data_buffer, self.key, to_str=False)\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if 'w' in self.mode:\n            final_data = self.data_buffer\n            if self.key:\n                final_data = Code.encrypt_symmetric(final_data, self.key)\n\n            try:\n                raw_blob_data = self.storage.read_blob(self.blob_id)\n                blob_content = pickle.loads(raw_blob_data)\n            except Exception:\n                blob_content = {}\n\n            # Safely navigate and create path\n            current_level = blob_content\n            if self.folder:\n                if self.folder not in current_level:\n                    current_level[self.folder] = {}\n                current_level = current_level[self.folder]\n\n            current_level[self.datei] = final_data\n            self.storage.update_blob(self.blob_id, pickle.dumps(blob_content))\n\n\n\n\n    def exists(self) -&gt; bool:\n        \"\"\"\n        Checks if the specific file path exists within the blob without reading its content.\n        This is an efficient, read-only operation.\n\n        Returns:\n            bool: True if the file exists within the blob, False otherwise.\n        \"\"\"\n        try:\n            # Fetch the raw blob data. This leverages the local cache if available.\n            raw_blob_data = self.storage.read_blob(self.blob_id)\n            # Unpickle the directory structure.\n            if raw_blob_data:\n                blob_content = pickle.loads(raw_blob_data)\n            else:\n                return False\n        except (requests.exceptions.HTTPError, EOFError, pickle.UnpicklingError, ConnectionError):\n            # If the blob itself doesn't exist, is empty, or can't be reached,\n            # then the file within it cannot exist.\n            return False\n\n        # Navigate the dictionary to check for the file's existence.\n        current_level = blob_content\n        if self.folder:\n            if self.folder not in current_level:\n                return False\n            current_level = current_level[self.folder]\n\n        return self.datei in current_level\n\n    def clear(self):\n        self.data_buffer = b''\n\n    def write(self, data):\n        if 'w' not in self.mode: raise IOError(\"File not opened in write mode.\")\n        if isinstance(data, str):\n            self.data_buffer += data.encode()\n        elif isinstance(data, bytes):\n            self.data_buffer += data\n        else:\n            raise TypeError(\"write() argument must be str or bytes\")\n\n    def read(self):\n        if 'r' not in self.mode: raise IOError(\"File not opened in read mode.\")\n        return self.data_buffer\n\n    def read_json(self):\n        if 'r' not in self.mode: raise ValueError(\"File not opened in read mode.\")\n        if self.data_buffer == b\"\": return {}\n        return json.loads(self.data_buffer.decode())\n\n    def write_json(self, data):\n        if 'w' not in self.mode: raise ValueError(\"File not opened in write mode.\")\n        self.data_buffer += json.dumps(data).encode()\n\n    def read_pickle(self):\n        if 'r' not in self.mode: raise ValueError(\"File not opened in read mode.\")\n        if self.data_buffer == b\"\": return {}\n        return pickle.loads(self.data_buffer)\n\n    def write_pickle(self, data):\n        if 'w' not in self.mode: raise ValueError(\"File not opened in write mode.\")\n        self.data_buffer += pickle.dumps(data)\n\n    def read_yaml(self):\n        if 'r' not in self.mode: raise ValueError(\"File not opened in read mode.\")\n        if self.data_buffer == b\"\": return {}\n        return yaml.safe_load(self.data_buffer)\n\n    def write_yaml(self, data):\n        if 'w' not in self.mode: raise ValueError(\"File not opened in write mode.\")\n        yaml.dump(data, self)\n</code></pre> <code>exists()</code> \u00b6 <p>Checks if the specific file path exists within the blob without reading its content. This is an efficient, read-only operation.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file exists within the blob, False otherwise.</p> Source code in <code>toolboxv2/utils/extras/blobs.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"\n    Checks if the specific file path exists within the blob without reading its content.\n    This is an efficient, read-only operation.\n\n    Returns:\n        bool: True if the file exists within the blob, False otherwise.\n    \"\"\"\n    try:\n        # Fetch the raw blob data. This leverages the local cache if available.\n        raw_blob_data = self.storage.read_blob(self.blob_id)\n        # Unpickle the directory structure.\n        if raw_blob_data:\n            blob_content = pickle.loads(raw_blob_data)\n        else:\n            return False\n    except (requests.exceptions.HTTPError, EOFError, pickle.UnpicklingError, ConnectionError):\n        # If the blob itself doesn't exist, is empty, or can't be reached,\n        # then the file within it cannot exist.\n        return False\n\n    # Navigate the dictionary to check for the file's existence.\n    current_level = blob_content\n    if self.folder:\n        if self.folder not in current_level:\n            return False\n        current_level = current_level[self.folder]\n\n    return self.datei in current_level\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.blobs.BlobStorage","title":"<code>BlobStorage</code>","text":"<p>A production-ready client for the distributed blob storage server. It handles communication with a list of server instances, manages a local cache, and implements backoff/retry logic for resilience.</p> Source code in <code>toolboxv2/utils/extras/blobs.py</code> <pre><code>class BlobStorage:\n    \"\"\"\n    A production-ready client for the distributed blob storage server.\n    It handles communication with a list of server instances, manages a local cache,\n    and implements backoff/retry logic for resilience.\n    \"\"\"\n\n    def __init__(self, servers: list[str], storage_directory: str = './.data/blob_cache'):\n\n\n        self.servers = servers\n        self.session = requests.Session()\n        self.storage_directory = storage_directory\n        self.blob_ids = []\n        os.makedirs(storage_directory, exist_ok=True)\n\n        # Initialize the consistent hash ring\n        self.hash_ring = ConsistentHashRing()\n        for server in self.servers:\n            self.hash_ring.add_node(server)\n\n    def _make_request(self, method, endpoint, blob_id: str = None, max_retries=2, **kwargs):\n        \"\"\"\n        Makes a resilient HTTP request to the server cluster.\n        - If a blob_id is provided, it uses the consistent hash ring to find the\n          primary server and subsequent backup servers in a predictable order.\n        - If no blob_id is given (e.g., for broadcast actions), it tries servers randomly.\n        - Implements exponential backoff on server errors.\n        \"\"\"\n        if not self.servers:\n            res = requests.Response()\n            res.status_code = 503\n            res.reason = \"No servers available\"\n            return res\n\n        if blob_id:\n            # Get the ordered list of servers for this specific blob\n            preferred_servers = self.hash_ring.get_nodes_for_key(blob_id)\n        else:\n            # For non-specific requests, shuffle all servers\n            preferred_servers = random.sample(self.servers, len(self.servers))\n\n        last_error = None\n        for attempt in range(max_retries):\n            for server in preferred_servers:\n                url = f\"{server.rstrip('/')}{endpoint}\"\n                try:\n                    # In a targeted request, print which server we are trying\n                    response = self.session.request(method, url, timeout=10, **kwargs)\n\n                    if 500 &lt;= response.status_code &lt; 600:\n                        get_logger().warning(f\"Warning: Server {server} returned status {response.status_code}. Retrying...\")\n                        continue\n                    response.raise_for_status()\n                    return response\n                except requests.exceptions.RequestException as e:\n                    last_error = e\n                    get_logger().warning(f\"Warning: Could not connect to server {server}: {e}. Trying next server.\")\n\n            if attempt &lt; max_retries - 1:\n                wait_time = 2 ** (attempt*0.1)\n                get_logger().warning(f\"Warning: All preferred servers failed. Retrying in {wait_time} seconds...\")\n                time.sleep(wait_time)\n                if len(preferred_servers) == 1 and len(self.servers) &gt; 1:\n                    preferred_servers = random.sample(self.servers, len(self.servers))\n\n        raise ConnectionError(f\"Failed to execute request after {max_retries} attempts. Last error: {last_error}\")\n\n\n    def create_blob(self, data: bytes, blob_id=None) -&gt; str:\n        \"\"\"\n        Creates a new blob. The blob_id is calculated client-side by hashing\n        the content, and the data is sent to the correct server determined\n        by the consistent hash ring. This uses a PUT request, making creation\n        idempotent.\n        \"\"\"\n        # The blob ID is the hash of its content, ensuring content-addressable storage.\n        if not blob_id:\n            blob_id = hashlib.sha256(data).hexdigest()\n\n        # Use PUT, as we now know the blob's final ID/URL.\n        # Pass blob_id to _make_request so it uses the hash ring.\n        print(f\"Creating blob {blob_id} on {self._make_request('PUT', f'/blob/{blob_id}',blob_id=blob_id, data=data).status_code}\")\n        # blob_id = response.text\n        self._save_blob_to_cache(blob_id, data)\n        return blob_id\n\n    def read_blob(self, blob_id: str) -&gt; bytes:\n        cached_data = self._load_blob_from_cache(blob_id)\n        if cached_data is not None:\n            return cached_data\n\n        get_logger().info(f\"Info: Blob '{blob_id}' not in cache, fetching from network.\")\n        # Pass blob_id to _make_request to target the correct server(s).\n        response = self._make_request('GET', f'/blob/{blob_id}', blob_id=blob_id)\n\n        blob_data = response.content\n        self._save_blob_to_cache(blob_id, blob_data)\n        return blob_data\n\n    def update_blob(self, blob_id: str, data: bytes):\n        # Pass blob_id to _make_request to target the correct server(s).\n        response = self._make_request('PUT', f'/blob/{blob_id}', blob_id=blob_id, data=data)\n        self._save_blob_to_cache(blob_id, data)\n        return response\n\n    def delete_blob(self, blob_id: str):\n        # Pass blob_id to _make_request to target the correct server(s).\n        self._make_request('DELETE', f'/blob/{blob_id}', blob_id=blob_id)\n        cache_file = self._get_blob_cache_filename(blob_id)\n        if os.path.exists(cache_file):\n            os.remove(cache_file)\n\n    # NOTE: share_blobs and recover_blob are coordination endpoints. They do not\n    # act on a single blob, so they will continue to use the non-targeted (random)\n    # request mode to contact any available server to act as a coordinator.\n    def share_blobs(self, blob_ids: list[str]):\n        get_logger().info(f\"Info: Instructing a server to share blobs for recovery: {blob_ids}\")\n        payload = {\"blob_ids\": blob_ids}\n        # No blob_id passed, will try any server as a coordinator.\n        self._make_request('POST', '/share', json=payload)\n        get_logger().info(\"Info: Sharing command sent successfully.\")\n\n    def recover_blob(self, lost_blob_id: str) -&gt; bytes:\n        get_logger().info(f\"Info: Attempting to recover '{lost_blob_id}' from the cluster.\")\n        payload = {\"blob_id\": lost_blob_id}\n        # No blob_id passed, recovery can be initiated by any server.\n        response = self._make_request('POST', '/recover', json=payload)\n\n        recovered_data = response.content\n        get_logger().info(f\"Info: Successfully recovered blob '{lost_blob_id}'.\")\n        self._save_blob_to_cache(lost_blob_id, recovered_data)\n        return recovered_data\n\n    def _get_blob_cache_filename(self, blob_id: str) -&gt; str:\n        return os.path.join(self.storage_directory, blob_id + '.blobcache')\n\n    def _save_blob_to_cache(self, blob_id: str, data: bytes):\n        if blob_id not in self.blob_ids:\n            self.blob_ids.append(blob_id)\n        with open(self._get_blob_cache_filename(blob_id), 'wb') as f:\n            f.write(data)\n\n    def _load_blob_from_cache(self, blob_id: str) -&gt; bytes | None:\n        cache_file = self._get_blob_cache_filename(blob_id)\n        if not os.path.exists(cache_file):\n            return None\n        with open(cache_file, 'rb') as f:\n            return f.read()\n\n    def exit(self):\n        if len(self.blob_ids) &lt; 5:\n            return\n        for i in range(len(self.servers)//2+1):\n            self.share_blobs(self.blob_ids)\n</code></pre> <code>create_blob(data, blob_id=None)</code> \u00b6 <p>Creates a new blob. The blob_id is calculated client-side by hashing the content, and the data is sent to the correct server determined by the consistent hash ring. This uses a PUT request, making creation idempotent.</p> Source code in <code>toolboxv2/utils/extras/blobs.py</code> <pre><code>def create_blob(self, data: bytes, blob_id=None) -&gt; str:\n    \"\"\"\n    Creates a new blob. The blob_id is calculated client-side by hashing\n    the content, and the data is sent to the correct server determined\n    by the consistent hash ring. This uses a PUT request, making creation\n    idempotent.\n    \"\"\"\n    # The blob ID is the hash of its content, ensuring content-addressable storage.\n    if not blob_id:\n        blob_id = hashlib.sha256(data).hexdigest()\n\n    # Use PUT, as we now know the blob's final ID/URL.\n    # Pass blob_id to _make_request so it uses the hash ring.\n    print(f\"Creating blob {blob_id} on {self._make_request('PUT', f'/blob/{blob_id}',blob_id=blob_id, data=data).status_code}\")\n    # blob_id = response.text\n    self._save_blob_to_cache(blob_id, data)\n    return blob_id\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.blobs.ConsistentHashRing","title":"<code>ConsistentHashRing</code>","text":"<p>A consistent hash ring implementation to map keys (blob_ids) to nodes (servers). It uses virtual nodes (replicas) to ensure a more uniform distribution of keys.</p> Source code in <code>toolboxv2/utils/extras/blobs.py</code> <pre><code>class ConsistentHashRing:\n    \"\"\"\n    A consistent hash ring implementation to map keys (blob_ids) to nodes (servers).\n    It uses virtual nodes (replicas) to ensure a more uniform distribution of keys.\n    \"\"\"\n    def __init__(self, replicas=100):\n        \"\"\"\n        :param replicas: The number of virtual nodes for each physical node.\n                         Higher values lead to more balanced distribution.\n        \"\"\"\n        self.replicas = replicas\n        self._keys = []  # Sorted list of hash values (the ring)\n        self._nodes = {} # Maps hash values to physical node URLs\n\n    def _hash(self, key: str) -&gt; int:\n        \"\"\"Hashes a key to an integer using md5 for speed and distribution.\"\"\"\n        return int(hashlib.md5(key.encode('utf-8')).hexdigest(), 16)\n\n    def add_node(self, node: str):\n        \"\"\"Adds a physical node to the hash ring.\"\"\"\n        for i in range(self.replicas):\n            vnode_key = f\"{node}:{i}\"\n            h = self._hash(vnode_key)\n            bisect.insort(self._keys, h)\n            self._nodes[h] = node\n\n    def get_nodes_for_key(self, key: str) -&gt; list[str]:\n        \"\"\"\n        Returns an ordered list of nodes responsible for the given key.\n        The first node in the list is the primary, the rest are failover candidates\n        in preferential order.\n        \"\"\"\n        if not self._nodes:\n            return []\n\n        h = self._hash(key)\n        start_idx = bisect.bisect_left(self._keys, h)\n\n        # Collect unique physical nodes by iterating around the ring\n        found_nodes = []\n        for i in range(len(self._keys)):\n            idx = (start_idx + i) % len(self._keys)\n            node_hash = self._keys[idx]\n            physical_node = self._nodes[node_hash]\n            if physical_node not in found_nodes:\n                found_nodes.append(physical_node)\n            # Stop when we have found all unique physical nodes\n            if len(found_nodes) == len(set(self._nodes.values())):\n                break\n        return found_nodes\n</code></pre> <code>__init__(replicas=100)</code> \u00b6 <p>:param replicas: The number of virtual nodes for each physical node.                  Higher values lead to more balanced distribution.</p> Source code in <code>toolboxv2/utils/extras/blobs.py</code> <pre><code>def __init__(self, replicas=100):\n    \"\"\"\n    :param replicas: The number of virtual nodes for each physical node.\n                     Higher values lead to more balanced distribution.\n    \"\"\"\n    self.replicas = replicas\n    self._keys = []  # Sorted list of hash values (the ring)\n    self._nodes = {} # Maps hash values to physical node URLs\n</code></pre> <code>add_node(node)</code> \u00b6 <p>Adds a physical node to the hash ring.</p> Source code in <code>toolboxv2/utils/extras/blobs.py</code> <pre><code>def add_node(self, node: str):\n    \"\"\"Adds a physical node to the hash ring.\"\"\"\n    for i in range(self.replicas):\n        vnode_key = f\"{node}:{i}\"\n        h = self._hash(vnode_key)\n        bisect.insort(self._keys, h)\n        self._nodes[h] = node\n</code></pre> <code>get_nodes_for_key(key)</code> \u00b6 <p>Returns an ordered list of nodes responsible for the given key. The first node in the list is the primary, the rest are failover candidates in preferential order.</p> Source code in <code>toolboxv2/utils/extras/blobs.py</code> <pre><code>def get_nodes_for_key(self, key: str) -&gt; list[str]:\n    \"\"\"\n    Returns an ordered list of nodes responsible for the given key.\n    The first node in the list is the primary, the rest are failover candidates\n    in preferential order.\n    \"\"\"\n    if not self._nodes:\n        return []\n\n    h = self._hash(key)\n    start_idx = bisect.bisect_left(self._keys, h)\n\n    # Collect unique physical nodes by iterating around the ring\n    found_nodes = []\n    for i in range(len(self._keys)):\n        idx = (start_idx + i) % len(self._keys)\n        node_hash = self._keys[idx]\n        physical_node = self._nodes[node_hash]\n        if physical_node not in found_nodes:\n            found_nodes.append(physical_node)\n        # Stop when we have found all unique physical nodes\n        if len(found_nodes) == len(set(self._nodes.values())):\n            break\n    return found_nodes\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.gist_control","title":"<code>gist_control</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.gist_control.GistLoader","title":"<code>GistLoader</code>","text":"Source code in <code>toolboxv2/utils/extras/gist_control.py</code> <pre><code>class GistLoader:\n    def __init__(self, gist_url):\n        self.gist_url = gist_url\n        self.module_code = None\n\n    def load_module(self, module_name):\n        \"\"\"L\u00e4dt das Modul mit dem gegebenen Namen.\"\"\"\n        if self.module_code is None:\n            self.module_code = self._fetch_gist_content()\n\n        # Erstelle ein neues Modul\n        module = importlib.util.module_from_spec(self.get_spec(module_name))\n        exec(self.module_code, module.__dict__)\n        return module\n\n    def get_spec(self, module_name):\n        \"\"\"Gibt die Modul-Specifikation zur\u00fcck.\"\"\"\n        return ModuleSpec(module_name, self)\n\n    def get_filename(self, module_name):\n        return f\"&lt;gist:{self.gist_url}&gt;\"\n\n    def _fetch_gist_content(self):\n        \"\"\"L\u00e4dt den Inhalt des Gists von der GitHub API herunter.\"\"\"\n        gist_id = self.gist_url.split('/')[-1]\n        api_url = f\"https://api.github.com/gists/{gist_id}\"\n\n        response = requests.get(api_url)\n\n        if response.status_code == 200:\n            gist_data = response.json()\n            first_file = next(iter(gist_data['files'].values()))\n            return first_file['content']\n        else:\n            raise Exception(f\"Failed to fetch gist: {response.status_code}\")\n</code></pre> <code>get_spec(module_name)</code> \u00b6 <p>Gibt die Modul-Specifikation zur\u00fcck.</p> Source code in <code>toolboxv2/utils/extras/gist_control.py</code> <pre><code>def get_spec(self, module_name):\n    \"\"\"Gibt die Modul-Specifikation zur\u00fcck.\"\"\"\n    return ModuleSpec(module_name, self)\n</code></pre> <code>load_module(module_name)</code> \u00b6 <p>L\u00e4dt das Modul mit dem gegebenen Namen.</p> Source code in <code>toolboxv2/utils/extras/gist_control.py</code> <pre><code>def load_module(self, module_name):\n    \"\"\"L\u00e4dt das Modul mit dem gegebenen Namen.\"\"\"\n    if self.module_code is None:\n        self.module_code = self._fetch_gist_content()\n\n    # Erstelle ein neues Modul\n    module = importlib.util.module_from_spec(self.get_spec(module_name))\n    exec(self.module_code, module.__dict__)\n    return module\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.helper_test_functions","title":"<code>helper_test_functions</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.helper_test_functions.generate_edge_value","title":"<code>generate_edge_value(param_type)</code>","text":"<p>Generiert Edge-Case-Werte basierend auf dem Parametertyp.</p> Source code in <code>toolboxv2/utils/extras/helper_test_functions.py</code> <pre><code>def generate_edge_value(param_type: Any) -&gt; Any:\n    \"\"\"\n    Generiert Edge-Case-Werte basierend auf dem Parametertyp.\n    \"\"\"\n    if param_type in [int, float]:\n        return -999  # Beispiel f\u00fcr negative Zahlen\n    elif param_type == str:\n        return \"test \" * 100  # Lange zuf\u00e4llige Strings\n    # F\u00fcgen Sie hier weitere Bedingungen f\u00fcr andere Datentypen hinzu\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.helper_test_functions.generate_normal_value","title":"<code>generate_normal_value(param_type)</code>","text":"<p>Generiert normale Werte basierend auf dem Parametertyp.</p> Source code in <code>toolboxv2/utils/extras/helper_test_functions.py</code> <pre><code>def generate_normal_value(param_type: Any) -&gt; Any:\n    \"\"\"\n    Generiert normale Werte basierend auf dem Parametertyp.\n    \"\"\"\n    from toolboxv2 import RequestData\n    if param_type in [int, float]:\n        return random.randint(0, 100)  # Zuf\u00e4llige normale Zahlen\n    elif param_type == str:\n        return \"test\" # Zuf\u00e4lliges Wort\n    elif param_type == RequestData:\n        return RequestData.moc()\n    # F\u00fcgen Sie hier weitere Bedingungen f\u00fcr andere Datentypen hinzu\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.keword_matcher","title":"<code>keword_matcher</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.keword_matcher.calculate_keyword_score","title":"<code>calculate_keyword_score(text, keywords)</code>","text":"<p>Berechnet den Keyword-Score basierend auf der H\u00e4ufigkeit der Keywords im Text. Case-insensitive und optimiert f\u00fcr Geschwindigkeit.</p> <p>:param text: Eingabetext als String :param keywords: Set von Keywords :return: Gesamt-Score als Integer</p> Source code in <code>toolboxv2/utils/extras/keword_matcher.py</code> <pre><code>def calculate_keyword_score(text: str, keywords: set[str]) -&gt; int:\n    \"\"\"\n    Berechnet den Keyword-Score basierend auf der H\u00e4ufigkeit der Keywords im Text.\n    Case-insensitive und optimiert f\u00fcr Geschwindigkeit.\n\n    :param text: Eingabetext als String\n    :param keywords: Set von Keywords\n    :return: Gesamt-Score als Integer\n    \"\"\"\n    # Vorverarbeitung der Keywords\n    keyword_pattern = re.compile(\n        r'\\b(' + '|'.join(re.escape(k.lower()) for k in keywords) + r')\\b',\n        flags=re.IGNORECASE\n    )\n\n    # Erstelle Frequenz-W\u00f6rterbuch\n    freq_dict = defaultdict(int)\n\n    # Finde alle \u00dcbereinstimmungen\n    matches = keyword_pattern.findall(text.lower())\n\n    # Z\u00e4hle die Treffer\n    for match in matches:\n        freq_dict[match.lower()] += 1\n\n    # Berechne Gesamt-Score\n    total_score = sum(freq_dict.values())\n\n    return total_score\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.keword_matcher.calculate_weighted_score","title":"<code>calculate_weighted_score(text, keyword_weights)</code>","text":"<p>Berechnet gewichteten Score mit unterschiedlichen Gewichten pro Keyword</p> <p>:param text: Eingabetext :param keyword_weights: Dictionary mit {Keyword: Gewicht} :return: Gewichteter Gesamt-Score</p> Source code in <code>toolboxv2/utils/extras/keword_matcher.py</code> <pre><code>def calculate_weighted_score(text: str, keyword_weights: dict or list) -&gt; float:\n    \"\"\"\n    Berechnet gewichteten Score mit unterschiedlichen Gewichten pro Keyword\n\n    :param text: Eingabetext\n    :param keyword_weights: Dictionary mit {Keyword: Gewicht}\n    :return: Gewichteter Gesamt-Score\n    \"\"\"\n    total = 0.0\n    text_lower = text.lower()\n\n    if isinstance(keyword_weights, list):\n        keyword_weights = {k:v for k, v in keyword_weights}\n\n    for keyword, weight in keyword_weights.items():\n        count = len(re.findall(r'\\b' + re.escape(keyword.lower()) + r'\\b', text_lower))\n        total += count * weight\n\n    return round(total, 2)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.keword_matcher.extract_keywords","title":"<code>extract_keywords(text, max_len=-1, min_word_length=3, with_weights=False, remove_stopwords=True, stopwords=True)</code>","text":"<p>Extrahiert Keywords mit optionaler Frequenzgewichtung</p> <p>:param text: Eingabetext :param max_len: Maximale Anzahl Keywords (-1 = alle) :param min_word_length: Minimale Wortl\u00e4nge :param with_weights: Gibt Wort+Frequenz zur\u00fcck wenn True :param remove_stopwords: Filtert deutsche Stopw\u00f6rter :param german_stopwords: Verwendet deutsche Standard-Stopw\u00f6rter :return: Keywords oder (Keyword, H\u00e4ufigkeit) Paare</p> Source code in <code>toolboxv2/utils/extras/keword_matcher.py</code> <pre><code>def extract_keywords(\n    text: str,\n    max_len: int = -1,\n    min_word_length: int = 3,\n    with_weights: bool = False,\n    remove_stopwords: bool = True,\n    stopwords: bool = True\n) -&gt; list[str] | list[tuple[str, int]]:\n    \"\"\"\n    Extrahiert Keywords mit optionaler Frequenzgewichtung\n\n    :param text: Eingabetext\n    :param max_len: Maximale Anzahl Keywords (-1 = alle)\n    :param min_word_length: Minimale Wortl\u00e4nge\n    :param with_weights: Gibt Wort+Frequenz zur\u00fcck wenn True\n    :param remove_stopwords: Filtert deutsche Stopw\u00f6rter\n    :param german_stopwords: Verwendet deutsche Standard-Stopw\u00f6rter\n    :return: Keywords oder (Keyword, H\u00e4ufigkeit) Paare\n    \"\"\"\n\n    # Deutsche Basis-Stopw\u00f6rter\n    DEFAULT_STOPWORDS = STOPWORDS if stopwords else set()\n\n    # Text vorverarbeiten\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Worte filtern\n    filtered_words = [\n        word for word in words\n        if len(word) &gt; min_word_length\n           and (not remove_stopwords or word not in DEFAULT_STOPWORDS)\n    ]\n\n    # Frequenzanalyse\n    word_counts = defaultdict(int)\n    for word in filtered_words:\n        word_counts[word] += 1\n\n    # Sortierung: Zuerst H\u00e4ufigkeit, dann alphabetisch\n    sorted_words = sorted(\n        word_counts.items(),\n        key=lambda x: (-x[1], x[0])\n    )\n\n    # L\u00e4ngenbegrenzung\n    if max_len == -1:\n        max_len = None\n    result = sorted_words[:max_len]\n\n    return result if with_weights else [word for word, _ in result]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.reqbuilder","title":"<code>reqbuilder</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.extras.reqbuilder.generate_requirements","title":"<code>generate_requirements(folder, output_file)</code>","text":"<p>Generates requirements.txt for the specified folder using pipreqs.</p> Source code in <code>toolboxv2/utils/extras/reqbuilder.py</code> <pre><code>def generate_requirements(folder: str, output_file: str):\n    \"\"\"Generates requirements.txt for the specified folder using pipreqs.\"\"\"\n    print(folder, output_file, os.path.abspath(os.curdir))\n    try:\n        from pipreqs.pipreqs import get_all_imports\n    except ImportError:\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pipreqs\"], check=True)\n    from pipreqs.pipreqs import get_all_imports\n    imports = set(get_all_imports(os.path.abspath(folder)))\n    imports.remove('toolboxv2') if 'toolboxv2' in imports else None\n    with open(os.path.abspath(output_file), \"w\") as f:\n        f.write(\"\\n\".join(imports))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.extras.reqbuilder.run_pipeline","title":"<code>run_pipeline(base_dir)</code>","text":"<p>Runs the entire pipeline to generate requirements files.</p> Source code in <code>toolboxv2/utils/extras/reqbuilder.py</code> <pre><code>def run_pipeline(base_dir: str):\n    \"\"\"Runs the entire pipeline to generate requirements files.\"\"\"\n    toolbox_path = os.path.join(base_dir, \"toolboxv2\")\n    utils_path = os.path.join(toolbox_path, \"utils\")\n    mini_req_file = os.path.join(base_dir, \"requirements_mini.txt\")\n    extras_req_file = os.path.join(base_dir, \"requirements_tests.txt\")\n\n    # Step 1: Generate minimal requirements\n    print(\"Step 1/2: \")\n    generate_requirements(utils_path, mini_req_file)\n\n    # Step 2: Generate extended requirements\n    print(\"Step 2/2: \")\n    extras_path = os.path.join(toolbox_path, \"tests\")\n    generate_requirements(extras_path, extras_req_file)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.proxy","title":"<code>proxy</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.proxy.ProxyUtil","title":"<code>ProxyUtil</code>","text":"Source code in <code>toolboxv2/utils/proxy/prox_util.py</code> <pre><code>class ProxyUtil:\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.__storedargs = args, kwargs\n        self.async_initialized = False\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        # assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n\n    async def __ainit__(self, class_instance: Any, host='0.0.0.0', port=6587, timeout=6,\n                        app: (App or AppType) | None = None,\n                        remote_functions=None, peer=False, name='ProxyApp-client', do_connect=True, unix_socket=False,\n                        test_override=False):\n        self.class_instance = class_instance\n        self.client = None\n        self.test_override = test_override\n        self.port = port\n        self.host = host\n        self.timeout = timeout\n        if app is None:\n            app = get_app(\"ProxyUtil\")\n        self.app = app\n        self._name = name\n        self.unix_socket = unix_socket\n        if remote_functions is None:\n            remote_functions = [\"run_any\", \"a_run_any\", \"remove_mod\", \"save_load\", \"exit_main\", \"show_console\", \"hide_console\",\n                                \"rrun_flow\",\n                                \"get_autocompletion_dict\",\n                                \"exit_main\", \"watch_mod\"]\n        self.remote_functions = remote_functions\n\n        from toolboxv2.mods.SocketManager import SocketType\n        self.connection_type = SocketType.client\n        if peer:\n            self.connection_type = SocketType.peer\n        if do_connect:\n            await self.connect()\n\n    async def connect(self):\n        client_result = await self.app.a_run_local(SOCKETMANAGER.CREATE_SOCKET,\n                                           get_results=True,\n                                           name=self._name,\n                                           host=self.host,\n                                           port=self.port,\n                                           type_id=self.connection_type,\n                                           max_connections=-1,\n                                           return_full_object=True,\n                                           test_override=self.test_override,\n                                           unix_file=self.unix_socket)\n\n        if client_result.is_error():\n            raise Exception(f\"Client {self._name} error: {client_result.print(False)}\")\n        if not client_result.is_data():\n            raise Exception(f\"Client {self._name} error: {client_result.print(False)}\")\n        # 'socket': socket,\n        # 'receiver_socket': r_socket,\n        # 'host': host,\n        # 'port': port,\n        # 'p2p-port': endpoint_port,\n        # 'sender': send,\n        # 'receiver_queue': receiver_queue,\n        # 'connection_error': connection_error,\n        # 'receiver_thread': s_thread,\n        # 'keepalive_thread': keep_alive_thread,\n        # 'running_dict': running_dict,\n        # 'client_to_receiver_thread': to_receive,\n        # 'client_receiver_threads': threeds,\n        result = await client_result.aget()\n        if result is None or result.get('connection_error') != 0:\n            raise Exception(f\"Client {self._name} error: {client_result.print(False)}\")\n        self.client = Result.ok(result)\n\n    async def disconnect(self):\n        time.sleep(1)\n        close = self.client.get(\"close\")\n        await close()\n        self.client = None\n\n    async def reconnect(self):\n        if self.client is not None:\n            await self.disconnect()\n        await self.connect()\n\n    async def verify(self, message=b\"verify\"):\n        await asyncio.sleep(1)\n        # self.client.get('sender')({'keepalive': 0})\n        await self.client.get('sender')(message)\n\n    def __getattr__(self, name):\n\n        # print(f\"ProxyApp: {name}, {self.client is None}\")\n        if name == \"on_exit\":\n            return self.disconnect\n        if name == \"rc\":\n            return self.reconnect\n\n        if name == \"r\":\n            try:\n                return self.client.get('receiver_queue').get(timeout=self.timeout)\n            except:\n                return \"No data\"\n\n        app_attr = getattr(self.class_instance, name)\n\n        async def method(*args, **kwargs):\n            # if name == 'run_any':\n            #     print(\"method\", name, kwargs.get('get_results', False), args[0])\n            if self.client is None:\n                await self.reconnect()\n            if kwargs.get('spec', '-') == 'app':\n                if asyncio.iscoroutinefunction(app_attr):\n                    return await app_attr(*args, **kwargs)\n                return app_attr(*args, **kwargs)\n            try:\n                if name in self.remote_functions:\n                    if (name == 'run_any' or name == 'a_run_any') and not kwargs.get('get_results', False):\n                        if asyncio.iscoroutinefunction(app_attr):\n                            return await app_attr(*args, **kwargs)\n                        return app_attr(*args, **kwargs)\n                    if (name == 'run_any' or name == 'a_run_any') and kwargs.get('get_results', False):\n                        if isinstance(args[0], Enum):\n                            args = (args[0].__class__.NAME.value, args[0].value), args[1:]\n                    self.app.sprint(f\"Calling method {name}, {args=}, {kwargs}=\")\n                    await self.client.get('sender')({'name': name, 'args': args, 'kwargs': kwargs})\n                    while Spinner(\"Waiting for result\"):\n                        try:\n                            data = self.client.get('receiver_queue').get(timeout=self.timeout)\n                            if isinstance(data, dict) and 'identifier' in data:\n                                del data[\"identifier\"]\n                            if 'error' in data and 'origin' in data and 'result' in data and 'info' in data:\n                                data = ApiResult(**data).as_result()\n                            return data\n                        except:\n                            print(\"No data look later with class_instance.r\")\n                            return Result.default_internal_error(\"No data received from Demon.\"\n                                                                 \" uns class_instance.r to get data later\")\n            except:\n                if self.client.get('socket') is None:\n                    self.client = None\n            return app_attr(*args, **kwargs)\n\n        if callable(app_attr) and name in self.remote_functions and self.client is not None:\n            return method\n        return app_attr\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.proxy.ProxyUtil.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/proxy/prox_util.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.__storedargs = args, kwargs\n    self.async_initialized = False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.proxy.ProxyUtil.__initobj","title":"<code>__initobj()</code>  <code>async</code>","text":"<p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/proxy/prox_util.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    # assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.proxy.prox_util","title":"<code>prox_util</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.proxy.prox_util.ProxyUtil","title":"<code>ProxyUtil</code>","text":"Source code in <code>toolboxv2/utils/proxy/prox_util.py</code> <pre><code>class ProxyUtil:\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.__storedargs = args, kwargs\n        self.async_initialized = False\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        # assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n\n    async def __ainit__(self, class_instance: Any, host='0.0.0.0', port=6587, timeout=6,\n                        app: (App or AppType) | None = None,\n                        remote_functions=None, peer=False, name='ProxyApp-client', do_connect=True, unix_socket=False,\n                        test_override=False):\n        self.class_instance = class_instance\n        self.client = None\n        self.test_override = test_override\n        self.port = port\n        self.host = host\n        self.timeout = timeout\n        if app is None:\n            app = get_app(\"ProxyUtil\")\n        self.app = app\n        self._name = name\n        self.unix_socket = unix_socket\n        if remote_functions is None:\n            remote_functions = [\"run_any\", \"a_run_any\", \"remove_mod\", \"save_load\", \"exit_main\", \"show_console\", \"hide_console\",\n                                \"rrun_flow\",\n                                \"get_autocompletion_dict\",\n                                \"exit_main\", \"watch_mod\"]\n        self.remote_functions = remote_functions\n\n        from toolboxv2.mods.SocketManager import SocketType\n        self.connection_type = SocketType.client\n        if peer:\n            self.connection_type = SocketType.peer\n        if do_connect:\n            await self.connect()\n\n    async def connect(self):\n        client_result = await self.app.a_run_local(SOCKETMANAGER.CREATE_SOCKET,\n                                           get_results=True,\n                                           name=self._name,\n                                           host=self.host,\n                                           port=self.port,\n                                           type_id=self.connection_type,\n                                           max_connections=-1,\n                                           return_full_object=True,\n                                           test_override=self.test_override,\n                                           unix_file=self.unix_socket)\n\n        if client_result.is_error():\n            raise Exception(f\"Client {self._name} error: {client_result.print(False)}\")\n        if not client_result.is_data():\n            raise Exception(f\"Client {self._name} error: {client_result.print(False)}\")\n        # 'socket': socket,\n        # 'receiver_socket': r_socket,\n        # 'host': host,\n        # 'port': port,\n        # 'p2p-port': endpoint_port,\n        # 'sender': send,\n        # 'receiver_queue': receiver_queue,\n        # 'connection_error': connection_error,\n        # 'receiver_thread': s_thread,\n        # 'keepalive_thread': keep_alive_thread,\n        # 'running_dict': running_dict,\n        # 'client_to_receiver_thread': to_receive,\n        # 'client_receiver_threads': threeds,\n        result = await client_result.aget()\n        if result is None or result.get('connection_error') != 0:\n            raise Exception(f\"Client {self._name} error: {client_result.print(False)}\")\n        self.client = Result.ok(result)\n\n    async def disconnect(self):\n        time.sleep(1)\n        close = self.client.get(\"close\")\n        await close()\n        self.client = None\n\n    async def reconnect(self):\n        if self.client is not None:\n            await self.disconnect()\n        await self.connect()\n\n    async def verify(self, message=b\"verify\"):\n        await asyncio.sleep(1)\n        # self.client.get('sender')({'keepalive': 0})\n        await self.client.get('sender')(message)\n\n    def __getattr__(self, name):\n\n        # print(f\"ProxyApp: {name}, {self.client is None}\")\n        if name == \"on_exit\":\n            return self.disconnect\n        if name == \"rc\":\n            return self.reconnect\n\n        if name == \"r\":\n            try:\n                return self.client.get('receiver_queue').get(timeout=self.timeout)\n            except:\n                return \"No data\"\n\n        app_attr = getattr(self.class_instance, name)\n\n        async def method(*args, **kwargs):\n            # if name == 'run_any':\n            #     print(\"method\", name, kwargs.get('get_results', False), args[0])\n            if self.client is None:\n                await self.reconnect()\n            if kwargs.get('spec', '-') == 'app':\n                if asyncio.iscoroutinefunction(app_attr):\n                    return await app_attr(*args, **kwargs)\n                return app_attr(*args, **kwargs)\n            try:\n                if name in self.remote_functions:\n                    if (name == 'run_any' or name == 'a_run_any') and not kwargs.get('get_results', False):\n                        if asyncio.iscoroutinefunction(app_attr):\n                            return await app_attr(*args, **kwargs)\n                        return app_attr(*args, **kwargs)\n                    if (name == 'run_any' or name == 'a_run_any') and kwargs.get('get_results', False):\n                        if isinstance(args[0], Enum):\n                            args = (args[0].__class__.NAME.value, args[0].value), args[1:]\n                    self.app.sprint(f\"Calling method {name}, {args=}, {kwargs}=\")\n                    await self.client.get('sender')({'name': name, 'args': args, 'kwargs': kwargs})\n                    while Spinner(\"Waiting for result\"):\n                        try:\n                            data = self.client.get('receiver_queue').get(timeout=self.timeout)\n                            if isinstance(data, dict) and 'identifier' in data:\n                                del data[\"identifier\"]\n                            if 'error' in data and 'origin' in data and 'result' in data and 'info' in data:\n                                data = ApiResult(**data).as_result()\n                            return data\n                        except:\n                            print(\"No data look later with class_instance.r\")\n                            return Result.default_internal_error(\"No data received from Demon.\"\n                                                                 \" uns class_instance.r to get data later\")\n            except:\n                if self.client.get('socket') is None:\n                    self.client = None\n            return app_attr(*args, **kwargs)\n\n        if callable(app_attr) and name in self.remote_functions and self.client is not None:\n            return method\n        return app_attr\n</code></pre> <code>__init__(*args, **kwargs)</code> \u00b6 <p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/proxy/prox_util.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.__storedargs = args, kwargs\n    self.async_initialized = False\n</code></pre> <code>__initobj()</code> <code>async</code> \u00b6 <p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/proxy/prox_util.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    # assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security","title":"<code>security</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.security.Code","title":"<code>Code</code>","text":"Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>class Code:\n\n    @staticmethod\n    def DK():\n        return DEVICE_KEY\n\n    def decode_code(self, encrypted_data, key=None):\n\n        if not isinstance(encrypted_data, str):\n            encrypted_data = str(encrypted_data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.decrypt_symmetric(encrypted_data, key)\n\n    def encode_code(self, data, key=None):\n\n        if not isinstance(data, str):\n            data = str(data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.encrypt_symmetric(data, key)\n\n    @staticmethod\n    def generate_seed() -&gt; int:\n        \"\"\"\n        Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n        Returns:\n            int: Eine zuf\u00e4llige Zahl.\n        \"\"\"\n        return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n\n    @staticmethod\n    def one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n        \"\"\"\n        Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n        Args:\n            text (str): Der zu hashende Text.\n            salt (str): Der Salt-Wert.\n            pepper (str): Der Pepper-Wert.\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            str: Der resultierende Hash-Wert.\n        \"\"\"\n        return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n\n    @staticmethod\n    def generate_symmetric_key(as_str=True) -&gt; str or bytes:\n        \"\"\"\n        Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n        Returns:\n            str: Der generierte Schl\u00fcssel.\n        \"\"\"\n        key = Fernet.generate_key()\n        if as_str:\n            key = key.decode()\n        return key\n\n    @staticmethod\n    def encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        if isinstance(text, str):\n            text = text.encode()\n\n        try:\n            fernet = Fernet(key.encode())\n            return fernet.encrypt(text).decode()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n            return \"Error encrypt\"\n\n    @staticmethod\n    def decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n            to_str (bool): default true returns str if false returns bytes\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n\n        if isinstance(key, str):\n            key = key.encode()\n\n        #try:\n        fernet = Fernet(key)\n        text_b = fernet.decrypt(encrypted_text)\n        if not to_str:\n            return text_b\n        return text_b.decode()\n        # except Exception as e:\n        #     get_logger().error(f\"Error decrypt_symmetric {e}\")\n        #     if not mute:\n        #         raise e\n        #     if not to_str:\n        #         return f\"Error decoding\".encode()\n        #     return f\"Error decoding\"\n\n    @staticmethod\n    def generate_asymmetric_keys() -&gt; (str, str):\n        \"\"\"\n        Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n        Args:\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n        \"\"\"\n        private_key = rsa.generate_private_key(\n            public_exponent=65537,\n            key_size=2048 * 3,\n        )\n        public_key = private_key.public_key()\n\n        # Serialisieren der Schl\u00fcssel\n        pem_private_key = private_key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.NoEncryption()\n        ).decode()\n\n        pem_public_key = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        ).decode()\n\n        return pem_public_key, pem_private_key\n\n    @staticmethod\n    def save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n        \"\"\"\n        Speichert die generierten Schl\u00fcssel in separate Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n        Args:\n            public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n            private_key (str): Der private Schl\u00fcssel im PEM-Format\n            directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n        \"\"\"\n        # Erstelle das Verzeichnis, falls es nicht existiert\n        os.makedirs(directory, exist_ok=True)\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n        encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n        # Speichere den \u00f6ffentlichen Schl\u00fcssel\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        with open(public_key_path, \"w\") as f:\n            f.write(public_key)\n\n        # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n        with open(private_key_path, \"w\") as f:\n            f.write(encrypted_private_key)\n\n        print(\"Saved keys in \", public_key_path)\n\n    @staticmethod\n    def load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n        \"\"\"\n        L\u00e4dt die Schl\u00fcssel aus den Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n        Args:\n            directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n        Raises:\n            FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n        \"\"\"\n        # Pfade zu den Schl\u00fcsseldateien\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n\n        # Pr\u00fcfe ob die Dateien existieren\n        if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n            return \"\", \"\"\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Lade den \u00f6ffentlichen Schl\u00fcssel\n        with open(public_key_path) as f:\n            public_key = f.read()\n\n        # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n        with open(private_key_path) as f:\n            encrypted_private_key = f.read()\n            private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n        return public_key, private_key\n\n    @staticmethod\n    def encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        # try:\n        #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        #  except Exception as e:\n        #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            encrypted = public_key.encrypt(\n                text.encode(),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return encrypted.hex()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_asymmetric {e}\")\n            return \"Invalid\"\n\n    @staticmethod\n    def decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n        Args:\n            encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n            private_key_str (str): Der private Schl\u00fcssel als String.\n\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            decrypted = private_key.decrypt(\n                bytes.fromhex(encrypted_text_hex),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return decrypted.decode()\n\n        except Exception as e:\n            get_logger().error(f\"Error decrypt_asymmetric {e}\")\n        return \"Invalid\"\n\n    @staticmethod\n    def verify_signature(signature: str or bytes, message: str or bytes, public_key_str: str,\n                         salt_length=padding.PSS.MAX_LENGTH) -&gt; bool:\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                padding=padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                algorithm=hashes.SHA512()\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def verify_signature_web_algo(signature: str or bytes, message: str or bytes, public_key_str: str,\n                                  algo: int = -512) -&gt; bool:\n        signature_algorithm = ECDSA(hashes.SHA512())\n        if algo != -512:\n            signature_algorithm = ECDSA(hashes.SHA256())\n\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                # padding=padding.PSS(\n                #    mgf=padding.MGF1(hashes.SHA512()),\n                #    salt_length=padding.PSS.MAX_LENGTH\n                # ),\n                signature_algorithm=signature_algorithm\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def create_signature(message: str, private_key_str: str, salt_length=padding.PSS.MAX_LENGTH,\n                         row=False) -&gt; str or bytes:\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            signature = private_key.sign(\n                message.encode(),\n                padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                hashes.SHA512()\n            )\n            if row:\n                return signature\n            return base64.b64encode(signature).decode()\n        except Exception as e:\n            get_logger().error(f\"Error create_signature {e}\")\n            print(e)\n        return \"Invalid Key\"\n\n    @staticmethod\n    def pem_to_public_key(pem_key: str):\n        \"\"\"\n        Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n        Args:\n            pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n        Returns:\n            PublicKey: Das PublicKey-Objekt.\n        \"\"\"\n        public_key = serialization.load_pem_public_key(pem_key.encode())\n        return public_key\n\n    @staticmethod\n    def public_key_to_pem(public_key: RSAPublicKey):\n        \"\"\"\n        Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n        Args:\n            public_key (PublicKey): Das PublicKey-Objekt.\n\n        Returns:\n            str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n        \"\"\"\n        pem = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n        return pem.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.decrypt_asymmetric","title":"<code>decrypt_asymmetric(encrypted_text_hex, private_key_str)</code>  <code>staticmethod</code>","text":"<p>Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text_hex</code> <code>str</code> <p>Der verschl\u00fcsselte Text als Hex-String.</p> required <code>private_key_str</code> <code>str</code> <p>Der private Schl\u00fcssel als String.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n    Args:\n        encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n        private_key_str (str): Der private Schl\u00fcssel als String.\n\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n    try:\n        private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n        decrypted = private_key.decrypt(\n            bytes.fromhex(encrypted_text_hex),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return decrypted.decode()\n\n    except Exception as e:\n        get_logger().error(f\"Error decrypt_asymmetric {e}\")\n    return \"Invalid\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.decrypt_symmetric","title":"<code>decrypt_symmetric(encrypted_text, key, to_str=True, mute=False)</code>  <code>staticmethod</code>","text":"<p>Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text</code> <code>str</code> <p>Der zu entschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <code>to_str</code> <code>bool</code> <p>default true returns str if false returns bytes</p> <code>True</code> <p>Returns:     str: Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n        to_str (bool): default true returns str if false returns bytes\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n\n    if isinstance(key, str):\n        key = key.encode()\n\n    #try:\n    fernet = Fernet(key)\n    text_b = fernet.decrypt(encrypted_text)\n    if not to_str:\n        return text_b\n    return text_b.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.encrypt_asymmetric","title":"<code>encrypt_asymmetric(text, public_key_str)</code>  <code>staticmethod</code>","text":"<p>Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>public_key_str</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    # try:\n    #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n    #  except Exception as e:\n    #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n    try:\n        public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        encrypted = public_key.encrypt(\n            text.encode(),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return encrypted.hex()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        return \"Invalid\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.encrypt_symmetric","title":"<code>encrypt_symmetric(text, key)</code>  <code>staticmethod</code>","text":"<p>Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    if isinstance(text, str):\n        text = text.encode()\n\n    try:\n        fernet = Fernet(key.encode())\n        return fernet.encrypt(text).decode()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n        return \"Error encrypt\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.generate_asymmetric_keys","title":"<code>generate_asymmetric_keys()</code>  <code>staticmethod</code>","text":"<p>Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_asymmetric_keys() -&gt; (str, str):\n    \"\"\"\n    Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n    Args:\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n    \"\"\"\n    private_key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048 * 3,\n    )\n    public_key = private_key.public_key()\n\n    # Serialisieren der Schl\u00fcssel\n    pem_private_key = private_key.private_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PrivateFormat.PKCS8,\n        encryption_algorithm=serialization.NoEncryption()\n    ).decode()\n\n    pem_public_key = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    ).decode()\n\n    return pem_public_key, pem_private_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.generate_seed","title":"<code>generate_seed()</code>  <code>staticmethod</code>","text":"<p>Erzeugt eine zuf\u00e4llige Zahl als Seed.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Eine zuf\u00e4llige Zahl.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_seed() -&gt; int:\n    \"\"\"\n    Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n    Returns:\n        int: Eine zuf\u00e4llige Zahl.\n    \"\"\"\n    return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.generate_symmetric_key","title":"<code>generate_symmetric_key(as_str=True)</code>  <code>staticmethod</code>","text":"<p>Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str or bytes</code> <p>Der generierte Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_symmetric_key(as_str=True) -&gt; str or bytes:\n    \"\"\"\n    Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n    Returns:\n        str: Der generierte Schl\u00fcssel.\n    \"\"\"\n    key = Fernet.generate_key()\n    if as_str:\n        key = key.decode()\n    return key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.load_keys_from_files","title":"<code>load_keys_from_files(directory='keys')</code>  <code>staticmethod</code>","text":"<p>L\u00e4dt die Schl\u00fcssel aus den Dateien. Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen</p> <code>'keys'</code> <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n    \"\"\"\n    L\u00e4dt die Schl\u00fcssel aus den Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n    Args:\n        directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n    Raises:\n        FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n    \"\"\"\n    # Pfade zu den Schl\u00fcsseldateien\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n\n    # Pr\u00fcfe ob die Dateien existieren\n    if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n        return \"\", \"\"\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Lade den \u00f6ffentlichen Schl\u00fcssel\n    with open(public_key_path) as f:\n        public_key = f.read()\n\n    # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n    with open(private_key_path) as f:\n        encrypted_private_key = f.read()\n        private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n    return public_key, private_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.one_way_hash","title":"<code>one_way_hash(text, salt='', pepper='')</code>  <code>staticmethod</code>","text":"<p>Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu hashende Text.</p> required <code>salt</code> <code>str</code> <p>Der Salt-Wert.</p> <code>''</code> <code>pepper</code> <code>str</code> <p>Der Pepper-Wert.</p> <code>''</code> <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der resultierende Hash-Wert.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n    \"\"\"\n    Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n    Args:\n        text (str): Der zu hashende Text.\n        salt (str): Der Salt-Wert.\n        pepper (str): Der Pepper-Wert.\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        str: Der resultierende Hash-Wert.\n    \"\"\"\n    return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.pem_to_public_key","title":"<code>pem_to_public_key(pem_key)</code>  <code>staticmethod</code>","text":"<p>Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.</p> <p>Parameters:</p> Name Type Description Default <code>pem_key</code> <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef pem_to_public_key(pem_key: str):\n    \"\"\"\n    Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n    Args:\n        pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n    Returns:\n        PublicKey: Das PublicKey-Objekt.\n    \"\"\"\n    public_key = serialization.load_pem_public_key(pem_key.encode())\n    return public_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.public_key_to_pem","title":"<code>public_key_to_pem(public_key)</code>  <code>staticmethod</code>","text":"<p>Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef public_key_to_pem(public_key: RSAPublicKey):\n    \"\"\"\n    Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n    Args:\n        public_key (PublicKey): Das PublicKey-Objekt.\n\n    Returns:\n        str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n    \"\"\"\n    pem = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    )\n    return pem.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.Code.save_keys_to_files","title":"<code>save_keys_to_files(public_key, private_key, directory='keys')</code>  <code>staticmethod</code>","text":"<p>Speichert die generierten Schl\u00fcssel in separate Dateien. Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel im PEM-Format</p> required <code>private_key</code> <code>str</code> <p>Der private Schl\u00fcssel im PEM-Format</p> required <code>directory</code> <code>str</code> <p>Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen</p> <code>'keys'</code> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n    \"\"\"\n    Speichert die generierten Schl\u00fcssel in separate Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n    Args:\n        public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n        private_key (str): Der private Schl\u00fcssel im PEM-Format\n        directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n    \"\"\"\n    # Erstelle das Verzeichnis, falls es nicht existiert\n    os.makedirs(directory, exist_ok=True)\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n    encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n    # Speichere den \u00f6ffentlichen Schl\u00fcssel\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    with open(public_key_path, \"w\") as f:\n        f.write(public_key)\n\n    # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n    with open(private_key_path, \"w\") as f:\n        f.write(encrypted_private_key)\n\n    print(\"Saved keys in \", public_key_path)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.security.cryp","title":"<code>cryp</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.security.cryp.Code","title":"<code>Code</code>","text":"Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>class Code:\n\n    @staticmethod\n    def DK():\n        return DEVICE_KEY\n\n    def decode_code(self, encrypted_data, key=None):\n\n        if not isinstance(encrypted_data, str):\n            encrypted_data = str(encrypted_data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.decrypt_symmetric(encrypted_data, key)\n\n    def encode_code(self, data, key=None):\n\n        if not isinstance(data, str):\n            data = str(data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.encrypt_symmetric(data, key)\n\n    @staticmethod\n    def generate_seed() -&gt; int:\n        \"\"\"\n        Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n        Returns:\n            int: Eine zuf\u00e4llige Zahl.\n        \"\"\"\n        return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n\n    @staticmethod\n    def one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n        \"\"\"\n        Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n        Args:\n            text (str): Der zu hashende Text.\n            salt (str): Der Salt-Wert.\n            pepper (str): Der Pepper-Wert.\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            str: Der resultierende Hash-Wert.\n        \"\"\"\n        return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n\n    @staticmethod\n    def generate_symmetric_key(as_str=True) -&gt; str or bytes:\n        \"\"\"\n        Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n        Returns:\n            str: Der generierte Schl\u00fcssel.\n        \"\"\"\n        key = Fernet.generate_key()\n        if as_str:\n            key = key.decode()\n        return key\n\n    @staticmethod\n    def encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        if isinstance(text, str):\n            text = text.encode()\n\n        try:\n            fernet = Fernet(key.encode())\n            return fernet.encrypt(text).decode()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n            return \"Error encrypt\"\n\n    @staticmethod\n    def decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n            to_str (bool): default true returns str if false returns bytes\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n\n        if isinstance(key, str):\n            key = key.encode()\n\n        #try:\n        fernet = Fernet(key)\n        text_b = fernet.decrypt(encrypted_text)\n        if not to_str:\n            return text_b\n        return text_b.decode()\n        # except Exception as e:\n        #     get_logger().error(f\"Error decrypt_symmetric {e}\")\n        #     if not mute:\n        #         raise e\n        #     if not to_str:\n        #         return f\"Error decoding\".encode()\n        #     return f\"Error decoding\"\n\n    @staticmethod\n    def generate_asymmetric_keys() -&gt; (str, str):\n        \"\"\"\n        Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n        Args:\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n        \"\"\"\n        private_key = rsa.generate_private_key(\n            public_exponent=65537,\n            key_size=2048 * 3,\n        )\n        public_key = private_key.public_key()\n\n        # Serialisieren der Schl\u00fcssel\n        pem_private_key = private_key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.NoEncryption()\n        ).decode()\n\n        pem_public_key = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        ).decode()\n\n        return pem_public_key, pem_private_key\n\n    @staticmethod\n    def save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n        \"\"\"\n        Speichert die generierten Schl\u00fcssel in separate Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n        Args:\n            public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n            private_key (str): Der private Schl\u00fcssel im PEM-Format\n            directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n        \"\"\"\n        # Erstelle das Verzeichnis, falls es nicht existiert\n        os.makedirs(directory, exist_ok=True)\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n        encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n        # Speichere den \u00f6ffentlichen Schl\u00fcssel\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        with open(public_key_path, \"w\") as f:\n            f.write(public_key)\n\n        # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n        with open(private_key_path, \"w\") as f:\n            f.write(encrypted_private_key)\n\n        print(\"Saved keys in \", public_key_path)\n\n    @staticmethod\n    def load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n        \"\"\"\n        L\u00e4dt die Schl\u00fcssel aus den Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n        Args:\n            directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n        Raises:\n            FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n        \"\"\"\n        # Pfade zu den Schl\u00fcsseldateien\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n\n        # Pr\u00fcfe ob die Dateien existieren\n        if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n            return \"\", \"\"\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Lade den \u00f6ffentlichen Schl\u00fcssel\n        with open(public_key_path) as f:\n            public_key = f.read()\n\n        # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n        with open(private_key_path) as f:\n            encrypted_private_key = f.read()\n            private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n        return public_key, private_key\n\n    @staticmethod\n    def encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        # try:\n        #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        #  except Exception as e:\n        #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            encrypted = public_key.encrypt(\n                text.encode(),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return encrypted.hex()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_asymmetric {e}\")\n            return \"Invalid\"\n\n    @staticmethod\n    def decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n        Args:\n            encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n            private_key_str (str): Der private Schl\u00fcssel als String.\n\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            decrypted = private_key.decrypt(\n                bytes.fromhex(encrypted_text_hex),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return decrypted.decode()\n\n        except Exception as e:\n            get_logger().error(f\"Error decrypt_asymmetric {e}\")\n        return \"Invalid\"\n\n    @staticmethod\n    def verify_signature(signature: str or bytes, message: str or bytes, public_key_str: str,\n                         salt_length=padding.PSS.MAX_LENGTH) -&gt; bool:\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                padding=padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                algorithm=hashes.SHA512()\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def verify_signature_web_algo(signature: str or bytes, message: str or bytes, public_key_str: str,\n                                  algo: int = -512) -&gt; bool:\n        signature_algorithm = ECDSA(hashes.SHA512())\n        if algo != -512:\n            signature_algorithm = ECDSA(hashes.SHA256())\n\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                # padding=padding.PSS(\n                #    mgf=padding.MGF1(hashes.SHA512()),\n                #    salt_length=padding.PSS.MAX_LENGTH\n                # ),\n                signature_algorithm=signature_algorithm\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def create_signature(message: str, private_key_str: str, salt_length=padding.PSS.MAX_LENGTH,\n                         row=False) -&gt; str or bytes:\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            signature = private_key.sign(\n                message.encode(),\n                padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                hashes.SHA512()\n            )\n            if row:\n                return signature\n            return base64.b64encode(signature).decode()\n        except Exception as e:\n            get_logger().error(f\"Error create_signature {e}\")\n            print(e)\n        return \"Invalid Key\"\n\n    @staticmethod\n    def pem_to_public_key(pem_key: str):\n        \"\"\"\n        Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n        Args:\n            pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n        Returns:\n            PublicKey: Das PublicKey-Objekt.\n        \"\"\"\n        public_key = serialization.load_pem_public_key(pem_key.encode())\n        return public_key\n\n    @staticmethod\n    def public_key_to_pem(public_key: RSAPublicKey):\n        \"\"\"\n        Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n        Args:\n            public_key (PublicKey): Das PublicKey-Objekt.\n\n        Returns:\n            str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n        \"\"\"\n        pem = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n        return pem.decode()\n</code></pre> <code>decrypt_asymmetric(encrypted_text_hex, private_key_str)</code> <code>staticmethod</code> \u00b6 <p>Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text_hex</code> <code>str</code> <p>Der verschl\u00fcsselte Text als Hex-String.</p> required <code>private_key_str</code> <code>str</code> <p>Der private Schl\u00fcssel als String.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n    Args:\n        encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n        private_key_str (str): Der private Schl\u00fcssel als String.\n\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n    try:\n        private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n        decrypted = private_key.decrypt(\n            bytes.fromhex(encrypted_text_hex),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return decrypted.decode()\n\n    except Exception as e:\n        get_logger().error(f\"Error decrypt_asymmetric {e}\")\n    return \"Invalid\"\n</code></pre> <code>decrypt_symmetric(encrypted_text, key, to_str=True, mute=False)</code> <code>staticmethod</code> \u00b6 <p>Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text</code> <code>str</code> <p>Der zu entschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <code>to_str</code> <code>bool</code> <p>default true returns str if false returns bytes</p> <code>True</code> <p>Returns:     str: Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n        to_str (bool): default true returns str if false returns bytes\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n\n    if isinstance(key, str):\n        key = key.encode()\n\n    #try:\n    fernet = Fernet(key)\n    text_b = fernet.decrypt(encrypted_text)\n    if not to_str:\n        return text_b\n    return text_b.decode()\n</code></pre> <code>encrypt_asymmetric(text, public_key_str)</code> <code>staticmethod</code> \u00b6 <p>Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>public_key_str</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    # try:\n    #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n    #  except Exception as e:\n    #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n    try:\n        public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        encrypted = public_key.encrypt(\n            text.encode(),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return encrypted.hex()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        return \"Invalid\"\n</code></pre> <code>encrypt_symmetric(text, key)</code> <code>staticmethod</code> \u00b6 <p>Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    if isinstance(text, str):\n        text = text.encode()\n\n    try:\n        fernet = Fernet(key.encode())\n        return fernet.encrypt(text).decode()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n        return \"Error encrypt\"\n</code></pre> <code>generate_asymmetric_keys()</code> <code>staticmethod</code> \u00b6 <p>Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_asymmetric_keys() -&gt; (str, str):\n    \"\"\"\n    Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n    Args:\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n    \"\"\"\n    private_key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048 * 3,\n    )\n    public_key = private_key.public_key()\n\n    # Serialisieren der Schl\u00fcssel\n    pem_private_key = private_key.private_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PrivateFormat.PKCS8,\n        encryption_algorithm=serialization.NoEncryption()\n    ).decode()\n\n    pem_public_key = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    ).decode()\n\n    return pem_public_key, pem_private_key\n</code></pre> <code>generate_seed()</code> <code>staticmethod</code> \u00b6 <p>Erzeugt eine zuf\u00e4llige Zahl als Seed.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Eine zuf\u00e4llige Zahl.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_seed() -&gt; int:\n    \"\"\"\n    Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n    Returns:\n        int: Eine zuf\u00e4llige Zahl.\n    \"\"\"\n    return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n</code></pre> <code>generate_symmetric_key(as_str=True)</code> <code>staticmethod</code> \u00b6 <p>Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str or bytes</code> <p>Der generierte Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_symmetric_key(as_str=True) -&gt; str or bytes:\n    \"\"\"\n    Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n    Returns:\n        str: Der generierte Schl\u00fcssel.\n    \"\"\"\n    key = Fernet.generate_key()\n    if as_str:\n        key = key.decode()\n    return key\n</code></pre> <code>load_keys_from_files(directory='keys')</code> <code>staticmethod</code> \u00b6 <p>L\u00e4dt die Schl\u00fcssel aus den Dateien. Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen</p> <code>'keys'</code> <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n    \"\"\"\n    L\u00e4dt die Schl\u00fcssel aus den Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n    Args:\n        directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n    Raises:\n        FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n    \"\"\"\n    # Pfade zu den Schl\u00fcsseldateien\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n\n    # Pr\u00fcfe ob die Dateien existieren\n    if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n        return \"\", \"\"\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Lade den \u00f6ffentlichen Schl\u00fcssel\n    with open(public_key_path) as f:\n        public_key = f.read()\n\n    # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n    with open(private_key_path) as f:\n        encrypted_private_key = f.read()\n        private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n    return public_key, private_key\n</code></pre> <code>one_way_hash(text, salt='', pepper='')</code> <code>staticmethod</code> \u00b6 <p>Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu hashende Text.</p> required <code>salt</code> <code>str</code> <p>Der Salt-Wert.</p> <code>''</code> <code>pepper</code> <code>str</code> <p>Der Pepper-Wert.</p> <code>''</code> <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der resultierende Hash-Wert.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n    \"\"\"\n    Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n    Args:\n        text (str): Der zu hashende Text.\n        salt (str): Der Salt-Wert.\n        pepper (str): Der Pepper-Wert.\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        str: Der resultierende Hash-Wert.\n    \"\"\"\n    return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n</code></pre> <code>pem_to_public_key(pem_key)</code> <code>staticmethod</code> \u00b6 <p>Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.</p> <p>Parameters:</p> Name Type Description Default <code>pem_key</code> <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef pem_to_public_key(pem_key: str):\n    \"\"\"\n    Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n    Args:\n        pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n    Returns:\n        PublicKey: Das PublicKey-Objekt.\n    \"\"\"\n    public_key = serialization.load_pem_public_key(pem_key.encode())\n    return public_key\n</code></pre> <code>public_key_to_pem(public_key)</code> <code>staticmethod</code> \u00b6 <p>Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef public_key_to_pem(public_key: RSAPublicKey):\n    \"\"\"\n    Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n    Args:\n        public_key (PublicKey): Das PublicKey-Objekt.\n\n    Returns:\n        str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n    \"\"\"\n    pem = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    )\n    return pem.decode()\n</code></pre> <code>save_keys_to_files(public_key, private_key, directory='keys')</code> <code>staticmethod</code> \u00b6 <p>Speichert die generierten Schl\u00fcssel in separate Dateien. Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel im PEM-Format</p> required <code>private_key</code> <code>str</code> <p>Der private Schl\u00fcssel im PEM-Format</p> required <code>directory</code> <code>str</code> <p>Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen</p> <code>'keys'</code> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n    \"\"\"\n    Speichert die generierten Schl\u00fcssel in separate Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n    Args:\n        public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n        private_key (str): Der private Schl\u00fcssel im PEM-Format\n        directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n    \"\"\"\n    # Erstelle das Verzeichnis, falls es nicht existiert\n    os.makedirs(directory, exist_ok=True)\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n    encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n    # Speichere den \u00f6ffentlichen Schl\u00fcssel\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    with open(public_key_path, \"w\") as f:\n        f.write(public_key)\n\n    # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n    with open(private_key_path, \"w\") as f:\n        f.write(encrypted_private_key)\n\n    print(\"Saved keys in \", public_key_path)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.singelton_class","title":"<code>singelton_class</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.singelton_class.Singleton","title":"<code>Singleton</code>","text":"<p>Singleton metaclass for ensuring only one instance of a class.</p> Source code in <code>toolboxv2/utils/singelton_class.py</code> <pre><code>class Singleton(type):\n    \"\"\"\n    Singleton metaclass for ensuring only one instance of a class.\n    \"\"\"\n\n    _instances = {}\n    _kwargs = {}\n    _args = {}\n\n    def __call__(cls, *args, **kwargs):\n        if cls not in cls._instances:\n            cls._instances[cls] = super().__call__(*args, **kwargs)\n            cls._args[cls] = args\n            cls._kwargs[cls] = kwargs\n        return cls._instances[cls]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system","title":"<code>system</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.AppType","title":"<code>AppType</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class AppType:\n    prefix: str\n    id: str\n    globals: dict[str, Any] = {\"root\": dict, }\n    locals: dict[str, Any] = {\"user\": {'app': \"self\"}, }\n\n    local_test: bool = False\n    start_dir: str\n    data_dir: str\n    config_dir: str\n    info_dir: str\n\n    logger: logging.Logger\n    logging_filename: str\n\n    api_allowed_mods_list: list[str] = []\n\n    version: str\n    loop: asyncio.AbstractEventLoop\n\n    keys: dict[str, str] = {\n        \"MACRO\": \"macro~~~~:\",\n        \"MACRO_C\": \"m_color~~:\",\n        \"HELPER\": \"helper~~~:\",\n        \"debug\": \"debug~~~~:\",\n        \"id\": \"name-spa~:\",\n        \"st-load\": \"mute~load:\",\n        \"comm-his\": \"comm-his~:\",\n        \"develop-mode\": \"dev~mode~:\",\n        \"provider::\": \"provider::\",\n    }\n\n    defaults: dict[str, (bool or dict or dict[str, dict[str, str]] or str or list[str] or list[list]) | None] = {\n        \"MACRO\": list[str],\n        \"MACRO_C\": dict,\n        \"HELPER\": dict,\n        \"debug\": str,\n        \"id\": str,\n        \"st-load\": False,\n        \"comm-his\": list[list],\n        \"develop-mode\": bool,\n    }\n\n    cluster_manager: ClusterManager\n    root_blob_storage: BlobStorage\n    config_fh: FileHandler\n    _debug: bool\n    flows: dict[str, Callable]\n    dev_modi: bool\n    functions: dict[str, Any]\n    modules: dict[str, Any]\n\n    interface_type: ToolBoxInterfaces\n    REFIX: str\n\n    alive: bool\n    called_exit: tuple[bool, float]\n    args_sto: AppArgs\n    system_flag = None\n    session = None\n    appdata = None\n    exit_tasks = []\n\n    enable_profiling: bool = False\n    sto = None\n\n    def __init__(self, prefix: None | str= None, args: AppArgs | None = None):\n        self.args_sto = args\n        self.prefix = prefix\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def exit_main(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def hide_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def show_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def disconnect(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def set_logger(self, debug=False):\n        \"\"\"proxi attr\"\"\"\n\n    @property\n    def debug(self):\n        \"\"\"proxi attr\"\"\"\n        return self._debug\n\n    def debug_rains(self, e):\n        \"\"\"proxi attr\"\"\"\n\n    def set_flows(self, r):\n        \"\"\"proxi attr\"\"\"\n\n    def run_flows(self, name, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def rrun_flows(self, name, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def idle(self):\n        import time\n        self.print(\"idle\")\n        try:\n            while self.alive:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            pass\n        self.print(\"idle done\")\n\n    async def a_idle(self):\n        self.print(\"a idle\")\n        try:\n            if hasattr(self, 'daemon_app'):\n                self.print(\"serving daemon\")\n                await self.daemon_app.connect(self)\n            else:\n                self.print(\"serving default\")\n                while self.alive:\n                    await asyncio.sleep(1)\n        except KeyboardInterrupt:\n            pass\n        self.print(\"a idle done\")\n\n    @debug.setter\n    def debug(self, value):\n        \"\"\"proxi attr\"\"\"\n\n    def _coppy_mod(self, content, new_mod_dir, mod_name, file_type='py'):\n        \"\"\"proxi attr\"\"\"\n\n    def _pre_lib_mod(self, mod_name, path_to=\"./runtime\", file_type='py'):\n        \"\"\"proxi attr\"\"\"\n\n    def _copy_load(self, mod_name, file_type='py', **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True):\n        \"\"\"proxi attr\"\"\"\n\n    def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n        \"\"\"proxi attr\"\"\"\n\n    def save_initialized_module(self, tools_class, spec):\n        \"\"\"proxi attr\"\"\"\n\n    def mod_online(self, mod_name, installed=False):\n        \"\"\"proxi attr\"\"\"\n\n    def _get_function(self,\n                      name: Enum or None,\n                      state: bool = True,\n                      specification: str = \"app\",\n                      metadata=False, as_str: tuple or None = None, r=0):\n        \"\"\"proxi attr\"\"\"\n\n    def save_exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def load_mod(self, mod_name: str, mlm='I', **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    async def init_module(self, modular):\n        return await self.load_mod(modular)\n\n    async def load_all_mods_in_file(self, working_dir=\"mods\"):\n        \"\"\"proxi attr\"\"\"\n\n    def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\"):\n        \"\"\"proxi attr\"\"\"\n\n    def remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            self.remove_mod(mod, delete=delete)\n\n    async def a_remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            await self.a_remove_mod(mod, delete=delete)\n\n    def print_ok(self):\n        \"\"\"proxi attr\"\"\"\n        self.logger.info(\"OK\")\n\n    def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n        \"\"\"proxi attr\"\"\"\n\n    def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None):\n        \"\"\"proxi attr\"\"\"\n\n    def remove_mod(self, mod_name, spec='app', delete=True):\n        \"\"\"proxi attr\"\"\"\n\n    async def a_remove_mod(self, mod_name, spec='app', delete=True):\n        \"\"\"proxi attr\"\"\"\n\n    def exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def web_context(self) -&gt; str:\n        \"\"\"returns the build index ( toolbox web component )\"\"\"\n\n    async def a_exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def save_load(self, modname, spec='app'):\n        \"\"\"proxi attr\"\"\"\n\n    def get_function(self, name: Enum or tuple, **kwargs):\n        \"\"\"\n        Kwargs for _get_function\n            metadata:: return the registered function dictionary\n                stateless: (function_data, None), 0\n                stateful: (function_data, higher_order_function), 0\n            state::boolean\n                specification::str default app\n        \"\"\"\n\n    def run_a_from_sync(self, function, *args):\n        \"\"\"\n        run a async fuction\n        \"\"\"\n\n    def run_bg_task_advanced(self, task, *args, **kwargs):\n        \"\"\"\n        proxi attr\n        \"\"\"\n\n    def wait_for_bg_tasks(self, timeout=None):\n        \"\"\"\n        proxi attr\n        \"\"\"\n\n    def run_bg_task(self, task):\n        \"\"\"\n                run a async fuction\n                \"\"\"\n    def run_function(self, mod_function_name: Enum or tuple,\n                     tb_run_function_with_state=True,\n                     tb_run_with_specification='app',\n                     args_=None,\n                     kwargs_=None,\n                     *args,\n                     **kwargs) -&gt; Result:\n\n        \"\"\"proxi attr\"\"\"\n\n    async def a_run_function(self, mod_function_name: Enum or tuple,\n                             tb_run_function_with_state=True,\n                             tb_run_with_specification='app',\n                             args_=None,\n                             kwargs_=None,\n                             *args,\n                             **kwargs) -&gt; Result:\n\n        \"\"\"proxi attr\"\"\"\n\n    def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n        \"\"\"\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        proxi attr\n        \"\"\"\n\n    async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict):\n        \"\"\"\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        proxi attr\n        \"\"\"\n\n    async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None, method=\"GET\",\n                       args_=None,\n                       kwargs_=None,\n                       *args, **kwargs):\n        \"\"\"run a function remote via http / https\"\"\"\n\n    def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n                get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                kwargs_=None,\n                *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                        backwords_compability_variabel_string_holder=None,\n                        get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                        kwargs_=None,\n                        *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def print(text, *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def sprint(text, *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    # ----------------------------------------------------------------\n    # Decorators for the toolbox\n\n    def _register_function(self, module_name, func_name, data):\n        \"\"\"proxi attr\"\"\"\n\n    def _create_decorator(self, type_: str,\n                          name: str = \"\",\n                          mod_name: str = \"\",\n                          level: int = -1,\n                          restrict_in_virtual_mode: bool = False,\n                          api: bool = False,\n                          helper: str = \"\",\n                          version: str or None = None,\n                          initial=False,\n                          exit_f=False,\n                          test=True,\n                          samples=None,\n                          state=None,\n                          pre_compute=None,\n                          post_compute=None,\n                          memory_cache=False,\n                          file_cache=False,\n                          row=False,\n                          request_as_kwarg=False,\n                          memory_cache_max_size=100,\n                          memory_cache_ttl=300):\n        \"\"\"proxi attr\"\"\"\n\n        # data = {\n        #     \"type\": type_,\n        #     \"module_name\": module_name,\n        #     \"func_name\": func_name,\n        #     \"level\": level,\n        #     \"restrict_in_virtual_mode\": restrict_in_virtual_mode,\n        #     \"func\": func,\n        #     \"api\": api,\n        #     \"helper\": helper,\n        #     \"version\": version,\n        #     \"initial\": initial,\n        #     \"exit_f\": exit_f,\n        #     \"__module__\": func.__module__,\n        #     \"signature\": sig,\n        #     \"params\": params,\n        #     \"state\": (\n        #         False if len(params) == 0 else params[0] in ['self', 'state', 'app']) if state is None else state,\n        #     \"do_test\": test,\n        #     \"samples\": samples,\n        #     \"request_as_kwarg\": request_as_kwarg,\n\n    def tb(self, name=None,\n           mod_name: str = \"\",\n           helper: str = \"\",\n           version: str or None = None,\n           test: bool = True,\n           restrict_in_virtual_mode: bool = False,\n           api: bool = False,\n           initial: bool = False,\n           exit_f: bool = False,\n           test_only: bool = False,\n           memory_cache: bool = False,\n           file_cache: bool = False,\n           row=False,\n           request_as_kwarg: bool = False,\n           state: bool or None = None,\n           level: int = 0,\n           memory_cache_max_size: int = 100,\n           memory_cache_ttl: int = 300,\n           samples: list or dict or None = None,\n           interface: ToolBoxInterfaces or None or str = None,\n           pre_compute=None,\n           post_compute=None,\n           api_methods=None,\n           ):\n        \"\"\"\n    A decorator for registering and configuring functions within a module.\n\n    This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\n    Args:\n        name (str, optional): The name to register the function under. Defaults to the function's own name.\n        mod_name (str, optional): The name of the module the function belongs to.\n        helper (str, optional): A helper string providing additional information about the function.\n        version (str or None, optional): The version of the function or module.\n        test (bool, optional): Flag to indicate if the function is for testing purposes.\n        restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n        api (bool, optional): Flag to indicate if the function is part of an API.\n        initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n        exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n        test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n        memory_cache (bool, optional): Flag to enable memory caching for the function.\n        request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n        file_cache (bool, optional): Flag to enable file caching for the function.\n        row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n        state (bool or None, optional): Flag to indicate if the function maintains state.\n        level (int, optional): The level of the function, used for prioritization or categorization.\n        memory_cache_max_size (int, optional): Maximum size of the memory cache.\n        memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n        samples (list or dict or None, optional): Samples or examples of function usage.\n        interface (str, optional): The interface type for the function.\n        pre_compute (callable, optional): A function to be called before the main function.\n        post_compute (callable, optional): A function to be called after the main function.\n        api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\n    Returns:\n        function: The decorated function with additional processing and registration capabilities.\n    \"\"\"\n        if interface is None:\n            interface = \"tb\"\n        if test_only and 'test' not in self.id:\n            return lambda *args, **kwargs: args\n        return self._create_decorator(interface,\n                                      name,\n                                      mod_name,\n                                      level=level,\n                                      restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                      helper=helper,\n                                      api=api,\n                                      version=version,\n                                      initial=initial,\n                                      exit_f=exit_f,\n                                      test=test,\n                                      samples=samples,\n                                      state=state,\n                                      pre_compute=pre_compute,\n                                      post_compute=post_compute,\n                                      memory_cache=memory_cache,\n                                      file_cache=file_cache,\n                                      row=row,\n                                      request_as_kwarg=request_as_kwarg,\n                                      memory_cache_max_size=memory_cache_max_size,\n                                      memory_cache_ttl=memory_cache_ttl)\n\n    def print_functions(self, name=None):\n\n\n        if not self.functions:\n            print(\"Nothing to see\")\n            return\n\n        def helper(_functions):\n            for func_name, data in _functions.items():\n                if not isinstance(data, dict):\n                    continue\n\n                func_type = data.get('type', 'Unknown')\n                func_level = 'r' if data['level'] == -1 else data['level']\n                api_status = 'Api' if data.get('api', False) else 'Non-Api'\n\n                print(f\"  Function: {func_name}{data.get('signature', '()')}; \"\n                      f\"Type: {func_type}, Level: {func_level}, {api_status}\")\n\n        if name is not None:\n            functions = self.functions.get(name)\n            if functions is not None:\n                print(f\"\\nModule: {name}; Type: {functions.get('app_instance_type', 'Unknown')}\")\n                helper(functions)\n                return\n        for module, functions in self.functions.items():\n            print(f\"\\nModule: {module}; Type: {functions.get('app_instance_type', 'Unknown')}\")\n            helper(functions)\n\n    def save_autocompletion_dict(self):\n        \"\"\"proxi attr\"\"\"\n\n    def get_autocompletion_dict(self):\n        \"\"\"proxi attr\"\"\"\n\n    def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n        \"\"\"proxi attr\"\"\"\n\n    def save_registry_as_enums(self, directory: str, filename: str):\n        \"\"\"proxi attr\"\"\"\n\n    async def execute_all_functions_(self, m_query='', f_query=''):\n        print(\"Executing all functions\")\n        from ..extras import generate_test_cases\n        all_data = {\n            \"modular_run\": 0,\n            \"modular_fatal_error\": 0,\n            \"errors\": 0,\n            \"modular_sug\": 0,\n            \"coverage\": [],\n            \"total_coverage\": {},\n        }\n        items = list(self.functions.items()).copy()\n        for module_name, functions in items:\n            infos = {\n                \"functions_run\": 0,\n                \"functions_fatal_error\": 0,\n                \"error\": 0,\n                \"functions_sug\": 0,\n                'calls': {},\n                'callse': {},\n                \"coverage\": [0, 0],\n            }\n            all_data['modular_run'] += 1\n            if not module_name.startswith(m_query):\n                all_data['modular_sug'] += 1\n                continue\n\n            with Spinner(message=f\"In {module_name}| \"):\n                f_items = list(functions.items()).copy()\n                for function_name, function_data in f_items:\n                    if not isinstance(function_data, dict):\n                        continue\n                    if not function_name.startswith(f_query):\n                        continue\n                    test: list = function_data.get('do_test')\n                    # print(test, module_name, function_name, function_data)\n                    infos[\"coverage\"][0] += 1\n                    if test is False:\n                        continue\n\n                    with Spinner(message=f\"\\t\\t\\t\\t\\t\\tfuction {function_name}...\"):\n                        params: list = function_data.get('params')\n                        sig: signature = function_data.get('signature')\n                        state: bool = function_data.get('state')\n                        samples: bool = function_data.get('samples')\n\n                        test_kwargs_list = [{}]\n\n                        if params is not None:\n                            test_kwargs_list = samples if samples is not None else generate_test_cases(sig=sig)\n                            # print(test_kwargs)\n                            # print(test_kwargs[0])\n                            # test_kwargs = test_kwargs_list[0]\n                        # print(module_name, function_name, test_kwargs_list)\n                        infos[\"coverage\"][1] += 1\n                        for test_kwargs in test_kwargs_list:\n                            try:\n                                # print(f\"test Running {state=} |{module_name}.{function_name}\")\n                                result = await self.a_run_function((module_name, function_name),\n                                                                   tb_run_function_with_state=state,\n                                                                   **test_kwargs)\n                                if not isinstance(result, Result):\n                                    result = Result.ok(result)\n                                if result.info.exec_code == 0:\n                                    infos['calls'][function_name] = [test_kwargs, str(result)]\n                                    infos['functions_sug'] += 1\n                                else:\n                                    infos['functions_sug'] += 1\n                                    infos['error'] += 1\n                                    infos['callse'][function_name] = [test_kwargs, str(result)]\n                            except Exception as e:\n                                infos['functions_fatal_error'] += 1\n                                infos['callse'][function_name] = [test_kwargs, str(e)]\n                            finally:\n                                infos['functions_run'] += 1\n\n                if infos['functions_run'] == infos['functions_sug']:\n                    all_data['modular_sug'] += 1\n                else:\n                    all_data['modular_fatal_error'] += 1\n                if infos['error'] &gt; 0:\n                    all_data['errors'] += infos['error']\n\n                all_data[module_name] = infos\n                if infos['coverage'][0] == 0:\n                    c = 0\n                else:\n                    c = infos['coverage'][1] / infos['coverage'][0]\n                all_data[\"coverage\"].append(f\"{module_name}:{c:.2f}\\n\")\n        total_coverage = sum([float(t.split(\":\")[-1]) for t in all_data[\"coverage\"]]) / len(all_data[\"coverage\"])\n        print(\n            f\"\\n{all_data['modular_run']=}\\n{all_data['modular_sug']=}\\n{all_data['modular_fatal_error']=}\\n{total_coverage=}\")\n        d = analyze_data(all_data)\n        return Result.ok(data=all_data, data_info=d)\n\n    @staticmethod\n    def calculate_complexity(filename_or_code):\n        from radon.complexity import cc_rank, cc_visit\n        if os.path.exists(filename_or_code):\n            with open(filename_or_code) as file:\n                code = file.read()\n        else:\n            code = filename_or_code\n\n        # Calculate and print Cyclomatic Complexity\n        complexity_results = cc_visit(code)\n        i = -1\n        avg_complexity = 0\n        for block in complexity_results:\n            complexity = block.complexity\n            i += 1\n            print(f\"block: {block.name} {i} Class/Fuction/Methode : {block.letter}\")\n            print(f\"    fullname: {block.fullname}\")\n            print(f\"    Cyclomatic Complexity: {complexity}\")\n            # Optional: Get complexity rank\n            avg_complexity += complexity\n            rank = cc_rank(complexity)\n            print(f\"    Complexity Rank: {rank}\")\n            # print(f\"    lineno: {block.lineno}\")\n            print(f\"    endline: {block.endline}\")\n            print(f\"    col_offset: {block.col_offset}\\n\")\n        if i &lt;= 0:\n            i += 2\n        avg_complexity = avg_complexity / i\n        print(f\"\\nAVG Complexity: {avg_complexity:.2f}\")\n        print(f\"Total Rank: {cc_rank(int(avg_complexity + i // 10))}\")\n\n    async def execute_function_test(self, module_name: str, function_name: str,\n                                    function_data: dict, test_kwargs: dict,\n                                    profiler: cProfile.Profile) -&gt; tuple[bool, str, dict, float]:\n        start_time = time.time()\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            try:\n                result = await self.a_run_function(\n                    (module_name, function_name),\n                    tb_run_function_with_state=function_data.get('state'),\n                    **test_kwargs\n                )\n\n                if not isinstance(result, Result):\n                    result = Result.ok(result)\n\n                success = result.info.exec_code == 0\n                execution_time = time.time() - start_time\n                return success, str(result), test_kwargs, execution_time\n            except Exception as e:\n                execution_time = time.time() - start_time\n                return False, str(e), test_kwargs, execution_time\n\n    async def process_function(self, module_name: str, function_name: str,\n                               function_data: dict, profiler: cProfile.Profile) -&gt; tuple[str, ModuleInfo]:\n        start_time = time.time()\n        info = ModuleInfo()\n\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            if not isinstance(function_data, dict):\n                return function_name, info\n\n            test = function_data.get('do_test')\n            info.coverage[0] += 1\n\n            if test is False:\n                return function_name, info\n\n            params = function_data.get('params')\n            sig = function_data.get('signature')\n            samples = function_data.get('samples')\n\n            test_kwargs_list = [{}] if params is None else (\n                samples if samples is not None else generate_test_cases(sig=sig)\n            )\n\n            info.coverage[1] += 1\n\n            # Create tasks for all test cases\n            tasks = [\n                self.execute_function_test(module_name, function_name, function_data, test_kwargs, profiler)\n                for test_kwargs in test_kwargs_list\n            ]\n\n            # Execute all tests concurrently\n            results = await asyncio.gather(*tasks)\n\n            total_execution_time = 0\n            for success, result_str, test_kwargs, execution_time in results:\n                info.functions_run += 1\n                total_execution_time += execution_time\n\n                if success:\n                    info.functions_sug += 1\n                    info.calls[function_name] = [test_kwargs, result_str]\n                else:\n                    info.functions_sug += 1\n                    info.error += 1\n                    info.callse[function_name] = [test_kwargs, result_str]\n\n            info.execution_time = time.time() - start_time\n            return function_name, info\n\n    async def process_module(self, module_name: str, functions: dict,\n                             f_query: str, profiler: cProfile.Profile) -&gt; tuple[str, ModuleInfo]:\n        start_time = time.time()\n\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            async with asyncio.Semaphore(mp.cpu_count()):\n                tasks = [\n                    self.process_function(module_name, fname, fdata, profiler)\n                    for fname, fdata in functions.items()\n                    if fname.startswith(f_query)\n                ]\n\n                if not tasks:\n                    return module_name, ModuleInfo()\n\n                results = await asyncio.gather(*tasks)\n\n                # Combine results from all functions in the module\n                combined_info = ModuleInfo()\n                total_execution_time = 0\n\n                for _, info in results:\n                    combined_info.functions_run += info.functions_run\n                    combined_info.functions_fatal_error += info.functions_fatal_error\n                    combined_info.error += info.error\n                    combined_info.functions_sug += info.functions_sug\n                    combined_info.calls.update(info.calls)\n                    combined_info.callse.update(info.callse)\n                    combined_info.coverage[0] += info.coverage[0]\n                    combined_info.coverage[1] += info.coverage[1]\n                    total_execution_time += info.execution_time\n\n                combined_info.execution_time = time.time() - start_time\n                return module_name, combined_info\n\n    async def execute_all_functions(self, m_query='', f_query='', enable_profiling=True):\n        \"\"\"\n        Execute all functions with parallel processing and optional profiling.\n\n        Args:\n            m_query (str): Module name query filter\n            f_query (str): Function name query filter\n            enable_profiling (bool): Enable detailed profiling information\n        \"\"\"\n        print(\"Executing all functions in parallel\" + (\" with profiling\" if enable_profiling else \"\"))\n\n        start_time = time.time()\n        stats = ExecutionStats()\n        items = list(self.functions.items()).copy()\n\n        # Set up profiling\n        self.enable_profiling = enable_profiling\n        profiler = cProfile.Profile()\n\n        with profile_section(profiler, enable_profiling):\n            # Filter modules based on query\n            filtered_modules = [\n                (mname, mfuncs) for mname, mfuncs in items\n                if mname.startswith(m_query)\n            ]\n\n            stats.modular_run = len(filtered_modules)\n\n            # Process all modules concurrently\n            async with asyncio.Semaphore(mp.cpu_count()):\n                tasks = [\n                    self.process_module(mname, mfuncs, f_query, profiler)\n                    for mname, mfuncs in filtered_modules\n                ]\n\n                results = await asyncio.gather(*tasks)\n\n            # Combine results and calculate statistics\n            for module_name, info in results:\n                if info.functions_run == info.functions_sug:\n                    stats.modular_sug += 1\n                else:\n                    stats.modular_fatal_error += 1\n\n                stats.errors += info.error\n\n                # Calculate coverage\n                coverage = (info.coverage[1] / info.coverage[0]) if info.coverage[0] &gt; 0 else 0\n                stats.coverage.append(f\"{module_name}:{coverage:.2f}\\n\")\n\n                # Store module info\n                stats.__dict__[module_name] = info\n\n            # Calculate total coverage\n            total_coverage = (\n                sum(float(t.split(\":\")[-1]) for t in stats.coverage) / len(stats.coverage)\n                if stats.coverage else 0\n            )\n\n            stats.total_execution_time = time.time() - start_time\n\n            # Generate profiling stats if enabled\n            if enable_profiling:\n                s = io.StringIO()\n                ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n                ps.print_stats()\n                stats.profiling_data = {\n                    'detailed_stats': s.getvalue(),\n                    'total_time': stats.total_execution_time,\n                    'function_count': stats.modular_run,\n                    'successful_functions': stats.modular_sug\n                }\n\n            print(\n                f\"\\n{stats.modular_run=}\"\n                f\"\\n{stats.modular_sug=}\"\n                f\"\\n{stats.modular_fatal_error=}\"\n                f\"\\n{total_coverage=}\"\n                f\"\\nTotal execution time: {stats.total_execution_time:.2f}s\"\n            )\n\n            if enable_profiling:\n                print(\"\\nProfiling Summary:\")\n                print(f\"{'=' * 50}\")\n                print(\"Top 10 time-consuming functions:\")\n                ps.print_stats(10)\n\n            analyzed_data = analyze_data(stats.__dict__)\n            return Result.ok(data=stats.__dict__, data_info=analyzed_data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.debug","title":"<code>debug</code>  <code>property</code> <code>writable</code>","text":"<p>proxi attr</p>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.prefix","title":"<code>prefix = prefix</code>  <code>instance-attribute</code>","text":"<p>proxi attr</p>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.a_exit","title":"<code>a_exit()</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.a_fuction_runner","title":"<code>a_fuction_runner(function, function_data, args, kwargs)</code>  <code>async</code>","text":"<p>parameters = function_data.get('params') modular_name = function_data.get('module_name') function_name = function_data.get('func_name') mod_function_name = f\"{modular_name}.{function_name}\"</p> <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict):\n    \"\"\"\n    parameters = function_data.get('params')\n    modular_name = function_data.get('module_name')\n    function_name = function_data.get('func_name')\n    mod_function_name = f\"{modular_name}.{function_name}\"\n\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.a_remove_mod","title":"<code>a_remove_mod(mod_name, spec='app', delete=True)</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_remove_mod(self, mod_name, spec='app', delete=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.a_run_any","title":"<code>a_run_any(mod_function_name, backwords_compability_variabel_string_holder=None, get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                    backwords_compability_variabel_string_holder=None,\n                    get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                    kwargs_=None,\n                    *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.a_run_function","title":"<code>a_run_function(mod_function_name, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_run_function(self, mod_function_name: Enum or tuple,\n                         tb_run_function_with_state=True,\n                         tb_run_with_specification='app',\n                         args_=None,\n                         kwargs_=None,\n                         *args,\n                         **kwargs) -&gt; Result:\n\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.debug_rains","title":"<code>debug_rains(e)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def debug_rains(self, e):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.disconnect","title":"<code>disconnect(*args, **kwargs)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def disconnect(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.execute_all_functions","title":"<code>execute_all_functions(m_query='', f_query='', enable_profiling=True)</code>  <code>async</code>","text":"<p>Execute all functions with parallel processing and optional profiling.</p> <p>Parameters:</p> Name Type Description Default <code>m_query</code> <code>str</code> <p>Module name query filter</p> <code>''</code> <code>f_query</code> <code>str</code> <p>Function name query filter</p> <code>''</code> <code>enable_profiling</code> <code>bool</code> <p>Enable detailed profiling information</p> <code>True</code> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def execute_all_functions(self, m_query='', f_query='', enable_profiling=True):\n    \"\"\"\n    Execute all functions with parallel processing and optional profiling.\n\n    Args:\n        m_query (str): Module name query filter\n        f_query (str): Function name query filter\n        enable_profiling (bool): Enable detailed profiling information\n    \"\"\"\n    print(\"Executing all functions in parallel\" + (\" with profiling\" if enable_profiling else \"\"))\n\n    start_time = time.time()\n    stats = ExecutionStats()\n    items = list(self.functions.items()).copy()\n\n    # Set up profiling\n    self.enable_profiling = enable_profiling\n    profiler = cProfile.Profile()\n\n    with profile_section(profiler, enable_profiling):\n        # Filter modules based on query\n        filtered_modules = [\n            (mname, mfuncs) for mname, mfuncs in items\n            if mname.startswith(m_query)\n        ]\n\n        stats.modular_run = len(filtered_modules)\n\n        # Process all modules concurrently\n        async with asyncio.Semaphore(mp.cpu_count()):\n            tasks = [\n                self.process_module(mname, mfuncs, f_query, profiler)\n                for mname, mfuncs in filtered_modules\n            ]\n\n            results = await asyncio.gather(*tasks)\n\n        # Combine results and calculate statistics\n        for module_name, info in results:\n            if info.functions_run == info.functions_sug:\n                stats.modular_sug += 1\n            else:\n                stats.modular_fatal_error += 1\n\n            stats.errors += info.error\n\n            # Calculate coverage\n            coverage = (info.coverage[1] / info.coverage[0]) if info.coverage[0] &gt; 0 else 0\n            stats.coverage.append(f\"{module_name}:{coverage:.2f}\\n\")\n\n            # Store module info\n            stats.__dict__[module_name] = info\n\n        # Calculate total coverage\n        total_coverage = (\n            sum(float(t.split(\":\")[-1]) for t in stats.coverage) / len(stats.coverage)\n            if stats.coverage else 0\n        )\n\n        stats.total_execution_time = time.time() - start_time\n\n        # Generate profiling stats if enabled\n        if enable_profiling:\n            s = io.StringIO()\n            ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n            ps.print_stats()\n            stats.profiling_data = {\n                'detailed_stats': s.getvalue(),\n                'total_time': stats.total_execution_time,\n                'function_count': stats.modular_run,\n                'successful_functions': stats.modular_sug\n            }\n\n        print(\n            f\"\\n{stats.modular_run=}\"\n            f\"\\n{stats.modular_sug=}\"\n            f\"\\n{stats.modular_fatal_error=}\"\n            f\"\\n{total_coverage=}\"\n            f\"\\nTotal execution time: {stats.total_execution_time:.2f}s\"\n        )\n\n        if enable_profiling:\n            print(\"\\nProfiling Summary:\")\n            print(f\"{'=' * 50}\")\n            print(\"Top 10 time-consuming functions:\")\n            ps.print_stats(10)\n\n        analyzed_data = analyze_data(stats.__dict__)\n        return Result.ok(data=stats.__dict__, data_info=analyzed_data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.exit","title":"<code>exit()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.exit_main","title":"<code>exit_main(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef exit_main(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.fuction_runner","title":"<code>fuction_runner(function, function_data, args, kwargs, t0=0.0)</code>","text":"<p>parameters = function_data.get('params') modular_name = function_data.get('module_name') function_name = function_data.get('func_name') mod_function_name = f\"{modular_name}.{function_name}\"</p> <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n    \"\"\"\n    parameters = function_data.get('params')\n    modular_name = function_data.get('module_name')\n    function_name = function_data.get('func_name')\n    mod_function_name = f\"{modular_name}.{function_name}\"\n\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.get_all_mods","title":"<code>get_all_mods(working_dir='mods', path_to='./runtime')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.get_autocompletion_dict","title":"<code>get_autocompletion_dict()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_autocompletion_dict(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.get_function","title":"<code>get_function(name, **kwargs)</code>","text":"<p>Kwargs for _get_function     metadata:: return the registered function dictionary         stateless: (function_data, None), 0         stateful: (function_data, higher_order_function), 0     state::boolean         specification::str default app</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_function(self, name: Enum or tuple, **kwargs):\n    \"\"\"\n    Kwargs for _get_function\n        metadata:: return the registered function dictionary\n            stateless: (function_data, None), 0\n            stateful: (function_data, higher_order_function), 0\n        state::boolean\n            specification::str default app\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.get_mod","title":"<code>get_mod(name, spec='app')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.get_username","title":"<code>get_username(get_input=False, default='loot')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.hide_console","title":"<code>hide_console(*args, **kwargs)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def hide_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.inplace_load_instance","title":"<code>inplace_load_instance(mod_name, loc='toolboxv2.mods.', spec='app', save=True)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.load_all_mods_in_file","title":"<code>load_all_mods_in_file(working_dir='mods')</code>  <code>async</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def load_all_mods_in_file(self, working_dir=\"mods\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.load_mod","title":"<code>load_mod(mod_name, mlm='I', **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def load_mod(self, mod_name: str, mlm='I', **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.mod_online","title":"<code>mod_online(mod_name, installed=False)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def mod_online(self, mod_name, installed=False):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.print","title":"<code>print(text, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef print(text, *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.print_ok","title":"<code>print_ok()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def print_ok(self):\n    \"\"\"proxi attr\"\"\"\n    self.logger.info(\"OK\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.reload_mod","title":"<code>reload_mod(mod_name, spec='app', is_file=True, loc='toolboxv2.mods.')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.remove_mod","title":"<code>remove_mod(mod_name, spec='app', delete=True)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def remove_mod(self, mod_name, spec='app', delete=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.rrun_flows","title":"<code>rrun_flows(name, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def rrun_flows(self, name, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_a_from_sync","title":"<code>run_a_from_sync(function, *args)</code>","text":"<p>run a async fuction</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_a_from_sync(self, function, *args):\n    \"\"\"\n    run a async fuction\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_any","title":"<code>run_any(mod_function_name, backwords_compability_variabel_string_holder=None, get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n            get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n            kwargs_=None,\n            *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_bg_task","title":"<code>run_bg_task(task)</code>","text":"<p>run a async fuction</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_bg_task(self, task):\n    \"\"\"\n            run a async fuction\n            \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_bg_task_advanced","title":"<code>run_bg_task_advanced(task, *args, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_bg_task_advanced(self, task, *args, **kwargs):\n    \"\"\"\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_flows","title":"<code>run_flows(name, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_flows(self, name, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_function","title":"<code>run_function(mod_function_name, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_function(self, mod_function_name: Enum or tuple,\n                 tb_run_function_with_state=True,\n                 tb_run_with_specification='app',\n                 args_=None,\n                 kwargs_=None,\n                 *args,\n                 **kwargs) -&gt; Result:\n\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.run_http","title":"<code>run_http(mod_function_name, function_name=None, method='GET', args_=None, kwargs_=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>run a function remote via http / https</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None, method=\"GET\",\n                   args_=None,\n                   kwargs_=None,\n                   *args, **kwargs):\n    \"\"\"run a function remote via http / https\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.save_autocompletion_dict","title":"<code>save_autocompletion_dict()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_autocompletion_dict(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.save_exit","title":"<code>save_exit()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.save_initialized_module","title":"<code>save_initialized_module(tools_class, spec)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_initialized_module(self, tools_class, spec):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.save_instance","title":"<code>save_instance(instance, modular_id, spec='app', instance_type='file/application', tools_class=None)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.save_load","title":"<code>save_load(modname, spec='app')</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_load(self, modname, spec='app'):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.save_registry_as_enums","title":"<code>save_registry_as_enums(directory, filename)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_registry_as_enums(self, directory: str, filename: str):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.set_flows","title":"<code>set_flows(r)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def set_flows(self, r):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.set_logger","title":"<code>set_logger(debug=False)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def set_logger(self, debug=False):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.show_console","title":"<code>show_console(*args, **kwargs)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def show_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.sprint","title":"<code>sprint(text, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef sprint(text, *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.tb","title":"<code>tb(name=None, mod_name='', helper='', version=None, test=True, restrict_in_virtual_mode=False, api=False, initial=False, exit_f=False, test_only=False, memory_cache=False, file_cache=False, row=False, request_as_kwarg=False, state=None, level=0, memory_cache_max_size=100, memory_cache_ttl=300, samples=None, interface=None, pre_compute=None, post_compute=None, api_methods=None)</code>","text":"<p>A decorator for registering and configuring functions within a module.</p> <p>This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to register the function under. Defaults to the function's own name.</p> <code>None</code> <code>mod_name</code> <code>str</code> <p>The name of the module the function belongs to.</p> <code>''</code> <code>helper</code> <code>str</code> <p>A helper string providing additional information about the function.</p> <code>''</code> <code>version</code> <code>str or None</code> <p>The version of the function or module.</p> <code>None</code> <code>test</code> <code>bool</code> <p>Flag to indicate if the function is for testing purposes.</p> <code>True</code> <code>restrict_in_virtual_mode</code> <code>bool</code> <p>Flag to restrict the function in virtual mode.</p> <code>False</code> <code>api</code> <code>bool</code> <p>Flag to indicate if the function is part of an API.</p> <code>False</code> <code>initial</code> <code>bool</code> <p>Flag to indicate if the function should be executed at initialization.</p> <code>False</code> <code>exit_f</code> <code>bool</code> <p>Flag to indicate if the function should be executed at exit.</p> <code>False</code> <code>test_only</code> <code>bool</code> <p>Flag to indicate if the function should only be used for testing.</p> <code>False</code> <code>memory_cache</code> <code>bool</code> <p>Flag to enable memory caching for the function.</p> <code>False</code> <code>request_as_kwarg</code> <code>bool</code> <p>Flag to get request if the fuction is calld from api.</p> <code>False</code> <code>file_cache</code> <code>bool</code> <p>Flag to enable file caching for the function.</p> <code>False</code> <code>row</code> <code>bool</code> <p>rather to auto wrap the result in Result type default False means no row data aka result type</p> <code>False</code> <code>state</code> <code>bool or None</code> <p>Flag to indicate if the function maintains state.</p> <code>None</code> <code>level</code> <code>int</code> <p>The level of the function, used for prioritization or categorization.</p> <code>0</code> <code>memory_cache_max_size</code> <code>int</code> <p>Maximum size of the memory cache.</p> <code>100</code> <code>memory_cache_ttl</code> <code>int</code> <p>Time-to-live for the memory cache entries.</p> <code>300</code> <code>samples</code> <code>list or dict or None</code> <p>Samples or examples of function usage.</p> <code>None</code> <code>interface</code> <code>str</code> <p>The interface type for the function.</p> <code>None</code> <code>pre_compute</code> <code>callable</code> <p>A function to be called before the main function.</p> <code>None</code> <code>post_compute</code> <code>callable</code> <p>A function to be called after the main function.</p> <code>None</code> <code>api_methods</code> <code>list[str]</code> <p>default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>function</code> <p>The decorated function with additional processing and registration capabilities.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def tb(self, name=None,\n       mod_name: str = \"\",\n       helper: str = \"\",\n       version: str or None = None,\n       test: bool = True,\n       restrict_in_virtual_mode: bool = False,\n       api: bool = False,\n       initial: bool = False,\n       exit_f: bool = False,\n       test_only: bool = False,\n       memory_cache: bool = False,\n       file_cache: bool = False,\n       row=False,\n       request_as_kwarg: bool = False,\n       state: bool or None = None,\n       level: int = 0,\n       memory_cache_max_size: int = 100,\n       memory_cache_ttl: int = 300,\n       samples: list or dict or None = None,\n       interface: ToolBoxInterfaces or None or str = None,\n       pre_compute=None,\n       post_compute=None,\n       api_methods=None,\n       ):\n    \"\"\"\nA decorator for registering and configuring functions within a module.\n\nThis decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\nArgs:\n    name (str, optional): The name to register the function under. Defaults to the function's own name.\n    mod_name (str, optional): The name of the module the function belongs to.\n    helper (str, optional): A helper string providing additional information about the function.\n    version (str or None, optional): The version of the function or module.\n    test (bool, optional): Flag to indicate if the function is for testing purposes.\n    restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n    api (bool, optional): Flag to indicate if the function is part of an API.\n    initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n    exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n    test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n    memory_cache (bool, optional): Flag to enable memory caching for the function.\n    request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n    file_cache (bool, optional): Flag to enable file caching for the function.\n    row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n    state (bool or None, optional): Flag to indicate if the function maintains state.\n    level (int, optional): The level of the function, used for prioritization or categorization.\n    memory_cache_max_size (int, optional): Maximum size of the memory cache.\n    memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n    samples (list or dict or None, optional): Samples or examples of function usage.\n    interface (str, optional): The interface type for the function.\n    pre_compute (callable, optional): A function to be called before the main function.\n    post_compute (callable, optional): A function to be called after the main function.\n    api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\nReturns:\n    function: The decorated function with additional processing and registration capabilities.\n\"\"\"\n    if interface is None:\n        interface = \"tb\"\n    if test_only and 'test' not in self.id:\n        return lambda *args, **kwargs: args\n    return self._create_decorator(interface,\n                                  name,\n                                  mod_name,\n                                  level=level,\n                                  restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                  helper=helper,\n                                  api=api,\n                                  version=version,\n                                  initial=initial,\n                                  exit_f=exit_f,\n                                  test=test,\n                                  samples=samples,\n                                  state=state,\n                                  pre_compute=pre_compute,\n                                  post_compute=post_compute,\n                                  memory_cache=memory_cache,\n                                  file_cache=file_cache,\n                                  row=row,\n                                  request_as_kwarg=request_as_kwarg,\n                                  memory_cache_max_size=memory_cache_max_size,\n                                  memory_cache_ttl=memory_cache_ttl)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.wait_for_bg_tasks","title":"<code>wait_for_bg_tasks(timeout=None)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def wait_for_bg_tasks(self, timeout=None):\n    \"\"\"\n    proxi attr\n    \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.watch_mod","title":"<code>watch_mod(mod_name, spec='app', loc='toolboxv2.mods.', use_thread=True, path_name=None)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.AppType.web_context","title":"<code>web_context()</code>","text":"<p>returns the build index ( toolbox web component )</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def web_context(self) -&gt; str:\n    \"\"\"returns the build index ( toolbox web component )\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainTool","title":"<code>MainTool</code>","text":"Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>class MainTool:\n    toolID: str = \"\"\n    # app = None\n    interface = None\n    spec = \"app\"\n    name = \"\"\n    color = \"Bold\"\n    stuf = False\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.__storedargs = args, kwargs\n        self.tools = kwargs.get(\"tool\", {})\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n        if \"on_exit\" in kwargs and isinstance(kwargs.get(\"on_exit\"), Callable):\n            self.on_exit =self.app.tb(\n                mod_name=self.name,\n                name=kwargs.get(\"on_exit\").__name__,\n                version=self.version if hasattr(self, 'version') else \"0.0.0\",\n            )(kwargs.get(\"on_exit\"))\n        self.async_initialized = False\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    pass\n                else:\n                    self.todo()\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\"Error loading mod {self.name} {e}\")\n                if self.app.debug:\n                    import traceback\n                    traceback.print_exc()\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n\n    async def __ainit__(self, *args, **kwargs):\n        self.version = kwargs[\"v\"]\n        self.tools = kwargs.get(\"tool\", {})\n        self.name = kwargs[\"name\"]\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", None))\n        if not hasattr(self, 'config'):\n            self.config = {}\n        self.user = None\n        self.description = \"A toolbox mod\" if kwargs.get(\"description\") is None else kwargs.get(\"description\")\n        if MainTool.interface is None:\n            MainTool.interface = self.app.interface_type\n        # Result.default(self.app.interface)\n\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    await self.todo()\n                else:\n                    pass\n                await asyncio.sleep(0.1)\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\"Error loading mod {self.name} {e}\")\n                if self.app.debug:\n                    import traceback\n                    traceback.print_exc()\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n        self.app.print(f\"TOOL : {self.spec}.{self.name} online\")\n\n\n\n    @property\n    def app(self):\n        return get_app(\n            from_=f\"{self.spec}.{self.name}|{self.toolID if self.toolID else '*' + MainTool.toolID} {self.interface if self.interface else MainTool.interface}\")\n\n    @app.setter\n    def app(self, v):\n        raise PermissionError(f\"You cannot set the App Instance! {v=}\")\n\n    @staticmethod\n    def return_result(error: ToolBoxError = ToolBoxError.none,\n                      exec_code: int = 0,\n                      help_text: str = \"\",\n                      data_info=None,\n                      data=None,\n                      data_to=None):\n\n        if data_to is None:\n            data_to = MainTool.interface if MainTool.interface is not None else ToolBoxInterfaces.cli\n\n        if data is None:\n            data = {}\n\n        if data_info is None:\n            data_info = {}\n\n        return Result(\n            error,\n            ToolBoxResult(data_info=data_info, data=data, data_to=data_to),\n            ToolBoxInfo(exec_code=exec_code, help_text=help_text)\n        )\n\n    def print(self, message, end=\"\\n\", **kwargs):\n        if self.stuf:\n            return\n\n        self.app.print(Style.style_dic[self.color] + self.name + Style.style_dic[\"END\"] + \":\", message, end=end,\n                       **kwargs)\n\n    def add_str_to_config(self, command):\n        if len(command) != 2:\n            self.logger.error('Invalid command must be key value')\n            return False\n        self.config[command[0]] = command[1]\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n\n    def get_version(self) -&gt; str:\n        \"\"\"\"Returns the version\"\"\"\n        return self.version\n\n    async def get_user(self, username: str) -&gt; Result:\n        return await self.app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME, username=username, get_results=True)\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainTool.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.__storedargs = args, kwargs\n    self.tools = kwargs.get(\"tool\", {})\n    self.logger = kwargs.get(\"logs\", get_logger())\n    self.color = kwargs.get(\"color\", \"WHITE\")\n    self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n    if \"on_exit\" in kwargs and isinstance(kwargs.get(\"on_exit\"), Callable):\n        self.on_exit =self.app.tb(\n            mod_name=self.name,\n            name=kwargs.get(\"on_exit\").__name__,\n            version=self.version if hasattr(self, 'version') else \"0.0.0\",\n        )(kwargs.get(\"on_exit\"))\n    self.async_initialized = False\n    if self.todo:\n        try:\n            if inspect.iscoroutinefunction(self.todo):\n                pass\n            else:\n                self.todo()\n            get_logger().info(f\"{self.name} on load suspended\")\n        except Exception as e:\n            get_logger().error(f\"Error loading mod {self.name} {e}\")\n            if self.app.debug:\n                import traceback\n                traceback.print_exc()\n    else:\n        get_logger().info(f\"{self.name} no load require\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainTool.__initobj","title":"<code>__initobj()</code>  <code>async</code>","text":"<p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainTool.get_version","title":"<code>get_version()</code>","text":"<p>\"Returns the version</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\"Returns the version\"\"\"\n    return self.version\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainTool.webInstall","title":"<code>webInstall(user_instance, construct_render)</code>","text":"<p>\"Returns a web installer for the given user instance and construct render template</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def webInstall(self, user_instance, construct_render) -&gt; str:\n    \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainToolType","title":"<code>MainToolType</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class MainToolType:\n    toolID: str\n    app: A\n    interface: ToolBoxInterfaces\n    spec: str\n\n    version: str\n    tools: dict  # legacy\n    name: str\n    logger: logging\n    color: str\n    todo: Callable\n    _on_exit: Callable\n    stuf: bool\n    config: dict\n    user: U | None\n    description: str\n\n    @staticmethod\n    def return_result(error: ToolBoxError = ToolBoxError.none,\n                      exec_code: int = 0,\n                      help_text: str = \"\",\n                      data_info=None,\n                      data=None,\n                      data_to=None) -&gt; Result:\n        \"\"\"proxi attr\"\"\"\n\n    def load(self):\n        \"\"\"proxi attr\"\"\"\n\n    def print(self, message, end=\"\\n\", **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def add_str_to_config(self, command):\n        if len(command) != 2:\n            self.logger.error('Invalid command must be key value')\n            return False\n        self.config[command[0]] = command[1]\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n\n    async def get_user(self, username: str) -&gt; Result:\n        return self.app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME, username=username, get_results=True)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainToolType.load","title":"<code>load()</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def load(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainToolType.print","title":"<code>print(message, end='\\n', **kwargs)</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def print(self, message, end=\"\\n\", **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainToolType.return_result","title":"<code>return_result(error=ToolBoxError.none, exec_code=0, help_text='', data_info=None, data=None, data_to=None)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef return_result(error: ToolBoxError = ToolBoxError.none,\n                  exec_code: int = 0,\n                  help_text: str = \"\",\n                  data_info=None,\n                  data=None,\n                  data_to=None) -&gt; Result:\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.MainToolType.webInstall","title":"<code>webInstall(user_instance, construct_render)</code>","text":"<p>\"Returns a web installer for the given user instance and construct render template</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def webInstall(self, user_instance, construct_render) -&gt; str:\n    \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result","title":"<code>Result</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class Result:\n    _task = None\n    def __init__(self,\n                 error: ToolBoxError,\n                 result: ToolBoxResult,\n                 info: ToolBoxInfo,\n                 origin: Any | None = None,\n                 ):\n        self.error: ToolBoxError = error\n        self.result: ToolBoxResult = result\n        self.info: ToolBoxInfo = info\n        self.origin = origin\n\n    def as_result(self):\n        return self\n\n    def as_dict(self):\n        return {\n            \"error\":self.error.value if isinstance(self.error, Enum) else self.error,\n        \"result\" : {\n            \"data_to\":self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n            \"data_info\":self.result.data_info,\n            \"data\":self.result.data,\n            \"data_type\":self.result.data_type\n        } if self.result else None,\n        \"info\" : {\n            \"exec_code\" : self.info.exec_code,  # exec_code umwandel in http resposn codes\n        \"help_text\" : self.info.help_text\n        } if self.info else None,\n        \"origin\" : self.origin\n        }\n\n    def set_origin(self, origin):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = origin\n        return self\n\n    def set_dir_origin(self, name, extras=\"assets/\"):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = f\"mods/{name}/{extras}\"\n        return self\n\n    def is_error(self):\n        if _test_is_result(self.result.data):\n            return self.result.data.is_error()\n        if self.error == ToolBoxError.none:\n            return False\n        if self.info.exec_code == 0:\n            return False\n        if self.info.exec_code == 200:\n            return False\n        return True\n\n    def is_ok(self):\n        return not self.is_error()\n\n    def is_data(self):\n        return self.result.data is not None\n\n    def to_api_result(self):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=self.error.value if isinstance(self.error, Enum) else self.error,\n            result=ToolBoxResultBM(\n                data_to=self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n                data_info=self.result.data_info,\n                data=self.result.data,\n                data_type=self.result.data_type\n            ) if self.result else None,\n            info=ToolBoxInfoBM(\n                exec_code=self.info.exec_code,  # exec_code umwandel in http resposn codes\n                help_text=self.info.help_text\n            ) if self.info else None,\n            origin=self.origin\n        )\n\n    def task(self, task):\n        self._task = task\n        return self\n\n    @staticmethod\n    def result_from_dict(error: str, result: dict, info: dict, origin: list or None or str):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=error if isinstance(error, Enum) else error,\n            result=ToolBoxResultBM(\n                data_to=result.get('data_to') if isinstance(result.get('data_to'), Enum) else result.get('data_to'),\n                data_info=result.get('data_info', '404'),\n                data=result.get('data'),\n                data_type=result.get('data_type', '404'),\n            ) if result else ToolBoxResultBM(\n                data_to=ToolBoxInterfaces.cli.value,\n                data_info='',\n                data='404',\n                data_type='404',\n            ),\n            info=ToolBoxInfoBM(\n                exec_code=info.get('exec_code', 404),\n                help_text=info.get('help_text', '404')\n            ) if info else ToolBoxInfoBM(\n                exec_code=404,\n                help_text='404'\n            ),\n            origin=origin\n        ).as_result()\n\n    @classmethod\n    def stream(cls,\n               stream_generator: Any,  # Renamed from source for clarity\n               content_type: str = \"text/event-stream\",  # Default to SSE\n               headers: Union[dict, None] = None,\n               info: str = \"OK\",\n               interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n               cleanup_func: Union[\n                   Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None):\n        \"\"\"\n        Create a streaming response Result. Handles SSE and other stream types.\n\n        Args:\n            stream_generator: Any stream source (async generator, sync generator, iterable, or single item).\n            content_type: Content-Type header (default: text/event-stream for SSE).\n            headers: Additional HTTP headers for the response.\n            info: Help text for the result.\n            interface: Interface to send data to.\n            cleanup_func: Optional function for cleanup.\n\n        Returns:\n            A Result object configured for streaming.\n        \"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        final_generator: AsyncGenerator[str, None]\n\n        if content_type == \"text/event-stream\":\n            # For SSE, always use SSEGenerator.create_sse_stream to wrap the source.\n            # SSEGenerator.create_sse_stream handles various types of stream_generator internally.\n            final_generator = SSEGenerator.create_sse_stream(source=stream_generator, cleanup_func=cleanup_func)\n\n            # Standard SSE headers for the HTTP response itself\n            # These will be stored in the Result object. Rust side decides how to use them.\n            standard_sse_headers = {\n                \"Cache-Control\": \"no-cache\",  # SSE specific\n                \"Connection\": \"keep-alive\",  # SSE specific\n                \"X-Accel-Buffering\": \"no\",  # Useful for proxies with SSE\n                # Content-Type is implicitly text/event-stream, will be in streaming_data below\n            }\n            all_response_headers = standard_sse_headers.copy()\n            if headers:\n                all_response_headers.update(headers)\n        else:\n            # For non-SSE streams.\n            # If stream_generator is sync, wrap it to be async.\n            # If already async or single item, it will be handled.\n            # Rust's stream_generator in ToolboxClient seems to handle both sync/async Python generators.\n            # For consistency with how SSEGenerator does it, we can wrap sync ones.\n            if inspect.isgenerator(stream_generator) or \\\n                (not isinstance(stream_generator, str) and hasattr(stream_generator, '__iter__')):\n                final_generator = SSEGenerator.wrap_sync_generator(stream_generator)  # Simple async wrapper\n            elif inspect.isasyncgen(stream_generator):\n                final_generator = stream_generator\n            else:  # Single item or string\n                async def _single_item_gen():\n                    yield stream_generator\n\n                final_generator = _single_item_gen()\n            all_response_headers = headers if headers else {}\n\n        # Prepare streaming data to be stored in the Result object\n        streaming_data = {\n            \"type\": \"stream\",  # Indicator for Rust side\n            \"generator\": final_generator,\n            \"content_type\": content_type,  # Let Rust know the intended content type\n            \"headers\": all_response_headers  # Intended HTTP headers for the overall response\n        }\n\n        result_payload = ToolBoxResult(\n            data_to=interface,\n            data=streaming_data,\n            data_info=\"Streaming response\" if content_type != \"text/event-stream\" else \"SSE Event Stream\",\n            data_type=\"stream\"  # Generic type for Rust to identify it needs to stream from 'generator'\n        )\n\n        return cls(error=error, info=info_obj, result=result_payload)\n\n    @classmethod\n    def sse(cls,\n            stream_generator: Any,\n            info: str = \"OK\",\n            interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n            cleanup_func: Union[\n                Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None,\n            # http_headers: Optional[dict] = None # If we want to allow overriding default SSE HTTP headers\n            ):\n        \"\"\"\n        Create an Server-Sent Events (SSE) streaming response Result.\n\n        Args:\n            stream_generator: A source yielding individual data items. This can be an\n                              async generator, sync generator, iterable, or a single item.\n                              Each item will be formatted as an SSE event.\n            info: Optional help text for the Result.\n            interface: Optional ToolBoxInterface to target.\n            cleanup_func: Optional cleanup function to run when the stream ends or is cancelled.\n            #http_headers: Optional dictionary of custom HTTP headers for the SSE response.\n\n        Returns:\n            A Result object configured for SSE streaming.\n        \"\"\"\n        # Result.stream will handle calling SSEGenerator.create_sse_stream\n        # and setting appropriate default headers for SSE when content_type is \"text/event-stream\".\n        return cls.stream(\n            stream_generator=stream_generator,\n            content_type=\"text/event-stream\",\n            # headers=http_headers, # Pass if we add http_headers param\n            info=info,\n            interface=interface,\n            cleanup_func=cleanup_func\n        )\n\n    @classmethod\n    def default(cls, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=-1, help_text=\"\")\n        result = ToolBoxResult(data_to=interface)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None):\n        \"\"\"Create a JSON response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code or exec_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=data,\n            data_info=\"JSON response\",\n            data_type=\"json\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n        \"\"\"Create a text response Result with specific content type.\"\"\"\n        if headers is not None:\n            return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=text_data,\n            data_info=\"Text response\",\n            data_type=content_type\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n               interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a binary data response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        # Create a dictionary with binary data and metadata\n        binary_data = {\n            \"data\": data,\n            \"content_type\": content_type,\n            \"filename\": download_name\n        }\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=binary_data,\n            data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n            data_type=\"binary\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def file(cls, data, filename, content_type=None, info=\"OK\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a file download response Result.\n\n        Args:\n            data: File data as bytes or base64 string\n            filename: Name of the file for download\n            content_type: MIME type of the file (auto-detected if None)\n            info: Response info text\n            interface: Target interface\n\n        Returns:\n            Result object configured for file download\n        \"\"\"\n        import base64\n        import mimetypes\n\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=200, help_text=info)\n\n        # Auto-detect content type if not provided\n        if content_type is None:\n            content_type, _ = mimetypes.guess_type(filename)\n            if content_type is None:\n                content_type = \"application/octet-stream\"\n\n        # Ensure data is base64 encoded string (as expected by Rust server)\n        if isinstance(data, bytes):\n            base64_data = base64.b64encode(data).decode('utf-8')\n        elif isinstance(data, str):\n            # Assume it's already base64 encoded\n            base64_data = data\n        else:\n            raise ValueError(\"File data must be bytes or base64 string\")\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=base64_data,  # Rust expects base64 string for \"file\" type\n            data_info=f\"File download: {filename}\",\n            data_type=\"file\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a redirect response.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=url,\n            data_info=\"Redirect response\",\n            data_type=\"redirect\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def ok(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def html(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.remote, data_type=\"html\",status=200, headers=None, row=False):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=status, help_text=info)\n        from ...utils.system.getting_and_closing_app import get_app\n\n        if not row and not '\"&lt;div class=\"main-content\"\"' in data:\n            data = f'&lt;div class=\"main-content frosted-glass\"&gt;{data}&lt;div&gt;'\n        if not row and not get_app().web_context() in data:\n            data = get_app().web_context() + data\n\n        if isinstance(headers, dict):\n            result = ToolBoxResult(data_to=interface, data={'html':data,'headers':headers}, data_info=data_info,\n                                   data_type=\"special_html\")\n        else:\n            result = ToolBoxResult(data_to=interface, data=data, data_info=data_info,\n                                   data_type=data_type if data_type is not None else type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def future(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.future):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=\"future\")\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def custom_error(cls, data=None, data_info=\"\", info=\"\", exec_code=-1, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def error(cls, data=None, data_info=\"\", info=\"\", exec_code=450, interface=ToolBoxInterfaces.remote):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_user_error(cls, info=\"\", exec_code=-3, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.input_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_internal_error(cls, info=\"\", exec_code=-2, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.internal_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    def print(self, show=True, show_data=True, prifix=\"\"):\n        data = '\\n' + f\"{((prifix + 'Data: ' + str(self.result.data) if self.result.data is not None else 'NO Data') if not isinstance(self.result.data, Result) else self.result.data.print(show=False, show_data=show_data, prifix=prifix + '-')) if show_data else 'Data: private'}\"\n        origin = '\\n' + f\"{prifix + 'Origin: ' + str(self.origin) if self.origin is not None else 'NO Origin'}\"\n        text = (f\"Function Exec code: {self.info.exec_code}\"\n                f\"\\n{prifix}Info's:\"\n                f\" {self.info.help_text} {'&lt;|&gt; ' + str(self.result.data_info) if self.result.data_info is not None else ''}\"\n                f\"{origin}{data if not data.endswith('NO Data') else ''}\")\n        if not show:\n            return text\n        print(\"\\n======== Result ========\\n\" + text + \"\\n------- EndOfD -------\")\n        return self\n\n    def log(self, show_data=True, prifix=\"\"):\n        from toolboxv2 import get_logger\n        get_logger().debug(self.print(show=False, show_data=show_data, prifix=prifix).replace(\"\\n\", \" - \"))\n        return self\n\n    def __str__(self):\n        return self.print(show=False, show_data=True)\n\n    def get(self, key=None, default=None):\n        data = self.result.data\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    async def aget(self, key=None, default=None):\n        if asyncio.isfuture(self.result.data) or asyncio.iscoroutine(self.result.data) or (\n            isinstance(self.result.data_to, Enum) and self.result.data_to.name == ToolBoxInterfaces.future.name):\n            data = await self.result.data\n        else:\n            data = self.get(key=None, default=None)\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    def lazy_return(self, _=0, data=None, **kwargs):\n        flags = ['raise', 'logg', 'user', 'intern']\n        flag = flags[_] if isinstance(_, int) else _\n        if self.info.exec_code == 0:\n            return self if data is None else data if _test_is_result(data) else self.ok(data=data, **kwargs)\n        if flag == 'raise':\n            raise ValueError(self.print(show=False))\n        if flag == 'logg':\n            from .. import get_logger\n            get_logger().error(self.print(show=False))\n\n        if flag == 'user':\n            return self if data is None else data if _test_is_result(data) else self.default_user_error(data=data,\n                                                                                                        **kwargs)\n        if flag == 'intern':\n            return self if data is None else data if _test_is_result(data) else self.default_internal_error(data=data,\n                                                                                                            **kwargs)\n\n        return self if data is None else data if _test_is_result(data) else self.custom_error(data=data, **kwargs)\n\n    @property\n    def bg_task(self):\n        return self._task\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.binary","title":"<code>binary(data, content_type='application/octet-stream', download_name=None, info='OK', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a binary data response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n           interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a binary data response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    # Create a dictionary with binary data and metadata\n    binary_data = {\n        \"data\": data,\n        \"content_type\": content_type,\n        \"filename\": download_name\n    }\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=binary_data,\n        data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n        data_type=\"binary\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.file","title":"<code>file(data, filename, content_type=None, info='OK', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a file download response Result.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>File data as bytes or base64 string</p> required <code>filename</code> <p>Name of the file for download</p> required <code>content_type</code> <p>MIME type of the file (auto-detected if None)</p> <code>None</code> <code>info</code> <p>Response info text</p> <code>'OK'</code> <code>interface</code> <p>Target interface</p> <code>remote</code> <p>Returns:</p> Type Description <p>Result object configured for file download</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef file(cls, data, filename, content_type=None, info=\"OK\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a file download response Result.\n\n    Args:\n        data: File data as bytes or base64 string\n        filename: Name of the file for download\n        content_type: MIME type of the file (auto-detected if None)\n        info: Response info text\n        interface: Target interface\n\n    Returns:\n        Result object configured for file download\n    \"\"\"\n    import base64\n    import mimetypes\n\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=200, help_text=info)\n\n    # Auto-detect content type if not provided\n    if content_type is None:\n        content_type, _ = mimetypes.guess_type(filename)\n        if content_type is None:\n            content_type = \"application/octet-stream\"\n\n    # Ensure data is base64 encoded string (as expected by Rust server)\n    if isinstance(data, bytes):\n        base64_data = base64.b64encode(data).decode('utf-8')\n    elif isinstance(data, str):\n        # Assume it's already base64 encoded\n        base64_data = data\n    else:\n        raise ValueError(\"File data must be bytes or base64 string\")\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=base64_data,  # Rust expects base64 string for \"file\" type\n        data_info=f\"File download: {filename}\",\n        data_type=\"file\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.json","title":"<code>json(data, info='OK', interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None)</code>  <code>classmethod</code>","text":"<p>Create a JSON response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None):\n    \"\"\"Create a JSON response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code or exec_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=data,\n        data_info=\"JSON response\",\n        data_type=\"json\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.redirect","title":"<code>redirect(url, status_code=302, info='Redirect', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a redirect response.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a redirect response.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=url,\n        data_info=\"Redirect response\",\n        data_type=\"redirect\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.sse","title":"<code>sse(stream_generator, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code>  <code>classmethod</code>","text":"<p>Create an Server-Sent Events (SSE) streaming response Result.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <code>Any</code> <p>A source yielding individual data items. This can be an               async generator, sync generator, iterable, or a single item.               Each item will be formatted as an SSE event.</p> required <code>info</code> <code>str</code> <p>Optional help text for the Result.</p> <code>'OK'</code> <code>interface</code> <code>ToolBoxInterfaces</code> <p>Optional ToolBoxInterface to target.</p> <code>remote</code> <code>cleanup_func</code> <code>Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None]</code> <p>Optional cleanup function to run when the stream ends or is cancelled.</p> <code>None</code> <code>#http_headers</code> <p>Optional dictionary of custom HTTP headers for the SSE response.</p> required <p>Returns:</p> Type Description <p>A Result object configured for SSE streaming.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef sse(cls,\n        stream_generator: Any,\n        info: str = \"OK\",\n        interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n        cleanup_func: Union[\n            Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None,\n        # http_headers: Optional[dict] = None # If we want to allow overriding default SSE HTTP headers\n        ):\n    \"\"\"\n    Create an Server-Sent Events (SSE) streaming response Result.\n\n    Args:\n        stream_generator: A source yielding individual data items. This can be an\n                          async generator, sync generator, iterable, or a single item.\n                          Each item will be formatted as an SSE event.\n        info: Optional help text for the Result.\n        interface: Optional ToolBoxInterface to target.\n        cleanup_func: Optional cleanup function to run when the stream ends or is cancelled.\n        #http_headers: Optional dictionary of custom HTTP headers for the SSE response.\n\n    Returns:\n        A Result object configured for SSE streaming.\n    \"\"\"\n    # Result.stream will handle calling SSEGenerator.create_sse_stream\n    # and setting appropriate default headers for SSE when content_type is \"text/event-stream\".\n    return cls.stream(\n        stream_generator=stream_generator,\n        content_type=\"text/event-stream\",\n        # headers=http_headers, # Pass if we add http_headers param\n        info=info,\n        interface=interface,\n        cleanup_func=cleanup_func\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.stream","title":"<code>stream(stream_generator, content_type='text/event-stream', headers=None, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code>  <code>classmethod</code>","text":"<p>Create a streaming response Result. Handles SSE and other stream types.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <code>Any</code> <p>Any stream source (async generator, sync generator, iterable, or single item).</p> required <code>content_type</code> <code>str</code> <p>Content-Type header (default: text/event-stream for SSE).</p> <code>'text/event-stream'</code> <code>headers</code> <code>Union[dict, None]</code> <p>Additional HTTP headers for the response.</p> <code>None</code> <code>info</code> <code>str</code> <p>Help text for the result.</p> <code>'OK'</code> <code>interface</code> <code>ToolBoxInterfaces</code> <p>Interface to send data to.</p> <code>remote</code> <code>cleanup_func</code> <code>Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None]</code> <p>Optional function for cleanup.</p> <code>None</code> <p>Returns:</p> Type Description <p>A Result object configured for streaming.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef stream(cls,\n           stream_generator: Any,  # Renamed from source for clarity\n           content_type: str = \"text/event-stream\",  # Default to SSE\n           headers: Union[dict, None] = None,\n           info: str = \"OK\",\n           interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n           cleanup_func: Union[\n               Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None):\n    \"\"\"\n    Create a streaming response Result. Handles SSE and other stream types.\n\n    Args:\n        stream_generator: Any stream source (async generator, sync generator, iterable, or single item).\n        content_type: Content-Type header (default: text/event-stream for SSE).\n        headers: Additional HTTP headers for the response.\n        info: Help text for the result.\n        interface: Interface to send data to.\n        cleanup_func: Optional function for cleanup.\n\n    Returns:\n        A Result object configured for streaming.\n    \"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    final_generator: AsyncGenerator[str, None]\n\n    if content_type == \"text/event-stream\":\n        # For SSE, always use SSEGenerator.create_sse_stream to wrap the source.\n        # SSEGenerator.create_sse_stream handles various types of stream_generator internally.\n        final_generator = SSEGenerator.create_sse_stream(source=stream_generator, cleanup_func=cleanup_func)\n\n        # Standard SSE headers for the HTTP response itself\n        # These will be stored in the Result object. Rust side decides how to use them.\n        standard_sse_headers = {\n            \"Cache-Control\": \"no-cache\",  # SSE specific\n            \"Connection\": \"keep-alive\",  # SSE specific\n            \"X-Accel-Buffering\": \"no\",  # Useful for proxies with SSE\n            # Content-Type is implicitly text/event-stream, will be in streaming_data below\n        }\n        all_response_headers = standard_sse_headers.copy()\n        if headers:\n            all_response_headers.update(headers)\n    else:\n        # For non-SSE streams.\n        # If stream_generator is sync, wrap it to be async.\n        # If already async or single item, it will be handled.\n        # Rust's stream_generator in ToolboxClient seems to handle both sync/async Python generators.\n        # For consistency with how SSEGenerator does it, we can wrap sync ones.\n        if inspect.isgenerator(stream_generator) or \\\n            (not isinstance(stream_generator, str) and hasattr(stream_generator, '__iter__')):\n            final_generator = SSEGenerator.wrap_sync_generator(stream_generator)  # Simple async wrapper\n        elif inspect.isasyncgen(stream_generator):\n            final_generator = stream_generator\n        else:  # Single item or string\n            async def _single_item_gen():\n                yield stream_generator\n\n            final_generator = _single_item_gen()\n        all_response_headers = headers if headers else {}\n\n    # Prepare streaming data to be stored in the Result object\n    streaming_data = {\n        \"type\": \"stream\",  # Indicator for Rust side\n        \"generator\": final_generator,\n        \"content_type\": content_type,  # Let Rust know the intended content type\n        \"headers\": all_response_headers  # Intended HTTP headers for the overall response\n    }\n\n    result_payload = ToolBoxResult(\n        data_to=interface,\n        data=streaming_data,\n        data_info=\"Streaming response\" if content_type != \"text/event-stream\" else \"SSE Event Stream\",\n        data_type=\"stream\"  # Generic type for Rust to identify it needs to stream from 'generator'\n    )\n\n    return cls(error=error, info=info_obj, result=result_payload)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.Result.text","title":"<code>text(text_data, content_type='text/plain', exec_code=None, status=200, info='OK', interface=ToolBoxInterfaces.remote, headers=None)</code>  <code>classmethod</code>","text":"<p>Create a text response Result with specific content type.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n    \"\"\"Create a text response Result with specific content type.\"\"\"\n    if headers is not None:\n        return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=text_data,\n        data_info=\"Text response\",\n        data_type=content_type\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.all_functions_enums","title":"<code>all_functions_enums</code>","text":"<p>Automatic generated by ToolBox v = 0.1.21</p>"},{"location":"toolboxv2/#toolboxv2.utils.system.api","title":"<code>api</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.api.build_cargo_project","title":"<code>build_cargo_project(debug=False)</code>","text":"<p>Build the Cargo project, optionally in debug mode.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def build_cargo_project(debug=False):\n    \"\"\"Build the Cargo project, optionally in debug mode.\"\"\"\n    mode = \"debug\" if debug else \"release\"\n    args = [\"cargo\", \"build\"]\n    if not debug:\n        args.append(\"--release\")\n\n    print(f\"Building in {mode} mode...\")\n    try:\n        subprocess.run(args, cwd=os.path.join(\".\", \"src-core\"), check=True)\n        exe_path = get_executable_name_with_extension()\n        if exe_path:\n            bin_dir = tb_root_dir / \"bin\"\n            bin_dir.mkdir(exist_ok=True)\n            exe_path = Path(exe_path)\n            try:\n                shutil.copy(exe_path, bin_dir / exe_path.name)\n            except Exception as e:\n                bin_dir = tb_root_dir / \"ubin\"\n                bin_dir.mkdir(exist_ok=True)\n                (bin_dir / exe_path.name).unlink(missing_ok=True)\n                try:\n                    shutil.copy(exe_path, bin_dir / exe_path.name)\n                except Exception as e:\n                    print(f\"Failed to copy executable: {e}\")\n            print(f\"Copied executable to '{bin_dir.resolve()}'\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Cargo build failed: {e}\")\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.check_cargo_installed","title":"<code>check_cargo_installed()</code>","text":"<p>Check if Cargo (Rust package manager) is installed on the system.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def check_cargo_installed():\n    \"\"\"Check if Cargo (Rust package manager) is installed on the system.\"\"\"\n    try:\n        subprocess.run([\"cargo\", \"--version\"], check=True, capture_output=True)\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.cleanup_build_files","title":"<code>cleanup_build_files()</code>","text":"<p>Cleans up build files.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def cleanup_build_files():\n    \"\"\"Cleans up build files.\"\"\"\n    src_core_path = os.path.join(\".\", \"src-core\")\n    target_path = os.path.join(src_core_path, \"target\")\n\n    if os.path.exists(target_path):\n        try:\n            print(f\"Cleaning up build files in {target_path}...\")\n            # First try using cargo clean\n            try:\n                subprocess.run([\"cargo\", \"clean\"], cwd=src_core_path, check=True)\n                print(\"Successfully cleaned up build files with cargo clean\")\n            except subprocess.CalledProcessError:\n                # If cargo clean fails, manually remove directories\n                print(\"Cargo clean failed, manually removing build directories...\")\n                for item in os.listdir(target_path):\n                    item_path = os.path.join(target_path, item)\n                    if os.path.isdir(item_path) and item != \".rustc_info.json\":\n                        shutil.rmtree(item_path)\n                        print(f\"Removed {item_path}\")\n            return True\n        except Exception as e:\n            print(f\"Failed to clean up build files: {e}\")\n            return False\n    else:\n        print(f\"Build directory {target_path} not found\")\n        return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.detect_os_and_arch","title":"<code>detect_os_and_arch()</code>","text":"<p>Detect the current operating system and architecture.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def detect_os_and_arch():\n    \"\"\"Detect the current operating system and architecture.\"\"\"\n    current_os = platform.system().lower()  # e.g., 'windows', 'linux', 'darwin'\n    machine = platform.machine().lower()  # e.g., 'x86_64', 'amd64'\n    return current_os, machine\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.download_executable","title":"<code>download_executable(url, file_name)</code>","text":"<p>Attempt to download the executable from the provided URL.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def download_executable(url, file_name):\n    \"\"\"Attempt to download the executable from the provided URL.\"\"\"\n    try:\n        import requests\n    except ImportError:\n        print(\"The 'requests' library is required. Please install it via pip install requests\")\n        sys.exit(1)\n\n    print(f\"Attempting to download executable from {url}...\")\n    try:\n        response = requests.get(url, stream=True)\n    except Exception as e:\n        print(f\"Download error: {e}\")\n        return None\n\n    if response.status_code == 200:\n        with open(file_name, \"wb\") as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:\n                    f.write(chunk)\n        # Make the file executable on non-Windows systems\n        if platform.system().lower() != \"windows\":\n            os.chmod(file_name, 0o755)\n        return file_name\n    else:\n        print(\"Download failed. Status code:\", response.status_code)\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.find_highest_zip_version","title":"<code>find_highest_zip_version(name_filter, app_version=None, root_dir='mods_sto', version_only=False)</code>","text":"<p>Findet die h\u00f6chste verf\u00fcgbare ZIP-Version in einem Verzeichnis basierend auf einem Namensfilter.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Wurzelverzeichnis f\u00fcr die Suche</p> <code>'mods_sto'</code> <code>name_filter</code> <code>str</code> <p>Namensfilter f\u00fcr die ZIP-Dateien</p> required <code>app_version</code> <code>str</code> <p>Aktuelle App-Version f\u00fcr Kompatibilit\u00e4tspr\u00fcfung</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Pfad zur ZIP-Datei mit der h\u00f6chsten Version oder None wenn keine gefunden</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def find_highest_zip_version(name_filter: str, app_version: str = None, root_dir: str = \"mods_sto\", version_only=False) -&gt; str:\n    \"\"\"\n    Findet die h\u00f6chste verf\u00fcgbare ZIP-Version in einem Verzeichnis basierend auf einem Namensfilter.\n\n    Args:\n        root_dir (str): Wurzelverzeichnis f\u00fcr die Suche\n        name_filter (str): Namensfilter f\u00fcr die ZIP-Dateien\n        app_version (str, optional): Aktuelle App-Version f\u00fcr Kompatibilit\u00e4tspr\u00fcfung\n\n    Returns:\n        str: Pfad zur ZIP-Datei mit der h\u00f6chsten Version oder None wenn keine gefunden\n    \"\"\"\n\n    # Kompiliere den Regex-Pattern f\u00fcr die Dateinamen\n    pattern = fr\"{name_filter}&amp;v[0-9.]+\u00a7([0-9.]+)\\.zip$\"\n\n    highest_version = None\n    highest_version_file = None\n\n    # Durchsuche das Verzeichnis\n    root_path = Path(root_dir)\n    for file_path in root_path.rglob(\"*.zip\"):\n        if \"RST$\"+name_filter not in str(file_path):\n            continue\n        match = re.search(pattern, str(file_path).split(\"RST$\")[-1].strip())\n        if match:\n            zip_version = match.group(1)\n\n            # Pr\u00fcfe App-Version Kompatibilit\u00e4t falls angegeben\n            if app_version:\n                file_app_version = re.search(r\"&amp;v([0-9.]+)\u00a7\", str(file_path)).group(1)\n                if version.parse(file_app_version) &gt; version.parse(app_version):\n                    continue\n\n            # Vergleiche Versionen\n            current_version = version.parse(zip_version)\n            if highest_version is None or current_version &gt; highest_version:\n                highest_version = current_version\n                highest_version_file = str(file_path)\n    if version_only:\n        return str(highest_version)\n    return highest_version_file\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.find_highest_zip_version_entry","title":"<code>find_highest_zip_version_entry(name, target_app_version=None, filepath='tbState.yaml')</code>","text":"<p>Findet den Eintrag mit der h\u00f6chsten ZIP-Version f\u00fcr einen gegebenen Namen und eine optionale Ziel-App-Version in einer YAML-Datei.</p> <p>:param name: Der Name des gesuchten Eintrags. :param target_app_version: Die Zielversion der App als String (optional). :param filepath: Der Pfad zur YAML-Datei. :return: Den Eintrag mit der h\u00f6chsten ZIP-Version innerhalb der Ziel-App-Version oder None, falls nicht gefunden.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def find_highest_zip_version_entry(name, target_app_version=None, filepath='tbState.yaml'):\n    \"\"\"\n    Findet den Eintrag mit der h\u00f6chsten ZIP-Version f\u00fcr einen gegebenen Namen und eine optionale Ziel-App-Version in einer YAML-Datei.\n\n    :param name: Der Name des gesuchten Eintrags.\n    :param target_app_version: Die Zielversion der App als String (optional).\n    :param filepath: Der Pfad zur YAML-Datei.\n    :return: Den Eintrag mit der h\u00f6chsten ZIP-Version innerhalb der Ziel-App-Version oder None, falls nicht gefunden.\n    \"\"\"\n    import yaml\n    highest_zip_ver = None\n    highest_entry = {}\n\n    with open(filepath) as file:\n        data = yaml.safe_load(file)\n        # print(data)\n        app_ver_h = None\n        for key, value in list(data.get('installable', {}).items())[::-1]:\n            # Pr\u00fcfe, ob der Name im Schl\u00fcssel enthalten ist\n\n            if name in key:\n                v = value['version']\n                if len(v) == 1:\n                    app_ver = v[0].split('v')[-1]\n                    zip_ver = \"0.0.0\"\n                else:\n                    app_ver, zip_ver = v\n                    app_ver = app_ver.split('v')[-1]\n                app_ver = version.parse(app_ver)\n                # Wenn eine Ziel-App-Version angegeben ist, vergleiche sie\n                if target_app_version is None or app_ver == version.parse(target_app_version):\n                    current_zip_ver = version.parse(zip_ver)\n                    # print(current_zip_ver, highest_zip_ver)\n\n                    if highest_zip_ver is None or current_zip_ver &gt; highest_zip_ver:\n                        highest_zip_ver = current_zip_ver\n                        highest_entry = value\n\n                    if app_ver_h is None or app_ver &gt; app_ver_h:\n                        app_ver_h = app_ver\n                        highest_zip_ver = current_zip_ver\n                        highest_entry = value\n    return highest_entry\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.get_executable_path","title":"<code>get_executable_path()</code>","text":"<p>Find the release executable in standard locations.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def get_executable_path():\n    \"\"\"Find the release executable in standard locations.\"\"\"\n    # This function is simplified from your example to match this script's scope\n    exe_name = get_executable_name_with_extension()\n    from toolboxv2 import tb_root_dir\n    search_paths = [\n        tb_root_dir / Path(\"bin\") / exe_name,\n        tb_root_dir / Path(\"src-core\") / exe_name,\n        tb_root_dir / exe_name,\n        tb_root_dir / Path(\"src-core\") / \"target\" / \"release\" / exe_name,\n    ]\n    for path in search_paths:\n        print(path)\n        if path.exists() and path.is_file():\n            return path.resolve()\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.query_executable_url","title":"<code>query_executable_url(current_os, machine)</code>","text":"<p>Query a remote URL for a matching executable based on OS and architecture. The file name is built dynamically based on parameters.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def query_executable_url(current_os, machine):\n    \"\"\"\n    Query a remote URL for a matching executable based on OS and architecture.\n    The file name is built dynamically based on parameters.\n    \"\"\"\n    base_url = \"https://example.com/downloads\"  # Replace with the actual URL\n    # Windows executables have .exe extension\n    if current_os == \"windows\":\n        file_name = f\"server_{current_os}_{machine}.exe\"\n    else:\n        file_name = f\"server_{current_os}_{machine}\"\n    full_url = f\"{base_url}/{file_name}\"\n    return full_url, file_name\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.remove_release_executable","title":"<code>remove_release_executable()</code>","text":"<p>Removes the release executable.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def remove_release_executable():\n    \"\"\"Removes the release executable.\"\"\"\n    src_core_path = os.path.join(\".\", \"src-core\")\n    expected_name = \"simple-core-server.exe\" if platform.system().lower() == \"windows\" else \"simple-core-server\"\n\n    # Remove from src-core root\n    direct_path = os.path.join(src_core_path, expected_name)\n    if os.path.exists(direct_path):\n        try:\n            os.remove(direct_path)\n            print(f\"Removed release executable: {direct_path}\")\n        except Exception as e:\n            print(f\"Failed to remove {direct_path}: {e}\")\n\n    # Remove from target/release\n    release_path = os.path.join(src_core_path, \"target\", \"release\", expected_name)\n    if os.path.exists(release_path):\n        try:\n            os.remove(release_path)\n            print(f\"Removed release executable: {release_path}\")\n        except Exception as e:\n            print(f\"Failed to remove {release_path}: {e}\")\n\n    return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.run_executable","title":"<code>run_executable(file_path)</code>","text":"<p>Run the executable file.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def run_executable(file_path):\n    \"\"\"Run the executable file.\"\"\"\n    try:\n        print(\"Running it.\")\n        subprocess.run([os.path.abspath(file_path)], check=True)\n    except subprocess.CalledProcessError as e:\n        print(f\"Failed to execute {file_path}: {e}\")\n    except KeyboardInterrupt:\n        print(\"Exiting call from:\", file_path)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.run_in_debug_mode","title":"<code>run_in_debug_mode()</code>","text":"<p>Run the Cargo project in debug mode.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def run_in_debug_mode():\n    \"\"\"Run the Cargo project in debug mode.\"\"\"\n    src_core_path = os.path.join(\".\", \"src-core\")\n    print(\"Running in debug mode...\")\n    try:\n        subprocess.run([\"cargo\", \"run\"], cwd=src_core_path)\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Debug execution failed: {e}\")\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.run_with_hot_reload","title":"<code>run_with_hot_reload()</code>","text":"<p>Run the Cargo project with hot reloading.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def run_with_hot_reload():\n    \"\"\"Run the Cargo project with hot reloading.\"\"\"\n    src_core_path = os.path.join(\".\", \"src-core\")\n\n    # Check if cargo-watch is installed\n    try:\n        subprocess.run([\"cargo\", \"watch\", \"--version\"], check=True, capture_output=True)\n    except Exception:\n        print(\"cargo-watch is not installed. Installing now...\")\n        try:\n            subprocess.run([\"cargo\", \"install\", \"cargo-watch\"], check=True)\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to install cargo-watch: {e}\")\n            print(\"Running without hot reload\")\n            return run_in_debug_mode()\n\n    print(\"Running with hot reload in debug mode...\")\n    try:\n        subprocess.run([\"cargo\", \"watch\", \"-x\", \"run\"], cwd=src_core_path)\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Hot reload execution failed: {e}\")\n        return False\n    except KeyboardInterrupt:\n        print(f\"Exiting hot reload: KeyboardInterrupt\")\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.api.update_server","title":"<code>update_server(new_executable_path, new_version, use_posix_zdt)</code>","text":"<p>High-level update function, calls platform-specific logic.</p> Source code in <code>toolboxv2/utils/system/api.py</code> <pre><code>def update_server(new_executable_path: str, new_version: str, use_posix_zdt: bool):\n    \"\"\"High-level update function, calls platform-specific logic.\"\"\"\n    # Only use POSIX ZDT if flag is set AND on a non-windows system\n    is_posix = platform.system().lower() != \"windows\"\n    if is_posix and use_posix_zdt:\n        return update_server_posix(new_executable_path, new_version)\n    else:\n        if use_posix_zdt and not is_posix:\n            print(Style.YELLOW(\"Warning: --posix-zdt flag ignored on Windows. Using graceful restart.\"))\n        return update_server_graceful_restart(new_executable_path, new_version)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.conda_runner","title":"<code>conda_runner</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.conda_runner.create_env_registry","title":"<code>create_env_registry(env_name)</code>","text":"<p>Create a JSON registry of all packages installed in the specified conda environment.</p> <p>Args: env_name (str): Name of the conda environment</p> <p>Returns: bool: True if registry creation was successful, False otherwise</p> Source code in <code>toolboxv2/utils/system/conda_runner.py</code> <pre><code>def create_env_registry(env_name: str) -&gt; bool:\n    \"\"\"\n    Create a JSON registry of all packages installed in the specified conda environment.\n\n    Args:\n    env_name (str): Name of the conda environment\n\n    Returns:\n    bool: True if registry creation was successful, False otherwise\n    \"\"\"\n    # Get list of installed packages\n    command = f\"conda list -n {env_name} --json\"\n    success, output = run_command(command, live=False)\n\n    if not success or output is None:\n        print(f\"Failed to get package list for environment {env_name}\")\n        return False\n\n    try:\n        # Parse the JSON output\n        packages = json.loads(output)\n\n        # Create a simplified registry with package names and versions\n        registry = [{\"name\": pkg[\"name\"], \"version\": pkg[\"version\"]} for pkg in packages]\n\n        # Write the registry to a JSON file\n        registry_file = f\"{env_name}_registry.json\"\n        with open(registry_file, 'w') as f:\n            json.dump(registry, f, indent=2)\n\n        print(f\"Registry created successfully: {registry_file}\")\n        return True\n\n    except json.JSONDecodeError:\n        print(f\"Failed to parse package list for environment {env_name}\")\n        return False\n    except OSError:\n        print(f\"Failed to write registry file for environment {env_name}\")\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.db_cli_manager","title":"<code>db_cli_manager</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.db_cli_manager.ClusterManager","title":"<code>ClusterManager</code>","text":"<p>Manages a cluster of r_blob_db instances defined in a config file.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>class ClusterManager:\n    \"\"\"Manages a cluster of r_blob_db instances defined in a config file.\"\"\"\n\n    def __init__(self, config_path: str = CLUSTER_CONFIG_FILE):\n        self.config_path = Path(config_path)\n        self.instances: Dict[str, DBInstanceManager] = self._load_config()\n\n    def _load_config(self) -&gt; Dict[str, DBInstanceManager]:\n        \"\"\"Loads and validates the cluster configuration.\"\"\"\n        from toolboxv2 import tb_root_dir\n        if not self.config_path.is_absolute():\n            self.config_path = tb_root_dir / self.config_path\n\n        default_config_dir = (tb_root_dir / \".data/db_data/\").resolve()\n        default_config = {\n            \"instance-01\": {\"port\": 3001, \"data_dir\": str(default_config_dir / \"01\")},\n            \"instance-02\": {\"port\": 3002, \"data_dir\": str(default_config_dir / \"02\")},\n        }\n\n        if not self.config_path.exists():\n            print(Style.YELLOW(f\"Warning: Cluster config '{self.config_path}' not found. Creating a default example.\"))\n\n            with open(self.config_path, 'w') as f:\n                json.dump(default_config, f, indent=4)\n            config_data = default_config\n        else:\n            try:\n                with open(self.config_path, 'r') as f:\n                    config_data = json.load(f)\n            except json.JSONDecodeError:\n                print(Style.RED(f\"Error: Cluster config '{self.config_path}' is not valid JSON. using default config.\"))\n                config_data = default_config\n\n        return {id: DBInstanceManager(id, cfg) for id, cfg in config_data.items()}\n\n    def get_instances(self, instance_id: Optional[str] = None) -&gt; List[DBInstanceManager]:\n        \"\"\"Returns a list of instances to operate on.\"\"\"\n        if instance_id:\n            if instance_id not in self.instances:\n                raise ValueError(f\"Instance ID '{instance_id}' not found in '{self.config_path}'.\")\n            return [self.instances[instance_id]]\n        return list(self.instances.values())\n\n    def start_all(self, executable_path: Path, version: str, instance_id: Optional[str] = None):\n        for instance in self.get_instances(instance_id):\n            instance.start(executable_path, version)\n\n    def stop_all(self, instance_id: Optional[str] = None):\n        for instance in self.get_instances(instance_id):\n            instance.stop()\n\n    def status_all(self, instance_id: Optional[str] = None, silent=False):\n        if not silent:\n            header = f\"--- {Style.Bold('Cluster Status')} ---\"\n            print(header)\n            print(\n                f\"{Style.Underline('INSTANCE ID'):&lt;18} {Style.Underline('STATUS'):&lt;20} {Style.Underline('PID'):&lt;8} {Style.Underline('VERSION'):&lt;12} {Style.Underline('PORT')}\")\n\n        services_online = 0\n        server_list = []\n        for instance in self.get_instances(instance_id):\n            pid, version = instance.read_state()\n            is_running = instance.is_running()\n            if is_running:\n                server_list.append(f\"http://{instance.host}:{instance.port}\")\n                services_online += 1\n            if not silent:\n                status_str = \"\u2705 RUNNING\" if is_running else \"\u274c STOPPED\"\n                status_color = Style.GREEN2 if is_running else Style.RED2\n                print(\n                    f\"  {Style.WHITE(instance.id):&lt;16} {status_color(status_str):&lt;20} {Style.GREY(str(pid or 'N/A')):&lt;8} {Style.BLUE2(version or 'N/A'):&lt;12} {Style.YELLOW(str(instance.port))}\"\n                )\n        if not silent:\n            print(\"-\" * len(header))\n        return services_online, server_list\n\n    def health_check_all(self, instance_id: Optional[str] = None):\n        header = f\"--- {Style.Bold('Cluster Health Check')} ---\"\n        print(header)\n        print(\n            f\"{Style.Underline('INSTANCE ID'):&lt;18} {Style.Underline('STATUS'):&lt;22} {Style.Underline('PID'):&lt;8} {Style.Underline('LATENCY'):&lt;12} {Style.Underline('DETAILS')}\")\n\n        for instance in self.get_instances(instance_id):\n            health = instance.get_health()\n            status = health.get('status', 'UNKNOWN')\n            pid = health.get('pid', 'N/A')\n            details = \"\"\n\n            if status == 'OK':\n                status_str, color = \"\u2705 OK\", Style.GREEN2\n                latency = f\"{health['latency_ms']}ms\"\n                details = f\"Blobs: {Style.YELLOW(str(health['blobs_managed']))} | Version: {Style.BLUE2(health['server_version'])}\"\n            elif status == 'STOPPED':\n                status_str, color = \"\u274c STOPPED\", Style.RED2\n                latency = \"N/A\"\n            else:\n                status_str, color = f\"\ud83d\udd25 {status}\", Style.RED\n                latency = \"N/A\"\n                details = Style.GREY(str(health.get('error', 'N/A')))\n\n            print(\n                f\"  {Style.WHITE(instance.id):&lt;16} {color(status_str):&lt;22} {Style.GREY(str(pid)):&lt;8} {Style.GREEN(latency):&lt;12} {details}\")\n        print(\"-\" * len(header))\n\n    def update_all_rolling(self, new_executable_path: Path, new_version: str, instance_id: Optional[str] = None):\n        \"\"\"Performs a zero-downtime rolling update of the cluster.\"\"\"\n        print(f\"--- {Style.Bold(f'Starting Rolling Update to Version {Style.YELLOW(new_version)}')} ---\")\n        instances_to_update = self.get_instances(instance_id)\n        for i, instance in enumerate(instances_to_update):\n            print(\n                f\"\\n{Style.CYAN(f'[{i + 1}/{len(instances_to_update)}] Updating instance')} '{Style.WHITE(instance.id)}'...\")\n\n            if not instance.stop():\n                print(Style.RED2(f\"CRITICAL: Failed to stop old instance '{instance.id}'. Aborting update.\"))\n                return\n\n            if not instance.start(new_executable_path, new_version):\n                print(Style.RED2(f\"CRITICAL: Failed to start new version for '{instance.id}'. Update halted.\"))\n                print(Style.YELLOW(\"The cluster might be in a partially updated state. Please investigate.\"))\n                return\n\n            with Spinner(f\"Waiting for '{instance.id}' to become healthy\", symbols=\"t\") as s:\n                for attempt in range(5):\n                    s.message = f\"Waiting for '{instance.id}' to become healthy (attempt {attempt + 1}/5)\"\n                    time.sleep(2)\n                    health = instance.get_health()\n                    if health.get('status') == 'OK':\n                        print(\n                            f\"\\n{Style.GREEN('\u2705 Instance')} '{instance.id}' {Style.GREEN('is healthy with new version.')}\")\n                        break\n                else:\n                    print(\n                        f\"\\n{Style.RED2('CRITICAL:')} Instance '{instance.id}' did not become healthy after update. Update halted.\")\n                    return\n\n        print(f\"\\n--- {Style.GREEN2('Rolling Update Complete')} ---\")\n</code></pre> <code>get_instances(instance_id=None)</code> \u00b6 <p>Returns a list of instances to operate on.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def get_instances(self, instance_id: Optional[str] = None) -&gt; List[DBInstanceManager]:\n    \"\"\"Returns a list of instances to operate on.\"\"\"\n    if instance_id:\n        if instance_id not in self.instances:\n            raise ValueError(f\"Instance ID '{instance_id}' not found in '{self.config_path}'.\")\n        return [self.instances[instance_id]]\n    return list(self.instances.values())\n</code></pre> <code>update_all_rolling(new_executable_path, new_version, instance_id=None)</code> \u00b6 <p>Performs a zero-downtime rolling update of the cluster.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def update_all_rolling(self, new_executable_path: Path, new_version: str, instance_id: Optional[str] = None):\n    \"\"\"Performs a zero-downtime rolling update of the cluster.\"\"\"\n    print(f\"--- {Style.Bold(f'Starting Rolling Update to Version {Style.YELLOW(new_version)}')} ---\")\n    instances_to_update = self.get_instances(instance_id)\n    for i, instance in enumerate(instances_to_update):\n        print(\n            f\"\\n{Style.CYAN(f'[{i + 1}/{len(instances_to_update)}] Updating instance')} '{Style.WHITE(instance.id)}'...\")\n\n        if not instance.stop():\n            print(Style.RED2(f\"CRITICAL: Failed to stop old instance '{instance.id}'. Aborting update.\"))\n            return\n\n        if not instance.start(new_executable_path, new_version):\n            print(Style.RED2(f\"CRITICAL: Failed to start new version for '{instance.id}'. Update halted.\"))\n            print(Style.YELLOW(\"The cluster might be in a partially updated state. Please investigate.\"))\n            return\n\n        with Spinner(f\"Waiting for '{instance.id}' to become healthy\", symbols=\"t\") as s:\n            for attempt in range(5):\n                s.message = f\"Waiting for '{instance.id}' to become healthy (attempt {attempt + 1}/5)\"\n                time.sleep(2)\n                health = instance.get_health()\n                if health.get('status') == 'OK':\n                    print(\n                        f\"\\n{Style.GREEN('\u2705 Instance')} '{instance.id}' {Style.GREEN('is healthy with new version.')}\")\n                    break\n            else:\n                print(\n                    f\"\\n{Style.RED2('CRITICAL:')} Instance '{instance.id}' did not become healthy after update. Update halted.\")\n                return\n\n    print(f\"\\n--- {Style.GREEN2('Rolling Update Complete')} ---\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.db_cli_manager.DBInstanceManager","title":"<code>DBInstanceManager</code>","text":"<p>Manages a single r_blob_db instance.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>class DBInstanceManager:\n    \"\"\"Manages a single r_blob_db instance.\"\"\"\n\n    def __init__(self, instance_id: str, config: dict):\n        self.id = instance_id\n        self.port = config['port']\n        self.host = config.get('host', '127.0.0.1')\n        self.data_dir = Path(config['data_dir'])\n        self.state_file = self.data_dir / \"instance_state.json\"\n        self.log_file = self.data_dir / \"instance.log\"  # Added for better logging info\n\n    def read_state(self) -&gt; Tuple[Optional[int], Optional[str]]:\n        \"\"\"Reads the PID and version from the instance's state file.\"\"\"\n        if not self.state_file.exists():\n            return None, None\n        try:\n            with open(self.state_file, 'r') as f:\n                state = json.load(f)\n            return state.get('pid'), state.get('version')\n        except (json.JSONDecodeError, ValueError, FileNotFoundError):\n            return None, None\n\n    def write_state(self, pid: Optional[int], version: Optional[str]):\n        \"\"\"Writes the PID and version to the state file.\"\"\"\n        self.data_dir.mkdir(parents=True, exist_ok=True)\n        state = {'pid': pid, 'version': version}\n        with open(self.state_file, 'w') as f:\n            json.dump(state, f, indent=4)\n\n    def is_running(self) -&gt; bool:\n        \"\"\"Checks if the process associated with this instance is running.\"\"\"\n        pid, _ = self.read_state()\n        return psutil.pid_exists(pid) if pid else False\n\n    def start(self, executable_path: Path, version: str) -&gt; bool:\n        \"\"\"Starts the instance process and detaches, redirecting output to a log file.\"\"\"\n        if self.is_running():\n            print(Style.YELLOW(f\"Instance '{self.id}' is already running.\"))\n            return True\n\n        print(Style.CYAN(f\"\ud83d\ude80 Starting instance '{self.id}' on port {self.port}...\"))\n        self.data_dir.mkdir(parents=True, exist_ok=True)\n        log_handle = open(self.log_file, 'a')\n\n        env = os.environ.copy()\n        env[\"R_BLOB_DB_CLEAN\"] = os.getenv(\"R_BLOB_DB_CLEAN\", \"false\")\n        env[\"R_BLOB_DB_PORT\"] = str(self.port)\n        env[\"R_BLOB_DB_DATA_DIR\"] = str(self.data_dir.resolve())\n        env[\"RUST_LOG\"] = \"info,tower_http=debug\" # \"error\"\n\n        try:\n            if executable_path is None:\n                raise ValueError(f\"\\n{Style.RED2('\u274c ERROR:')} Executable not found. Build it first.\")\n            with Spinner(f\"Launching process for '{self.id}'\", symbols=\"d\"):\n                process = subprocess.Popen(\n                    [str(executable_path.resolve())],\n                    env=env,\n                    stdout=log_handle,\n                    stderr=log_handle,\n                    creationflags=subprocess.DETACHED_PROCESS if platform.system() == \"Windows\" else 0\n                )\n                time.sleep(1.5)\n\n            if process.poll() is not None:\n                print(f\"\\n{Style.RED2('\u274c ERROR:')} Instance '{self.id}' failed to start. Check logs:\")\n                print(f\"    {Style.GREY(self.log_file)}\")\n                return False\n\n            self.write_state(process.pid, version)\n            print(\n                f\"\\n{Style.GREEN2('\u2705 Instance')} '{Style.Bold(self.id)}' {Style.GREEN2('started successfully.')} {Style.GREY(f'(PID: {process.pid})')}\")\n            print(f\"   {Style.BLUE('Logging to:')} {Style.GREY(self.log_file)}\")\n            return True\n        except Exception as e:\n            print(f\"\\n{Style.RED2('\u274c ERROR:')} Failed to launch instance '{self.id}': {e}\")\n            log_handle.close()\n            return False\n\n    def stop(self, timeout: int = 10) -&gt; bool:\n        \"\"\"Stops the instance process gracefully.\"\"\"\n        if not self.is_running():\n            print(Style.YELLOW(f\"Instance '{self.id}' is not running.\"))\n            self.write_state(None, None)\n            return True\n\n        pid, _ = self.read_state()\n        with Spinner(f\"Stopping '{self.id}' (PID: {pid})\", symbols=\"+\", time_in_s=timeout, count_down=True) as s:\n            try:\n                proc = psutil.Process(pid)\n                proc.terminate()\n                proc.wait(timeout)\n            except psutil.TimeoutExpired:\n                s.message = f\"Force killing '{self.id}'\"\n                proc.kill()\n            except psutil.NoSuchProcess:\n                pass\n            except Exception as e:\n                print(f\"\\n{Style.RED2('\u274c ERROR:')} Failed to stop instance '{self.id}': {e}\")\n                return False\n\n        self.write_state(None, None)\n        print(f\"\\n{Style.VIOLET2('\u23f9\ufe0f  Instance')} '{Style.Bold(self.id)}' {Style.VIOLET2('stopped.')}\")\n        return True\n\n    def get_health(self) -&gt; dict:\n        \"\"\"Performs a health check on the running instance.\"\"\"\n        if not self.is_running():\n            return {'id': self.id, 'status': 'STOPPED', 'error': 'Process not running'}\n\n        pid, version = self.read_state()\n        health_url = f\"http://{self.host}:{self.port}/health\"\n        start_time = time.monotonic()\n        try:\n            response = requests.get(health_url, timeout=2)\n            latency_ms = (time.monotonic() - start_time) * 1000\n            response.raise_for_status()\n            health_data = response.json()\n            health_data.update({\n                'id': self.id, 'pid': pid, 'latency_ms': round(latency_ms),\n                'server_version': health_data.pop('version', 'unknown'),\n                'manager_known_version': version\n            })\n            return health_data\n        except requests.exceptions.RequestException as e:\n            return {'id': self.id, 'status': 'UNREACHABLE', 'pid': pid, 'error': str(e)}\n        except Exception as e:\n            return {'id': self.id, 'status': 'ERROR', 'pid': pid, 'error': f'Failed to parse health response: {e}'}\n</code></pre> <code>get_health()</code> \u00b6 <p>Performs a health check on the running instance.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def get_health(self) -&gt; dict:\n    \"\"\"Performs a health check on the running instance.\"\"\"\n    if not self.is_running():\n        return {'id': self.id, 'status': 'STOPPED', 'error': 'Process not running'}\n\n    pid, version = self.read_state()\n    health_url = f\"http://{self.host}:{self.port}/health\"\n    start_time = time.monotonic()\n    try:\n        response = requests.get(health_url, timeout=2)\n        latency_ms = (time.monotonic() - start_time) * 1000\n        response.raise_for_status()\n        health_data = response.json()\n        health_data.update({\n            'id': self.id, 'pid': pid, 'latency_ms': round(latency_ms),\n            'server_version': health_data.pop('version', 'unknown'),\n            'manager_known_version': version\n        })\n        return health_data\n    except requests.exceptions.RequestException as e:\n        return {'id': self.id, 'status': 'UNREACHABLE', 'pid': pid, 'error': str(e)}\n    except Exception as e:\n        return {'id': self.id, 'status': 'ERROR', 'pid': pid, 'error': f'Failed to parse health response: {e}'}\n</code></pre> <code>is_running()</code> \u00b6 <p>Checks if the process associated with this instance is running.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def is_running(self) -&gt; bool:\n    \"\"\"Checks if the process associated with this instance is running.\"\"\"\n    pid, _ = self.read_state()\n    return psutil.pid_exists(pid) if pid else False\n</code></pre> <code>read_state()</code> \u00b6 <p>Reads the PID and version from the instance's state file.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def read_state(self) -&gt; Tuple[Optional[int], Optional[str]]:\n    \"\"\"Reads the PID and version from the instance's state file.\"\"\"\n    if not self.state_file.exists():\n        return None, None\n    try:\n        with open(self.state_file, 'r') as f:\n            state = json.load(f)\n        return state.get('pid'), state.get('version')\n    except (json.JSONDecodeError, ValueError, FileNotFoundError):\n        return None, None\n</code></pre> <code>start(executable_path, version)</code> \u00b6 <p>Starts the instance process and detaches, redirecting output to a log file.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def start(self, executable_path: Path, version: str) -&gt; bool:\n    \"\"\"Starts the instance process and detaches, redirecting output to a log file.\"\"\"\n    if self.is_running():\n        print(Style.YELLOW(f\"Instance '{self.id}' is already running.\"))\n        return True\n\n    print(Style.CYAN(f\"\ud83d\ude80 Starting instance '{self.id}' on port {self.port}...\"))\n    self.data_dir.mkdir(parents=True, exist_ok=True)\n    log_handle = open(self.log_file, 'a')\n\n    env = os.environ.copy()\n    env[\"R_BLOB_DB_CLEAN\"] = os.getenv(\"R_BLOB_DB_CLEAN\", \"false\")\n    env[\"R_BLOB_DB_PORT\"] = str(self.port)\n    env[\"R_BLOB_DB_DATA_DIR\"] = str(self.data_dir.resolve())\n    env[\"RUST_LOG\"] = \"info,tower_http=debug\" # \"error\"\n\n    try:\n        if executable_path is None:\n            raise ValueError(f\"\\n{Style.RED2('\u274c ERROR:')} Executable not found. Build it first.\")\n        with Spinner(f\"Launching process for '{self.id}'\", symbols=\"d\"):\n            process = subprocess.Popen(\n                [str(executable_path.resolve())],\n                env=env,\n                stdout=log_handle,\n                stderr=log_handle,\n                creationflags=subprocess.DETACHED_PROCESS if platform.system() == \"Windows\" else 0\n            )\n            time.sleep(1.5)\n\n        if process.poll() is not None:\n            print(f\"\\n{Style.RED2('\u274c ERROR:')} Instance '{self.id}' failed to start. Check logs:\")\n            print(f\"    {Style.GREY(self.log_file)}\")\n            return False\n\n        self.write_state(process.pid, version)\n        print(\n            f\"\\n{Style.GREEN2('\u2705 Instance')} '{Style.Bold(self.id)}' {Style.GREEN2('started successfully.')} {Style.GREY(f'(PID: {process.pid})')}\")\n        print(f\"   {Style.BLUE('Logging to:')} {Style.GREY(self.log_file)}\")\n        return True\n    except Exception as e:\n        print(f\"\\n{Style.RED2('\u274c ERROR:')} Failed to launch instance '{self.id}': {e}\")\n        log_handle.close()\n        return False\n</code></pre> <code>stop(timeout=10)</code> \u00b6 <p>Stops the instance process gracefully.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def stop(self, timeout: int = 10) -&gt; bool:\n    \"\"\"Stops the instance process gracefully.\"\"\"\n    if not self.is_running():\n        print(Style.YELLOW(f\"Instance '{self.id}' is not running.\"))\n        self.write_state(None, None)\n        return True\n\n    pid, _ = self.read_state()\n    with Spinner(f\"Stopping '{self.id}' (PID: {pid})\", symbols=\"+\", time_in_s=timeout, count_down=True) as s:\n        try:\n            proc = psutil.Process(pid)\n            proc.terminate()\n            proc.wait(timeout)\n        except psutil.TimeoutExpired:\n            s.message = f\"Force killing '{self.id}'\"\n            proc.kill()\n        except psutil.NoSuchProcess:\n            pass\n        except Exception as e:\n            print(f\"\\n{Style.RED2('\u274c ERROR:')} Failed to stop instance '{self.id}': {e}\")\n            return False\n\n    self.write_state(None, None)\n    print(f\"\\n{Style.VIOLET2('\u23f9\ufe0f  Instance')} '{Style.Bold(self.id)}' {Style.VIOLET2('stopped.')}\")\n    return True\n</code></pre> <code>write_state(pid, version)</code> \u00b6 <p>Writes the PID and version to the state file.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def write_state(self, pid: Optional[int], version: Optional[str]):\n    \"\"\"Writes the PID and version to the state file.\"\"\"\n    self.data_dir.mkdir(parents=True, exist_ok=True)\n    state = {'pid': pid, 'version': version}\n    with open(self.state_file, 'w') as f:\n        json.dump(state, f, indent=4)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.db_cli_manager.cli_db_runner","title":"<code>cli_db_runner()</code>","text":"<p>The main entry point for the CLI application.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def cli_db_runner():\n    \"\"\"The main entry point for the CLI application.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=f\"\ud83d\ude80 {Style.Bold('A manager for r_blob_db instances and clusters.')}\",\n        formatter_class=argparse.RawTextHelpFormatter\n    )\n    subparsers = parser.add_subparsers(dest=\"action\", required=True, help=\"Available actions\")\n\n    # Define common arguments\n    instance_arg = {'name_or_flags': ['--instance-id'], 'type': str,\n                    'help': 'Target a specific instance ID. If omitted, action applies to the whole cluster.',\n                    'default': None}\n    version_arg = {'name_or_flags': ['--version'], 'type': str,\n                   'help': 'Specify a version string for the executable (e.g., \"1.2.0\").', 'default': 'dev'}\n\n    # --- Define Commands ---\n    p_start = subparsers.add_parser('start', help='Start instance(s).')\n    p_start.add_argument(*instance_arg['name_or_flags'],\n                         **{k: v for k, v in instance_arg.items() if k != 'name_or_flags'})\n    p_start.add_argument(*version_arg['name_or_flags'],\n                         **{k: v for k, v in version_arg.items() if k != 'name_or_flags'})\n\n    p_stop = subparsers.add_parser('stop', help='Stop instance(s).')\n    p_stop.add_argument(*instance_arg['name_or_flags'],\n                        **{k: v for k, v in instance_arg.items() if k != 'name_or_flags'})\n\n    p_status = subparsers.add_parser('status', help='Show the running status of instance(s).')\n    p_status.add_argument(*instance_arg['name_or_flags'],\n                          **{k: v for k, v in instance_arg.items() if k != 'name_or_flags'})\n\n    p_health = subparsers.add_parser('health', help='Perform a health check on instance(s).')\n    p_health.add_argument(*instance_arg['name_or_flags'],\n                          **{k: v for k, v in instance_arg.items() if k != 'name_or_flags'})\n\n    p_update = subparsers.add_parser('update', help='Perform a rolling update on the cluster.')\n    p_update.add_argument(*instance_arg['name_or_flags'],\n                          **{k: v for k, v in instance_arg.items() if k != 'name_or_flags'})\n    version_arg_update = {**version_arg, 'required': True}\n    p_update.add_argument(*version_arg_update['name_or_flags'],\n                          **{k: v for k, v in version_arg_update.items() if k != 'name_or_flags'})\n\n    subparsers.add_parser('build', help='Build the Rust executable from source.')\n    subparsers.add_parser('clean', help='Clean the Rust build artifacts.')\n\n    # --- Execute Command ---\n    args = parser.parse_args()\n\n    if args.action == 'build':\n        handle_build()\n        return\n    if args.action == 'clean':\n        handle_clean()\n        return\n\n    manager = ClusterManager()\n\n    if args.action in ['start', 'update']:\n        executable_path = get_executable_path(update=(args.action == 'update'))\n        if not executable_path:\n            print(Style.RED(f\"ERROR: Could not find the {EXECUTABLE_NAME} executable.\"))\n            print(Style.YELLOW(\"Please build it first with: python -m toolboxv2.r_blob_db.db_cli build\"))\n            return\n\n    if args.action == 'start':\n        manager.start_all(executable_path, args.version, args.instance_id)\n    elif args.action == 'stop':\n        manager.stop_all(args.instance_id)\n    elif args.action == 'status':\n        manager.status_all(args.instance_id)\n    elif args.action == 'health':\n        manager.health_check_all(args.instance_id)\n    elif args.action == 'update':\n        manager.update_all_rolling(executable_path, args.version, args.instance_id)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.db_cli_manager.get_executable_path","title":"<code>get_executable_path(base_name=EXECUTABLE_NAME, update=False)</code>","text":"<p>Finds the release executable in standard locations.</p> Source code in <code>toolboxv2/utils/system/db_cli_manager.py</code> <pre><code>def get_executable_path(base_name: str = EXECUTABLE_NAME, update=False) -&gt; Optional[Path]:\n    \"\"\"Finds the release executable in standard locations.\"\"\"\n    name_with_ext = f\"{base_name}.exe\" if platform.system() == \"Windows\" else base_name\n    from toolboxv2 import tb_root_dir\n    search_paths = [\n        tb_root_dir / \"bin\" / name_with_ext,\n        tb_root_dir / \"r_blob_db\" / \"target\" / \"release\" / name_with_ext,\n    ]\n    if update:\n        search_paths = search_paths[::-1]\n    for path in search_paths:\n        if path.is_file():\n            return path.resolve()\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.main_tool","title":"<code>main_tool</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.main_tool.MainTool","title":"<code>MainTool</code>","text":"Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>class MainTool:\n    toolID: str = \"\"\n    # app = None\n    interface = None\n    spec = \"app\"\n    name = \"\"\n    color = \"Bold\"\n    stuf = False\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Standard constructor used for arguments pass\n        Do not override. Use __ainit__ instead\n        \"\"\"\n        self.__storedargs = args, kwargs\n        self.tools = kwargs.get(\"tool\", {})\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n        if \"on_exit\" in kwargs and isinstance(kwargs.get(\"on_exit\"), Callable):\n            self.on_exit =self.app.tb(\n                mod_name=self.name,\n                name=kwargs.get(\"on_exit\").__name__,\n                version=self.version if hasattr(self, 'version') else \"0.0.0\",\n            )(kwargs.get(\"on_exit\"))\n        self.async_initialized = False\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    pass\n                else:\n                    self.todo()\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\"Error loading mod {self.name} {e}\")\n                if self.app.debug:\n                    import traceback\n                    traceback.print_exc()\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n\n    async def __ainit__(self, *args, **kwargs):\n        self.version = kwargs[\"v\"]\n        self.tools = kwargs.get(\"tool\", {})\n        self.name = kwargs[\"name\"]\n        self.logger = kwargs.get(\"logs\", get_logger())\n        self.color = kwargs.get(\"color\", \"WHITE\")\n        self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", None))\n        if not hasattr(self, 'config'):\n            self.config = {}\n        self.user = None\n        self.description = \"A toolbox mod\" if kwargs.get(\"description\") is None else kwargs.get(\"description\")\n        if MainTool.interface is None:\n            MainTool.interface = self.app.interface_type\n        # Result.default(self.app.interface)\n\n        if self.todo:\n            try:\n                if inspect.iscoroutinefunction(self.todo):\n                    await self.todo()\n                else:\n                    pass\n                await asyncio.sleep(0.1)\n                get_logger().info(f\"{self.name} on load suspended\")\n            except Exception as e:\n                get_logger().error(f\"Error loading mod {self.name} {e}\")\n                if self.app.debug:\n                    import traceback\n                    traceback.print_exc()\n        else:\n            get_logger().info(f\"{self.name} no load require\")\n        self.app.print(f\"TOOL : {self.spec}.{self.name} online\")\n\n\n\n    @property\n    def app(self):\n        return get_app(\n            from_=f\"{self.spec}.{self.name}|{self.toolID if self.toolID else '*' + MainTool.toolID} {self.interface if self.interface else MainTool.interface}\")\n\n    @app.setter\n    def app(self, v):\n        raise PermissionError(f\"You cannot set the App Instance! {v=}\")\n\n    @staticmethod\n    def return_result(error: ToolBoxError = ToolBoxError.none,\n                      exec_code: int = 0,\n                      help_text: str = \"\",\n                      data_info=None,\n                      data=None,\n                      data_to=None):\n\n        if data_to is None:\n            data_to = MainTool.interface if MainTool.interface is not None else ToolBoxInterfaces.cli\n\n        if data is None:\n            data = {}\n\n        if data_info is None:\n            data_info = {}\n\n        return Result(\n            error,\n            ToolBoxResult(data_info=data_info, data=data, data_to=data_to),\n            ToolBoxInfo(exec_code=exec_code, help_text=help_text)\n        )\n\n    def print(self, message, end=\"\\n\", **kwargs):\n        if self.stuf:\n            return\n\n        self.app.print(Style.style_dic[self.color] + self.name + Style.style_dic[\"END\"] + \":\", message, end=end,\n                       **kwargs)\n\n    def add_str_to_config(self, command):\n        if len(command) != 2:\n            self.logger.error('Invalid command must be key value')\n            return False\n        self.config[command[0]] = command[1]\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n\n    def get_version(self) -&gt; str:\n        \"\"\"\"Returns the version\"\"\"\n        return self.version\n\n    async def get_user(self, username: str) -&gt; Result:\n        return await self.app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME, username=username, get_results=True)\n\n    async def __initobj(self):\n        \"\"\"Crutch used for __await__ after spawning\"\"\"\n        assert not self.async_initialized\n        self.async_initialized = True\n        # pass the parameters to __ainit__ that passed to __init__\n        await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n        return self\n\n    def __await__(self):\n        return self.__initobj().__await__()\n</code></pre> <code>__init__(*args, **kwargs)</code> \u00b6 <p>Standard constructor used for arguments pass Do not override. Use ainit instead</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Standard constructor used for arguments pass\n    Do not override. Use __ainit__ instead\n    \"\"\"\n    self.__storedargs = args, kwargs\n    self.tools = kwargs.get(\"tool\", {})\n    self.logger = kwargs.get(\"logs\", get_logger())\n    self.color = kwargs.get(\"color\", \"WHITE\")\n    self.todo = kwargs.get(\"load\", kwargs.get(\"on_start\", lambda: None))\n    if \"on_exit\" in kwargs and isinstance(kwargs.get(\"on_exit\"), Callable):\n        self.on_exit =self.app.tb(\n            mod_name=self.name,\n            name=kwargs.get(\"on_exit\").__name__,\n            version=self.version if hasattr(self, 'version') else \"0.0.0\",\n        )(kwargs.get(\"on_exit\"))\n    self.async_initialized = False\n    if self.todo:\n        try:\n            if inspect.iscoroutinefunction(self.todo):\n                pass\n            else:\n                self.todo()\n            get_logger().info(f\"{self.name} on load suspended\")\n        except Exception as e:\n            get_logger().error(f\"Error loading mod {self.name} {e}\")\n            if self.app.debug:\n                import traceback\n                traceback.print_exc()\n    else:\n        get_logger().info(f\"{self.name} no load require\")\n</code></pre> <code>__initobj()</code> <code>async</code> \u00b6 <p>Crutch used for await after spawning</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>async def __initobj(self):\n    \"\"\"Crutch used for __await__ after spawning\"\"\"\n    assert not self.async_initialized\n    self.async_initialized = True\n    # pass the parameters to __ainit__ that passed to __init__\n    await self.__ainit__(*self.__storedargs[0], **self.__storedargs[1])\n    return self\n</code></pre> <code>get_version()</code> \u00b6 <p>\"Returns the version</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\"Returns the version\"\"\"\n    return self.version\n</code></pre> <code>webInstall(user_instance, construct_render)</code> \u00b6 <p>\"Returns a web installer for the given user instance and construct render template</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def webInstall(self, user_instance, construct_render) -&gt; str:\n    \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.main_tool.get_version_from_pyproject","title":"<code>get_version_from_pyproject(pyproject_path='../pyproject.toml')</code>","text":"<p>Reads the version from the pyproject.toml file.</p> Source code in <code>toolboxv2/utils/system/main_tool.py</code> <pre><code>def get_version_from_pyproject(pyproject_path='../pyproject.toml'):\n    \"\"\"Reads the version from the pyproject.toml file.\"\"\"\n    if not os.path.exists(pyproject_path) and pyproject_path=='../pyproject.toml':\n        pyproject_path = 'pyproject.toml'\n    if not os.path.exists(pyproject_path) and pyproject_path=='pyproject.toml':\n        return \"0.1.21\"\n\n    try:\n        import toml\n        # Load the pyproject.toml file\n        with open(pyproject_path) as file:\n            pyproject_data = toml.load(file)\n\n        # Extract the version from the 'project' section\n        version = pyproject_data.get('project', {}).get('version')\n\n        if version is None:\n            raise ValueError(f\"Version not found in {pyproject_path}\")\n\n        return version\n    except Exception as e:\n        print(f\"Error reading version: {e}\")\n        return \"0.0.0\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.state_system","title":"<code>state_system</code>","text":"<p>The Task of the State System is : 1 Kep trak of the current state of the ToolBox and its dependency's 2 tracks the shasum of all mod and runnabael 3 the version of all mod</p> <p>The state : {\"utils\":{\"file_name\": {\"version\":##,\"shasum\"}} ,\"mods\":{\"file_name\": {\"version\":##,\"shasum\":##,\"src-url\":##}} ,\"runnable\":{\"file_name\": {\"version\":##,\"shasum\":##,\"src-url\":##}} ,\"api\":{\"file_name\": {\"version\":##,\"shasum\"}} ,\"app\":{\"file_name\": {\"version\":##,\"shasum\":##,\"src-url\":##}} }</p> <p>trans form state from on to an other.</p>"},{"location":"toolboxv2/#toolboxv2.utils.system.tcm_p2p_cli","title":"<code>tcm_p2p_cli</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.tcm_p2p_cli.InstanceManager","title":"<code>InstanceManager</code>","text":"<p>Manages a single named instance (relay or peer) of the P2P application.</p> Source code in <code>toolboxv2/utils/system/tcm_p2p_cli.py</code> <pre><code>class InstanceManager:\n    \"\"\"Manages a single named instance (relay or peer) of the P2P application.\"\"\"\n\n    def __init__(self, name: str):\n        self.name = name\n        self.instance_dir = INSTANCES_ROOT_DIR / self.name\n        self.state_file = self.instance_dir / \"state.json\"\n        self.config_file = self.instance_dir / \"config.toml\"\n        self.log_file = self.instance_dir / \"instance.log\"\n\n    def read_state(self) -&gt; Dict:\n        \"\"\"Reads the instance's state (pid, mode, etc.) from its state file.\"\"\"\n        if not self.state_file.exists():\n            return {}\n        try:\n            with open(self.state_file, 'r') as f:\n                return json.load(f)\n        except (json.JSONDecodeError, FileNotFoundError):\n            return {}\n\n    def write_state(self, state_data: Dict):\n        \"\"\"Writes the instance's state to its state file.\"\"\"\n        self.instance_dir.mkdir(parents=True, exist_ok=True)\n        with open(self.state_file, 'w') as f:\n            json.dump(state_data, f, indent=2)\n\n    def is_running(self) -&gt; bool:\n        \"\"\"Checks if the process associated with this instance is active.\"\"\"\n        pid = self.read_state().get('pid')\n        return psutil.pid_exists(pid) if pid else False\n\n    def generate_config(self, mode: str, config_data: Dict):\n        \"\"\"Generates the config.toml file for this specific instance.\"\"\"\n        content = f'mode = \"{mode}\"\\n\\n'\n\n        if mode == \"relay\":\n            content += \"[relay]\\n\"\n            content += f'bind_address = \"{config_data.get(\"bind_address\", \"0.0.0.0:9000\")}\"\\n'\n            content += f'password = \"{config_data.get(\"password\", \"\")}\"\\n'\n\n        elif mode == \"peer\":\n            content += \"[peer]\\n\"\n            content += f'relay_address = \"{config_data.get(\"relay_address\", \"127.0.0.1:9000\")}\"\\n'\n            content += f'relay_password = \"{config_data.get(\"relay_password\", \"\")}\"\\n'\n            content += f'peer_id = \"{config_data.get(\"peer_id\", \"default-peer\")}\"\\n'\n            content += f'listen_address = \"{config_data.get(\"listen_address\", \"127.0.0.1:8000\")}\"\\n'\n            content += f'forward_to_address = \"{config_data.get(\"forward_to_address\", \"127.0.0.1:3000\")}\"\\n'\n            if config_data.get(\"target_peer_id\"):\n                content += f'target_peer_id = \"{config_data.get(\"target_peer_id\")}\"\\n'\n\n        self.instance_dir.mkdir(parents=True, exist_ok=True)\n        with open(self.config_file, \"w\") as f:\n            f.write(content)\n        print(f\"    {Style.GREEN('Generated config:')} {Style.GREY(str(self.config_file))}\")\n\n    def start(self, executable_path: Path, mode: str, config_data: dict) -&gt; bool:\n        \"\"\"Starts the instance process, detaches it, and logs its state.\"\"\"\n        if self.is_running():\n            print(Style.YELLOW(f\"Instance '{self.name}' is already running.\"))\n            return True\n\n        print(Style.CYAN(f\"\ud83d\ude80 Starting instance '{self.name}'...\"))\n        self.generate_config(mode, config_data)\n        log_handle = open(self.log_file, 'a')\n\n        try:\n            with Spinner(f\"Launching process for '{self.name}'\", symbols=\"d\"):\n                process = subprocess.Popen(\n                    [str(executable_path)],\n                    cwd=str(self.instance_dir),\n                    stdout=log_handle,\n                    stderr=log_handle,\n                    creationflags=subprocess.DETACHED_PROCESS if platform.system() == \"Windows\" else 0\n                )\n                time.sleep(1.5)  # Give it a moment to stabilize or crash\n\n            if process.poll() is not None:\n                print(f\"\\n{Style.RED2('\u274c ERROR:')} Instance '{self.name}' failed to start. Check logs for details:\")\n                print(f\"    {Style.GREY(self.log_file)}\")\n                return False\n\n            state = {'pid': process.pid, 'mode': mode, 'config': config_data}\n            self.write_state(state)\n            print(\n                f\"\\n{Style.GREEN2('\u2705 Instance')} '{Style.Bold(self.name)}' {Style.GREEN2('started successfully.')} {Style.GREY(f'(PID: {process.pid})')}\")\n            print(f\"   {Style.BLUE('Logging to:')} {Style.GREY(self.log_file)}\")\n            return True\n        except Exception as e:\n            print(f\"\\n{Style.RED2('\u274c ERROR:')} Failed to launch instance '{self.name}': {e}\")\n            log_handle.close()\n            return False\n\n    def stop(self, timeout: int = 10) -&gt; bool:\n        \"\"\"Stops the instance process gracefully with a forced kill fallback.\"\"\"\n        if not self.is_running():\n            print(Style.YELLOW(f\"Instance '{self.name}' is not running.\"))\n            self.write_state({})\n            return True\n\n        pid = self.read_state().get('pid')\n        with Spinner(f\"Stopping '{self.name}' (PID: {pid})\", symbols=\"+\", time_in_s=timeout, count_down=True) as s:\n            try:\n                proc = psutil.Process(pid)\n                proc.terminate()\n                proc.wait(timeout)\n            except psutil.TimeoutExpired:\n                s.message = f\"Force killing '{self.name}'\"\n                proc.kill()\n            except psutil.NoSuchProcess:\n                pass\n            except Exception as e:\n                print(f\"\\n{Style.RED2('\u274c ERROR:')} Failed to stop instance '{self.name}': {e}\")\n                return False\n\n        self.write_state({})\n        print(f\"\\n{Style.VIOLET2('\u23f9\ufe0f  Instance')} '{Style.Bold(self.name)}' {Style.VIOLET2('stopped.')}\")\n        return True\n</code></pre> <code>generate_config(mode, config_data)</code> \u00b6 <p>Generates the config.toml file for this specific instance.</p> Source code in <code>toolboxv2/utils/system/tcm_p2p_cli.py</code> <pre><code>def generate_config(self, mode: str, config_data: Dict):\n    \"\"\"Generates the config.toml file for this specific instance.\"\"\"\n    content = f'mode = \"{mode}\"\\n\\n'\n\n    if mode == \"relay\":\n        content += \"[relay]\\n\"\n        content += f'bind_address = \"{config_data.get(\"bind_address\", \"0.0.0.0:9000\")}\"\\n'\n        content += f'password = \"{config_data.get(\"password\", \"\")}\"\\n'\n\n    elif mode == \"peer\":\n        content += \"[peer]\\n\"\n        content += f'relay_address = \"{config_data.get(\"relay_address\", \"127.0.0.1:9000\")}\"\\n'\n        content += f'relay_password = \"{config_data.get(\"relay_password\", \"\")}\"\\n'\n        content += f'peer_id = \"{config_data.get(\"peer_id\", \"default-peer\")}\"\\n'\n        content += f'listen_address = \"{config_data.get(\"listen_address\", \"127.0.0.1:8000\")}\"\\n'\n        content += f'forward_to_address = \"{config_data.get(\"forward_to_address\", \"127.0.0.1:3000\")}\"\\n'\n        if config_data.get(\"target_peer_id\"):\n            content += f'target_peer_id = \"{config_data.get(\"target_peer_id\")}\"\\n'\n\n    self.instance_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.config_file, \"w\") as f:\n        f.write(content)\n    print(f\"    {Style.GREEN('Generated config:')} {Style.GREY(str(self.config_file))}\")\n</code></pre> <code>is_running()</code> \u00b6 <p>Checks if the process associated with this instance is active.</p> Source code in <code>toolboxv2/utils/system/tcm_p2p_cli.py</code> <pre><code>def is_running(self) -&gt; bool:\n    \"\"\"Checks if the process associated with this instance is active.\"\"\"\n    pid = self.read_state().get('pid')\n    return psutil.pid_exists(pid) if pid else False\n</code></pre> <code>read_state()</code> \u00b6 <p>Reads the instance's state (pid, mode, etc.) from its state file.</p> Source code in <code>toolboxv2/utils/system/tcm_p2p_cli.py</code> <pre><code>def read_state(self) -&gt; Dict:\n    \"\"\"Reads the instance's state (pid, mode, etc.) from its state file.\"\"\"\n    if not self.state_file.exists():\n        return {}\n    try:\n        with open(self.state_file, 'r') as f:\n            return json.load(f)\n    except (json.JSONDecodeError, FileNotFoundError):\n        return {}\n</code></pre> <code>start(executable_path, mode, config_data)</code> \u00b6 <p>Starts the instance process, detaches it, and logs its state.</p> Source code in <code>toolboxv2/utils/system/tcm_p2p_cli.py</code> <pre><code>def start(self, executable_path: Path, mode: str, config_data: dict) -&gt; bool:\n    \"\"\"Starts the instance process, detaches it, and logs its state.\"\"\"\n    if self.is_running():\n        print(Style.YELLOW(f\"Instance '{self.name}' is already running.\"))\n        return True\n\n    print(Style.CYAN(f\"\ud83d\ude80 Starting instance '{self.name}'...\"))\n    self.generate_config(mode, config_data)\n    log_handle = open(self.log_file, 'a')\n\n    try:\n        with Spinner(f\"Launching process for '{self.name}'\", symbols=\"d\"):\n            process = subprocess.Popen(\n                [str(executable_path)],\n                cwd=str(self.instance_dir),\n                stdout=log_handle,\n                stderr=log_handle,\n                creationflags=subprocess.DETACHED_PROCESS if platform.system() == \"Windows\" else 0\n            )\n            time.sleep(1.5)  # Give it a moment to stabilize or crash\n\n        if process.poll() is not None:\n            print(f\"\\n{Style.RED2('\u274c ERROR:')} Instance '{self.name}' failed to start. Check logs for details:\")\n            print(f\"    {Style.GREY(self.log_file)}\")\n            return False\n\n        state = {'pid': process.pid, 'mode': mode, 'config': config_data}\n        self.write_state(state)\n        print(\n            f\"\\n{Style.GREEN2('\u2705 Instance')} '{Style.Bold(self.name)}' {Style.GREEN2('started successfully.')} {Style.GREY(f'(PID: {process.pid})')}\")\n        print(f\"   {Style.BLUE('Logging to:')} {Style.GREY(self.log_file)}\")\n        return True\n    except Exception as e:\n        print(f\"\\n{Style.RED2('\u274c ERROR:')} Failed to launch instance '{self.name}': {e}\")\n        log_handle.close()\n        return False\n</code></pre> <code>stop(timeout=10)</code> \u00b6 <p>Stops the instance process gracefully with a forced kill fallback.</p> Source code in <code>toolboxv2/utils/system/tcm_p2p_cli.py</code> <pre><code>def stop(self, timeout: int = 10) -&gt; bool:\n    \"\"\"Stops the instance process gracefully with a forced kill fallback.\"\"\"\n    if not self.is_running():\n        print(Style.YELLOW(f\"Instance '{self.name}' is not running.\"))\n        self.write_state({})\n        return True\n\n    pid = self.read_state().get('pid')\n    with Spinner(f\"Stopping '{self.name}' (PID: {pid})\", symbols=\"+\", time_in_s=timeout, count_down=True) as s:\n        try:\n            proc = psutil.Process(pid)\n            proc.terminate()\n            proc.wait(timeout)\n        except psutil.TimeoutExpired:\n            s.message = f\"Force killing '{self.name}'\"\n            proc.kill()\n        except psutil.NoSuchProcess:\n            pass\n        except Exception as e:\n            print(f\"\\n{Style.RED2('\u274c ERROR:')} Failed to stop instance '{self.name}': {e}\")\n            return False\n\n    self.write_state({})\n    print(f\"\\n{Style.VIOLET2('\u23f9\ufe0f  Instance')} '{Style.Bold(self.name)}' {Style.VIOLET2('stopped.')}\")\n    return True\n</code></pre> <code>write_state(state_data)</code> \u00b6 <p>Writes the instance's state to its state file.</p> Source code in <code>toolboxv2/utils/system/tcm_p2p_cli.py</code> <pre><code>def write_state(self, state_data: Dict):\n    \"\"\"Writes the instance's state to its state file.\"\"\"\n    self.instance_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.state_file, 'w') as f:\n        json.dump(state_data, f, indent=2)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.tcm_p2p_cli.find_instances","title":"<code>find_instances()</code>","text":"<p>Discovers all managed instances by scanning the instances directory.</p> Source code in <code>toolboxv2/utils/system/tcm_p2p_cli.py</code> <pre><code>def find_instances() -&gt; List['InstanceManager']:\n    \"\"\"Discovers all managed instances by scanning the instances directory.\"\"\"\n    if not INSTANCES_ROOT_DIR.is_dir():\n        return []\n\n    instance_managers = []\n    for instance_dir in INSTANCES_ROOT_DIR.iterdir():\n        if instance_dir.is_dir():\n            instance_managers.append(InstanceManager(instance_dir.name))\n    return instance_managers\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.tcm_p2p_cli.get_executable_path","title":"<code>get_executable_path(update=False)</code>","text":"<p>Finds the release executable in standard locations.</p> Source code in <code>toolboxv2/utils/system/tcm_p2p_cli.py</code> <pre><code>def get_executable_path(update=False) -&gt; Path | None:\n    \"\"\"Finds the release executable in standard locations.\"\"\"\n    # Look in a dedicated 'bin' folder first, then cargo's default\n    from toolboxv2 import tb_root_dir\n    search_paths = [\n        tb_root_dir /\"bin\" / EXECUTABLE_NAME,\n        tb_root_dir / \"tcm\"/ \"target\" / \"release\" / EXECUTABLE_NAME,\n    ]\n    if update:\n        search_paths = search_paths[::-1]\n    for path in search_paths:\n        print(path)\n        if path.is_file():\n            return path.resolve()\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types","title":"<code>types</code>","text":""},{"location":"toolboxv2/#toolboxv2.utils.system.types.AppType","title":"<code>AppType</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class AppType:\n    prefix: str\n    id: str\n    globals: dict[str, Any] = {\"root\": dict, }\n    locals: dict[str, Any] = {\"user\": {'app': \"self\"}, }\n\n    local_test: bool = False\n    start_dir: str\n    data_dir: str\n    config_dir: str\n    info_dir: str\n\n    logger: logging.Logger\n    logging_filename: str\n\n    api_allowed_mods_list: list[str] = []\n\n    version: str\n    loop: asyncio.AbstractEventLoop\n\n    keys: dict[str, str] = {\n        \"MACRO\": \"macro~~~~:\",\n        \"MACRO_C\": \"m_color~~:\",\n        \"HELPER\": \"helper~~~:\",\n        \"debug\": \"debug~~~~:\",\n        \"id\": \"name-spa~:\",\n        \"st-load\": \"mute~load:\",\n        \"comm-his\": \"comm-his~:\",\n        \"develop-mode\": \"dev~mode~:\",\n        \"provider::\": \"provider::\",\n    }\n\n    defaults: dict[str, (bool or dict or dict[str, dict[str, str]] or str or list[str] or list[list]) | None] = {\n        \"MACRO\": list[str],\n        \"MACRO_C\": dict,\n        \"HELPER\": dict,\n        \"debug\": str,\n        \"id\": str,\n        \"st-load\": False,\n        \"comm-his\": list[list],\n        \"develop-mode\": bool,\n    }\n\n    cluster_manager: ClusterManager\n    root_blob_storage: BlobStorage\n    config_fh: FileHandler\n    _debug: bool\n    flows: dict[str, Callable]\n    dev_modi: bool\n    functions: dict[str, Any]\n    modules: dict[str, Any]\n\n    interface_type: ToolBoxInterfaces\n    REFIX: str\n\n    alive: bool\n    called_exit: tuple[bool, float]\n    args_sto: AppArgs\n    system_flag = None\n    session = None\n    appdata = None\n    exit_tasks = []\n\n    enable_profiling: bool = False\n    sto = None\n\n    def __init__(self, prefix: None | str= None, args: AppArgs | None = None):\n        self.args_sto = args\n        self.prefix = prefix\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def exit_main(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def hide_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def show_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    async def disconnect(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def set_logger(self, debug=False):\n        \"\"\"proxi attr\"\"\"\n\n    @property\n    def debug(self):\n        \"\"\"proxi attr\"\"\"\n        return self._debug\n\n    def debug_rains(self, e):\n        \"\"\"proxi attr\"\"\"\n\n    def set_flows(self, r):\n        \"\"\"proxi attr\"\"\"\n\n    def run_flows(self, name, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def rrun_flows(self, name, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def idle(self):\n        import time\n        self.print(\"idle\")\n        try:\n            while self.alive:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            pass\n        self.print(\"idle done\")\n\n    async def a_idle(self):\n        self.print(\"a idle\")\n        try:\n            if hasattr(self, 'daemon_app'):\n                self.print(\"serving daemon\")\n                await self.daemon_app.connect(self)\n            else:\n                self.print(\"serving default\")\n                while self.alive:\n                    await asyncio.sleep(1)\n        except KeyboardInterrupt:\n            pass\n        self.print(\"a idle done\")\n\n    @debug.setter\n    def debug(self, value):\n        \"\"\"proxi attr\"\"\"\n\n    def _coppy_mod(self, content, new_mod_dir, mod_name, file_type='py'):\n        \"\"\"proxi attr\"\"\"\n\n    def _pre_lib_mod(self, mod_name, path_to=\"./runtime\", file_type='py'):\n        \"\"\"proxi attr\"\"\"\n\n    def _copy_load(self, mod_name, file_type='py', **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True):\n        \"\"\"proxi attr\"\"\"\n\n    def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n        \"\"\"proxi attr\"\"\"\n\n    def save_initialized_module(self, tools_class, spec):\n        \"\"\"proxi attr\"\"\"\n\n    def mod_online(self, mod_name, installed=False):\n        \"\"\"proxi attr\"\"\"\n\n    def _get_function(self,\n                      name: Enum or None,\n                      state: bool = True,\n                      specification: str = \"app\",\n                      metadata=False, as_str: tuple or None = None, r=0):\n        \"\"\"proxi attr\"\"\"\n\n    def save_exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def load_mod(self, mod_name: str, mlm='I', **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    async def init_module(self, modular):\n        return await self.load_mod(modular)\n\n    async def load_all_mods_in_file(self, working_dir=\"mods\"):\n        \"\"\"proxi attr\"\"\"\n\n    def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\"):\n        \"\"\"proxi attr\"\"\"\n\n    def remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            self.remove_mod(mod, delete=delete)\n\n    async def a_remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            await self.a_remove_mod(mod, delete=delete)\n\n    def print_ok(self):\n        \"\"\"proxi attr\"\"\"\n        self.logger.info(\"OK\")\n\n    def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n        \"\"\"proxi attr\"\"\"\n\n    def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None):\n        \"\"\"proxi attr\"\"\"\n\n    def remove_mod(self, mod_name, spec='app', delete=True):\n        \"\"\"proxi attr\"\"\"\n\n    async def a_remove_mod(self, mod_name, spec='app', delete=True):\n        \"\"\"proxi attr\"\"\"\n\n    def exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def web_context(self) -&gt; str:\n        \"\"\"returns the build index ( toolbox web component )\"\"\"\n\n    async def a_exit(self):\n        \"\"\"proxi attr\"\"\"\n\n    def save_load(self, modname, spec='app'):\n        \"\"\"proxi attr\"\"\"\n\n    def get_function(self, name: Enum or tuple, **kwargs):\n        \"\"\"\n        Kwargs for _get_function\n            metadata:: return the registered function dictionary\n                stateless: (function_data, None), 0\n                stateful: (function_data, higher_order_function), 0\n            state::boolean\n                specification::str default app\n        \"\"\"\n\n    def run_a_from_sync(self, function, *args):\n        \"\"\"\n        run a async fuction\n        \"\"\"\n\n    def run_bg_task_advanced(self, task, *args, **kwargs):\n        \"\"\"\n        proxi attr\n        \"\"\"\n\n    def wait_for_bg_tasks(self, timeout=None):\n        \"\"\"\n        proxi attr\n        \"\"\"\n\n    def run_bg_task(self, task):\n        \"\"\"\n                run a async fuction\n                \"\"\"\n    def run_function(self, mod_function_name: Enum or tuple,\n                     tb_run_function_with_state=True,\n                     tb_run_with_specification='app',\n                     args_=None,\n                     kwargs_=None,\n                     *args,\n                     **kwargs) -&gt; Result:\n\n        \"\"\"proxi attr\"\"\"\n\n    async def a_run_function(self, mod_function_name: Enum or tuple,\n                             tb_run_function_with_state=True,\n                             tb_run_with_specification='app',\n                             args_=None,\n                             kwargs_=None,\n                             *args,\n                             **kwargs) -&gt; Result:\n\n        \"\"\"proxi attr\"\"\"\n\n    def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n        \"\"\"\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        proxi attr\n        \"\"\"\n\n    async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict):\n        \"\"\"\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        proxi attr\n        \"\"\"\n\n    async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None, method=\"GET\",\n                       args_=None,\n                       kwargs_=None,\n                       *args, **kwargs):\n        \"\"\"run a function remote via http / https\"\"\"\n\n    def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n                get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                kwargs_=None,\n                *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                        backwords_compability_variabel_string_holder=None,\n                        get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                        kwargs_=None,\n                        *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def print(text, *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def sprint(text, *args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    # ----------------------------------------------------------------\n    # Decorators for the toolbox\n\n    def _register_function(self, module_name, func_name, data):\n        \"\"\"proxi attr\"\"\"\n\n    def _create_decorator(self, type_: str,\n                          name: str = \"\",\n                          mod_name: str = \"\",\n                          level: int = -1,\n                          restrict_in_virtual_mode: bool = False,\n                          api: bool = False,\n                          helper: str = \"\",\n                          version: str or None = None,\n                          initial=False,\n                          exit_f=False,\n                          test=True,\n                          samples=None,\n                          state=None,\n                          pre_compute=None,\n                          post_compute=None,\n                          memory_cache=False,\n                          file_cache=False,\n                          row=False,\n                          request_as_kwarg=False,\n                          memory_cache_max_size=100,\n                          memory_cache_ttl=300):\n        \"\"\"proxi attr\"\"\"\n\n        # data = {\n        #     \"type\": type_,\n        #     \"module_name\": module_name,\n        #     \"func_name\": func_name,\n        #     \"level\": level,\n        #     \"restrict_in_virtual_mode\": restrict_in_virtual_mode,\n        #     \"func\": func,\n        #     \"api\": api,\n        #     \"helper\": helper,\n        #     \"version\": version,\n        #     \"initial\": initial,\n        #     \"exit_f\": exit_f,\n        #     \"__module__\": func.__module__,\n        #     \"signature\": sig,\n        #     \"params\": params,\n        #     \"state\": (\n        #         False if len(params) == 0 else params[0] in ['self', 'state', 'app']) if state is None else state,\n        #     \"do_test\": test,\n        #     \"samples\": samples,\n        #     \"request_as_kwarg\": request_as_kwarg,\n\n    def tb(self, name=None,\n           mod_name: str = \"\",\n           helper: str = \"\",\n           version: str or None = None,\n           test: bool = True,\n           restrict_in_virtual_mode: bool = False,\n           api: bool = False,\n           initial: bool = False,\n           exit_f: bool = False,\n           test_only: bool = False,\n           memory_cache: bool = False,\n           file_cache: bool = False,\n           row=False,\n           request_as_kwarg: bool = False,\n           state: bool or None = None,\n           level: int = 0,\n           memory_cache_max_size: int = 100,\n           memory_cache_ttl: int = 300,\n           samples: list or dict or None = None,\n           interface: ToolBoxInterfaces or None or str = None,\n           pre_compute=None,\n           post_compute=None,\n           api_methods=None,\n           ):\n        \"\"\"\n    A decorator for registering and configuring functions within a module.\n\n    This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\n    Args:\n        name (str, optional): The name to register the function under. Defaults to the function's own name.\n        mod_name (str, optional): The name of the module the function belongs to.\n        helper (str, optional): A helper string providing additional information about the function.\n        version (str or None, optional): The version of the function or module.\n        test (bool, optional): Flag to indicate if the function is for testing purposes.\n        restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n        api (bool, optional): Flag to indicate if the function is part of an API.\n        initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n        exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n        test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n        memory_cache (bool, optional): Flag to enable memory caching for the function.\n        request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n        file_cache (bool, optional): Flag to enable file caching for the function.\n        row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n        state (bool or None, optional): Flag to indicate if the function maintains state.\n        level (int, optional): The level of the function, used for prioritization or categorization.\n        memory_cache_max_size (int, optional): Maximum size of the memory cache.\n        memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n        samples (list or dict or None, optional): Samples or examples of function usage.\n        interface (str, optional): The interface type for the function.\n        pre_compute (callable, optional): A function to be called before the main function.\n        post_compute (callable, optional): A function to be called after the main function.\n        api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\n    Returns:\n        function: The decorated function with additional processing and registration capabilities.\n    \"\"\"\n        if interface is None:\n            interface = \"tb\"\n        if test_only and 'test' not in self.id:\n            return lambda *args, **kwargs: args\n        return self._create_decorator(interface,\n                                      name,\n                                      mod_name,\n                                      level=level,\n                                      restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                      helper=helper,\n                                      api=api,\n                                      version=version,\n                                      initial=initial,\n                                      exit_f=exit_f,\n                                      test=test,\n                                      samples=samples,\n                                      state=state,\n                                      pre_compute=pre_compute,\n                                      post_compute=post_compute,\n                                      memory_cache=memory_cache,\n                                      file_cache=file_cache,\n                                      row=row,\n                                      request_as_kwarg=request_as_kwarg,\n                                      memory_cache_max_size=memory_cache_max_size,\n                                      memory_cache_ttl=memory_cache_ttl)\n\n    def print_functions(self, name=None):\n\n\n        if not self.functions:\n            print(\"Nothing to see\")\n            return\n\n        def helper(_functions):\n            for func_name, data in _functions.items():\n                if not isinstance(data, dict):\n                    continue\n\n                func_type = data.get('type', 'Unknown')\n                func_level = 'r' if data['level'] == -1 else data['level']\n                api_status = 'Api' if data.get('api', False) else 'Non-Api'\n\n                print(f\"  Function: {func_name}{data.get('signature', '()')}; \"\n                      f\"Type: {func_type}, Level: {func_level}, {api_status}\")\n\n        if name is not None:\n            functions = self.functions.get(name)\n            if functions is not None:\n                print(f\"\\nModule: {name}; Type: {functions.get('app_instance_type', 'Unknown')}\")\n                helper(functions)\n                return\n        for module, functions in self.functions.items():\n            print(f\"\\nModule: {module}; Type: {functions.get('app_instance_type', 'Unknown')}\")\n            helper(functions)\n\n    def save_autocompletion_dict(self):\n        \"\"\"proxi attr\"\"\"\n\n    def get_autocompletion_dict(self):\n        \"\"\"proxi attr\"\"\"\n\n    def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n        \"\"\"proxi attr\"\"\"\n\n    def save_registry_as_enums(self, directory: str, filename: str):\n        \"\"\"proxi attr\"\"\"\n\n    async def execute_all_functions_(self, m_query='', f_query=''):\n        print(\"Executing all functions\")\n        from ..extras import generate_test_cases\n        all_data = {\n            \"modular_run\": 0,\n            \"modular_fatal_error\": 0,\n            \"errors\": 0,\n            \"modular_sug\": 0,\n            \"coverage\": [],\n            \"total_coverage\": {},\n        }\n        items = list(self.functions.items()).copy()\n        for module_name, functions in items:\n            infos = {\n                \"functions_run\": 0,\n                \"functions_fatal_error\": 0,\n                \"error\": 0,\n                \"functions_sug\": 0,\n                'calls': {},\n                'callse': {},\n                \"coverage\": [0, 0],\n            }\n            all_data['modular_run'] += 1\n            if not module_name.startswith(m_query):\n                all_data['modular_sug'] += 1\n                continue\n\n            with Spinner(message=f\"In {module_name}| \"):\n                f_items = list(functions.items()).copy()\n                for function_name, function_data in f_items:\n                    if not isinstance(function_data, dict):\n                        continue\n                    if not function_name.startswith(f_query):\n                        continue\n                    test: list = function_data.get('do_test')\n                    # print(test, module_name, function_name, function_data)\n                    infos[\"coverage\"][0] += 1\n                    if test is False:\n                        continue\n\n                    with Spinner(message=f\"\\t\\t\\t\\t\\t\\tfuction {function_name}...\"):\n                        params: list = function_data.get('params')\n                        sig: signature = function_data.get('signature')\n                        state: bool = function_data.get('state')\n                        samples: bool = function_data.get('samples')\n\n                        test_kwargs_list = [{}]\n\n                        if params is not None:\n                            test_kwargs_list = samples if samples is not None else generate_test_cases(sig=sig)\n                            # print(test_kwargs)\n                            # print(test_kwargs[0])\n                            # test_kwargs = test_kwargs_list[0]\n                        # print(module_name, function_name, test_kwargs_list)\n                        infos[\"coverage\"][1] += 1\n                        for test_kwargs in test_kwargs_list:\n                            try:\n                                # print(f\"test Running {state=} |{module_name}.{function_name}\")\n                                result = await self.a_run_function((module_name, function_name),\n                                                                   tb_run_function_with_state=state,\n                                                                   **test_kwargs)\n                                if not isinstance(result, Result):\n                                    result = Result.ok(result)\n                                if result.info.exec_code == 0:\n                                    infos['calls'][function_name] = [test_kwargs, str(result)]\n                                    infos['functions_sug'] += 1\n                                else:\n                                    infos['functions_sug'] += 1\n                                    infos['error'] += 1\n                                    infos['callse'][function_name] = [test_kwargs, str(result)]\n                            except Exception as e:\n                                infos['functions_fatal_error'] += 1\n                                infos['callse'][function_name] = [test_kwargs, str(e)]\n                            finally:\n                                infos['functions_run'] += 1\n\n                if infos['functions_run'] == infos['functions_sug']:\n                    all_data['modular_sug'] += 1\n                else:\n                    all_data['modular_fatal_error'] += 1\n                if infos['error'] &gt; 0:\n                    all_data['errors'] += infos['error']\n\n                all_data[module_name] = infos\n                if infos['coverage'][0] == 0:\n                    c = 0\n                else:\n                    c = infos['coverage'][1] / infos['coverage'][0]\n                all_data[\"coverage\"].append(f\"{module_name}:{c:.2f}\\n\")\n        total_coverage = sum([float(t.split(\":\")[-1]) for t in all_data[\"coverage\"]]) / len(all_data[\"coverage\"])\n        print(\n            f\"\\n{all_data['modular_run']=}\\n{all_data['modular_sug']=}\\n{all_data['modular_fatal_error']=}\\n{total_coverage=}\")\n        d = analyze_data(all_data)\n        return Result.ok(data=all_data, data_info=d)\n\n    @staticmethod\n    def calculate_complexity(filename_or_code):\n        from radon.complexity import cc_rank, cc_visit\n        if os.path.exists(filename_or_code):\n            with open(filename_or_code) as file:\n                code = file.read()\n        else:\n            code = filename_or_code\n\n        # Calculate and print Cyclomatic Complexity\n        complexity_results = cc_visit(code)\n        i = -1\n        avg_complexity = 0\n        for block in complexity_results:\n            complexity = block.complexity\n            i += 1\n            print(f\"block: {block.name} {i} Class/Fuction/Methode : {block.letter}\")\n            print(f\"    fullname: {block.fullname}\")\n            print(f\"    Cyclomatic Complexity: {complexity}\")\n            # Optional: Get complexity rank\n            avg_complexity += complexity\n            rank = cc_rank(complexity)\n            print(f\"    Complexity Rank: {rank}\")\n            # print(f\"    lineno: {block.lineno}\")\n            print(f\"    endline: {block.endline}\")\n            print(f\"    col_offset: {block.col_offset}\\n\")\n        if i &lt;= 0:\n            i += 2\n        avg_complexity = avg_complexity / i\n        print(f\"\\nAVG Complexity: {avg_complexity:.2f}\")\n        print(f\"Total Rank: {cc_rank(int(avg_complexity + i // 10))}\")\n\n    async def execute_function_test(self, module_name: str, function_name: str,\n                                    function_data: dict, test_kwargs: dict,\n                                    profiler: cProfile.Profile) -&gt; tuple[bool, str, dict, float]:\n        start_time = time.time()\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            try:\n                result = await self.a_run_function(\n                    (module_name, function_name),\n                    tb_run_function_with_state=function_data.get('state'),\n                    **test_kwargs\n                )\n\n                if not isinstance(result, Result):\n                    result = Result.ok(result)\n\n                success = result.info.exec_code == 0\n                execution_time = time.time() - start_time\n                return success, str(result), test_kwargs, execution_time\n            except Exception as e:\n                execution_time = time.time() - start_time\n                return False, str(e), test_kwargs, execution_time\n\n    async def process_function(self, module_name: str, function_name: str,\n                               function_data: dict, profiler: cProfile.Profile) -&gt; tuple[str, ModuleInfo]:\n        start_time = time.time()\n        info = ModuleInfo()\n\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            if not isinstance(function_data, dict):\n                return function_name, info\n\n            test = function_data.get('do_test')\n            info.coverage[0] += 1\n\n            if test is False:\n                return function_name, info\n\n            params = function_data.get('params')\n            sig = function_data.get('signature')\n            samples = function_data.get('samples')\n\n            test_kwargs_list = [{}] if params is None else (\n                samples if samples is not None else generate_test_cases(sig=sig)\n            )\n\n            info.coverage[1] += 1\n\n            # Create tasks for all test cases\n            tasks = [\n                self.execute_function_test(module_name, function_name, function_data, test_kwargs, profiler)\n                for test_kwargs in test_kwargs_list\n            ]\n\n            # Execute all tests concurrently\n            results = await asyncio.gather(*tasks)\n\n            total_execution_time = 0\n            for success, result_str, test_kwargs, execution_time in results:\n                info.functions_run += 1\n                total_execution_time += execution_time\n\n                if success:\n                    info.functions_sug += 1\n                    info.calls[function_name] = [test_kwargs, result_str]\n                else:\n                    info.functions_sug += 1\n                    info.error += 1\n                    info.callse[function_name] = [test_kwargs, result_str]\n\n            info.execution_time = time.time() - start_time\n            return function_name, info\n\n    async def process_module(self, module_name: str, functions: dict,\n                             f_query: str, profiler: cProfile.Profile) -&gt; tuple[str, ModuleInfo]:\n        start_time = time.time()\n\n        with profile_section(profiler, hasattr(self, 'enable_profiling') and self.enable_profiling):\n            async with asyncio.Semaphore(mp.cpu_count()):\n                tasks = [\n                    self.process_function(module_name, fname, fdata, profiler)\n                    for fname, fdata in functions.items()\n                    if fname.startswith(f_query)\n                ]\n\n                if not tasks:\n                    return module_name, ModuleInfo()\n\n                results = await asyncio.gather(*tasks)\n\n                # Combine results from all functions in the module\n                combined_info = ModuleInfo()\n                total_execution_time = 0\n\n                for _, info in results:\n                    combined_info.functions_run += info.functions_run\n                    combined_info.functions_fatal_error += info.functions_fatal_error\n                    combined_info.error += info.error\n                    combined_info.functions_sug += info.functions_sug\n                    combined_info.calls.update(info.calls)\n                    combined_info.callse.update(info.callse)\n                    combined_info.coverage[0] += info.coverage[0]\n                    combined_info.coverage[1] += info.coverage[1]\n                    total_execution_time += info.execution_time\n\n                combined_info.execution_time = time.time() - start_time\n                return module_name, combined_info\n\n    async def execute_all_functions(self, m_query='', f_query='', enable_profiling=True):\n        \"\"\"\n        Execute all functions with parallel processing and optional profiling.\n\n        Args:\n            m_query (str): Module name query filter\n            f_query (str): Function name query filter\n            enable_profiling (bool): Enable detailed profiling information\n        \"\"\"\n        print(\"Executing all functions in parallel\" + (\" with profiling\" if enable_profiling else \"\"))\n\n        start_time = time.time()\n        stats = ExecutionStats()\n        items = list(self.functions.items()).copy()\n\n        # Set up profiling\n        self.enable_profiling = enable_profiling\n        profiler = cProfile.Profile()\n\n        with profile_section(profiler, enable_profiling):\n            # Filter modules based on query\n            filtered_modules = [\n                (mname, mfuncs) for mname, mfuncs in items\n                if mname.startswith(m_query)\n            ]\n\n            stats.modular_run = len(filtered_modules)\n\n            # Process all modules concurrently\n            async with asyncio.Semaphore(mp.cpu_count()):\n                tasks = [\n                    self.process_module(mname, mfuncs, f_query, profiler)\n                    for mname, mfuncs in filtered_modules\n                ]\n\n                results = await asyncio.gather(*tasks)\n\n            # Combine results and calculate statistics\n            for module_name, info in results:\n                if info.functions_run == info.functions_sug:\n                    stats.modular_sug += 1\n                else:\n                    stats.modular_fatal_error += 1\n\n                stats.errors += info.error\n\n                # Calculate coverage\n                coverage = (info.coverage[1] / info.coverage[0]) if info.coverage[0] &gt; 0 else 0\n                stats.coverage.append(f\"{module_name}:{coverage:.2f}\\n\")\n\n                # Store module info\n                stats.__dict__[module_name] = info\n\n            # Calculate total coverage\n            total_coverage = (\n                sum(float(t.split(\":\")[-1]) for t in stats.coverage) / len(stats.coverage)\n                if stats.coverage else 0\n            )\n\n            stats.total_execution_time = time.time() - start_time\n\n            # Generate profiling stats if enabled\n            if enable_profiling:\n                s = io.StringIO()\n                ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n                ps.print_stats()\n                stats.profiling_data = {\n                    'detailed_stats': s.getvalue(),\n                    'total_time': stats.total_execution_time,\n                    'function_count': stats.modular_run,\n                    'successful_functions': stats.modular_sug\n                }\n\n            print(\n                f\"\\n{stats.modular_run=}\"\n                f\"\\n{stats.modular_sug=}\"\n                f\"\\n{stats.modular_fatal_error=}\"\n                f\"\\n{total_coverage=}\"\n                f\"\\nTotal execution time: {stats.total_execution_time:.2f}s\"\n            )\n\n            if enable_profiling:\n                print(\"\\nProfiling Summary:\")\n                print(f\"{'=' * 50}\")\n                print(\"Top 10 time-consuming functions:\")\n                ps.print_stats(10)\n\n            analyzed_data = analyze_data(stats.__dict__)\n            return Result.ok(data=stats.__dict__, data_info=analyzed_data)\n</code></pre> <code>debug</code> <code>property</code> <code>writable</code> \u00b6 <p>proxi attr</p> <code>prefix = prefix</code> <code>instance-attribute</code> \u00b6 <p>proxi attr</p> <code>a_exit()</code> <code>async</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>a_fuction_runner(function, function_data, args, kwargs)</code> <code>async</code> \u00b6 <p>parameters = function_data.get('params') modular_name = function_data.get('module_name') function_name = function_data.get('func_name') mod_function_name = f\"{modular_name}.{function_name}\"</p> <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict):\n    \"\"\"\n    parameters = function_data.get('params')\n    modular_name = function_data.get('module_name')\n    function_name = function_data.get('func_name')\n    mod_function_name = f\"{modular_name}.{function_name}\"\n\n    proxi attr\n    \"\"\"\n</code></pre> <code>a_remove_mod(mod_name, spec='app', delete=True)</code> <code>async</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_remove_mod(self, mod_name, spec='app', delete=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>a_run_any(mod_function_name, backwords_compability_variabel_string_holder=None, get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code> <code>async</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                    backwords_compability_variabel_string_holder=None,\n                    get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                    kwargs_=None,\n                    *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>a_run_function(mod_function_name, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code> <code>async</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def a_run_function(self, mod_function_name: Enum or tuple,\n                         tb_run_function_with_state=True,\n                         tb_run_with_specification='app',\n                         args_=None,\n                         kwargs_=None,\n                         *args,\n                         **kwargs) -&gt; Result:\n\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>debug_rains(e)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def debug_rains(self, e):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>disconnect(*args, **kwargs)</code> <code>async</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def disconnect(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>execute_all_functions(m_query='', f_query='', enable_profiling=True)</code> <code>async</code> \u00b6 <p>Execute all functions with parallel processing and optional profiling.</p> <p>Parameters:</p> Name Type Description Default <code>m_query</code> <code>str</code> <p>Module name query filter</p> <code>''</code> <code>f_query</code> <code>str</code> <p>Function name query filter</p> <code>''</code> <code>enable_profiling</code> <code>bool</code> <p>Enable detailed profiling information</p> <code>True</code> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def execute_all_functions(self, m_query='', f_query='', enable_profiling=True):\n    \"\"\"\n    Execute all functions with parallel processing and optional profiling.\n\n    Args:\n        m_query (str): Module name query filter\n        f_query (str): Function name query filter\n        enable_profiling (bool): Enable detailed profiling information\n    \"\"\"\n    print(\"Executing all functions in parallel\" + (\" with profiling\" if enable_profiling else \"\"))\n\n    start_time = time.time()\n    stats = ExecutionStats()\n    items = list(self.functions.items()).copy()\n\n    # Set up profiling\n    self.enable_profiling = enable_profiling\n    profiler = cProfile.Profile()\n\n    with profile_section(profiler, enable_profiling):\n        # Filter modules based on query\n        filtered_modules = [\n            (mname, mfuncs) for mname, mfuncs in items\n            if mname.startswith(m_query)\n        ]\n\n        stats.modular_run = len(filtered_modules)\n\n        # Process all modules concurrently\n        async with asyncio.Semaphore(mp.cpu_count()):\n            tasks = [\n                self.process_module(mname, mfuncs, f_query, profiler)\n                for mname, mfuncs in filtered_modules\n            ]\n\n            results = await asyncio.gather(*tasks)\n\n        # Combine results and calculate statistics\n        for module_name, info in results:\n            if info.functions_run == info.functions_sug:\n                stats.modular_sug += 1\n            else:\n                stats.modular_fatal_error += 1\n\n            stats.errors += info.error\n\n            # Calculate coverage\n            coverage = (info.coverage[1] / info.coverage[0]) if info.coverage[0] &gt; 0 else 0\n            stats.coverage.append(f\"{module_name}:{coverage:.2f}\\n\")\n\n            # Store module info\n            stats.__dict__[module_name] = info\n\n        # Calculate total coverage\n        total_coverage = (\n            sum(float(t.split(\":\")[-1]) for t in stats.coverage) / len(stats.coverage)\n            if stats.coverage else 0\n        )\n\n        stats.total_execution_time = time.time() - start_time\n\n        # Generate profiling stats if enabled\n        if enable_profiling:\n            s = io.StringIO()\n            ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n            ps.print_stats()\n            stats.profiling_data = {\n                'detailed_stats': s.getvalue(),\n                'total_time': stats.total_execution_time,\n                'function_count': stats.modular_run,\n                'successful_functions': stats.modular_sug\n            }\n\n        print(\n            f\"\\n{stats.modular_run=}\"\n            f\"\\n{stats.modular_sug=}\"\n            f\"\\n{stats.modular_fatal_error=}\"\n            f\"\\n{total_coverage=}\"\n            f\"\\nTotal execution time: {stats.total_execution_time:.2f}s\"\n        )\n\n        if enable_profiling:\n            print(\"\\nProfiling Summary:\")\n            print(f\"{'=' * 50}\")\n            print(\"Top 10 time-consuming functions:\")\n            ps.print_stats(10)\n\n        analyzed_data = analyze_data(stats.__dict__)\n        return Result.ok(data=stats.__dict__, data_info=analyzed_data)\n</code></pre> <code>exit()</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>exit_main(*args, **kwargs)</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef exit_main(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>fuction_runner(function, function_data, args, kwargs, t0=0.0)</code> \u00b6 <p>parameters = function_data.get('params') modular_name = function_data.get('module_name') function_name = function_data.get('func_name') mod_function_name = f\"{modular_name}.{function_name}\"</p> <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n    \"\"\"\n    parameters = function_data.get('params')\n    modular_name = function_data.get('module_name')\n    function_name = function_data.get('func_name')\n    mod_function_name = f\"{modular_name}.{function_name}\"\n\n    proxi attr\n    \"\"\"\n</code></pre> <code>get_all_mods(working_dir='mods', path_to='./runtime')</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>get_autocompletion_dict()</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_autocompletion_dict(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>get_function(name, **kwargs)</code> \u00b6 <p>Kwargs for _get_function     metadata:: return the registered function dictionary         stateless: (function_data, None), 0         stateful: (function_data, higher_order_function), 0     state::boolean         specification::str default app</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_function(self, name: Enum or tuple, **kwargs):\n    \"\"\"\n    Kwargs for _get_function\n        metadata:: return the registered function dictionary\n            stateless: (function_data, None), 0\n            stateful: (function_data, higher_order_function), 0\n        state::boolean\n            specification::str default app\n    \"\"\"\n</code></pre> <code>get_mod(name, spec='app')</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>get_username(get_input=False, default='loot')</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>hide_console(*args, **kwargs)</code> <code>async</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def hide_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>inplace_load_instance(mod_name, loc='toolboxv2.mods.', spec='app', save=True)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>load_all_mods_in_file(working_dir='mods')</code> <code>async</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def load_all_mods_in_file(self, working_dir=\"mods\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>load_mod(mod_name, mlm='I', **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def load_mod(self, mod_name: str, mlm='I', **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>mod_online(mod_name, installed=False)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def mod_online(self, mod_name, installed=False):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>print(text, *args, **kwargs)</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef print(text, *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>print_ok()</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def print_ok(self):\n    \"\"\"proxi attr\"\"\"\n    self.logger.info(\"OK\")\n</code></pre> <code>reload_mod(mod_name, spec='app', is_file=True, loc='toolboxv2.mods.')</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>remove_mod(mod_name, spec='app', delete=True)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def remove_mod(self, mod_name, spec='app', delete=True):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>rrun_flows(name, **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def rrun_flows(self, name, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>run_a_from_sync(function, *args)</code> \u00b6 <p>run a async fuction</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_a_from_sync(self, function, *args):\n    \"\"\"\n    run a async fuction\n    \"\"\"\n</code></pre> <code>run_any(mod_function_name, backwords_compability_variabel_string_holder=None, get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n            get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n            kwargs_=None,\n            *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>run_bg_task(task)</code> \u00b6 <p>run a async fuction</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_bg_task(self, task):\n    \"\"\"\n            run a async fuction\n            \"\"\"\n</code></pre> <code>run_bg_task_advanced(task, *args, **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_bg_task_advanced(self, task, *args, **kwargs):\n    \"\"\"\n    proxi attr\n    \"\"\"\n</code></pre> <code>run_flows(name, **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_flows(self, name, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>run_function(mod_function_name, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None, kwargs_=None, *args, **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def run_function(self, mod_function_name: Enum or tuple,\n                 tb_run_function_with_state=True,\n                 tb_run_with_specification='app',\n                 args_=None,\n                 kwargs_=None,\n                 *args,\n                 **kwargs) -&gt; Result:\n\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>run_http(mod_function_name, function_name=None, method='GET', args_=None, kwargs_=None, *args, **kwargs)</code> <code>async</code> \u00b6 <p>run a function remote via http / https</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None, method=\"GET\",\n                   args_=None,\n                   kwargs_=None,\n                   *args, **kwargs):\n    \"\"\"run a function remote via http / https\"\"\"\n</code></pre> <code>save_autocompletion_dict()</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_autocompletion_dict(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>save_exit()</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_exit(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>save_initialized_module(tools_class, spec)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_initialized_module(self, tools_class, spec):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>save_instance(instance, modular_id, spec='app', instance_type='file/application', tools_class=None)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>save_load(modname, spec='app')</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_load(self, modname, spec='app'):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>save_registry_as_enums(directory, filename)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def save_registry_as_enums(self, directory: str, filename: str):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>set_flows(r)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def set_flows(self, r):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>set_logger(debug=False)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def set_logger(self, debug=False):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>show_console(*args, **kwargs)</code> <code>async</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\nasync def show_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>sprint(text, *args, **kwargs)</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef sprint(text, *args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>tb(name=None, mod_name='', helper='', version=None, test=True, restrict_in_virtual_mode=False, api=False, initial=False, exit_f=False, test_only=False, memory_cache=False, file_cache=False, row=False, request_as_kwarg=False, state=None, level=0, memory_cache_max_size=100, memory_cache_ttl=300, samples=None, interface=None, pre_compute=None, post_compute=None, api_methods=None)</code> \u00b6 <p>A decorator for registering and configuring functions within a module.</p> <p>This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to register the function under. Defaults to the function's own name.</p> <code>None</code> <code>mod_name</code> <code>str</code> <p>The name of the module the function belongs to.</p> <code>''</code> <code>helper</code> <code>str</code> <p>A helper string providing additional information about the function.</p> <code>''</code> <code>version</code> <code>str or None</code> <p>The version of the function or module.</p> <code>None</code> <code>test</code> <code>bool</code> <p>Flag to indicate if the function is for testing purposes.</p> <code>True</code> <code>restrict_in_virtual_mode</code> <code>bool</code> <p>Flag to restrict the function in virtual mode.</p> <code>False</code> <code>api</code> <code>bool</code> <p>Flag to indicate if the function is part of an API.</p> <code>False</code> <code>initial</code> <code>bool</code> <p>Flag to indicate if the function should be executed at initialization.</p> <code>False</code> <code>exit_f</code> <code>bool</code> <p>Flag to indicate if the function should be executed at exit.</p> <code>False</code> <code>test_only</code> <code>bool</code> <p>Flag to indicate if the function should only be used for testing.</p> <code>False</code> <code>memory_cache</code> <code>bool</code> <p>Flag to enable memory caching for the function.</p> <code>False</code> <code>request_as_kwarg</code> <code>bool</code> <p>Flag to get request if the fuction is calld from api.</p> <code>False</code> <code>file_cache</code> <code>bool</code> <p>Flag to enable file caching for the function.</p> <code>False</code> <code>row</code> <code>bool</code> <p>rather to auto wrap the result in Result type default False means no row data aka result type</p> <code>False</code> <code>state</code> <code>bool or None</code> <p>Flag to indicate if the function maintains state.</p> <code>None</code> <code>level</code> <code>int</code> <p>The level of the function, used for prioritization or categorization.</p> <code>0</code> <code>memory_cache_max_size</code> <code>int</code> <p>Maximum size of the memory cache.</p> <code>100</code> <code>memory_cache_ttl</code> <code>int</code> <p>Time-to-live for the memory cache entries.</p> <code>300</code> <code>samples</code> <code>list or dict or None</code> <p>Samples or examples of function usage.</p> <code>None</code> <code>interface</code> <code>str</code> <p>The interface type for the function.</p> <code>None</code> <code>pre_compute</code> <code>callable</code> <p>A function to be called before the main function.</p> <code>None</code> <code>post_compute</code> <code>callable</code> <p>A function to be called after the main function.</p> <code>None</code> <code>api_methods</code> <code>list[str]</code> <p>default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>function</code> <p>The decorated function with additional processing and registration capabilities.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def tb(self, name=None,\n       mod_name: str = \"\",\n       helper: str = \"\",\n       version: str or None = None,\n       test: bool = True,\n       restrict_in_virtual_mode: bool = False,\n       api: bool = False,\n       initial: bool = False,\n       exit_f: bool = False,\n       test_only: bool = False,\n       memory_cache: bool = False,\n       file_cache: bool = False,\n       row=False,\n       request_as_kwarg: bool = False,\n       state: bool or None = None,\n       level: int = 0,\n       memory_cache_max_size: int = 100,\n       memory_cache_ttl: int = 300,\n       samples: list or dict or None = None,\n       interface: ToolBoxInterfaces or None or str = None,\n       pre_compute=None,\n       post_compute=None,\n       api_methods=None,\n       ):\n    \"\"\"\nA decorator for registering and configuring functions within a module.\n\nThis decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\nArgs:\n    name (str, optional): The name to register the function under. Defaults to the function's own name.\n    mod_name (str, optional): The name of the module the function belongs to.\n    helper (str, optional): A helper string providing additional information about the function.\n    version (str or None, optional): The version of the function or module.\n    test (bool, optional): Flag to indicate if the function is for testing purposes.\n    restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n    api (bool, optional): Flag to indicate if the function is part of an API.\n    initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n    exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n    test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n    memory_cache (bool, optional): Flag to enable memory caching for the function.\n    request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n    file_cache (bool, optional): Flag to enable file caching for the function.\n    row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n    state (bool or None, optional): Flag to indicate if the function maintains state.\n    level (int, optional): The level of the function, used for prioritization or categorization.\n    memory_cache_max_size (int, optional): Maximum size of the memory cache.\n    memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n    samples (list or dict or None, optional): Samples or examples of function usage.\n    interface (str, optional): The interface type for the function.\n    pre_compute (callable, optional): A function to be called before the main function.\n    post_compute (callable, optional): A function to be called after the main function.\n    api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\nReturns:\n    function: The decorated function with additional processing and registration capabilities.\n\"\"\"\n    if interface is None:\n        interface = \"tb\"\n    if test_only and 'test' not in self.id:\n        return lambda *args, **kwargs: args\n    return self._create_decorator(interface,\n                                  name,\n                                  mod_name,\n                                  level=level,\n                                  restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                  helper=helper,\n                                  api=api,\n                                  version=version,\n                                  initial=initial,\n                                  exit_f=exit_f,\n                                  test=test,\n                                  samples=samples,\n                                  state=state,\n                                  pre_compute=pre_compute,\n                                  post_compute=post_compute,\n                                  memory_cache=memory_cache,\n                                  file_cache=file_cache,\n                                  row=row,\n                                  request_as_kwarg=request_as_kwarg,\n                                  memory_cache_max_size=memory_cache_max_size,\n                                  memory_cache_ttl=memory_cache_ttl)\n</code></pre> <code>wait_for_bg_tasks(timeout=None)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def wait_for_bg_tasks(self, timeout=None):\n    \"\"\"\n    proxi attr\n    \"\"\"\n</code></pre> <code>watch_mod(mod_name, spec='app', loc='toolboxv2.mods.', use_thread=True, path_name=None)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>web_context()</code> \u00b6 <p>returns the build index ( toolbox web component )</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def web_context(self) -&gt; str:\n    \"\"\"returns the build index ( toolbox web component )\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.Headers","title":"<code>Headers</code>  <code>dataclass</code>","text":"<p>Class representing HTTP headers with strongly typed common fields.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@dataclass\nclass Headers:\n    \"\"\"Class representing HTTP headers with strongly typed common fields.\"\"\"\n    # General Headers\n    accept: None | str= None\n    accept_charset: None | str= None\n    accept_encoding: None | str= None\n    accept_language: None | str= None\n    accept_ranges: None | str= None\n    access_control_allow_credentials: None | str= None\n    access_control_allow_headers: None | str= None\n    access_control_allow_methods: None | str= None\n    access_control_allow_origin: None | str= None\n    access_control_expose_headers: None | str= None\n    access_control_max_age: None | str= None\n    access_control_request_headers: None | str= None\n    access_control_request_method: None | str= None\n    age: None | str= None\n    allow: None | str= None\n    alt_svc: None | str= None\n    authorization: None | str= None\n    cache_control: None | str= None\n    clear_site_data: None | str= None\n    connection: None | str= None\n    content_disposition: None | str= None\n    content_encoding: None | str= None\n    content_language: None | str= None\n    content_length: None | str= None\n    content_location: None | str= None\n    content_range: None | str= None\n    content_security_policy: None | str= None\n    content_security_policy_report_only: None | str= None\n    content_type: None | str= None\n    cookie: None | str= None\n    cross_origin_embedder_policy: None | str= None\n    cross_origin_opener_policy: None | str= None\n    cross_origin_resource_policy: None | str= None\n    date: None | str= None\n    device_memory: None | str= None\n    digest: None | str= None\n    dnt: None | str= None\n    dpr: None | str= None\n    etag: None | str= None\n    expect: None | str= None\n    expires: None | str= None\n    feature_policy: None | str= None\n    forwarded: None | str= None\n    from_header: None | str= None  # 'from' is a Python keyword\n    host: None | str= None\n    if_match: None | str= None\n    if_modified_since: None | str= None\n    if_none_match: None | str= None\n    if_range: None | str= None\n    if_unmodified_since: None | str= None\n    keep_alive: None | str= None\n    large_allocation: None | str= None\n    last_modified: None | str= None\n    link: None | str= None\n    location: None | str= None\n    max_forwards: None | str= None\n    origin: None | str= None\n    pragma: None | str= None\n    proxy_authenticate: None | str= None\n    proxy_authorization: None | str= None\n    public_key_pins: None | str= None\n    public_key_pins_report_only: None | str= None\n    range: None | str= None\n    referer: None | str= None\n    referrer_policy: None | str= None\n    retry_after: None | str= None\n    save_data: None | str= None\n    sec_fetch_dest: None | str= None\n    sec_fetch_mode: None | str= None\n    sec_fetch_site: None | str= None\n    sec_fetch_user: None | str= None\n    sec_websocket_accept: None | str= None\n    sec_websocket_extensions: None | str= None\n    sec_websocket_key: None | str= None\n    sec_websocket_protocol: None | str= None\n    sec_websocket_version: None | str= None\n    server: None | str= None\n    server_timing: None | str= None\n    service_worker_allowed: None | str= None\n    set_cookie: None | str= None\n    sourcemap: None | str= None\n    strict_transport_security: None | str= None\n    te: None | str= None\n    timing_allow_origin: None | str= None\n    tk: None | str= None\n    trailer: None | str= None\n    transfer_encoding: None | str= None\n    upgrade: None | str= None\n    upgrade_insecure_requests: None | str= None\n    user_agent: None | str= None\n    vary: None | str= None\n    via: None | str= None\n    warning: None | str= None\n    www_authenticate: None | str= None\n    x_content_type_options: None | str= None\n    x_dns_prefetch_control: None | str= None\n    x_forwarded_for: None | str= None\n    x_forwarded_host: None | str= None\n    x_forwarded_proto: None | str= None\n    x_frame_options: None | str= None\n    x_xss_protection: None | str= None\n\n    # Browser-specific and custom headers\n    sec_ch_ua: None | str= None\n    sec_ch_ua_mobile: None | str= None\n    sec_ch_ua_platform: None | str= None\n    sec_ch_ua_arch: None | str= None\n    sec_ch_ua_bitness: None | str= None\n    sec_ch_ua_full_version: None | str= None\n    sec_ch_ua_full_version_list: None | str= None\n    sec_ch_ua_platform_version: None | str= None\n\n    # HTMX specific headers\n    hx_boosted: None | str= None\n    hx_current_url: None | str= None\n    hx_history_restore_request: None | str= None\n    hx_prompt: None | str= None\n    hx_request: None | str= None\n    hx_target: None | str= None\n    hx_trigger: None | str= None\n    hx_trigger_name: None | str= None\n\n    # Additional fields can be stored in extra_headers\n    extra_headers: dict[str, str] = field(default_factory=dict)\n\n    def __post_init__(self):\n        \"\"\"Convert header keys with hyphens to underscores for attribute access.\"\"\"\n        # Handle the 'from' header specifically since it's a Python keyword\n        if 'from' in self.__dict__:\n            self.from_header = self.__dict__.pop('from')\n\n        # Store any attributes that weren't explicitly defined in extra_headers\n        all_attrs = self.__annotations__.keys()\n        for key in list(self.__dict__.keys()):\n            if key not in all_attrs and key != \"extra_headers\":\n                self.extra_headers[key.replace(\"_\", \"-\")] = getattr(self, key)\n                delattr(self, key)\n\n    @classmethod\n    def from_dict(cls, headers_dict: dict[str, str]) -&gt; 'Headers':\n        \"\"\"Create a Headers instance from a dictionary.\"\"\"\n        # Convert header keys from hyphenated to underscore format for Python attributes\n        processed_headers = {}\n        extra_headers = {}\n\n        for key, value in headers_dict.items():\n            # Handle 'from' header specifically\n            if key.lower() == 'from':\n                processed_headers['from_header'] = value\n                continue\n\n            python_key = key.replace(\"-\", \"_\").lower()\n            if python_key in cls.__annotations__ and python_key != \"extra_headers\":\n                processed_headers[python_key] = value\n            else:\n                extra_headers[key] = value\n\n        return cls(**processed_headers, extra_headers=extra_headers)\n\n    def to_dict(self) -&gt; dict[str, str]:\n        \"\"\"Convert the Headers object back to a dictionary.\"\"\"\n        result = {}\n\n        # Add regular attributes\n        for key, value in self.__dict__.items():\n            if key != \"extra_headers\" and value is not None:\n                # Handle from_header specially\n                if key == \"from_header\":\n                    result[\"from\"] = value\n                else:\n                    result[key.replace(\"_\", \"-\")] = value\n\n        # Add extra headers\n        result.update(self.extra_headers)\n\n        return result\n</code></pre> <code>__post_init__()</code> \u00b6 <p>Convert header keys with hyphens to underscores for attribute access.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Convert header keys with hyphens to underscores for attribute access.\"\"\"\n    # Handle the 'from' header specifically since it's a Python keyword\n    if 'from' in self.__dict__:\n        self.from_header = self.__dict__.pop('from')\n\n    # Store any attributes that weren't explicitly defined in extra_headers\n    all_attrs = self.__annotations__.keys()\n    for key in list(self.__dict__.keys()):\n        if key not in all_attrs and key != \"extra_headers\":\n            self.extra_headers[key.replace(\"_\", \"-\")] = getattr(self, key)\n            delattr(self, key)\n</code></pre> <code>from_dict(headers_dict)</code> <code>classmethod</code> \u00b6 <p>Create a Headers instance from a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef from_dict(cls, headers_dict: dict[str, str]) -&gt; 'Headers':\n    \"\"\"Create a Headers instance from a dictionary.\"\"\"\n    # Convert header keys from hyphenated to underscore format for Python attributes\n    processed_headers = {}\n    extra_headers = {}\n\n    for key, value in headers_dict.items():\n        # Handle 'from' header specifically\n        if key.lower() == 'from':\n            processed_headers['from_header'] = value\n            continue\n\n        python_key = key.replace(\"-\", \"_\").lower()\n        if python_key in cls.__annotations__ and python_key != \"extra_headers\":\n            processed_headers[python_key] = value\n        else:\n            extra_headers[key] = value\n\n    return cls(**processed_headers, extra_headers=extra_headers)\n</code></pre> <code>to_dict()</code> \u00b6 <p>Convert the Headers object back to a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, str]:\n    \"\"\"Convert the Headers object back to a dictionary.\"\"\"\n    result = {}\n\n    # Add regular attributes\n    for key, value in self.__dict__.items():\n        if key != \"extra_headers\" and value is not None:\n            # Handle from_header specially\n            if key == \"from_header\":\n                result[\"from\"] = value\n            else:\n                result[key.replace(\"_\", \"-\")] = value\n\n    # Add extra headers\n    result.update(self.extra_headers)\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.MainToolType","title":"<code>MainToolType</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class MainToolType:\n    toolID: str\n    app: A\n    interface: ToolBoxInterfaces\n    spec: str\n\n    version: str\n    tools: dict  # legacy\n    name: str\n    logger: logging\n    color: str\n    todo: Callable\n    _on_exit: Callable\n    stuf: bool\n    config: dict\n    user: U | None\n    description: str\n\n    @staticmethod\n    def return_result(error: ToolBoxError = ToolBoxError.none,\n                      exec_code: int = 0,\n                      help_text: str = \"\",\n                      data_info=None,\n                      data=None,\n                      data_to=None) -&gt; Result:\n        \"\"\"proxi attr\"\"\"\n\n    def load(self):\n        \"\"\"proxi attr\"\"\"\n\n    def print(self, message, end=\"\\n\", **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def add_str_to_config(self, command):\n        if len(command) != 2:\n            self.logger.error('Invalid command must be key value')\n            return False\n        self.config[command[0]] = command[1]\n\n    def webInstall(self, user_instance, construct_render) -&gt; str:\n        \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n\n    async def get_user(self, username: str) -&gt; Result:\n        return self.app.a_run_any(CLOUDM_AUTHMANAGER.GET_USER_BY_NAME, username=username, get_results=True)\n</code></pre> <code>load()</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def load(self):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>print(message, end='\\n', **kwargs)</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def print(self, message, end=\"\\n\", **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>return_result(error=ToolBoxError.none, exec_code=0, help_text='', data_info=None, data=None, data_to=None)</code> <code>staticmethod</code> \u00b6 <p>proxi attr</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef return_result(error: ToolBoxError = ToolBoxError.none,\n                  exec_code: int = 0,\n                  help_text: str = \"\",\n                  data_info=None,\n                  data=None,\n                  data_to=None) -&gt; Result:\n    \"\"\"proxi attr\"\"\"\n</code></pre> <code>webInstall(user_instance, construct_render)</code> \u00b6 <p>\"Returns a web installer for the given user instance and construct render template</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def webInstall(self, user_instance, construct_render) -&gt; str:\n    \"\"\"\"Returns a web installer for the given user instance and construct render template\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.Request","title":"<code>Request</code>  <code>dataclass</code>","text":"<p>Class representing an HTTP request.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@dataclass\nclass Request:\n    \"\"\"Class representing an HTTP request.\"\"\"\n    content_type: str\n    headers: Headers\n    method: str\n    path: str\n    query_params: dict[str, Any] = field(default_factory=dict)\n    form_data: dict[str, Any] | None = None\n    body: Any | None = None\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any]) -&gt; 'Request':\n        \"\"\"Create a Request instance from a dictionary.\"\"\"\n        headers = Headers.from_dict(data.get('headers', {}))\n\n        # Extract other fields\n        return cls(\n            content_type=data.get('content_type', ''),\n            headers=headers,\n            method=data.get('method', ''),\n            path=data.get('path', ''),\n            query_params=data.get('query_params', {}),\n            form_data=data.get('form_data'),\n            body=data.get('body')\n        )\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert the Request object back to a dictionary.\"\"\"\n        result = {\n            'content_type': self.content_type,\n            'headers': self.headers.to_dict(),\n            'method': self.method,\n            'path': self.path,\n            'query_params': self.query_params,\n        }\n\n        if self.form_data is not None:\n            result['form_data'] = self.form_data\n\n        if self.body is not None:\n            result['body'] = self.body\n\n        return result\n</code></pre> <code>from_dict(data)</code> <code>classmethod</code> \u00b6 <p>Create a Request instance from a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; 'Request':\n    \"\"\"Create a Request instance from a dictionary.\"\"\"\n    headers = Headers.from_dict(data.get('headers', {}))\n\n    # Extract other fields\n    return cls(\n        content_type=data.get('content_type', ''),\n        headers=headers,\n        method=data.get('method', ''),\n        path=data.get('path', ''),\n        query_params=data.get('query_params', {}),\n        form_data=data.get('form_data'),\n        body=data.get('body')\n    )\n</code></pre> <code>to_dict()</code> \u00b6 <p>Convert the Request object back to a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert the Request object back to a dictionary.\"\"\"\n    result = {\n        'content_type': self.content_type,\n        'headers': self.headers.to_dict(),\n        'method': self.method,\n        'path': self.path,\n        'query_params': self.query_params,\n    }\n\n    if self.form_data is not None:\n        result['form_data'] = self.form_data\n\n    if self.body is not None:\n        result['body'] = self.body\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.RequestData","title":"<code>RequestData</code>  <code>dataclass</code>","text":"<p>Main class representing the complete request data structure.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@dataclass\nclass RequestData:\n    \"\"\"Main class representing the complete request data structure.\"\"\"\n    request: Request\n    session: Session\n    session_id: str\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any]) -&gt; 'RequestData':\n        \"\"\"Create a RequestData instance from a dictionary.\"\"\"\n        return cls(\n            request=Request.from_dict(data.get('request', {})),\n            session=Session.from_dict(data.get('session', {})),\n            session_id=data.get('session_id', '')\n        )\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert the RequestData object back to a dictionary.\"\"\"\n        return {\n            'request': self.request.to_dict(),\n            'session': self.session.to_dict(),\n            'session_id': self.session_id\n        }\n\n    def __getattr__(self, name: str) -&gt; Any:\n        \"\"\"Delegate unknown attributes to the `request` object.\"\"\"\n        # Nur wenn das Attribut nicht direkt in RequestData existiert\n        # und auch nicht `session` oder `session_id` ist\n        if hasattr(self.request, name):\n            return getattr(self.request, name)\n        raise AttributeError(f\"'RequestData' object has no attribute '{name}'\")\n\n    @classmethod\n    def moc(cls):\n        return cls(\n            request=Request.from_dict({\n                'content_type': 'application/x-www-form-urlencoded',\n                'headers': {\n                    'accept': '*/*',\n                    'accept-encoding': 'gzip, deflate, br, zstd',\n                    'accept-language': 'de-DE,de;q=0.9,en-US;q=0.8,en;q=0.7',\n                    'connection': 'keep-alive',\n                    'content-length': '107',\n                    'content-type': 'application/x-www-form-urlencoded',\n                    'cookie': 'session=abc123',\n                    'host': 'localhost:8080',\n                    'hx-current-url': 'http://localhost:8080/api/TruthSeeker/get_main_ui',\n                    'hx-request': 'true',\n                    'hx-target': 'estimates-guest_1fc2c9',\n                    'hx-trigger': 'config-form-guest_1fc2c9',\n                    'origin': 'http://localhost:8080',\n                    'referer': 'http://localhost:8080/api/TruthSeeker/get_main_ui',\n                    'sec-ch-ua': '\"Chromium\";v=\"134\", \"Not:A-Brand\";v=\"24\", \"Google Chrome\";v=\"134\"',\n                    'sec-ch-ua-mobile': '?0',\n                    'sec-ch-ua-platform': '\"Windows\"',\n                    'sec-fetch-dest': 'empty',\n                    'sec-fetch-mode': 'cors',\n                    'sec-fetch-site': 'same-origin',\n                    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n                },\n                'method': 'POST',\n                'path': '/api/TruthSeeker/update_estimates',\n                'query_params': {},\n                'form_data': {\n                    'param1': 'value1',\n                    'param2': 'value2'\n                }\n            }),\n            session=Session.from_dict({\n                'SiID': '29a2e258e18252e2afd5ff943523f09c82f1bb9adfe382a6f33fc6a8381de898',\n                'level': '1',\n                'spec': '74eed1c8de06886842e235486c3c2fd6bcd60586998ac5beb87f13c0d1750e1d',\n                'user_name': 'root',\n                'custom_field': 'custom_value'\n            }),\n            session_id='0x29dd1ac0d1e30d3f'\n        )\n</code></pre> <code>__getattr__(name)</code> \u00b6 <p>Delegate unknown attributes to the <code>request</code> object.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"Delegate unknown attributes to the `request` object.\"\"\"\n    # Nur wenn das Attribut nicht direkt in RequestData existiert\n    # und auch nicht `session` oder `session_id` ist\n    if hasattr(self.request, name):\n        return getattr(self.request, name)\n    raise AttributeError(f\"'RequestData' object has no attribute '{name}'\")\n</code></pre> <code>from_dict(data)</code> <code>classmethod</code> \u00b6 <p>Create a RequestData instance from a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; 'RequestData':\n    \"\"\"Create a RequestData instance from a dictionary.\"\"\"\n    return cls(\n        request=Request.from_dict(data.get('request', {})),\n        session=Session.from_dict(data.get('session', {})),\n        session_id=data.get('session_id', '')\n    )\n</code></pre> <code>to_dict()</code> \u00b6 <p>Convert the RequestData object back to a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert the RequestData object back to a dictionary.\"\"\"\n    return {\n        'request': self.request.to_dict(),\n        'session': self.session.to_dict(),\n        'session_id': self.session_id\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.Result","title":"<code>Result</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class Result:\n    _task = None\n    def __init__(self,\n                 error: ToolBoxError,\n                 result: ToolBoxResult,\n                 info: ToolBoxInfo,\n                 origin: Any | None = None,\n                 ):\n        self.error: ToolBoxError = error\n        self.result: ToolBoxResult = result\n        self.info: ToolBoxInfo = info\n        self.origin = origin\n\n    def as_result(self):\n        return self\n\n    def as_dict(self):\n        return {\n            \"error\":self.error.value if isinstance(self.error, Enum) else self.error,\n        \"result\" : {\n            \"data_to\":self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n            \"data_info\":self.result.data_info,\n            \"data\":self.result.data,\n            \"data_type\":self.result.data_type\n        } if self.result else None,\n        \"info\" : {\n            \"exec_code\" : self.info.exec_code,  # exec_code umwandel in http resposn codes\n        \"help_text\" : self.info.help_text\n        } if self.info else None,\n        \"origin\" : self.origin\n        }\n\n    def set_origin(self, origin):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = origin\n        return self\n\n    def set_dir_origin(self, name, extras=\"assets/\"):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = f\"mods/{name}/{extras}\"\n        return self\n\n    def is_error(self):\n        if _test_is_result(self.result.data):\n            return self.result.data.is_error()\n        if self.error == ToolBoxError.none:\n            return False\n        if self.info.exec_code == 0:\n            return False\n        if self.info.exec_code == 200:\n            return False\n        return True\n\n    def is_ok(self):\n        return not self.is_error()\n\n    def is_data(self):\n        return self.result.data is not None\n\n    def to_api_result(self):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=self.error.value if isinstance(self.error, Enum) else self.error,\n            result=ToolBoxResultBM(\n                data_to=self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n                data_info=self.result.data_info,\n                data=self.result.data,\n                data_type=self.result.data_type\n            ) if self.result else None,\n            info=ToolBoxInfoBM(\n                exec_code=self.info.exec_code,  # exec_code umwandel in http resposn codes\n                help_text=self.info.help_text\n            ) if self.info else None,\n            origin=self.origin\n        )\n\n    def task(self, task):\n        self._task = task\n        return self\n\n    @staticmethod\n    def result_from_dict(error: str, result: dict, info: dict, origin: list or None or str):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=error if isinstance(error, Enum) else error,\n            result=ToolBoxResultBM(\n                data_to=result.get('data_to') if isinstance(result.get('data_to'), Enum) else result.get('data_to'),\n                data_info=result.get('data_info', '404'),\n                data=result.get('data'),\n                data_type=result.get('data_type', '404'),\n            ) if result else ToolBoxResultBM(\n                data_to=ToolBoxInterfaces.cli.value,\n                data_info='',\n                data='404',\n                data_type='404',\n            ),\n            info=ToolBoxInfoBM(\n                exec_code=info.get('exec_code', 404),\n                help_text=info.get('help_text', '404')\n            ) if info else ToolBoxInfoBM(\n                exec_code=404,\n                help_text='404'\n            ),\n            origin=origin\n        ).as_result()\n\n    @classmethod\n    def stream(cls,\n               stream_generator: Any,  # Renamed from source for clarity\n               content_type: str = \"text/event-stream\",  # Default to SSE\n               headers: Union[dict, None] = None,\n               info: str = \"OK\",\n               interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n               cleanup_func: Union[\n                   Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None):\n        \"\"\"\n        Create a streaming response Result. Handles SSE and other stream types.\n\n        Args:\n            stream_generator: Any stream source (async generator, sync generator, iterable, or single item).\n            content_type: Content-Type header (default: text/event-stream for SSE).\n            headers: Additional HTTP headers for the response.\n            info: Help text for the result.\n            interface: Interface to send data to.\n            cleanup_func: Optional function for cleanup.\n\n        Returns:\n            A Result object configured for streaming.\n        \"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        final_generator: AsyncGenerator[str, None]\n\n        if content_type == \"text/event-stream\":\n            # For SSE, always use SSEGenerator.create_sse_stream to wrap the source.\n            # SSEGenerator.create_sse_stream handles various types of stream_generator internally.\n            final_generator = SSEGenerator.create_sse_stream(source=stream_generator, cleanup_func=cleanup_func)\n\n            # Standard SSE headers for the HTTP response itself\n            # These will be stored in the Result object. Rust side decides how to use them.\n            standard_sse_headers = {\n                \"Cache-Control\": \"no-cache\",  # SSE specific\n                \"Connection\": \"keep-alive\",  # SSE specific\n                \"X-Accel-Buffering\": \"no\",  # Useful for proxies with SSE\n                # Content-Type is implicitly text/event-stream, will be in streaming_data below\n            }\n            all_response_headers = standard_sse_headers.copy()\n            if headers:\n                all_response_headers.update(headers)\n        else:\n            # For non-SSE streams.\n            # If stream_generator is sync, wrap it to be async.\n            # If already async or single item, it will be handled.\n            # Rust's stream_generator in ToolboxClient seems to handle both sync/async Python generators.\n            # For consistency with how SSEGenerator does it, we can wrap sync ones.\n            if inspect.isgenerator(stream_generator) or \\\n                (not isinstance(stream_generator, str) and hasattr(stream_generator, '__iter__')):\n                final_generator = SSEGenerator.wrap_sync_generator(stream_generator)  # Simple async wrapper\n            elif inspect.isasyncgen(stream_generator):\n                final_generator = stream_generator\n            else:  # Single item or string\n                async def _single_item_gen():\n                    yield stream_generator\n\n                final_generator = _single_item_gen()\n            all_response_headers = headers if headers else {}\n\n        # Prepare streaming data to be stored in the Result object\n        streaming_data = {\n            \"type\": \"stream\",  # Indicator for Rust side\n            \"generator\": final_generator,\n            \"content_type\": content_type,  # Let Rust know the intended content type\n            \"headers\": all_response_headers  # Intended HTTP headers for the overall response\n        }\n\n        result_payload = ToolBoxResult(\n            data_to=interface,\n            data=streaming_data,\n            data_info=\"Streaming response\" if content_type != \"text/event-stream\" else \"SSE Event Stream\",\n            data_type=\"stream\"  # Generic type for Rust to identify it needs to stream from 'generator'\n        )\n\n        return cls(error=error, info=info_obj, result=result_payload)\n\n    @classmethod\n    def sse(cls,\n            stream_generator: Any,\n            info: str = \"OK\",\n            interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n            cleanup_func: Union[\n                Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None,\n            # http_headers: Optional[dict] = None # If we want to allow overriding default SSE HTTP headers\n            ):\n        \"\"\"\n        Create an Server-Sent Events (SSE) streaming response Result.\n\n        Args:\n            stream_generator: A source yielding individual data items. This can be an\n                              async generator, sync generator, iterable, or a single item.\n                              Each item will be formatted as an SSE event.\n            info: Optional help text for the Result.\n            interface: Optional ToolBoxInterface to target.\n            cleanup_func: Optional cleanup function to run when the stream ends or is cancelled.\n            #http_headers: Optional dictionary of custom HTTP headers for the SSE response.\n\n        Returns:\n            A Result object configured for SSE streaming.\n        \"\"\"\n        # Result.stream will handle calling SSEGenerator.create_sse_stream\n        # and setting appropriate default headers for SSE when content_type is \"text/event-stream\".\n        return cls.stream(\n            stream_generator=stream_generator,\n            content_type=\"text/event-stream\",\n            # headers=http_headers, # Pass if we add http_headers param\n            info=info,\n            interface=interface,\n            cleanup_func=cleanup_func\n        )\n\n    @classmethod\n    def default(cls, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=-1, help_text=\"\")\n        result = ToolBoxResult(data_to=interface)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None):\n        \"\"\"Create a JSON response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code or exec_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=data,\n            data_info=\"JSON response\",\n            data_type=\"json\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n        \"\"\"Create a text response Result with specific content type.\"\"\"\n        if headers is not None:\n            return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=text_data,\n            data_info=\"Text response\",\n            data_type=content_type\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n               interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a binary data response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        # Create a dictionary with binary data and metadata\n        binary_data = {\n            \"data\": data,\n            \"content_type\": content_type,\n            \"filename\": download_name\n        }\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=binary_data,\n            data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n            data_type=\"binary\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def file(cls, data, filename, content_type=None, info=\"OK\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a file download response Result.\n\n        Args:\n            data: File data as bytes or base64 string\n            filename: Name of the file for download\n            content_type: MIME type of the file (auto-detected if None)\n            info: Response info text\n            interface: Target interface\n\n        Returns:\n            Result object configured for file download\n        \"\"\"\n        import base64\n        import mimetypes\n\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=200, help_text=info)\n\n        # Auto-detect content type if not provided\n        if content_type is None:\n            content_type, _ = mimetypes.guess_type(filename)\n            if content_type is None:\n                content_type = \"application/octet-stream\"\n\n        # Ensure data is base64 encoded string (as expected by Rust server)\n        if isinstance(data, bytes):\n            base64_data = base64.b64encode(data).decode('utf-8')\n        elif isinstance(data, str):\n            # Assume it's already base64 encoded\n            base64_data = data\n        else:\n            raise ValueError(\"File data must be bytes or base64 string\")\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=base64_data,  # Rust expects base64 string for \"file\" type\n            data_info=f\"File download: {filename}\",\n            data_type=\"file\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a redirect response.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=url,\n            data_info=\"Redirect response\",\n            data_type=\"redirect\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def ok(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def html(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.remote, data_type=\"html\",status=200, headers=None, row=False):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=status, help_text=info)\n        from ...utils.system.getting_and_closing_app import get_app\n\n        if not row and not '\"&lt;div class=\"main-content\"\"' in data:\n            data = f'&lt;div class=\"main-content frosted-glass\"&gt;{data}&lt;div&gt;'\n        if not row and not get_app().web_context() in data:\n            data = get_app().web_context() + data\n\n        if isinstance(headers, dict):\n            result = ToolBoxResult(data_to=interface, data={'html':data,'headers':headers}, data_info=data_info,\n                                   data_type=\"special_html\")\n        else:\n            result = ToolBoxResult(data_to=interface, data=data, data_info=data_info,\n                                   data_type=data_type if data_type is not None else type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def future(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.future):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=\"future\")\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def custom_error(cls, data=None, data_info=\"\", info=\"\", exec_code=-1, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def error(cls, data=None, data_info=\"\", info=\"\", exec_code=450, interface=ToolBoxInterfaces.remote):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_user_error(cls, info=\"\", exec_code=-3, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.input_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_internal_error(cls, info=\"\", exec_code=-2, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.internal_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    def print(self, show=True, show_data=True, prifix=\"\"):\n        data = '\\n' + f\"{((prifix + 'Data: ' + str(self.result.data) if self.result.data is not None else 'NO Data') if not isinstance(self.result.data, Result) else self.result.data.print(show=False, show_data=show_data, prifix=prifix + '-')) if show_data else 'Data: private'}\"\n        origin = '\\n' + f\"{prifix + 'Origin: ' + str(self.origin) if self.origin is not None else 'NO Origin'}\"\n        text = (f\"Function Exec code: {self.info.exec_code}\"\n                f\"\\n{prifix}Info's:\"\n                f\" {self.info.help_text} {'&lt;|&gt; ' + str(self.result.data_info) if self.result.data_info is not None else ''}\"\n                f\"{origin}{data if not data.endswith('NO Data') else ''}\")\n        if not show:\n            return text\n        print(\"\\n======== Result ========\\n\" + text + \"\\n------- EndOfD -------\")\n        return self\n\n    def log(self, show_data=True, prifix=\"\"):\n        from toolboxv2 import get_logger\n        get_logger().debug(self.print(show=False, show_data=show_data, prifix=prifix).replace(\"\\n\", \" - \"))\n        return self\n\n    def __str__(self):\n        return self.print(show=False, show_data=True)\n\n    def get(self, key=None, default=None):\n        data = self.result.data\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    async def aget(self, key=None, default=None):\n        if asyncio.isfuture(self.result.data) or asyncio.iscoroutine(self.result.data) or (\n            isinstance(self.result.data_to, Enum) and self.result.data_to.name == ToolBoxInterfaces.future.name):\n            data = await self.result.data\n        else:\n            data = self.get(key=None, default=None)\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    def lazy_return(self, _=0, data=None, **kwargs):\n        flags = ['raise', 'logg', 'user', 'intern']\n        flag = flags[_] if isinstance(_, int) else _\n        if self.info.exec_code == 0:\n            return self if data is None else data if _test_is_result(data) else self.ok(data=data, **kwargs)\n        if flag == 'raise':\n            raise ValueError(self.print(show=False))\n        if flag == 'logg':\n            from .. import get_logger\n            get_logger().error(self.print(show=False))\n\n        if flag == 'user':\n            return self if data is None else data if _test_is_result(data) else self.default_user_error(data=data,\n                                                                                                        **kwargs)\n        if flag == 'intern':\n            return self if data is None else data if _test_is_result(data) else self.default_internal_error(data=data,\n                                                                                                            **kwargs)\n\n        return self if data is None else data if _test_is_result(data) else self.custom_error(data=data, **kwargs)\n\n    @property\n    def bg_task(self):\n        return self._task\n</code></pre> <code>binary(data, content_type='application/octet-stream', download_name=None, info='OK', interface=ToolBoxInterfaces.remote)</code> <code>classmethod</code> \u00b6 <p>Create a binary data response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n           interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a binary data response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    # Create a dictionary with binary data and metadata\n    binary_data = {\n        \"data\": data,\n        \"content_type\": content_type,\n        \"filename\": download_name\n    }\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=binary_data,\n        data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n        data_type=\"binary\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre> <code>file(data, filename, content_type=None, info='OK', interface=ToolBoxInterfaces.remote)</code> <code>classmethod</code> \u00b6 <p>Create a file download response Result.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>File data as bytes or base64 string</p> required <code>filename</code> <p>Name of the file for download</p> required <code>content_type</code> <p>MIME type of the file (auto-detected if None)</p> <code>None</code> <code>info</code> <p>Response info text</p> <code>'OK'</code> <code>interface</code> <p>Target interface</p> <code>remote</code> <p>Returns:</p> Type Description <p>Result object configured for file download</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef file(cls, data, filename, content_type=None, info=\"OK\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a file download response Result.\n\n    Args:\n        data: File data as bytes or base64 string\n        filename: Name of the file for download\n        content_type: MIME type of the file (auto-detected if None)\n        info: Response info text\n        interface: Target interface\n\n    Returns:\n        Result object configured for file download\n    \"\"\"\n    import base64\n    import mimetypes\n\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=200, help_text=info)\n\n    # Auto-detect content type if not provided\n    if content_type is None:\n        content_type, _ = mimetypes.guess_type(filename)\n        if content_type is None:\n            content_type = \"application/octet-stream\"\n\n    # Ensure data is base64 encoded string (as expected by Rust server)\n    if isinstance(data, bytes):\n        base64_data = base64.b64encode(data).decode('utf-8')\n    elif isinstance(data, str):\n        # Assume it's already base64 encoded\n        base64_data = data\n    else:\n        raise ValueError(\"File data must be bytes or base64 string\")\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=base64_data,  # Rust expects base64 string for \"file\" type\n        data_info=f\"File download: {filename}\",\n        data_type=\"file\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre> <code>json(data, info='OK', interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None)</code> <code>classmethod</code> \u00b6 <p>Create a JSON response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None):\n    \"\"\"Create a JSON response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code or exec_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=data,\n        data_info=\"JSON response\",\n        data_type=\"json\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre> <code>redirect(url, status_code=302, info='Redirect', interface=ToolBoxInterfaces.remote)</code> <code>classmethod</code> \u00b6 <p>Create a redirect response.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a redirect response.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=url,\n        data_info=\"Redirect response\",\n        data_type=\"redirect\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre> <code>sse(stream_generator, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code> <code>classmethod</code> \u00b6 <p>Create an Server-Sent Events (SSE) streaming response Result.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <code>Any</code> <p>A source yielding individual data items. This can be an               async generator, sync generator, iterable, or a single item.               Each item will be formatted as an SSE event.</p> required <code>info</code> <code>str</code> <p>Optional help text for the Result.</p> <code>'OK'</code> <code>interface</code> <code>ToolBoxInterfaces</code> <p>Optional ToolBoxInterface to target.</p> <code>remote</code> <code>cleanup_func</code> <code>Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None]</code> <p>Optional cleanup function to run when the stream ends or is cancelled.</p> <code>None</code> <code>#http_headers</code> <p>Optional dictionary of custom HTTP headers for the SSE response.</p> required <p>Returns:</p> Type Description <p>A Result object configured for SSE streaming.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef sse(cls,\n        stream_generator: Any,\n        info: str = \"OK\",\n        interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n        cleanup_func: Union[\n            Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None,\n        # http_headers: Optional[dict] = None # If we want to allow overriding default SSE HTTP headers\n        ):\n    \"\"\"\n    Create an Server-Sent Events (SSE) streaming response Result.\n\n    Args:\n        stream_generator: A source yielding individual data items. This can be an\n                          async generator, sync generator, iterable, or a single item.\n                          Each item will be formatted as an SSE event.\n        info: Optional help text for the Result.\n        interface: Optional ToolBoxInterface to target.\n        cleanup_func: Optional cleanup function to run when the stream ends or is cancelled.\n        #http_headers: Optional dictionary of custom HTTP headers for the SSE response.\n\n    Returns:\n        A Result object configured for SSE streaming.\n    \"\"\"\n    # Result.stream will handle calling SSEGenerator.create_sse_stream\n    # and setting appropriate default headers for SSE when content_type is \"text/event-stream\".\n    return cls.stream(\n        stream_generator=stream_generator,\n        content_type=\"text/event-stream\",\n        # headers=http_headers, # Pass if we add http_headers param\n        info=info,\n        interface=interface,\n        cleanup_func=cleanup_func\n    )\n</code></pre> <code>stream(stream_generator, content_type='text/event-stream', headers=None, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code> <code>classmethod</code> \u00b6 <p>Create a streaming response Result. Handles SSE and other stream types.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <code>Any</code> <p>Any stream source (async generator, sync generator, iterable, or single item).</p> required <code>content_type</code> <code>str</code> <p>Content-Type header (default: text/event-stream for SSE).</p> <code>'text/event-stream'</code> <code>headers</code> <code>Union[dict, None]</code> <p>Additional HTTP headers for the response.</p> <code>None</code> <code>info</code> <code>str</code> <p>Help text for the result.</p> <code>'OK'</code> <code>interface</code> <code>ToolBoxInterfaces</code> <p>Interface to send data to.</p> <code>remote</code> <code>cleanup_func</code> <code>Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None]</code> <p>Optional function for cleanup.</p> <code>None</code> <p>Returns:</p> Type Description <p>A Result object configured for streaming.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef stream(cls,\n           stream_generator: Any,  # Renamed from source for clarity\n           content_type: str = \"text/event-stream\",  # Default to SSE\n           headers: Union[dict, None] = None,\n           info: str = \"OK\",\n           interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n           cleanup_func: Union[\n               Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None):\n    \"\"\"\n    Create a streaming response Result. Handles SSE and other stream types.\n\n    Args:\n        stream_generator: Any stream source (async generator, sync generator, iterable, or single item).\n        content_type: Content-Type header (default: text/event-stream for SSE).\n        headers: Additional HTTP headers for the response.\n        info: Help text for the result.\n        interface: Interface to send data to.\n        cleanup_func: Optional function for cleanup.\n\n    Returns:\n        A Result object configured for streaming.\n    \"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    final_generator: AsyncGenerator[str, None]\n\n    if content_type == \"text/event-stream\":\n        # For SSE, always use SSEGenerator.create_sse_stream to wrap the source.\n        # SSEGenerator.create_sse_stream handles various types of stream_generator internally.\n        final_generator = SSEGenerator.create_sse_stream(source=stream_generator, cleanup_func=cleanup_func)\n\n        # Standard SSE headers for the HTTP response itself\n        # These will be stored in the Result object. Rust side decides how to use them.\n        standard_sse_headers = {\n            \"Cache-Control\": \"no-cache\",  # SSE specific\n            \"Connection\": \"keep-alive\",  # SSE specific\n            \"X-Accel-Buffering\": \"no\",  # Useful for proxies with SSE\n            # Content-Type is implicitly text/event-stream, will be in streaming_data below\n        }\n        all_response_headers = standard_sse_headers.copy()\n        if headers:\n            all_response_headers.update(headers)\n    else:\n        # For non-SSE streams.\n        # If stream_generator is sync, wrap it to be async.\n        # If already async or single item, it will be handled.\n        # Rust's stream_generator in ToolboxClient seems to handle both sync/async Python generators.\n        # For consistency with how SSEGenerator does it, we can wrap sync ones.\n        if inspect.isgenerator(stream_generator) or \\\n            (not isinstance(stream_generator, str) and hasattr(stream_generator, '__iter__')):\n            final_generator = SSEGenerator.wrap_sync_generator(stream_generator)  # Simple async wrapper\n        elif inspect.isasyncgen(stream_generator):\n            final_generator = stream_generator\n        else:  # Single item or string\n            async def _single_item_gen():\n                yield stream_generator\n\n            final_generator = _single_item_gen()\n        all_response_headers = headers if headers else {}\n\n    # Prepare streaming data to be stored in the Result object\n    streaming_data = {\n        \"type\": \"stream\",  # Indicator for Rust side\n        \"generator\": final_generator,\n        \"content_type\": content_type,  # Let Rust know the intended content type\n        \"headers\": all_response_headers  # Intended HTTP headers for the overall response\n    }\n\n    result_payload = ToolBoxResult(\n        data_to=interface,\n        data=streaming_data,\n        data_info=\"Streaming response\" if content_type != \"text/event-stream\" else \"SSE Event Stream\",\n        data_type=\"stream\"  # Generic type for Rust to identify it needs to stream from 'generator'\n    )\n\n    return cls(error=error, info=info_obj, result=result_payload)\n</code></pre> <code>text(text_data, content_type='text/plain', exec_code=None, status=200, info='OK', interface=ToolBoxInterfaces.remote, headers=None)</code> <code>classmethod</code> \u00b6 <p>Create a text response Result with specific content type.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n    \"\"\"Create a text response Result with specific content type.\"\"\"\n    if headers is not None:\n        return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=text_data,\n        data_info=\"Text response\",\n        data_type=content_type\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.SSEGenerator","title":"<code>SSEGenerator</code>","text":"<p>Production-ready SSE generator that converts any data source to properly formatted Server-Sent Events compatible with browsers.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class SSEGenerator:\n    \"\"\"\n    Production-ready SSE generator that converts any data source to\n    properly formatted Server-Sent Events compatible with browsers.\n    \"\"\"\n\n    @staticmethod\n    def format_sse_event(data: Any) -&gt; str:\n        \"\"\"Format any data as a proper SSE event message.\"\"\"\n        # Already formatted as SSE\n        if isinstance(data, str) and (data.startswith('data:') or data.startswith('event:')) and '\\n\\n' in data:\n            return data\n\n        # Handle bytes (binary data)\n        if isinstance(data, bytes):\n            try:\n                # Try to decode as UTF-8 first\n                decoded_data_str = data.decode('utf-8')\n                # If decoding works, treat it as a string for further processing\n                # This allows binary data that is valid UTF-8 JSON to be processed as JSON.\n                data = decoded_data_str\n            except UnicodeDecodeError:\n                # Binary data that is not UTF-8, encode as base64\n                b64_data = base64.b64encode(data).decode('utf-8')\n                return f\"event: binary\\ndata: {b64_data}\\n\\n\"\n\n        # Convert non-string objects (that are not already bytes) to JSON string\n        # If data was bytes and successfully decoded to UTF-8 string, it will be processed here.\n        original_data_type_was_complex = False\n        if not isinstance(data, str):\n            original_data_type_was_complex = True\n            try:\n                data_str = json.dumps(data)\n            except Exception:\n                data_str = str(data)  # Fallback to string representation\n        else:\n            data_str = data  # data is already a string\n\n        # Handle JSON data with special event formatting\n        # data_str now holds the string representation (either original string or JSON string)\n        if data_str.strip().startswith('{'):\n            try:\n                json_data = json.loads(data_str)\n                if isinstance(json_data, dict) and 'event' in json_data:\n                    event_type = json_data['event']\n                    event_id = json_data.get('id', None)  # Use None to distinguish from empty string\n\n                    # Determine the actual data payload for the SSE 'data:' field\n                    # If 'data' key exists in json_data, use its content.\n                    # Otherwise, use the original data_str (which is the JSON of json_data).\n                    if 'data' in json_data:\n                        payload_content = json_data['data']\n                        # If payload_content is complex, re-serialize it to JSON string\n                        if isinstance(payload_content, (dict, list)):\n                            sse_data_field = json.dumps(payload_content)\n                        else:  # Simple type (string, number, bool)\n                            sse_data_field = str(payload_content)\n                    else:\n                        # If original data was complex (e.g. dict) and became json_data,\n                        # and no 'data' key in it, then use the full json_data as payload.\n                        # If original data was a simple string that happened to be JSON parsable\n                        # but without 'event' key, it would have been handled by \"Regular JSON without event\"\n                        # or \"Plain text\" later.\n                        # This path implies original data was a dict with 'event' key.\n                        sse_data_field = data_str\n\n                    sse_lines = []\n                    if event_type:  # Should always be true here\n                        sse_lines.append(f\"event: {event_type}\")\n                    if event_id is not None:  # Check for None, allow empty string id\n                        sse_lines.append(f\"id: {event_id}\")\n\n                    # Handle multi-line data for the data field\n                    for line in sse_data_field.splitlines():\n                        sse_lines.append(f\"data: {line}\")\n\n                    return \"\\n\".join(sse_lines) + \"\\n\\n\"\n                else:\n                    # Regular JSON without special 'event' key\n                    sse_lines = []\n                    for line in data_str.splitlines():\n                        sse_lines.append(f\"data: {line}\")\n                    return \"\\n\".join(sse_lines) + \"\\n\\n\"\n            except json.JSONDecodeError:\n                # Not valid JSON, treat as plain text\n                sse_lines = []\n                for line in data_str.splitlines():\n                    sse_lines.append(f\"data: {line}\")\n                return \"\\n\".join(sse_lines) + \"\\n\\n\"\n        else:\n            # Plain text\n            sse_lines = []\n            for line in data_str.splitlines():\n                sse_lines.append(f\"data: {line}\")\n            return \"\\n\".join(sse_lines) + \"\\n\\n\"\n\n    @classmethod\n    async def wrap_sync_generator(cls, generator):\n        \"\"\"Convert a synchronous generator to an async generator.\"\"\"\n        for item in generator:\n            yield item\n            # Allow other tasks to run\n            await asyncio.sleep(0)\n\n    @classmethod\n    async def create_sse_stream(\n        cls,\n        source: Any,  # Changed from positional arg to keyword for clarity in Result.stream\n        cleanup_func: Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None\n    ) -&gt; AsyncGenerator[str, None]:\n        \"\"\"\n        Convert any source to a properly formatted SSE stream.\n\n        Args:\n            source: Can be async generator, sync generator, iterable, or a single item.\n            cleanup_func: Optional function to call when the stream ends or is cancelled.\n                          Can be a synchronous function, async function, or async generator.\n\n        Yields:\n            Properly formatted SSE messages (strings).\n        \"\"\"\n        # Send stream start event\n        # This structure ensures data field contains {\"id\":\"0\"}\n        yield cls.format_sse_event({\"event\": \"stream_start\", \"data\": {\"id\": \"0\"}})\n\n        try:\n            # Handle different types of sources\n            if inspect.isasyncgen(source):\n                # Source is already an async generator\n                async for item in source:\n                    yield cls.format_sse_event(item)\n            elif inspect.isgenerator(source) or (not isinstance(source, str) and hasattr(source, '__iter__')):\n                # Source is a sync generator or iterable (but not a string)\n                # Strings are iterable but should be treated as single items unless explicitly made a generator\n                async for item in cls.wrap_sync_generator(source):\n                    yield cls.format_sse_event(item)\n            else:\n                # Single item (including strings)\n                yield cls.format_sse_event(source)\n        except asyncio.CancelledError:\n            # Client disconnected\n            yield cls.format_sse_event({\"event\": \"cancelled\", \"data\": {\"id\": \"cancelled\"}})\n            raise\n        except Exception as e:\n            # Error in stream\n            error_info = {\n                \"event\": \"error\",\n                \"data\": {  # Ensure payload is under 'data' key for the new format_sse_event logic\n                    \"message\": str(e),\n                    \"traceback\": traceback.format_exc()\n                }\n            }\n            yield cls.format_sse_event(error_info)\n        finally:\n            # Always send end event\n            yield cls.format_sse_event({\"event\": \"stream_end\", \"data\": {\"id\": \"final\"}})\n\n            # Execute cleanup function if provided\n            if cleanup_func:\n                try:\n                    if inspect.iscoroutinefunction(cleanup_func):  # Check if it's an async def function\n                        await cleanup_func()\n                    elif inspect.isasyncgenfunction(cleanup_func) or inspect.isasyncgen(\n                        cleanup_func):  # Check if it's an async def generator function or already an async generator\n                        # If it's a function, call it to get the generator\n                        gen_to_exhaust = cleanup_func() if inspect.isasyncgenfunction(cleanup_func) else cleanup_func\n                        async for _ in gen_to_exhaust:\n                            pass  # Exhaust the generator to ensure cleanup completes\n                    else:\n                        # Synchronous function\n                        cleanup_func()\n                except Exception as e:\n                    # Log cleanup errors but don't propagate them to client\n                    error_info_cleanup = {\n                        \"event\": \"cleanup_error\",\n                        \"data\": {  # Ensure payload is under 'data' key\n                            \"message\": str(e),\n                            \"traceback\": traceback.format_exc()\n                        }\n                    }\n                    # We can't yield here as the stream is already closing/closed.\n                    # Instead, log the error.\n                    # In a real app, use a proper logger.\n                    print(f\"SSE cleanup error: {cls.format_sse_event(error_info_cleanup)}\", flush=True)\n</code></pre> <code>create_sse_stream(source, cleanup_func=None)</code> <code>async</code> <code>classmethod</code> \u00b6 <p>Convert any source to a properly formatted SSE stream.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Any</code> <p>Can be async generator, sync generator, iterable, or a single item.</p> required <code>cleanup_func</code> <code>Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None]</code> <p>Optional function to call when the stream ends or is cancelled.           Can be a synchronous function, async function, or async generator.</p> <code>None</code> <p>Yields:</p> Type Description <code>AsyncGenerator[str, None]</code> <p>Properly formatted SSE messages (strings).</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\nasync def create_sse_stream(\n    cls,\n    source: Any,  # Changed from positional arg to keyword for clarity in Result.stream\n    cleanup_func: Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None\n) -&gt; AsyncGenerator[str, None]:\n    \"\"\"\n    Convert any source to a properly formatted SSE stream.\n\n    Args:\n        source: Can be async generator, sync generator, iterable, or a single item.\n        cleanup_func: Optional function to call when the stream ends or is cancelled.\n                      Can be a synchronous function, async function, or async generator.\n\n    Yields:\n        Properly formatted SSE messages (strings).\n    \"\"\"\n    # Send stream start event\n    # This structure ensures data field contains {\"id\":\"0\"}\n    yield cls.format_sse_event({\"event\": \"stream_start\", \"data\": {\"id\": \"0\"}})\n\n    try:\n        # Handle different types of sources\n        if inspect.isasyncgen(source):\n            # Source is already an async generator\n            async for item in source:\n                yield cls.format_sse_event(item)\n        elif inspect.isgenerator(source) or (not isinstance(source, str) and hasattr(source, '__iter__')):\n            # Source is a sync generator or iterable (but not a string)\n            # Strings are iterable but should be treated as single items unless explicitly made a generator\n            async for item in cls.wrap_sync_generator(source):\n                yield cls.format_sse_event(item)\n        else:\n            # Single item (including strings)\n            yield cls.format_sse_event(source)\n    except asyncio.CancelledError:\n        # Client disconnected\n        yield cls.format_sse_event({\"event\": \"cancelled\", \"data\": {\"id\": \"cancelled\"}})\n        raise\n    except Exception as e:\n        # Error in stream\n        error_info = {\n            \"event\": \"error\",\n            \"data\": {  # Ensure payload is under 'data' key for the new format_sse_event logic\n                \"message\": str(e),\n                \"traceback\": traceback.format_exc()\n            }\n        }\n        yield cls.format_sse_event(error_info)\n    finally:\n        # Always send end event\n        yield cls.format_sse_event({\"event\": \"stream_end\", \"data\": {\"id\": \"final\"}})\n\n        # Execute cleanup function if provided\n        if cleanup_func:\n            try:\n                if inspect.iscoroutinefunction(cleanup_func):  # Check if it's an async def function\n                    await cleanup_func()\n                elif inspect.isasyncgenfunction(cleanup_func) or inspect.isasyncgen(\n                    cleanup_func):  # Check if it's an async def generator function or already an async generator\n                    # If it's a function, call it to get the generator\n                    gen_to_exhaust = cleanup_func() if inspect.isasyncgenfunction(cleanup_func) else cleanup_func\n                    async for _ in gen_to_exhaust:\n                        pass  # Exhaust the generator to ensure cleanup completes\n                else:\n                    # Synchronous function\n                    cleanup_func()\n            except Exception as e:\n                # Log cleanup errors but don't propagate them to client\n                error_info_cleanup = {\n                    \"event\": \"cleanup_error\",\n                    \"data\": {  # Ensure payload is under 'data' key\n                        \"message\": str(e),\n                        \"traceback\": traceback.format_exc()\n                    }\n                }\n                # We can't yield here as the stream is already closing/closed.\n                # Instead, log the error.\n                # In a real app, use a proper logger.\n                print(f\"SSE cleanup error: {cls.format_sse_event(error_info_cleanup)}\", flush=True)\n</code></pre> <code>format_sse_event(data)</code> <code>staticmethod</code> \u00b6 <p>Format any data as a proper SSE event message.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@staticmethod\ndef format_sse_event(data: Any) -&gt; str:\n    \"\"\"Format any data as a proper SSE event message.\"\"\"\n    # Already formatted as SSE\n    if isinstance(data, str) and (data.startswith('data:') or data.startswith('event:')) and '\\n\\n' in data:\n        return data\n\n    # Handle bytes (binary data)\n    if isinstance(data, bytes):\n        try:\n            # Try to decode as UTF-8 first\n            decoded_data_str = data.decode('utf-8')\n            # If decoding works, treat it as a string for further processing\n            # This allows binary data that is valid UTF-8 JSON to be processed as JSON.\n            data = decoded_data_str\n        except UnicodeDecodeError:\n            # Binary data that is not UTF-8, encode as base64\n            b64_data = base64.b64encode(data).decode('utf-8')\n            return f\"event: binary\\ndata: {b64_data}\\n\\n\"\n\n    # Convert non-string objects (that are not already bytes) to JSON string\n    # If data was bytes and successfully decoded to UTF-8 string, it will be processed here.\n    original_data_type_was_complex = False\n    if not isinstance(data, str):\n        original_data_type_was_complex = True\n        try:\n            data_str = json.dumps(data)\n        except Exception:\n            data_str = str(data)  # Fallback to string representation\n    else:\n        data_str = data  # data is already a string\n\n    # Handle JSON data with special event formatting\n    # data_str now holds the string representation (either original string or JSON string)\n    if data_str.strip().startswith('{'):\n        try:\n            json_data = json.loads(data_str)\n            if isinstance(json_data, dict) and 'event' in json_data:\n                event_type = json_data['event']\n                event_id = json_data.get('id', None)  # Use None to distinguish from empty string\n\n                # Determine the actual data payload for the SSE 'data:' field\n                # If 'data' key exists in json_data, use its content.\n                # Otherwise, use the original data_str (which is the JSON of json_data).\n                if 'data' in json_data:\n                    payload_content = json_data['data']\n                    # If payload_content is complex, re-serialize it to JSON string\n                    if isinstance(payload_content, (dict, list)):\n                        sse_data_field = json.dumps(payload_content)\n                    else:  # Simple type (string, number, bool)\n                        sse_data_field = str(payload_content)\n                else:\n                    # If original data was complex (e.g. dict) and became json_data,\n                    # and no 'data' key in it, then use the full json_data as payload.\n                    # If original data was a simple string that happened to be JSON parsable\n                    # but without 'event' key, it would have been handled by \"Regular JSON without event\"\n                    # or \"Plain text\" later.\n                    # This path implies original data was a dict with 'event' key.\n                    sse_data_field = data_str\n\n                sse_lines = []\n                if event_type:  # Should always be true here\n                    sse_lines.append(f\"event: {event_type}\")\n                if event_id is not None:  # Check for None, allow empty string id\n                    sse_lines.append(f\"id: {event_id}\")\n\n                # Handle multi-line data for the data field\n                for line in sse_data_field.splitlines():\n                    sse_lines.append(f\"data: {line}\")\n\n                return \"\\n\".join(sse_lines) + \"\\n\\n\"\n            else:\n                # Regular JSON without special 'event' key\n                sse_lines = []\n                for line in data_str.splitlines():\n                    sse_lines.append(f\"data: {line}\")\n                return \"\\n\".join(sse_lines) + \"\\n\\n\"\n        except json.JSONDecodeError:\n            # Not valid JSON, treat as plain text\n            sse_lines = []\n            for line in data_str.splitlines():\n                sse_lines.append(f\"data: {line}\")\n            return \"\\n\".join(sse_lines) + \"\\n\\n\"\n    else:\n        # Plain text\n        sse_lines = []\n        for line in data_str.splitlines():\n            sse_lines.append(f\"data: {line}\")\n        return \"\\n\".join(sse_lines) + \"\\n\\n\"\n</code></pre> <code>wrap_sync_generator(generator)</code> <code>async</code> <code>classmethod</code> \u00b6 <p>Convert a synchronous generator to an async generator.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\nasync def wrap_sync_generator(cls, generator):\n    \"\"\"Convert a synchronous generator to an async generator.\"\"\"\n    for item in generator:\n        yield item\n        # Allow other tasks to run\n        await asyncio.sleep(0)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.Session","title":"<code>Session</code>  <code>dataclass</code>","text":"<p>Class representing a session.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@dataclass\nclass Session:\n    \"\"\"Class representing a session.\"\"\"\n    SiID: str\n    level: str\n    spec: str\n    user_name: str\n    # Allow for additional fields\n    extra_data: dict[str, Any] = field(default_factory=dict)\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any]) -&gt; 'Session':\n        \"\"\"Create a Session instance from a dictionary with default values.\"\"\"\n        known_fields = {\n            'SiID': data.get('SiID', '#0'),\n            'level': data.get('level', -1),\n            'spec': data.get('spec', 'app'),\n            'user_name': data.get('user_name', 'anonymous'),\n        }\n\n        extra_data = {k: v for k, v in data.items() if k not in known_fields}\n        return cls(**known_fields, extra_data=extra_data)\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert the Session object back to a dictionary.\"\"\"\n        result = {\n            'SiID': self.SiID,\n            'level': self.level,\n            'spec': self.spec,\n            'user_name': self.user_name,\n        }\n\n        # Add extra data\n        result.update(self.extra_data)\n\n        return result\n\n    @property\n    def valid(self):\n        return int(self.level) &gt; 0\n</code></pre> <code>from_dict(data)</code> <code>classmethod</code> \u00b6 <p>Create a Session instance from a dictionary with default values.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; 'Session':\n    \"\"\"Create a Session instance from a dictionary with default values.\"\"\"\n    known_fields = {\n        'SiID': data.get('SiID', '#0'),\n        'level': data.get('level', -1),\n        'spec': data.get('spec', 'app'),\n        'user_name': data.get('user_name', 'anonymous'),\n    }\n\n    extra_data = {k: v for k, v in data.items() if k not in known_fields}\n    return cls(**known_fields, extra_data=extra_data)\n</code></pre> <code>to_dict()</code> \u00b6 <p>Convert the Session object back to a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert the Session object back to a dictionary.\"\"\"\n    result = {\n        'SiID': self.SiID,\n        'level': self.level,\n        'spec': self.spec,\n        'user_name': self.user_name,\n    }\n\n    # Add extra data\n    result.update(self.extra_data)\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.system.types.parse_request_data","title":"<code>parse_request_data(data)</code>","text":"<p>Parse the incoming request data into a strongly typed structure.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def parse_request_data(data: dict[str, Any]) -&gt; RequestData:\n    \"\"\"Parse the incoming request data into a strongly typed structure.\"\"\"\n    return RequestData.from_dict(data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox","title":"<code>toolbox</code>","text":"<p>Main module.</p>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App","title":"<code>App</code>","text":"Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>class App(AppType, metaclass=Singleton):\n\n    def __init__(self, prefix: str = \"\", args=AppArgs().default()):\n        super().__init__(prefix, args)\n        self._web_context = None\n        t0 = time.perf_counter()\n        abspath = os.path.abspath(__file__)\n        self.system_flag = system()  # Linux: Linux Mac: Darwin Windows: Windows\n\n        self.appdata = os.getenv('APPDATA') if os.name == 'nt' else os.getenv('XDG_CONFIG_HOME') or os.path.expanduser(\n                '~/.config') if os.name == 'posix' else None\n\n        if self.system_flag == \"Darwin\" or self.system_flag == \"Linux\":\n            dir_name = os.path.dirname(abspath).replace(\"/utils\", \"\")\n        else:\n            dir_name = os.path.dirname(abspath).replace(\"\\\\utils\", \"\")\n\n        self.start_dir = str(dir_name)\n\n        self.bg_tasks = []\n\n        lapp = dir_name + '\\\\.data\\\\'\n\n        if not prefix:\n            if not os.path.exists(f\"{lapp}last-app-prefix.txt\"):\n                os.makedirs(lapp, exist_ok=True)\n                open(f\"{lapp}last-app-prefix.txt\", \"a\").close()\n            with open(f\"{lapp}last-app-prefix.txt\") as prefix_file:\n                cont = prefix_file.read()\n                if cont:\n                    prefix = cont.rstrip()\n        else:\n            if not os.path.exists(f\"{lapp}last-app-prefix.txt\"):\n                os.makedirs(lapp, exist_ok=True)\n                open(f\"{lapp}last-app-prefix.txt\", \"a\").close()\n            with open(f\"{lapp}last-app-prefix.txt\", \"w\") as prefix_file:\n                prefix_file.write(prefix)\n\n        self.prefix = prefix\n\n        node_ = node()\n\n        if 'localhost' in node_ and (host := os.getenv('HOSTNAME', 'localhost')) != 'localhost':\n            node_ = node_.replace('localhost', host)\n        self.id = prefix + '-' + node_\n        self.globals = {\n            \"root\": {**globals()},\n        }\n        self.locals = {\n            \"user\": {'app': self, **locals()},\n        }\n\n        identification = self.id\n        collective_identification = self.id\n        if \"test\" in prefix:\n            if self.system_flag == \"Darwin\" or self.system_flag == \"Linux\":\n                start_dir = self.start_dir.replace(\"ToolBoxV2/toolboxv2\", \"toolboxv2\")\n            else:\n                start_dir = self.start_dir.replace(\"ToolBoxV2\\\\toolboxv2\", \"toolboxv2\")\n            self.data_dir = start_dir + '\\\\.data\\\\' + \"test\"\n            self.config_dir = start_dir + '\\\\.config\\\\' + \"test\"\n            self.info_dir = start_dir + '\\\\.info\\\\' + \"test\"\n        elif identification.startswith('collective-'):\n            collective_identification = identification.split('-')[1]\n            self.data_dir = self.start_dir + '\\\\.data\\\\' + collective_identification\n            self.config_dir = self.start_dir + '\\\\.config\\\\' + collective_identification\n            self.info_dir = self.start_dir + '\\\\.info\\\\' + collective_identification\n            self.id = collective_identification\n        else:\n            self.data_dir = self.start_dir + '\\\\.data\\\\' + identification\n            self.config_dir = self.start_dir + '\\\\.config\\\\' + identification\n            self.info_dir = self.start_dir + '\\\\.info\\\\' + identification\n\n        if self.appdata is None:\n            self.appdata = self.data_dir\n        else:\n            self.appdata += \"/ToolBoxV2\"\n\n        if not os.path.exists(self.appdata):\n            os.makedirs(self.appdata, exist_ok=True)\n        if not os.path.exists(self.data_dir):\n            os.makedirs(self.data_dir, exist_ok=True)\n        if not os.path.exists(self.config_dir):\n            os.makedirs(self.config_dir, exist_ok=True)\n        if not os.path.exists(self.info_dir):\n            os.makedirs(self.info_dir, exist_ok=True)\n\n        print(f\"Starting ToolBox as {prefix} from :\", Style.Bold(Style.CYAN(f\"{os.getcwd()}\")))\n\n        logger_info_str, self.logger, self.logging_filename = self.set_logger(args.debug)\n\n        print(\"Logger \" + logger_info_str)\n        print(\"================================\")\n        self.logger.info(\"Logger initialized\")\n        get_logger().info(Style.GREEN(\"Starting Application instance\"))\n        if args.init and args.init is not None and self.start_dir not in sys.path:\n            sys.path.append(self.start_dir)\n\n        __version__ = get_version_from_pyproject()\n        self.version = __version__\n\n        self.keys = {\n            \"MACRO\": \"macro~~~~:\",\n            \"MACRO_C\": \"m_color~~:\",\n            \"HELPER\": \"helper~~~:\",\n            \"debug\": \"debug~~~~:\",\n            \"id\": \"name-spa~:\",\n            \"st-load\": \"mute~load:\",\n            \"comm-his\": \"comm-his~:\",\n            \"develop-mode\": \"dev~mode~:\",\n            \"provider::\": \"provider::\",\n        }\n\n        defaults = {\n            \"MACRO\": ['Exit'],\n            \"MACRO_C\": {},\n            \"HELPER\": {},\n            \"debug\": args.debug,\n            \"id\": self.id,\n            \"st-load\": False,\n            \"comm-his\": [[]],\n            \"develop-mode\": False,\n        }\n        self.config_fh = FileHandler(collective_identification + \".config\", keys=self.keys, defaults=defaults)\n        self.config_fh.load_file_handler()\n        self._debug = args.debug\n        self.flows = {}\n        self.dev_modi = self.config_fh.get_file_handler(self.keys[\"develop-mode\"])\n        if self.config_fh.get_file_handler(\"provider::\") is None:\n            self.config_fh.add_to_save_file_handler(\"provider::\", \"http://localhost:\" + str(\n                self.args_sto.port) if os.environ.get(\"HOSTNAME\",\"localhost\") == \"localhost\" else \"https://simplecore.app\")\n        self.functions = {}\n        self.modules = {}\n\n        self.interface_type = ToolBoxInterfaces.native\n        self.PREFIX = Style.CYAN(f\"~{node()}@&gt;\")\n        self.alive = True\n        self.called_exit = False, time.time()\n\n        self.print(f\"Infos:\\n  {'Name':&lt;8} -&gt; {node()}\\n  {'ID':&lt;8} -&gt; {self.id}\\n  {'Version':&lt;8} -&gt; {self.version}\\n\")\n\n        self.logger.info(\n            Style.GREEN(\n                f\"Finish init up in {time.perf_counter() - t0:.2f}s\"\n            )\n        )\n\n        self.args_sto = args\n        self.loop = None\n\n        from .system.session import Session\n        self.session: Session = Session(self.get_username())\n        if len(sys.argv) &gt; 2 and \"db\" == sys.argv[1]:\n            return\n        from .system.db_cli_manager import ClusterManager, get_executable_path\n        self.cluster_manager = ClusterManager()\n        online_list, server_list = self.cluster_manager.status_all(silent=True)\n        if not server_list:\n            self.cluster_manager.start_all(get_executable_path(), self.version)\n            _, server_list = self.cluster_manager.status_all()\n        from .extras.blobs import BlobStorage\n        self.root_blob_storage = BlobStorage(servers=server_list, storage_directory=self.data_dir+ '\\\\blob_cache\\\\')\n\n    def get_username(self, get_input=False, default=\"loot\") -&gt; str:\n        user_name = self.config_fh.get_file_handler(\"ac_user:::\")\n        if get_input and user_name is None:\n            user_name = input(\"Input your username: \")\n            self.config_fh.add_to_save_file_handler(\"ac_user:::\", user_name)\n        if user_name is None:\n            user_name = default\n            self.config_fh.add_to_save_file_handler(\"ac_user:::\", user_name)\n        return user_name\n\n    def set_username(self, username):\n        return self.config_fh.add_to_save_file_handler(\"ac_user:::\", username)\n\n    @staticmethod\n    def exit_main(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def hide_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def show_console(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    @staticmethod\n    def disconnect(*args, **kwargs):\n        \"\"\"proxi attr\"\"\"\n\n    def set_logger(self, debug=False):\n        if \"test\" in self.prefix and not debug:\n            logger, logging_filename = setup_logging(logging.NOTSET, name=\"toolbox-test\", interminal=True,\n                                                     file_level=logging.NOTSET, app_name=self.id)\n            logger_info_str = \"in Test Mode\"\n        elif \"live\" in self.prefix and not debug:\n            logger, logging_filename = setup_logging(logging.DEBUG, name=\"toolbox-live\", interminal=False,\n                                                     file_level=logging.WARNING, app_name=self.id)\n            logger_info_str = \"in Live Mode\"\n            # setup_logging(logging.WARNING, name=\"toolbox-live\", is_online=True\n            #              , online_level=logging.WARNING).info(\"Logger initialized\")\n        elif \"debug\" in self.prefix or self.prefix.endswith(\"D\"):\n            self.prefix = self.prefix.replace(\"-debug\", '').replace(\"debug\", '')\n            logger, logging_filename = setup_logging(logging.DEBUG, name=\"toolbox-debug\", interminal=True,\n                                                     file_level=logging.WARNING, app_name=self.id)\n            logger_info_str = \"in debug Mode\"\n            self.debug = True\n        elif debug:\n            logger, logging_filename = setup_logging(logging.DEBUG, name=f\"toolbox-{self.prefix}-debug\",\n                                                     interminal=True,\n                                                     file_level=logging.DEBUG, app_name=self.id)\n            logger_info_str = \"in args debug Mode\"\n        else:\n            logger, logging_filename = setup_logging(logging.ERROR, name=f\"toolbox-{self.prefix}\", app_name=self.id)\n            logger_info_str = \"in Default\"\n\n        return logger_info_str, logger, logging_filename\n\n    @property\n    def debug(self):\n        return self._debug\n\n    @debug.setter\n    def debug(self, value):\n        if not isinstance(value, bool):\n            self.logger.debug(f\"Value must be an boolean. is : {value} type of {type(value)}\")\n            raise ValueError(\"Value must be an boolean.\")\n\n        # self.logger.info(f\"Setting debug {value}\")\n        self._debug = value\n\n    def debug_rains(self, e):\n        if self.debug:\n            import traceback\n            x = \"=\"*5\n            x += \" DEBUG \"\n            x += \"=\"*5\n            self.print(x)\n            self.print(traceback.format_exc())\n            self.print(x)\n            raise e\n        else:\n            self.logger.error(f\"Error: {e}\")\n            import traceback\n            x = \"=\"*5\n            x += \" DEBUG \"\n            x += \"=\"*5\n            self.print(x)\n            self.print(traceback.format_exc())\n            self.print(x)\n\n    def set_flows(self, r):\n        self.flows = r\n\n    async def run_flows(self, name, **kwargs):\n        from ..flows import flows_dict as flows_dict_func\n        if name not in self.flows:\n            self.flows = {**self.flows, **flows_dict_func(s=name, remote=True)}\n        if name in self.flows:\n            if asyncio.iscoroutinefunction(self.flows[name]):\n                return await self.flows[name](get_app(from_=\"runner\"), self.args_sto, **kwargs)\n            else:\n                return self.flows[name](get_app(from_=\"runner\"), self.args_sto, **kwargs)\n        else:\n            print(\"Flow not found, active flows:\", len(self.flows.keys()))\n\n    def _coppy_mod(self, content, new_mod_dir, mod_name, file_type='py'):\n\n        mode = 'xb'\n        self.logger.info(f\" coppy mod {mod_name} to {new_mod_dir} size : {sys.getsizeof(content) / 8388608:.3f} mb\")\n\n        if not os.path.exists(new_mod_dir):\n            os.makedirs(new_mod_dir)\n            with open(f\"{new_mod_dir}/__init__.py\", \"w\") as nmd:\n                nmd.write(f\"__version__ = '{self.version}'\")\n\n        if os.path.exists(f\"{new_mod_dir}/{mod_name}.{file_type}\"):\n            mode = False\n\n            with open(f\"{new_mod_dir}/{mod_name}.{file_type}\", 'rb') as d:\n                runtime_mod = d.read()  # Testing version but not efficient\n\n            if len(content) != len(runtime_mod):\n                mode = 'wb'\n\n        if mode:\n            with open(f\"{new_mod_dir}/{mod_name}.{file_type}\", mode) as f:\n                f.write(content)\n\n    def _pre_lib_mod(self, mod_name, path_to=\"./runtime\", file_type='py'):\n        working_dir = self.id.replace(\".\", \"_\")\n        lib_mod_dir = f\"toolboxv2.runtime.{working_dir}.mod_lib.\"\n\n        self.logger.info(f\"pre_lib_mod {mod_name} from {lib_mod_dir}\")\n\n        postfix = \"_dev\" if self.dev_modi else \"\"\n        mod_file_dir = f\"./mods{postfix}/{mod_name}.{file_type}\"\n        new_mod_dir = f\"{path_to}/{working_dir}/mod_lib\"\n        with open(mod_file_dir, \"rb\") as c:\n            content = c.read()\n        self._coppy_mod(content, new_mod_dir, mod_name, file_type=file_type)\n        return lib_mod_dir\n\n    def _copy_load(self, mod_name, file_type='py', **kwargs):\n        loc = self._pre_lib_mod(mod_name, file_type)\n        return self.inplace_load_instance(mod_name, loc=loc, **kwargs)\n\n    def helper_install_pip_module(self, module_name):\n        if 'main' in self.id:\n            return\n        self.print(f\"Installing {module_name} GREEDY\")\n        os.system(f\"{sys.executable} -m pip install {module_name}\")\n\n    def python_module_import_classifier(self, mod_name, error_message):\n\n        if error_message.startswith(\"No module named 'toolboxv2.utils\"):\n            return Result.default_internal_error(f\"404 {error_message.split('utils')[1]} not found\")\n        if error_message.startswith(\"No module named 'toolboxv2.mods\"):\n            if mod_name.startswith('.'):\n                return\n            return self.run_a_from_sync(self.a_run_any, (\"CloudM\", \"install\"), module_name=mod_name)\n        if error_message.startswith(\"No module named '\"):\n            pip_requ = error_message.split(\"'\")[1].replace(\"'\", \"\").strip()\n            # if 'y' in input(f\"\\t\\t\\tAuto install {pip_requ} Y/n\").lower:\n            return self.helper_install_pip_module(pip_requ)\n            # return Result.default_internal_error(f\"404 {pip_requ} not found\")\n\n    def inplace_load_instance(self, mod_name, loc=\"toolboxv2.mods.\", spec='app', save=True, mfo=None):\n        if self.dev_modi and loc == \"toolboxv2.mods.\":\n            loc = \"toolboxv2.mods_dev.\"\n        if spec=='app' and self.mod_online(mod_name):\n            self.logger.info(f\"Reloading mod from : {loc + mod_name}\")\n            self.remove_mod(mod_name, spec=spec, delete=False)\n\n        if (os.path.exists(self.start_dir + '/mods/' + mod_name) or os.path.exists(\n            self.start_dir + '/mods/' + mod_name + '.py')) and (\n            os.path.isdir(self.start_dir + '/mods/' + mod_name) or os.path.isfile(\n            self.start_dir + '/mods/' + mod_name + '.py')):\n            try:\n                if mfo is None:\n                    modular_file_object = import_module(loc + mod_name)\n                else:\n                    modular_file_object = mfo\n                self.modules[mod_name] = modular_file_object\n            except ModuleNotFoundError as e:\n                self.logger.error(Style.RED(f\"module {loc + mod_name} not found is type sensitive {e}\"))\n                self.print(Style.RED(f\"module {loc + mod_name} not found is type sensitive {e}\"))\n                if self.debug or self.args_sto.sysPrint:\n                    self.python_module_import_classifier(mod_name, str(e))\n                self.debug_rains(e)\n                return None\n        else:\n            self.print(f\"module {loc + mod_name} is not valid\")\n            return None\n        if hasattr(modular_file_object, \"Tools\"):\n            tools_class = modular_file_object.Tools\n        else:\n            if hasattr(modular_file_object, \"name\"):\n                tools_class = modular_file_object\n                modular_file_object = import_module(loc + mod_name)\n            else:\n                tools_class = None\n\n        modular_id = None\n        instance = modular_file_object\n        app_instance_type = \"file/application\"\n\n        if tools_class is None:\n            modular_id = modular_file_object.Name if hasattr(modular_file_object, \"Name\") else mod_name\n\n        if tools_class is None and modular_id is None:\n            modular_id = str(modular_file_object.__name__)\n            self.logger.warning(f\"Unknown instance loaded {mod_name}\")\n            return modular_file_object\n\n        if tools_class is not None:\n            tools_class = self.save_initialized_module(tools_class, spec)\n            modular_id = tools_class.name\n            app_instance_type = \"functions/class\"\n        else:\n            instance.spec = spec\n        # if private:\n        #     self.functions[modular_id][f\"{spec}_private\"] = private\n\n        if not save:\n            return instance if tools_class is None else tools_class\n\n        return self.save_instance(instance, modular_id, spec, app_instance_type, tools_class=tools_class)\n\n    def save_instance(self, instance, modular_id, spec='app', instance_type=\"file/application\", tools_class=None):\n\n        if modular_id in self.functions and tools_class is None:\n            if self.functions[modular_id].get(f\"{spec}_instance\", None) is None:\n                self.functions[modular_id][f\"{spec}_instance\"] = instance\n                self.functions[modular_id][f\"{spec}_instance_type\"] = instance_type\n            else:\n                self.print(\"ERROR OVERRIDE\")\n                raise ImportError(f\"Module already known {modular_id}\")\n\n        elif tools_class is not None:\n            if modular_id not in self.functions:\n                self.functions[modular_id] = {}\n            self.functions[modular_id][f\"{spec}_instance\"] = tools_class\n            self.functions[modular_id][f\"{spec}_instance_type\"] = instance_type\n\n            try:\n                if not hasattr(tools_class, 'tools'):\n                    tools_class.tools = {\"Version\": tools_class.get_version, 'name': tools_class.name}\n                for function_name in list(tools_class.tools.keys()):\n                    t_function_name = function_name.lower()\n                    if t_function_name != \"all\" and t_function_name != \"name\":\n                        self.tb(function_name, mod_name=modular_id)(tools_class.tools.get(function_name))\n                self.functions[modular_id][f\"{spec}_instance_type\"] += \"/BC\"\n                if hasattr(tools_class, 'on_exit'):\n                    if \"on_exit\" in self.functions[modular_id]:\n                        self.functions[modular_id][\"on_exit\"].append(tools_class.on_exit)\n                    else:\n                        self.functions[modular_id][\"on_exit\"] = [tools_class.on_exit]\n            except Exception as e:\n                self.logger.error(f\"Starting Module {modular_id} compatibility failed with : {e}\")\n                pass\n        elif modular_id not in self.functions and tools_class is None:\n            self.functions[modular_id] = {}\n            self.functions[modular_id][f\"{spec}_instance\"] = instance\n            self.functions[modular_id][f\"{spec}_instance_type\"] = instance_type\n\n        else:\n            raise ImportError(f\"Modular {modular_id} is not a valid mod\")\n        on_start = self.functions[modular_id].get(\"on_start\")\n        if on_start is not None:\n            i = 1\n            for f in on_start:\n                try:\n                    f_, e = self.get_function((modular_id, f), state=True, specification=spec)\n                    if e == 0:\n                        self.logger.info(Style.GREY(f\"Running On start {f} {i}/{len(on_start)}\"))\n                        if asyncio.iscoroutinefunction(f_):\n                            self.print(f\"Async on start is only in Tool claas supported for {modular_id}.{f}\" if tools_class is None else f\"initialization starting soon for {modular_id}.{f}\")\n                            self.run_bg_task_advanced(f_)\n                        else:\n                            o = f_()\n                            if o is not None:\n                                self.print(f\"Function {modular_id} On start result: {o}\")\n                    else:\n                        self.logger.warning(f\"starting function not found {e}\")\n                except Exception as e:\n                    self.logger.debug(Style.YELLOW(\n                        Style.Bold(f\"modular:{modular_id}.{f} on_start error {i}/{len(on_start)} -&gt; {e}\")))\n                    self.debug_rains(e)\n                finally:\n                    i += 1\n        return instance if tools_class is None else tools_class\n\n    def save_initialized_module(self, tools_class, spec):\n        tools_class.spec = spec\n        live_tools_class = tools_class(app=self)\n        return live_tools_class\n\n    def mod_online(self, mod_name, installed=False):\n        if installed and mod_name not in self.functions:\n            self.save_load(mod_name)\n        return mod_name in self.functions\n\n    def _get_function(self,\n                      name: Enum or None,\n                      state: bool = True,\n                      specification: str = \"app\",\n                      metadata=False, as_str: tuple or None = None, r=0, **kwargs):\n\n        if as_str is None and isinstance(name, Enum):\n            modular_id = str(name.NAME.value)\n            function_id = str(name.value)\n        elif as_str is None and isinstance(name, list):\n            modular_id, function_id = name[0], name[1]\n        else:\n            modular_id, function_id = as_str\n\n        self.logger.info(f\"getting function : {specification}.{modular_id}.{function_id}\")\n\n        if modular_id not in self.functions:\n            if r == 0:\n                self.save_load(modular_id, spec=specification)\n                return self.get_function(name=(modular_id, function_id),\n                                         state=state,\n                                         specification=specification,\n                                         metadata=metadata,\n                                         r=1)\n            self.logger.warning(f\"function modular not found {modular_id} 404\")\n            return \"404\", 404\n\n        if function_id not in self.functions[modular_id]:\n            self.logger.warning(f\"function data not found {modular_id}.{function_id} 404\")\n            return \"404\", 404\n\n        function_data = self.functions[modular_id][function_id]\n\n        if isinstance(function_data, list):\n            print(f\"functions {function_id} : {function_data}\")\n            function_data = self.functions[modular_id][function_data[kwargs.get('i', -1)]]\n            print(f\"functions {modular_id} : {function_data}\")\n        function = function_data.get(\"func\")\n        params = function_data.get(\"params\")\n\n        state_ = function_data.get(\"state\")\n        if state_ is not None and state != state_:\n            state = state_\n\n        if function is None:\n            self.logger.warning(\"No function found\")\n            return \"404\", 404\n\n        if params is None:\n            self.logger.warning(\"No function (params) found\")\n            return \"404\", 301\n\n        if metadata and not state:\n            self.logger.info(\"returning metadata stateless\")\n            return (function_data, function), 0\n\n        if not state:  # mens a stateless function\n            self.logger.info(\"returning stateless function\")\n            return function, 0\n\n        instance = self.functions[modular_id].get(f\"{specification}_instance\")\n\n        # instance_type = self.functions[modular_id].get(f\"{specification}_instance_type\", \"functions/class\")\n\n        if params[0] == 'app':\n            instance = get_app(from_=f\"fuction {specification}.{modular_id}.{function_id}\")\n\n        if instance is None and self.alive:\n            self.inplace_load_instance(modular_id, spec=specification)\n            instance = self.functions[modular_id].get(f\"{specification}_instance\")\n\n        if instance is None:\n            self.logger.warning(\"No live Instance found\")\n            return \"404\", 400\n\n        # if instance_type.endswith(\"/BC\"):  # for backwards compatibility  functions/class/BC old modules\n        #     # returning as stateless\n        #     # return \"422\", -1\n        #     self.logger.info(\n        #         f\"returning stateless function, cant find tools class for state handling found {instance_type}\")\n        #     if metadata:\n        #         self.logger.info(f\"returning metadata stateless\")\n        #         return (function_data, function), 0\n        #     return function, 0\n\n        self.logger.info(\"wrapping in higher_order_function\")\n\n        self.logger.info(f\"returned fuction {specification}.{modular_id}.{function_id}\")\n        higher_order_function = partial(function, instance)\n\n        if metadata:\n            self.logger.info(\"returning metadata stateful\")\n            return (function_data, higher_order_function), 0\n\n        self.logger.info(\"returning stateful function\")\n        return higher_order_function, 0\n\n    def save_exit(self):\n        self.logger.info(f\"save exiting saving data to {self.config_fh.file_handler_filename} states of {self.debug=}\")\n        self.config_fh.add_to_save_file_handler(self.keys[\"debug\"], str(self.debug))\n\n    def init_mod(self, mod_name, spec='app'):\n        if '.' in mod_name:\n            mod_name = mod_name.split('.')[0]\n        return self.loop_gard().run_until_complete(self.a_init_mod(mod_name, spec))\n\n    def run_bg_task(self, task: Callable, *args, **kwargs) -&gt; Optional[asyncio.Task]:\n        \"\"\"\n        Runs a coroutine in the background without blocking the caller.\n\n        This is the primary method for \"fire-and-forget\" async tasks. It schedules\n        the coroutine to run on the application's main event loop.\n\n        Args:\n            task: The coroutine function to run.\n            *args: Arguments to pass to the coroutine function.\n            **kwargs: Keyword arguments to pass to the coroutine function.\n\n        Returns:\n            An asyncio.Task object representing the scheduled task, or None if\n            the task could not be scheduled.\n        \"\"\"\n        if not callable(task):\n            self.logger.warning(\"Task passed to run_bg_task is not callable!\")\n            return None\n\n        if not asyncio.iscoroutinefunction(task) and not asyncio.iscoroutine(task):\n            self.logger.warning(f\"Task '{getattr(task, '__name__', 'unknown')}' is not a coroutine. \"\n                                f\"Use run_bg_task_advanced for synchronous functions.\")\n            # Fallback to advanced runner for convenience\n            self.run_bg_task_advanced(task, *args, **kwargs)\n            return None\n\n        try:\n            loop = self.loop_gard()\n            if not loop.is_running():\n                # If the main loop isn't running, we can't create a task on it.\n                # This scenario is handled by run_bg_task_advanced.\n                self.logger.info(\"Main event loop not running. Delegating to advanced background runner.\")\n                return self.run_bg_task_advanced(task, *args, **kwargs)\n\n            # Create the coroutine if it's a function\n            coro = task(*args, **kwargs) if asyncio.iscoroutinefunction(task) else task\n\n            # Create a task on the running event loop\n            bg_task = loop.create_task(coro)\n\n            # Add a callback to log exceptions from the background task\n            def _log_exception(the_task: asyncio.Task):\n                if not the_task.cancelled() and the_task.exception():\n                    self.logger.error(f\"Exception in background task '{the_task.get_name()}':\",\n                                      exc_info=the_task.exception())\n\n            bg_task.add_done_callback(_log_exception)\n            self.bg_tasks.append(bg_task)\n            return bg_task\n\n        except Exception as e:\n            self.logger.error(f\"Failed to schedule background task: {e}\", exc_info=True)\n            return None\n\n    def run_bg_task_advanced(self, task: Callable, *args, **kwargs) -&gt; threading.Thread:\n        \"\"\"\n        Runs a task in a separate, dedicated background thread with its own event loop.\n\n        This is ideal for:\n        1. Running an async task from a synchronous context.\n        2. Launching a long-running, independent operation that should not\n           interfere with the main application's event loop.\n\n        Args:\n            task: The function to run (can be sync or async).\n            *args: Arguments for the task.\n            **kwargs: Keyword arguments for the task.\n\n        Returns:\n            The threading.Thread object managing the background execution.\n        \"\"\"\n        if not callable(task):\n            self.logger.warning(\"Task for run_bg_task_advanced is not callable!\")\n            return None\n\n        def thread_target():\n            # Each thread gets its own event loop.\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n\n            try:\n                # Prepare the coroutine we need to run\n                if asyncio.iscoroutinefunction(task):\n                    coro = task(*args, **kwargs)\n                elif asyncio.iscoroutine(task):\n                    # It's already a coroutine object\n                    coro = task\n                else:\n                    # It's a synchronous function, run it in an executor\n                    # to avoid blocking the new event loop.\n                    coro = loop.run_in_executor(None, lambda: task(*args, **kwargs))\n\n                # Run the coroutine to completion\n                result = loop.run_until_complete(coro)\n                self.logger.debug(f\"Advanced background task '{getattr(task, '__name__', 'unknown')}' completed.\")\n                if result is not None:\n                    self.logger.debug(f\"Task result: {str(result)[:100]}\")\n\n            except Exception as e:\n                self.logger.error(f\"Error in advanced background task '{getattr(task, '__name__', 'unknown')}':\",\n                                  exc_info=e)\n            finally:\n                # Cleanly shut down the event loop in this thread.\n                try:\n                    all_tasks = asyncio.all_tasks(loop=loop)\n                    if all_tasks:\n                        for t in all_tasks:\n                            t.cancel()\n                        loop.run_until_complete(asyncio.gather(*all_tasks, return_exceptions=True))\n                finally:\n                    loop.close()\n                    asyncio.set_event_loop(None)\n\n        # Create, start, and return the thread.\n        # It's a daemon thread so it won't prevent the main app from exiting.\n        t = threading.Thread(target=thread_target, daemon=True, name=f\"BGTask-{getattr(task, '__name__', 'unknown')}\")\n        self.bg_tasks.append(t)\n        t.start()\n        return t\n\n    # Helper method to wait for background tasks to complete (optional)\n    def wait_for_bg_tasks(self, timeout=None):\n        \"\"\"\n        Wait for all background tasks to complete.\n\n        Args:\n            timeout: Maximum time to wait (in seconds) for all tasks to complete.\n                     None means wait indefinitely.\n\n        Returns:\n            bool: True if all tasks completed, False if timeout occurred\n        \"\"\"\n        active_tasks = [t for t in self.bg_tasks if t.is_alive()]\n\n        for task in active_tasks:\n            task.join(timeout=timeout)\n            if task.is_alive():\n                return False\n\n        return True\n\n    def __call__(self, *args, **kwargs):\n        return self.run(*args, **kwargs)\n\n    def run(self, *args, request=None, running_function_coro=None, **kwargs):\n        \"\"\"\n        Run a function with support for SSE streaming in both\n        threaded and non-threaded contexts.\n        \"\"\"\n        if running_function_coro is None:\n            mn, fn = args[0]\n            if self.functions.get(mn, {}).get(fn, {}).get('request_as_kwarg', False):\n                kwargs[\"request\"] = RequestData.from_dict(request)\n                if 'data' in kwargs and 'data' not in self.functions.get(mn, {}).get(fn, {}).get('params', []):\n                    kwargs[\"request\"].data = kwargs[\"request\"].body = kwargs['data']\n                    del kwargs['data']\n                if 'form_data' in kwargs and 'form_data' not in self.functions.get(mn, {}).get(fn, {}).get('params',\n                                                                                                           []):\n                    kwargs[\"request\"].form_data = kwargs[\"request\"].body = kwargs['form_data']\n                    del kwargs['form_data']\n\n        # Create the coroutine\n        coro = running_function_coro or self.a_run_any(*args, **kwargs)\n\n        # Get or create an event loop\n        try:\n            loop = asyncio.get_event_loop()\n            is_running = loop.is_running()\n        except RuntimeError:\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n            is_running = False\n\n        # If the loop is already running, run in a separate thread\n        if is_running:\n            # Create thread pool executor as needed\n            if not hasattr(self.__class__, '_executor'):\n                self.__class__._executor = ThreadPoolExecutor(max_workers=4)\n\n            def run_in_new_thread():\n                # Set up a new loop in this thread\n                new_loop = asyncio.new_event_loop()\n                asyncio.set_event_loop(new_loop)\n\n                try:\n                    # Run the coroutine\n                    return new_loop.run_until_complete(coro)\n                finally:\n                    new_loop.close()\n\n            # Run in thread and get result\n            thread_result = self.__class__._executor.submit(run_in_new_thread).result()\n\n            # Handle streaming results from thread\n            if isinstance(thread_result, dict) and thread_result.get(\"is_stream\"):\n                # Create a new SSE stream in the main thread\n                async def stream_from_function():\n                    # Re-run the function with direct async access\n                    stream_result = await self.a_run_any(*args, **kwargs)\n\n                    if (isinstance(stream_result, Result) and\n                        getattr(stream_result.result, 'data_type', None) == \"stream\"):\n                        # Get and forward data from the original generator\n                        original_gen = stream_result.result.data.get(\"generator\")\n                        if inspect.isasyncgen(original_gen):\n                            async for item in original_gen:\n                                yield item\n\n                # Return a new streaming Result\n                return Result.stream(\n                    stream_generator=stream_from_function(),\n                    headers=thread_result.get(\"headers\", {})\n                )\n\n            result = thread_result\n        else:\n            # Direct execution when loop is not running\n            result = loop.run_until_complete(coro)\n\n        # Process the final result\n        if isinstance(result, Result):\n            if 'debug' in self.id:\n                result.print()\n            if getattr(result.result, 'data_type', None) == \"stream\":\n                return result\n            return result.to_api_result().model_dump(mode='json')\n\n        return result\n\n    def loop_gard(self):\n        if self.loop is None:\n            self.loop = asyncio.get_event_loop()\n        if self.loop.is_closed():\n            self.loop = asyncio.get_event_loop()\n        return self.loop\n\n    async def a_init_mod(self, mod_name, spec='app'):\n        mod = self.save_load(mod_name, spec=spec)\n        if hasattr(mod, \"__initobj\") and not mod.async_initialized:\n            await mod\n        return mod\n\n\n    def load_mod(self, mod_name: str, mlm='I', **kwargs):\n\n        action_list_helper = ['I (inplace load dill on error python)',\n                              # 'C (coppy py file to runtime dir)',\n                              # 'S (save py file to dill)',\n                              # 'CS (coppy and save py file)',\n                              # 'D (development mode, inplace load py file)'\n                              ]\n        action_list = {\"I\": lambda: self.inplace_load_instance(mod_name, **kwargs),\n                       \"C\": lambda: self._copy_load(mod_name, **kwargs)\n                       }\n\n        try:\n            if mlm in action_list:\n\n                return action_list.get(mlm)()\n            else:\n                self.logger.critical(\n                    f\"config mlm must be {' or '.join(action_list_helper)} is {mlm=}\")\n                raise ValueError(f\"config mlm must be {' or '.join(action_list_helper)} is {mlm=}\")\n        except ValueError as e:\n            self.logger.warning(Style.YELLOW(f\"Error Loading Module '{mod_name}', with error :{e}\"))\n            self.debug_rains(e)\n        except ImportError as e:\n            self.logger.error(Style.YELLOW(f\"Error Loading Module '{mod_name}', with error :{e}\"))\n            self.debug_rains(e)\n        except Exception as e:\n            self.logger.critical(Style.RED(f\"Error Loading Module '{mod_name}', with critical error :{e}\"))\n            print(Style.RED(f\"Error Loading Module '{mod_name}'\"))\n            self.debug_rains(e)\n\n        return Result.default_internal_error(info=\"info's in logs.\")\n\n    async def load_all_mods_in_file(self, working_dir=\"mods\"):\n        print(f\"LOADING ALL MODS FROM FOLDER : {working_dir}\")\n        t0 = time.perf_counter()\n        # Get the list of all modules\n        module_list = self.get_all_mods(working_dir)\n        open_modules = self.functions.keys()\n        start_len = len(open_modules)\n\n        for om in open_modules:\n            if om in module_list:\n                module_list.remove(om)\n\n        tasks: set[Task] = set()\n\n        _ = {tasks.add(asyncio.create_task(asyncio.to_thread(self.save_load, mod, 'app'))) for mod in module_list}\n        for t in asyncio.as_completed(tasks):\n            try:\n                result = await t\n                if hasattr(result, 'Name'):\n                    print('Opened :', result.Name)\n                elif hasattr(result, 'name'):\n                    if hasattr(result, 'async_initialized'):\n                        if not result.async_initialized:\n                            async def _():\n                                try:\n                                    if asyncio.iscoroutine(result):\n                                        await result\n                                    if hasattr(result, 'Name'):\n                                        print('Opened :', result.Name)\n                                    elif hasattr(result, 'name'):\n                                        print('Opened :', result.name)\n                                except Exception as e:\n                                    self.debug_rains(e)\n                                    if hasattr(result, 'Name'):\n                                        print('Error opening :', result.Name)\n                                    elif hasattr(result, 'name'):\n                                        print('Error opening :', result.name)\n                            asyncio.create_task(_())\n                        else:\n                            print('Opened :', result.name)\n                else:\n                    print('Opened :', result)\n            except Exception as e:\n                self.logger.error(Style.RED(f\"An Error occurred while opening all modules error: {str(e)}\"))\n                self.debug_rains(e)\n        opened = len(self.functions.keys()) - start_len\n\n        self.logger.info(f\"Opened {opened} modules in {time.perf_counter() - t0:.2f}s\")\n        return f\"Opened {opened} modules in {time.perf_counter() - t0:.2f}s\"\n\n    def get_all_mods(self, working_dir=\"mods\", path_to=\"./runtime\", use_wd=True):\n        self.logger.info(f\"collating all mods in working directory {working_dir}\")\n\n        pr = \"_dev\" if self.dev_modi else \"\"\n        if working_dir == \"mods\" and use_wd:\n            working_dir = f\"{self.start_dir}/mods{pr}\"\n        elif use_wd:\n            pass\n        else:\n            w_dir = self.id.replace(\".\", \"_\")\n            working_dir = f\"{path_to}/{w_dir}/mod_lib{pr}/\"\n        res = os.listdir(working_dir)\n\n        self.logger.info(f\"found : {len(res)} files\")\n\n        def do_helper(_mod):\n            if \"mainTool\" in _mod:\n                return False\n            # if not _mod.endswith(\".py\"):\n            #     return False\n            if _mod.startswith(\"__\"):\n                return False\n            if _mod.startswith(\".\"):\n                return False\n            return not _mod.startswith(\"test_\")\n\n        def r_endings(word: str):\n            if word.endswith(\".py\"):\n                return word[:-3]\n            return word\n\n        mods_list = list(map(r_endings, filter(do_helper, res)))\n\n        self.logger.info(f\"found : {len(mods_list)} Modules\")\n        return mods_list\n\n    def remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            self.remove_mod(mod, delete=delete)\n\n    def remove_mod(self, mod_name, spec='app', delete=True):\n        if mod_name not in self.functions:\n            self.logger.info(f\"mod not active {mod_name}\")\n            return\n\n        on_exit = self.functions[mod_name].get(\"on_exit\")\n        self.logger.info(f\"closing: {on_exit}\")\n        def helper():\n            if f\"{spec}_instance\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance\"]\n            if f\"{spec}_instance_type\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance_type\"]\n\n        if on_exit is None and self.functions[mod_name].get(f\"{spec}_instance_type\", \"\").endswith(\"/BC\"):\n            instance = self.functions[mod_name].get(f\"{spec}_instance\", None)\n            if instance is not None and hasattr(instance, 'on_exit'):\n                if asyncio.iscoroutinefunction(instance.on_exit):\n                    self.exit_tasks.append(instance.on_exit)\n                else:\n                    instance.on_exit()\n\n        if on_exit is None and delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n            return\n        if on_exit is None:\n            helper()\n            return\n\n        i = 1\n\n        for j, f in enumerate(on_exit):\n            try:\n                f_, e = self.get_function((mod_name, f), state=True, specification=spec, i=j)\n                if e == 0:\n                    self.logger.info(Style.GREY(f\"Running On exit {f} {i}/{len(on_exit)}\"))\n                    if asyncio.iscoroutinefunction(f_):\n                        self.exit_tasks.append(f_)\n                        o = None\n                    else:\n                        o = f_()\n                    if o is not None:\n                        self.print(f\"Function On Exit result: {o}\")\n                else:\n                    self.logger.warning(\"closing function not found\")\n            except Exception as e:\n                self.logger.debug(\n                    Style.YELLOW(Style.Bold(f\"modular:{mod_name}.{f} on_exit error {i}/{len(on_exit)} -&gt; {e}\")))\n\n                self.debug_rains(e)\n            finally:\n                i += 1\n\n        helper()\n\n        if delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n\n    async def a_remove_all_modules(self, delete=False):\n        for mod in list(self.functions.keys()):\n            self.logger.info(f\"closing: {mod}\")\n            await self.a_remove_mod(mod, delete=delete)\n\n    async def a_remove_mod(self, mod_name, spec='app', delete=True):\n        if mod_name not in self.functions:\n            self.logger.info(f\"mod not active {mod_name}\")\n            return\n        on_exit = self.functions[mod_name].get(\"on_exit\")\n        self.logger.info(f\"closing: {on_exit}\")\n        def helper():\n            if f\"{spec}_instance\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance\"]\n            if f\"{spec}_instance_type\" in self.functions[mod_name]:\n                del self.functions[mod_name][f\"{spec}_instance_type\"]\n\n        if on_exit is None and self.functions[mod_name].get(f\"{spec}_instance_type\", \"\").endswith(\"/BC\"):\n            instance = self.functions[mod_name].get(f\"{spec}_instance\", None)\n            if instance is not None and hasattr(instance, 'on_exit'):\n                if asyncio.iscoroutinefunction(instance.on_exit):\n                    await instance.on_exit()\n                else:\n                    instance.on_exit()\n\n        if on_exit is None and delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n            return\n        if on_exit is None:\n            helper()\n            return\n\n        i = 1\n        for f in on_exit:\n            try:\n                e = 1\n                if isinstance(f, str):\n                    f_, e = self.get_function((mod_name, f), state=True, specification=spec)\n                elif isinstance(f, Callable):\n                    f_, e, f  = f, 0, f.__name__\n                if e == 0:\n                    self.logger.info(Style.GREY(f\"Running On exit {f} {i}/{len(on_exit)}\"))\n                    if asyncio.iscoroutinefunction(f_):\n                        o = await f_()\n                    else:\n                        o = f_()\n                    if o is not None:\n                        self.print(f\"Function On Exit result: {o}\")\n                else:\n                    self.logger.warning(\"closing function not found\")\n            except Exception as e:\n                self.logger.debug(\n                    Style.YELLOW(Style.Bold(f\"modular:{mod_name}.{f} on_exit error {i}/{len(on_exit)} -&gt; {e}\")))\n                self.debug_rains(e)\n            finally:\n                i += 1\n\n        helper()\n\n        if delete:\n            self.functions[mod_name] = {}\n            del self.functions[mod_name]\n\n    def exit(self, remove_all=True):\n        if not self.alive:\n            return\n        if self.args_sto.debug:\n            self.hide_console()\n        self.disconnect()\n        if remove_all:\n            self.remove_all_modules()\n        self.logger.info(\"Exiting ToolBox interface\")\n        self.alive = False\n        self.called_exit = True, time.time()\n        self.save_exit()\n        if hasattr(self, 'root_blob_storage') and self.root_blob_storage:\n            self.root_blob_storage.exit()\n        try:\n            self.config_fh.save_file_handler()\n        except SystemExit:\n            print(\"If u ar testing this is fine else ...\")\n\n        if hasattr(self, 'daemon_app'):\n            import threading\n\n            for thread in threading.enumerate()[::-1]:\n                if thread.name == \"MainThread\":\n                    continue\n                try:\n                    with Spinner(f\"closing Thread {thread.name:^50}|\", symbols=\"s\", count_down=True,\n                                 time_in_s=0.751 if not self.debug else 0.6):\n                        thread.join(timeout=0.751 if not self.debug else 0.6)\n                except TimeoutError as e:\n                    self.logger.error(f\"Timeout error on exit {thread.name} {str(e)}\")\n                    print(str(e), f\"Timeout {thread.name}\")\n                except KeyboardInterrupt:\n                    print(\"Unsave Exit\")\n                    break\n        if hasattr(self, 'loop') and self.loop is not None:\n            with Spinner(\"closing Event loop:\", symbols=\"+\"):\n                self.loop.stop()\n\n    async def a_exit(self):\n        await self.a_remove_all_modules(delete=True)\n        results = await asyncio.gather(\n            *[asyncio.create_task(f()) for f in self.exit_tasks if asyncio.iscoroutinefunction(f)])\n        for result in results:\n            self.print(f\"Function On Exit result: {result}\")\n        self.exit(remove_all=False)\n\n    def save_load(self, modname, spec='app'):\n        self.logger.debug(f\"Save load module {modname}\")\n        if not modname:\n            self.logger.warning(\"no filename specified\")\n            return False\n        try:\n            return self.load_mod(modname, spec=spec)\n        except ModuleNotFoundError as e:\n            self.logger.error(Style.RED(f\"Module {modname} not found\"))\n            self.debug_rains(e)\n\n        return False\n\n    def get_function(self, name: Enum or tuple, **kwargs):\n        \"\"\"\n        Kwargs for _get_function\n            metadata:: return the registered function dictionary\n                stateless: (function_data, None), 0\n                stateful: (function_data, higher_order_function), 0\n            state::boolean\n                specification::str default app\n        \"\"\"\n        if isinstance(name, tuple):\n            return self._get_function(None, as_str=name, **kwargs)\n        else:\n            return self._get_function(name, **kwargs)\n\n    async def a_run_function(self, mod_function_name: Enum or tuple,\n                             tb_run_function_with_state=True,\n                             tb_run_with_specification='app',\n                             args_=None,\n                             kwargs_=None,\n                             *args,\n                             **kwargs) -&gt; Result:\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n        if isinstance(mod_function_name, tuple):\n            modular_name, function_name = mod_function_name\n        elif isinstance(mod_function_name, list):\n            modular_name, function_name = mod_function_name[0], mod_function_name[1]\n        elif isinstance(mod_function_name, Enum):\n            modular_name, function_name = mod_function_name.__class__.NAME.value, mod_function_name.value\n        else:\n            raise TypeError(\"Unknown function type\")\n\n        if not self.mod_online(modular_name, installed=True):\n            self.get_mod(modular_name)\n\n        function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                      metadata=True, specification=tb_run_with_specification)\n        self.logger.info(f\"Received fuction : {mod_function_name}, with execode: {error_code}\")\n        if error_code == 404:\n            mod = self.get_mod(modular_name)\n            if hasattr(mod, \"async_initialized\") and not mod.async_initialized:\n                await mod\n            function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                          metadata=True, specification=tb_run_with_specification)\n\n        if error_code == 404:\n            self.logger.warning(Style.RED(\"Function Not Found\"))\n            return (Result.default_user_error(interface=self.interface_type,\n                                              exec_code=404,\n                                              info=\"function not found function is not decorated\").\n                    set_origin(mod_function_name))\n\n        if error_code == 300:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 info=f\"module {modular_name}\"\n                                                      f\" has no state (instance)\").set_origin(mod_function_name)\n\n        if error_code != 0:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=error_code,\n                                                 info=f\"Internal error\"\n                                                      f\" {modular_name}.\"\n                                                      f\"{function_name}\").set_origin(mod_function_name)\n\n        if not tb_run_function_with_state:\n            function_data, _ = function_data\n            function = function_data.get('func')\n        else:\n            function_data, function = function_data\n\n        if not function:\n            self.logger.warning(Style.RED(f\"Function {function_name} not found\"))\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=404,\n                                                 info=\"function not found function\").set_origin(mod_function_name)\n\n        self.logger.info(\"Profiling function\")\n        t0 = time.perf_counter()\n        if asyncio.iscoroutinefunction(function):\n            return await self.a_fuction_runner(function, function_data, args, kwargs, t0)\n        else:\n            return self.fuction_runner(function, function_data, args, kwargs, t0)\n\n    def run_function(self, mod_function_name: Enum or tuple,\n                     tb_run_function_with_state=True,\n                     tb_run_with_specification='app',\n                     args_=None,\n                     kwargs_=None,\n                     *args,\n                     **kwargs) -&gt; Result:\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n        if isinstance(mod_function_name, tuple):\n            modular_name, function_name = mod_function_name\n        elif isinstance(mod_function_name, list):\n            modular_name, function_name = mod_function_name[0], mod_function_name[1]\n        elif isinstance(mod_function_name, Enum):\n            modular_name, function_name = mod_function_name.__class__.NAME.value, mod_function_name.value\n        else:\n            raise TypeError(\"Unknown function type\")\n\n        if not self.mod_online(modular_name, installed=True):\n            self.get_mod(modular_name)\n\n        function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                      metadata=True, specification=tb_run_with_specification)\n        self.logger.info(f\"Received fuction : {mod_function_name}, with execode: {error_code}\")\n        if error_code == 1 or error_code == 3 or error_code == 400:\n            self.get_mod(modular_name)\n            function_data, error_code = self.get_function(mod_function_name, state=tb_run_function_with_state,\n                                                          metadata=True, specification=tb_run_with_specification)\n\n        if error_code == 2:\n            self.logger.warning(Style.RED(\"Function Not Found\"))\n            return (Result.default_user_error(interface=self.interface_type,\n                                              exec_code=404,\n                                              info=\"function not found function is not decorated\").\n                    set_origin(mod_function_name))\n\n        if error_code == -1:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 info=f\"module {modular_name}\"\n                                                      f\" has no state (instance)\").set_origin(mod_function_name)\n\n        if error_code != 0:\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=error_code,\n                                                 info=f\"Internal error\"\n                                                      f\" {modular_name}.\"\n                                                      f\"{function_name}\").set_origin(mod_function_name)\n\n        if not tb_run_function_with_state:\n            function_data, _ = function_data\n            function = function_data.get('func')\n        else:\n            function_data, function = function_data\n\n        if not function:\n            self.logger.warning(Style.RED(f\"Function {function_name} not found\"))\n            return Result.default_internal_error(interface=self.interface_type,\n                                                 exec_code=404,\n                                                 info=\"function not found function\").set_origin(mod_function_name)\n\n        self.logger.info(\"Profiling function\")\n        t0 = time.perf_counter()\n        if asyncio.iscoroutinefunction(function):\n            raise ValueError(f\"Fuction {function_name} is Async use a_run_any\")\n        else:\n            return self.fuction_runner(function, function_data, args, kwargs, t0)\n\n    def run_a_from_sync(self, function, *args, **kwargs):\n        # Initialize self.loop if not already set.\n        if self.loop is None:\n            try:\n                self.loop = asyncio.get_running_loop()\n            except RuntimeError:\n                self.loop = asyncio.new_event_loop()\n\n        # If the loop is running, offload the coroutine to a new thread.\n        if self.loop.is_running():\n            result_future = Future()\n\n            def run_in_new_loop():\n                new_loop = asyncio.new_event_loop()\n                asyncio.set_event_loop(new_loop)\n                try:\n                    result = new_loop.run_until_complete(function(*args, **kwargs))\n                    result_future.set_result(result)\n                except Exception as e:\n                    result_future.set_exception(e)\n                finally:\n                    new_loop.close()\n\n            thread = threading.Thread(target=run_in_new_loop)\n            thread.start()\n            thread.join()  # Block until the thread completes.\n            return result_future.result()\n        else:\n            # If the loop is not running, schedule and run the coroutine directly.\n            future = self.loop.create_task(function(*args, **kwargs))\n            return self.loop.run_until_complete(future)\n\n    def fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        row = function_data.get('row')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        if_self_state = 1 if 'self' in parameters else 0\n\n        try:\n            if len(parameters) == 0:\n                res = function()\n            elif len(parameters) == len(args) + if_self_state:\n                res = function(*args)\n            elif len(parameters) == len(kwargs.keys()) + if_self_state:\n                res = function(**kwargs)\n            else:\n                res = function(*args, **kwargs)\n            self.logger.info(f\"Execution done in {time.perf_counter()-t0:.4f}\")\n            if isinstance(res, Result):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.set_origin(mod_function_name)\n            elif isinstance(res, ApiResult):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.as_result().set_origin(mod_function_name).to_api_result()\n            elif row:\n                formatted_result = res\n            else:\n                # Wrap the result in a Result object\n                formatted_result = Result.ok(\n                    interface=self.interface_type,\n                    data_info=\"Auto generated result\",\n                    data=res,\n                    info=\"Function executed successfully\"\n                ).set_origin(mod_function_name)\n            if not row:\n                self.logger.info(\n                    f\"Function Exec code: {formatted_result.info.exec_code} Info's: {formatted_result.info.help_text}\")\n            else:\n                self.logger.info(\n                    f\"Function Exec data: {formatted_result}\")\n        except Exception as e:\n            self.logger.error(\n                Style.YELLOW(Style.Bold(\n                    f\"! Function ERROR: in {modular_name}.{function_name}\")))\n            # Wrap the exception in a Result object\n            formatted_result = Result.default_internal_error(info=str(e)).set_origin(mod_function_name)\n            # res = formatted_result\n            self.logger.error(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed wit an error {str(e)}, {type(e)}\")\n            self.debug_rains(e)\n            self.print(f\"! Function ERROR: in {modular_name}.{function_name} \")\n\n\n\n        else:\n            self.print_ok()\n\n            self.logger.info(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed successfully\")\n\n        return formatted_result\n\n    async def a_fuction_runner(self, function, function_data: dict, args: list, kwargs: dict, t0=.0):\n\n        parameters = function_data.get('params')\n        modular_name = function_data.get('module_name')\n        function_name = function_data.get('func_name')\n        row = function_data.get('row')\n        mod_function_name = f\"{modular_name}.{function_name}\"\n\n        if_self_state = 1 if 'self' in parameters else 0\n\n        try:\n            if len(parameters) == 0:\n                res = await function()\n            elif len(parameters) == len(args) + if_self_state:\n                res = await function(*args)\n            elif len(parameters) == len(kwargs.keys()) + if_self_state:\n                res = await function(**kwargs)\n            else:\n                res = await function(*args, **kwargs)\n            self.logger.info(f\"Execution done in {time.perf_counter()-t0:.4f}\")\n            if isinstance(res, Result):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.set_origin(mod_function_name)\n            elif isinstance(res, ApiResult):\n                formatted_result = res\n                if formatted_result.origin is None:\n                    formatted_result.as_result().set_origin(mod_function_name).to_api_result()\n            elif row:\n                formatted_result = res\n            else:\n                # Wrap the result in a Result object\n                formatted_result = Result.ok(\n                    interface=self.interface_type,\n                    data_info=\"Auto generated result\",\n                    data=res,\n                    info=\"Function executed successfully\"\n                ).set_origin(mod_function_name)\n            if not row:\n                self.logger.info(\n                    f\"Function Exec code: {formatted_result.info.exec_code} Info's: {formatted_result.info.help_text}\")\n            else:\n                self.logger.info(\n                    f\"Function Exec data: {formatted_result}\")\n        except Exception as e:\n            self.logger.error(\n                Style.YELLOW(Style.Bold(\n                    f\"! Function ERROR: in {modular_name}.{function_name}\")))\n            # Wrap the exception in a Result object\n            formatted_result = Result.default_internal_error(info=str(e)).set_origin(mod_function_name)\n            # res = formatted_result\n            self.logger.error(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed wit an error {str(e)}, {type(e)}\")\n            self.debug_rains(e)\n\n        else:\n            self.print_ok()\n\n            self.logger.info(\n                f\"Function {modular_name}.{function_name}\"\n                f\" executed successfully\")\n\n        return formatted_result\n\n    async def run_http(self, mod_function_name: Enum or str or tuple, function_name=None,\n                       args_=None,\n                       kwargs_=None, method=\"GET\",\n                       *args, **kwargs):\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n\n        modular_name = mod_function_name\n        function_name = function_name\n\n        if isinstance(mod_function_name, str) and isinstance(function_name, str):\n            mod_function_name = (mod_function_name, function_name)\n\n        if isinstance(mod_function_name, tuple):\n            modular_name, function_name = mod_function_name\n        elif isinstance(mod_function_name, list):\n            modular_name, function_name = mod_function_name[0], mod_function_name[1]\n        elif isinstance(mod_function_name, Enum):\n            modular_name, function_name = mod_function_name.__class__.NAME.value, mod_function_name.value\n\n        self.logger.info(f\"getting function : {modular_name}.{function_name} from http {self.session.base}\")\n        r = await self.session.fetch(f\"/api/{modular_name}/{function_name}{'?' + args_ if args_ is not None else ''}\",\n                                     data=kwargs, method=method)\n        try:\n            if not r:\n                print(\"\u00a7 Session server Offline!\", self.session.base)\n                return Result.default_internal_error(info=\"Session fetch failed\").as_dict()\n\n            content_type = r.headers.get('Content-Type', '').lower()\n\n            if 'application/json' in content_type:\n                try:\n                    return r.json()\n                except Exception as e:\n                    print(f\"\u26a0 JSON decode error: {e}\")\n                    # Fallback to text if JSON decoding fails\n                    text = r.text\n            else:\n                text = r.text\n\n\n            # Attempt YAML\n            if 'yaml' in content_type or text.strip().startswith('---'):\n                try:\n                    import yaml\n                    return yaml.safe_load(text)\n                except Exception as e:\n                    print(f\"\u26a0 YAML decode error: {e}\")\n\n            # Attempt XML\n            if 'xml' in content_type or text.strip().startswith('&lt;?xml'):\n                try:\n                    import xmltodict\n                    return xmltodict.parse(text)\n                except Exception as e:\n                    print(f\"\u26a0 XML decode error: {e}\")\n\n            # Fallback: return plain text\n            return Result.default_internal_error(data={'raw_text': text, 'content_type': content_type}).as_dict()\n\n        except Exception as e:\n            print(\"\u274c Fatal error during API call:\", e)\n            self.debug_rains(e)\n            return Result.default_internal_error(str(e)).as_dict()\n\n    def run_local(self, *args, **kwargs):\n        return self.run_any(*args, **kwargs)\n\n    async def a_run_local(self, *args, **kwargs):\n        return await self.a_run_any(*args, **kwargs)\n\n    def run_any(self, mod_function_name: Enum or str or tuple, backwords_compability_variabel_string_holder=None,\n                get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                kwargs_=None,\n                *args, **kwargs):\n\n        # if self.debug:\n        #     self.logger.info(f'Called from: {getouterframes(currentframe(), 2)}')\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n\n        if isinstance(mod_function_name, str) and backwords_compability_variabel_string_holder is None:\n            backwords_compability_variabel_string_holder = mod_function_name.split('.')[-1]\n            mod_function_name = mod_function_name.replace(f\".{backwords_compability_variabel_string_holder}\", \"\")\n\n        if isinstance(mod_function_name, str) and isinstance(backwords_compability_variabel_string_holder, str):\n            mod_function_name = (mod_function_name, backwords_compability_variabel_string_holder)\n\n        res: Result = self.run_function(mod_function_name,\n                                        tb_run_function_with_state=tb_run_function_with_state,\n                                        tb_run_with_specification=tb_run_with_specification,\n                                        args_=args, kwargs_=kwargs).as_result()\n        if isinstance(res, ApiResult):\n            res = res.as_result()\n\n        if isinstance(res, Result) and res.bg_task is not None:\n            self.run_bg_task(res.bg_task)\n\n        if self.debug:\n            res.log(show_data=False)\n\n        if not get_results and isinstance(res, Result):\n            return res.get()\n\n        if get_results and not isinstance(res, Result):\n            return Result.ok(data=res)\n\n        return res\n\n    async def a_run_any(self, mod_function_name: Enum or str or tuple,\n                        backwords_compability_variabel_string_holder=None,\n                        get_results=False, tb_run_function_with_state=True, tb_run_with_specification='app', args_=None,\n                        kwargs_=None,\n                        *args, **kwargs):\n\n        # if self.debug:\n        #     self.logger.info(f'Called from: {getouterframes(currentframe(), 2)}')\n\n        if kwargs_ is not None and not kwargs:\n            kwargs = kwargs_\n        if args_ is not None and not args:\n            args = args_\n\n        if isinstance(mod_function_name, str) and backwords_compability_variabel_string_holder is None:\n            backwords_compability_variabel_string_holder = mod_function_name.split('.')[-1]\n            mod_function_name = mod_function_name.replace(f\".{backwords_compability_variabel_string_holder}\", \"\")\n\n        if isinstance(mod_function_name, str) and isinstance(backwords_compability_variabel_string_holder, str):\n            mod_function_name = (mod_function_name, backwords_compability_variabel_string_holder)\n\n        res: Result = await self.a_run_function(mod_function_name,\n                                                tb_run_function_with_state=tb_run_function_with_state,\n                                                tb_run_with_specification=tb_run_with_specification,\n                                                args_=args, kwargs_=kwargs)\n        if isinstance(res, ApiResult):\n            res = res.as_result()\n\n        if isinstance(res, Result) and res.bg_task is not None:\n            self.run_bg_task(res.bg_task)\n\n        if self.debug:\n            res.print()\n            res.log(show_data=False) if isinstance(res, Result) else self.logger.debug(res)\n        if not get_results and isinstance(res, Result):\n            return res.get()\n\n        if get_results and not isinstance(res, Result):\n            return Result.ok(data=res)\n\n        return res\n\n\n    def web_context(self):\n        if self._web_context is None:\n            try:\n                self._web_context = open(\"./dist/helper.html\", encoding=\"utf-8\").read()\n            except Exception as e:\n                self.logger.error(f\"Could not load web context: {e}\")\n                self._web_context = \"&lt;div&gt;&lt;h1&gt;Web Context not found&lt;/h1&gt;&lt;/div&gt;\"\n        return self._web_context\n\n    def get_mod(self, name, spec='app') -&gt; ModuleType or MainToolType:\n        if spec != \"app\":\n            self.print(f\"Getting Module {name} spec: {spec}\")\n        if name not in self.functions:\n            mod = self.save_load(name, spec=spec)\n            if mod is False or (isinstance(mod, Result) and mod.is_error()):\n                self.logger.warning(f\"Could not find {name} in {list(self.functions.keys())}\")\n                raise ValueError(f\"Could not find {name} in {list(self.functions.keys())} pleas install the module, or its posibly broken use --debug for infos\")\n        # private = self.functions[name].get(f\"{spec}_private\")\n        # if private is not None:\n        #     if private and spec != 'app':\n        #         raise ValueError(\"Module is private\")\n        if name not in self.functions:\n            self.logger.warning(f\"Module '{name}' is not found\")\n            return None\n        instance = self.functions[name].get(f\"{spec}_instance\")\n        if instance is None:\n            return self.load_mod(name, spec=spec)\n        return self.functions[name].get(f\"{spec}_instance\")\n\n    def print(self, text=\"\", *args, **kwargs):\n        # self.logger.info(f\"Output : {text}\")\n        if 'live' in self.id:\n            return\n\n        flush = kwargs.pop('flush', True)\n        if self.sprint(None):\n            print(Style.CYAN(f\"System${self.id}:\"), end=\" \", flush=flush)\n        print(text, *args, **kwargs, flush=flush)\n\n    def sprint(self, text=\"\", *args, **kwargs):\n        if text is None:\n            return True\n        if 'live' in self.id:\n            return\n        flush = kwargs.pop('flush', True)\n        # self.logger.info(f\"Output : {text}\")\n        print(Style.CYAN(f\"System${self.id}:\"), end=\" \", flush=flush)\n        if isinstance(text, str) and kwargs == {} and text:\n            stram_print(text + ' '.join(args))\n            print()\n        else:\n            print(text, *args, **kwargs, flush=flush)\n\n    # ----------------------------------------------------------------\n    # Decorators for the toolbox\n\n    def reload_mod(self, mod_name, spec='app', is_file=True, loc=\"toolboxv2.mods.\"):\n        self.remove_mod(mod_name, delete=True)\n        if mod_name not in self.modules:\n            self.logger.warning(f\"Module '{mod_name}' is not found\")\n            return\n        if hasattr(self.modules[mod_name], 'reload_save') and self.modules[mod_name].reload_save:\n            def reexecute_module_code(x):\n                return x\n        else:\n            def reexecute_module_code(module_name):\n                if isinstance(module_name, str):\n                    module = import_module(module_name)\n                else:\n                    module = module_name\n                # Get the source code of the module\n                try:\n                    source = inspect.getsource(module)\n                except Exception:\n                    # print(f\"No source for {str(module_name).split('from')[0]}: {e}\")\n                    return module\n                # Compile the source code\n                try:\n                    code = compile(source, module.__file__, 'exec')\n                    # Execute the code in the module's namespace\n                    exec(code, module.__dict__)\n                except Exception:\n                    # print(f\"No source for {str(module_name).split('from')[0]}: {e}\")\n                    pass\n                return module\n\n        if not is_file:\n            mods = self.get_all_mods(\"./mods/\" + mod_name)\n            def recursive_reload(package_name):\n                package = import_module(package_name)\n\n                # First, reload all submodules\n                if hasattr(package, '__path__'):\n                    for _finder, name, _ispkg in pkgutil.walk_packages(package.__path__, package.__name__ + \".\"):\n                        try:\n                            mod = import_module(name)\n                            reexecute_module_code(mod)\n                            reload(mod)\n                        except Exception as e:\n                            print(f\"Error reloading module {name}: {e}\")\n                            break\n\n                # Finally, reload the package itself\n                reexecute_module_code(package)\n                reload(package)\n\n            for mod in mods:\n                if mod.endswith(\".txt\") or mod.endswith(\".yaml\"):\n                    continue\n                try:\n                    recursive_reload(loc + mod_name + '.' + mod)\n                    self.print(f\"Reloaded {mod_name}.{mod}\")\n                except ImportError:\n                    self.print(f\"Could not load {mod_name}.{mod}\")\n        reexecute_module_code(self.modules[mod_name])\n        if mod_name in self.functions:\n            if \"on_exit\" in self.functions[mod_name]:\n                self.functions[mod_name][\"on_exit\"] = []\n            if \"on_start\" in self.functions[mod_name]:\n                self.functions[mod_name][\"on_start\"] = []\n        self.inplace_load_instance(mod_name, spec=spec, mfo=reload(self.modules[mod_name]) if mod_name in self.modules else None)\n\n    def watch_mod(self, mod_name, spec='app', loc=\"toolboxv2.mods.\", use_thread=True, path_name=None, on_reload=None):\n        if path_name is None:\n            path_name = mod_name\n        is_file = os.path.isfile(self.start_dir + '/mods/' + path_name + '.py')\n        import watchfiles\n        def helper():\n            paths = f'mods/{path_name}' + ('.py' if is_file else '')\n            self.logger.info(f'Watching Path: {paths}')\n            try:\n                for changes in watchfiles.watch(paths):\n                    if not changes:\n                        continue\n                    self.reload_mod(mod_name, spec, is_file, loc)\n                    if on_reload:\n                        on_reload()\n            except FileNotFoundError:\n                self.logger.warning(f\"Path {paths} not found\")\n\n        if not use_thread:\n            helper()\n        else:\n            threading.Thread(target=helper, daemon=True).start()\n\n    def _register_function(self, module_name, func_name, data):\n        if module_name not in self.functions:\n            self.functions[module_name] = {}\n        if func_name in self.functions[module_name]:\n            self.print(f\"Overriding function {func_name} from {module_name}\", end=\"\\r\")\n            self.functions[module_name][func_name] = data\n        else:\n            self.functions[module_name][func_name] = data\n\n    def _create_decorator(self, type_: str,\n                          name: str = \"\",\n                          mod_name: str = \"\",\n                          level: int = -1,\n                          restrict_in_virtual_mode: bool = False,\n                          api: bool = False,\n                          helper: str = \"\",\n                          version: str or None = None,\n                          initial: bool=False,\n                          exit_f: bool=False,\n                          test: bool=True,\n                          samples:list[dict[str, Any]] | None=None,\n                          state:bool | None=None,\n                          pre_compute:Callable | None=None,\n                          post_compute:Callable[[], Result] | None=None,\n                          api_methods:list[str] | None=None,\n                          memory_cache: bool=False,\n                          file_cache: bool=False,\n                          request_as_kwarg: bool=False,\n                          row: bool=False,\n                          memory_cache_max_size:int=100,\n                          memory_cache_ttl:int=300):\n\n        if isinstance(type_, Enum):\n            type_ = type_.value\n\n        if memory_cache and file_cache:\n            raise ValueError(\"Don't use both cash at the same time for the same fuction\")\n\n        use_cache = memory_cache or file_cache\n        cache = {}\n        if file_cache:\n            cache = FileCache(folder=self.data_dir + f'\\\\cache\\\\{mod_name}\\\\',\n                              filename=self.data_dir + f'\\\\cache\\\\{mod_name}\\\\{name}cache.db')\n        if memory_cache:\n            cache = MemoryCache(maxsize=memory_cache_max_size, ttl=memory_cache_ttl)\n\n        version = self.version if version is None else self.version + ':' + version\n\n        def a_additional_process(func):\n\n            async def executor(*args, **kwargs):\n\n                if pre_compute is not None:\n                    args, kwargs = await pre_compute(*args, **kwargs)\n                if asyncio.iscoroutinefunction(func):\n                    result = await func(*args, **kwargs)\n                else:\n                    result = func(*args, **kwargs)\n                if post_compute is not None:\n                    result = await post_compute(result)\n                if row:\n                    return result\n                if not isinstance(result, Result):\n                    result = Result.ok(data=result)\n                if result.origin is None:\n                    result.set_origin((mod_name if mod_name else func.__module__.split('.')[-1]\n                                       , name if name else func.__name__\n                                       , type_))\n                if result.result.data_to == ToolBoxInterfaces.native.name:\n                    result.result.data_to = ToolBoxInterfaces.remote if api else ToolBoxInterfaces.native\n                # Wenden Sie die to_api_result Methode auf das Ergebnis an, falls verf\u00fcgbar\n                if api and hasattr(result, 'to_api_result'):\n                    return result.to_api_result()\n                return result\n\n            @wraps(func)\n            async def wrapper(*args, **kwargs):\n\n                if not use_cache:\n                    return await executor(*args, **kwargs)\n\n                try:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{str(args)},{str(kwargs.items())}\")\n                except ValueError:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{bytes(args)},{str(kwargs.items())}\")\n\n                result = cache.get(cache_key)\n                if result is not None:\n                    return result\n\n                result = await executor(*args, **kwargs)\n\n                cache.set(cache_key, result)\n\n                return result\n\n            return wrapper\n\n        def additional_process(func):\n\n            def executor(*args, **kwargs):\n\n                if pre_compute is not None:\n                    args, kwargs = pre_compute(*args, **kwargs)\n                if asyncio.iscoroutinefunction(func):\n                    result = func(*args, **kwargs)\n                else:\n                    result = func(*args, **kwargs)\n                if post_compute is not None:\n                    result = post_compute(result)\n                if row:\n                    return result\n                if not isinstance(result, Result):\n                    result = Result.ok(data=result)\n                if result.origin is None:\n                    result.set_origin((mod_name if mod_name else func.__module__.split('.')[-1]\n                                       , name if name else func.__name__\n                                       , type_))\n                if result.result.data_to == ToolBoxInterfaces.native.name:\n                    result.result.data_to = ToolBoxInterfaces.remote if api else ToolBoxInterfaces.native\n                # Wenden Sie die to_api_result Methode auf das Ergebnis an, falls verf\u00fcgbar\n                if api and hasattr(result, 'to_api_result'):\n                    return result.to_api_result()\n                return result\n\n            @wraps(func)\n            def wrapper(*args, **kwargs):\n\n                if not use_cache:\n                    return executor(*args, **kwargs)\n\n                try:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{str(args)},{str(kwargs.items())}\")\n                except ValueError:\n                    cache_key = (f\"{mod_name if mod_name else func.__module__.split('.')[-1]}\"\n                                 f\"-{func.__name__}-{bytes(args)},{str(kwargs.items())}\")\n\n                result = cache.get(cache_key)\n                if result is not None:\n                    return result\n\n                result = executor(*args, **kwargs)\n\n                cache.set(cache_key, result)\n\n                return result\n\n            return wrapper\n\n        def decorator(func):\n            sig = signature(func)\n            params = list(sig.parameters)\n            module_name = mod_name if mod_name else func.__module__.split('.')[-1]\n            func_name = name if name else func.__name__\n            if func_name == 'on_start':\n                func_name = 'on_startup'\n            if func_name == 'on_exit':\n                func_name = 'on_close'\n            if api or pre_compute is not None or post_compute is not None or memory_cache or file_cache:\n                if asyncio.iscoroutinefunction(func):\n                    func = a_additional_process(func)\n                else:\n                    func = additional_process(func)\n            if api and str(sig.return_annotation) == 'Result':\n                raise ValueError(f\"Fuction {module_name}.{func_name} registered as \"\n                                 f\"Api fuction but uses {str(sig.return_annotation)}\\n\"\n                                 f\"Please change the sig from ..)-&gt; Result to ..)-&gt; ApiResult\")\n            data = {\n                \"type\": type_,\n                \"module_name\": module_name,\n                \"func_name\": func_name,\n                \"level\": level,\n                \"restrict_in_virtual_mode\": restrict_in_virtual_mode,\n                \"func\": func,\n                \"api\": api,\n                \"helper\": helper,\n                \"version\": version,\n                \"initial\": initial,\n                \"exit_f\": exit_f,\n                \"api_methods\": api_methods if api_methods is not None else [\"AUTO\"],\n                \"__module__\": func.__module__,\n                \"signature\": sig,\n                \"params\": params,\n                \"row\": row,\n                \"state\": (\n                    False if len(params) == 0 else params[0] in ['self', 'state', 'app']) if state is None else state,\n                \"do_test\": test,\n                \"samples\": samples,\n                \"request_as_kwarg\": request_as_kwarg,\n\n            }\n            self._register_function(module_name, func_name, data)\n            if exit_f:\n                if \"on_exit\" not in self.functions[module_name]:\n                    self.functions[module_name][\"on_exit\"] = []\n                self.functions[module_name][\"on_exit\"].append(func_name)\n            if initial:\n                if \"on_start\" not in self.functions[module_name]:\n                    self.functions[module_name][\"on_start\"] = []\n                self.functions[module_name][\"on_start\"].append(func_name)\n\n            return func\n\n        decorator.tb_init = True\n\n        return decorator\n\n    def tb(self, name=None,\n           mod_name: str = \"\",\n           helper: str = \"\",\n           version: str | None = None,\n           test: bool = True,\n           restrict_in_virtual_mode: bool = False,\n           api: bool = False,\n           initial: bool = False,\n           exit_f: bool = False,\n           test_only: bool = False,\n           memory_cache: bool = False,\n           file_cache: bool = False,\n           request_as_kwarg: bool = False,\n           row: bool = False,\n           state: bool | None = None,\n           level: int = -1,\n           memory_cache_max_size: int = 100,\n           memory_cache_ttl: int = 300,\n           samples: list or dict or None = None,\n           interface: ToolBoxInterfaces or None or str = None,\n           pre_compute=None,\n           post_compute=None,\n           api_methods=None,\n           ):\n        \"\"\"\n    A decorator for registering and configuring functions within a module.\n\n    This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\n    Args:\n        name (str, optional): The name to register the function under. Defaults to the function's own name.\n        mod_name (str, optional): The name of the module the function belongs to.\n        helper (str, optional): A helper string providing additional information about the function.\n        version (str or None, optional): The version of the function or module.\n        test (bool, optional): Flag to indicate if the function is for testing purposes.\n        restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n        api (bool, optional): Flag to indicate if the function is part of an API.\n        initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n        exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n        test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n        memory_cache (bool, optional): Flag to enable memory caching for the function.\n        request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n        file_cache (bool, optional): Flag to enable file caching for the function.\n        row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n        state (bool or None, optional): Flag to indicate if the function maintains state.\n        level (int, optional): The level of the function, used for prioritization or categorization.\n        memory_cache_max_size (int, optional): Maximum size of the memory cache.\n        memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n        samples (list or dict or None, optional): Samples or examples of function usage.\n        interface (str, optional): The interface type for the function.\n        pre_compute (callable, optional): A function to be called before the main function.\n        post_compute (callable, optional): A function to be called after the main function.\n        api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\n    Returns:\n        function: The decorated function with additional processing and registration capabilities.\n    \"\"\"\n        if interface is None:\n            interface = \"tb\"\n        if test_only and 'test' not in self.id:\n            return lambda *args, **kwargs: args\n        return self._create_decorator(interface,\n                                      name,\n                                      mod_name,\n                                      level=level,\n                                      restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                      helper=helper,\n                                      api=api,\n                                      version=version,\n                                      initial=initial,\n                                      exit_f=exit_f,\n                                      test=test,\n                                      samples=samples,\n                                      state=state,\n                                      pre_compute=pre_compute,\n                                      post_compute=post_compute,\n                                      memory_cache=memory_cache,\n                                      file_cache=file_cache,\n                                      request_as_kwarg=request_as_kwarg,\n                                      row=row,\n                                      api_methods=api_methods,\n                                      memory_cache_max_size=memory_cache_max_size,\n                                      memory_cache_ttl=memory_cache_ttl)\n\n    def save_autocompletion_dict(self):\n        autocompletion_dict = {}\n        for module_name, _module in self.functions.items():\n            data = {}\n            for function_name, function_data in self.functions[module_name].items():\n                if not isinstance(function_data, dict):\n                    continue\n                data[function_name] = {arg: None for arg in\n                                       function_data.get(\"params\", [])}\n                if len(data[function_name].keys()) == 0:\n                    data[function_name] = None\n            autocompletion_dict[module_name] = data if len(data.keys()) &gt; 0 else None\n        self.config_fh.add_to_save_file_handler(\"auto~~~~~~\", str(autocompletion_dict))\n\n    def get_autocompletion_dict(self):\n        return self.config_fh.get_file_handler(\"auto~~~~~~\")\n\n    def save_registry_as_enums(self, directory: str, filename: str):\n        # Ordner erstellen, falls nicht vorhanden\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n        # Dateipfad vorbereiten\n        filepath = os.path.join(directory, filename)\n\n        # Enum-Klassen als Strings generieren\n        enum_classes = [f'\"\"\"Automatic generated by ToolBox v = {self.version}\"\"\"'\n                        f'\\nfrom enum import Enum\\nfrom dataclasses import dataclass'\n                        f'\\n\\n\\n']\n        for module, functions in self.functions.items():\n            if module.startswith(\"APP_INSTANCE\"):\n                continue\n            class_name = module\n            enum_members = \"\\n    \".join(\n                [\n                    f\"{func_name.upper().replace('-', '')}\"\n                    f\" = '{func_name}' \"\n                    f\"# Input: ({fuction_data['params'] if isinstance(fuction_data, dict) else ''}),\"\n                    f\" Output: {fuction_data['signature'].return_annotation if isinstance(fuction_data, dict) else 'None'}\"\n                    for func_name, fuction_data in functions.items()])\n            enum_class = (f'@dataclass\\nclass {class_name.upper().replace(\".\", \"_\").replace(\"-\", \"\")}(Enum):'\n                          f\"\\n    NAME = '{class_name}'\\n    {enum_members}\")\n            enum_classes.append(enum_class)\n\n        # Enums in die Datei schreiben\n        data = \"\\n\\n\\n\".join(enum_classes)\n        if len(data) &lt; 12:\n            raise ValueError(\n                \"Invalid Enums Loosing content pleas delete it ur self in the (utils/system/all_functions_enums.py) or add mor new stuff :}\")\n        with open(filepath, 'w') as file:\n            file.write(data)\n\n        print(Style.Bold(Style.BLUE(f\"Enums gespeichert in {filepath}\")))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.disconnect","title":"<code>disconnect(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef disconnect(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.exit_main","title":"<code>exit_main(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef exit_main(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.get_function","title":"<code>get_function(name, **kwargs)</code>","text":"<p>Kwargs for _get_function     metadata:: return the registered function dictionary         stateless: (function_data, None), 0         stateful: (function_data, higher_order_function), 0     state::boolean         specification::str default app</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def get_function(self, name: Enum or tuple, **kwargs):\n    \"\"\"\n    Kwargs for _get_function\n        metadata:: return the registered function dictionary\n            stateless: (function_data, None), 0\n            stateful: (function_data, higher_order_function), 0\n        state::boolean\n            specification::str default app\n    \"\"\"\n    if isinstance(name, tuple):\n        return self._get_function(None, as_str=name, **kwargs)\n    else:\n        return self._get_function(name, **kwargs)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.hide_console","title":"<code>hide_console(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef hide_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.run","title":"<code>run(*args, request=None, running_function_coro=None, **kwargs)</code>","text":"<p>Run a function with support for SSE streaming in both threaded and non-threaded contexts.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def run(self, *args, request=None, running_function_coro=None, **kwargs):\n    \"\"\"\n    Run a function with support for SSE streaming in both\n    threaded and non-threaded contexts.\n    \"\"\"\n    if running_function_coro is None:\n        mn, fn = args[0]\n        if self.functions.get(mn, {}).get(fn, {}).get('request_as_kwarg', False):\n            kwargs[\"request\"] = RequestData.from_dict(request)\n            if 'data' in kwargs and 'data' not in self.functions.get(mn, {}).get(fn, {}).get('params', []):\n                kwargs[\"request\"].data = kwargs[\"request\"].body = kwargs['data']\n                del kwargs['data']\n            if 'form_data' in kwargs and 'form_data' not in self.functions.get(mn, {}).get(fn, {}).get('params',\n                                                                                                       []):\n                kwargs[\"request\"].form_data = kwargs[\"request\"].body = kwargs['form_data']\n                del kwargs['form_data']\n\n    # Create the coroutine\n    coro = running_function_coro or self.a_run_any(*args, **kwargs)\n\n    # Get or create an event loop\n    try:\n        loop = asyncio.get_event_loop()\n        is_running = loop.is_running()\n    except RuntimeError:\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        is_running = False\n\n    # If the loop is already running, run in a separate thread\n    if is_running:\n        # Create thread pool executor as needed\n        if not hasattr(self.__class__, '_executor'):\n            self.__class__._executor = ThreadPoolExecutor(max_workers=4)\n\n        def run_in_new_thread():\n            # Set up a new loop in this thread\n            new_loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(new_loop)\n\n            try:\n                # Run the coroutine\n                return new_loop.run_until_complete(coro)\n            finally:\n                new_loop.close()\n\n        # Run in thread and get result\n        thread_result = self.__class__._executor.submit(run_in_new_thread).result()\n\n        # Handle streaming results from thread\n        if isinstance(thread_result, dict) and thread_result.get(\"is_stream\"):\n            # Create a new SSE stream in the main thread\n            async def stream_from_function():\n                # Re-run the function with direct async access\n                stream_result = await self.a_run_any(*args, **kwargs)\n\n                if (isinstance(stream_result, Result) and\n                    getattr(stream_result.result, 'data_type', None) == \"stream\"):\n                    # Get and forward data from the original generator\n                    original_gen = stream_result.result.data.get(\"generator\")\n                    if inspect.isasyncgen(original_gen):\n                        async for item in original_gen:\n                            yield item\n\n            # Return a new streaming Result\n            return Result.stream(\n                stream_generator=stream_from_function(),\n                headers=thread_result.get(\"headers\", {})\n            )\n\n        result = thread_result\n    else:\n        # Direct execution when loop is not running\n        result = loop.run_until_complete(coro)\n\n    # Process the final result\n    if isinstance(result, Result):\n        if 'debug' in self.id:\n            result.print()\n        if getattr(result.result, 'data_type', None) == \"stream\":\n            return result\n        return result.to_api_result().model_dump(mode='json')\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.run_bg_task","title":"<code>run_bg_task(task, *args, **kwargs)</code>","text":"<p>Runs a coroutine in the background without blocking the caller.</p> <p>This is the primary method for \"fire-and-forget\" async tasks. It schedules the coroutine to run on the application's main event loop.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Callable</code> <p>The coroutine function to run.</p> required <code>*args</code> <p>Arguments to pass to the coroutine function.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the coroutine function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Task]</code> <p>An asyncio.Task object representing the scheduled task, or None if</p> <code>Optional[Task]</code> <p>the task could not be scheduled.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def run_bg_task(self, task: Callable, *args, **kwargs) -&gt; Optional[asyncio.Task]:\n    \"\"\"\n    Runs a coroutine in the background without blocking the caller.\n\n    This is the primary method for \"fire-and-forget\" async tasks. It schedules\n    the coroutine to run on the application's main event loop.\n\n    Args:\n        task: The coroutine function to run.\n        *args: Arguments to pass to the coroutine function.\n        **kwargs: Keyword arguments to pass to the coroutine function.\n\n    Returns:\n        An asyncio.Task object representing the scheduled task, or None if\n        the task could not be scheduled.\n    \"\"\"\n    if not callable(task):\n        self.logger.warning(\"Task passed to run_bg_task is not callable!\")\n        return None\n\n    if not asyncio.iscoroutinefunction(task) and not asyncio.iscoroutine(task):\n        self.logger.warning(f\"Task '{getattr(task, '__name__', 'unknown')}' is not a coroutine. \"\n                            f\"Use run_bg_task_advanced for synchronous functions.\")\n        # Fallback to advanced runner for convenience\n        self.run_bg_task_advanced(task, *args, **kwargs)\n        return None\n\n    try:\n        loop = self.loop_gard()\n        if not loop.is_running():\n            # If the main loop isn't running, we can't create a task on it.\n            # This scenario is handled by run_bg_task_advanced.\n            self.logger.info(\"Main event loop not running. Delegating to advanced background runner.\")\n            return self.run_bg_task_advanced(task, *args, **kwargs)\n\n        # Create the coroutine if it's a function\n        coro = task(*args, **kwargs) if asyncio.iscoroutinefunction(task) else task\n\n        # Create a task on the running event loop\n        bg_task = loop.create_task(coro)\n\n        # Add a callback to log exceptions from the background task\n        def _log_exception(the_task: asyncio.Task):\n            if not the_task.cancelled() and the_task.exception():\n                self.logger.error(f\"Exception in background task '{the_task.get_name()}':\",\n                                  exc_info=the_task.exception())\n\n        bg_task.add_done_callback(_log_exception)\n        self.bg_tasks.append(bg_task)\n        return bg_task\n\n    except Exception as e:\n        self.logger.error(f\"Failed to schedule background task: {e}\", exc_info=True)\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.run_bg_task_advanced","title":"<code>run_bg_task_advanced(task, *args, **kwargs)</code>","text":"<p>Runs a task in a separate, dedicated background thread with its own event loop.</p> <p>This is ideal for: 1. Running an async task from a synchronous context. 2. Launching a long-running, independent operation that should not    interfere with the main application's event loop.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Callable</code> <p>The function to run (can be sync or async).</p> required <code>*args</code> <p>Arguments for the task.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments for the task.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Thread</code> <p>The threading.Thread object managing the background execution.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def run_bg_task_advanced(self, task: Callable, *args, **kwargs) -&gt; threading.Thread:\n    \"\"\"\n    Runs a task in a separate, dedicated background thread with its own event loop.\n\n    This is ideal for:\n    1. Running an async task from a synchronous context.\n    2. Launching a long-running, independent operation that should not\n       interfere with the main application's event loop.\n\n    Args:\n        task: The function to run (can be sync or async).\n        *args: Arguments for the task.\n        **kwargs: Keyword arguments for the task.\n\n    Returns:\n        The threading.Thread object managing the background execution.\n    \"\"\"\n    if not callable(task):\n        self.logger.warning(\"Task for run_bg_task_advanced is not callable!\")\n        return None\n\n    def thread_target():\n        # Each thread gets its own event loop.\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n\n        try:\n            # Prepare the coroutine we need to run\n            if asyncio.iscoroutinefunction(task):\n                coro = task(*args, **kwargs)\n            elif asyncio.iscoroutine(task):\n                # It's already a coroutine object\n                coro = task\n            else:\n                # It's a synchronous function, run it in an executor\n                # to avoid blocking the new event loop.\n                coro = loop.run_in_executor(None, lambda: task(*args, **kwargs))\n\n            # Run the coroutine to completion\n            result = loop.run_until_complete(coro)\n            self.logger.debug(f\"Advanced background task '{getattr(task, '__name__', 'unknown')}' completed.\")\n            if result is not None:\n                self.logger.debug(f\"Task result: {str(result)[:100]}\")\n\n        except Exception as e:\n            self.logger.error(f\"Error in advanced background task '{getattr(task, '__name__', 'unknown')}':\",\n                              exc_info=e)\n        finally:\n            # Cleanly shut down the event loop in this thread.\n            try:\n                all_tasks = asyncio.all_tasks(loop=loop)\n                if all_tasks:\n                    for t in all_tasks:\n                        t.cancel()\n                    loop.run_until_complete(asyncio.gather(*all_tasks, return_exceptions=True))\n            finally:\n                loop.close()\n                asyncio.set_event_loop(None)\n\n    # Create, start, and return the thread.\n    # It's a daemon thread so it won't prevent the main app from exiting.\n    t = threading.Thread(target=thread_target, daemon=True, name=f\"BGTask-{getattr(task, '__name__', 'unknown')}\")\n    self.bg_tasks.append(t)\n    t.start()\n    return t\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.show_console","title":"<code>show_console(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>proxi attr</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>@staticmethod\ndef show_console(*args, **kwargs):\n    \"\"\"proxi attr\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.tb","title":"<code>tb(name=None, mod_name='', helper='', version=None, test=True, restrict_in_virtual_mode=False, api=False, initial=False, exit_f=False, test_only=False, memory_cache=False, file_cache=False, request_as_kwarg=False, row=False, state=None, level=-1, memory_cache_max_size=100, memory_cache_ttl=300, samples=None, interface=None, pre_compute=None, post_compute=None, api_methods=None)</code>","text":"<p>A decorator for registering and configuring functions within a module.</p> <p>This decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to register the function under. Defaults to the function's own name.</p> <code>None</code> <code>mod_name</code> <code>str</code> <p>The name of the module the function belongs to.</p> <code>''</code> <code>helper</code> <code>str</code> <p>A helper string providing additional information about the function.</p> <code>''</code> <code>version</code> <code>str or None</code> <p>The version of the function or module.</p> <code>None</code> <code>test</code> <code>bool</code> <p>Flag to indicate if the function is for testing purposes.</p> <code>True</code> <code>restrict_in_virtual_mode</code> <code>bool</code> <p>Flag to restrict the function in virtual mode.</p> <code>False</code> <code>api</code> <code>bool</code> <p>Flag to indicate if the function is part of an API.</p> <code>False</code> <code>initial</code> <code>bool</code> <p>Flag to indicate if the function should be executed at initialization.</p> <code>False</code> <code>exit_f</code> <code>bool</code> <p>Flag to indicate if the function should be executed at exit.</p> <code>False</code> <code>test_only</code> <code>bool</code> <p>Flag to indicate if the function should only be used for testing.</p> <code>False</code> <code>memory_cache</code> <code>bool</code> <p>Flag to enable memory caching for the function.</p> <code>False</code> <code>request_as_kwarg</code> <code>bool</code> <p>Flag to get request if the fuction is calld from api.</p> <code>False</code> <code>file_cache</code> <code>bool</code> <p>Flag to enable file caching for the function.</p> <code>False</code> <code>row</code> <code>bool</code> <p>rather to auto wrap the result in Result type default False means no row data aka result type</p> <code>False</code> <code>state</code> <code>bool or None</code> <p>Flag to indicate if the function maintains state.</p> <code>None</code> <code>level</code> <code>int</code> <p>The level of the function, used for prioritization or categorization.</p> <code>-1</code> <code>memory_cache_max_size</code> <code>int</code> <p>Maximum size of the memory cache.</p> <code>100</code> <code>memory_cache_ttl</code> <code>int</code> <p>Time-to-live for the memory cache entries.</p> <code>300</code> <code>samples</code> <code>list or dict or None</code> <p>Samples or examples of function usage.</p> <code>None</code> <code>interface</code> <code>str</code> <p>The interface type for the function.</p> <code>None</code> <code>pre_compute</code> <code>callable</code> <p>A function to be called before the main function.</p> <code>None</code> <code>post_compute</code> <code>callable</code> <p>A function to be called after the main function.</p> <code>None</code> <code>api_methods</code> <code>list[str]</code> <p>default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>function</code> <p>The decorated function with additional processing and registration capabilities.</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def tb(self, name=None,\n       mod_name: str = \"\",\n       helper: str = \"\",\n       version: str | None = None,\n       test: bool = True,\n       restrict_in_virtual_mode: bool = False,\n       api: bool = False,\n       initial: bool = False,\n       exit_f: bool = False,\n       test_only: bool = False,\n       memory_cache: bool = False,\n       file_cache: bool = False,\n       request_as_kwarg: bool = False,\n       row: bool = False,\n       state: bool | None = None,\n       level: int = -1,\n       memory_cache_max_size: int = 100,\n       memory_cache_ttl: int = 300,\n       samples: list or dict or None = None,\n       interface: ToolBoxInterfaces or None or str = None,\n       pre_compute=None,\n       post_compute=None,\n       api_methods=None,\n       ):\n    \"\"\"\nA decorator for registering and configuring functions within a module.\n\nThis decorator is used to wrap functions with additional functionality such as caching, API conversion, and lifecycle management (initialization and exit). It also handles the registration of the function in the module's function registry.\n\nArgs:\n    name (str, optional): The name to register the function under. Defaults to the function's own name.\n    mod_name (str, optional): The name of the module the function belongs to.\n    helper (str, optional): A helper string providing additional information about the function.\n    version (str or None, optional): The version of the function or module.\n    test (bool, optional): Flag to indicate if the function is for testing purposes.\n    restrict_in_virtual_mode (bool, optional): Flag to restrict the function in virtual mode.\n    api (bool, optional): Flag to indicate if the function is part of an API.\n    initial (bool, optional): Flag to indicate if the function should be executed at initialization.\n    exit_f (bool, optional): Flag to indicate if the function should be executed at exit.\n    test_only (bool, optional): Flag to indicate if the function should only be used for testing.\n    memory_cache (bool, optional): Flag to enable memory caching for the function.\n    request_as_kwarg (bool, optional): Flag to get request if the fuction is calld from api.\n    file_cache (bool, optional): Flag to enable file caching for the function.\n    row (bool, optional): rather to auto wrap the result in Result type default False means no row data aka result type\n    state (bool or None, optional): Flag to indicate if the function maintains state.\n    level (int, optional): The level of the function, used for prioritization or categorization.\n    memory_cache_max_size (int, optional): Maximum size of the memory cache.\n    memory_cache_ttl (int, optional): Time-to-live for the memory cache entries.\n    samples (list or dict or None, optional): Samples or examples of function usage.\n    interface (str, optional): The interface type for the function.\n    pre_compute (callable, optional): A function to be called before the main function.\n    post_compute (callable, optional): A function to be called after the main function.\n    api_methods (list[str], optional): default [\"AUTO\"] (GET if not params, POST if params) , GET, POST, PUT or DELETE.\n\nReturns:\n    function: The decorated function with additional processing and registration capabilities.\n\"\"\"\n    if interface is None:\n        interface = \"tb\"\n    if test_only and 'test' not in self.id:\n        return lambda *args, **kwargs: args\n    return self._create_decorator(interface,\n                                  name,\n                                  mod_name,\n                                  level=level,\n                                  restrict_in_virtual_mode=restrict_in_virtual_mode,\n                                  helper=helper,\n                                  api=api,\n                                  version=version,\n                                  initial=initial,\n                                  exit_f=exit_f,\n                                  test=test,\n                                  samples=samples,\n                                  state=state,\n                                  pre_compute=pre_compute,\n                                  post_compute=post_compute,\n                                  memory_cache=memory_cache,\n                                  file_cache=file_cache,\n                                  request_as_kwarg=request_as_kwarg,\n                                  row=row,\n                                  api_methods=api_methods,\n                                  memory_cache_max_size=memory_cache_max_size,\n                                  memory_cache_ttl=memory_cache_ttl)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.utils.toolbox.App.wait_for_bg_tasks","title":"<code>wait_for_bg_tasks(timeout=None)</code>","text":"<p>Wait for all background tasks to complete.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <p>Maximum time to wait (in seconds) for all tasks to complete.      None means wait indefinitely.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if all tasks completed, False if timeout occurred</p> Source code in <code>toolboxv2/utils/toolbox.py</code> <pre><code>def wait_for_bg_tasks(self, timeout=None):\n    \"\"\"\n    Wait for all background tasks to complete.\n\n    Args:\n        timeout: Maximum time to wait (in seconds) for all tasks to complete.\n                 None means wait indefinitely.\n\n    Returns:\n        bool: True if all tasks completed, False if timeout occurred\n    \"\"\"\n    active_tasks = [t for t in self.bg_tasks if t.is_alive()]\n\n    for task in active_tasks:\n        task.join(timeout=timeout)\n        if task.is_alive():\n            return False\n\n    return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.show_console","title":"<code>toolboxv2.show_console(show=True)</code>","text":"Source code in <code>toolboxv2/utils/extras/show_and_hide_console.py</code> <pre><code>def show_console(show=True):\n    global TBRUNNER_console_viabel\n    \"\"\"Brings up the Console Window.\"\"\"\n    try:\n        if show and not TBRUNNER_console_viabel:\n            # Show console\n            ctypes.windll.user32.ShowWindow(ctypes.windll.kernel32.GetConsoleWindow(), 4)\n            TBRUNNER_console_viabel = True\n            return True\n        elif not show and TBRUNNER_console_viabel:\n            # Hide console\n            ctypes.windll.user32.ShowWindow(ctypes.windll.kernel32.GetConsoleWindow(), 0)\n            TBRUNNER_console_viabel = False\n            return True\n    except:\n        print(f\"Could not show_console {show=}\", )\n        return False\n    return False\n</code></pre>"},{"location":"toolboxv2/#logging","title":"Logging","text":""},{"location":"toolboxv2/#toolboxv2.get_logger","title":"<code>toolboxv2.get_logger()</code>","text":"Source code in <code>toolboxv2/utils/system/tb_logger.py</code> <pre><code>def get_logger() -&gt; logging.Logger:\n    return logging.getLogger(loggerNameOfToolboxv2)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.setup_logging","title":"<code>toolboxv2.setup_logging(level, name=loggerNameOfToolboxv2, online_level=None, is_online=False, file_level=None, interminal=False, logs_directory='../logs', app_name='main')</code>","text":"Source code in <code>toolboxv2/utils/system/tb_logger.py</code> <pre><code>def setup_logging(level: int, name=loggerNameOfToolboxv2, online_level=None, is_online=False, file_level=None,\n                  interminal=False, logs_directory=\"../logs\", app_name=\"main\"):\n    global loggerNameOfToolboxv2\n\n    if not online_level:\n        online_level = level\n\n    if not file_level:\n        file_level = level\n\n    if not os.path.exists(logs_directory):\n        os.makedirs(logs_directory, exist_ok=True)\n    if not os.path.exists(logs_directory + \"/Logs.info\"):\n        open(f\"{logs_directory}/Logs.info\", \"a\").close()\n\n    loggerNameOfToolboxv2 = name\n\n    available_log_levels = [logging.CRITICAL, logging.FATAL, logging.ERROR, logging.WARNING, logging.WARN, logging.INFO,\n                            logging.DEBUG, logging.NOTSET]\n\n    if level not in available_log_levels:\n        raise ValueError(f\"level must be one of {available_log_levels}, but logging level is {level}\")\n\n    if online_level not in available_log_levels:\n        raise ValueError(f\"online_level must be one of {available_log_levels}, but logging level is {online_level}\")\n\n    if file_level not in available_log_levels:\n        raise ValueError(f\"file_level must be one of {available_log_levels}, but logging level is {file_level}\")\n\n    log_date = datetime.datetime.today().strftime('%Y-%m-%d')\n    log_levels = [\"CRITICAL\", \"ERROR\", \"WARNING\", \"INFO\", \"DEBUG\", \"NOTSET\"]\n    log_level_index = log_levels.index(logging.getLevelName(level))\n\n    filename = f\"Logs-{name}-{log_date}-{log_levels[log_level_index]}\"\n    log_filename = f\"{logs_directory}/{filename}.log\"\n\n    log_info_data = {\n        filename: 0,\n        \"H\": \"localhost\",\n        \"P\": 62435\n    }\n\n    with open(f\"{logs_directory}/Logs.info\") as li:\n        log_info_data_str = li.read()\n        try:\n            log_info_data = eval(log_info_data_str)\n        except SyntaxError:\n            if log_info_data_str:\n                print(Style.RED(Style.Bold(\"Could not parse log info data\")))\n\n        if filename not in log_info_data:\n            log_info_data[filename] = 0\n\n        if not os.path.exists(log_filename):\n            log_info_data[filename] = 0\n            print(\"new log file\")\n\n        if os.path.exists(log_filename):\n            log_info_data[filename] += 1\n\n            while os.path.exists(f\"{logs_directory}/{filename}#{log_info_data[filename]}.log\"):\n                log_info_data[filename] += 1\n\n            try:\n                os.rename(log_filename,\n                          f\"{logs_directory}/{filename}#{log_info_data[filename]}.log\")\n            except PermissionError:\n                print(Style.YELLOW(Style.Bold(f\"Could not rename log file appending on {filename}\")))\n\n    with open(f\"{logs_directory}/Logs.info\", \"w\") as li:\n        if len(log_info_data.keys()) &gt;= 7:\n            log_info_data = {\n                filename: log_info_data[filename],\n                \"H\": log_info_data[\"H\"],\n                \"P\": log_info_data[\"P\"]\n            }\n        li.write(str(log_info_data))\n\n    try:\n        with open(log_filename, \"a\"):\n            pass\n    except OSError:\n        log_filename = f\"{logs_directory}/Logs-Test-{log_date}-{log_levels[log_level_index]}.log\"\n        with open(log_filename, \"a\"):\n            pass\n\n    logger = logging.getLogger(name)\n\n    logger.setLevel(level)\n    # Prevent logger from propagating to parent loggers\n    logger.propagate = False\n\n    terminal_format = f\"{app_name} %(asctime)s %(levelname)s %(name)s - %(message)s\"\n    file_format = f\"{app_name} %(asctime)s - %(name)s - %(levelname)s - %(filename)s - %(funcName)s:%(lineno)d - %(message)s\"\n\n    # Configure handlers\n    handlers = []\n\n    # File handler (always added)\n    file_handler = logging.FileHandler(log_filename)\n    file_handler.setFormatter(logging.Formatter(file_format))\n    file_handler.setLevel(file_level)\n    handlers.append(file_handler)\n\n    # Terminal handler (if requested)\n    if interminal:\n        terminal_handler = logging.StreamHandler()\n        terminal_handler.setFormatter(logging.Formatter(terminal_format))\n        terminal_handler.setLevel(level)\n        handlers.append(terminal_handler)\n\n    # Socket handler (if requested)\n    if is_online:\n        socket_handler = SocketHandler(log_info_data[\"H\"], log_info_data[\"P\"])\n        socket_handler.setFormatter(logging.Formatter(file_format))\n        socket_handler.setLevel(online_level)\n        handlers.append(socket_handler)\n\n    # Add all handlers to logger\n    for handler in handlers:\n        logger.addHandler(handler)\n\n    return logger, filename\n</code></pre>"},{"location":"toolboxv2/#styling-console-output","title":"Styling &amp; Console Output","text":""},{"location":"toolboxv2/#toolboxv2.Style","title":"<code>toolboxv2.Style</code>","text":"Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>class Style:\n    _END = '\\33[0m'\n    _BLACK = '\\33[30m'\n    _RED = '\\33[31m'\n    _GREEN = '\\33[32m'\n    _YELLOW = '\\33[33m'\n    _BLUE = '\\33[34m'\n    _MAGENTA = '\\33[35m'\n    _CYAN = '\\33[36m'\n    _WHITE = '\\33[37m'\n\n    _Bold = '\\33[1m'\n    _ITALIC = '\\33[3m'\n    _Underline = '\\33[4m'\n    _BLINK = '\\33[5m'\n    _BLINK2 = '\\33[6m'\n    _Reversed = '\\33[7m'\n\n    _BLACKBG = '\\33[40m'\n    _REDBG = '\\33[41m'\n    _GREENBG = '\\33[42m'\n    _YELLOWBG = '\\33[43m'\n    _BLUEBG = '\\33[44m'\n    _VIOLETBG = '\\33[45m'\n    _BEIGEBG = '\\33[46m'\n    _WHITEBG = '\\33[47m'\n\n    _GREY = '\\33[90m'\n    _RED2 = '\\33[91m'\n    _GREEN2 = '\\33[92m'\n    _YELLOW2 = '\\33[93m'\n    _BLUE2 = '\\33[94m'\n    _VIOLET2 = '\\33[95m'\n    _BEIGE2 = '\\33[96m'\n    _WHITE2 = '\\33[97m'\n\n    _GREYBG = '\\33[100m'\n    _REDBG2 = '\\33[101m'\n    _GREENBG2 = '\\33[102m'\n    _YELLOWBG2 = '\\33[103m'\n    _BLUEBG2 = '\\33[104m'\n    _VIOLETBG2 = '\\33[105m'\n    _BEIGEBG2 = '\\33[106m'\n    _WHITEBG2 = '\\33[107m'\n\n    style_dic = {\n        \"END\": _END,\n        \"BLACK\": _BLACK,\n        \"RED\": _RED,\n        \"GREEN\": _GREEN,\n        \"YELLOW\": _YELLOW,\n        \"BLUE\": _BLUE,\n        \"MAGENTA\": _MAGENTA,\n        \"CYAN\": _CYAN,\n        \"WHITE\": _WHITE,\n        \"Bold\": _Bold,\n        \"Underline\": _Underline,\n        \"Reversed\": _Reversed,\n\n        \"ITALIC\": _ITALIC,\n        \"BLINK\": _BLINK,\n        \"BLINK2\": _BLINK2,\n        \"BLACKBG\": _BLACKBG,\n        \"REDBG\": _REDBG,\n        \"GREENBG\": _GREENBG,\n        \"YELLOWBG\": _YELLOWBG,\n        \"BLUEBG\": _BLUEBG,\n        \"VIOLETBG\": _VIOLETBG,\n        \"BEIGEBG\": _BEIGEBG,\n        \"WHITEBG\": _WHITEBG,\n        \"GREY\": _GREY,\n        \"RED2\": _RED2,\n        \"GREEN2\": _GREEN2,\n        \"YELLOW2\": _YELLOW2,\n        \"BLUE2\": _BLUE2,\n        \"VIOLET2\": _VIOLET2,\n        \"BEIGE2\": _BEIGE2,\n        \"WHITE2\": _WHITE2,\n        \"GREYBG\": _GREYBG,\n        \"REDBG2\": _REDBG2,\n        \"GREENBG2\": _GREENBG2,\n        \"YELLOWBG2\": _YELLOWBG2,\n        \"BLUEBG2\": _BLUEBG2,\n        \"VIOLETBG2\": _VIOLETBG2,\n        \"BEIGEBG2\": _BEIGEBG2,\n        \"WHITEBG2\": _WHITEBG2,\n\n    }\n\n    @staticmethod\n    @text_save\n    def END_():\n        print(Style._END)\n\n    @staticmethod\n    @text_save\n    def GREEN_():\n        print(Style._GREEN)\n\n    @staticmethod\n    @text_save\n    def BLUE(text: str):\n        return Style._BLUE + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BLACK(text: str):\n        return Style._BLACK + text + Style._END\n\n    @staticmethod\n    @text_save\n    def RED(text: str):\n        return Style._RED + text + Style._END\n\n    @staticmethod\n    @text_save\n    def GREEN(text: str):\n        return Style._GREEN + text + Style._END\n\n    @staticmethod\n    @text_save\n    def YELLOW(text: str):\n        return Style._YELLOW + text + Style._END\n\n    @staticmethod\n    @text_save\n    def MAGENTA(text: str):\n        return Style._MAGENTA + text + Style._END\n\n    @staticmethod\n    @text_save\n    def CYAN(text: str):\n        return Style._CYAN + text + Style._END\n\n    @staticmethod\n    @text_save\n    def WHITE(text: str):\n        return Style._WHITE + text + Style._END\n\n    @staticmethod\n    @text_save\n    def Bold(text: str):\n        return Style._Bold + text + Style._END\n\n    @staticmethod\n    @text_save\n    def Underline(text: str):\n        return Style._Underline + text + Style._END\n\n    @staticmethod\n    @text_save\n    def Underlined(text: str):\n        return Style._Underline + text + Style._END\n\n    @staticmethod\n    @text_save\n    def Reversed(text: str):\n        return Style._Reversed + text + Style._END\n\n    @staticmethod\n    @text_save\n    def ITALIC(text: str):\n        return Style._ITALIC + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BLINK(text: str):\n        return Style._BLINK + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BLINK2(text: str):\n        return Style._BLINK2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BLACKBG(text: str):\n        return Style._BLACKBG + text + Style._END\n\n    @staticmethod\n    @text_save\n    def REDBG(text: str):\n        return Style._REDBG + text + Style._END\n\n    @staticmethod\n    @text_save\n    def GREENBG(text: str):\n        return Style._GREENBG + text + Style._END\n\n    @staticmethod\n    @text_save\n    def YELLOWBG(text: str):\n        return Style._YELLOWBG + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BLUEBG(text: str):\n        return Style._BLUEBG + text + Style._END\n\n    @staticmethod\n    @text_save\n    def VIOLETBG(text: str):\n        return Style._VIOLETBG + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BEIGEBG(text: str):\n        return Style._BEIGEBG + text + Style._END\n\n    @staticmethod\n    @text_save\n    def WHITEBG(text: str):\n        return Style._WHITEBG + text + Style._END\n\n    @staticmethod\n    @text_save\n    def GREY(text: str):\n        return Style._GREY + str(text) + Style._END\n\n    @staticmethod\n    @text_save\n    def RED2(text: str):\n        return Style._RED2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def GREEN2(text: str):\n        return Style._GREEN2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def YELLOW2(text: str):\n        return Style._YELLOW2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BLUE2(text: str):\n        return Style._BLUE2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def VIOLET2(text: str):\n        return Style._VIOLET2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BEIGE2(text: str):\n        return Style._BEIGE2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def WHITE2(text: str):\n        return Style._WHITE2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def GREYBG(text: str):\n        return Style._GREYBG + text + Style._END\n\n    @staticmethod\n    @text_save\n    def REDBG2(text: str):\n        return Style._REDBG2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def GREENBG2(text: str):\n        return Style._GREENBG2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def YELLOWBG2(text: str):\n        return Style._YELLOWBG2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BLUEBG2(text: str):\n        return Style._BLUEBG2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def VIOLETBG2(text: str):\n        return Style._VIOLETBG2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def BEIGEBG2(text: str):\n        return Style._BEIGEBG2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def WHITEBG2(text: str):\n        return Style._WHITEBG2 + text + Style._END\n\n    @staticmethod\n    @text_save\n    def loading_al(text: str):\n        b = f\"{text} /\"\n        print(b)\n        sleep(0.05)\n        cls()\n        b = f\"{text} -\"\n        print(b)\n        sleep(0.05)\n        cls()\n        b = f\"{text} \\\\\"\n        print(b)\n        sleep(0.05)\n        cls()\n        b = f\"{text} |\"\n        print(b)\n        sleep(0.05)\n        cls()\n\n    @property\n    def END(self):\n        return self._END\n\n    def color_demo(self):\n        for color in self.style_dic:\n            print(f\"{color} -&gt; {self.style_dic[color]}Effect{self._END}\")\n\n    @property\n    def Underline2(self):\n        return self._Underline\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Spinner","title":"<code>toolboxv2.Spinner</code>","text":"<p>Enhanced Spinner with tqdm-like line rendering.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>class Spinner:\n    \"\"\"\n    Enhanced Spinner with tqdm-like line rendering.\n    \"\"\"\n    SYMBOL_SETS = {\n        \"c\": [\"\u25d0\", \"\u25d3\", \"\u25d1\", \"\u25d2\"],\n        \"b\": [\"\u2581\", \"\u2583\", \"\u2584\", \"\u2585\", \"\u2586\", \"\u2587\", \"\u2588\", \"\u2587\", \"\u2586\", \"\u2585\", \"\u2584\", \"\u2583\"],\n        \"d\": [\"\u28fe\", \"\u28fd\", \"\u28fb\", \"\u28bf\", \"\u287f\", \"\u28df\", \"\u28ef\", \"\u28f7\"],\n        \"w\": [\"\ud83c\udf0d\", \"\ud83c\udf0e\", \"\ud83c\udf0f\"],\n        \"s\": [\"\ud83c\udf00   \", \" \ud83c\udf00  \", \"  \ud83c\udf00 \", \"   \ud83c\udf00\", \"  \ud83c\udf00 \", \" \ud83c\udf00  \"],\n        \"+\": [\"+\", \"x\"],\n        \"t\": [\"\u2736\", \"\u2738\", \"\u2739\", \"\u273a\", \"\u2739\", \"\u2737\"]\n    }\n\n    def __init__(\n        self,\n        message: str = \"Loading...\",\n        delay: float = 0.1,\n        symbols=None,\n        count_down: bool = False,\n        time_in_s: float = 0\n    ):\n        \"\"\"Initialize spinner with flexible configuration.\"\"\"\n        # Resolve symbol set.\n        if isinstance(symbols, str):\n            symbols = self.SYMBOL_SETS.get(symbols, None)\n\n        # Default symbols if not provided.\n        if symbols is None:\n            symbols = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n\n        # Test mode symbol set.\n        if 'unittest' in sys.argv[0]:\n            symbols = ['#', '=', '-']\n\n        self.spinner = itertools.cycle(symbols)\n        self.delay = delay\n        self.message = message\n        self.running = False\n        self.spinner_thread = None\n        self.max_t = time_in_s\n        self.contd = count_down\n\n        # Rendering management.\n        self._is_primary = False\n        self._start_time = 0\n\n        # Central manager.\n        self.manager = SpinnerManager()\n\n    def _generate_render_line(self):\n        \"\"\"Generate the primary render line.\"\"\"\n        current_time = time.time()\n        if self.contd:\n            remaining = max(0, self.max_t - (current_time - self._start_time))\n            time_display = f\"{remaining:.2f}\"\n        else:\n            time_display = f\"{current_time - self._start_time:.2f}\"\n\n        symbol = next(self.spinner)\n        return f\"{symbol} {self.message} | {time_display}\"\n\n    def _generate_secondary_info(self):\n        \"\"\"Generate secondary spinner info for additional spinners.\"\"\"\n        return f\"{self.message}\"\n\n    def __enter__(self):\n        \"\"\"Start the spinner.\"\"\"\n        self.running = True\n        self._start_time = time.time()\n        self.manager.register_spinner(self)\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        \"\"\"Stop the spinner.\"\"\"\n        self.running = False\n        self.manager.unregister_spinner(self)\n        # Clear the spinner's line if it was the primary spinner.\n        if self._is_primary:\n            sys.stdout.write(\"\\r\\033[K\")\n            sys.stdout.flush()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Spinner.__enter__","title":"<code>__enter__()</code>","text":"<p>Start the spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __enter__(self):\n    \"\"\"Start the spinner.\"\"\"\n    self.running = True\n    self._start_time = time.time()\n    self.manager.register_spinner(self)\n    return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Spinner.__exit__","title":"<code>__exit__(exc_type, exc_value, exc_traceback)</code>","text":"<p>Stop the spinner.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __exit__(self, exc_type, exc_value, exc_traceback):\n    \"\"\"Stop the spinner.\"\"\"\n    self.running = False\n    self.manager.unregister_spinner(self)\n    # Clear the spinner's line if it was the primary spinner.\n    if self._is_primary:\n        sys.stdout.write(\"\\r\\033[K\")\n        sys.stdout.flush()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Spinner.__init__","title":"<code>__init__(message='Loading...', delay=0.1, symbols=None, count_down=False, time_in_s=0)</code>","text":"<p>Initialize spinner with flexible configuration.</p> Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def __init__(\n    self,\n    message: str = \"Loading...\",\n    delay: float = 0.1,\n    symbols=None,\n    count_down: bool = False,\n    time_in_s: float = 0\n):\n    \"\"\"Initialize spinner with flexible configuration.\"\"\"\n    # Resolve symbol set.\n    if isinstance(symbols, str):\n        symbols = self.SYMBOL_SETS.get(symbols, None)\n\n    # Default symbols if not provided.\n    if symbols is None:\n        symbols = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n\n    # Test mode symbol set.\n    if 'unittest' in sys.argv[0]:\n        symbols = ['#', '=', '-']\n\n    self.spinner = itertools.cycle(symbols)\n    self.delay = delay\n    self.message = message\n    self.running = False\n    self.spinner_thread = None\n    self.max_t = time_in_s\n    self.contd = count_down\n\n    # Rendering management.\n    self._is_primary = False\n    self._start_time = 0\n\n    # Central manager.\n    self.manager = SpinnerManager()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.remove_styles","title":"<code>toolboxv2.remove_styles(text, infos=False)</code>","text":"Source code in <code>toolboxv2/utils/extras/Style.py</code> <pre><code>def remove_styles(text: str, infos=False):\n    in_ = []\n    for key, style in Style.style_dic.items():\n        if style in text:\n            text = text.replace(style, '')\n            if infos:\n                in_.append([key for key, st in Style.style_dic.items() if style == st][0])\n    if infos:\n        if \"END\" in in_:\n            in_.remove('END')\n        return text, in_\n    return text\n</code></pre>"},{"location":"toolboxv2/#data-types-structures","title":"Data Types &amp; Structures","text":""},{"location":"toolboxv2/#toolboxv2.AppArgs","title":"<code>toolboxv2.AppArgs</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class AppArgs:\n    init = None\n    init_file = 'init.config'\n    get_version = False\n    mm = False\n    sm = False\n    lm = False\n    modi = 'cli'\n    kill = False\n    remote = False\n    remote_direct_key = None\n    background_application = False\n    background_application_runner = False\n    docker = False\n    build = False\n    install = None\n    remove = None\n    update = None\n    name = 'main'\n    port = 5000\n    host = '0.0.0.0'\n    load_all_mod_in_files = False\n    mods_folder = 'toolboxv2.mods.'\n    debug = None\n    test = None\n    profiler = None\n    hot_reload = False\n    live_application = True\n    sysPrint = False\n    kwargs = {}\n    session = None\n\n    def default(self):\n        return self\n\n    def set(self, name, value):\n        setattr(self, name, value)\n        return self\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result","title":"<code>toolboxv2.Result</code>","text":"Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class Result:\n    _task = None\n    def __init__(self,\n                 error: ToolBoxError,\n                 result: ToolBoxResult,\n                 info: ToolBoxInfo,\n                 origin: Any | None = None,\n                 ):\n        self.error: ToolBoxError = error\n        self.result: ToolBoxResult = result\n        self.info: ToolBoxInfo = info\n        self.origin = origin\n\n    def as_result(self):\n        return self\n\n    def as_dict(self):\n        return {\n            \"error\":self.error.value if isinstance(self.error, Enum) else self.error,\n        \"result\" : {\n            \"data_to\":self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n            \"data_info\":self.result.data_info,\n            \"data\":self.result.data,\n            \"data_type\":self.result.data_type\n        } if self.result else None,\n        \"info\" : {\n            \"exec_code\" : self.info.exec_code,  # exec_code umwandel in http resposn codes\n        \"help_text\" : self.info.help_text\n        } if self.info else None,\n        \"origin\" : self.origin\n        }\n\n    def set_origin(self, origin):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = origin\n        return self\n\n    def set_dir_origin(self, name, extras=\"assets/\"):\n        if self.origin is not None:\n            raise ValueError(\"You cannot Change the origin of a Result!\")\n        self.origin = f\"mods/{name}/{extras}\"\n        return self\n\n    def is_error(self):\n        if _test_is_result(self.result.data):\n            return self.result.data.is_error()\n        if self.error == ToolBoxError.none:\n            return False\n        if self.info.exec_code == 0:\n            return False\n        if self.info.exec_code == 200:\n            return False\n        return True\n\n    def is_ok(self):\n        return not self.is_error()\n\n    def is_data(self):\n        return self.result.data is not None\n\n    def to_api_result(self):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=self.error.value if isinstance(self.error, Enum) else self.error,\n            result=ToolBoxResultBM(\n                data_to=self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n                data_info=self.result.data_info,\n                data=self.result.data,\n                data_type=self.result.data_type\n            ) if self.result else None,\n            info=ToolBoxInfoBM(\n                exec_code=self.info.exec_code,  # exec_code umwandel in http resposn codes\n                help_text=self.info.help_text\n            ) if self.info else None,\n            origin=self.origin\n        )\n\n    def task(self, task):\n        self._task = task\n        return self\n\n    @staticmethod\n    def result_from_dict(error: str, result: dict, info: dict, origin: list or None or str):\n        # print(f\" error={self.error}, result= {self.result}, info= {self.info}, origin= {self.origin}\")\n        return ApiResult(\n            error=error if isinstance(error, Enum) else error,\n            result=ToolBoxResultBM(\n                data_to=result.get('data_to') if isinstance(result.get('data_to'), Enum) else result.get('data_to'),\n                data_info=result.get('data_info', '404'),\n                data=result.get('data'),\n                data_type=result.get('data_type', '404'),\n            ) if result else ToolBoxResultBM(\n                data_to=ToolBoxInterfaces.cli.value,\n                data_info='',\n                data='404',\n                data_type='404',\n            ),\n            info=ToolBoxInfoBM(\n                exec_code=info.get('exec_code', 404),\n                help_text=info.get('help_text', '404')\n            ) if info else ToolBoxInfoBM(\n                exec_code=404,\n                help_text='404'\n            ),\n            origin=origin\n        ).as_result()\n\n    @classmethod\n    def stream(cls,\n               stream_generator: Any,  # Renamed from source for clarity\n               content_type: str = \"text/event-stream\",  # Default to SSE\n               headers: Union[dict, None] = None,\n               info: str = \"OK\",\n               interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n               cleanup_func: Union[\n                   Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None):\n        \"\"\"\n        Create a streaming response Result. Handles SSE and other stream types.\n\n        Args:\n            stream_generator: Any stream source (async generator, sync generator, iterable, or single item).\n            content_type: Content-Type header (default: text/event-stream for SSE).\n            headers: Additional HTTP headers for the response.\n            info: Help text for the result.\n            interface: Interface to send data to.\n            cleanup_func: Optional function for cleanup.\n\n        Returns:\n            A Result object configured for streaming.\n        \"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        final_generator: AsyncGenerator[str, None]\n\n        if content_type == \"text/event-stream\":\n            # For SSE, always use SSEGenerator.create_sse_stream to wrap the source.\n            # SSEGenerator.create_sse_stream handles various types of stream_generator internally.\n            final_generator = SSEGenerator.create_sse_stream(source=stream_generator, cleanup_func=cleanup_func)\n\n            # Standard SSE headers for the HTTP response itself\n            # These will be stored in the Result object. Rust side decides how to use them.\n            standard_sse_headers = {\n                \"Cache-Control\": \"no-cache\",  # SSE specific\n                \"Connection\": \"keep-alive\",  # SSE specific\n                \"X-Accel-Buffering\": \"no\",  # Useful for proxies with SSE\n                # Content-Type is implicitly text/event-stream, will be in streaming_data below\n            }\n            all_response_headers = standard_sse_headers.copy()\n            if headers:\n                all_response_headers.update(headers)\n        else:\n            # For non-SSE streams.\n            # If stream_generator is sync, wrap it to be async.\n            # If already async or single item, it will be handled.\n            # Rust's stream_generator in ToolboxClient seems to handle both sync/async Python generators.\n            # For consistency with how SSEGenerator does it, we can wrap sync ones.\n            if inspect.isgenerator(stream_generator) or \\\n                (not isinstance(stream_generator, str) and hasattr(stream_generator, '__iter__')):\n                final_generator = SSEGenerator.wrap_sync_generator(stream_generator)  # Simple async wrapper\n            elif inspect.isasyncgen(stream_generator):\n                final_generator = stream_generator\n            else:  # Single item or string\n                async def _single_item_gen():\n                    yield stream_generator\n\n                final_generator = _single_item_gen()\n            all_response_headers = headers if headers else {}\n\n        # Prepare streaming data to be stored in the Result object\n        streaming_data = {\n            \"type\": \"stream\",  # Indicator for Rust side\n            \"generator\": final_generator,\n            \"content_type\": content_type,  # Let Rust know the intended content type\n            \"headers\": all_response_headers  # Intended HTTP headers for the overall response\n        }\n\n        result_payload = ToolBoxResult(\n            data_to=interface,\n            data=streaming_data,\n            data_info=\"Streaming response\" if content_type != \"text/event-stream\" else \"SSE Event Stream\",\n            data_type=\"stream\"  # Generic type for Rust to identify it needs to stream from 'generator'\n        )\n\n        return cls(error=error, info=info_obj, result=result_payload)\n\n    @classmethod\n    def sse(cls,\n            stream_generator: Any,\n            info: str = \"OK\",\n            interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n            cleanup_func: Union[\n                Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None,\n            # http_headers: Optional[dict] = None # If we want to allow overriding default SSE HTTP headers\n            ):\n        \"\"\"\n        Create an Server-Sent Events (SSE) streaming response Result.\n\n        Args:\n            stream_generator: A source yielding individual data items. This can be an\n                              async generator, sync generator, iterable, or a single item.\n                              Each item will be formatted as an SSE event.\n            info: Optional help text for the Result.\n            interface: Optional ToolBoxInterface to target.\n            cleanup_func: Optional cleanup function to run when the stream ends or is cancelled.\n            #http_headers: Optional dictionary of custom HTTP headers for the SSE response.\n\n        Returns:\n            A Result object configured for SSE streaming.\n        \"\"\"\n        # Result.stream will handle calling SSEGenerator.create_sse_stream\n        # and setting appropriate default headers for SSE when content_type is \"text/event-stream\".\n        return cls.stream(\n            stream_generator=stream_generator,\n            content_type=\"text/event-stream\",\n            # headers=http_headers, # Pass if we add http_headers param\n            info=info,\n            interface=interface,\n            cleanup_func=cleanup_func\n        )\n\n    @classmethod\n    def default(cls, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=-1, help_text=\"\")\n        result = ToolBoxResult(data_to=interface)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None):\n        \"\"\"Create a JSON response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code or exec_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=data,\n            data_info=\"JSON response\",\n            data_type=\"json\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n        \"\"\"Create a text response Result with specific content type.\"\"\"\n        if headers is not None:\n            return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=text_data,\n            data_info=\"Text response\",\n            data_type=content_type\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n               interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a binary data response Result.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n        # Create a dictionary with binary data and metadata\n        binary_data = {\n            \"data\": data,\n            \"content_type\": content_type,\n            \"filename\": download_name\n        }\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=binary_data,\n            data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n            data_type=\"binary\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def file(cls, data, filename, content_type=None, info=\"OK\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a file download response Result.\n\n        Args:\n            data: File data as bytes or base64 string\n            filename: Name of the file for download\n            content_type: MIME type of the file (auto-detected if None)\n            info: Response info text\n            interface: Target interface\n\n        Returns:\n            Result object configured for file download\n        \"\"\"\n        import base64\n        import mimetypes\n\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=200, help_text=info)\n\n        # Auto-detect content type if not provided\n        if content_type is None:\n            content_type, _ = mimetypes.guess_type(filename)\n            if content_type is None:\n                content_type = \"application/octet-stream\"\n\n        # Ensure data is base64 encoded string (as expected by Rust server)\n        if isinstance(data, bytes):\n            base64_data = base64.b64encode(data).decode('utf-8')\n        elif isinstance(data, str):\n            # Assume it's already base64 encoded\n            base64_data = data\n        else:\n            raise ValueError(\"File data must be bytes or base64 string\")\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=base64_data,  # Rust expects base64 string for \"file\" type\n            data_info=f\"File download: {filename}\",\n            data_type=\"file\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n        \"\"\"Create a redirect response.\"\"\"\n        error = ToolBoxError.none\n        info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n        result = ToolBoxResult(\n            data_to=interface,\n            data=url,\n            data_info=\"Redirect response\",\n            data_type=\"redirect\"\n        )\n\n        return cls(error=error, info=info_obj, result=result)\n\n    @classmethod\n    def ok(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def html(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.remote, data_type=\"html\",status=200, headers=None, row=False):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=status, help_text=info)\n        from ...utils.system.getting_and_closing_app import get_app\n\n        if not row and not '\"&lt;div class=\"main-content\"\"' in data:\n            data = f'&lt;div class=\"main-content frosted-glass\"&gt;{data}&lt;div&gt;'\n        if not row and not get_app().web_context() in data:\n            data = get_app().web_context() + data\n\n        if isinstance(headers, dict):\n            result = ToolBoxResult(data_to=interface, data={'html':data,'headers':headers}, data_info=data_info,\n                                   data_type=\"special_html\")\n        else:\n            result = ToolBoxResult(data_to=interface, data=data, data_info=data_info,\n                                   data_type=data_type if data_type is not None else type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def future(cls, data=None, data_info=\"\", info=\"OK\", interface=ToolBoxInterfaces.future):\n        error = ToolBoxError.none\n        info = ToolBoxInfo(exec_code=0, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=\"future\")\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def custom_error(cls, data=None, data_info=\"\", info=\"\", exec_code=-1, interface=ToolBoxInterfaces.native):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def error(cls, data=None, data_info=\"\", info=\"\", exec_code=450, interface=ToolBoxInterfaces.remote):\n        error = ToolBoxError.custom_error\n        info = ToolBoxInfo(exec_code=exec_code, help_text=info)\n        result = ToolBoxResult(data_to=interface, data=data, data_info=data_info, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_user_error(cls, info=\"\", exec_code=-3, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.input_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    @classmethod\n    def default_internal_error(cls, info=\"\", exec_code=-2, interface=ToolBoxInterfaces.native, data=None):\n        error = ToolBoxError.internal_error\n        info = ToolBoxInfo(exec_code, info)\n        result = ToolBoxResult(data_to=interface, data=data, data_type=type(data).__name__)\n        return cls(error=error, info=info, result=result)\n\n    def print(self, show=True, show_data=True, prifix=\"\"):\n        data = '\\n' + f\"{((prifix + 'Data: ' + str(self.result.data) if self.result.data is not None else 'NO Data') if not isinstance(self.result.data, Result) else self.result.data.print(show=False, show_data=show_data, prifix=prifix + '-')) if show_data else 'Data: private'}\"\n        origin = '\\n' + f\"{prifix + 'Origin: ' + str(self.origin) if self.origin is not None else 'NO Origin'}\"\n        text = (f\"Function Exec code: {self.info.exec_code}\"\n                f\"\\n{prifix}Info's:\"\n                f\" {self.info.help_text} {'&lt;|&gt; ' + str(self.result.data_info) if self.result.data_info is not None else ''}\"\n                f\"{origin}{data if not data.endswith('NO Data') else ''}\")\n        if not show:\n            return text\n        print(\"\\n======== Result ========\\n\" + text + \"\\n------- EndOfD -------\")\n        return self\n\n    def log(self, show_data=True, prifix=\"\"):\n        from toolboxv2 import get_logger\n        get_logger().debug(self.print(show=False, show_data=show_data, prifix=prifix).replace(\"\\n\", \" - \"))\n        return self\n\n    def __str__(self):\n        return self.print(show=False, show_data=True)\n\n    def get(self, key=None, default=None):\n        data = self.result.data\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    async def aget(self, key=None, default=None):\n        if asyncio.isfuture(self.result.data) or asyncio.iscoroutine(self.result.data) or (\n            isinstance(self.result.data_to, Enum) and self.result.data_to.name == ToolBoxInterfaces.future.name):\n            data = await self.result.data\n        else:\n            data = self.get(key=None, default=None)\n        if isinstance(data, Result):\n            return data.get(key=key, default=default)\n        if key is not None and isinstance(data, dict):\n            return data.get(key, default)\n        return data if data is not None else default\n\n    def lazy_return(self, _=0, data=None, **kwargs):\n        flags = ['raise', 'logg', 'user', 'intern']\n        flag = flags[_] if isinstance(_, int) else _\n        if self.info.exec_code == 0:\n            return self if data is None else data if _test_is_result(data) else self.ok(data=data, **kwargs)\n        if flag == 'raise':\n            raise ValueError(self.print(show=False))\n        if flag == 'logg':\n            from .. import get_logger\n            get_logger().error(self.print(show=False))\n\n        if flag == 'user':\n            return self if data is None else data if _test_is_result(data) else self.default_user_error(data=data,\n                                                                                                        **kwargs)\n        if flag == 'intern':\n            return self if data is None else data if _test_is_result(data) else self.default_internal_error(data=data,\n                                                                                                            **kwargs)\n\n        return self if data is None else data if _test_is_result(data) else self.custom_error(data=data, **kwargs)\n\n    @property\n    def bg_task(self):\n        return self._task\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.binary","title":"<code>binary(data, content_type='application/octet-stream', download_name=None, info='OK', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a binary data response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef binary(cls, data, content_type=\"application/octet-stream\", download_name=None, info=\"OK\",\n           interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a binary data response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    # Create a dictionary with binary data and metadata\n    binary_data = {\n        \"data\": data,\n        \"content_type\": content_type,\n        \"filename\": download_name\n    }\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=binary_data,\n        data_info=f\"Binary response: {download_name}\" if download_name else \"Binary response\",\n        data_type=\"binary\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.file","title":"<code>file(data, filename, content_type=None, info='OK', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a file download response Result.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>File data as bytes or base64 string</p> required <code>filename</code> <p>Name of the file for download</p> required <code>content_type</code> <p>MIME type of the file (auto-detected if None)</p> <code>None</code> <code>info</code> <p>Response info text</p> <code>'OK'</code> <code>interface</code> <p>Target interface</p> <code>remote</code> <p>Returns:</p> Type Description <p>Result object configured for file download</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef file(cls, data, filename, content_type=None, info=\"OK\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a file download response Result.\n\n    Args:\n        data: File data as bytes or base64 string\n        filename: Name of the file for download\n        content_type: MIME type of the file (auto-detected if None)\n        info: Response info text\n        interface: Target interface\n\n    Returns:\n        Result object configured for file download\n    \"\"\"\n    import base64\n    import mimetypes\n\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=200, help_text=info)\n\n    # Auto-detect content type if not provided\n    if content_type is None:\n        content_type, _ = mimetypes.guess_type(filename)\n        if content_type is None:\n            content_type = \"application/octet-stream\"\n\n    # Ensure data is base64 encoded string (as expected by Rust server)\n    if isinstance(data, bytes):\n        base64_data = base64.b64encode(data).decode('utf-8')\n    elif isinstance(data, str):\n        # Assume it's already base64 encoded\n        base64_data = data\n    else:\n        raise ValueError(\"File data must be bytes or base64 string\")\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=base64_data,  # Rust expects base64 string for \"file\" type\n        data_info=f\"File download: {filename}\",\n        data_type=\"file\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.json","title":"<code>json(data, info='OK', interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None)</code>  <code>classmethod</code>","text":"<p>Create a JSON response Result.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef json(cls, data, info=\"OK\", interface=ToolBoxInterfaces.remote, exec_code=0, status_code=None):\n    \"\"\"Create a JSON response Result.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code or exec_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=data,\n        data_info=\"JSON response\",\n        data_type=\"json\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.redirect","title":"<code>redirect(url, status_code=302, info='Redirect', interface=ToolBoxInterfaces.remote)</code>  <code>classmethod</code>","text":"<p>Create a redirect response.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef redirect(cls, url, status_code=302, info=\"Redirect\", interface=ToolBoxInterfaces.remote):\n    \"\"\"Create a redirect response.\"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=status_code, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=url,\n        data_info=\"Redirect response\",\n        data_type=\"redirect\"\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.sse","title":"<code>sse(stream_generator, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code>  <code>classmethod</code>","text":"<p>Create an Server-Sent Events (SSE) streaming response Result.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <code>Any</code> <p>A source yielding individual data items. This can be an               async generator, sync generator, iterable, or a single item.               Each item will be formatted as an SSE event.</p> required <code>info</code> <code>str</code> <p>Optional help text for the Result.</p> <code>'OK'</code> <code>interface</code> <code>ToolBoxInterfaces</code> <p>Optional ToolBoxInterface to target.</p> <code>remote</code> <code>cleanup_func</code> <code>Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None]</code> <p>Optional cleanup function to run when the stream ends or is cancelled.</p> <code>None</code> <code>#http_headers</code> <p>Optional dictionary of custom HTTP headers for the SSE response.</p> required <p>Returns:</p> Type Description <p>A Result object configured for SSE streaming.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef sse(cls,\n        stream_generator: Any,\n        info: str = \"OK\",\n        interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n        cleanup_func: Union[\n            Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None,\n        # http_headers: Optional[dict] = None # If we want to allow overriding default SSE HTTP headers\n        ):\n    \"\"\"\n    Create an Server-Sent Events (SSE) streaming response Result.\n\n    Args:\n        stream_generator: A source yielding individual data items. This can be an\n                          async generator, sync generator, iterable, or a single item.\n                          Each item will be formatted as an SSE event.\n        info: Optional help text for the Result.\n        interface: Optional ToolBoxInterface to target.\n        cleanup_func: Optional cleanup function to run when the stream ends or is cancelled.\n        #http_headers: Optional dictionary of custom HTTP headers for the SSE response.\n\n    Returns:\n        A Result object configured for SSE streaming.\n    \"\"\"\n    # Result.stream will handle calling SSEGenerator.create_sse_stream\n    # and setting appropriate default headers for SSE when content_type is \"text/event-stream\".\n    return cls.stream(\n        stream_generator=stream_generator,\n        content_type=\"text/event-stream\",\n        # headers=http_headers, # Pass if we add http_headers param\n        info=info,\n        interface=interface,\n        cleanup_func=cleanup_func\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.stream","title":"<code>stream(stream_generator, content_type='text/event-stream', headers=None, info='OK', interface=ToolBoxInterfaces.remote, cleanup_func=None)</code>  <code>classmethod</code>","text":"<p>Create a streaming response Result. Handles SSE and other stream types.</p> <p>Parameters:</p> Name Type Description Default <code>stream_generator</code> <code>Any</code> <p>Any stream source (async generator, sync generator, iterable, or single item).</p> required <code>content_type</code> <code>str</code> <p>Content-Type header (default: text/event-stream for SSE).</p> <code>'text/event-stream'</code> <code>headers</code> <code>Union[dict, None]</code> <p>Additional HTTP headers for the response.</p> <code>None</code> <code>info</code> <code>str</code> <p>Help text for the result.</p> <code>'OK'</code> <code>interface</code> <code>ToolBoxInterfaces</code> <p>Interface to send data to.</p> <code>remote</code> <code>cleanup_func</code> <code>Union[Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None]</code> <p>Optional function for cleanup.</p> <code>None</code> <p>Returns:</p> Type Description <p>A Result object configured for streaming.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef stream(cls,\n           stream_generator: Any,  # Renamed from source for clarity\n           content_type: str = \"text/event-stream\",  # Default to SSE\n           headers: Union[dict, None] = None,\n           info: str = \"OK\",\n           interface: ToolBoxInterfaces = ToolBoxInterfaces.remote,\n           cleanup_func: Union[\n               Callable[[], None], Callable[[], T], Callable[[], AsyncGenerator[T, None]], None] = None):\n    \"\"\"\n    Create a streaming response Result. Handles SSE and other stream types.\n\n    Args:\n        stream_generator: Any stream source (async generator, sync generator, iterable, or single item).\n        content_type: Content-Type header (default: text/event-stream for SSE).\n        headers: Additional HTTP headers for the response.\n        info: Help text for the result.\n        interface: Interface to send data to.\n        cleanup_func: Optional function for cleanup.\n\n    Returns:\n        A Result object configured for streaming.\n    \"\"\"\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=0, help_text=info)\n\n    final_generator: AsyncGenerator[str, None]\n\n    if content_type == \"text/event-stream\":\n        # For SSE, always use SSEGenerator.create_sse_stream to wrap the source.\n        # SSEGenerator.create_sse_stream handles various types of stream_generator internally.\n        final_generator = SSEGenerator.create_sse_stream(source=stream_generator, cleanup_func=cleanup_func)\n\n        # Standard SSE headers for the HTTP response itself\n        # These will be stored in the Result object. Rust side decides how to use them.\n        standard_sse_headers = {\n            \"Cache-Control\": \"no-cache\",  # SSE specific\n            \"Connection\": \"keep-alive\",  # SSE specific\n            \"X-Accel-Buffering\": \"no\",  # Useful for proxies with SSE\n            # Content-Type is implicitly text/event-stream, will be in streaming_data below\n        }\n        all_response_headers = standard_sse_headers.copy()\n        if headers:\n            all_response_headers.update(headers)\n    else:\n        # For non-SSE streams.\n        # If stream_generator is sync, wrap it to be async.\n        # If already async or single item, it will be handled.\n        # Rust's stream_generator in ToolboxClient seems to handle both sync/async Python generators.\n        # For consistency with how SSEGenerator does it, we can wrap sync ones.\n        if inspect.isgenerator(stream_generator) or \\\n            (not isinstance(stream_generator, str) and hasattr(stream_generator, '__iter__')):\n            final_generator = SSEGenerator.wrap_sync_generator(stream_generator)  # Simple async wrapper\n        elif inspect.isasyncgen(stream_generator):\n            final_generator = stream_generator\n        else:  # Single item or string\n            async def _single_item_gen():\n                yield stream_generator\n\n            final_generator = _single_item_gen()\n        all_response_headers = headers if headers else {}\n\n    # Prepare streaming data to be stored in the Result object\n    streaming_data = {\n        \"type\": \"stream\",  # Indicator for Rust side\n        \"generator\": final_generator,\n        \"content_type\": content_type,  # Let Rust know the intended content type\n        \"headers\": all_response_headers  # Intended HTTP headers for the overall response\n    }\n\n    result_payload = ToolBoxResult(\n        data_to=interface,\n        data=streaming_data,\n        data_info=\"Streaming response\" if content_type != \"text/event-stream\" else \"SSE Event Stream\",\n        data_type=\"stream\"  # Generic type for Rust to identify it needs to stream from 'generator'\n    )\n\n    return cls(error=error, info=info_obj, result=result_payload)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Result.text","title":"<code>text(text_data, content_type='text/plain', exec_code=None, status=200, info='OK', interface=ToolBoxInterfaces.remote, headers=None)</code>  <code>classmethod</code>","text":"<p>Create a text response Result with specific content type.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef text(cls, text_data, content_type=\"text/plain\",exec_code=None,status=200, info=\"OK\", interface=ToolBoxInterfaces.remote, headers=None):\n    \"\"\"Create a text response Result with specific content type.\"\"\"\n    if headers is not None:\n        return cls.html(text_data, status= exec_code or status, info=info, headers=headers)\n    error = ToolBoxError.none\n    info_obj = ToolBoxInfo(exec_code=exec_code or status, help_text=info)\n\n    result = ToolBoxResult(\n        data_to=interface,\n        data=text_data,\n        data_info=\"Text response\",\n        data_type=content_type\n    )\n\n    return cls(error=error, info=info_obj, result=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.ApiResult","title":"<code>toolboxv2.ApiResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>class ApiResult(BaseModel):\n    error: None | str= None\n    origin: Any | None\n    result: ToolBoxResultBM | None = None\n    info: ToolBoxInfoBM | None\n\n    def as_result(self):\n        return Result(\n            error=self.error.value if isinstance(self.error, Enum) else self.error,\n            result=ToolBoxResult(\n                data_to=self.result.data_to.value if isinstance(self.result.data_to, Enum) else self.result.data_to,\n                data_info=self.result.data_info,\n                data=self.result.data,\n                data_type=self.result.data_type\n            ) if self.result else None,\n            info=ToolBoxInfo(\n                exec_code=self.info.exec_code,\n                help_text=self.info.help_text\n            ) if self.info else None,\n            origin=self.origin\n        )\n\n    def to_api_result(self):\n        return self\n\n    def print(self, *args, **kwargs):\n        res = self.as_result().print(*args, **kwargs)\n        if not isinstance(res, str):\n            res = res.to_api_result()\n        return res\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.RequestData","title":"<code>toolboxv2.RequestData</code>  <code>dataclass</code>","text":"<p>Main class representing the complete request data structure.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@dataclass\nclass RequestData:\n    \"\"\"Main class representing the complete request data structure.\"\"\"\n    request: Request\n    session: Session\n    session_id: str\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any]) -&gt; 'RequestData':\n        \"\"\"Create a RequestData instance from a dictionary.\"\"\"\n        return cls(\n            request=Request.from_dict(data.get('request', {})),\n            session=Session.from_dict(data.get('session', {})),\n            session_id=data.get('session_id', '')\n        )\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert the RequestData object back to a dictionary.\"\"\"\n        return {\n            'request': self.request.to_dict(),\n            'session': self.session.to_dict(),\n            'session_id': self.session_id\n        }\n\n    def __getattr__(self, name: str) -&gt; Any:\n        \"\"\"Delegate unknown attributes to the `request` object.\"\"\"\n        # Nur wenn das Attribut nicht direkt in RequestData existiert\n        # und auch nicht `session` oder `session_id` ist\n        if hasattr(self.request, name):\n            return getattr(self.request, name)\n        raise AttributeError(f\"'RequestData' object has no attribute '{name}'\")\n\n    @classmethod\n    def moc(cls):\n        return cls(\n            request=Request.from_dict({\n                'content_type': 'application/x-www-form-urlencoded',\n                'headers': {\n                    'accept': '*/*',\n                    'accept-encoding': 'gzip, deflate, br, zstd',\n                    'accept-language': 'de-DE,de;q=0.9,en-US;q=0.8,en;q=0.7',\n                    'connection': 'keep-alive',\n                    'content-length': '107',\n                    'content-type': 'application/x-www-form-urlencoded',\n                    'cookie': 'session=abc123',\n                    'host': 'localhost:8080',\n                    'hx-current-url': 'http://localhost:8080/api/TruthSeeker/get_main_ui',\n                    'hx-request': 'true',\n                    'hx-target': 'estimates-guest_1fc2c9',\n                    'hx-trigger': 'config-form-guest_1fc2c9',\n                    'origin': 'http://localhost:8080',\n                    'referer': 'http://localhost:8080/api/TruthSeeker/get_main_ui',\n                    'sec-ch-ua': '\"Chromium\";v=\"134\", \"Not:A-Brand\";v=\"24\", \"Google Chrome\";v=\"134\"',\n                    'sec-ch-ua-mobile': '?0',\n                    'sec-ch-ua-platform': '\"Windows\"',\n                    'sec-fetch-dest': 'empty',\n                    'sec-fetch-mode': 'cors',\n                    'sec-fetch-site': 'same-origin',\n                    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n                },\n                'method': 'POST',\n                'path': '/api/TruthSeeker/update_estimates',\n                'query_params': {},\n                'form_data': {\n                    'param1': 'value1',\n                    'param2': 'value2'\n                }\n            }),\n            session=Session.from_dict({\n                'SiID': '29a2e258e18252e2afd5ff943523f09c82f1bb9adfe382a6f33fc6a8381de898',\n                'level': '1',\n                'spec': '74eed1c8de06886842e235486c3c2fd6bcd60586998ac5beb87f13c0d1750e1d',\n                'user_name': 'root',\n                'custom_field': 'custom_value'\n            }),\n            session_id='0x29dd1ac0d1e30d3f'\n        )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.RequestData.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Delegate unknown attributes to the <code>request</code> object.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"Delegate unknown attributes to the `request` object.\"\"\"\n    # Nur wenn das Attribut nicht direkt in RequestData existiert\n    # und auch nicht `session` oder `session_id` ist\n    if hasattr(self.request, name):\n        return getattr(self.request, name)\n    raise AttributeError(f\"'RequestData' object has no attribute '{name}'\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.RequestData.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create a RequestData instance from a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; 'RequestData':\n    \"\"\"Create a RequestData instance from a dictionary.\"\"\"\n    return cls(\n        request=Request.from_dict(data.get('request', {})),\n        session=Session.from_dict(data.get('session', {})),\n        session_id=data.get('session_id', '')\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.RequestData.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert the RequestData object back to a dictionary.</p> Source code in <code>toolboxv2/utils/system/types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert the RequestData object back to a dictionary.\"\"\"\n    return {\n        'request': self.request.to_dict(),\n        'session': self.session.to_dict(),\n        'session_id': self.session_id\n    }\n</code></pre>"},{"location":"toolboxv2/#security","title":"Security","text":""},{"location":"toolboxv2/#toolboxv2.Code","title":"<code>toolboxv2.Code</code>","text":"Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>class Code:\n\n    @staticmethod\n    def DK():\n        return DEVICE_KEY\n\n    def decode_code(self, encrypted_data, key=None):\n\n        if not isinstance(encrypted_data, str):\n            encrypted_data = str(encrypted_data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.decrypt_symmetric(encrypted_data, key)\n\n    def encode_code(self, data, key=None):\n\n        if not isinstance(data, str):\n            data = str(data)\n\n        if key is None:\n            key = DEVICE_KEY()\n\n        return self.encrypt_symmetric(data, key)\n\n    @staticmethod\n    def generate_seed() -&gt; int:\n        \"\"\"\n        Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n        Returns:\n            int: Eine zuf\u00e4llige Zahl.\n        \"\"\"\n        return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n\n    @staticmethod\n    def one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n        \"\"\"\n        Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n        Args:\n            text (str): Der zu hashende Text.\n            salt (str): Der Salt-Wert.\n            pepper (str): Der Pepper-Wert.\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            str: Der resultierende Hash-Wert.\n        \"\"\"\n        return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n\n    @staticmethod\n    def generate_symmetric_key(as_str=True) -&gt; str or bytes:\n        \"\"\"\n        Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n        Returns:\n            str: Der generierte Schl\u00fcssel.\n        \"\"\"\n        key = Fernet.generate_key()\n        if as_str:\n            key = key.decode()\n        return key\n\n    @staticmethod\n    def encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        if isinstance(text, str):\n            text = text.encode()\n\n        try:\n            fernet = Fernet(key.encode())\n            return fernet.encrypt(text).decode()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n            return \"Error encrypt\"\n\n    @staticmethod\n    def decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n        Args:\n            encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n            key (str): Der symmetrische Schl\u00fcssel.\n            to_str (bool): default true returns str if false returns bytes\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n\n        if isinstance(key, str):\n            key = key.encode()\n\n        #try:\n        fernet = Fernet(key)\n        text_b = fernet.decrypt(encrypted_text)\n        if not to_str:\n            return text_b\n        return text_b.decode()\n        # except Exception as e:\n        #     get_logger().error(f\"Error decrypt_symmetric {e}\")\n        #     if not mute:\n        #         raise e\n        #     if not to_str:\n        #         return f\"Error decoding\".encode()\n        #     return f\"Error decoding\"\n\n    @staticmethod\n    def generate_asymmetric_keys() -&gt; (str, str):\n        \"\"\"\n        Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n        Args:\n            seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n        \"\"\"\n        private_key = rsa.generate_private_key(\n            public_exponent=65537,\n            key_size=2048 * 3,\n        )\n        public_key = private_key.public_key()\n\n        # Serialisieren der Schl\u00fcssel\n        pem_private_key = private_key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.NoEncryption()\n        ).decode()\n\n        pem_public_key = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        ).decode()\n\n        return pem_public_key, pem_private_key\n\n    @staticmethod\n    def save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n        \"\"\"\n        Speichert die generierten Schl\u00fcssel in separate Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n        Args:\n            public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n            private_key (str): Der private Schl\u00fcssel im PEM-Format\n            directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n        \"\"\"\n        # Erstelle das Verzeichnis, falls es nicht existiert\n        os.makedirs(directory, exist_ok=True)\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n        encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n        # Speichere den \u00f6ffentlichen Schl\u00fcssel\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        with open(public_key_path, \"w\") as f:\n            f.write(public_key)\n\n        # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n        with open(private_key_path, \"w\") as f:\n            f.write(encrypted_private_key)\n\n        print(\"Saved keys in \", public_key_path)\n\n    @staticmethod\n    def load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n        \"\"\"\n        L\u00e4dt die Schl\u00fcssel aus den Dateien.\n        Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n        Args:\n            directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n        Returns:\n            (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n        Raises:\n            FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n        \"\"\"\n        # Pfade zu den Schl\u00fcsseldateien\n        public_key_path = os.path.join(directory, \"public_key.pem\")\n        private_key_path = os.path.join(directory, \"private_key.pem\")\n\n        # Pr\u00fcfe ob die Dateien existieren\n        if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n            return \"\", \"\"\n\n        # Hole den Device Key\n        device_key = DEVICE_KEY()\n\n        # Lade den \u00f6ffentlichen Schl\u00fcssel\n        with open(public_key_path) as f:\n            public_key = f.read()\n\n        # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n        with open(private_key_path) as f:\n            encrypted_private_key = f.read()\n            private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n        return public_key, private_key\n\n    @staticmethod\n    def encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n        \"\"\"\n        Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n        Args:\n            text (str): Der zu verschl\u00fcsselnde Text.\n            public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n        Returns:\n            str: Der verschl\u00fcsselte Text.\n        \"\"\"\n        # try:\n        #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        #  except Exception as e:\n        #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            encrypted = public_key.encrypt(\n                text.encode(),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return encrypted.hex()\n        except Exception as e:\n            get_logger().error(f\"Error encrypt_asymmetric {e}\")\n            return \"Invalid\"\n\n    @staticmethod\n    def decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n        \"\"\"\n        Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n        Args:\n            encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n            private_key_str (str): Der private Schl\u00fcssel als String.\n\n        Returns:\n            str: Der entschl\u00fcsselte Text.\n        \"\"\"\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            decrypted = private_key.decrypt(\n                bytes.fromhex(encrypted_text_hex),\n                padding.OAEP(\n                    mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                    algorithm=hashes.SHA512(),\n                    label=None\n                )\n            )\n            return decrypted.decode()\n\n        except Exception as e:\n            get_logger().error(f\"Error decrypt_asymmetric {e}\")\n        return \"Invalid\"\n\n    @staticmethod\n    def verify_signature(signature: str or bytes, message: str or bytes, public_key_str: str,\n                         salt_length=padding.PSS.MAX_LENGTH) -&gt; bool:\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                padding=padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                algorithm=hashes.SHA512()\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def verify_signature_web_algo(signature: str or bytes, message: str or bytes, public_key_str: str,\n                                  algo: int = -512) -&gt; bool:\n        signature_algorithm = ECDSA(hashes.SHA512())\n        if algo != -512:\n            signature_algorithm = ECDSA(hashes.SHA256())\n\n        if isinstance(signature, str):\n            signature = signature.encode()\n        if isinstance(message, str):\n            message = message.encode()\n        try:\n            public_key = serialization.load_pem_public_key(public_key_str.encode())\n            public_key.verify(\n                signature=signature,\n                data=message,\n                # padding=padding.PSS(\n                #    mgf=padding.MGF1(hashes.SHA512()),\n                #    salt_length=padding.PSS.MAX_LENGTH\n                # ),\n                signature_algorithm=signature_algorithm\n            )\n            return True\n        except:\n            pass\n        return False\n\n    @staticmethod\n    def create_signature(message: str, private_key_str: str, salt_length=padding.PSS.MAX_LENGTH,\n                         row=False) -&gt; str or bytes:\n        try:\n            private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n            signature = private_key.sign(\n                message.encode(),\n                padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA512()),\n                    salt_length=salt_length\n                ),\n                hashes.SHA512()\n            )\n            if row:\n                return signature\n            return base64.b64encode(signature).decode()\n        except Exception as e:\n            get_logger().error(f\"Error create_signature {e}\")\n            print(e)\n        return \"Invalid Key\"\n\n    @staticmethod\n    def pem_to_public_key(pem_key: str):\n        \"\"\"\n        Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n        Args:\n            pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n        Returns:\n            PublicKey: Das PublicKey-Objekt.\n        \"\"\"\n        public_key = serialization.load_pem_public_key(pem_key.encode())\n        return public_key\n\n    @staticmethod\n    def public_key_to_pem(public_key: RSAPublicKey):\n        \"\"\"\n        Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n        Args:\n            public_key (PublicKey): Das PublicKey-Objekt.\n\n        Returns:\n            str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n        \"\"\"\n        pem = public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n        return pem.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.decrypt_asymmetric","title":"<code>decrypt_asymmetric(encrypted_text_hex, private_key_str)</code>  <code>staticmethod</code>","text":"<p>Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text_hex</code> <code>str</code> <p>Der verschl\u00fcsselte Text als Hex-String.</p> required <code>private_key_str</code> <code>str</code> <p>Der private Schl\u00fcssel als String.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_asymmetric(encrypted_text_hex: str, private_key_str: str) -&gt; str:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen privaten Schl\u00fcssel.\n\n    Args:\n        encrypted_text_hex (str): Der verschl\u00fcsselte Text als Hex-String.\n        private_key_str (str): Der private Schl\u00fcssel als String.\n\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n    try:\n        private_key = serialization.load_pem_private_key(private_key_str.encode(), password=None)\n        decrypted = private_key.decrypt(\n            bytes.fromhex(encrypted_text_hex),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return decrypted.decode()\n\n    except Exception as e:\n        get_logger().error(f\"Error decrypt_asymmetric {e}\")\n    return \"Invalid\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.decrypt_symmetric","title":"<code>decrypt_symmetric(encrypted_text, key, to_str=True, mute=False)</code>  <code>staticmethod</code>","text":"<p>Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_text</code> <code>str</code> <p>Der zu entschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <code>to_str</code> <code>bool</code> <p>default true returns str if false returns bytes</p> <code>True</code> <p>Returns:     str: Der entschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef decrypt_symmetric(encrypted_text: str, key: str, to_str=True, mute=False) -&gt; str or bytes:\n    \"\"\"\n    Entschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        encrypted_text (str): Der zu entschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n        to_str (bool): default true returns str if false returns bytes\n    Returns:\n        str: Der entschl\u00fcsselte Text.\n    \"\"\"\n\n    if isinstance(key, str):\n        key = key.encode()\n\n    #try:\n    fernet = Fernet(key)\n    text_b = fernet.decrypt(encrypted_text)\n    if not to_str:\n        return text_b\n    return text_b.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.encrypt_asymmetric","title":"<code>encrypt_asymmetric(text, public_key_str)</code>  <code>staticmethod</code>","text":"<p>Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>public_key_str</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_asymmetric(text: str, public_key_str: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen \u00f6ffentlichen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        public_key_str (str): Der \u00f6ffentliche Schl\u00fcssel als String oder im pem format.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    # try:\n    #    public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n    #  except Exception as e:\n    #     get_logger().error(f\"Error encrypt_asymmetric {e}\")\n    try:\n        public_key: RSAPublicKey = serialization.load_pem_public_key(public_key_str.encode())\n        encrypted = public_key.encrypt(\n            text.encode(),\n            padding.OAEP(\n                mgf=padding.MGF1(algorithm=hashes.SHA512()),\n                algorithm=hashes.SHA512(),\n                label=None\n            )\n        )\n        return encrypted.hex()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_asymmetric {e}\")\n        return \"Invalid\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.encrypt_symmetric","title":"<code>encrypt_symmetric(text, key)</code>  <code>staticmethod</code>","text":"<p>Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu verschl\u00fcsselnde Text.</p> required <code>key</code> <code>str</code> <p>Der symmetrische Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der verschl\u00fcsselte Text.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef encrypt_symmetric(text: str or bytes, key: str) -&gt; str:\n    \"\"\"\n    Verschl\u00fcsselt einen Text mit einem gegebenen symmetrischen Schl\u00fcssel.\n\n    Args:\n        text (str): Der zu verschl\u00fcsselnde Text.\n        key (str): Der symmetrische Schl\u00fcssel.\n\n    Returns:\n        str: Der verschl\u00fcsselte Text.\n    \"\"\"\n    if isinstance(text, str):\n        text = text.encode()\n\n    try:\n        fernet = Fernet(key.encode())\n        return fernet.encrypt(text).decode()\n    except Exception as e:\n        get_logger().error(f\"Error encrypt_symmetric #{str(e)}#\")\n        return \"Error encrypt\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.generate_asymmetric_keys","title":"<code>generate_asymmetric_keys()</code>  <code>staticmethod</code>","text":"<p>Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_asymmetric_keys() -&gt; (str, str):\n    \"\"\"\n    Generiert ein Paar von \u00f6ffentlichen und privaten Schl\u00fcsseln f\u00fcr die asymmetrische Verschl\u00fcsselung.\n\n    Args:\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel.\n    \"\"\"\n    private_key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048 * 3,\n    )\n    public_key = private_key.public_key()\n\n    # Serialisieren der Schl\u00fcssel\n    pem_private_key = private_key.private_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PrivateFormat.PKCS8,\n        encryption_algorithm=serialization.NoEncryption()\n    ).decode()\n\n    pem_public_key = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    ).decode()\n\n    return pem_public_key, pem_private_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.generate_seed","title":"<code>generate_seed()</code>  <code>staticmethod</code>","text":"<p>Erzeugt eine zuf\u00e4llige Zahl als Seed.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Eine zuf\u00e4llige Zahl.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_seed() -&gt; int:\n    \"\"\"\n    Erzeugt eine zuf\u00e4llige Zahl als Seed.\n\n    Returns:\n        int: Eine zuf\u00e4llige Zahl.\n    \"\"\"\n    return random.randint(2 ** 32 - 1, 2 ** 64 - 1)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.generate_symmetric_key","title":"<code>generate_symmetric_key(as_str=True)</code>  <code>staticmethod</code>","text":"<p>Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str or bytes</code> <p>Der generierte Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef generate_symmetric_key(as_str=True) -&gt; str or bytes:\n    \"\"\"\n    Generiert einen Schl\u00fcssel f\u00fcr die symmetrische Verschl\u00fcsselung.\n\n    Returns:\n        str: Der generierte Schl\u00fcssel.\n    \"\"\"\n    key = Fernet.generate_key()\n    if as_str:\n        key = key.decode()\n    return key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.load_keys_from_files","title":"<code>load_keys_from_files(directory='keys')</code>  <code>staticmethod</code>","text":"<p>L\u00e4dt die Schl\u00fcssel aus den Dateien. Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen</p> <code>'keys'</code> <p>Returns:</p> Type Description <code>(str, str)</code> <p>Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef load_keys_from_files(directory: str = \"keys\") -&gt; (str, str):\n    \"\"\"\n    L\u00e4dt die Schl\u00fcssel aus den Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key entschl\u00fcsselt.\n\n    Args:\n        directory (str): Das Verzeichnis, aus dem die Schl\u00fcssel geladen werden sollen\n\n    Returns:\n        (str, str): Ein Tupel aus \u00f6ffentlichem und privatem Schl\u00fcssel\n\n    Raises:\n        FileNotFoundError: Wenn die Schl\u00fcsseldateien nicht gefunden werden k\u00f6nnen\n    \"\"\"\n    # Pfade zu den Schl\u00fcsseldateien\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n\n    # Pr\u00fcfe ob die Dateien existieren\n    if not os.path.exists(public_key_path) or not os.path.exists(private_key_path):\n        return \"\", \"\"\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Lade den \u00f6ffentlichen Schl\u00fcssel\n    with open(public_key_path) as f:\n        public_key = f.read()\n\n    # Lade und entschl\u00fcssele den privaten Schl\u00fcssel\n    with open(private_key_path) as f:\n        encrypted_private_key = f.read()\n        private_key = Code.decrypt_symmetric(encrypted_private_key, device_key)\n\n    return public_key, private_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.one_way_hash","title":"<code>one_way_hash(text, salt='', pepper='')</code>  <code>staticmethod</code>","text":"<p>Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Der zu hashende Text.</p> required <code>salt</code> <code>str</code> <p>Der Salt-Wert.</p> <code>''</code> <code>pepper</code> <code>str</code> <p>Der Pepper-Wert.</p> <code>''</code> <code>seed</code> <code>int</code> <p>Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Der resultierende Hash-Wert.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef one_way_hash(text: str, salt: str = '', pepper: str = '') -&gt; str:\n    \"\"\"\n    Erzeugt einen Hash eines gegebenen Textes mit Salt, Pepper und optional einem Seed.\n\n    Args:\n        text (str): Der zu hashende Text.\n        salt (str): Der Salt-Wert.\n        pepper (str): Der Pepper-Wert.\n        seed (int, optional): Ein optionaler Seed-Wert. Standardm\u00e4\u00dfig None.\n\n    Returns:\n        str: Der resultierende Hash-Wert.\n    \"\"\"\n    return hashlib.sha256((salt + text + pepper).encode()).hexdigest()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.pem_to_public_key","title":"<code>pem_to_public_key(pem_key)</code>  <code>staticmethod</code>","text":"<p>Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.</p> <p>Parameters:</p> Name Type Description Default <code>pem_key</code> <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> required <p>Returns:</p> Name Type Description <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef pem_to_public_key(pem_key: str):\n    \"\"\"\n    Konvertiert einen PEM-kodierten \u00f6ffentlichen Schl\u00fcssel in ein PublicKey-Objekt.\n\n    Args:\n        pem_key (str): Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n\n    Returns:\n        PublicKey: Das PublicKey-Objekt.\n    \"\"\"\n    public_key = serialization.load_pem_public_key(pem_key.encode())\n    return public_key\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.public_key_to_pem","title":"<code>public_key_to_pem(public_key)</code>  <code>staticmethod</code>","text":"<p>Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>PublicKey</code> <p>Das PublicKey-Objekt.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.</p> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef public_key_to_pem(public_key: RSAPublicKey):\n    \"\"\"\n    Konvertiert ein PublicKey-Objekt in einen PEM-kodierten String.\n\n    Args:\n        public_key (PublicKey): Das PublicKey-Objekt.\n\n    Returns:\n        str: Der PEM-kodierte \u00f6ffentliche Schl\u00fcssel.\n    \"\"\"\n    pem = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    )\n    return pem.decode()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.Code.save_keys_to_files","title":"<code>save_keys_to_files(public_key, private_key, directory='keys')</code>  <code>staticmethod</code>","text":"<p>Speichert die generierten Schl\u00fcssel in separate Dateien. Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.</p> <p>Parameters:</p> Name Type Description Default <code>public_key</code> <code>str</code> <p>Der \u00f6ffentliche Schl\u00fcssel im PEM-Format</p> required <code>private_key</code> <code>str</code> <p>Der private Schl\u00fcssel im PEM-Format</p> required <code>directory</code> <code>str</code> <p>Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen</p> <code>'keys'</code> Source code in <code>toolboxv2/utils/security/cryp.py</code> <pre><code>@staticmethod\ndef save_keys_to_files(public_key: str, private_key: str, directory: str = \"keys\") -&gt; None:\n    \"\"\"\n    Speichert die generierten Schl\u00fcssel in separate Dateien.\n    Der private Schl\u00fcssel wird mit dem Device Key verschl\u00fcsselt.\n\n    Args:\n        public_key (str): Der \u00f6ffentliche Schl\u00fcssel im PEM-Format\n        private_key (str): Der private Schl\u00fcssel im PEM-Format\n        directory (str): Das Verzeichnis, in dem die Schl\u00fcssel gespeichert werden sollen\n    \"\"\"\n    # Erstelle das Verzeichnis, falls es nicht existiert\n    os.makedirs(directory, exist_ok=True)\n\n    # Hole den Device Key\n    device_key = DEVICE_KEY()\n\n    # Verschl\u00fcssele den privaten Schl\u00fcssel mit dem Device Key\n    encrypted_private_key = Code.encrypt_symmetric(private_key, device_key)\n\n    # Speichere den \u00f6ffentlichen Schl\u00fcssel\n    public_key_path = os.path.join(directory, \"public_key.pem\")\n    with open(public_key_path, \"w\") as f:\n        f.write(public_key)\n\n    # Speichere den verschl\u00fcsselten privaten Schl\u00fcssel\n    private_key_path = os.path.join(directory, \"private_key.pem\")\n    with open(private_key_path, \"w\") as f:\n        f.write(encrypted_private_key)\n\n    print(\"Saved keys in \", public_key_path)\n</code></pre>"},{"location":"toolboxv2/#modules-flows","title":"Modules &amp; Flows","text":""},{"location":"toolboxv2/#toolboxv2.mods","title":"<code>toolboxv2.mods</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.Canvas","title":"<code>Canvas</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.Canvas.Tools","title":"<code>Tools</code>","text":"<p>               Bases: <code>MainTool</code></p> Source code in <code>toolboxv2/mods/Canvas.py</code> <pre><code>class Tools(MainTool):  # Removed EventManager for simplicity, as it was causing the issue. Direct SSE is better here.\n    def __init__(self, app: App):\n        self.name = MOD_NAME\n        self.version = VERSION\n        self.color = \"GREEN\"\n        self.tools_dict = {\"name\": MOD_NAME, \"Version\": self.show_version}\n\n        # Canvas specific state\n        self.live_canvas_sessions: Dict[str, List[asyncio.Queue]] = defaultdict(list)\n        self.active_user_previews: Dict[str, Dict[str, Any]] = defaultdict(dict)\n        self.previews_lock = asyncio.Lock()\n\n        MainTool.__init__(self, load=on_start, v=self.version, tool=self.tools_dict, name=self.name,\n                          color=self.color, app=app)\n        self.app.logger.info(f\"Canvas Tools (v{self.version}) initialized for app {self.app.id}.\")\n\n    @property\n    def db_mod(self):\n        db = self.app.get_mod(\"DB\", spec=Name)\n        if db.mode.value != \"CLUSTER_BLOB\":\n            db.edit_cli(\"CB\")\n        return db\n\n    def _broadcast_to_canvas_listeners(self, canvas_id: str, event_type: str, data: Dict[str, Any],\n                                       originator_user_id: Optional[str] = None):\n        \"\"\"\n        Creates a broadcast coroutine and submits it to the app's dedicated\n        async manager to be run in the background.\n        This is now a non-blocking fire-and-forget operation.\n        \"\"\"\n\n        async def broadcast_coro():\n            if canvas_id not in self.live_canvas_sessions:\n                return\n\n            message_obj = {\n                \"event\": event_type,\n                \"data\": json.dumps({\n                    \"canvas_id\": canvas_id,\n                    \"originator_user_id\": originator_user_id,\n                    **data\n                })\n            }\n\n            listeners = list(self.live_canvas_sessions.get(canvas_id, []))\n\n            for q in listeners:\n                try:\n                    # Non-blocking put. If the queue is full, the client is lagging,\n                    # and it's better to drop a message than to block the server.\n                    q.put_nowait(message_obj)\n                except asyncio.QueueFull:\n                    self.app.logger.warning(\n                        f\"SSE queue full for canvas {canvas_id}. Message '{event_type}' dropped for one client.\")\n                except Exception as e:\n                    self.app.logger.error(f\"Error putting message on SSE queue: {e}\")\n\n        # Use the app's robust background runner to execute immediately and not block the caller.\n        self.app.run_bg_task(broadcast_coro)\n\n    def show_version(self):\n        self.app.logger.info(f\"{self.name} Version: {self.version}\")\n        return self.version\n\n    async def _get_user_specific_db_key(self, request: RequestData, base_key: str) -&gt; Optional[str]:\n        # This logic is correct and can remain as is.\n\n        user = await get_user_from_request(self.app, request)\n        if user and user.uid:\n            return f\"{base_key}_{user.uid}\"\n        self.print(\"ok\")\n        # Fallback for public/guest access if you want to support it\n        return f\"{base_key}_public\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.Canvas.handle_send_canvas_action","title":"<code>handle_send_canvas_action(app, request, data)</code>  <code>async</code>","text":"<p>Handles incremental, real-time actions from clients (e.g., adding an element). It persists the change to the database and then broadcasts it to all live listeners.</p> Source code in <code>toolboxv2/mods/Canvas.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, name=\"send_canvas_action\", api_methods=['POST'],\n        request_as_kwarg=True)\nasync def handle_send_canvas_action(app: App, request: RequestData, data: Dict[str, Any]):\n    \"\"\"\n    Handles incremental, real-time actions from clients (e.g., adding an element).\n    It persists the change to the database and then broadcasts it to all live listeners.\n    \"\"\"\n    canvas_tool = app.get_mod(MOD_NAME)\n    if not canvas_tool or not canvas_tool.db_mod:\n        return Result.default_internal_error(\"Canvas module or DB not loaded.\")\n\n    if not data:\n        return Result.default_user_error(\"Request data is missing.\", 400)\n\n    canvas_id = data.get(\"canvas_id\")\n    action_type = data.get(\"action_type\")\n    action_payload = data.get(\"payload\")\n    user_id = data.get(\"user_id\")\n\n    if not all([canvas_id, action_type, user_id]) or action_payload is None:\n        return Result.default_user_error(\"Request missing required fields.\", 400)\n\n    # --- Flow 1: Ephemeral 'preview' actions that DO NOT get persisted ---\n    if action_type in [\"preview_update\", \"preview_clear\"]:\n        sse_event_type = \"user_preview_update\" if action_type == \"preview_update\" else \"clear_user_preview\"\n        sse_data = {\"user_id\": user_id}\n\n        async with canvas_tool.previews_lock:\n            if action_type == \"preview_update\":\n                canvas_tool.active_user_previews[canvas_id][user_id] = action_payload\n                sse_data[\"preview_data\"] = action_payload\n            elif user_id in canvas_tool.active_user_previews.get(canvas_id, {}):\n                del canvas_tool.active_user_previews[canvas_id][user_id]\n\n        # MODIFICATION: Call the non-blocking broadcast method. This returns immediately.\n        canvas_tool._broadcast_to_canvas_listeners(\n            canvas_id=canvas_id, event_type=sse_event_type,\n            data=sse_data, originator_user_id=user_id\n        )\n        return Result.ok(info=f\"'{action_type}' broadcasted.\")\n\n    # --- Flow 2: Persistent actions that modify the canvas state ---\n    if action_type not in [\"element_add\", \"element_update\", \"element_remove\"]:\n        return Result.default_user_error(f\"Unknown persistent action_type: {action_type}\", 400)\n\n    # Load the full, current session state from the database\n    user_db_key_base = await canvas_tool._get_user_specific_db_key(request, SESSION_DATA_PREFIX)\n    session_db_key = f\"{user_db_key_base}_{canvas_id}\"\n    try:\n        db_result = canvas_tool.db_mod.get(session_db_key)\n        if not db_result or db_result.is_error() or not db_result.get():\n            return Result.default_user_error(\"Canvas session not found in database.\", 404)\n\n        session_data_str = db_result.get()[0] if isinstance(db_result.get(), list) else db_result.get()\n        session_data = IdeaSessionData.model_validate_json(session_data_str)\n    except Exception as e:\n        app.logger.error(f\"DB Load/Parse failed for C:{canvas_id}. Error: {e}\", exc_info=True)\n        return Result.default_internal_error(\"Could not load canvas data to apply changes.\")\n\n    # Apply the action to the in-memory Pydantic object\n    if action_type == \"element_add\":\n        session_data.canvas_elements.append(CanvasElement(**action_payload))\n    elif action_type == \"element_update\":\n        element_id = action_payload.get(\"id\")\n        for i, el in enumerate(session_data.canvas_elements):\n            if el.id == element_id:\n                session_data.canvas_elements[i] = el.model_copy(update=action_payload)\n                break\n    elif action_type == \"element_remove\":\n        ids_to_remove = set(action_payload.get(\"ids\", [action_payload.get(\"id\")]))\n        session_data.canvas_elements = [el for el in session_data.canvas_elements if el.id not in ids_to_remove]\n\n    # Save the modified object back to the database\n    session_data.last_modified = datetime.now(timezone.utc).timestamp()\n    canvas_tool.db_mod.set(session_db_key, session_data.model_dump_json(exclude_none=True))\n\n    # Broadcast the successful, persisted action to all connected clients\n    # MODIFICATION: Call the non-blocking broadcast method.\n    canvas_tool._broadcast_to_canvas_listeners(\n        canvas_id=canvas_id,\n        event_type=\"canvas_elements_changed\",\n        data={\"action\": action_type, \"element\": action_payload},\n        originator_user_id=user_id\n    )\n\n    # Clear the temporary preview of the user who made the change\n    async with canvas_tool.previews_lock:\n        if user_id in canvas_tool.active_user_previews.get(canvas_id, {}):\n            del canvas_tool.active_user_previews[canvas_id][user_id]\n\n    # MODIFICATION: Call the non-blocking broadcast method.\n    canvas_tool._broadcast_to_canvas_listeners(\n        canvas_id=canvas_id, event_type=\"clear_user_preview\",\n        data={\"user_id\": user_id}, originator_user_id=user_id\n    )\n\n    return Result.ok(info=f\"Action '{action_type}' persisted and broadcast.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.Canvas.markdown_to_svg","title":"<code>markdown_to_svg(self, request, markdown_text='', width=400, font_family='sans-serif', font_size=14, bg_color='#ffffff', text_color='#000000')</code>  <code>async</code>","text":"<p>Converts a string of Markdown text into an SVG image. The SVG is returned as a base64 encoded data URL. This version uses a viewBox for better scalability and multi-line handling.</p> Source code in <code>toolboxv2/mods/Canvas.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, name=\"markdown_to_svg\", api_methods=['POST'],\n        request_as_kwarg=True)\nasync def markdown_to_svg(self, request: RequestData, markdown_text: str = \"\", width: int = 400,\n                          font_family: str = \"sans-serif\", font_size: int = 14,\n                          bg_color: str = \"#ffffff\", text_color: str = \"#000000\") -&gt; Result:\n    \"\"\"\n    Converts a string of Markdown text into an SVG image.\n    The SVG is returned as a base64 encoded data URL.\n    This version uses a viewBox for better scalability and multi-line handling.\n    \"\"\"\n    if request is None:\n        return Result.default_user_error(\"Request data is missing.\", 400)\n    if not markdown_text and request.data:\n        markdown_text = request.data.get(\"markdown_text\", \"\")\n\n    if not markdown_text:\n        return Result.default_user_error(\"markdown_text cannot be empty.\")\n\n    try:\n        # Convert Markdown to HTML\n        html_content = markdown2.markdown(markdown_text, extras=[\"fenced-code-blocks\", \"tables\", \"strike\"])\n\n        # --- FIX for Multi-line text ---\n        # The key is to NOT set a fixed height on the SVG itself, but to use a viewBox.\n        # The client will determine the final rendered size.\n        # The width of the div inside the foreignObject controls the line wrapping.\n\n        # We still need a rough height for the viewBox.\n        # Estimate height: (number of lines * line-height) + padding\n        # A simple line-height estimate is font_size * 1.6\n        line_height_estimate = font_size * 1.6\n        num_lines_estimate = len(html_content.split('\\n')) + html_content.count('&lt;br') + html_content.count(\n            '&lt;p&gt;') + html_content.count('&lt;li&gt;')\n        estimated_height = (num_lines_estimate * line_height_estimate) + 40  # 20px top/bottom padding\n\n        svg_template = f\"\"\"\n        &lt;svg viewBox=\"0 0 {width} {int(estimated_height)}\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\n            &lt;foreignObject x=\"0\" y=\"0\" width=\"{width}\" height=\"{int(estimated_height)}\"&gt;\n                &lt;div xmlns=\"http://www.w3.org/1999/xhtml\"&gt;\n                    &lt;style&gt;\n                        div {{\n                            font-family: {font_family};\n                            font-size: {font_size}px;\n                            color: {text_color};\n                            background-color: {bg_color};\n                            padding: 10px;\n                            border-radius: 5px;\n                            line-height: 1.6;\n                            width: {width - 20}px; /* Width minus padding */\n                            word-wrap: break-word;\n                            height: 100%;\n                            overflow-y: auto; /* Allow scrolling if content overflows estimate */\n                        }}\n                        h1, h2, h3 {{ border-bottom: 1px solid #ccc; padding-bottom: 5px; margin-top: 1em; }}\n                        pre {{ background-color: #f0f0f0; padding: 10px; border-radius: 4px; overflow-x: auto; }}\n                        code {{ font-family: monospace; }}\n                        table {{ border-collapse: collapse; width: 100%; }}\n                        th, td {{ border: 1px solid #ddd; padding: 8px; }}\n                        th {{ background-color: #f2f2f2; }}\n                        blockquote {{ border-left: 4px solid #ccc; padding-left: 10px; color: #555; margin-left: 0; }}\n                    &lt;/style&gt;\n                    {html_content}\n                &lt;/div&gt;\n            &lt;/foreignObject&gt;\n        &lt;/svg&gt;\n        \"\"\"\n\n        svg_base64 = base64.b64encode(svg_template.encode('utf-8')).decode('utf-8')\n        data_url = f\"data:image/svg+xml;base64,{svg_base64}\"\n\n        # --- FIX for Editability ---\n        # Return the original markdown text along with the SVG\n        return Result.ok(data={\"svg_data_url\": data_url, \"original_markdown\": markdown_text})\n\n    except Exception as e:\n        self.app.logger.error(f\"Error converting Markdown to SVG: {e}\", exc_info=True)\n        return Result.default_internal_error(\"Failed to convert Markdown to SVG.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.Canvas.save_session","title":"<code>save_session(app, request, data)</code>  <code>async</code>","text":"<p>Saves the entire state of a canvas session to the database. This is typically triggered by a user's explicit \"Save\" action.</p> Source code in <code>toolboxv2/mods/Canvas.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, name=\"save_session\", api_methods=['POST'], request_as_kwarg=True)\nasync def save_session(app: App, request: RequestData, data: Union[Dict[str, Any], IdeaSessionData]) -&gt; Result:\n    \"\"\"\n    Saves the entire state of a canvas session to the database.\n    This is typically triggered by a user's explicit \"Save\" action.\n    \"\"\"\n    if not data:\n        return Result.default_user_error(\"Request data is missing.\", 400)\n    if request is None:\n        return Result.default_user_error(\"Request data is missing.\", 400)\n    canvas_tool = app.get_mod(MOD_NAME)\n    if not canvas_tool or not canvas_tool.db_mod:\n        app.logger.error(\"Save failed: Canvas module or DB not available.\")\n        return Result.custom_error(info=\"Database module not available.\", exec_code=503)\n\n    user_db_key_base = await canvas_tool._get_user_specific_db_key(request, SESSION_DATA_PREFIX)\n    if not user_db_key_base:\n        return Result.default_user_error(info=\"User authentication required to save.\", exec_code=401)\n\n    try:\n        # Validate the incoming data against the Pydantic model\n        session_data_obj = IdeaSessionData(**data) if isinstance(data, dict) else data\n    except Exception as e:\n        app.logger.error(f\"Invalid session data for save: {e}. Data: {str(data)[:500]}\", exc_info=True)\n        return Result.default_user_error(info=f\"Invalid session data format: {e}\", exec_code=400)\n\n    # Update timestamp and construct the main session key\n    if session_data_obj:\n        session_data_obj.last_modified = datetime.now(timezone.utc).timestamp()\n    session_db_key = f\"{user_db_key_base}_{session_data_obj.id}\"\n\n    # Save the full session object to the database\n    canvas_tool.db_mod.set(session_db_key, session_data_obj.model_dump_json(exclude_none=True))\n    app.logger.info(f\"Saved session data for C:{session_data_obj.id}\")\n\n    # --- Update the session list metadata ---\n    session_list_key = f\"{user_db_key_base}{SESSION_LIST_KEY_SUFFIX}\"\n    try:\n        list_res_obj = canvas_tool.db_mod.get(session_list_key)\n        user_sessions = []\n        if list_res_obj and not list_res_obj.is_error() and list_res_obj.get():\n            list_content = list_res_obj.get()[0] if isinstance(list_res_obj.get(), list) else list_res_obj.get()\n            user_sessions = json.loads(list_content)\n\n        # Find and update the existing entry, or add a new one\n        session_metadata = {\n            \"id\": session_data_obj.id,\n            \"name\": session_data_obj.name,\n            \"last_modified\": session_data_obj.last_modified\n        }\n        found_in_list = False\n        for i, sess_meta in enumerate(user_sessions):\n            if sess_meta.get(\"id\") == session_data_obj.id:\n                user_sessions[i] = session_metadata\n                found_in_list = True\n                break\n        if not found_in_list:\n            user_sessions.append(session_metadata)\n\n        canvas_tool.db_mod.set(session_list_key, json.dumps(user_sessions))\n        app.logger.info(f\"Updated session list for user key ending in ...{user_db_key_base[-12:]}\")\n\n    except Exception as e:\n        app.logger.error(f\"Failed to update session list for C:{session_data_obj.id}. Error: {e}\", exc_info=True)\n        # Non-fatal error; the main data was saved. We can continue.\n\n    return Result.ok(\n        info=\"Session saved successfully.\",\n        data={\"id\": session_data_obj.id, \"last_modified\": session_data_obj.last_modified}\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM","title":"<code>CloudM</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.check_multiple_processes","title":"<code>check_multiple_processes(pids)</code>","text":"<p>Checks the status of multiple processes in a single system call. Returns a dictionary mapping PIDs to their status (GREEN_CIRCLE, RED_CIRCLE, or YELLOW_CIRCLE).</p> Source code in <code>toolboxv2/mods/CloudM/mini.py</code> <pre><code>def check_multiple_processes(pids: list[int]) -&gt; dict[int, str]:\n    \"\"\"\n    Checks the status of multiple processes in a single system call.\n    Returns a dictionary mapping PIDs to their status (GREEN_CIRCLE, RED_CIRCLE, or YELLOW_CIRCLE).\n    \"\"\"\n    if not pids:\n        return {}\n\n    pid_status = {}\n\n    if os.name == 'nt':  # Windows\n        try:\n            # Windows tasklist requires separate /FI for each filter\n            command = 'tasklist'\n\n            # Add encoding handling for Windows\n            result = subprocess.run(\n                command,\n                capture_output=True,\n                text=True,\n                shell=True,\n                encoding='cp850'  # Use cp850 for Windows console output\n            )\n            # Create a set of running PIDs from the output\n            running_pids = set()\n            for line in result.stdout.lower().split('\\n'):\n                for pid in pids:\n                    if str(pid) in line:\n                        running_pids.add(pid)\n            # Assign status based on whether PID was found in output\n            for pid in pids:\n                if pid in running_pids:\n                    pid_status[pid] = GREEN_CIRCLE\n                else:\n                    pid_status[pid] = RED_CIRCLE\n\n        except subprocess.SubprocessError as e:\n            print(f\"SubprocessError: {e}\")  # For debugging\n            # Mark all as YELLOW_CIRCLE if there's an error running the command\n            for pid in pids:\n                pid_status[pid] = YELLOW_CIRCLE\n        except UnicodeDecodeError as e:\n            print(f\"UnicodeDecodeError: {e}\")  # For debugging\n            # Try alternate encoding if cp850 fails\n            try:\n                result = subprocess.run(\n                    command,\n                    capture_output=True,\n                    text=True,\n                    shell=True,\n                    encoding='utf-8'\n                )\n                running_pids = set()\n                for line in result.stdout.lower().split('\\n'):\n                    for pid in pids:\n                        if str(pid) in line:\n                            running_pids.add(pid)\n\n                for pid in pids:\n                    pid_status[pid] = GREEN_CIRCLE if pid in running_pids else RED_CIRCLE\n            except Exception as e:\n                print(f\"Failed with alternate encoding: {e}\")  # For debugging\n                for pid in pids:\n                    pid_status[pid] = YELLOW_CIRCLE\n\n    else:  # Unix/Linux/Mac\n        try:\n            pids_str = ','.join(str(pid) for pid in pids)\n            command = f'ps -p {pids_str} -o pid='\n\n            result = subprocess.run(\n                command,\n                capture_output=True,\n                text=True,\n                shell=True,\n                encoding='utf-8'\n            )\n            running_pids = set(int(pid) for pid in result.stdout.strip().split())\n\n            for pid in pids:\n                pid_status[pid] = GREEN_CIRCLE if pid in running_pids else RED_CIRCLE\n\n        except subprocess.SubprocessError as e:\n            print(f\"SubprocessError: {e}\")  # For debugging\n            for pid in pids:\n                pid_status[pid] = YELLOW_CIRCLE\n\n    return pid_status\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.get_service_pids","title":"<code>get_service_pids(info_dir)</code>","text":"<p>Extracts service names and PIDs from pid files.</p> Source code in <code>toolboxv2/mods/CloudM/mini.py</code> <pre><code>def get_service_pids(info_dir):\n    \"\"\"Extracts service names and PIDs from pid files.\"\"\"\n    services = {}\n    pid_files = [f for f in os.listdir(info_dir) if re.match(r'(.+)-(.+)\\.pid', f)]\n    for pid_file in pid_files:\n        match = re.match(r'(.+)-(.+)\\.pid', pid_file)\n        if match:\n            services_type, service_name = match.groups()\n            # Read the PID from the file\n            with open(os.path.join(info_dir, pid_file)) as file:\n                pid = file.read().strip()\n                # Store the PID using a formatted key\n                services[f\"{service_name} - {services_type}\"] = int(pid)\n    return services\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.get_service_status","title":"<code>get_service_status(dir)</code>","text":"<p>Displays the status of all services.</p> Source code in <code>toolboxv2/mods/CloudM/mini.py</code> <pre><code>def get_service_status(dir: str) -&gt; str:\n    \"\"\"Displays the status of all services.\"\"\"\n    if time.time()-services_data_sto_last_update_time[0] &gt; 30:\n        services = get_service_pids(dir)\n        services_data_sto[0] = services\n        services_data_sto_last_update_time[0] = time.time()\n    else:\n        services = services_data_sto[0]\n    if not services:\n        return \"No services found\"\n\n    # Get status for all PIDs in a single call\n    pid_statuses = check_multiple_processes(list(services.values()))\n\n    # Build the status string\n    res_s = \"Service(s):\" + (\"\\n\" if len(services) &gt; 1 else ' ')\n    for service_name, pid in services.items():\n        status = pid_statuses.get(pid, YELLOW_CIRCLE)\n        res_s += f\"{status} {service_name} (PID: {pid})\\n\"\n    services_data_display[0] = res_s.strip()\n    return res_s.rstrip()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.AuthManager","title":"<code>AuthManager</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.AuthManager.delete_user","title":"<code>delete_user(app, username)</code>","text":"<p>Deletes a user and all their data.</p> Source code in <code>toolboxv2/mods/CloudM/AuthManager.py</code> <pre><code>@export(mod_name=Name, state=True, test=False, interface=ToolBoxInterfaces.native)\ndef delete_user(app: App, username: str):\n    \"\"\"Deletes a user and all their data.\"\"\"\n    if not db_helper_test_exist(app, username):\n        return Result.default_user_error(f\"User '{username}' not found.\")\n\n    # This will delete all entries matching the user\n    result = db_helper_delete_user(app, username, '*', matching=True)\n\n    if result.is_ok():\n        # Also remove the local private key file if it exists\n        app.config_fh.remove_key_file_handler(\"Pk\" + Code.one_way_hash(username, \"dvp-k\")[:8])\n        return Result.ok(f\"User '{username}' deleted successfully.\")\n    else:\n        return Result.default_internal_error(f\"Failed to delete user '{username}'.\", data=result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.AuthManager.list_users","title":"<code>list_users(app)</code>","text":"<p>Lists all registered users.</p> Source code in <code>toolboxv2/mods/CloudM/AuthManager.py</code> <pre><code>@export(mod_name=Name, state=True, test=False, interface=ToolBoxInterfaces.native)\ndef list_users(app: App):\n    \"\"\"Lists all registered users.\"\"\"\n    keys_result = app.run_any(TBEF.DB.GET, query=\"USER::*::*\", get_results=True)\n    if keys_result.is_error():\n        return keys_result\n\n    user_keys = keys_result.get()\n    if not user_keys:\n        return Result.ok(\"No users found.\")\n\n    users = []\n    for key in user_keys:\n        if isinstance(key, bytes):\n            key = key.decode()\n        # Extract username from the key USER::username::uid\n        parts = key.split('::')\n        if len(parts) &gt; 1 and parts[1] not in [u['username'] for u in users]:\n            user_res = get_user_by_name(app, parts[1])\n            if user_res.is_ok():\n                user_data = user_res.get()\n                users.append({\"username\": user_data.name, \"email\": user_data.email, \"level\": user_data.level})\n\n    return Result.ok(data=users)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager","title":"<code>ModManager</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.create_and_pack_module","title":"<code>create_and_pack_module(path, module_name='', version='-.-.-', additional_dirs=None, yaml_data=None)</code>","text":"<p>Erstellt ein Python-Modul und packt es in eine ZIP-Datei.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Pfad zum Ordner oder zur Datei, die in das Modul aufgenommen werden soll.</p> required <code>additional_dirs</code> <code>dict</code> <p>Zus\u00e4tzliche Verzeichnisse, die hinzugef\u00fcgt werden sollen.</p> <code>None</code> <code>version</code> <code>str</code> <p>Version des Moduls.</p> <code>'-.-.-'</code> <code>module_name</code> <code>str</code> <p>Name des Moduls.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <p>Pfad zur erstellten ZIP-Datei.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def create_and_pack_module(path, module_name='', version='-.-.-', additional_dirs=None, yaml_data=None):\n    \"\"\"\n    Erstellt ein Python-Modul und packt es in eine ZIP-Datei.\n\n    Args:\n        path (str): Pfad zum Ordner oder zur Datei, die in das Modul aufgenommen werden soll.\n        additional_dirs (dict): Zus\u00e4tzliche Verzeichnisse, die hinzugef\u00fcgt werden sollen.\n        version (str): Version des Moduls.\n        module_name (str): Name des Moduls.\n\n    Returns:\n        str: Pfad zur erstellten ZIP-Datei.\n    \"\"\"\n    if additional_dirs is None:\n        additional_dirs = {}\n    if yaml_data is None:\n        yaml_data = {}\n\n    os.makedirs(\"./mods_sto/temp/\", exist_ok=True)\n\n    module_path = os.path.join(path, module_name)\n    print(\"module_pathmodule_pathmodule_path\", module_path)\n    if not os.path.exists(module_path):\n        module_path += '.py'\n\n    temp_dir = tempfile.mkdtemp(dir=os.path.join(\"./mods_sto\", \"temp\"))\n    zip_file_name = f\"RST${module_name}&amp;{__version__}\u00a7{version}.zip\"\n    zip_path = f\"./mods_sto/{zip_file_name}\"\n\n    # Modulverzeichnis erstellen, falls es nicht existiert\n    if not os.path.exists(module_path):\n        return False\n\n    if os.path.isdir(module_path):\n        # tbConfig.yaml erstellen\n        config_path = os.path.join(module_path, \"tbConfig.yaml\")\n        with open(config_path, 'w') as config_file:\n            yaml.dump({\"version\": version, \"module_name\": module_name,\n                       \"dependencies_file\": f\"./mods/{module_name}/requirements.txt\",\n                       \"zip\": zip_file_name, **yaml_data}, config_file)\n\n        generate_requirements(module_path, os.path.join(module_path, \"requirements.txt\"))\n    # Datei oder Ordner in das Modulverzeichnis kopieren\n    if os.path.isdir(module_path):\n        shutil.copytree(module_path, os.path.join(temp_dir, os.path.basename(module_path)), dirs_exist_ok=True)\n    else:\n        shutil.copy2(module_path, temp_dir)\n        config_path = os.path.join(temp_dir, f\"{module_name}.yaml\")\n        with open(config_path, 'w') as config_file:\n            yaml.dump({\"version\": version, \"dependencies_file\": f\"./mods/{module_name}/requirements.txt\",\n                       \"module_name\": module_name, **yaml_data}, config_file)\n        generate_requirements(temp_dir, os.path.join(temp_dir, \"requirements.txt\"))\n    # Zus\u00e4tzliche Verzeichnisse hinzuf\u00fcgen\n    for dir_name, dir_paths in additional_dirs.items():\n        if isinstance(dir_paths, str):\n            dir_paths = [dir_paths]\n        for dir_path in dir_paths:\n            full_path = os.path.join(temp_dir, dir_name)\n            if os.path.isdir(dir_path):\n                shutil.copytree(dir_path, full_path, dirs_exist_ok=True)\n            elif os.path.isfile(dir_path):\n                # Stellen Sie sicher, dass das Zielverzeichnis existiert\n                os.makedirs(full_path, exist_ok=True)\n                # Kopieren Sie die Datei statt des Verzeichnisses\n                shutil.copy2(dir_path, full_path)\n            else:\n                print(f\"Der Pfad {dir_path} ist weder ein Verzeichnis noch eine Datei.\")\n\n    # Modul in eine ZIP-Datei packen\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, _dirs, files in os.walk(temp_dir):\n            for file in files:\n                file_path = os.path.join(root, file)\n                zipf.write(file_path, os.path.relpath(file_path, temp_dir))\n\n    # Temperatures Modulverzeichnis l\u00f6schen\n    shutil.rmtree(temp_dir)\n\n    return zip_path\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.download_files","title":"<code>download_files(urls, directory, desc, print_func, filename=None)</code>","text":"<p>Hilfsfunktion zum Herunterladen von Dateien.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def download_files(urls, directory, desc, print_func, filename=None):\n    \"\"\" Hilfsfunktion zum Herunterladen von Dateien. \"\"\"\n    for url in tqdm(urls, desc=desc):\n        if filename is None:\n            filename = os.path.basename(url)\n        print_func(f\"Download {filename}\")\n        print_func(f\"{url} -&gt; {directory}/{filename}\")\n        os.makedirs(directory, exist_ok=True)\n        urllib.request.urlretrieve(url, f\"{directory}/{filename}\")\n    return f\"{directory}/{filename}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.handle_requirements","title":"<code>handle_requirements(requirements_url, module_name, print_func)</code>","text":"<p>Verarbeitet und installiert Requirements.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def handle_requirements(requirements_url, module_name, print_func):\n    \"\"\" Verarbeitet und installiert Requirements. \"\"\"\n    if requirements_url:\n        requirements_filename = f\"{module_name}-requirements.txt\"\n        print_func(f\"Download requirements {requirements_filename}\")\n        urllib.request.urlretrieve(requirements_url, requirements_filename)\n\n        print_func(\"Install requirements\")\n        run_command(\n            [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", requirements_filename])\n\n        os.remove(requirements_filename)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.increment_version","title":"<code>increment_version(version_str, max_value=99)</code>","text":"<p>Inkrementiert eine Versionsnummer im Format \"vX.Y.Z\".</p> <p>Parameters:</p> Name Type Description Default <code>version_str</code> <code>str</code> <p>Die aktuelle Versionsnummer, z. B. \"v0.0.1\".</p> required <code>max_value</code> <code>int</code> <p>Die maximale Zahl pro Stelle (default: 99).</p> <code>99</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Die inkrementierte Versionsnummer.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def increment_version(version_str: str, max_value: int = 99) -&gt; str:\n    \"\"\"\n    Inkrementiert eine Versionsnummer im Format \"vX.Y.Z\".\n\n    Args:\n        version_str (str): Die aktuelle Versionsnummer, z. B. \"v0.0.1\".\n        max_value (int): Die maximale Zahl pro Stelle (default: 99).\n\n    Returns:\n        str: Die inkrementierte Versionsnummer.\n    \"\"\"\n    if not version_str.startswith(\"v\"):\n        raise ValueError(\"Die Versionsnummer muss mit 'v' beginnen, z. B. 'v0.0.1'.\")\n\n    # Entferne das f\u00fchrende 'v' und parse die Versionsnummer\n    version_core = version_str[1:]\n    try:\n        version = Version(version_core)\n    except ValueError as e:\n        raise ValueError(f\"Ung\u00fcltige Versionsnummer: {version_core}\") from e\n\n    # Extrahiere die Versionsteile und konvertiere sie zu einer Liste\n    parts = list(version.release)\n\n    # Inkrementiere die letzte Stelle\n    for i in range(len(parts) - 1, -1, -1):\n        if parts[i] &lt; max_value:\n            parts[i] += 1\n            break\n        else:\n            parts[i] = 0\n            # Schleife f\u00e4hrt fort, um die n\u00e4chsth\u00f6here Stelle zu inkrementieren\n    else:\n        # Wenn alle Stellen auf \"max_value\" sind, f\u00fcge eine neue Stelle hinzu\n        parts.insert(0, 1)\n\n    # Baue die neue Version\n    new_version = \"v\" + \".\".join(map(str, parts))\n    return new_version\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.installer","title":"<code>installer(app, module_name, build_state=True)</code>  <code>async</code>","text":"<p>Installiert oder aktualisiert ein Modul basierend auf der Remote-Version.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>@export(mod_name=Name, name=\"install\", test=False)\nasync def installer(app: App | None, module_name: str, build_state=True):\n    \"\"\"\n    Installiert oder aktualisiert ein Modul basierend auf der Remote-Version.\n    \"\"\"\n    if app is None:\n        app = get_app(f\"{Name}.installer\")\n\n    if not app.session.valid and not await app.session.login():\n        return Result.default_user_error(\"Please login with CloudM login\")\n\n    # Hole nur die h\u00f6chste verf\u00fcgbare Version vom Server\n    response = await app.session.fetch(f\"/api/{Name}/getModVersion?module_name={module_name}\", method=\"GET\")\n    remote_version: str = await response.text()\n    if remote_version == \"None\":\n        remote_version = None\n    # Finde lokale Version\n    local_version = find_highest_zip_version(\n        module_name, version_only=True\n    )\n\n    if not local_version and not remote_version:\n        return Result.default_user_error(f\"404 mod {module_name} not found\")\n\n    # Vergleiche Versionen\n    local_ver = pv.parse(local_version) if local_version else pv.parse(\"0.0.0\")\n    remote_ver = pv.parse(remote_version)\n\n    app.print(f\"Mod versions - Local: {local_ver}, Remote: {remote_ver}\")\n\n    if remote_ver &gt; local_ver:\n        # Konstruiere die URL direkt aus Modulname und Version\n        download_path = Path(app.start_dir) / 'mods_sto'\n\n        app.print(f\"Fetching Mod from {app.session.base}/api/{Name}/download_mod?module_name={module_name}\")\n        if not await app.session.download_file(f\"/api/{Name}/download_mod?module_name={module_name}\", str(download_path)):\n            app.print(\"Failed to download mod\")\n            if 'y' not in input(\"Download manually and place in mods_sto folder. Done? (y/n) \").lower():\n                return Result.default_user_error(\"Installation cancelled\")\n\n        # Korrigiere Dateinamen\n        zip_name = f\"RST${module_name}&amp;{app.version}\u00a7{remote_version}.zip\"\n\n        with Spinner(\"Installing from zip\"):\n            report = install_from_zip(app, zip_name)\n\n        if not report:\n            return Result.default_user_error(\"Setup error occurred\")\n\n        if build_state:\n            get_state_from_app(app)\n\n        return report\n\n    app.print(\"Module is already up to date\")\n    return Result.ok()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.run_command","title":"<code>run_command(command, cwd=None)</code>","text":"<p>F\u00fchrt einen Befehl aus und gibt den Output zur\u00fcck.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def run_command(command, cwd=None):\n    \"\"\"F\u00fchrt einen Befehl aus und gibt den Output zur\u00fcck.\"\"\"\n    result = subprocess.run(command, cwd=cwd, capture_output=True, text=True, check=True,\n                            encoding='cp850')\n    return result.stdout\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.uninstall_module","title":"<code>uninstall_module(path, module_name='', version='-.-.-', additional_dirs=None, yaml_data=None)</code>","text":"<p>Deinstalliert ein Python-Modul, indem es das Modulverzeichnis oder die ZIP-Datei entfernt.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Pfad zum Ordner oder zur Datei, die in das Modul aufgenommen werden soll.</p> required <code>additional_dirs</code> <code>dict</code> <p>Zus\u00e4tzliche Verzeichnisse, die hinzugef\u00fcgt werden sollen.</p> <code>None</code> <code>version</code> <code>str</code> <p>Version des Moduls.</p> <code>'-.-.-'</code> <code>module_name</code> <code>str</code> <p>Name des Moduls.</p> <code>''</code> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def uninstall_module(path, module_name='', version='-.-.-', additional_dirs=None, yaml_data=None):\n    \"\"\"\n    Deinstalliert ein Python-Modul, indem es das Modulverzeichnis oder die ZIP-Datei entfernt.\n\n    Args:\n        path (str): Pfad zum Ordner oder zur Datei, die in das Modul aufgenommen werden soll.\n        additional_dirs (dict): Zus\u00e4tzliche Verzeichnisse, die hinzugef\u00fcgt werden sollen.\n        version (str): Version des Moduls.\n        module_name (str): Name des Moduls.\n\n    \"\"\"\n    if additional_dirs is None:\n        additional_dirs = {}\n    if yaml_data is None:\n        yaml_data = {}\n\n    os.makedirs(\"./mods_sto/temp/\", exist_ok=True)\n\n    base_path = os.path.dirname(path)\n    module_path = os.path.join(base_path, module_name)\n    zip_path = f\"./mods_sto/RST${module_name}&amp;{__version__}\u00a7{version}.zip\"\n\n    # Modulverzeichnis erstellen, falls es nicht existiert\n    if not os.path.exists(module_path):\n        print(\"Module %s already uninstalled\")\n        return False\n\n    # Datei oder Ordner in das Modulverzeichnis kopieren\n    shutil.rmtree(module_path)\n\n    # Zus\u00e4tzliche Verzeichnisse hinzuf\u00fcgen\n    for _dir_name, dir_paths in additional_dirs.items():\n        if isinstance(dir_paths, str):\n            dir_paths = [dir_paths]\n        for dir_path in dir_paths:\n            shutil.rmtree(dir_path)\n            print(f\"Der Pfad {dir_path} wurde entfernt\")\n\n    # Urspr\u00fcngliches Modulverzeichnis l\u00f6schen\n    shutil.rmtree(zip_path)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager.unpack_and_move_module","title":"<code>unpack_and_move_module(zip_path, base_path='./mods', module_name='')</code>","text":"<p>Entpackt eine ZIP-Datei und verschiebt die Inhalte an die richtige Stelle. \u00dcberschreibt existierende Dateien f\u00fcr Update-Unterst\u00fctzung.</p> <p>Parameters:</p> Name Type Description Default <code>zip_path</code> <code>str</code> <p>Pfad zur ZIP-Datei, die entpackt werden soll</p> required <code>base_path</code> <code>str</code> <p>Basispfad, unter dem das Modul gespeichert werden soll</p> <code>'./mods'</code> <code>module_name</code> <code>str</code> <p>Name des Moduls (optional, wird sonst aus ZIP-Namen extrahiert)</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Name des installierten Moduls</p> Source code in <code>toolboxv2/mods/CloudM/ModManager.py</code> <pre><code>def unpack_and_move_module(zip_path: str, base_path: str = './mods', module_name: str = '') -&gt; str:\n    \"\"\"\n    Entpackt eine ZIP-Datei und verschiebt die Inhalte an die richtige Stelle.\n    \u00dcberschreibt existierende Dateien f\u00fcr Update-Unterst\u00fctzung.\n\n    Args:\n        zip_path (str): Pfad zur ZIP-Datei, die entpackt werden soll\n        base_path (str): Basispfad, unter dem das Modul gespeichert werden soll\n        module_name (str): Name des Moduls (optional, wird sonst aus ZIP-Namen extrahiert)\n\n    Returns:\n        str: Name des installierten Moduls\n    \"\"\"\n    # Konvertiere Pfade zu Path-Objekten f\u00fcr bessere Handhabung\n    zip_path = Path(zip_path)\n    base_path = Path(base_path)\n\n    # Extrahiere Modulnamen falls nicht angegeben\n    if not module_name:\n        module_name = zip_path.name.split('$')[1].split('&amp;')[0]\n\n    module_path = base_path / module_name\n    temp_base = Path('./mods_sto/temp')\n\n    try:\n        # Erstelle tempor\u00e4res Verzeichnis\n        temp_base.mkdir(parents=True, exist_ok=True)\n        with tempfile.TemporaryDirectory(dir=str(temp_base)) as temp_dir:\n            temp_dir = Path(temp_dir)\n\n            with Spinner(f\"Extracting {zip_path.name}\"):\n                # Entpacke ZIP-Datei\n                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                    zip_ref.extractall(temp_dir)\n\n            # Behandle Modul-Verzeichnis\n            source_module = temp_dir / module_name\n            if source_module.exists():\n                with Spinner(f\"Installing module to {module_path}\"):\n                    if module_path.exists():\n                        # L\u00f6sche existierendes Modul-Verzeichnis f\u00fcr sauberes Update\n                        shutil.rmtree(module_path)\n                    # Verschiebe neues Modul-Verzeichnis\n                    shutil.copytree(source_module, module_path, dirs_exist_ok=True)\n\n            # Behandle zus\u00e4tzliche Dateien im Root\n            with Spinner(\"Installing additional files\"):\n                for item in temp_dir.iterdir():\n                    if item.name == module_name:\n                        continue\n\n                    target = Path('./') / item.name\n                    if item.is_dir():\n                        with Spinner(f\"Installing directory {item.name}\"):\n                            if target.exists():\n                                shutil.rmtree(target)\n                            shutil.copytree(item, target, dirs_exist_ok=True)\n                    else:\n                        with Spinner(f\"Installing file {item.name}\"):\n                            shutil.copy2(item, target)\n\n            print(f\"Successfully installed/updated module {module_name} to {module_path}\")\n            return module_name\n\n    except Exception as e:\n        print(f\"Error during installation: {str(e)}\")\n        # Cleanup bei Fehler\n        if module_path.exists():\n            shutil.rmtree(module_path)\n        raise\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager_tests","title":"<code>ModManager_tests</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager_tests.TestModManager","title":"<code>TestModManager</code>","text":"<p>               Bases: <code>TestCase</code></p> Source code in <code>toolboxv2/mods/CloudM/ModManager_tests.py</code> <pre><code>class TestModManager(unittest.TestCase):\n    app: App = None\n\n    def test_increment_version(self):\n        \"\"\"Tests the version increment logic.\"\"\"\n        print(\"\\nTesting increment_version...\")\n        self.assertEqual(increment_version(\"v0.0.1\"), \"v0.0.2\")\n        self.assertEqual(increment_version(\"v0.0.99\", max_value=99), \"v0.1.0\")\n        self.assertEqual(increment_version(\"v0.99.99\", max_value=99), \"v1.0.0\")\n        self.assertEqual(increment_version(\"v98\"), \"v99\")\n        with self.assertRaises(ValueError, msg=\"Should fail if 'v' is missing\"):\n            print(increment_version(\"0.0.1\"))\n        print(\"increment_version tests passed.\")\n\n    def setUp(self):\n        \"\"\"Set up a temporary environment for each test.\"\"\"\n        self.original_cwd = os.getcwd()\n        self.test_dir = tempfile.mkdtemp(prefix=\"mod_manager_test_\")\n\n        # The functions in ModManager use relative paths like './mods' and './mods_sto'\n        # We'll create these inside our temp directory and chdir into it.\n        os.chdir(self.test_dir)\n        os.makedirs(\"mods\", exist_ok=True)\n        os.makedirs(\"mods_sto\", exist_ok=True)\n        os.makedirs(\"source_module\", exist_ok=True)\n\n    def tearDown(self):\n        \"\"\"Clean up the temporary environment after each test.\"\"\"\n        os.chdir(self.original_cwd)\n        shutil.rmtree(self.test_dir, ignore_errors=True)\n\n    def test_create_pack_unpack_cycle(self):\n        \"\"\"Tests the full cycle of creating, packing, and unpacking a module.\"\"\"\n        print(\"\\nTesting create_pack_unpack_cycle...\")\n        module_name = \"MyTestMod\"\n        module_version = \"v0.1.0\"\n\n        # 1. Create a dummy module structure inside the temp 'source_module' dir\n        source_path = Path(\"source_module\")\n        module_source_path = source_path / module_name\n        module_source_path.mkdir()\n        (module_source_path / \"main.py\").write_text(\"print('hello from my test mod')\")\n        (module_source_path / \"data.txt\").write_text(\"some test data\")\n\n        # 2. Call create_and_pack_module\n        # The 'path' argument is the parent directory of the module directory.\n        zip_path_str = create_and_pack_module(\n            path=str(source_path),\n            module_name=module_name,\n            version=module_version\n        )\n        self.assertTrue(zip_path_str, \"create_and_pack_module should return a path.\")\n        zip_path = Path(zip_path_str)\n\n        # 3. Assert the zip file was created in the correct location ('./mods_sto')\n        self.assertTrue(zip_path.exists(), f\"Zip file should exist at {zip_path}\")\n        self.assertEqual(zip_path.parent.name, \"mods_sto\")\n\n        # 4. Call unpack_and_move_module\n        # We unpack into the './mods' directory.\n        unpacked_name = unpack_and_move_module(\n            zip_path=str(zip_path),\n            base_path=\"mods\"\n        )\n\n        # 5. Assert the module was unpacked correctly\n        self.assertEqual(unpacked_name, module_name)\n        unpacked_dir = Path(\"mods\") / module_name\n        self.assertTrue(unpacked_dir.is_dir(), \"Unpacked module directory should exist.\")\n\n        # Verify content\n        self.assertTrue((unpacked_dir / \"main.py\").exists())\n        self.assertEqual((unpacked_dir / \"main.py\").read_text(), \"print('hello from my test mod')\")\n        self.assertTrue((unpacked_dir / \"data.txt\").exists())\n        self.assertEqual((unpacked_dir / \"data.txt\").read_text(), \"some test data\")\n\n        # Verify that the tbConfig.yaml was created and has correct info\n        config_path = unpacked_dir / \"tbConfig.yaml\"\n        self.assertTrue(config_path.exists())\n        with open(config_path, 'r') as f:\n            config = yaml.safe_load(f)\n        self.assertEqual(config.get(\"module_name\"), module_name)\n        self.assertEqual(config.get(\"version\"), module_version)\n\n        print(\"create_pack_unpack_cycle tests passed.\")\n\n    def test_install_from_zip(self):\n        \"\"\"Tests the install_from_zip helper function.\"\"\"\n        print(\"\\nTesting install_from_zip...\")\n        module_name = \"MyInstallTestMod\"\n        module_version = \"v0.1.1\"\n\n        # 1. Create a dummy module and zip it\n        source_path = Path(\"source_module\")\n        module_source_path = source_path / module_name\n        module_source_path.mkdir()\n        (module_source_path / \"main.py\").write_text(\"pass\")\n        zip_path_str = create_and_pack_module(\n            path=str(source_path),\n            module_name=module_name,\n            version=module_version\n        )\n        zip_path = Path(zip_path_str)\n        zip_name = zip_path.name\n\n        # 2. Mock the app object needed by install_from_zip\n        mock_app = lambda :None\n        mock_app.start_dir = self.test_dir\n\n        # 3. Call install_from_zip\n        result = install_from_zip(mock_app, zip_name, no_dep=True)\n\n        # 4. Assert the installation was successful\n        self.assertTrue(result)\n        unpacked_dir = Path(\"mods\") / module_name\n        self.assertTrue(unpacked_dir.is_dir())\n        self.assertTrue((unpacked_dir / \"main.py\").exists())\n        print(\"install_from_zip tests passed.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager_tests.TestModManager.setUp","title":"<code>setUp()</code>","text":"<p>Set up a temporary environment for each test.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager_tests.py</code> <pre><code>def setUp(self):\n    \"\"\"Set up a temporary environment for each test.\"\"\"\n    self.original_cwd = os.getcwd()\n    self.test_dir = tempfile.mkdtemp(prefix=\"mod_manager_test_\")\n\n    # The functions in ModManager use relative paths like './mods' and './mods_sto'\n    # We'll create these inside our temp directory and chdir into it.\n    os.chdir(self.test_dir)\n    os.makedirs(\"mods\", exist_ok=True)\n    os.makedirs(\"mods_sto\", exist_ok=True)\n    os.makedirs(\"source_module\", exist_ok=True)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager_tests.TestModManager.tearDown","title":"<code>tearDown()</code>","text":"<p>Clean up the temporary environment after each test.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager_tests.py</code> <pre><code>def tearDown(self):\n    \"\"\"Clean up the temporary environment after each test.\"\"\"\n    os.chdir(self.original_cwd)\n    shutil.rmtree(self.test_dir, ignore_errors=True)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager_tests.TestModManager.test_create_pack_unpack_cycle","title":"<code>test_create_pack_unpack_cycle()</code>","text":"<p>Tests the full cycle of creating, packing, and unpacking a module.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager_tests.py</code> <pre><code>def test_create_pack_unpack_cycle(self):\n    \"\"\"Tests the full cycle of creating, packing, and unpacking a module.\"\"\"\n    print(\"\\nTesting create_pack_unpack_cycle...\")\n    module_name = \"MyTestMod\"\n    module_version = \"v0.1.0\"\n\n    # 1. Create a dummy module structure inside the temp 'source_module' dir\n    source_path = Path(\"source_module\")\n    module_source_path = source_path / module_name\n    module_source_path.mkdir()\n    (module_source_path / \"main.py\").write_text(\"print('hello from my test mod')\")\n    (module_source_path / \"data.txt\").write_text(\"some test data\")\n\n    # 2. Call create_and_pack_module\n    # The 'path' argument is the parent directory of the module directory.\n    zip_path_str = create_and_pack_module(\n        path=str(source_path),\n        module_name=module_name,\n        version=module_version\n    )\n    self.assertTrue(zip_path_str, \"create_and_pack_module should return a path.\")\n    zip_path = Path(zip_path_str)\n\n    # 3. Assert the zip file was created in the correct location ('./mods_sto')\n    self.assertTrue(zip_path.exists(), f\"Zip file should exist at {zip_path}\")\n    self.assertEqual(zip_path.parent.name, \"mods_sto\")\n\n    # 4. Call unpack_and_move_module\n    # We unpack into the './mods' directory.\n    unpacked_name = unpack_and_move_module(\n        zip_path=str(zip_path),\n        base_path=\"mods\"\n    )\n\n    # 5. Assert the module was unpacked correctly\n    self.assertEqual(unpacked_name, module_name)\n    unpacked_dir = Path(\"mods\") / module_name\n    self.assertTrue(unpacked_dir.is_dir(), \"Unpacked module directory should exist.\")\n\n    # Verify content\n    self.assertTrue((unpacked_dir / \"main.py\").exists())\n    self.assertEqual((unpacked_dir / \"main.py\").read_text(), \"print('hello from my test mod')\")\n    self.assertTrue((unpacked_dir / \"data.txt\").exists())\n    self.assertEqual((unpacked_dir / \"data.txt\").read_text(), \"some test data\")\n\n    # Verify that the tbConfig.yaml was created and has correct info\n    config_path = unpacked_dir / \"tbConfig.yaml\"\n    self.assertTrue(config_path.exists())\n    with open(config_path, 'r') as f:\n        config = yaml.safe_load(f)\n    self.assertEqual(config.get(\"module_name\"), module_name)\n    self.assertEqual(config.get(\"version\"), module_version)\n\n    print(\"create_pack_unpack_cycle tests passed.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager_tests.TestModManager.test_increment_version","title":"<code>test_increment_version()</code>","text":"<p>Tests the version increment logic.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager_tests.py</code> <pre><code>def test_increment_version(self):\n    \"\"\"Tests the version increment logic.\"\"\"\n    print(\"\\nTesting increment_version...\")\n    self.assertEqual(increment_version(\"v0.0.1\"), \"v0.0.2\")\n    self.assertEqual(increment_version(\"v0.0.99\", max_value=99), \"v0.1.0\")\n    self.assertEqual(increment_version(\"v0.99.99\", max_value=99), \"v1.0.0\")\n    self.assertEqual(increment_version(\"v98\"), \"v99\")\n    with self.assertRaises(ValueError, msg=\"Should fail if 'v' is missing\"):\n        print(increment_version(\"0.0.1\"))\n    print(\"increment_version tests passed.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager_tests.TestModManager.test_install_from_zip","title":"<code>test_install_from_zip()</code>","text":"<p>Tests the install_from_zip helper function.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager_tests.py</code> <pre><code>def test_install_from_zip(self):\n    \"\"\"Tests the install_from_zip helper function.\"\"\"\n    print(\"\\nTesting install_from_zip...\")\n    module_name = \"MyInstallTestMod\"\n    module_version = \"v0.1.1\"\n\n    # 1. Create a dummy module and zip it\n    source_path = Path(\"source_module\")\n    module_source_path = source_path / module_name\n    module_source_path.mkdir()\n    (module_source_path / \"main.py\").write_text(\"pass\")\n    zip_path_str = create_and_pack_module(\n        path=str(source_path),\n        module_name=module_name,\n        version=module_version\n    )\n    zip_path = Path(zip_path_str)\n    zip_name = zip_path.name\n\n    # 2. Mock the app object needed by install_from_zip\n    mock_app = lambda :None\n    mock_app.start_dir = self.test_dir\n\n    # 3. Call install_from_zip\n    result = install_from_zip(mock_app, zip_name, no_dep=True)\n\n    # 4. Assert the installation was successful\n    self.assertTrue(result)\n    unpacked_dir = Path(\"mods\") / module_name\n    self.assertTrue(unpacked_dir.is_dir())\n    self.assertTrue((unpacked_dir / \"main.py\").exists())\n    print(\"install_from_zip tests passed.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.ModManager_tests.run_mod_manager_tests","title":"<code>run_mod_manager_tests(app)</code>","text":"<p>This function will be automatically discovered and run by the test runner. It uses the standard unittest framework to run tests.</p> Source code in <code>toolboxv2/mods/CloudM/ModManager_tests.py</code> <pre><code>@export(test_only=True)\ndef run_mod_manager_tests(app: App):\n    \"\"\"\n    This function will be automatically discovered and run by the test runner.\n    It uses the standard unittest framework to run tests.\n    \"\"\"\n    print(\"Running ModManager Tests...\")\n    # We pass the app instance to the test class so it can be used if needed.\n    TestModManager.app = app\n    suite = unittest.TestSuite()\n    suite.addTest(unittest.makeSuite(TestModManager))\n    runner = unittest.TextTestRunner()\n    result = runner.run(suite)\n    if not result.wasSuccessful():\n        # Raise an exception to signal failure to the toolboxv2 test runner\n        raise AssertionError(f\"ModManager tests failed: {result.errors} {result.failures}\")\n    print(\"ModManager tests passed successfully.\")\n    return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.UserAccountManager","title":"<code>UserAccountManager</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.UserAccountManager.get_current_user_from_request_api_wrapper","title":"<code>get_current_user_from_request_api_wrapper(app, request)</code>  <code>async</code>","text":"<p>API callable version of get_current_user_from_request for tbjs/admin panel</p> Source code in <code>toolboxv2/mods/CloudM/UserAccountManager.py</code> <pre><code>@export(mod_name=Name, api=True, version=version, request_as_kwarg=True, row=False)  # row=False to return JSON\nasync def get_current_user_from_request_api_wrapper(app: App, request: RequestData):\n    \"\"\" API callable version of get_current_user_from_request for tbjs/admin panel \"\"\"\n    user = await get_current_user_from_request(app, request)\n    if not user:\n        # Return error that tbjs can handle\n        return Result.default_user_error(info=\"User not authenticated or found.\", data=None, exec_code=401)\n    user_dict = asdict(user)\n    pub_user_data = {}\n    for key in ['name','pub_key','email','creation_time','is_persona','level','log_level','settings']:\n        pub_user_data[key] = user_dict.get(key, None)\n    return Result.ok(data=pub_user_data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.email_services","title":"<code>email_services</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.email_services.send_email_verification_email","title":"<code>send_email_verification_email(app, user_email, username, verification_url)</code>","text":"<p>Sends an email verification link to the user.</p> Source code in <code>toolboxv2/mods/CloudM/email_services.py</code> <pre><code>@s_export\ndef send_email_verification_email(app: App, user_email: str, username: str, verification_url: str):\n    \"\"\"Sends an email verification link to the user.\"\"\"\n    sender = EmailSender(app)\n    subject = f\"Verify Your Email for {APP_NAME}\"\n    preview_text = f\"Almost there, {username}! Just one more step to activate your account.\"\n\n    content_html = f\"\"\"\n        &lt;h2&gt;Hi {username},&lt;/h2&gt;\n        &lt;p&gt;Thanks for signing up for {APP_NAME}! To complete your registration, please verify your email address by clicking the button below.&lt;/p&gt;\n        &lt;a href=\"{verification_url}\" class=\"button\"&gt;Verify Email Address&lt;/a&gt;\n        &lt;p&gt;If you didn't create an account with {APP_NAME}, you can safely ignore this email.&lt;/p&gt;\n        &lt;p&gt;If the button doesn't work, copy and paste this link into your browser:&lt;br&gt;&lt;span class=\"link-in-text\"&gt;{verification_url}&lt;/span&gt;&lt;/p&gt;\n        &lt;p&gt;Sincerely,&lt;br&gt;The {APP_NAME} Team&lt;/p&gt;\n    \"\"\"\n    return sender.send_html_email(user_email, subject, content_html, preview_text)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.email_services.send_magic_link_email","title":"<code>send_magic_link_email(app, user_email, magic_link_url, username=None)</code>","text":"<p>Sends a magic link email for login.</p> Source code in <code>toolboxv2/mods/CloudM/email_services.py</code> <pre><code>@s_export\ndef send_magic_link_email(app: App, user_email: str, magic_link_url: str, username: str = None):\n    \"\"\"Sends a magic link email for login.\"\"\"\n    sender = EmailSender(app)\n    greeting_name = f\", {username}\" if username else \"\"\n    subject = f\"Your Magic Login Link for {APP_NAME}\"\n    preview_text = \"Securely access your account with this one-time link.\"\n\n    content_html = f\"\"\"\n        &lt;h2&gt;Hello{greeting_name}!&lt;/h2&gt;\n        &lt;p&gt;You requested a magic link to sign in to your {APP_NAME} account.&lt;/p&gt;\n        &lt;p&gt;Click the button below to log in. This link is temporary and will expire shortly.&lt;/p&gt;\n        &lt;a href=\"{magic_link_url}\" class=\"button\"&gt;Log In Securely&lt;/a&gt;\n        &lt;p&gt; Invitation key: {magic_link_url.split('?key=')[1].split('&amp;name=')[0].replace('%23', '#')}&lt;/p&gt;\n        &lt;p&gt;If you did not request this link, please ignore this email. Your account is safe.&lt;/p&gt;\n        &lt;p&gt;If the button doesn't work, copy and paste this link into your browser:&lt;br&gt;&lt;span class=\"link-in-text\"&gt;{magic_link_url}&lt;/span&gt;&lt;/p&gt;\n        &lt;p&gt;Thanks,&lt;br&gt;The {APP_NAME} Team&lt;/p&gt;\n    \"\"\"\n    return sender.send_html_email(user_email, subject, content_html, preview_text)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.email_services.send_signup_invitation_email","title":"<code>send_signup_invitation_email(app, invited_user_email, invited_username, inviter_username=None)</code>","text":"<p>Generates an invitation link and sends it via email.</p> Source code in <code>toolboxv2/mods/CloudM/email_services.py</code> <pre><code>@s_export\ndef send_signup_invitation_email(app: App, invited_user_email: str, invited_username: str,\n                                 inviter_username: str = None):\n    \"\"\"Generates an invitation link and sends it via email.\"\"\"\n    sender = EmailSender(app)\n\n    # Generate invitation code as specified in the prompt\n    # This uses the Code class, assuming TB_R_KEY is set in the environment\n    invitation_code = Code.one_way_hash(invited_username, \"00#\", os.getenv(\"TB_R_KEY\", \"pepper123\"))[:12] + str(\n        uuid.uuid4())[:6]\n\n    # Construct the signup link URL (adjust your frontend signup path as needed)\n    signup_link_url = f\"{APP_BASE_URL}/web/assets/signup.html?invitation={quote(invitation_code)}&amp;email={quote(invited_user_email)}&amp;username={quote(invited_username)}\"\n\n    subject = f\"You're Invited to Join {APP_NAME}!\"\n    preview_text = f\"{inviter_username or 'A friend'} has invited you to {APP_NAME}!\"\n    inviter_line = f\"&lt;p&gt;{inviter_username} has invited you to join.&lt;/p&gt;\" if inviter_username else \"&lt;p&gt;You've been invited to join.&lt;/p&gt;\"\n\n    content_html = f\"\"\"\n        &lt;h2&gt;Hello {invited_username},&lt;/h2&gt;\n        {inviter_line}\n        &lt;p&gt;{APP_NAME} is an exciting platform, and we'd love for you to be a part of it!&lt;/p&gt;\n        &lt;p&gt;Click the button below to accept the invitation and create your account:&lt;/p&gt;\n        &lt;a href=\"{signup_link_url}\" class=\"button\"&gt;Accept Invitation &amp; Sign Up&lt;/a&gt;\n        &lt;p&gt;This invitation is unique to you : {invitation_code}&lt;/p&gt;\n        &lt;p&gt;If the button doesn't work, copy and paste this link into your browser:&lt;br&gt;&lt;span class=\"link-in-text\"&gt;{signup_link_url}&lt;/span&gt;&lt;/p&gt;\n        &lt;p&gt;We look forward to seeing you there!&lt;br&gt;The {APP_NAME} Team&lt;/p&gt;\n    \"\"\"\n    return sender.send_html_email(invited_user_email, subject, content_html, preview_text)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.email_services.send_waiting_list_confirmation_email","title":"<code>send_waiting_list_confirmation_email(app, user_email)</code>","text":"<p>Sends a confirmation email for joining the waiting list.</p> Source code in <code>toolboxv2/mods/CloudM/email_services.py</code> <pre><code>@s_export\ndef send_waiting_list_confirmation_email(app: App, user_email: str):\n    \"\"\"Sends a confirmation email for joining the waiting list.\"\"\"\n    sender = EmailSender(app)\n    subject = f\"You're on the Waiting List for {APP_NAME}!\"\n    preview_text = \"Thanks for your interest! We'll keep you updated.\"\n\n    content_html = f\"\"\"\n        &lt;h2&gt;You're In!&lt;/h2&gt;\n        &lt;p&gt;Thank you for joining the waiting list for {APP_NAME}. We're working hard to get things ready and appreciate your interest.&lt;/p&gt;\n        &lt;p&gt;We'll notify you as soon as we have updates or when access becomes available.&lt;/p&gt;\n        &lt;p&gt;In the meantime, you can follow our progress or learn more at &lt;a href=\"{APP_BASE_URL}\" class=\"link-in-text\"&gt;{APP_BASE_URL}&lt;/a&gt;.&lt;/p&gt;\n        &lt;p&gt;Stay tuned,&lt;br&gt;The {APP_NAME} Team&lt;/p&gt;\n    \"\"\"\n    return sender.send_html_email(user_email, subject, content_html, preview_text,\n                                  recipient_email_for_unsubscribe=user_email, show_unsubscribe_link=True)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.email_services.send_welcome_email","title":"<code>send_welcome_email(app, user_email, username, welcome_action_url=None)</code>","text":"<p>Sends a welcome email to a new user.</p> Source code in <code>toolboxv2/mods/CloudM/email_services.py</code> <pre><code>@s_export  # Changed to native, api=False as it's a backend function\ndef send_welcome_email(app: App, user_email: str, username: str, welcome_action_url: str = None):\n    \"\"\"Sends a welcome email to a new user.\"\"\"\n    sender = EmailSender(app)\n    subject = f\"Welcome to {APP_NAME}, {username}!\"\n    preview_text = f\"We're thrilled to have you, {username}!\"\n    action_url = welcome_action_url or f\"{APP_BASE_URL}/dashboard\"  # Default to dashboard\n\n    content_html = f\"\"\"\n        &lt;h2&gt;Welcome Aboard, {username}!&lt;/h2&gt;\n        &lt;p&gt;Thank you for signing up for {APP_NAME}. We're excited to have you join our community!&lt;/p&gt;\n        &lt;p&gt;Here are a few things you might want to do next:&lt;/p&gt;\n        &lt;ul&gt;\n            &lt;li&gt;Explore your new account features.&lt;/li&gt;\n            &lt;li&gt;Customize your profile.&lt;/li&gt;\n        &lt;/ul&gt;\n        &lt;p&gt;Click the button below to get started:&lt;/p&gt;\n        &lt;a href=\"{action_url}\" class=\"button\"&gt;Go to Your Dashboard&lt;/a&gt;\n        &lt;p&gt;If the button doesn't work, copy and paste this link into your browser:&lt;br&gt;&lt;span class=\"link-in-text\"&gt;{action_url}&lt;/span&gt;&lt;/p&gt;\n        &lt;p&gt;Best regards,&lt;br&gt;The {APP_NAME} Team&lt;/p&gt;\n    \"\"\"\n    return sender.send_html_email(user_email, subject, content_html, preview_text,\n                                  recipient_email_for_unsubscribe=user_email, show_unsubscribe_link=True)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.mini","title":"<code>mini</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.mini.check_multiple_processes","title":"<code>check_multiple_processes(pids)</code>","text":"<p>Checks the status of multiple processes in a single system call. Returns a dictionary mapping PIDs to their status (GREEN_CIRCLE, RED_CIRCLE, or YELLOW_CIRCLE).</p> Source code in <code>toolboxv2/mods/CloudM/mini.py</code> <pre><code>def check_multiple_processes(pids: list[int]) -&gt; dict[int, str]:\n    \"\"\"\n    Checks the status of multiple processes in a single system call.\n    Returns a dictionary mapping PIDs to their status (GREEN_CIRCLE, RED_CIRCLE, or YELLOW_CIRCLE).\n    \"\"\"\n    if not pids:\n        return {}\n\n    pid_status = {}\n\n    if os.name == 'nt':  # Windows\n        try:\n            # Windows tasklist requires separate /FI for each filter\n            command = 'tasklist'\n\n            # Add encoding handling for Windows\n            result = subprocess.run(\n                command,\n                capture_output=True,\n                text=True,\n                shell=True,\n                encoding='cp850'  # Use cp850 for Windows console output\n            )\n            # Create a set of running PIDs from the output\n            running_pids = set()\n            for line in result.stdout.lower().split('\\n'):\n                for pid in pids:\n                    if str(pid) in line:\n                        running_pids.add(pid)\n            # Assign status based on whether PID was found in output\n            for pid in pids:\n                if pid in running_pids:\n                    pid_status[pid] = GREEN_CIRCLE\n                else:\n                    pid_status[pid] = RED_CIRCLE\n\n        except subprocess.SubprocessError as e:\n            print(f\"SubprocessError: {e}\")  # For debugging\n            # Mark all as YELLOW_CIRCLE if there's an error running the command\n            for pid in pids:\n                pid_status[pid] = YELLOW_CIRCLE\n        except UnicodeDecodeError as e:\n            print(f\"UnicodeDecodeError: {e}\")  # For debugging\n            # Try alternate encoding if cp850 fails\n            try:\n                result = subprocess.run(\n                    command,\n                    capture_output=True,\n                    text=True,\n                    shell=True,\n                    encoding='utf-8'\n                )\n                running_pids = set()\n                for line in result.stdout.lower().split('\\n'):\n                    for pid in pids:\n                        if str(pid) in line:\n                            running_pids.add(pid)\n\n                for pid in pids:\n                    pid_status[pid] = GREEN_CIRCLE if pid in running_pids else RED_CIRCLE\n            except Exception as e:\n                print(f\"Failed with alternate encoding: {e}\")  # For debugging\n                for pid in pids:\n                    pid_status[pid] = YELLOW_CIRCLE\n\n    else:  # Unix/Linux/Mac\n        try:\n            pids_str = ','.join(str(pid) for pid in pids)\n            command = f'ps -p {pids_str} -o pid='\n\n            result = subprocess.run(\n                command,\n                capture_output=True,\n                text=True,\n                shell=True,\n                encoding='utf-8'\n            )\n            running_pids = set(int(pid) for pid in result.stdout.strip().split())\n\n            for pid in pids:\n                pid_status[pid] = GREEN_CIRCLE if pid in running_pids else RED_CIRCLE\n\n        except subprocess.SubprocessError as e:\n            print(f\"SubprocessError: {e}\")  # For debugging\n            for pid in pids:\n                pid_status[pid] = YELLOW_CIRCLE\n\n    return pid_status\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.mini.get_service_pids","title":"<code>get_service_pids(info_dir)</code>","text":"<p>Extracts service names and PIDs from pid files.</p> Source code in <code>toolboxv2/mods/CloudM/mini.py</code> <pre><code>def get_service_pids(info_dir):\n    \"\"\"Extracts service names and PIDs from pid files.\"\"\"\n    services = {}\n    pid_files = [f for f in os.listdir(info_dir) if re.match(r'(.+)-(.+)\\.pid', f)]\n    for pid_file in pid_files:\n        match = re.match(r'(.+)-(.+)\\.pid', pid_file)\n        if match:\n            services_type, service_name = match.groups()\n            # Read the PID from the file\n            with open(os.path.join(info_dir, pid_file)) as file:\n                pid = file.read().strip()\n                # Store the PID using a formatted key\n                services[f\"{service_name} - {services_type}\"] = int(pid)\n    return services\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.mini.get_service_status","title":"<code>get_service_status(dir)</code>","text":"<p>Displays the status of all services.</p> Source code in <code>toolboxv2/mods/CloudM/mini.py</code> <pre><code>def get_service_status(dir: str) -&gt; str:\n    \"\"\"Displays the status of all services.\"\"\"\n    if time.time()-services_data_sto_last_update_time[0] &gt; 30:\n        services = get_service_pids(dir)\n        services_data_sto[0] = services\n        services_data_sto_last_update_time[0] = time.time()\n    else:\n        services = services_data_sto[0]\n    if not services:\n        return \"No services found\"\n\n    # Get status for all PIDs in a single call\n    pid_statuses = check_multiple_processes(list(services.values()))\n\n    # Build the status string\n    res_s = \"Service(s):\" + (\"\\n\" if len(services) &gt; 1 else ' ')\n    for service_name, pid in services.items():\n        status = pid_statuses.get(pid, YELLOW_CIRCLE)\n        res_s += f\"{status} {service_name} (PID: {pid})\\n\"\n    services_data_display[0] = res_s.strip()\n    return res_s.rstrip()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.module","title":"<code>module</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CloudM.module.hash_password","title":"<code>hash_password(password)</code>","text":"<p>Hash a password for storing.</p> Source code in <code>toolboxv2/mods/CloudM/module.py</code> <pre><code>def hash_password(password):\n    \"\"\"Hash a password for storing.\"\"\"\n    salt = hashlib.sha256(os.urandom(60)).hexdigest().encode('ascii')\n    pwdhash = hashlib.pbkdf2_hmac('sha512', password.encode('utf-8'), salt,\n                                  100000)\n    pwdhash = binascii.hexlify(pwdhash)\n    return (salt + pwdhash).decode('ascii')\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CloudM.module.verify_password","title":"<code>verify_password(stored_password, provided_password)</code>","text":"<p>Verify a stored password against one provided by user</p> Source code in <code>toolboxv2/mods/CloudM/module.py</code> <pre><code>def verify_password(stored_password, provided_password):\n    \"\"\"Verify a stored password against one provided by user\"\"\"\n    salt = stored_password[:64]\n    stored_password = stored_password[64:]\n    pwdhash = hashlib.pbkdf2_hmac('sha512', provided_password.encode('utf-8'),\n                                  salt.encode('ascii'), 100000)\n    pwdhash = binascii.hexlify(pwdhash).decode('ascii')\n    return pwdhash == stored_password\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification","title":"<code>CodeVerification</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification.VerificationSystem","title":"<code>VerificationSystem</code>","text":"Source code in <code>toolboxv2/mods/CodeVerification.py</code> <pre><code>class VerificationSystem:\n    def __init__(self, tools_db, scope=\"main\"):\n        \"\"\"\n        Initialize VerificationSystem with DB Tools integration\n\n        Args:\n            tools_db (Tools): Database tools from toolboxv2.mods.DB\n            scope (str, optional): Scope for templates and codes. Defaults to \"main\".\n        \"\"\"\n        self.tools_db = tools_db\n        self.scope = scope\n        self.tidmp = {}\n        self._ensure_scope_templates()\n\n    def get(self):\n        return self\n\n    def reset_scope_templates(self):\n        \"\"\"\n        Ensure a templates dictionary exists for the current scope in the database\n        \"\"\"\n        templates_key = f\"verification_templates_{self.scope}\"\n\n        self.tools_db.set(templates_key, json.dumps({}))\n\n    def _ensure_scope_templates(self):\n        \"\"\"\n        Ensure a templates dictionary exists for the current scope in the database\n        \"\"\"\n        templates_key = f\"verification_templates_{self.scope}\"\n\n        # Check if templates exist for this scope\n        templates_exist = self.tools_db.if_exist(templates_key)\n\n        if templates_exist.is_error() and not templates_exist.is_data():\n            # Initialize empty templates dictionary if not exists\n            self.tools_db.set(templates_key, json.dumps({}))\n        else:\n            allt = self.get_all_templates()\n\n            for k, v in allt.items():\n                if 'name' not in v:\n                    continue\n                self.tidmp[v['name']] = k\n\n    def add_config_template(self, template: ConfigTemplate) -&gt; str:\n        \"\"\"\n        Add a new configuration template to the database\n\n        Args:\n            template (ConfigTemplate): The configuration template\n\n        Returns:\n            str: Unique identifier of the template\n        \"\"\"\n        # Ensure template has the current scope\n        template.scope = self.scope\n\n        # Generate a unique template ID\n        template_id = secrets.token_urlsafe(8)\n\n        # Get existing templates for this scope\n        templates = self.get_all_templates()\n\n        # Add new template\n        self.tidmp[template.name] = template_id\n        templates[template_id] = asdict(template)\n\n        # Save updated templates back to database\n        templates_key = f\"verification_templates_{self.scope}\"\n        save_result = self.tools_db.set(templates_key, json.dumps(templates))\n\n        if save_result.is_error():\n            raise ValueError(\"Could not save template\")\n\n        return template_id\n\n    def get_all_templates(self):\n        templates_key = f\"verification_templates_{self.scope}\"\n        templates_result = self.tools_db.get(templates_key)\n\n        if not templates_result.is_error() and templates_result.is_data():\n            try:\n                templates_result.result.data = json.loads(templates_result.get())\n            except Exception as e:\n                templates_result.print()\n                print(f\"Errro loding template data curupted : {str(e)}\")\n                templates_result.result.data = {}\n        else:\n            templates_result.result.data = {}\n        if not isinstance(templates_result, dict):\n            templates_result = templates_result.result.data\n        return templates_result\n\n    def generate_code(self, template_id: str) -&gt; str:\n        \"\"\"\n        Generate a code based on the configuration template\n\n        Args:\n            template_id (str): ID of the configuration template\n\n        Returns:\n            str: Generated verification code\n        \"\"\"\n        # Get templates for this scope\n        templates = self.get_all_templates()\n        print(templates, self.tidmp, template_id)\n        if template_id not in templates:\n            template_id = self.tidmp.get(template_id, template_id)\n        if template_id not in templates:\n            raise ValueError(\"Invalid configuration template\")\n\n        template_dict = templates[template_id]\n        ConfigTemplate(**template_dict)\n\n        # Generate a random code with max 16 characters\n        code = secrets.token_urlsafe(10)[:16]\n\n        # Prepare code information\n        code_info = {\n            'template_id': template_id,\n            'created_at': time.time(),\n            'uses_count': 0,\n            'scope': self.scope\n        }\n\n        # Store code information in database\n        codes_key = f\"verification_codes_{self.scope}\"\n        existing_codes_result = self.tools_db.get(codes_key)\n\n        existing_codes = {}\n        if not existing_codes_result.is_error() and existing_codes_result.is_data():\n            d = existing_codes_result.get()\n            if isinstance(d, list):\n                d = d[0]\n            existing_codes = json.loads(d)\n\n        existing_codes[code] = code_info\n\n        save_result = self.tools_db.set(codes_key, json.dumps(existing_codes))\n\n        if save_result.is_error():\n            raise ValueError(\"Could not save generated code\")\n\n        return code\n\n    def validate_code(self, code: str) -&gt; dict[str, Any] | None:\n        \"\"\"\n        Validate a code and return template information\n\n        Args:\n            code (str): Code to validate\n\n        Returns:\n            Optional[Dict[str, Any]]: Template information for valid code, else None\n        \"\"\"\n        # Get codes for this scope\n        codes_key = f\"verification_codes_{self.scope}\"\n        codes_result = self.tools_db.get(codes_key)\n\n        if codes_result.is_error() or not codes_result.is_data():\n            return None\n\n        d = codes_result.get()\n        if isinstance(d, list):\n            d = d[0]\n        existing_codes = json.loads(d)\n\n        if code not in existing_codes:\n            return None\n\n        code_info = existing_codes[code]\n\n        # Check if code is from the same scope\n        if code_info.get('scope') != self.scope:\n            return None\n\n        # Get templates for this scope\n        templates = self.get_all_templates()\n        template_id = code_info['template_id']\n\n        if template_id not in templates:\n            return templates\n\n        template_dict = templates[template_id]\n        template = ConfigTemplate(**template_dict)\n\n        # Check usage count\n        if code_info['uses_count'] &gt;= template.max_uses:\n            del existing_codes[code]\n            self.tools_db.set(codes_key, json.dumps(existing_codes))\n            return None\n\n        # Check time validity for timed codes\n        if template.usage_type == 'timed':\n            current_time = time.time()\n            if template.valid_duration and (current_time - code_info['created_at']) &gt; template.valid_duration:\n                del existing_codes[code]\n                self.tools_db.set(codes_key, json.dumps(existing_codes))\n                return None\n\n        # Update uses count\n        existing_codes[code]['uses_count'] += 1\n        uses_count = existing_codes[code].get('uses_count', 1)\n        # Remove code if it's a one-time use\n        if template.usage_type == 'one_time':\n            del existing_codes[code]\n\n        # Save updated codes\n        self.tools_db.set(codes_key, json.dumps(existing_codes))\n\n        return {\n            'template_name': template.name,\n            'usage_type': template.usage_type,\n            'uses_count': uses_count\n        }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification.VerificationSystem.__init__","title":"<code>__init__(tools_db, scope='main')</code>","text":"<p>Initialize VerificationSystem with DB Tools integration</p> <p>Parameters:</p> Name Type Description Default <code>tools_db</code> <code>Tools</code> <p>Database tools from toolboxv2.mods.DB</p> required <code>scope</code> <code>str</code> <p>Scope for templates and codes. Defaults to \"main\".</p> <code>'main'</code> Source code in <code>toolboxv2/mods/CodeVerification.py</code> <pre><code>def __init__(self, tools_db, scope=\"main\"):\n    \"\"\"\n    Initialize VerificationSystem with DB Tools integration\n\n    Args:\n        tools_db (Tools): Database tools from toolboxv2.mods.DB\n        scope (str, optional): Scope for templates and codes. Defaults to \"main\".\n    \"\"\"\n    self.tools_db = tools_db\n    self.scope = scope\n    self.tidmp = {}\n    self._ensure_scope_templates()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification.VerificationSystem.add_config_template","title":"<code>add_config_template(template)</code>","text":"<p>Add a new configuration template to the database</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>ConfigTemplate</code> <p>The configuration template</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unique identifier of the template</p> Source code in <code>toolboxv2/mods/CodeVerification.py</code> <pre><code>def add_config_template(self, template: ConfigTemplate) -&gt; str:\n    \"\"\"\n    Add a new configuration template to the database\n\n    Args:\n        template (ConfigTemplate): The configuration template\n\n    Returns:\n        str: Unique identifier of the template\n    \"\"\"\n    # Ensure template has the current scope\n    template.scope = self.scope\n\n    # Generate a unique template ID\n    template_id = secrets.token_urlsafe(8)\n\n    # Get existing templates for this scope\n    templates = self.get_all_templates()\n\n    # Add new template\n    self.tidmp[template.name] = template_id\n    templates[template_id] = asdict(template)\n\n    # Save updated templates back to database\n    templates_key = f\"verification_templates_{self.scope}\"\n    save_result = self.tools_db.set(templates_key, json.dumps(templates))\n\n    if save_result.is_error():\n        raise ValueError(\"Could not save template\")\n\n    return template_id\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification.VerificationSystem.generate_code","title":"<code>generate_code(template_id)</code>","text":"<p>Generate a code based on the configuration template</p> <p>Parameters:</p> Name Type Description Default <code>template_id</code> <code>str</code> <p>ID of the configuration template</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated verification code</p> Source code in <code>toolboxv2/mods/CodeVerification.py</code> <pre><code>def generate_code(self, template_id: str) -&gt; str:\n    \"\"\"\n    Generate a code based on the configuration template\n\n    Args:\n        template_id (str): ID of the configuration template\n\n    Returns:\n        str: Generated verification code\n    \"\"\"\n    # Get templates for this scope\n    templates = self.get_all_templates()\n    print(templates, self.tidmp, template_id)\n    if template_id not in templates:\n        template_id = self.tidmp.get(template_id, template_id)\n    if template_id not in templates:\n        raise ValueError(\"Invalid configuration template\")\n\n    template_dict = templates[template_id]\n    ConfigTemplate(**template_dict)\n\n    # Generate a random code with max 16 characters\n    code = secrets.token_urlsafe(10)[:16]\n\n    # Prepare code information\n    code_info = {\n        'template_id': template_id,\n        'created_at': time.time(),\n        'uses_count': 0,\n        'scope': self.scope\n    }\n\n    # Store code information in database\n    codes_key = f\"verification_codes_{self.scope}\"\n    existing_codes_result = self.tools_db.get(codes_key)\n\n    existing_codes = {}\n    if not existing_codes_result.is_error() and existing_codes_result.is_data():\n        d = existing_codes_result.get()\n        if isinstance(d, list):\n            d = d[0]\n        existing_codes = json.loads(d)\n\n    existing_codes[code] = code_info\n\n    save_result = self.tools_db.set(codes_key, json.dumps(existing_codes))\n\n    if save_result.is_error():\n        raise ValueError(\"Could not save generated code\")\n\n    return code\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification.VerificationSystem.reset_scope_templates","title":"<code>reset_scope_templates()</code>","text":"<p>Ensure a templates dictionary exists for the current scope in the database</p> Source code in <code>toolboxv2/mods/CodeVerification.py</code> <pre><code>def reset_scope_templates(self):\n    \"\"\"\n    Ensure a templates dictionary exists for the current scope in the database\n    \"\"\"\n    templates_key = f\"verification_templates_{self.scope}\"\n\n    self.tools_db.set(templates_key, json.dumps({}))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.CodeVerification.VerificationSystem.validate_code","title":"<code>validate_code(code)</code>","text":"<p>Validate a code and return template information</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>Code to validate</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>Optional[Dict[str, Any]]: Template information for valid code, else None</p> Source code in <code>toolboxv2/mods/CodeVerification.py</code> <pre><code>def validate_code(self, code: str) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Validate a code and return template information\n\n    Args:\n        code (str): Code to validate\n\n    Returns:\n        Optional[Dict[str, Any]]: Template information for valid code, else None\n    \"\"\"\n    # Get codes for this scope\n    codes_key = f\"verification_codes_{self.scope}\"\n    codes_result = self.tools_db.get(codes_key)\n\n    if codes_result.is_error() or not codes_result.is_data():\n        return None\n\n    d = codes_result.get()\n    if isinstance(d, list):\n        d = d[0]\n    existing_codes = json.loads(d)\n\n    if code not in existing_codes:\n        return None\n\n    code_info = existing_codes[code]\n\n    # Check if code is from the same scope\n    if code_info.get('scope') != self.scope:\n        return None\n\n    # Get templates for this scope\n    templates = self.get_all_templates()\n    template_id = code_info['template_id']\n\n    if template_id not in templates:\n        return templates\n\n    template_dict = templates[template_id]\n    template = ConfigTemplate(**template_dict)\n\n    # Check usage count\n    if code_info['uses_count'] &gt;= template.max_uses:\n        del existing_codes[code]\n        self.tools_db.set(codes_key, json.dumps(existing_codes))\n        return None\n\n    # Check time validity for timed codes\n    if template.usage_type == 'timed':\n        current_time = time.time()\n        if template.valid_duration and (current_time - code_info['created_at']) &gt; template.valid_duration:\n            del existing_codes[code]\n            self.tools_db.set(codes_key, json.dumps(existing_codes))\n            return None\n\n    # Update uses count\n    existing_codes[code]['uses_count'] += 1\n    uses_count = existing_codes[code].get('uses_count', 1)\n    # Remove code if it's a one-time use\n    if template.usage_type == 'one_time':\n        del existing_codes[code]\n\n    # Save updated codes\n    self.tools_db.set(codes_key, json.dumps(existing_codes))\n\n    return {\n        'template_name': template.name,\n        'usage_type': template.usage_type,\n        'uses_count': uses_count\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB","title":"<code>DB</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.DB.blob_instance","title":"<code>blob_instance</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.DB.blob_instance.BlobDB","title":"<code>BlobDB</code>","text":"<p>A persistent, encrypted dictionary-like database that uses the BlobStorage system as its backend, making it networked and fault-tolerant.</p> Source code in <code>toolboxv2/mods/DB/blob_instance.py</code> <pre><code>class BlobDB:\n    \"\"\"\n    A persistent, encrypted dictionary-like database that uses the BlobStorage\n    system as its backend, making it networked and fault-tolerant.\n    \"\"\"\n    auth_type = AuthenticationTypes.location\n\n    def __init__(self):\n        self.data: dict = {}\n        self.key: str | None = None\n        self.db_path: str | None = None\n        self.storage_client: BlobStorage | None = None\n\n\n    def initialize(self, db_path: str, key: str, storage_client: BlobStorage) -&gt; Result:\n        \"\"\"\n        Initializes the database from a location within the blob storage.\n\n        Args:\n            db_path (str): The virtual path within the blob storage,\n                           e.g., \"my_database_blob/database.json\".\n            key (str): The encryption key for the database content.\n            storage_client (BlobStorage): An initialized BlobStorage client instance.\n\n        Returns:\n            Result: An OK result if successful.\n        \"\"\"\n        self.db_path = db_path\n        self.key = key\n        self.storage_client = storage_client\n\n        print(f\"Initializing BlobDB from blob path: '{self.db_path}'...\")\n\n        try:\n            # Use BlobFile for reading. It handles caching, networking, and decryption.\n            db_file = BlobFile(self.db_path, mode='r', storage=self.storage_client, key=self.key)\n            if not db_file.exists():\n                print(f\"Database file not found at '{self.db_path}'. Starting with an empty database.\")\n                db_file.create()\n                self.data = {}\n            else:\n                with db_file as f:\n                    # read_json safely loads the content.\n                    self.data = f.read_json()\n                    if not self.data:  # Handle case where file exists but is empty\n                        self.data = {}\n                print(\"Successfully initialized database.\")\n\n        except Exception as e:\n            print(f\"Warning: Could not initialize BlobDB from '{self.db_path}'. Error: {e}. Starting fresh.\")\n            self.data = {}\n\n        return Result.ok().set_origin(\"Blob Dict DB\")\n\n    def exit(self) -&gt; Result:\n        \"\"\"\n        Saves the current state of the database back to the blob storage.\n        \"\"\"\n        print(\"BLOB DB on exit \", not all([self.key, self.db_path, self.storage_client]))\n        if not all([self.key, self.db_path, self.storage_client]):\n            return Result.default_internal_error(\n                info=\"Database not initialized. Cannot exit.\"\n            ).set_origin(\"Blob Dict DB\")\n\n        print(f\"Saving database to blob path: '{self.db_path}'...\")\n        try:\n            # Use BlobFile for writing. It handles encryption, networking, and updates.\n            with BlobFile(self.db_path, mode='w', storage=self.storage_client, key=self.key) as f:\n                f.write_json(self.data)\n\n            print(\"Success: Database saved to blob storage.\")\n            return Result.ok().set_origin(\"Blob Dict DB\")\n\n        except Exception as e:\n            return Result.custom_error(\n                data=e,\n                info=f\"Error saving database to blob storage: {e}\"\n            ).set_origin(\"Blob Dict DB\")\n\n    # --- Data Manipulation Methods (Unchanged Logic) ---\n    # These methods operate on the in-memory `self.data` dictionary and do not\n    # need to be changed, as the loading/saving is handled by initialize/exit.\n\n    def get(self, key: str) -&gt; Result:\n        if not self.data:\n            return Result.default_internal_error(info=f\"No data found for key '{key}' (database is empty).\").set_origin(\n                \"Blob Dict DB\")\n\n        data = []\n        if key == 'all':\n            data_info = \"Returning all data available\"\n            data = list(self.data.items())\n        elif key == \"all-k\":\n            data_info = \"Returning all keys\"\n            data = list(self.data.keys())\n        else:\n            data_info = f\"Returning values for keys starting with '{key.replace('*', '')}'\"\n            data = [self.data[k] for k in self.scan_iter(key)]\n\n        if not data:\n            return Result.default_internal_error(info=f\"No data found for key '{key}'\").set_origin(\"Blob Dict DB\")\n\n        return Result.ok(data=data, data_info=data_info).set_origin(\"Blob Dict DB\")\n\n    def set(self, key: str, value) -&gt; Result:\n        if not isinstance(key, str) or not key:\n            return Result.default_user_error(info=\"Key must be a non-empty string.\").set_origin(\"Blob Dict DB\")\n\n        self.data[key] = value\n        return Result.ok().set_origin(\"Blob Dict DB\")\n\n    def scan_iter(self, search: str = ''):\n        if not self.data:\n            return []\n        prefix = search.replace('*', '')\n        return [key for key in self.data if key.startswith(prefix)]\n\n    def append_on_set(self, key: str, value: list) -&gt; Result:\n        if key not in self.data:\n            self.data[key] = []\n\n        if not isinstance(self.data[key], list):\n            return Result.default_user_error(info=f\"Existing value for key '{key}' is not a list.\").set_origin(\n                \"Blob Dict DB\")\n\n        # Use a set for efficient checking to avoid duplicates\n        existing_set = set(self.data[key])\n        new_items = [item for item in value if item not in existing_set]\n        self.data[key].extend(new_items)\n        return Result.ok().set_origin(\"Blob Dict DB\")\n\n    def if_exist(self, key: str) -&gt; int:\n        if key.endswith('*'):\n            return len(self.scan_iter(key))\n        return 1 if key in self.data else 0\n\n    def delete(self, key: str, matching: bool = False) -&gt; Result:\n        keys_to_delete = []\n        if matching:\n            keys_to_delete = self.scan_iter(key)\n        elif key in self.data:\n            keys_to_delete.append(key)\n\n        if not keys_to_delete:\n            return Result.default_internal_error(info=f\"No keys found to delete for pattern '{key}'\").set_origin(\n                \"Blob Dict DB\")\n\n        deleted_items = {k: self.data.pop(k) for k in keys_to_delete}\n        return Result.ok(\n            data=list(deleted_items.items()),\n            data_info=f\"Successfully removed {len(deleted_items)} item(s).\"\n        ).set_origin(\"Blob Dict DB\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.blob_instance.BlobDB.exit","title":"<code>exit()</code>","text":"<p>Saves the current state of the database back to the blob storage.</p> Source code in <code>toolboxv2/mods/DB/blob_instance.py</code> <pre><code>def exit(self) -&gt; Result:\n    \"\"\"\n    Saves the current state of the database back to the blob storage.\n    \"\"\"\n    print(\"BLOB DB on exit \", not all([self.key, self.db_path, self.storage_client]))\n    if not all([self.key, self.db_path, self.storage_client]):\n        return Result.default_internal_error(\n            info=\"Database not initialized. Cannot exit.\"\n        ).set_origin(\"Blob Dict DB\")\n\n    print(f\"Saving database to blob path: '{self.db_path}'...\")\n    try:\n        # Use BlobFile for writing. It handles encryption, networking, and updates.\n        with BlobFile(self.db_path, mode='w', storage=self.storage_client, key=self.key) as f:\n            f.write_json(self.data)\n\n        print(\"Success: Database saved to blob storage.\")\n        return Result.ok().set_origin(\"Blob Dict DB\")\n\n    except Exception as e:\n        return Result.custom_error(\n            data=e,\n            info=f\"Error saving database to blob storage: {e}\"\n        ).set_origin(\"Blob Dict DB\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.blob_instance.BlobDB.initialize","title":"<code>initialize(db_path, key, storage_client)</code>","text":"<p>Initializes the database from a location within the blob storage.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>The virtual path within the blob storage,            e.g., \"my_database_blob/database.json\".</p> required <code>key</code> <code>str</code> <p>The encryption key for the database content.</p> required <code>storage_client</code> <code>BlobStorage</code> <p>An initialized BlobStorage client instance.</p> required <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An OK result if successful.</p> Source code in <code>toolboxv2/mods/DB/blob_instance.py</code> <pre><code>def initialize(self, db_path: str, key: str, storage_client: BlobStorage) -&gt; Result:\n    \"\"\"\n    Initializes the database from a location within the blob storage.\n\n    Args:\n        db_path (str): The virtual path within the blob storage,\n                       e.g., \"my_database_blob/database.json\".\n        key (str): The encryption key for the database content.\n        storage_client (BlobStorage): An initialized BlobStorage client instance.\n\n    Returns:\n        Result: An OK result if successful.\n    \"\"\"\n    self.db_path = db_path\n    self.key = key\n    self.storage_client = storage_client\n\n    print(f\"Initializing BlobDB from blob path: '{self.db_path}'...\")\n\n    try:\n        # Use BlobFile for reading. It handles caching, networking, and decryption.\n        db_file = BlobFile(self.db_path, mode='r', storage=self.storage_client, key=self.key)\n        if not db_file.exists():\n            print(f\"Database file not found at '{self.db_path}'. Starting with an empty database.\")\n            db_file.create()\n            self.data = {}\n        else:\n            with db_file as f:\n                # read_json safely loads the content.\n                self.data = f.read_json()\n                if not self.data:  # Handle case where file exists but is empty\n                    self.data = {}\n            print(\"Successfully initialized database.\")\n\n    except Exception as e:\n        print(f\"Warning: Could not initialize BlobDB from '{self.db_path}'. Error: {e}. Starting fresh.\")\n        self.data = {}\n\n    return Result.ok().set_origin(\"Blob Dict DB\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.local_instance","title":"<code>local_instance</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.DB.local_instance.load_from_json","title":"<code>load_from_json(filename)</code>","text":"<p>L\u00e4dt Daten aus einer JSON-Datei.</p> <p>:param filename: Der Dateiname oder Pfad der zu ladenden Datei. :return: Die geladenen Daten.</p> Source code in <code>toolboxv2/mods/DB/local_instance.py</code> <pre><code>def load_from_json(filename):\n    \"\"\"\n    L\u00e4dt Daten aus einer JSON-Datei.\n\n    :param filename: Der Dateiname oder Pfad der zu ladenden Datei.\n    :return: Die geladenen Daten.\n    \"\"\"\n    if not os.path.exists(filename):\n        return {'data': ''}\n\n    with open(filename) as file:\n        return json.load(file)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.local_instance.save_to_json","title":"<code>save_to_json(data, filename)</code>","text":"<p>Speichert die \u00fcbergebenen Daten in einer JSON-Datei.</p> <p>:param data: Die zu speichernden Daten. :param filename: Der Dateiname oder Pfad, in dem die Daten gespeichert werden sollen.</p> Source code in <code>toolboxv2/mods/DB/local_instance.py</code> <pre><code>def save_to_json(data, filename):\n    \"\"\"\n    Speichert die \u00fcbergebenen Daten in einer JSON-Datei.\n\n    :param data: Die zu speichernden Daten.\n    :param filename: Der Dateiname oder Pfad, in dem die Daten gespeichert werden sollen.\n    \"\"\"\n    if not os.path.exists(filename):\n        open(filename, 'a').close()\n\n    with open(filename, 'w+') as file:\n        json.dump(data, file, indent=4)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.reddis_instance","title":"<code>reddis_instance</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.DB.reddis_instance.sync_redis_databases","title":"<code>sync_redis_databases(source_url, target_url)</code>","text":"<p>Synchronize keys from the source Redis database to the target Redis database. This function scans all keys in the source DB and uses DUMP/RESTORE to replicate data to the target.</p> <p>Parameters:</p> Name Type Description Default <code>source_url</code> <code>str</code> <p>The Redis URL of the source database.</p> required <code>target_url</code> <code>str</code> <p>The Redis URL of the target database.</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The number of keys successfully synchronized.</p> Source code in <code>toolboxv2/mods/DB/reddis_instance.py</code> <pre><code>def sync_redis_databases(source_url, target_url):\n    \"\"\"Synchronize keys from the source Redis database to the target Redis database.\n    This function scans all keys in the source DB and uses DUMP/RESTORE to replicate data to the target.\n\n    Args:\n        source_url (str): The Redis URL of the source database.\n        target_url (str): The Redis URL of the target database.\n\n    Returns:\n        int: The number of keys successfully synchronized.\n    \"\"\"\n    try:\n        src_client = redis.from_url(source_url)\n        tgt_client = redis.from_url(target_url)\n    except Exception as e:\n        print(f\"Error connecting to one of the Redis instances: {e}\")\n        return 0\n\n    total_synced = 0\n    cursor = 0\n    try:\n        while True:\n            cursor, keys = src_client.scan(cursor=cursor, count=100)\n            for key in keys:\n                try:\n                    serialized_value = src_client.dump(key)\n                    if serialized_value is None:\n                        continue\n                    # Restore key with TTL=0 and replace existing key\n                    tgt_client.restore(key, 0, serialized_value, replace=True)\n                    total_synced += 1\n                except Exception as e:\n                    print(f\"Error syncing key {key}: {e}\")\n            if cursor == 0:\n                break\n    except Exception as scan_error:\n        print(f\"Error during scanning keys: {scan_error}\")\n\n    print(f\"Synced {total_synced} keys from {source_url} to {target_url}\")\n    return total_synced\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter","title":"<code>tb_adapter</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB","title":"<code>DB</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>class DB(ABC):\n    @abc.abstractmethod\n    def get(self, query: str) -&gt; Result:\n        \"\"\"get data\"\"\"\n\n    @abc.abstractmethod\n    def set(self, query: str, value) -&gt; Result:\n        \"\"\"set data\"\"\"\n\n    @abc.abstractmethod\n    def append_on_set(self, query: str, value) -&gt; Result:\n        \"\"\"append set data\"\"\"\n\n    @abc.abstractmethod\n    def delete(self, query: str, matching=False) -&gt; Result:\n        \"\"\"delete data\"\"\"\n\n    @abc.abstractmethod\n    def if_exist(self, query: str) -&gt; bool:\n        \"\"\"return True if query exists\"\"\"\n\n    @abc.abstractmethod\n    def exit(self) -&gt; Result:\n        \"\"\"Close DB connection and optional save data\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB.append_on_set","title":"<code>append_on_set(query, value)</code>  <code>abstractmethod</code>","text":"<p>append set data</p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>@abc.abstractmethod\ndef append_on_set(self, query: str, value) -&gt; Result:\n    \"\"\"append set data\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB.delete","title":"<code>delete(query, matching=False)</code>  <code>abstractmethod</code>","text":"<p>delete data</p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>@abc.abstractmethod\ndef delete(self, query: str, matching=False) -&gt; Result:\n    \"\"\"delete data\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB.exit","title":"<code>exit()</code>  <code>abstractmethod</code>","text":"<p>Close DB connection and optional save data</p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>@abc.abstractmethod\ndef exit(self) -&gt; Result:\n    \"\"\"Close DB connection and optional save data\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB.get","title":"<code>get(query)</code>  <code>abstractmethod</code>","text":"<p>get data</p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>@abc.abstractmethod\ndef get(self, query: str) -&gt; Result:\n    \"\"\"get data\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB.if_exist","title":"<code>if_exist(query)</code>  <code>abstractmethod</code>","text":"<p>return True if query exists</p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>@abc.abstractmethod\ndef if_exist(self, query: str) -&gt; bool:\n    \"\"\"return True if query exists\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.tb_adapter.DB.set","title":"<code>set(query, value)</code>  <code>abstractmethod</code>","text":"<p>set data</p> Source code in <code>toolboxv2/mods/DB/tb_adapter.py</code> <pre><code>@abc.abstractmethod\ndef set(self, query: str, value) -&gt; Result:\n    \"\"\"set data\"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.ui","title":"<code>ui</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.DB.ui.api_change_mode","title":"<code>api_change_mode(self, request)</code>  <code>async</code>","text":"<p>Changes the database mode from a JSON POST body.</p> Source code in <code>toolboxv2/mods/DB/ui.py</code> <pre><code>@export(mod_name=Name, name=\"api_change_mode\", api=True, api_methods=['POST'], request_as_kwarg=True)\nasync def api_change_mode(self, request: RequestData):\n    \"\"\"Changes the database mode from a JSON POST body.\"\"\"\n    data = request.body\n    if not data or \"mode\" not in data:\n        return Result.default_user_error(\"Request body must contain 'mode'.\")\n    new_mode = data.get(\"mode\", \"LC\")\n    return self.edit_programmable(DatabaseModes.crate(new_mode))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.ui.api_delete_key","title":"<code>api_delete_key(self, request)</code>  <code>async</code>","text":"<p>Deletes a key from a JSON POST body.</p> Source code in <code>toolboxv2/mods/DB/ui.py</code> <pre><code>@export(mod_name=Name, name=\"api_delete_key\", api=True, api_methods=['POST'], request_as_kwarg=True)\nasync def api_delete_key(self, request: RequestData):\n    \"\"\"Deletes a key from a JSON POST body.\"\"\"\n    data = request.body\n    if not data or 'key' not in data:\n        return Result.default_user_error(\"Request body must contain 'key'.\")\n    key = data['key']\n    if not key:\n        return Result.default_user_error(\"Key parameter is required.\")\n    return self.delete(key)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.ui.api_get_all_keys","title":"<code>api_get_all_keys(self, request)</code>  <code>async</code>","text":"<p>Returns a list of all keys in the database.</p> Source code in <code>toolboxv2/mods/DB/ui.py</code> <pre><code>@export(mod_name=Name, name=\"api_get_all_keys\", api=True, request_as_kwarg=True)\nasync def api_get_all_keys(self, request: RequestData):\n    \"\"\"Returns a list of all keys in the database.\"\"\"\n    if self.data_base:\n        keys_result = self.data_base.get('all-k')\n        if keys_result.is_error():\n            return keys_result\n\n        unwrapped_keys = _unwrap_data(keys_result.get())\n        if not isinstance(unwrapped_keys, list):\n            self.app.logger.warning(f\"get_all_keys did not return a list. Got: {type(unwrapped_keys)}\")\n            return Result.json(data=[])\n\n        return Result.json(data=sorted(unwrapped_keys))\n    return Result.default_internal_error(\"DB not initialized\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.ui.api_get_status","title":"<code>api_get_status(self, request)</code>  <code>async</code>","text":"<p>Returns the current status of the DB manager.</p> Source code in <code>toolboxv2/mods/DB/ui.py</code> <pre><code>@export(mod_name=Name, name=\"api_get_status\", api=True, request_as_kwarg=True)\nasync def api_get_status(self, request: RequestData):\n    \"\"\"Returns the current status of the DB manager.\"\"\"\n    return Result.json(data={\"mode\": self.mode})\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.ui.api_get_value","title":"<code>api_get_value(self, request, key)</code>  <code>async</code>","text":"<p>Gets a value for a key and returns it as JSON-friendly text.</p> Source code in <code>toolboxv2/mods/DB/ui.py</code> <pre><code>@export(mod_name=Name, name=\"api_get_value\", api=True, request_as_kwarg=True)\nasync def api_get_value(self, request: RequestData, key: str):\n    \"\"\"Gets a value for a key and returns it as JSON-friendly text.\"\"\"\n    if not key:\n        return Result.default_user_error(\"Key parameter is required.\")\n    value_res = self.get(key)\n    if value_res.is_error():\n        return value_res\n\n    value_unwrapped = _unwrap_data(value_res.get())\n\n    if isinstance(value_unwrapped, bytes):\n        try:\n            value_str = value_unwrapped.decode('utf-8')\n        except UnicodeDecodeError:\n            value_str = str(value_unwrapped)\n    else:\n        value_str = str(value_unwrapped)\n\n    # Simplified for a JSON-focused UI. The client will handle formatting.\n    return Result.json(data={\"key\": key, \"value\": value_str})\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.ui.api_set_value","title":"<code>api_set_value(self, request)</code>  <code>async</code>","text":"<p>Sets a key-value pair from a JSON POST body.</p> Source code in <code>toolboxv2/mods/DB/ui.py</code> <pre><code>@export(mod_name=Name, name=\"api_set_value\", api=True, api_methods=['POST'], request_as_kwarg=True)\nasync def api_set_value(self, request: RequestData):\n    \"\"\"Sets a key-value pair from a JSON POST body.\"\"\"\n    data = request.body\n    if not data or 'key' not in data or 'value' not in data:\n        return Result.default_user_error(\"Request body must contain 'key' and 'value'.\")\n    key = data['key']\n    value = data['value']\n    if not key:\n        return Result.default_user_error(\"Key cannot be empty.\")\n    return self.set(key, value)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.DB.ui.db_manager_ui","title":"<code>db_manager_ui(**kwargs)</code>","text":"<p>Serves the refactored, JSON-focused UI for the DB Manager.</p> Source code in <code>toolboxv2/mods/DB/ui.py</code> <pre><code>@export(mod_name=Name, name=\"ui\", api=True, state=False)\ndef db_manager_ui(**kwargs):\n    \"\"\"Serves the refactored, JSON-focused UI for the DB Manager.\"\"\"\n    html_content = \"\"\"\n    &lt;!DOCTYPE html&gt;\n    &lt;html lang=\"en\"&gt;\n    &lt;head&gt;\n        &lt;meta charset=\"UTF-8\"&gt;\n        &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n        &lt;title&gt;DB Manager&lt;/title&gt;\n        &lt;style&gt;\n            :root {\n                --font-family-sans: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif;\n                --font-family-mono: \"SF Mono\", \"Menlo\", \"Monaco\", \"Courier New\", Courier, monospace;\n                --color-bg: #f8f9fa;\n                --color-panel-bg: #ffffff;\n                --color-border: #dee2e6;\n                --color-text: #212529;\n                --color-text-muted: #6c757d;\n                --color-primary: #0d6efd;\n                --color-primary-hover: #0b5ed7;\n                --color-danger: #dc3545;\n                --color-danger-hover: #bb2d3b;\n                --color-key-folder-icon: #f7b731;\n                --color-key-file-icon: #adb5bd;\n                --color-key-hover-bg: #e9ecef;\n                --color-key-selected-bg: #0d6efd;\n                --color-key-selected-text: #ffffff;\n                --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\n                --radius: 0.375rem;\n            }\n\n            /* Basic styles */\n            * { box-sizing: border-box; }\n            html { font-size: 16px; }\n\n            body {\n                font-family: var(--font-family-sans);\n                background-color: var(--color-bg);\n                color: var(--color-text);\n                margin: 0;\n                padding: 1rem;\n                display: flex;\n                flex-direction: column;\n                height: 100vh;\n            }\n\n            /* Main layout */\n            .db-manager-container { display: flex; flex-direction: column; height: 100%; gap: 1rem; }\n            .db-header { display: flex; justify-content: space-between; align-items: center; padding-bottom: 1rem; border-bottom: 1px solid var(--color-border); flex-shrink: 0; }\n            .db-main-content { display: flex; gap: 1rem; flex: 1; min-height: 0; }\n\n            /* Panels */\n            .db-panel { background-color: var(--color-panel-bg); border: 1px solid var(--color-border); border-radius: var(--radius); box-shadow: var(--shadow-sm); display: flex; flex-direction: column; min-height: 0; }\n            .key-panel { width: 350px; min-width: 250px; max-width: 450px; }\n            .editor-panel, .placeholder-panel { flex-grow: 1; }\n            .panel-header { display: flex; justify-content: space-between; align-items: center; padding: 0.75rem 1rem; border-bottom: 1px solid var(--color-border); flex-shrink: 0; }\n            .panel-header h2 { font-size: 1.1rem; margin: 0; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; }\n\n            /* Controls */\n            select, input[type=\"text\"], textarea, button { font-size: 1rem; }\n            select, input[type=\"text\"] { background-color: var(--color-bg); color: var(--color-text); border: 1px solid var(--color-border); border-radius: var(--radius); padding: 0.5rem 0.75rem; }\n            select:focus, input[type=\"text\"]:focus, textarea:focus { outline: 2px solid var(--color-primary); outline-offset: -1px; }\n            button { border: none; border-radius: var(--radius); padding: 0.5rem 1rem; font-weight: 500; cursor: pointer; transition: background-color 0.2s; }\n            button.primary { background-color: var(--color-primary); color: white; }\n            button.primary:hover { background-color: var(--color-primary-hover); }\n            button.danger { background-color: var(--color-danger); color: white; }\n            button.danger:hover { background-color: var(--color-danger-hover); }\n            .header-actions { display: flex; gap: 0.5rem; }\n\n            /* Key Tree View */\n            #keySearchInput { width: calc(100% - 2rem); margin: 1rem; flex-shrink: 0; }\n            .key-tree-container { font-family: var(--font-family-mono); font-size: 0.9rem; padding: 0 0.5rem 1rem; overflow-y: auto; flex: 1; min-height: 0; }\n            .key-tree-container ul { list-style: none; padding-left: 0; margin: 0; }\n            .key-tree-container li { padding-left: 20px; position: relative; }\n            .node-label { display: flex; align-items: center; padding: 4px 8px; cursor: pointer; border-radius: 4px; word-break: break-all; user-select: none; }\n            .node-label:hover { background-color: var(--color-key-hover-bg); }\n            .node-label.selected { background-color: var(--color-key-selected-bg); color: var(--color-key-selected-text); }\n            .node-label.selected .node-icon { color: var(--color-key-selected-text) !important; }\n            .node-icon { width: 20px; text-align: center; margin-right: 5px; flex-shrink: 0; }\n            .tree-folder &gt; .node-label .node-icon { color: var(--color-key-folder-icon); font-style: normal; }\n            .tree-folder &gt; .node-label .node-icon::before { content: '\u25b8'; display: inline-block; transition: transform 0.15s ease-in-out; }\n            .tree-folder.open &gt; .node-label .node-icon::before { transform: rotate(90deg); }\n            .tree-leaf &gt; .node-label .node-icon { color: var(--color-key-file-icon); }\n            .tree-leaf &gt; .node-label .node-icon::before { content: '\u2022'; }\n            .tree-children { display: none; }\n            .tree-folder.open &gt; .tree-children { display: block; }\n\n            /* Editor Panel */\n            .editor-toolbar { display: flex; gap: 1rem; align-items: center; padding: 0.75rem 1rem; border-bottom: 1px solid var(--color-border); flex-shrink: 0; }\n            #valueEditor { flex: 1; width: 100%; min-height: 0; border: none; resize: none; font-family: var(--font-family-mono); font-size: 0.95rem; line-height: 1.5; padding: 1rem; background: transparent; color: var(--color-text); }\n            #valueEditor:focus { outline: none; }\n\n            /* Placeholder and Utility */\n            .placeholder-panel { display: flex; flex-direction: column; align-items: center; justify-content: center; color: var(--color-text-muted); text-align: center; }\n            .hidden { display: none !important; }\n            .key-tree-container p.status-message { padding: 1rem; margin: 0; color: var(--color-text-muted); text-align: center; }\n\n            /* Custom Scrollbars */\n            .key-tree-container::-webkit-scrollbar, #valueEditor::-webkit-scrollbar { width: 8px; height: 8px; }\n            .key-tree-container::-webkit-scrollbar-track, #valueEditor::-webkit-scrollbar-track { background: transparent; }\n            .key-tree-container::-webkit-scrollbar-thumb, #valueEditor::-webkit-scrollbar-thumb { background-color: var(--color-border); border-radius: 4px; }\n            .key-tree-container::-webkit-scrollbar-thumb:hover, #valueEditor::-webkit-scrollbar-thumb:hover { background-color: var(--color-text-muted); }\n            #valueEditor::-webkit-scrollbar-corner { background: transparent; }\n\n            /* Responsive */\n            @media (max-width: 768px) {\n                body { padding: 0.5rem; }\n                .db-main-content { flex-direction: column; }\n                .key-panel { width: 100%; max-height: 40vh; }\n            }\n        &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;div id=\"dbManagerContainer\" class=\"db-manager-container\"&gt;\n            &lt;header class=\"db-header\"&gt;\n                &lt;h1&gt;DB Manager&lt;/h1&gt;\n                &lt;div class=\"db-mode-selector\"&gt;\n                    &lt;label for=\"modeSelect\"&gt;Mode:&lt;/label&gt;\n                    &lt;select id=\"modeSelect\"&gt;\n                        &lt;option value=\"LC\"&gt;Local Dict&lt;/option&gt;\n                        &lt;option value=\"CB\"&gt;Cloud Blob&lt;/option&gt;\n                        &lt;option value=\"LR\"&gt;Local Redis&lt;/option&gt;\n                        &lt;option value=\"RR\"&gt;Remote Redis&lt;/option&gt;\n                    &lt;/select&gt;\n                &lt;/div&gt;\n            &lt;/header&gt;\n            &lt;main class=\"db-main-content\"&gt;\n                &lt;aside id=\"keyPanel\" class=\"db-panel key-panel\"&gt;\n                    &lt;div class=\"panel-header\"&gt;\n                        &lt;h2&gt;Keys&lt;/h2&gt;\n                        &lt;div class=\"header-actions\"&gt;\n                            &lt;button id=\"addKeyBtn\" title=\"Add New Key\" style=\"font-size: 1.2rem;\"&gt;+&lt;/button&gt;\n                            &lt;button id=\"refreshKeysBtn\" title=\"Refresh Keys\"&gt;\ud83d\udd04&lt;/button&gt;\n                        &lt;/div&gt;\n                    &lt;/div&gt;\n                    &lt;input type=\"text\" id=\"keySearchInput\" placeholder=\"Search keys...\"&gt;\n                    &lt;div id=\"keyTreeContainer\" class=\"key-tree-container\"&gt;&lt;/div&gt;\n                &lt;/aside&gt;\n                &lt;section id=\"editorPanel\" class=\"db-panel editor-panel hidden\"&gt;\n                    &lt;div class=\"panel-header\"&gt;\n                        &lt;h2 id=\"selectedKey\"&gt;&lt;/h2&gt;\n                        &lt;div class=\"header-actions\"&gt;\n                            &lt;button id=\"saveBtn\" class=\"primary\"&gt;Save&lt;/button&gt;\n                            &lt;button id=\"deleteBtn\" class=\"danger\"&gt;Delete&lt;/button&gt;\n                        &lt;/div&gt;\n                    &lt;/div&gt;\n                    &lt;div class=\"editor-toolbar\"&gt;\n                        &lt;button id=\"formatBtn\"&gt;Format JSON&lt;/button&gt;\n                    &lt;/div&gt;\n                    &lt;textarea id=\"valueEditor\" placeholder=\"Select a key to view its value...\"&gt;&lt;/textarea&gt;\n                &lt;/section&gt;\n                &lt;section id=\"placeholderPanel\" class=\"db-panel editor-panel placeholder-panel\"&gt;\n                    &lt;h3&gt;Select a key to get started&lt;/h3&gt;\n                    &lt;p&gt;Or click the '+' button to add a new one.&lt;/p&gt;\n                &lt;/section&gt;\n            &lt;/main&gt;\n        &lt;/div&gt;\n        &lt;script&gt;\n        (() =&gt; {\n            \"use strict\";\n            const API_NAME = \"DB\";\n\n            class DBManager {\n                constructor() {\n                    this.cache = {\n                        keys: [],\n                        selectedKey: null\n                    };\n                    this.dom = {\n                        modeSelect: document.getElementById('modeSelect'),\n                        keySearchInput: document.getElementById('keySearchInput'),\n                        keyTreeContainer: document.getElementById('keyTreeContainer'),\n                        editorPanel: document.getElementById('editorPanel'),\n                        placeholderPanel: document.getElementById('placeholderPanel'),\n                        selectedKey: document.getElementById('selectedKey'),\n                        valueEditor: document.getElementById('valueEditor'),\n                        addKeyBtn: document.getElementById('addKeyBtn'),\n                        refreshKeysBtn: document.getElementById('refreshKeysBtn'),\n                        saveBtn: document.getElementById('saveBtn'),\n                        deleteBtn: document.getElementById('deleteBtn'),\n                        formatBtn: document.getElementById('formatBtn'),\n                    };\n                    this.init();\n                }\n\n                async init() {\n                    this.setStatusMessage('Loading...');\n                    this.addEventListeners();\n                    await this.loadInitialStatus();\n                    await this.loadKeys();\n                }\n\n                addEventListeners() {\n                    this.dom.refreshKeysBtn.addEventListener('click', () =&gt; this.loadKeys());\n                    this.dom.addKeyBtn.addEventListener('click', () =&gt; this.showAddKeyModal());\n                    this.dom.saveBtn.addEventListener('click', () =&gt; this.saveValue());\n                    this.dom.deleteBtn.addEventListener('click', () =&gt; this.confirmDeleteKey());\n                    this.dom.formatBtn.addEventListener('click', () =&gt; this.formatJson());\n                    this.dom.keySearchInput.addEventListener('input', (e) =&gt; this.renderKeyTree(e.target.value));\n                    this.dom.modeSelect.addEventListener('change', (e) =&gt; this.changeMode(e.target.value));\n\n                    this.dom.keyTreeContainer.addEventListener('click', (e) =&gt; {\n                        const label = e.target.closest('.node-label');\n                        if (!label) return;\n                        const node = label.parentElement;\n                        if (node.classList.contains('tree-folder')) {\n                            node.classList.toggle('open');\n                        } else if (node.dataset.key) {\n                            this.selectKey(node.dataset.key);\n                        }\n                    });\n                }\n\n                async apiRequest(endpoint, payload = null, method = 'POST') {\n                    if (!window.TB?.api?.request) {\n                        console.error(\"TB.api not available!\");\n                        return { error: true, message: \"TB.api not available\" };\n                    }\n                    try {\n                        const url = (method === 'GET' &amp;&amp; payload) ? `${endpoint}?${new URLSearchParams(payload)}` : endpoint;\n                        const body = (method !== 'GET') ? payload : null;\n                        const response = await window.TB.api.request(API_NAME, url, body, method);\n\n                        if (response.error &amp;&amp; response.error !== 'none') {\n                            const errorMsg = response.info?.help_text || response.error;\n                            console.error(`API Error on ${endpoint}:`, errorMsg, response);\n                            if (window.TB?.ui?.Toast) TB.ui.Toast.showError(errorMsg, { duration: 5000 });\n                            return { error: true, message: errorMsg, data: response.get() };\n                        }\n                        return { error: false, data: response.get() };\n                    } catch (err) {\n                        console.error(\"Framework/Network Error:\", err);\n                        if (window.TB?.ui?.Toast) TB.ui.Toast.showError(\"Application or network error.\", { duration: 5000 });\n                        return { error: true, message: \"Network error\" };\n                    }\n                }\n\n                async loadInitialStatus() {\n                    const res = await this.apiRequest('api_get_status', null, 'GET');\n                    if (!res.error) this.dom.modeSelect.value = res.data.mode;\n                }\n\n                async loadKeys() {\n                    this.setStatusMessage('Loading keys...');\n                    const res = await this.apiRequest('api_get_all_keys', null, 'GET');\n                    if (!res.error) {\n                        this.cache.keys = res.data || [];\n                        this.renderKeyTree();\n                    } else {\n                        this.setStatusMessage('Failed to load keys.', true);\n                    }\n                }\n\n                renderKeyTree(filter = '') {\n                    const treeData = {};\n                    const filteredKeys = this.cache.keys.filter(k =&gt; k.toLowerCase().includes(filter.toLowerCase().trim()));\n\n                    for (const key of filteredKeys) {\n                        let currentLevel = treeData;\n                        const parts = key.split(':');\n                        for (let i = 0; i &lt; parts.length; i++) {\n                            const part = parts[i];\n                            if (!part) continue; // Skip empty parts from keys like \"a::b\"\n                            const isLeaf = i === parts.length - 1;\n\n                            if (!currentLevel[part]) {\n                                currentLevel[part] = { _children: {} };\n                            }\n                            if (isLeaf) {\n                                currentLevel[part]._fullKey = key;\n                            }\n                            currentLevel = currentLevel[part]._children;\n                        }\n                    }\n\n                    const treeHtml = this.buildTreeHtml(treeData);\n                    if (treeHtml) {\n                        this.dom.keyTreeContainer.innerHTML = `&lt;ul class=\"key-tree\"&gt;${treeHtml}&lt;/ul&gt;`;\n                        // Re-select the key if it's still visible\n                        if (this.cache.selectedKey) {\n                             const nodeEl = this.dom.keyTreeContainer.querySelector(`[data-key=\"${this.cache.selectedKey}\"] .node-label`);\n                             if(nodeEl) nodeEl.classList.add('selected');\n                        }\n                    } else {\n                         this.setStatusMessage(filter ? 'No keys match your search.' : 'No keys found.');\n                    }\n                }\n\n                buildTreeHtml(node) {\n                    return Object.keys(node).sort().map(key =&gt; {\n                        const childNode = node[key];\n                        const isFolder = Object.keys(childNode._children).length &gt; 0;\n\n                        if (isFolder) {\n                            return `&lt;li class=\"tree-folder\" ${childNode._fullKey ? `data-key=\"${childNode._fullKey}\"`: ''}&gt;\n                                        &lt;div class=\"node-label\"&gt;&lt;i class=\"node-icon\"&gt;&lt;/i&gt;${key}&lt;/div&gt;\n                                        &lt;ul class=\"tree-children\"&gt;${this.buildTreeHtml(childNode._children)}&lt;/ul&gt;\n                                    &lt;/li&gt;`;\n                        } else {\n                            return `&lt;li class=\"tree-leaf\" data-key=\"${childNode._fullKey}\"&gt;\n                                        &lt;div class=\"node-label\"&gt;&lt;i class=\"node-icon\"&gt;&lt;/i&gt;${key}&lt;/div&gt;\n                                    &lt;/li&gt;`;\n                        }\n                    }).join('');\n                }\n\n                async selectKey(key) {\n                    if (!key) return;\n                    this.showEditor(true);\n                    this.cache.selectedKey = key;\n\n                    document.querySelectorAll('.node-label.selected').forEach(el =&gt; el.classList.remove('selected'));\n                    const nodeEl = this.dom.keyTreeContainer.querySelector(`[data-key=\"${key}\"] &gt; .node-label`);\n                    if (nodeEl) nodeEl.classList.add('selected');\n\n                    this.dom.selectedKey.textContent = key;\n                    this.dom.selectedKey.title = key;\n                    this.dom.valueEditor.value = \"Loading...\";\n\n                    const res = await this.apiRequest('api_get_value', { key }, 'GET');\n                    this.dom.valueEditor.value = res.error ? `Error: ${res.message}` : res.data.value;\n                    if (!res.error) this.formatJson(false); // Auto-format if it's valid JSON, without showing an error\n                }\n\n                async saveValue() {\n                    if (!this.cache.selectedKey) return;\n                    if (window.TB?.ui?.Loader) TB.ui.Loader.show(\"Saving...\");\n                    const res = await this.apiRequest('api_set_value', {\n                        key: this.cache.selectedKey,\n                        value: this.dom.valueEditor.value\n                    });\n                    if (window.TB?.ui?.Loader) TB.ui.Loader.hide();\n                    if (!res.error &amp;&amp; window.TB?.ui?.Toast) TB.ui.Toast.showSuccess(\"Key saved successfully!\");\n                }\n\n                async confirmDeleteKey() {\n                    if (!this.cache.selectedKey) return;\n                    if (!window.TB?.ui?.Modal) {\n                        if(confirm(`Delete key \"${this.cache.selectedKey}\"?`)) this.deleteKey();\n                        return;\n                    }\n                    TB.ui.Modal.confirm({\n                        title: 'Delete Key?',\n                        content: `Are you sure you want to delete the key \"&lt;strong&gt;${this.cache.selectedKey}&lt;/strong&gt;\"?&lt;br/&gt;This action cannot be undone.`,\n                        confirmButtonText: 'Delete',\n                        confirmButtonVariant: 'danger',\n                        onConfirm: () =&gt; this.deleteKey()\n                    });\n                }\n\n                async deleteKey() {\n                    const keyToDelete = this.cache.selectedKey;\n                    if (!keyToDelete) return;\n                    if (window.TB?.ui?.Loader) TB.ui.Loader.show(\"Deleting...\");\n                    const res = await this.apiRequest('api_delete_key', { key: keyToDelete });\n                    if (window.TB?.ui?.Loader) TB.ui.Loader.hide();\n\n                    if (!res.error) {\n                        if (window.TB?.ui?.Toast) TB.ui.Toast.showSuccess(`Key \"${keyToDelete}\" deleted.`);\n                        this.cache.selectedKey = null;\n                        this.showEditor(false);\n                        this.loadKeys(); // Refresh the key list\n                    }\n                }\n\n                formatJson(showErrorToast = true) {\n                    try {\n                        const currentVal = this.dom.valueEditor.value.trim();\n                        if (!currentVal) return;\n                        const formatted = JSON.stringify(JSON.parse(currentVal), null, 2);\n                        this.dom.valueEditor.value = formatted;\n                    } catch (e) {\n                        if (showErrorToast &amp;&amp; window.TB?.ui?.Toast) {\n                            TB.ui.Toast.showWarning(\"Value is not valid JSON.\", { duration: 3000 });\n                        }\n                    }\n                }\n\n                showAddKeyModal() {\n                     if (!window.TB?.ui?.Modal) { alert(\"Add Key modal not available.\"); return; }\n                     TB.ui.Modal.show({\n                        title: 'Add New Key',\n                        content: `&lt;input type=\"text\" id=\"newKeyInput\" placeholder=\"Enter new key name (e.g., app:settings:user)\" style=\"width: 100%; margin-bottom: 1rem;\"/&gt;\n                                  &lt;textarea id=\"newValueInput\" placeholder='Enter value (e.g., {\"theme\": \"dark\"})' style=\"width: 100%; height: 150px; font-family: var(--font-family-mono);\"&gt;&lt;/textarea&gt;`,\n                        onOpen: (modal) =&gt; document.getElementById('newKeyInput').focus(),\n                        buttons: [{\n                            text: 'Save', variant: 'primary',\n                            action: async (modal) =&gt; {\n                                const newKey = document.getElementById('newKeyInput').value.trim();\n                                const newValue = document.getElementById('newValueInput').value;\n                                if (!newKey) { if (window.TB?.ui?.Toast) TB.ui.Toast.showError(\"Key name cannot be empty.\"); return; }\n                                modal.close();\n                                if (window.TB?.ui.Loader) TB.ui.Loader.show(\"Saving...\");\n                                const res = await this.apiRequest('api_set_value', { key: newKey, value: newValue });\n                                if (window.TB?.ui.Loader) TB.ui.Loader.hide();\n                                if (!res.error) {\n                                    if (window.TB?.ui?.Toast) TB.ui.Toast.showSuccess(\"New key created!\");\n                                    await this.loadKeys();\n                                    this.selectKey(newKey);\n                                }\n                            }\n                        }, { text: 'Cancel', action: (modal) =&gt; modal.close() }]\n                    });\n                }\n\n                async changeMode(newMode) {\n                    if (window.TB?.ui?.Loader) TB.ui.Loader.show(`Switching to ${newMode}...`);\n                    const res = await this.apiRequest('api_change_mode', { mode: newMode });\n                    if (!res.error) {\n                       this.cache.selectedKey = null;\n                       this.showEditor(false);\n                       await this.loadKeys();\n                       if (window.TB?.ui?.Toast) TB.ui.Toast.showSuccess(`Switched to ${newMode} mode.`);\n                    } else {\n                       if (window.TB?.ui?.Toast) TB.ui.Toast.showError(`Failed to switch mode.`);\n                       await this.loadInitialStatus(); // Revert dropdown to actual status\n                    }\n                    if (window.TB?.ui?.Loader) TB.ui.Loader.hide();\n                }\n\n                showEditor(show) {\n                    this.dom.editorPanel.classList.toggle('hidden', !show);\n                    this.dom.placeholderPanel.classList.toggle('hidden', show);\n                }\n\n                setStatusMessage(message, isError = false) {\n                    this.dom.keyTreeContainer.innerHTML = `&lt;p class=\"status-message\" style=\"${isError ? 'color: var(--color-danger);' : ''}\"&gt;${message}&lt;/p&gt;`;\n                }\n            }\n\n            // Defer initialization until the ToolboxV2 framework is ready\n\n             function onTbReady() { new DBManager(); }\n             if (window.TB?.events) {\n    if (window.TB.config?.get('appRootId')) { // A sign that TB.init might have run\n         onTbReady();\n    } else {\n        window.TB.events.on('tbjs:initialized', onTbReady, { once: true });\n    }\n} else {\n    // Fallback if TB is not even an object yet, very early load\n    document.addEventListener('tbjs:initialized', onTbReady, { once: true }); // Custom event dispatch from TB.init\n}\n\n        })();\n        &lt;/script&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \"\"\"\n    app = get_app(Name)\n    try:\n        # Prepend the web context to include necessary framework scripts (like TB.js)\n        web_context = app.web_context()\n        return Result.html(web_context + html_content)\n    except Exception as e:\n        # Fallback in case web_context is not available\n        return Result.html(html_content)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.EventManager","title":"<code>EventManager</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.EventManager.module","title":"<code>module</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.EventManager.module.EventManagerClass","title":"<code>EventManagerClass</code>","text":"Source code in <code>toolboxv2/mods/EventManager/module.py</code> <pre><code>class EventManagerClass:\n    events: set[Event] = set()\n    source_id: str\n    _name: str\n    _identification: str\n\n    routes_client: dict[str, ProxyRout] = {}\n    routers_servers: dict[str, DaemonRout] = {}\n    routers_servers_tasks: list[Any] = []\n    routers_servers_tasks_running_flag: bool = False\n\n    receiver_que: queue.Queue\n    response_que: queue.Queue\n\n    def add_c_route(self, name, route: ProxyRout):\n        self.routes_client[name] = route\n\n    async def receive_all_client_data(self):\n\n        close_connections = []\n        add_ev = []\n        for name, client in self.routes_client.items():\n            if client.client is None or not client.client.get('alive', False):\n                close_connections.append(name)\n                continue\n            data = client.r\n\n            if isinstance(data, str) and data == \"No data\":\n                continue\n            elif isinstance(data, EventID) and len(data.get_source()) != 0:\n                await self.trigger_event(data)\n            elif isinstance(data, EventID) and len(data.get_source()) == 0:\n                print(f\"Event returned {data.payload}\")\n                self.response_que.put(data)\n            elif isinstance(data,\n                            dict) and 'error' in data and 'origin' in data and 'result' in data and 'info' in data:\n\n                self.response_que.put(Result.result_from_dict(**data).print())\n            elif isinstance(data,\n                            dict) and 'source' in data and 'path' in data and 'ID' in data and 'identifier' in data:\n                del data['identifier']\n                ev_id = EventID(**data)\n                await self.trigger_event(ev_id)\n            elif isinstance(data, Event):\n                print(\"Event:\", str(data.event_id), data.name)\n                add_ev.append(data)\n            elif isinstance(data, Result):\n                self.response_que.put(data.print())\n            else:\n                print(f\"Unknown Data {data}\")\n\n        for ev in add_ev:\n            await self.register_event(ev)\n\n        for client_name in close_connections:\n            print(f\"Client {client_name} closing connection\")\n            self.remove_c_route(client_name)\n\n    def remove_c_route(self, name):\n        self.routes_client[name].close()\n        del self.routes_client[name]\n\n    def crate_rout(self, source, addr=None):\n        if addr is None:\n            addr = ('0.0.0.0', 6588)\n        host, port = addr\n        if isinstance(port, str):\n            port = int(port)\n        return Rout(\n            _from=self.source_id,\n            _to=source,\n            _from_port=int(os.getenv(\"TOOLBOXV2_BASE_PORT\", 6588)),\n            _from_host=os.getenv(\"TOOLBOXV2_BASE_HOST\"),\n            _to_port=port,\n            _to_host=host,\n            routing_function=self.routing_function_router,\n        )\n\n    def __init__(self, source_id, _identification=\"PN\"):\n        self.bo = False\n        self.running = False\n        self.source_id = source_id\n        self.receiver_que = queue.Queue()\n        self.response_que = queue.Queue()\n        self._identification = _identification\n        self._name = self._identification + '-' + str(uuid.uuid4()).split('-')[1]\n        self.routes = {}\n        self.logger = get_logger()\n\n    @property\n    def identification(self) -&gt; str:\n        return self._identification\n\n    @identification.setter\n    def identification(self, _identification: str):\n        self.stop()\n        self._identification = _identification\n        self._name = self._identification + '-' + str(uuid.uuid4()).split('-')[1]\n\n    async def identity_post_setter(self):\n\n        do_reconnect = len(list(self.routers_servers.keys())) &gt; 0\n        if self._identification == \"P0\":\n            await self.add_server_route(self._identification, ('0.0.0.0', 6568))\n        if self._identification == \"P0|S0\":\n            await self.add_server_route(self._identification, ('0.0.0.0', 6567))\n\n        await asyncio.sleep(0.1)\n        self.start()\n        await asyncio.sleep(0.1)\n        if do_reconnect:\n            self.reconnect(\"ALL\")\n\n    async def open_connection_server(self, port):\n        await self.add_server_route(self._identification, ('0.0.0.0', port))\n\n    def start(self):\n        self.running = True\n        threading.Thread(target=async_test(self.receiver), daemon=True).start()\n\n    def make_event_from_fuction(self, fuction, name, *args, source_types=SourceTypes.F,\n                                scope=Scope.local,\n                                exec_in=ExecIn.local,\n                                threaded=False, **kwargs):\n\n        return Event(source=fuction,\n                     name=name,\n                     event_id=EventID.crate_with_source(self.source_id), args=args,\n                     kwargs_=kwargs,\n                     source_types=source_types,\n                     scope=scope,\n                     exec_in=exec_in,\n                     threaded=threaded,\n                     )\n\n    async def add_client_route(self, source_id, addr):\n        if source_id in self.routes_client:\n            if self.routes_client[source_id].client is None or not self.routes_client[source_id].client.get('alive'):\n                await self.routes_client[source_id].reconnect()\n                return True\n            print(\"Already connected\")\n            return False\n        try:\n            pr = await ProxyRout.toProxy(rout=self.crate_rout(source_id, addr=addr), name=source_id)\n            await asyncio.sleep(0.1)\n            await pr.client.get('sender')({\"id\": self._identification,\n                                           \"continue\": False,\n                                           \"key\": os.getenv('TB_R_KEY', 'root@remote')})\n            await asyncio.sleep(0.1)\n            self.add_c_route(source_id, pr)\n            return True\n        except Exception as e:\n            print(f\"Check the port {addr} Sever likely not Online : {e}\")\n            return False\n\n    async def add_mini_client(self, name: str, addr: tuple[str, int]):\n\n        mini_proxy = await ProxyRout(class_instance=None, timeout=15, app=get_app(),\n                                     remote_functions=[\"\"], peer=False, name=name, do_connect=False)\n\n        async def _(x):\n            return await self.routers_servers[self._identification].send(x, addr)\n\n        mini_proxy.put_data = _\n        mini_proxy.connect = lambda *x, **_: None\n        mini_proxy.reconnect = lambda *x, **_: None\n        mini_proxy.close = lambda *x, **_: None\n        mini_proxy.client = {'alive': True}\n        mini_proxy.r = \"No data\"\n        self.routes_client[name] = mini_proxy\n\n    async def on_register(self, id_, data):\n        try:\n            if \"unknown\" not in self.routes:\n                self.routes[\"unknown\"] = {}\n\n            if id_ != \"new_con\" and 'id' in data:\n                id_data = data.get('id')\n                id_ = eval(id_)\n                c_host, c_pot = id_\n                print(f\"Registering: new client {id_data} : {c_host, c_pot}\")\n                if id_data not in self.routes_client:\n                    await self.add_mini_client(id_data, (c_host, c_pot))\n                    self.routes[str((c_host, c_pot))] = id_data\n\n            # print(\"self.routes:\", self.routes)\n        except Exception as e:\n            print(\"Error in on_register\", str(e))\n\n    def on_client_exit(self, id_):\n\n        if isinstance(id_, str):\n            id_ = eval(id_)\n\n        c_name = self.routes.get(id_)\n\n        if c_name is None:\n            return\n\n        if c_name in self.routes_client:\n            self.remove_c_route(c_name)\n            print(f\"Removed route to {c_name}\")\n\n    async def add_server_route(self, source_id, addr=None):\n        if addr is None:\n            addr = ('0.0.0.0', 6588)\n        try:\n            self.routers_servers[source_id] = await DaemonRout(rout=self.crate_rout(source_id, addr=addr),\n                                                               name=source_id,\n                                                               on_r=self.on_register)\n            self.routers_servers_tasks.append(self.routers_servers[source_id].online)\n        except Exception as e:\n            print(f\"Sever already Online : {e}\")\n\n        if not self.routers_servers_tasks_running_flag:\n            self.routers_servers_tasks_running_flag = True\n            threading.Thread(target=self.server_route_runner, daemon=True).start()\n\n    def server_route_runner(self):\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n\n        # Sammle alle Ergebnisse zusammen\n        results = loop.run_until_complete(asyncio.gather(*self.routers_servers_tasks))\n\n        for result in results:\n            print(result)\n\n        loop.close()\n        self.routers_servers_tasks_running_flag = False\n\n    async def add_js_route(self, source_id=\"js:web\"):\n        await self.add_server_route(source_id, (\"./web/scripts/tb_socket.sock\", 0))\n\n    async def register_event(self, event: Event):\n\n        if event in self.events:\n            return Result.default_user_error(\"Event registration failed Event already registered\")\n\n        print(f\"Registration new Event : {event.name}, {str(event.event_id)}\")\n        self.events.add(event)\n\n        if event.scope.name == Scope.instance.name:\n            return\n\n        if event.scope.name == Scope.local.name:\n            if not self.bo and \"P0\" not in self.routes_client and os.getenv(\"TOOLBOXV2_BASE_HOST\",\n                                                                            \"localhost\") != \"localhost\":\n                await self.add_client_route(\"P0\", (os.getenv(\"TOOLBOXV2_BASE_HOST\", \"localhost\"),\n                                                   os.getenv(\"TOOLBOXV2_BASE_PORT\", 6568)))\n                self.bo = True\n            return\n\n        if event.scope.name == Scope.local_network.name:\n            if self.identification == \"P0\" and not self.bo:\n                t0 = threading.Thread(target=self.start_brodcast_router_local_network, daemon=True)\n                t0.start()\n            elif not self.bo and \"P0\" not in self.routes_client and os.getenv(\"TOOLBOXV2_BASE_HOST\",\n                                                                              \"localhost\") == \"localhost\":\n                self.bo = True\n                # self.add_server_route(self.identification, (\"127.0.0.1\", 44667))\n                with Spinner(message=\"Sercheing for Rooter instance\", count_down=True, time_in_s=6):\n                    with ThreadPoolExecutor(max_workers=1) as executor:\n                        t0 = executor.submit(make_known, self.identification)\n                        try:\n                            data = t0.result(timeout=6)\n                        except TimeoutError:\n                            print(\"No P0 found in network or on device\")\n                            return\n                    print(f\"Found P0 on {type(data)} {data.get('host')}\")\n                    await self.add_client_route(\"P0\", (data.get(\"host\"), os.getenv(\"TOOLBOXV2_BASE_PORT\", 6568)))\n            elif not self.bo and \"P0\" not in self.routes_client and os.getenv(\"TOOLBOXV2_BASE_HOST\",\n                                                                              \"localhost\") != \"localhost\":\n                do = await self.add_client_route(\"P0\", (\n                    os.getenv(\"TOOLBOXV2_BASE_HOST\", \"localhost\"), os.getenv(\"TOOLBOXV2_BASE_PORT\", 6568)))\n                self.bo = do\n                if not do:\n                    print(\"Connection failed\")\n                    os.environ[\"TOOLBOXV2_BASE_HOST\"] = \"localhost\"\n\n        if event.scope.name == Scope.global_network.name:\n            await self.add_server_route(self.source_id, ('0.0.0.0', os.getenv(\"TOOLBOXV2_REMOTE_PORT\", 6587)))\n\n    async def connect_to_remote(self, host=os.getenv(\"TOOLBOXV2_REMOTE_IP\"),\n                                port=os.getenv(\"TOOLBOXV2_REMOTE_PORT\", 6587)):\n        await self.add_client_route(\"S0\", (host, port))\n\n    def start_brodcast_router_local_network(self):\n        self.bo = True\n\n        # print(\"Starting brodcast router 0\")\n        router = start_client(get_local_ip())\n        # print(\"Starting brodcast router 1\")\n        # next(router)\n        # print(\"Starting brodcast router\")\n        while self.running:\n            source_id, connection = next(router)\n            print(f\"Infos :{source_id}, connection :{connection}\")\n            self.routes[source_id] = connection[0]\n            router.send(self.running)\n\n        router.send(\"e\")\n        router.close()\n\n    def _get_event_by_id_or_name(self, event_id: str or EventID):\n        if isinstance(event_id, str):\n            events = [e for e in self.events if e.name == event_id]\n            if len(events) &lt; 1:\n                return Result.default_user_error(\"Event not registered\")\n            event = events[0]\n\n        elif isinstance(event_id, EventID):\n            events = [e for e in self.events if e.event_id.ID == event_id.ID]\n            if len(events) &lt; 1:\n                events = [e for e in self.events if e.name == event_id.ID]\n            if len(events) &lt; 1:\n                return Result.default_user_error(\"Event not registered\")\n            event = events[0]\n\n        elif isinstance(event_id, Event):\n            if event_id not in self.events:\n                return Result.default_user_error(\"Event not registered\")\n            event = event_id\n\n        else:\n            event = Result.default_user_error(\"Event not registered\")\n\n        return event\n\n    def remove_event(self, event: Event or EventID or str):\n\n        event = self._get_event_by_id_or_name(event)\n        if isinstance(event, Event):\n            self.events.remove(event)\n        else:\n            return event\n\n    async def _trigger_local(self, event_id: EventID):\n        \"\"\"\n        Exec source based on\n\n        source_types\n            F -&gt; call directly\n            R -&gt; use get_app(str(event_id)).run_any(*args, **kwargs)\n            S -&gt; evaluate string\n        scope\n            instance -&gt; _trigger_local\n            local -&gt; if you ar proxy app run the event through get_app(str(event_id)).run_any(TBEF.EventManager._trigger_local, args=args, kwargs=kwargs, get_result=True)\n            local_network -&gt; use proxy0 app to communicate withe Daemon0 then local\n            global_network -&gt;\n        exec_in\n        event_id\n        threaded\n\n                       \"\"\"\n        event = self._get_event_by_id_or_name(event_id)\n\n        if isinstance(event, Result):\n            event.print()\n            if self.identification == \"P0\":\n                return event\n            print(f\"Routing to P0 {self.events}\")\n            if self.source_id not in self.routes_client:\n                # self.routers[self.source_id] = DaemonRout(rout=self.crate_rout(self.source_id))\n                await self.add_client_route(\"P0\", ('127.0.0.1', 6568))\n            return await self.route_event_id(event_id)\n\n        # if event.threaded:\n        #    threading.Thread(target=self.runner, args=(event, event_id), daemon=True).start()\n        #    return \"Event running In Thread\"\n        # else:\n\n        return await self.runner(event, event_id)\n\n    async def runner(self, event, event_id: EventID):\n\n        if event.kwargs_ is None:\n            event.kwargs_ = {}\n        if event.args is None:\n            event.args = []\n\n        if event.source_types.name is SourceTypes.P.name:\n            return event.source(*event.args, payload=event_id, **event.kwargs_)\n\n        if event.source_types.name is SourceTypes.F.name:\n            return event.source(*event.args, **event.kwargs_)\n\n        if event.source_types.name is SourceTypes.R.name:\n            return get_app(str(event_id)).run_any(mod_function_name=event.source, get_results=True, args_=event.args,\n                                                  kwargs_=event.kwargs_)\n\n        if event.source_types.name is SourceTypes.AP.name:\n            if 'payload' in event.kwargs_:\n                if event_id.payload != event.kwargs_['payload']:\n                    event_id.payload = event.kwargs_['payload']\n                del event.kwargs_['payload']\n            print(event.args, event.kwargs_, \"TODO: remove\")\n            return await event.source(*event.args, payload=event_id, **event.kwargs_)\n\n        if event.source_types.name is SourceTypes.AF.name:\n            return await event.source(*event.args, **event.kwargs_)\n\n        if event.source_types.name is SourceTypes.AR.name:\n            return await get_app(str(event_id)).run_any(mod_function_name=event.source, get_results=True,\n                                                        args_=event.args,\n                                                        kwargs_=event.kwargs_)\n\n        if event.source_types.name is SourceTypes.S.name:\n            return eval(event.source, __locals={'app': get_app(str(event_id)), 'event': event, 'eventManagerC': self})\n\n    async def routing_function_router(self, event_id: EventID):\n\n        result = await self.trigger_event(event_id)\n\n        if result is None:\n            result = Result.default_user_error(\"Invalid Event ID\")\n\n        if isinstance(result, bytes | dict):\n            pass\n        elif isinstance(result, Result):\n            result.result.data_info = str(event_id)\n        elif isinstance(result, EventID):\n            result = Result.default_internal_error(\"Event not found\", data=result)\n        else:\n            result = Result.ok(data=result, data_info=\"&lt;automatic&gt;\", info=str(event_id.path))\n\n        if isinstance(result, str):\n            result = result.encode()\n\n        return result\n\n    async def trigger_evnet_by_name(self, name: str):\n        await self.trigger_event(EventID.crate_name_as_id(name=name))\n\n    async def trigger_event(self, event_id: EventID):\n        \"\"\"\n        Exec source based on\n\n        source_types\n            F -&gt; call directly\n            R -&gt; use get_app(str(event_id)).run_any(*args, **kwargs)\n            S -&gt; evaluate string\n        scope\n            instance -&gt; _trigger_local\n            local -&gt; if you ar proxy app run the event through get_app(str(event_id)).run_any(TBEF.EventManager._trigger_local, args=args, kwargs=kwargs, get_result=True)\n            local_network -&gt; use proxy0 app to communicate withe Daemon0 then local\n            global_network -&gt;\n        exec_in\n        event_id\n        threaded\n\n                       \"\"\"\n        # print(f\"event-id Ptah : {event_id.get_path()}\")\n        # print(f\"testing trigger_event for {event_id.get_source()} {event_id.get_source()[-1] == self.source_id} \")\n        print(str(event_id))\n        if event_id.get_source()[-1] == self.source_id:\n            payload = await self._trigger_local(event_id)\n            event_id.set_payload(payload)\n            if len(event_id.path) &gt; 1:\n                event_id.source = ':'.join([e.split(':')[0] for e in event_id.get_path() if e != \"E\"])\n                res = await self.route_event_id(event_id)\n                if isinstance(res, Result):\n                    res.print()\n                else:\n                    print(res)\n            return payload\n        return await self.route_event_id(event_id)\n\n    async def route_event_id(self, event_id: EventID):\n\n        # print(f\"testing route_event_id for {event_id.get_source()[-1]}\")\n        if event_id.get_source()[-1] == '*':  # self.identification == \"P0\" and\n            responses = []\n            event_id.source = ':'.join(event_id.get_source()[:-1])\n            event_id.add_path(f\"{self._name}({self.source_id})\")\n            data = asdict(event_id)\n            for name, rout_ in self.routes_client.items():\n                if name in event_id.path:\n                    continue\n                ret = await rout_.put_data(data)\n                responses.append(ret)\n            return responses\n        route = self.routes_client.get(event_id.get_source()[-1])\n        # print(\"route:\", route)\n        if route is None:\n            route = self.routes_client.get(event_id.get_path()[-1])\n        if route is None:\n            return event_id.add_path((\"\" if len(event_id.get_source()) == 1 else \"404#\")+self.identification)\n        time.sleep(0.25)\n        event_id.source = ':'.join(event_id.get_source()[:-1])\n        event_id.add_path(f\"{self._name}({self.source_id})\")\n        return await route.put_data(asdict(event_id))\n\n    async def receiver(self):\n\n        t0 = time.time()\n\n        while self.running:\n            time.sleep(0.25)\n            if not self.receiver_que.empty():\n                event_id = self.receiver_que.get()\n                print(\"Receiver Event\", str(event_id))\n                await self.trigger_event(event_id)\n\n            if time.time() - t0 &gt; 5:\n                await self.receive_all_client_data()\n                t0 = time.time()\n\n    def info(self):\n        return {\"source\": self.source_id, \"known_routs:\": self.routers_servers, \"_router\": self.routes_client,\n                \"events\": self.events}\n\n    def stop(self):\n        self.running = False\n        list(map(lambda x: x.disconnect(), self.routes_client.values()))\n        list(map(lambda x: x.stop(), self.routers_servers.values()))\n\n    def reconnect(self, name):\n        if name is None:\n            pass\n        elif name in self.routes_client:\n            self.routes_client[name].reconnect()\n            return\n        list(map(lambda x: x.reconnect(), self.routes_client.values()))\n\n    async def verify(self, name):\n        if name is None:\n            pass\n        elif name in self.routes_client:\n            await self.routes_client[name].verify()\n            return\n        for x in self.routes_client.values():\n            await x.verify()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.EventManager.module.EventManagerClass.trigger_event","title":"<code>trigger_event(event_id)</code>  <code>async</code>","text":"<p>Exec source based on</p> <p>source_types     F -&gt; call directly     R -&gt; use get_app(str(event_id)).run_any(args, *kwargs)     S -&gt; evaluate string scope     instance -&gt; _trigger_local     local -&gt; if you ar proxy app run the event through get_app(str(event_id)).run_any(TBEF.EventManager._trigger_local, args=args, kwargs=kwargs, get_result=True)     local_network -&gt; use proxy0 app to communicate withe Daemon0 then local     global_network -&gt; exec_in event_id threaded</p> Source code in <code>toolboxv2/mods/EventManager/module.py</code> <pre><code>async def trigger_event(self, event_id: EventID):\n    \"\"\"\n    Exec source based on\n\n    source_types\n        F -&gt; call directly\n        R -&gt; use get_app(str(event_id)).run_any(*args, **kwargs)\n        S -&gt; evaluate string\n    scope\n        instance -&gt; _trigger_local\n        local -&gt; if you ar proxy app run the event through get_app(str(event_id)).run_any(TBEF.EventManager._trigger_local, args=args, kwargs=kwargs, get_result=True)\n        local_network -&gt; use proxy0 app to communicate withe Daemon0 then local\n        global_network -&gt;\n    exec_in\n    event_id\n    threaded\n\n                   \"\"\"\n    # print(f\"event-id Ptah : {event_id.get_path()}\")\n    # print(f\"testing trigger_event for {event_id.get_source()} {event_id.get_source()[-1] == self.source_id} \")\n    print(str(event_id))\n    if event_id.get_source()[-1] == self.source_id:\n        payload = await self._trigger_local(event_id)\n        event_id.set_payload(payload)\n        if len(event_id.path) &gt; 1:\n            event_id.source = ':'.join([e.split(':')[0] for e in event_id.get_path() if e != \"E\"])\n            res = await self.route_event_id(event_id)\n            if isinstance(res, Result):\n                res.print()\n            else:\n                print(res)\n        return payload\n    return await self.route_event_id(event_id)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.EventManager.module.Rout","title":"<code>Rout</code>  <code>dataclass</code>","text":"Source code in <code>toolboxv2/mods/EventManager/module.py</code> <pre><code>@dataclass\nclass Rout:\n    _from: str\n    _to: str\n\n    _from_port: int\n    _from_host: str\n\n    _to_port: int\n    _to_host: str\n\n    routing_function: Callable\n\n    @property\n    def to_host(self):\n        return self._to_host\n\n    @property\n    def to_port(self):\n        return self._to_port\n\n    async def put_data(self, event_id_data: dict[str, str]):\n        event_id: EventID = EventID(**event_id_data)\n        return await self.routing_function(event_id)\n\n    def close(self):\n        \"\"\" Close \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.EventManager.module.Rout.close","title":"<code>close()</code>","text":"<p>Close</p> Source code in <code>toolboxv2/mods/EventManager/module.py</code> <pre><code>def close(self):\n    \"\"\" Close \"\"\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi","title":"<code>FastApi</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_api_install","title":"<code>fast_api_install</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_api_install.FileBrowser","title":"<code>FileBrowser</code>","text":"Source code in <code>toolboxv2/mods/FastApi/fast_api_install.py</code> <pre><code>class FileBrowser:\n    ALLOWED_DIRECTORIES: set[str] = {\"mods_sto\", \"flows\", \"static\", \"apps\"}\n\n    def __init__(self, start_dir: str):\n        self.static_dir = pathlib.Path(start_dir).resolve()\n        self.current_container = None\n\n    def is_path_allowed(self, file_path: pathlib.Path) -&gt; bool:\n        \"\"\"Check if the path is within allowed directories.\"\"\"\n        if not file_path.is_relative_to(self.static_dir):\n            return False\n\n        relative_parts = file_path.parts[len(self.static_dir.parts):]\n        return any(part in self.ALLOWED_DIRECTORIES for part in relative_parts)\n\n    async def download_file(self, file_path: pathlib.Path) -&gt; None:\n        \"\"\"Handle file download.\"\"\"\n        if not file_path.is_file() or not self.is_path_allowed(file_path):\n            ui.notify('Access denied or file not found', type='negative')\n            return\n\n        # Use NiceGUI's download function\n        await ui.download(str(file_path))\n\n    def refresh_view(self, path: pathlib.Path) -&gt; None:\n        \"\"\"Refresh the file browser view.\"\"\"\n        if self.current_container:\n            self.current_container.clear()\n\n        with self.current_container:\n            # Add header with current path\n            ui.label(f'Current directory: {path.relative_to(self.static_dir)}').classes('text-h6')\n\n            # Add parent directory link if not at root\n            if path != self.static_dir and path.parent.is_relative_to(self.static_dir):\n                with ui.row().classes('w-full items-center'):\n                    ui.button('..', on_click=lambda p=path.parent: self.refresh_view(p)) \\\n                        .classes('bg-blue-100 px-4 py-2 rounded')\n\n            # List directories first\n            for item in sorted(path.iterdir()):\n                if not self.is_path_allowed(item):\n                    continue\n\n                with ui.row().classes('w-full items-center gap-2'):\n                    if item.is_dir():\n                        ui.button(f'\ud83d\udcc1 {item.name}/',\n                                  on_click=lambda p=item: self.refresh_view(p)) \\\n                            .classes('bg-blue-100 px-4 py-2 rounded')\n                    else:\n                        ui.label(f'\ud83d\udcc4 {item.name}').classes('flex-grow')\n                        ui.button('Download',\n                                  on_click=lambda p=item: self.download_file(p)) \\\n                            .classes('bg-green-100 px-4 py-2 rounded')\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_api_install.FileBrowser.download_file","title":"<code>download_file(file_path)</code>  <code>async</code>","text":"<p>Handle file download.</p> Source code in <code>toolboxv2/mods/FastApi/fast_api_install.py</code> <pre><code>async def download_file(self, file_path: pathlib.Path) -&gt; None:\n    \"\"\"Handle file download.\"\"\"\n    if not file_path.is_file() or not self.is_path_allowed(file_path):\n        ui.notify('Access denied or file not found', type='negative')\n        return\n\n    # Use NiceGUI's download function\n    await ui.download(str(file_path))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_api_install.FileBrowser.is_path_allowed","title":"<code>is_path_allowed(file_path)</code>","text":"<p>Check if the path is within allowed directories.</p> Source code in <code>toolboxv2/mods/FastApi/fast_api_install.py</code> <pre><code>def is_path_allowed(self, file_path: pathlib.Path) -&gt; bool:\n    \"\"\"Check if the path is within allowed directories.\"\"\"\n    if not file_path.is_relative_to(self.static_dir):\n        return False\n\n    relative_parts = file_path.parts[len(self.static_dir.parts):]\n    return any(part in self.ALLOWED_DIRECTORIES for part in relative_parts)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_api_install.FileBrowser.refresh_view","title":"<code>refresh_view(path)</code>","text":"<p>Refresh the file browser view.</p> Source code in <code>toolboxv2/mods/FastApi/fast_api_install.py</code> <pre><code>def refresh_view(self, path: pathlib.Path) -&gt; None:\n    \"\"\"Refresh the file browser view.\"\"\"\n    if self.current_container:\n        self.current_container.clear()\n\n    with self.current_container:\n        # Add header with current path\n        ui.label(f'Current directory: {path.relative_to(self.static_dir)}').classes('text-h6')\n\n        # Add parent directory link if not at root\n        if path != self.static_dir and path.parent.is_relative_to(self.static_dir):\n            with ui.row().classes('w-full items-center'):\n                ui.button('..', on_click=lambda p=path.parent: self.refresh_view(p)) \\\n                    .classes('bg-blue-100 px-4 py-2 rounded')\n\n        # List directories first\n        for item in sorted(path.iterdir()):\n            if not self.is_path_allowed(item):\n                continue\n\n            with ui.row().classes('w-full items-center gap-2'):\n                if item.is_dir():\n                    ui.button(f'\ud83d\udcc1 {item.name}/',\n                              on_click=lambda p=item: self.refresh_view(p)) \\\n                        .classes('bg-blue-100 px-4 py-2 rounded')\n                else:\n                    ui.label(f'\ud83d\udcc4 {item.name}').classes('flex-grow')\n                    ui.button('Download',\n                              on_click=lambda p=item: self.download_file(p)) \\\n                        .classes('bg-green-100 px-4 py-2 rounded')\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit","title":"<code>fast_lit</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit.APIRequestHelper","title":"<code>APIRequestHelper</code>","text":"Source code in <code>toolboxv2/mods/FastApi/fast_lit.py</code> <pre><code>class APIRequestHelper:\n    def __init__(self, token_secret: str):\n        self.token_secret = token_secret\n\n    async def make_api_request(self, endpoint: str, method: str, data: dict | None = None,\n                               headers: dict | None = None, session_token: str | None = None) -&gt; Any:\n        \"\"\"\n        Make API requests while maintaining session context\n        \"\"\"\n        import httpx\n\n        if headers is None:\n            headers = {}\n\n        if session_token:\n            try:\n                session_data = jwt.decode(session_token, self.token_secret, algorithms=[\"HS256\"])\n                headers['X-Session-ID'] = session_data.get('session_id')\n                headers['Authorization'] = f'Bearer {session_token}'\n            except jwt.InvalidTokenError:\n                raise ValueError(\"Invalid session token\")\n\n        async with httpx.AsyncClient() as client:\n            response = await client.request(\n                method=method,\n                url=endpoint,\n                json=data,\n                headers=headers\n            )\n\n            return response.json()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit.APIRequestHelper.make_api_request","title":"<code>make_api_request(endpoint, method, data=None, headers=None, session_token=None)</code>  <code>async</code>","text":"<p>Make API requests while maintaining session context</p> Source code in <code>toolboxv2/mods/FastApi/fast_lit.py</code> <pre><code>async def make_api_request(self, endpoint: str, method: str, data: dict | None = None,\n                           headers: dict | None = None, session_token: str | None = None) -&gt; Any:\n    \"\"\"\n    Make API requests while maintaining session context\n    \"\"\"\n    import httpx\n\n    if headers is None:\n        headers = {}\n\n    if session_token:\n        try:\n            session_data = jwt.decode(session_token, self.token_secret, algorithms=[\"HS256\"])\n            headers['X-Session-ID'] = session_data.get('session_id')\n            headers['Authorization'] = f'Bearer {session_token}'\n        except jwt.InvalidTokenError:\n            raise ValueError(\"Invalid session token\")\n\n    async with httpx.AsyncClient() as client:\n        response = await client.request(\n            method=method,\n            url=endpoint,\n            json=data,\n            headers=headers\n        )\n\n        return response.json()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit.BidirectionalStreamlitAppManager","title":"<code>BidirectionalStreamlitAppManager</code>","text":"<p>               Bases: <code>BaseHTTPMiddleware</code></p> Source code in <code>toolboxv2/mods/FastApi/fast_lit.py</code> <pre><code>class BidirectionalStreamlitAppManager(BaseHTTPMiddleware, metaclass=Singleton):\n    def __init__(self, app: FastAPI, streamlit_apps_dir: str = \"./apps\"):\n        super().__init__(app)\n        self.streamlit_manager = StreamlitAppManager()\n        self.streamlit_apps_dir = streamlit_apps_dir\n        self.token_secret = os.getenv(\"TOKEN_SECRET\", \"your-secret-key\")\n        self.api_helper = APIRequestHelper(self.token_secret)\n\n        # Run cleanup task\n        asyncio.create_task(self.periodic_cleanup())\n\n    #def add_ws(self, fast_app):\n        # Register WebSocket routes\n     #   fast_app.add_api_websocket_route(\"/ws/{session_id}/{app_id}\", self.websocket_endpoint, \"StWebSocket\")\n\n    async def periodic_cleanup(self):\n        while True:\n            self.streamlit_manager.cleanup_inactive_apps()\n            await asyncio.sleep(3600)\n\n    def create_streamlit_token(self, session_data: dict, app_name: str) -&gt; str:\n        payload = {\n            \"app_name\": app_name,\n            \"session_id\": session_data.get(\"ID\"),\n            \"user_data\": session_data.get(\"live_data\"),\n            \"exp\": datetime.utcnow() + timedelta(hours=1)\n        }\n        return jwt.encode(payload, self.token_secret, algorithm=\"HS256\")\n\n    #async def websocket_endpoint(self, websocket: WebSocket, session_id: str, app_id: str):\n    #    await self.streamlit_manager.ws_manager.connect(websocket, session_id, app_id)\n    #    try:\n    #        while True:\n    #            message = await websocket.receive_json()\n    #            await self.streamlit_manager.ws_manager.handle_message(session_id, message)\n    #    except WebSocketDisconnect:\n    #        await self.streamlit_manager.ws_manager.disconnect(session_id, app_id)\n\n    async def resolve_session_token(self, request: Request) -&gt; str | None:\n        \"\"\"\n        Extract and validate session token from request\n        \"\"\"\n        token = request.headers.get('Authorization', '').replace('Bearer ', '')\n        if not token:\n            token = request.query_params.get('token')\n\n        if token:\n            try:\n                jwt.decode(token, self.token_secret, algorithms=[\"HS256\"])\n                return token\n            except jwt.InvalidTokenError:\n                return None\n        return None\n\n    async def dispatch(self, request: Request, call_next) -&gt; Response:\n        # Handle API routes with session token resolution\n        if request.url.path.startswith(\"/api/\"):\n            session_token = await self.resolve_session_token(request)\n            if session_token:\n                # Inject session data into request state\n                request.state.session_token = session_token\n                request.state.api_helper = self.api_helper\n\n        # Handle Streamlit routes\n        elif request.url.path.startswith(\"/apps/\"):\n            app_name = request.url.path.split(\"/\")[-1]\n            app_path = os.path.join(self.streamlit_apps_dir, f\"{app_name}.py\")\n\n            # Verify session is valid\n            if 'public' not in app_name and not request.session.get(\"valid\", False):\n                return JSONResponse(\n                    status_code=401,\n                    content={\"message\": \"Invalid session\"}\n                )\n\n            if not os.path.exists(app_path):\n                return JSONResponse(\n                    status_code=401,\n                    content={\"message\": \"no app found\"}\n                )\n\n            streamlit_token = self.create_streamlit_token(request.session, app_name)\n            port = await self.streamlit_manager.start_app(app_path, request.session.get(\"ID\")+app_name)\n            streamlit_url = f\"http://{host}:{port}?token={streamlit_token}\"\n            return RedirectResponse(url=streamlit_url)\n\n        resposee = await call_next(request)\n        return resposee\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit.BidirectionalStreamlitAppManager.resolve_session_token","title":"<code>resolve_session_token(request)</code>  <code>async</code>","text":"<p>Extract and validate session token from request</p> Source code in <code>toolboxv2/mods/FastApi/fast_lit.py</code> <pre><code>async def resolve_session_token(self, request: Request) -&gt; str | None:\n    \"\"\"\n    Extract and validate session token from request\n    \"\"\"\n    token = request.headers.get('Authorization', '').replace('Bearer ', '')\n    if not token:\n        token = request.query_params.get('token')\n\n    if token:\n        try:\n            jwt.decode(token, self.token_secret, algorithms=[\"HS256\"])\n            return token\n        except jwt.InvalidTokenError:\n            return None\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit.inject_custom_css","title":"<code>inject_custom_css(css_file_path='./web/assets/styles.css')</code>","text":"<p>Liest eine CSS-Datei ein und injiziert sie in die Streamlit-App.</p> Source code in <code>toolboxv2/mods/FastApi/fast_lit.py</code> <pre><code>def inject_custom_css(css_file_path=\"./web/assets/styles.css\"):\n    \"\"\"\n    Liest eine CSS-Datei ein und injiziert sie in die Streamlit-App.\n    \"\"\"\n    import streamlit as st\n    try:\n        with open(css_file_path) as f:\n            css_content = f.read()\n\n        # CSS in einen &lt;style&gt;-Tag einbetten\n        css_injection = f\"&lt;style&gt;{css_content}&lt;/style&gt;\"\n\n        # CSS in Streamlit injizieren\n        st.markdown(css_injection, unsafe_allow_html=True)\n    except Exception as e:\n        st.error(f\"Fehler beim Laden des CSS: {e}\")\n\n    st.markdown(\"\"\"\n        &lt;style&gt;\n            .reportview-container {\n                margin-top: -2em;\n            }\n            #MainMenu {visibility: hidden;}\n            .stDeployButton {display:none;}\n            footer {visibility: hidden;}\n            #stDecoration {display:none;}\n        &lt;/style&gt;\n    \"\"\", unsafe_allow_html=True)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_lit.make_api_request","title":"<code>make_api_request(endpoint, method='GET', data=None)</code>  <code>async</code>","text":"<p>Helper function for making API requests from Streamlit apps</p> Source code in <code>toolboxv2/mods/FastApi/fast_lit.py</code> <pre><code>async def make_api_request(endpoint: str, method: str = \"GET\", data: dict | None = None):\n    \"\"\"Helper function for making API requests from Streamlit apps\"\"\"\n    import streamlit as st\n\n    if not hasattr(st.session_state, 'token'):\n        st.error(\"No valid session token found\")\n        st.stop()\n\n    headers = {\n        'Authorization': f'Bearer {st.session_state.token}',\n        'Content-Type': 'application/json'\n    }\n\n    try:\n        api_helper = APIRequestHelper(os.getenv(\"TOKEN_SECRET\", \"your-secret-key\"))\n        response = await api_helper.make_api_request(\n            endpoint=endpoint,\n            method=method,\n            data=data,\n            headers=headers,\n            session_token=st.session_state.token\n        )\n        return response\n    except Exception as e:\n        st.error(f\"API request failed: {str(e)}\")\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice","title":"<code>fast_nice</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager","title":"<code>NiceGUIManager</code>","text":"Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>class NiceGUIManager(metaclass=Singleton):\n    init = False\n    def __init__(self, fastapi_app: FastAPI = None, styles_path: str = \"./web/assets/styles.css\"):\n\n        if fastapi_app is None:\n            return None\n        self.admin_password = os.getenv(\"TB_R_KEY\", \"root@admin\")\n        self.app = fastapi_app\n        self.styles_path = styles_path\n        self.registered_guis: dict[str, dict[str, Any]] = {}\n        self.ws_connections: dict[str, dict[str, WebSocket]] = {}\n        self.mount_path = \"/gui\"\n        self.endpoints: list[UIEndpoint] = []\n\n        self.helper_contex = open(\"./dist/helper.html\", encoding=\"utf-8\").read()\n\n        self.app.add_middleware(BaseHTTPMiddleware, dispatch=self.middleware_dispatch)\n\n        # Add WebSocket endpoint\n        self.app.websocket(\"/ws/{session_id}/{gui_id}\")(self.websocket_endpoint)\n        self._setup_admin_gui()\n        self._setup_endpoints_api()\n\n    def _setup_endpoints_api(self):\n        @self.app.get(\"/api/CloudM/openui\")\n        def get_ui_endpoints(request: Request) -&gt; list[dict]:\n            def _(endpoint):\n                add_true = True\n                if endpoint.only_valid:\n                    add_true = request.session['valid']\n\n                if add_true and endpoint.only_root:\n                    add_true = request.session.get('live_data', {}).get('user_name') == 'root'\n                return add_true\n            return [{\"path\": endpoint.path,\n    \"title\": endpoint.title,\n    \"description\": endpoint.description} for endpoint in self.endpoints if endpoint.show and _(endpoint)]\n\n    def _setup_admin_gui(self):\n        \"\"\"Setup the admin GUI interface\"\"\"\n\n        @ui.page('/admin')\n        def admin_gui(user=None):\n            print(\"admin_gui;\", user)\n            if user is None or user.name != \"root\":\n                return\n\n            with ui.card().style(\"background-color: var(--background-color) !important\").classes('w-full'):\n                ui.label('NiceGUI Manager Admin Interface').classes('text-2xl font-bold mb-4')\n\n                # GUI Management Section\n                with ui.tabs().style(\"background-color: var(--background-color) !important\") as tabs:\n                    ui.tab('Registered GUIs')\n                    ui.tab('Add New GUI')\n                    ui.tab('System Status')\n\n                with ui.tab_panels(tabs, value='Registered GUIs').style(\n                    \"background-color: var(--background-color) !important\"):\n                    with ui.tab_panel('Registered GUIs'):\n                        self._show_registered_guis()\n\n                    with ui.tab_panel('Add New GUI'):\n                        self._show_add_gui_form()\n\n                    with ui.tab_panel('System Status'):\n                        self._show_system_status()\n\n        self.register_gui(\"admin\", admin_gui, \"/admin\", only_root=True)\n\n    def _show_registered_guis(self):\n        \"\"\"Show list of registered GUIs with management options\"\"\"\n        with ui.column().classes('w-full gap-4'):\n            for gui_id, gui_info in self.registered_guis.items():\n                with ui.card().classes('w-full').style(\"background-color: var(--background-color) !important\"):\n                    with ui.row().classes('w-full items-center justify-between').style(\n                        \"background-color: var(--background-color) !important\"):\n                        ui.label(f'GUI ID: {gui_id}').classes('font-bold')\n                        ui.label(f'Path: {gui_info[\"path\"]}')\n\n                        created_at = gui_info['created_at'].strftime('%Y-%m-%d %H:%M:%S')\n                        ui.label(f'Created: {created_at}')\n\n                        with ui.row().classes('gap-2').style(\"background-color: var(--background-color) !important\"):\n                            ui.button('View', on_click=lambda g=gui_info['path']: ui.navigate.to(g))\n                            ui.button('Remove', on_click=lambda g=gui_id: self._handle_gui_removal(g))\n                            ui.button('Restart', on_click=lambda g=gui_id: self._handle_gui_restart(g))\n\n                    # Show connection status\n                    active_connections = sum(\n                        1 for connections in self.ws_connections.values()\n                        if gui_id in connections\n                    )\n                    ui.label(f'Active Connections: {active_connections}')\n\n    def _show_add_gui_form(self):\n        \"\"\"Show form for adding new GUI\"\"\"\n        with ui.card().classes('w-full').style(\"background-color: var(--background-color) !important\"):\n            gui_id = ui.input('GUI ID').classes('w-full')\n            mount_path = ui.input('Mount Path (optional)').classes('w-full')\n\n            # Code editor for GUI setup\n            code_editor = ui.editor(\n                value='def setup_gui():\\n    ui.label(\"New GUI\")\\n',\n            ).classes('w-full h-64')\n\n            def add_new_gui():\n                try:\n                    # Create setup function from code\n                    setup_code = code_editor.value\n                    setup_namespace = {}\n                    exec(setup_code, {'ui': ui}, setup_namespace)\n                    setup_func = setup_namespace['setup_gui']\n\n                    # Register the new GUI\n                    self.register_gui(\n                        gui_id.value,\n                        setup_func,\n                        mount_path.value if mount_path.value else None\n                    )\n\n                    ui.notify('GUI added successfully')\n                    ui.navigate.to('admin')  # Refresh page\n                except Exception as e:\n                    ui.notify(f'Error adding GUI: {str(e)}', color='negative')\n\n            ui.button('Add GUI', on_click=add_new_gui).classes('w-full mt-4')\n\n    def _show_system_status(self):\n        \"\"\"Show system status information\"\"\"\n        with ui.card().classes('w-full').style(\"background-color: var(--background-color) !important\"):\n            ui.label('System Status').classes('text-xl font-bold mb-4')\n\n            # System stats\n            ui.label(f'Total GUIs: {len(self.registered_guis)}')\n            ui.label(f'Total WebSocket Connections: {sum(len(conns) for conns in self.ws_connections.values())}')\n\n            # Memory usage\n            import psutil\n            process = psutil.Process()\n            memory_usage = process.memory_info().rss / 1024 / 1024  # MB\n            ui.label(f'Memory Usage: {memory_usage:.2f} MB')\n\n            # Add refresh button\n            ui.button('Refresh Stats', on_click=lambda: ui.navigate.to('/admin'))\n\n    def _handle_gui_removal(self, gui_id: str):\n        \"\"\"Handle GUI removal with confirmation\"\"\"\n\n        def confirm_remove():\n            if self.remove_gui(gui_id):\n                ui.notify(f'GUI {gui_id} removed successfully')\n                ui.navigate.to('/admin')  # Refresh page\n            else:\n                ui.notify('Error removing GUI', color='negative')\n\n        ui.notify('Are you sure?',\n                  actions=[{'label': 'Yes', 'on_click': confirm_remove},\n                           {'label': 'No'}])\n\n    def _handle_gui_restart(self, gui_id: str):\n        \"\"\"Handle GUI restart\"\"\"\n        try:\n            if gui_id in self.registered_guis:\n                gui_info = self.registered_guis[gui_id]\n                # Re-register the GUI with the same setup\n                self.register_gui(gui_id, gui_info['setup'], gui_info['path'])\n                ui.notify(f'GUI {gui_id} restarted successfully')\n            else:\n                ui.notify('GUI not found', color='negative')\n        except Exception as e:\n            ui.notify(f'Error restarting GUI: {str(e)}', color='negative')\n\n    def _load_styles(self) -&gt; str:\n        \"\"\"Load custom styles from CSS file\"\"\"\n        try:\n            with open(self.styles_path) as f:\n                return f.read()\n        except Exception as e:\n            print(f\"Error loading styles: {e}\")\n            return \"\"\n\n    def register_gui(self, gui_id: str, setup_func: Callable, mount_path: str | None = None, additional: str | None = None, title: str | None = None , description: str | None = None, **kwargs) -&gt; None:\n        \"\"\"Register a new NiceGUI application\"\"\"\n        path = mount_path or f\"/{gui_id}\"\n        self.endpoints.append(UIEndpoint(path=self.mount_path+path, title=title if title is not None else path.replace('/', '') , description=description if description is not None else '', **kwargs))\n        if additional is None:\n            additional = \"\"\n\n        def has_parameters(func, *params):\n            \"\"\"\n            \u00dcberpr\u00fcft, ob die Funktion bestimmte Parameter hat.\n\n            :param func: Die zu analysierende Funktion.\n            :param params: Eine Liste der zu suchenden Parameter.\n            :return: Ein Dictionary mit den Parametern und einem booleschen Wert.\n            \"\"\"\n            signature = inspect.signature(func)\n            func_params = signature.parameters.keys()\n            return {param: param in func_params for param in params}\n\n        async def request_to_request_session(request):\n            jk = request.json()\n            if asyncio.iscoroutine(jk):\n                with contextlib.suppress(Exception):\n                    jk = await jk\n            def js():\n                return jk\n            return RequestSession(\n                session=request.session,\n                body=request.body,\n                json=js,\n                row=request,\n            )\n\n        get_app()\n\n        @ui.page(path)\n        async def wrapped_gui(request: Request):\n            # Inject custom styles\n            ui.add_body_html(self.helper_contex + additional)\n            # ui.switch('Dark').bind_value(ui, 'dark_mode')\n            # ui.add_css(\"q-card {background-color: var(--background-color)} !important\")\n            # ui.add_body_html('&lt;script src=\"../index.js\" type=\"module\" defer&gt;&lt;/script&gt;')\n\n            # Initialize the GUI\n            params_ = {}\n            params = has_parameters(setup_func, 'request', 'user', 'session', 'id', 'sid')\n\n            if params.get('request'):\n                params_['request'] = await request_to_request_session(request)\n            if params.get('user'):\n                params_['user'] = await get_user_from_request(get_app(), request)\n            if params.get('session'):\n                params_['session'] = request.session\n            if params.get('spec'):\n                params_['spec'] = get_spec(request)\n            if params.get('sid'):\n                params_['sid'] = get_s_id(request)\n\n            async def task():\n                if asyncio.iscoroutine(setup_func):\n\n                    # Event Listener f\u00fcr Button hinzuf\u00fcgen\n                    await ui.run_javascript('''\n                            Quasar.Dark.set(\"auto\");\n                            tailwind.config.darkMode = \"media\";\n                        ''')\n\n                    await ui.run_javascript(\"\"\"\n                    document.getElementById('darkModeToggle').addEventListener('click', function () {\n                    const labelToggel = document.getElementById('toggleLabel')\n                    if (labelToggel.innerHTML == `&lt;span class=\"material-symbols-outlined\"&gt;\ndark_mode\n&lt;/span&gt;`){\n                            Quasar.Dark.set(true);\n                            tailwind.config.darkMode = \"class\";\n                            document.body.classList.add(\"dark\");\n                        }else{\n                            Quasar.Dark.set(false);\n                            tailwind.config.darkMode = \"class\"\n                            document.body.classList.remove(\"dark\");\n                        }\n                    });\n                    \"\"\")\n\n                    if not params_:\n                        await setup_func()\n                    else:\n                        await setup_func(**params_)\n                else:\n                    if not params_:\n                        setup_func()\n                    else:\n                        setup_func(**params_)\n\n\n\n\n            await task()\n            # return result\n\n        self.registered_guis[gui_id] = {\n            'path': path,\n            'setup': setup_func,\n            'created_at': datetime.now()\n        }\n\n        print(\"Registered GUI:\", self.registered_guis[gui_id])\n        return True\n\n    def remove_gui(self, gui_id: str) -&gt; bool:\n        \"\"\"Remove a registered GUI application\"\"\"\n        if gui_id in self.registered_guis:\n            # Remove from registry\n            del self.registered_guis[gui_id]\n\n            # Clean up any WebSocket connections\n            for session_id in self.ws_connections:\n                if gui_id in self.ws_connections[session_id]:\n                    del self.ws_connections[session_id][gui_id]\n\n            return True\n        return False\n\n    async def websocket_endpoint(self, websocket: WebSocket, session_id: str, gui_id: str):\n        \"\"\"Handle WebSocket connections for real-time updates\"\"\"\n        await websocket.accept()\n\n        if session_id not in self.ws_connections:\n            self.ws_connections[session_id] = {}\n        self.ws_connections[session_id][gui_id] = websocket\n\n        try:\n            while True:\n                data = await websocket.receive_json()\n                # Handle incoming WebSocket messages\n                await self.handle_ws_message(session_id, gui_id, data)\n        except WebSocketDisconnect:\n            if session_id in self.ws_connections:\n                if gui_id in self.ws_connections[session_id]:\n                    del self.ws_connections[session_id][gui_id]\n\n    async def handle_ws_message(self, session_id: str, gui_id: str, message: dict):\n        \"\"\"Handle incoming WebSocket messages\"\"\"\n        # Implement custom WebSocket message handling\n        if message.get('type') == 'update':\n            # Broadcast updates to all connected clients for this GUI\n            await self.broadcast_to_gui(gui_id, {\n                'type': 'update',\n                'data': message.get('data')\n            })\n\n    async def broadcast_to_gui(self, gui_id: str, message: dict):\n        \"\"\"Broadcast a message to all sessions connected to a specific GUI\"\"\"\n        for session_connections in self.ws_connections.values():\n            if gui_id in session_connections:\n                await session_connections[gui_id].send_json(message)\n\n    async def middleware_dispatch(self, request: Request, call_next) -&gt; Response:\n        \"\"\"Custom middleware for session handling and authentication\"\"\"\n        async def callN():\n            response = await call_next(request)\n            return response\n\n        if not request.url.path.startswith(self.mount_path):\n            return await callN()\n\n        if request.url.path.endswith(\"/favicon.ico\"):\n            return await callN()\n        if \"_nicegui\" in request.url.path and \"static\" in request.url.path:\n            return await callN()\n        if \"_nicegui\" in request.url.path and \"components\" in request.url.path:\n            return await callN()\n        if \"_nicegui\" in request.url.path and \"codehilite\" in request.url.path:\n            return await callN()\n        if \"_nicegui\" in request.url.path and \"libraries\" in request.url.path:\n            return await callN()\n\n        if \"open\" in request.url.path:\n            return await callN()\n\n        # Verify session if needed\n        if not request.session.get(\"valid\", False):\n            return RedirectResponse(f\"/web/login?next={request.url.path}\")\n\n        response = await call_next(request)\n        return response\n\n    def init_app(self) -&gt; None:\n        \"\"\"Initialize the FastAPI application with NiceGUI integration\"\"\"\n        self.init = True\n        ui.run_with(\n            self.app,\n            mount_path=self.mount_path,\n            favicon=os.getenv(\"FAVI\"), # \"/root/Toolboxv2/toolboxv2/favicon.ico\"\n            show_welcome_message=False,\n            # prod_js=False,\n        )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.broadcast_to_gui","title":"<code>broadcast_to_gui(gui_id, message)</code>  <code>async</code>","text":"<p>Broadcast a message to all sessions connected to a specific GUI</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>async def broadcast_to_gui(self, gui_id: str, message: dict):\n    \"\"\"Broadcast a message to all sessions connected to a specific GUI\"\"\"\n    for session_connections in self.ws_connections.values():\n        if gui_id in session_connections:\n            await session_connections[gui_id].send_json(message)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.handle_ws_message","title":"<code>handle_ws_message(session_id, gui_id, message)</code>  <code>async</code>","text":"<p>Handle incoming WebSocket messages</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>async def handle_ws_message(self, session_id: str, gui_id: str, message: dict):\n    \"\"\"Handle incoming WebSocket messages\"\"\"\n    # Implement custom WebSocket message handling\n    if message.get('type') == 'update':\n        # Broadcast updates to all connected clients for this GUI\n        await self.broadcast_to_gui(gui_id, {\n            'type': 'update',\n            'data': message.get('data')\n        })\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.init_app","title":"<code>init_app()</code>","text":"<p>Initialize the FastAPI application with NiceGUI integration</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>def init_app(self) -&gt; None:\n    \"\"\"Initialize the FastAPI application with NiceGUI integration\"\"\"\n    self.init = True\n    ui.run_with(\n        self.app,\n        mount_path=self.mount_path,\n        favicon=os.getenv(\"FAVI\"), # \"/root/Toolboxv2/toolboxv2/favicon.ico\"\n        show_welcome_message=False,\n        # prod_js=False,\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.middleware_dispatch","title":"<code>middleware_dispatch(request, call_next)</code>  <code>async</code>","text":"<p>Custom middleware for session handling and authentication</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>async def middleware_dispatch(self, request: Request, call_next) -&gt; Response:\n    \"\"\"Custom middleware for session handling and authentication\"\"\"\n    async def callN():\n        response = await call_next(request)\n        return response\n\n    if not request.url.path.startswith(self.mount_path):\n        return await callN()\n\n    if request.url.path.endswith(\"/favicon.ico\"):\n        return await callN()\n    if \"_nicegui\" in request.url.path and \"static\" in request.url.path:\n        return await callN()\n    if \"_nicegui\" in request.url.path and \"components\" in request.url.path:\n        return await callN()\n    if \"_nicegui\" in request.url.path and \"codehilite\" in request.url.path:\n        return await callN()\n    if \"_nicegui\" in request.url.path and \"libraries\" in request.url.path:\n        return await callN()\n\n    if \"open\" in request.url.path:\n        return await callN()\n\n    # Verify session if needed\n    if not request.session.get(\"valid\", False):\n        return RedirectResponse(f\"/web/login?next={request.url.path}\")\n\n    response = await call_next(request)\n    return response\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.register_gui","title":"<code>register_gui(gui_id, setup_func, mount_path=None, additional=None, title=None, description=None, **kwargs)</code>","text":"<p>Register a new NiceGUI application</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>    def register_gui(self, gui_id: str, setup_func: Callable, mount_path: str | None = None, additional: str | None = None, title: str | None = None , description: str | None = None, **kwargs) -&gt; None:\n        \"\"\"Register a new NiceGUI application\"\"\"\n        path = mount_path or f\"/{gui_id}\"\n        self.endpoints.append(UIEndpoint(path=self.mount_path+path, title=title if title is not None else path.replace('/', '') , description=description if description is not None else '', **kwargs))\n        if additional is None:\n            additional = \"\"\n\n        def has_parameters(func, *params):\n            \"\"\"\n            \u00dcberpr\u00fcft, ob die Funktion bestimmte Parameter hat.\n\n            :param func: Die zu analysierende Funktion.\n            :param params: Eine Liste der zu suchenden Parameter.\n            :return: Ein Dictionary mit den Parametern und einem booleschen Wert.\n            \"\"\"\n            signature = inspect.signature(func)\n            func_params = signature.parameters.keys()\n            return {param: param in func_params for param in params}\n\n        async def request_to_request_session(request):\n            jk = request.json()\n            if asyncio.iscoroutine(jk):\n                with contextlib.suppress(Exception):\n                    jk = await jk\n            def js():\n                return jk\n            return RequestSession(\n                session=request.session,\n                body=request.body,\n                json=js,\n                row=request,\n            )\n\n        get_app()\n\n        @ui.page(path)\n        async def wrapped_gui(request: Request):\n            # Inject custom styles\n            ui.add_body_html(self.helper_contex + additional)\n            # ui.switch('Dark').bind_value(ui, 'dark_mode')\n            # ui.add_css(\"q-card {background-color: var(--background-color)} !important\")\n            # ui.add_body_html('&lt;script src=\"../index.js\" type=\"module\" defer&gt;&lt;/script&gt;')\n\n            # Initialize the GUI\n            params_ = {}\n            params = has_parameters(setup_func, 'request', 'user', 'session', 'id', 'sid')\n\n            if params.get('request'):\n                params_['request'] = await request_to_request_session(request)\n            if params.get('user'):\n                params_['user'] = await get_user_from_request(get_app(), request)\n            if params.get('session'):\n                params_['session'] = request.session\n            if params.get('spec'):\n                params_['spec'] = get_spec(request)\n            if params.get('sid'):\n                params_['sid'] = get_s_id(request)\n\n            async def task():\n                if asyncio.iscoroutine(setup_func):\n\n                    # Event Listener f\u00fcr Button hinzuf\u00fcgen\n                    await ui.run_javascript('''\n                            Quasar.Dark.set(\"auto\");\n                            tailwind.config.darkMode = \"media\";\n                        ''')\n\n                    await ui.run_javascript(\"\"\"\n                    document.getElementById('darkModeToggle').addEventListener('click', function () {\n                    const labelToggel = document.getElementById('toggleLabel')\n                    if (labelToggel.innerHTML == `&lt;span class=\"material-symbols-outlined\"&gt;\ndark_mode\n&lt;/span&gt;`){\n                            Quasar.Dark.set(true);\n                            tailwind.config.darkMode = \"class\";\n                            document.body.classList.add(\"dark\");\n                        }else{\n                            Quasar.Dark.set(false);\n                            tailwind.config.darkMode = \"class\"\n                            document.body.classList.remove(\"dark\");\n                        }\n                    });\n                    \"\"\")\n\n                    if not params_:\n                        await setup_func()\n                    else:\n                        await setup_func(**params_)\n                else:\n                    if not params_:\n                        setup_func()\n                    else:\n                        setup_func(**params_)\n\n\n\n\n            await task()\n            # return result\n\n        self.registered_guis[gui_id] = {\n            'path': path,\n            'setup': setup_func,\n            'created_at': datetime.now()\n        }\n\n        print(\"Registered GUI:\", self.registered_guis[gui_id])\n        return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.remove_gui","title":"<code>remove_gui(gui_id)</code>","text":"<p>Remove a registered GUI application</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>def remove_gui(self, gui_id: str) -&gt; bool:\n    \"\"\"Remove a registered GUI application\"\"\"\n    if gui_id in self.registered_guis:\n        # Remove from registry\n        del self.registered_guis[gui_id]\n\n        # Clean up any WebSocket connections\n        for session_id in self.ws_connections:\n            if gui_id in self.ws_connections[session_id]:\n                del self.ws_connections[session_id][gui_id]\n\n        return True\n    return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.NiceGUIManager.websocket_endpoint","title":"<code>websocket_endpoint(websocket, session_id, gui_id)</code>  <code>async</code>","text":"<p>Handle WebSocket connections for real-time updates</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>async def websocket_endpoint(self, websocket: WebSocket, session_id: str, gui_id: str):\n    \"\"\"Handle WebSocket connections for real-time updates\"\"\"\n    await websocket.accept()\n\n    if session_id not in self.ws_connections:\n        self.ws_connections[session_id] = {}\n    self.ws_connections[session_id][gui_id] = websocket\n\n    try:\n        while True:\n            data = await websocket.receive_json()\n            # Handle incoming WebSocket messages\n            await self.handle_ws_message(session_id, gui_id, data)\n    except WebSocketDisconnect:\n        if session_id in self.ws_connections:\n            if gui_id in self.ws_connections[session_id]:\n                del self.ws_connections[session_id][gui_id]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.fast_nice.create_nicegui_manager","title":"<code>create_nicegui_manager(app, token_secret=None)</code>","text":"<p>Create and initialize a NiceGUI manager instance</p> Source code in <code>toolboxv2/mods/FastApi/fast_nice.py</code> <pre><code>def create_nicegui_manager(app: FastAPI, token_secret: str | None = None) -&gt; NiceGUIManager:\n    \"\"\"Create and initialize a NiceGUI manager instance\"\"\"\n    manager = NiceGUIManager(app, token_secret)\n    manager.init_app()\n    manager_online[0] = True\n    return manager\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager","title":"<code>manager</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools","title":"<code>Tools</code>","text":"<p>               Bases: <code>MainTool</code>, <code>FileHandler</code></p> <p>A production-ready API Manager for running, monitoring, and managing FastAPI instances.</p> This class allows you to <ul> <li>Start API instances (live, development, debug)</li> <li>Stop and restart running APIs</li> <li>Update configuration for APIs</li> <li>Get live diagnostic info about running APIs</li> </ul> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>class Tools(MainTool, FileHandler):\n    \"\"\"\n    A production-ready API Manager for running, monitoring, and managing FastAPI instances.\n\n    This class allows you to:\n      - Start API instances (live, development, debug)\n      - Stop and restart running APIs\n      - Update configuration for APIs\n      - Get live diagnostic info about running APIs\n    \"\"\"\n\n    def __init__(self, app: Any | None = None) -&gt; None:\n        # Running APIs will be stored as a mapping from api_name to subprocess.Popen\n        self.running_apis: dict[str, multiprocessing.Process] = {}\n        self.api_config: dict[str, dict[str, str | int]] = {}\n        self.version: str = VERSION\n        self.name: str = NAME\n        self.logger: logging.Logger = app.logger if app else logging.getLogger(__name__)\n        self.color: str = \"WHITE\"\n        self.keys: dict[str, str] = {\"Apis\": \"api~config\"}\n        # In case app is not passed in, ensure that we have a dummy object with required properties\n\n        # Define available tool commands\n        self.tools: dict[str, Any] = {\n            \"all\": [\n                [\"Version\", \"Shows current Version\"],\n                [\"edit-api\", \"Set default API for name, host and port\"],\n                [\"start-api\", \"Start an API instance\"],\n                [\"stop-api\", \"Stop a running API instance\"],\n                [\"restart-api\", \"Restart an API instance\"],\n                [\"info\", \"Show API configurations and running APIs\"],\n            ],\n            \"name\": \"api_manager\",\n            \"Version\": self.show_version,\n            \"edit-api\": self.conf_api,\n            \"stop-api\": self.stop_api,\n            \"start\": self.start_live,\n            \"startE\": self._start_api,\n            \"startDev\": self.start_dev,\n            \"startDUG\": self.start_debug,\n            \"info\": self.show_running,\n            \"restart-api\": self.restart_api,\n        }\n\n        # Initialize FileHandler with default configuration data\n        default_config = {\n            \"Apis\": {\n                'main': {\n                    \"Name\": 'main',\n                    \"version\": self.version,\n                    \"port\": 5000,\n                    \"host\": '127.0.0.1'\n                }\n            }\n        }\n        FileHandler.__init__(self, \"apis.config\", self.app.id, self.keys, default_config)\n        MainTool.__init__(\n            self,\n            load=self.on_start,\n            v=self.version,\n            tool=self.tools,\n            name=self.name,\n            logs=self.logger,\n            color=self.color,\n            on_exit=self.on_exit,\n        )\n        os.makedirs(\"./.data\", exist_ok=True)\n\n    @staticmethod\n    def _get_pid_file_path(api_name: str) -&gt; str:\n        \"\"\"Get the path to the PID file for an API.\"\"\"\n        return os.path.join(\"./.data\", f\"api_pid_{api_name}\")\n\n\n    def show_version(self) -&gt; str:\n        \"\"\"Display and return the current version.\"\"\"\n        self.logger.info(\"Version: %s\", self.version)\n        return self.version\n\n    def info(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Return diagnostic information about API configurations and currently running APIs.\n        \"\"\"\n        config_info = {name: cfg for name, cfg in self.api_config.items()}\n        running_info = {name: proc.pid for name, proc in self.running_apis.items() if proc.is_alive()}\n        self.logger.info(\"API Configurations: %s\", config_info)\n        self.logger.info(\"Running APIs: %s\", running_info)\n        # Optionally, print to console as well\n        for api_name, cfg in config_info.items():\n            print(f\"Configured API - Name: {api_name}, Config: {cfg}\")\n        print(\"Running APIs:\")\n        for api_name, pid in running_info.items():\n            print(f\"API: {api_name}, Process ID: {pid}\")\n        return {\"configurations\": config_info, \"running\": running_info}\n\n    def conf_api(self, api_name: str, host: str = \"localhost\", port: int = 5000) -&gt; None:\n        \"\"\"\n        Update or create an API configuration.\n\n        Args:\n            api_name (str): The name of the API.\n            host (str): The host address (default \"localhost\"). Use \"lh\" for \"127.0.0.1\" or \"0\" for \"0.0.0.0\".\n            port (int): The port number (default 5000; use \"0\" for port 8000).\n        \"\"\"\n        if host.lower() == \"lh\":\n            host = \"127.0.0.1\"\n        if host == \"0\":\n            host = \"0.0.0.0\"\n        if str(port) == \"0\":\n            port = 8000\n\n        self.api_config[api_name] = {\n            \"Name\": api_name,\n            \"version\": self.version,\n            \"port\": int(port),\n            \"host\": host,\n        }\n        self.logger.info(\"Updated API configuration for '%s': %s\", api_name, self.api_config[api_name])\n        print(f\"API configuration updated: {self.api_config[api_name]}\")\n\n    def start_dev(self, api_name: str, *modules: str, **kwargs: Any) -&gt; str | None:\n        \"\"\"\n        Start an API in development mode.\n\n        If additional modules are provided, they are stored in a BlobFile for later use.\n\n        Args:\n            api_name (str): The API name.\n            *modules (str): Additional modules for the API.\n\n        Returns:\n            Optional[str]: Status message.\n        \"\"\"\n        if modules:\n            api_name_dev = f\"{api_name}_D\"\n            with BlobFile(f\"FastApi/{api_name_dev}/dev\", mode='w') as f:\n                f.write_json({'modules': modules})\n            api_name = api_name_dev\n\n        return self._start_api(api_name, live=False, reload=False, test_override=False, host=\"localhost\")\n\n    def start_live(self, api_name: str) -&gt; str | None:\n        \"\"\"\n        Start an API in live mode.\n        \"\"\"\n        return self._start_api(api_name, live=True, reload=False, test_override=False)\n\n    def start_debug(self, api_name: str) -&gt; str | None:\n        \"\"\"\n        Start an API in debug mode.\n        \"\"\"\n        return self._start_api(api_name, live=False, reload=True, test_override=True, host=\"localhost\")\n\n    def _start_api(\n        self,\n        api_name: str,\n        live: bool = False,\n        reload: bool = False,\n        test_override: bool = False,\n        host: str = \"localhost\"\n    ) -&gt; str | None:\n        \"\"\"\n        Start an API process with the given configuration.\n\n        Args:\n            api_name (str): The API name.\n            live (bool): Whether to run in live mode.\n            reload (bool): Whether to enable auto-reload.\n            test_override (bool): If True, allow start even if running in a test environment.\n            host (str): Host to bind the API on.\n\n        Returns:\n            Optional[str]: A status message or error message.\n        \"\"\"\n        # Prevent starting an API if in test mode unless explicitly overridden.\n        if 'test' in self.app.id and not test_override:\n            msg = \"No API allowed in test mode\"\n            self.logger.warning(msg)\n            return msg\n\n        if not api_name:\n            self.logger.error(\"No API name provided.\")\n            return None\n\n        # Check if API is already running.\n        if api_name in self.running_apis and self.running_apis[api_name].is_alive():\n            msg = f\"API '{api_name}' is already running.\"\n            self.logger.info(msg)\n            return msg\n\n        # Ensure that live and reload are not both enabled.\n        if live and reload:\n            raise ValueError(\"Live mode and reload mode cannot be enabled simultaneously.\")\n\n        # If configuration does not exist, add it automatically.\n        if api_name not in self.api_config:\n            self.api_config[api_name] = {\n                \"Name\": api_name,\n                \"version\": self.version,\n                \"port\": self.app.args_sto.port,\n                \"host\": host if host and isinstance(host, str) else \"localhost\",\n            }\n            if live:\n                self.api_config[api_name]['host'] = \"0.0.0.0\"\n            self.logger.info(\"Auto-added API configuration for '%s': %s\", api_name, self.api_config[api_name])\n\n        # For live mode, always bind to all interfaces.\n        if live:\n            self.api_config[api_name]['host'] = \"0.0.0.0\"\n\n        api_data = self.api_config[api_name]\n\n        # Check for required frontend dependencies.\n        node_modules_path = os.path.join(self.app.start_dir, \"web\", \"node_modules\")\n        if not os.path.exists(node_modules_path):\n            self.logger.info(\"Node modules folder not found. Installing dependencies in '%s'\", node_modules_path)\n            os.system(\"npm install --prefix ./web ./web\")\n\n        # Build the uvicorn command.\n        cmd_parts: list[str] = [\n            # sys.executable,\n            # \"-m\",\n            \"uvicorn\",\n            \"toolboxv2.mods.FastApi.fast_api_main:app\",\n            f\"--host {api_data['host']}\",\n            f\"--port {api_data['port']}\",\n            f\"--header data:{self.app.debug}:{api_name}\"\n        ]\n        if reload:\n            # Reload directories can be adjusted as needed.\n            cmd_parts.append(\"--reload\")\n            cmd_parts.append(\"--reload-dir ./utils\")\n            cmd_parts.append(\"--reload-dir ./mods/FastApi\")\n        command: str = \" \".join(cmd_parts)\n        self.logger.info(\"Starting API '%s' with command: %s\", api_name, command)\n\n        print(command)\n\n        # Print QR codes for local and public IPs for convenience.\n        protocol = \"http\"  # Adjust if SSL is configured\n        local_url = f\"{protocol}://{get_local_ip()}:{api_data['port']}\"\n        public_url = f\"{protocol}://{get_public_ip()}:{api_data['port']}\"\n        print_qrcode_to_console(local_url)\n        print_qrcode_to_console(public_url)\n\n        try:\n\n            process = multiprocessing.Process(\n                target=os.system,\n                args=(command,),\n                # daemon=True\n            )\n            process.start()\n\n            # Store the process\n            self.running_apis[api_name] = process\n\n            # Save PID to file\n            with open(self._get_pid_file_path(api_name), \"w\") as f:\n                f.write(str(process.pid))\n\n            # Store process info in file handler\n            self.add_to_save_file_handler(\n                key=f\"pr{api_name}\",\n                value=json.dumps({\n                    \"pid\": process.pid,\n                    \"start_time\": datetime.now().isoformat(),\n                    \"host\": api_data['host'],\n                    \"port\": api_data['port']\n                })\n            )\n\n            msg = f\"Starting API '{api_name}' at {api_data['host']}:{api_data['port']} (PID: {process.pid})\"\n            self.logger.info(msg)\n            return msg\n        except Exception as e:\n            self.logger.exception(\"Failed to start API '%s': %s\", api_name, e)\n            return f\"Failed to start API '{api_name}': {e}\"\n\n    async def stop_api(self, api_name: str, delete: bool = True) -&gt; str:\n        \"\"\"\n        Stop a running API and clean up resources.\n        \"\"\"\n        if api_name not in self.api_config:\n            msg = f\"API with the name '{api_name}' is not configured.\"\n            self.logger.warning(msg)\n            return msg\n\n        pid_file = self._get_pid_file_path(api_name)\n        if not os.path.exists(pid_file):\n            self.logger.warning(\"No pid file found for API '%s'\", api_name)\n            return f\"No pid file found for API '{api_name}'.\"\n\n        try:\n            # Read PID from file\n            with open(pid_file) as f:\n                api_pid = int(f.read().strip())\n\n            # Try graceful shutdown first\n            if 'core' in self.app.id:\n                if not await self.app.session.login():\n                    self.logger.warning(\"Could not login with username '%s'\", self.app.get_username())\n                try:\n                    response = await self.app.session.fetch(f\"/api/exit/{api_pid}\", method=\"POST\")\n                    self.logger.info(\"Exit response for API '%s': %s\", api_name, response)\n                except Exception as e:\n                    self.logger.warning(\"Failed to stop API gracefully: %s\", e)\n\n            # Force kill if process still exists\n            process = self.running_apis.get(api_name)\n            if process and process.is_alive():\n                process.terminate()\n                process.join(timeout=5)\n                if process.is_alive():\n                    process.kill()\n\n            # Fallback to system commands if needed\n            try:\n                if system() == \"Windows\":\n                    os.system(f\"taskkill /pid {api_pid} /F\")\n                else:\n                    os.kill(api_pid, signal.SIGKILL)\n            except ProcessLookupError:\n                pass  # Process already terminated\n\n            # Cleanup\n            if os.path.exists(pid_file):\n                os.remove(pid_file)\n            if delete and api_name in self.running_apis:\n                del self.running_apis[api_name]\n\n            # Update file handler\n            self.add_to_save_file_handler(\n                key=f\"pr{api_name}\",\n                value=json.dumps({\n                    \"stop_time\": datetime.now().isoformat(),\n                    \"status\": \"stopped\"\n                })\n            )\n            self.save_file_handler()\n\n            msg = f\"Stopped API '{api_name}'.\"\n            self.logger.info(msg)\n            return msg\n\n        except Exception as e:\n            self.logger.exception(\"Error stopping API '%s': %s\", api_name, e)\n            return f\"Error stopping API '{api_name}': {e}\"\n\n    def nf(self, name):\n        if len(name) &gt; 10:\n            return name[:10]\n        elif len(name) &lt; 10:\n            return name + '~' * (len(name)-10)\n        else:\n            return name\n\n    def show_running(self) -&gt; list[str]:\n        \"\"\"\n        Display and return the list of currently running APIs with their status.\n        \"\"\"\n        self.on_start()\n        running_list = []\n        print(self.api_config)\n        for api_name in self.api_config:\n\n            # Get stored process info\n            process_info = self.get_file_handler(f\"pr{api_name}\")\n            print('#',api_name, '#',process_info)\n            if process_info is None:\n                process_info = {}\n            status = {\n                \"name\": api_name,\n                \"online\": api_name in self.running_apis,\n                \"start_time\": process_info.get(\"start_time\", \"offline\"),\n                \"pid\": process_info.get(\"pid\", ''),\n                \"host\": process_info.get(\"host\", ''),\n                \"port\": process_info.get(\"port\", '')\n            }\n            running_list.append(status)\n\n        # Log and print current status\n        self.logger.info(\"APIs: %s\", running_list)\n        print(\"\\nAPIs:\")\n        for api in running_list:\n            print(f\"- {api['name']}: at {api['host']}:{api['port']}\")\n            print(f\"  Started: {api['start_time']}\")\n\n        return [api[\"name\"] for api in running_list]\n\n    async def restart_api(self, api_name: str) -&gt; str:\n        \"\"\"\n        Restart the given API by stopping it and starting it again.\n\n        Args:\n            api_name (str): The name of the API to restart.\n\n        Returns:\n            str: A status message.\n        \"\"\"\n        stop_message = await self.stop_api(api_name)\n        self.logger.info(\"Restart: %s\", stop_message)\n        # Allow some time for the process to fully terminate.\n        time.sleep(4)\n        start_message = self._start_api(api_name)\n        return f\"Restarting API '{api_name}': {start_message}\"\n\n    def on_start(self) -&gt; None:\n        \"\"\"\n        Load API configuration from file when the tool starts.\n        \"\"\"\n        self.load_file_handler()\n        data = self.get_file_handler(self.keys[\"Apis\"])\n        try:\n            if isinstance(data, str):\n                self.api_config = json.loads(data)\n            else:\n                self.api_config = data\n            self.logger.info(\"Loaded API configuration: %s\", self.api_config)\n        except Exception as e:\n            self.logger.exception(\"Error loading API configuration: %s\", e)\n            self.api_config = {}\n\n    async def on_exit(self) -&gt; None:\n        \"\"\"\n        Gracefully stop all running APIs and save configuration upon exit.\n        \"\"\"\n        # Save configuration data.\n        if len(self.api_config) != 0:\n            self.add_to_save_file_handler(self.keys[\"Apis\"], json.dumps(self.api_config))\n        # Attempt to stop all running APIs.\n        # for api_name in list(self.running_apis.keys()):\n        #     await self.stop_api(api_name, delete=False)\n        self.running_apis = {}\n        self.save_file_handler()\n        self.logger.info(\"Exiting API Manager. All running APIs stopped and configuration saved.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.conf_api","title":"<code>conf_api(api_name, host='localhost', port=5000)</code>","text":"<p>Update or create an API configuration.</p> <p>Parameters:</p> Name Type Description Default <code>api_name</code> <code>str</code> <p>The name of the API.</p> required <code>host</code> <code>str</code> <p>The host address (default \"localhost\"). Use \"lh\" for \"127.0.0.1\" or \"0\" for \"0.0.0.0\".</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>The port number (default 5000; use \"0\" for port 8000).</p> <code>5000</code> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def conf_api(self, api_name: str, host: str = \"localhost\", port: int = 5000) -&gt; None:\n    \"\"\"\n    Update or create an API configuration.\n\n    Args:\n        api_name (str): The name of the API.\n        host (str): The host address (default \"localhost\"). Use \"lh\" for \"127.0.0.1\" or \"0\" for \"0.0.0.0\".\n        port (int): The port number (default 5000; use \"0\" for port 8000).\n    \"\"\"\n    if host.lower() == \"lh\":\n        host = \"127.0.0.1\"\n    if host == \"0\":\n        host = \"0.0.0.0\"\n    if str(port) == \"0\":\n        port = 8000\n\n    self.api_config[api_name] = {\n        \"Name\": api_name,\n        \"version\": self.version,\n        \"port\": int(port),\n        \"host\": host,\n    }\n    self.logger.info(\"Updated API configuration for '%s': %s\", api_name, self.api_config[api_name])\n    print(f\"API configuration updated: {self.api_config[api_name]}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.info","title":"<code>info()</code>","text":"<p>Return diagnostic information about API configurations and currently running APIs.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def info(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Return diagnostic information about API configurations and currently running APIs.\n    \"\"\"\n    config_info = {name: cfg for name, cfg in self.api_config.items()}\n    running_info = {name: proc.pid for name, proc in self.running_apis.items() if proc.is_alive()}\n    self.logger.info(\"API Configurations: %s\", config_info)\n    self.logger.info(\"Running APIs: %s\", running_info)\n    # Optionally, print to console as well\n    for api_name, cfg in config_info.items():\n        print(f\"Configured API - Name: {api_name}, Config: {cfg}\")\n    print(\"Running APIs:\")\n    for api_name, pid in running_info.items():\n        print(f\"API: {api_name}, Process ID: {pid}\")\n    return {\"configurations\": config_info, \"running\": running_info}\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.on_exit","title":"<code>on_exit()</code>  <code>async</code>","text":"<p>Gracefully stop all running APIs and save configuration upon exit.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>async def on_exit(self) -&gt; None:\n    \"\"\"\n    Gracefully stop all running APIs and save configuration upon exit.\n    \"\"\"\n    # Save configuration data.\n    if len(self.api_config) != 0:\n        self.add_to_save_file_handler(self.keys[\"Apis\"], json.dumps(self.api_config))\n    # Attempt to stop all running APIs.\n    # for api_name in list(self.running_apis.keys()):\n    #     await self.stop_api(api_name, delete=False)\n    self.running_apis = {}\n    self.save_file_handler()\n    self.logger.info(\"Exiting API Manager. All running APIs stopped and configuration saved.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.on_start","title":"<code>on_start()</code>","text":"<p>Load API configuration from file when the tool starts.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def on_start(self) -&gt; None:\n    \"\"\"\n    Load API configuration from file when the tool starts.\n    \"\"\"\n    self.load_file_handler()\n    data = self.get_file_handler(self.keys[\"Apis\"])\n    try:\n        if isinstance(data, str):\n            self.api_config = json.loads(data)\n        else:\n            self.api_config = data\n        self.logger.info(\"Loaded API configuration: %s\", self.api_config)\n    except Exception as e:\n        self.logger.exception(\"Error loading API configuration: %s\", e)\n        self.api_config = {}\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.restart_api","title":"<code>restart_api(api_name)</code>  <code>async</code>","text":"<p>Restart the given API by stopping it and starting it again.</p> <p>Parameters:</p> Name Type Description Default <code>api_name</code> <code>str</code> <p>The name of the API to restart.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A status message.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>async def restart_api(self, api_name: str) -&gt; str:\n    \"\"\"\n    Restart the given API by stopping it and starting it again.\n\n    Args:\n        api_name (str): The name of the API to restart.\n\n    Returns:\n        str: A status message.\n    \"\"\"\n    stop_message = await self.stop_api(api_name)\n    self.logger.info(\"Restart: %s\", stop_message)\n    # Allow some time for the process to fully terminate.\n    time.sleep(4)\n    start_message = self._start_api(api_name)\n    return f\"Restarting API '{api_name}': {start_message}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.show_running","title":"<code>show_running()</code>","text":"<p>Display and return the list of currently running APIs with their status.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def show_running(self) -&gt; list[str]:\n    \"\"\"\n    Display and return the list of currently running APIs with their status.\n    \"\"\"\n    self.on_start()\n    running_list = []\n    print(self.api_config)\n    for api_name in self.api_config:\n\n        # Get stored process info\n        process_info = self.get_file_handler(f\"pr{api_name}\")\n        print('#',api_name, '#',process_info)\n        if process_info is None:\n            process_info = {}\n        status = {\n            \"name\": api_name,\n            \"online\": api_name in self.running_apis,\n            \"start_time\": process_info.get(\"start_time\", \"offline\"),\n            \"pid\": process_info.get(\"pid\", ''),\n            \"host\": process_info.get(\"host\", ''),\n            \"port\": process_info.get(\"port\", '')\n        }\n        running_list.append(status)\n\n    # Log and print current status\n    self.logger.info(\"APIs: %s\", running_list)\n    print(\"\\nAPIs:\")\n    for api in running_list:\n        print(f\"- {api['name']}: at {api['host']}:{api['port']}\")\n        print(f\"  Started: {api['start_time']}\")\n\n    return [api[\"name\"] for api in running_list]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.show_version","title":"<code>show_version()</code>","text":"<p>Display and return the current version.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def show_version(self) -&gt; str:\n    \"\"\"Display and return the current version.\"\"\"\n    self.logger.info(\"Version: %s\", self.version)\n    return self.version\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.start_debug","title":"<code>start_debug(api_name)</code>","text":"<p>Start an API in debug mode.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def start_debug(self, api_name: str) -&gt; str | None:\n    \"\"\"\n    Start an API in debug mode.\n    \"\"\"\n    return self._start_api(api_name, live=False, reload=True, test_override=True, host=\"localhost\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.start_dev","title":"<code>start_dev(api_name, *modules, **kwargs)</code>","text":"<p>Start an API in development mode.</p> <p>If additional modules are provided, they are stored in a BlobFile for later use.</p> <p>Parameters:</p> Name Type Description Default <code>api_name</code> <code>str</code> <p>The API name.</p> required <code>*modules</code> <code>str</code> <p>Additional modules for the API.</p> <code>()</code> <p>Returns:</p> Type Description <code>str | None</code> <p>Optional[str]: Status message.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def start_dev(self, api_name: str, *modules: str, **kwargs: Any) -&gt; str | None:\n    \"\"\"\n    Start an API in development mode.\n\n    If additional modules are provided, they are stored in a BlobFile for later use.\n\n    Args:\n        api_name (str): The API name.\n        *modules (str): Additional modules for the API.\n\n    Returns:\n        Optional[str]: Status message.\n    \"\"\"\n    if modules:\n        api_name_dev = f\"{api_name}_D\"\n        with BlobFile(f\"FastApi/{api_name_dev}/dev\", mode='w') as f:\n            f.write_json({'modules': modules})\n        api_name = api_name_dev\n\n    return self._start_api(api_name, live=False, reload=False, test_override=False, host=\"localhost\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.start_live","title":"<code>start_live(api_name)</code>","text":"<p>Start an API in live mode.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>def start_live(self, api_name: str) -&gt; str | None:\n    \"\"\"\n    Start an API in live mode.\n    \"\"\"\n    return self._start_api(api_name, live=True, reload=False, test_override=False)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FastApi.manager.Tools.stop_api","title":"<code>stop_api(api_name, delete=True)</code>  <code>async</code>","text":"<p>Stop a running API and clean up resources.</p> Source code in <code>toolboxv2/mods/FastApi/manager.py</code> <pre><code>async def stop_api(self, api_name: str, delete: bool = True) -&gt; str:\n    \"\"\"\n    Stop a running API and clean up resources.\n    \"\"\"\n    if api_name not in self.api_config:\n        msg = f\"API with the name '{api_name}' is not configured.\"\n        self.logger.warning(msg)\n        return msg\n\n    pid_file = self._get_pid_file_path(api_name)\n    if not os.path.exists(pid_file):\n        self.logger.warning(\"No pid file found for API '%s'\", api_name)\n        return f\"No pid file found for API '{api_name}'.\"\n\n    try:\n        # Read PID from file\n        with open(pid_file) as f:\n            api_pid = int(f.read().strip())\n\n        # Try graceful shutdown first\n        if 'core' in self.app.id:\n            if not await self.app.session.login():\n                self.logger.warning(\"Could not login with username '%s'\", self.app.get_username())\n            try:\n                response = await self.app.session.fetch(f\"/api/exit/{api_pid}\", method=\"POST\")\n                self.logger.info(\"Exit response for API '%s': %s\", api_name, response)\n            except Exception as e:\n                self.logger.warning(\"Failed to stop API gracefully: %s\", e)\n\n        # Force kill if process still exists\n        process = self.running_apis.get(api_name)\n        if process and process.is_alive():\n            process.terminate()\n            process.join(timeout=5)\n            if process.is_alive():\n                process.kill()\n\n        # Fallback to system commands if needed\n        try:\n            if system() == \"Windows\":\n                os.system(f\"taskkill /pid {api_pid} /F\")\n            else:\n                os.kill(api_pid, signal.SIGKILL)\n        except ProcessLookupError:\n            pass  # Process already terminated\n\n        # Cleanup\n        if os.path.exists(pid_file):\n            os.remove(pid_file)\n        if delete and api_name in self.running_apis:\n            del self.running_apis[api_name]\n\n        # Update file handler\n        self.add_to_save_file_handler(\n            key=f\"pr{api_name}\",\n            value=json.dumps({\n                \"stop_time\": datetime.now().isoformat(),\n                \"status\": \"stopped\"\n            })\n        )\n        self.save_file_handler()\n\n        msg = f\"Stopped API '{api_name}'.\"\n        self.logger.info(msg)\n        return msg\n\n    except Exception as e:\n        self.logger.exception(\"Error stopping API '%s': %s\", api_name, e)\n        return f\"Error stopping API '{api_name}': {e}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FileWidget","title":"<code>FileWidget</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.FileWidget.FileUploadHandler","title":"<code>FileUploadHandler</code>","text":"Source code in <code>toolboxv2/mods/FileWidget.py</code> <pre><code>class FileUploadHandler:\n    def __init__(self, upload_dir: str = 'uploads'):\n        self.upload_dir = Path(upload_dir)\n        self.upload_dir.mkdir(parents=True, exist_ok=True)\n        # self.app = get_app().app # If logger is needed here\n\n    def save_file(self, chunk_info: ChunkInfo, storage: BlobStorage) -&gt; str:\n        \"\"\"Speichert die Datei oder Chunk. Chunks werden lokal gespeichert, dann zu BlobStorage gemerged.\"\"\"\n        final_blob_path = Path(chunk_info.filename).name  # Use only filename part for security within blob storage\n\n        if chunk_info.total_chunks == 1:\n            # Komplette Datei direkt in BlobStorage speichern\n            # print(f\"Saving single part file: {final_blob_path} to BlobStorage directly.\") # Debug\n            with BlobFile(final_blob_path, 'w', storage=storage) as bf:\n                bf.write(chunk_info.content)\n        else:\n            # Chunk lokal speichern\n            # Sanitize filename for local path (original chunk_info.filename might contain path parts client-side)\n            safe_base_filename = \"\".join(\n                c if c.isalnum() or c in ('.', '_', '-') else '_' for c in Path(chunk_info.filename).name)\n            chunk_path = self.upload_dir / f\"{safe_base_filename}.part{chunk_info.chunk_index}\"\n            # print(f\"Saving chunk: {chunk_path} locally. Total chunks: {chunk_info.total_chunks}\") # Debug\n\n            with open(chunk_path, 'wb') as f:\n                f.write(chunk_info.content)\n\n            if self._all_chunks_received(safe_base_filename, chunk_info.total_chunks):\n                # print(f\"All chunks received for {safe_base_filename}. Merging to BlobStorage path: {final_blob_path}\") # Debug\n                self._merge_chunks_to_blob(safe_base_filename, chunk_info.total_chunks, final_blob_path, storage)\n                self._cleanup_chunks(safe_base_filename, chunk_info.total_chunks)\n            # else:\n            # print(f\"Still waiting for more chunks for {safe_base_filename}.\") # Debug\n\n        return final_blob_path  # Path within BlobStorage\n\n    def _all_chunks_received(self, safe_base_filename: str, total_chunks: int) -&gt; bool:\n        for i in range(total_chunks):\n            chunk_path = self.upload_dir / f\"{safe_base_filename}.part{i}\"\n            if not chunk_path.exists():\n                # print(f\"Chunk {i} for {safe_base_filename} not found. Path: {chunk_path}\") # Debug\n                return False\n        # print(f\"All {total_chunks} chunks found for {safe_base_filename}.\") # Debug\n        return True\n\n    def _merge_chunks_to_blob(self, safe_base_filename: str, total_chunks: int, final_blob_path: str,\n                              storage: BlobStorage):\n        # print(f\"Merging {total_chunks} chunks for {safe_base_filename} into Blob: {final_blob_path}\") # Debug\n        with BlobFile(final_blob_path, 'w', storage=storage) as outfile:\n            for i in range(total_chunks):\n                chunk_path = self.upload_dir / f\"{safe_base_filename}.part{i}\"\n                # print(f\"Appending chunk {i} ({chunk_path}) to Blob.\") # Debug\n                with open(chunk_path, 'rb') as chunk_file:\n                    outfile.write(chunk_file.read())\n        # print(f\"Finished merging chunks for {safe_base_filename} to Blob: {final_blob_path}\") # Debug\n\n    def _cleanup_chunks(self, safe_base_filename: str, total_chunks: int):\n        # print(f\"Cleaning up {total_chunks} chunks for {safe_base_filename}.\") # Debug\n        for i in range(total_chunks):\n            chunk_path = self.upload_dir / f\"{safe_base_filename}.part{i}\"\n            if chunk_path.exists():\n                # print(f\"Removing chunk: {chunk_path}\") # Debug\n                try:\n                    os.remove(chunk_path)\n                except OSError as e:\n                    # self.app.logger.error(f\"Error removing chunk {chunk_path}: {e}\") # If logger available\n                    print(f\"Error removing chunk {chunk_path}: {e}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FileWidget.FileUploadHandler.save_file","title":"<code>save_file(chunk_info, storage)</code>","text":"<p>Speichert die Datei oder Chunk. Chunks werden lokal gespeichert, dann zu BlobStorage gemerged.</p> Source code in <code>toolboxv2/mods/FileWidget.py</code> <pre><code>def save_file(self, chunk_info: ChunkInfo, storage: BlobStorage) -&gt; str:\n    \"\"\"Speichert die Datei oder Chunk. Chunks werden lokal gespeichert, dann zu BlobStorage gemerged.\"\"\"\n    final_blob_path = Path(chunk_info.filename).name  # Use only filename part for security within blob storage\n\n    if chunk_info.total_chunks == 1:\n        # Komplette Datei direkt in BlobStorage speichern\n        # print(f\"Saving single part file: {final_blob_path} to BlobStorage directly.\") # Debug\n        with BlobFile(final_blob_path, 'w', storage=storage) as bf:\n            bf.write(chunk_info.content)\n    else:\n        # Chunk lokal speichern\n        # Sanitize filename for local path (original chunk_info.filename might contain path parts client-side)\n        safe_base_filename = \"\".join(\n            c if c.isalnum() or c in ('.', '_', '-') else '_' for c in Path(chunk_info.filename).name)\n        chunk_path = self.upload_dir / f\"{safe_base_filename}.part{chunk_info.chunk_index}\"\n        # print(f\"Saving chunk: {chunk_path} locally. Total chunks: {chunk_info.total_chunks}\") # Debug\n\n        with open(chunk_path, 'wb') as f:\n            f.write(chunk_info.content)\n\n        if self._all_chunks_received(safe_base_filename, chunk_info.total_chunks):\n            # print(f\"All chunks received for {safe_base_filename}. Merging to BlobStorage path: {final_blob_path}\") # Debug\n            self._merge_chunks_to_blob(safe_base_filename, chunk_info.total_chunks, final_blob_path, storage)\n            self._cleanup_chunks(safe_base_filename, chunk_info.total_chunks)\n        # else:\n        # print(f\"Still waiting for more chunks for {safe_base_filename}.\") # Debug\n\n    return final_blob_path  # Path within BlobStorage\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FileWidget.access_shared_file","title":"<code>access_shared_file(self, request, share_id, filename=None, row=None)</code>  <code>async</code>","text":"<p>Accesses a shared file via its share_id. The URL for this would be like /api/FileWidget/shared/{share_id_value} The 'share_id: str' in signature implies ToolBoxV2 extracts it from path.</p> Source code in <code>toolboxv2/mods/FileWidget.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, name=\"open_shared\", api_methods=['GET'],\n        request_as_kwarg=True, level=-1, row=True)\nasync def access_shared_file(self, request: RequestData, share_id: str, filename: str = None, row=None) -&gt; Result:  # share_id from query params\n    \"\"\"\n    Accesses a shared file via its share_id.\n    The URL for this would be like /api/FileWidget/shared/{share_id_value}\n    The 'share_id: str' in signature implies ToolBoxV2 extracts it from path.\n    \"\"\"\n    if not share_id:\n        return Result.html(data=\"Share ID is missing in path.\", status=302)\n\n    share_info = self.shares.get(share_id) if self.shares is not None else None\n    if not share_info:\n        return Result.html(data=\"Share link is invalid or has expired.\", status=404)\n\n    owner_uid = share_info[\"owner_uid\"]\n    file_path_in_owner_storage = share_info[\"file_path\"]\n\n    try:\n        # Get BlobStorage for the owner, not the current request's user (if any)\n        owner_storage = await self.get_blob_storage(\n            owner_uid_override=owner_uid)  # Crucially, pass request=None if not needed\n        self.app.logger.info(\n            f\"Accessing shared file via link {share_id}: owner {owner_uid}, path {file_path_in_owner_storage}\")\n        result = await _prepare_file_response(self, owner_storage, file_path_in_owner_storage, row=row is not None)\n        if result.is_error():\n            self.app.logger.error(f\"Error preparing shared file response for {share_id}: {result.info.help_text}\")\n            return Result.html(data=f\"Failed to prepare shared file for download. {result.info.help_text} {result.result.data_info}\")\n        return result\n    except ValueError as e:  # From get_blob_storage if owner_uid is invalid for some reason\n        self.app.logger.error(f\"Error getting owner's storage for shared file {share_id} (owner {owner_uid}): {e}\",\n                              exc_info=True)\n        return Result.html(data=\"Could not access owner's storage for shared file.\")\n    except Exception as e:\n        self.app.logger.error(\n            f\"Error accessing shared file {share_id} (owner {owner_uid}, path {file_path_in_owner_storage}): {e}\",\n            exc_info=True)\n        return Result.html(data=\"Could not retrieve shared file.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FileWidget.get_main_ui","title":"<code>get_main_ui(self)</code>  <code>async</code>","text":"<p>Serves the main HTML UI for the FileWidget.</p> Source code in <code>toolboxv2/mods/FileWidget.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, name=\"ui\", api_methods=['GET'])\nasync def get_main_ui(self) -&gt; Result:\n    \"\"\"Serves the main HTML UI for the FileWidget.\"\"\"\n    html_content = get_template_content()\n    return Result.html(data=html_content)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.FileWidget.handle_upload","title":"<code>handle_upload(self, request, form_data=None)</code>  <code>async</code>","text":"<p>Handles file uploads. Expects chunked data via form_data kwarg from Rust server. 'form_data' structure (from Rust's parsing of multipart) after client sends FormData with fields: 'file' (the blob), 'fileName', 'chunkIndex', 'totalChunks'.</p> <p>Expected <code>form_data</code> in this Python function: {     \"file\": {  // This 'file' key is the NAME of the form field that held the file blob         \"filename\": \"original_file_name_for_this_chunk.txt\", // from Content-Disposition of the 'file' field part         \"content_type\": \"mime/type_of_chunk\",         \"content_base64\": \"BASE64_ENCODED_CHUNK_CONTENT\"     },     \"fileName\": \"overall_final_filename.txt\", // From a separate form field named 'fileName'     \"chunkIndex\": \"0\",                        // From a separate form field named 'chunkIndex'     \"totalChunks\": \"5\"                        // From a separate form field named 'totalChunks' }</p> Source code in <code>toolboxv2/mods/FileWidget.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, name=\"upload\", api_methods=['POST'], request_as_kwarg=True)\nasync def handle_upload(self, request: RequestData, form_data: Optional[Dict[str, Any]] = None) -&gt; Result:\n    \"\"\"\n    Handles file uploads. Expects chunked data via form_data kwarg from Rust server.\n    'form_data' structure (from Rust's parsing of multipart) after client sends FormData with fields:\n    'file' (the blob), 'fileName', 'chunkIndex', 'totalChunks'.\n\n    Expected `form_data` in this Python function:\n    {\n        \"file\": {  // This 'file' key is the NAME of the form field that held the file blob\n            \"filename\": \"original_file_name_for_this_chunk.txt\", // from Content-Disposition of the 'file' field part\n            \"content_type\": \"mime/type_of_chunk\",\n            \"content_base64\": \"BASE64_ENCODED_CHUNK_CONTENT\"\n        },\n        \"fileName\": \"overall_final_filename.txt\", // From a separate form field named 'fileName'\n        \"chunkIndex\": \"0\",                        // From a separate form field named 'chunkIndex'\n        \"totalChunks\": \"5\"                        // From a separate form field named 'totalChunks'\n    }\n    \"\"\"\n    self.app.logger.debug(\n        f\"FileWidget: handle_upload called. Received form_data keys: {list(form_data.keys()) if form_data else 'None'}\"\n    )\n    self.app.logger.debug(f\"FileWidget: handle_upload called. Received form_data: {request.to_dict()}\")\n    # self.app.logger.debug(f\"Full form_data: {form_data}\") # For deeper debugging if needed\n\n    if not form_data:\n        return Result.default_user_error(info=\"No form data received for upload.\", exec_code=400)\n\n    try:\n        storage = await self.get_blob_storage(request)\n\n        # Extract data from form_data (populated by Rust server from multipart)\n        file_field_data = form_data.get('file')  # This is the dict from UploadedFile struct\n        # The 'file_field_data.get('filename')' is the name of the chunk part,\n        # which the JS client sets to be the same as the original file's name.\n        # This is fine for FileUploadHandler.save_file's chunk_info.filename if total_chunks &gt; 1,\n        # as it will be used to create temporary part files like \"original_file_name.txt.part0\".\n\n        overall_filename_from_form = form_data.get('fileName') # This is the target filename for the assembled file.\n        chunk_index_str = form_data.get('chunkIndex')\n        total_chunks_str = form_data.get('totalChunks')\n\n        if not all([\n            file_field_data, isinstance(file_field_data, dict),\n            overall_filename_from_form,\n            chunk_index_str is not None, # Check for presence, not just truthiness (0 is valid)\n            total_chunks_str is not None # Check for presence\n        ]):\n            missing = []\n            if not file_field_data or not isinstance(file_field_data, dict): missing.append(\"'file' object field\")\n            if not overall_filename_from_form: missing.append(\"'fileName' field\")\n            if chunk_index_str is None: missing.append(\"'chunkIndex' field\")\n            if total_chunks_str is None: missing.append(\"'totalChunks' field\")\n\n            self.app.logger.error(\n                f\"Missing critical form data fields for upload: {missing}. Received form_data: {form_data}\")\n            return Result.default_user_error(info=f\"Incomplete upload data. Missing: {', '.join(missing)}\",\n                                             exec_code=400)\n\n        content_base64 = file_field_data.get('content_base64')\n        if not content_base64:\n            return Result.default_user_error(info=\"File content (base64) not found in 'file' field data.\",\n                                             exec_code=400)\n\n        try:\n            content_bytes = base64.b64decode(content_base64)\n        except base64.binascii.Error as b64_error:\n            self.app.logger.error(f\"Base64 decoding failed for upload: {b64_error}\")\n            return Result.default_user_error(info=\"Invalid file content encoding.\", exec_code=400)\n\n        try:\n            chunk_index = int(chunk_index_str)\n            total_chunks = int(total_chunks_str)\n        except ValueError:\n            return Result.default_user_error(info=\"Invalid chunk index or total chunks value. Must be integers.\", exec_code=400)\n\n        # Use the 'overall_filename_from_form' for the ChunkInfo.filename,\n        # as this is the intended final name in blob storage.\n        # FileUploadHandler will use Path(this_name).name to ensure it's just a filename.\n        chunk_info_to_save = ChunkInfo(\n            filename=overall_filename_from_form, # THIS IS THE KEY CHANGE FOR CONSISTENCY\n            chunk_index=chunk_index,\n            total_chunks=total_chunks,\n            content=content_bytes\n        )\n\n        self.app.logger.info(\n            f\"Processing chunk {chunk_index + 1}/{total_chunks} for final file '{overall_filename_from_form}'. \" # Log the intended final name\n            f\"Size: {len(content_bytes)} bytes.\"\n        )\n\n        saved_blob_path = self.upload_handler.save_file(chunk_info_to_save, storage) # saved_blob_path will be Path(overall_filename_from_form).name\n\n        msg = f\"Chunk {chunk_index + 1}/{total_chunks} for '{saved_blob_path}' saved.\"\n        if chunk_info_to_save.chunk_index == chunk_info_to_save.total_chunks - 1:\n            # Check if fully assembled\n            # The 'safe_base_filename' in FileUploadHandler is derived from ChunkInfo.filename,\n            # which we've now set to 'overall_filename_from_form'.\n            # So, this check should work correctly.\n            safe_base_filename_for_check = \"\".join(\n                c if c.isalnum() or c in ('.', '_', '-') else '_' for c in Path(overall_filename_from_form).name)\n\n            # A slight delay might be needed if file system operations are not instantly consistent across threads/processes\n            # For now, assume direct check is okay.\n            # await asyncio.sleep(0.1) # Optional small delay if race conditions are suspected with file system\n\n            if self.upload_handler._all_chunks_received(safe_base_filename_for_check, total_chunks):\n                msg = f\"File '{saved_blob_path}' upload complete and assembled.\"\n                self.app.logger.info(msg)\n            else:\n                msg = f\"Final chunk for '{saved_blob_path}' saved, but assembly check failed or is pending.\"\n                self.app.logger.warning(msg + f\" (Could not verify all chunks for '{safe_base_filename_for_check}' immediately after final one)\")\n\n\n        return Result.ok(data={\"message\": msg, \"path\": saved_blob_path}) # Return the blob-relative path\n\n    except ValueError as e:\n        self.app.logger.error(f\"Upload processing error: {e}\", exc_info=True)\n        return Result.default_user_error(info=f\"Upload error: {str(e)}\",\n                                         exec_code=400 if \"authentication\" in str(e).lower() else 400)\n    except Exception as e:\n        self.app.logger.error(f\"Unexpected error during file upload: {e}\", exc_info=True)\n        return Result.default_internal_error(info=\"An unexpected error occurred during upload.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.P2PRPCClient","title":"<code>P2PRPCClient</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.P2PRPCClient.P2PRPCClient","title":"<code>P2PRPCClient</code>","text":"Source code in <code>toolboxv2/mods/P2PRPCClient.py</code> <pre><code>class P2PRPCClient:\n    def __init__(self, app: App, host: str, port: int, tb_r_key: str = None):\n        self.app = app\n        self.host = host\n        self.port = port\n        self.reader = None\n        self.writer = None\n        self.futures = {}\n        self.code = Code()\n\n        if tb_r_key is None:\n            tb_r_key = os.getenv(\"TB_R_KEY\")\n            if tb_r_key is None:\n                raise ValueError(\"TB_R_KEY environment variable is not set.\")\n\n        if len(tb_r_key) &lt; 24:\n            raise ValueError(\"TB_R_KEY must be at least 24 characters long for security.\")\n        self.auth_key_part = tb_r_key[:24]\n        self.identification_part = tb_r_key[24:]\n        self.session_key = None\n\n    async def connect(self):\n        \"\"\"Connects to the local tcm instance and performs key exchange.\"\"\"\n        try:\n            self.reader, self.writer = await asyncio.open_connection(self.host, self.port)\n            print(f\"RPC Client: Connected to tcm at {self.host}:{self.port}\")\n\n            # Receive encrypted session key from server\n            len_data = await self.reader.readexactly(4)\n            encrypted_session_key_len = int.from_bytes(len_data, 'big')\n            encrypted_session_key = (await self.reader.readexactly(encrypted_session_key_len)).decode('utf-8')\n\n            # Decrypt session key using auth_key_part\n            self.session_key = self.code.decrypt_symmetric(encrypted_session_key, self.auth_key_part)\n\n            # Send challenge back to server, encrypted with session key\n            challenge = \"CHALLENGE_ACK\"\n            encrypted_challenge = self.code.encrypt_symmetric(challenge, self.session_key)\n            self.writer.write(len(encrypted_challenge).to_bytes(4, 'big'))\n            self.writer.write(encrypted_challenge.encode('utf-8'))\n            await self.writer.drain()\n\n            # Start a background task to listen for responses\n            asyncio.create_task(self.listen_for_responses())\n\n        except ConnectionRefusedError:\n            print(f\"RPC Client: Connection to {self.host}:{self.port} refused. Is the tcm peer running?\")\n            raise\n        except Exception as e:\n            print(f\"RPC Client: Error during connection/key exchange: {e}\")\n            raise\n\n    async def listen_for_responses(self):\n        \"\"\"Listens for incoming responses, decrypts them, and resolves the corresponding future.\"\"\"\n        try:\n            while True:\n                len_data = await self.reader.readexactly(4)\n                msg_len = int.from_bytes(len_data, 'big')\n                encrypted_msg_data = (await self.reader.readexactly(msg_len)).decode('utf-8')\n\n                decrypted_msg_data = self.code.decrypt_symmetric(encrypted_msg_data, self.session_key)\n                response = json.loads(decrypted_msg_data)\n\n                call_id = response.get('call_id')\n                if call_id in self.futures:\n                    future = self.futures.pop(call_id)\n                    future.set_result(response)\n        except asyncio.IncompleteReadError:\n            print(\"RPC Client: Connection closed.\")\n        except Exception as e:\n            print(f\"RPC Client: Error listening for responses: {e}\")\n        finally:\n            # Clean up any pending futures\n            for future in self.futures.values():\n                future.set_exception(ConnectionError(\"Connection lost\"))\n            self.futures.clear()\n\n    async def call(self, module: str, function: str, *args, **kwargs):\n        \"\"\"Makes a remote procedure call.\"\"\"\n        if not self.writer:\n            await self.connect()\n\n        call_id = str(uuid.uuid4())\n        request = {\n            \"type\": \"request\",\n            \"call_id\": call_id,\n            \"module\": module,\n            \"function\": function,\n            \"args\": args,\n            \"kwargs\": kwargs,\n            \"identification_part\": self.identification_part\n        }\n\n        future = asyncio.get_running_loop().create_future()\n        self.futures[call_id] = future\n\n        try:\n            request_str = json.dumps(request)\n            encrypted_request = self.code.encrypt_symmetric(request_str, self.session_key)\n\n            self.writer.write(len(encrypted_request).to_bytes(4, 'big'))\n            self.writer.write(encrypted_request.encode('utf-8'))\n            await self.writer.drain()\n\n            # Wait for the response with a timeout\n            response = await asyncio.wait_for(future, timeout=30.0)\n\n            if response.get('error'):\n                return Result(**response['error'])\n            else:\n                return Result.ok(response.get('result'))\n\n        except asyncio.TimeoutError:\n            self.futures.pop(call_id, None)\n            return Result.default_internal_error(\"RPC call timed out.\")\n        except Exception as e:\n            self.futures.pop(call_id, None)\n            return Result.default_internal_error(f\"RPC call failed: {e}\")\n\n    async def close(self):\n        \"\"\"Closes the connection.\"\"\"\n        if self.writer:\n            self.writer.close()\n            await self.writer.wait_closed()\n            print(\"RPC Client: Connection closed.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.P2PRPCClient.P2PRPCClient.call","title":"<code>call(module, function, *args, **kwargs)</code>  <code>async</code>","text":"<p>Makes a remote procedure call.</p> Source code in <code>toolboxv2/mods/P2PRPCClient.py</code> <pre><code>async def call(self, module: str, function: str, *args, **kwargs):\n    \"\"\"Makes a remote procedure call.\"\"\"\n    if not self.writer:\n        await self.connect()\n\n    call_id = str(uuid.uuid4())\n    request = {\n        \"type\": \"request\",\n        \"call_id\": call_id,\n        \"module\": module,\n        \"function\": function,\n        \"args\": args,\n        \"kwargs\": kwargs,\n        \"identification_part\": self.identification_part\n    }\n\n    future = asyncio.get_running_loop().create_future()\n    self.futures[call_id] = future\n\n    try:\n        request_str = json.dumps(request)\n        encrypted_request = self.code.encrypt_symmetric(request_str, self.session_key)\n\n        self.writer.write(len(encrypted_request).to_bytes(4, 'big'))\n        self.writer.write(encrypted_request.encode('utf-8'))\n        await self.writer.drain()\n\n        # Wait for the response with a timeout\n        response = await asyncio.wait_for(future, timeout=30.0)\n\n        if response.get('error'):\n            return Result(**response['error'])\n        else:\n            return Result.ok(response.get('result'))\n\n    except asyncio.TimeoutError:\n        self.futures.pop(call_id, None)\n        return Result.default_internal_error(\"RPC call timed out.\")\n    except Exception as e:\n        self.futures.pop(call_id, None)\n        return Result.default_internal_error(f\"RPC call failed: {e}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.P2PRPCClient.P2PRPCClient.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Closes the connection.</p> Source code in <code>toolboxv2/mods/P2PRPCClient.py</code> <pre><code>async def close(self):\n    \"\"\"Closes the connection.\"\"\"\n    if self.writer:\n        self.writer.close()\n        await self.writer.wait_closed()\n        print(\"RPC Client: Connection closed.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.P2PRPCClient.P2PRPCClient.connect","title":"<code>connect()</code>  <code>async</code>","text":"<p>Connects to the local tcm instance and performs key exchange.</p> Source code in <code>toolboxv2/mods/P2PRPCClient.py</code> <pre><code>async def connect(self):\n    \"\"\"Connects to the local tcm instance and performs key exchange.\"\"\"\n    try:\n        self.reader, self.writer = await asyncio.open_connection(self.host, self.port)\n        print(f\"RPC Client: Connected to tcm at {self.host}:{self.port}\")\n\n        # Receive encrypted session key from server\n        len_data = await self.reader.readexactly(4)\n        encrypted_session_key_len = int.from_bytes(len_data, 'big')\n        encrypted_session_key = (await self.reader.readexactly(encrypted_session_key_len)).decode('utf-8')\n\n        # Decrypt session key using auth_key_part\n        self.session_key = self.code.decrypt_symmetric(encrypted_session_key, self.auth_key_part)\n\n        # Send challenge back to server, encrypted with session key\n        challenge = \"CHALLENGE_ACK\"\n        encrypted_challenge = self.code.encrypt_symmetric(challenge, self.session_key)\n        self.writer.write(len(encrypted_challenge).to_bytes(4, 'big'))\n        self.writer.write(encrypted_challenge.encode('utf-8'))\n        await self.writer.drain()\n\n        # Start a background task to listen for responses\n        asyncio.create_task(self.listen_for_responses())\n\n    except ConnectionRefusedError:\n        print(f\"RPC Client: Connection to {self.host}:{self.port} refused. Is the tcm peer running?\")\n        raise\n    except Exception as e:\n        print(f\"RPC Client: Error during connection/key exchange: {e}\")\n        raise\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.P2PRPCClient.P2PRPCClient.listen_for_responses","title":"<code>listen_for_responses()</code>  <code>async</code>","text":"<p>Listens for incoming responses, decrypts them, and resolves the corresponding future.</p> Source code in <code>toolboxv2/mods/P2PRPCClient.py</code> <pre><code>async def listen_for_responses(self):\n    \"\"\"Listens for incoming responses, decrypts them, and resolves the corresponding future.\"\"\"\n    try:\n        while True:\n            len_data = await self.reader.readexactly(4)\n            msg_len = int.from_bytes(len_data, 'big')\n            encrypted_msg_data = (await self.reader.readexactly(msg_len)).decode('utf-8')\n\n            decrypted_msg_data = self.code.decrypt_symmetric(encrypted_msg_data, self.session_key)\n            response = json.loads(decrypted_msg_data)\n\n            call_id = response.get('call_id')\n            if call_id in self.futures:\n                future = self.futures.pop(call_id)\n                future.set_result(response)\n    except asyncio.IncompleteReadError:\n        print(\"RPC Client: Connection closed.\")\n    except Exception as e:\n        print(f\"RPC Client: Error listening for responses: {e}\")\n    finally:\n        # Clean up any pending futures\n        for future in self.futures.values():\n            future.set_exception(ConnectionError(\"Connection lost\"))\n        self.futures.clear()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.P2PRPCClient.test_rpc_client","title":"<code>test_rpc_client(app, host='127.0.0.1', port=8000, tb_r_key=None)</code>  <code>async</code>","text":"<p>An example of how to use the P2P RPC Client.</p> Source code in <code>toolboxv2/mods/P2PRPCClient.py</code> <pre><code>@export(mod_name=Name, name=\"test_rpc_client\", test=False)\nasync def test_rpc_client(app: App, host: str = '127.0.0.1', port: int = 8000, tb_r_key: str = None):\n    \"\"\"An example of how to use the P2P RPC Client.\"\"\"\n    if tb_r_key is None:\n        tb_r_key = os.getenv(\"TB_R_KEY\")\n        if tb_r_key is None:\n            raise ValueError(\"TB_R_KEY environment variable is not set.\")\n\n    client = P2PRPCClient(app, host, port, tb_r_key)\n    try:\n        await client.connect()\n        # Example: Call the 'list-users' function from the 'helper' module\n        result = await client.call(\"helper\", \"list-users\")\n        result.print()\n    finally:\n        await client.close()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.P2PRPCServer","title":"<code>P2PRPCServer</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.P2PRPCServer.P2PRPCServer","title":"<code>P2PRPCServer</code>","text":"Source code in <code>toolboxv2/mods/P2PRPCServer.py</code> <pre><code>class P2PRPCServer:\n    def __init__(self, app: App, host: str, port: int, tb_r_key: str, function_access_config: dict = None):\n        self.app = app\n        self.host = host\n        self.port = port\n        self.server = None\n        self.code = Code()\n\n        if len(tb_r_key) &lt; 24:\n            raise ValueError(\"TB_R_KEY must be at least 24 characters long for security.\")\n        self.auth_key_part = tb_r_key[:24]\n        self.identification_part_server = tb_r_key[24:]\n\n        self.function_access_config = function_access_config if function_access_config is not None else {}\n\n    async def handle_client(self, reader, writer):\n        \"\"\"Callback to handle a single client connection from a tcm instance.\"\"\"\n        addr = writer.get_extra_info('peername')\n        print(f\"RPC Server: New connection from {addr}\")\n\n        session_key = self.code.generate_symmetric_key()\n        encrypted_session_key = self.code.encrypt_symmetric(session_key, self.auth_key_part)\n\n        try:\n            writer.write(len(encrypted_session_key).to_bytes(4, 'big'))\n            writer.write(encrypted_session_key.encode('utf-8'))\n            await writer.drain()\n\n            len_data = await reader.readexactly(4)\n            encrypted_challenge_len = int.from_bytes(len_data, 'big')\n            encrypted_challenge = (await reader.readexactly(encrypted_challenge_len)).decode('utf-8')\n\n            decrypted_challenge = self.code.decrypt_symmetric(encrypted_challenge, session_key)\n            if decrypted_challenge != \"CHALLENGE_ACK\":\n                raise ValueError(\"Invalid challenge received.\")\n\n            print(f\"RPC Server: Authenticated client {addr}\")\n\n            while True:\n                len_data = await reader.readexactly(4)\n                msg_len = int.from_bytes(len_data, 'big')\n\n                encrypted_msg_data = (await reader.readexactly(msg_len)).decode('utf-8')\n\n                decrypted_msg_data = self.code.decrypt_symmetric(encrypted_msg_data, session_key)\n\n                response = await self.process_rpc(decrypted_msg_data, session_key)\n\n                encrypted_response = self.code.encrypt_symmetric(json.dumps(response), session_key)\n\n                writer.write(len(encrypted_response).to_bytes(4, 'big'))\n                writer.write(encrypted_response.encode('utf-8'))\n                await writer.drain()\n\n        except asyncio.IncompleteReadError:\n            print(f\"RPC Server: Connection from {addr} closed.\")\n        except Exception as e:\n            print(f\"RPC Server: Error with client {addr}: {e}\")\n        finally:\n            writer.close()\n            await writer.wait_closed()\n\n    async def process_rpc(self, msg_data: str, session_key: str) -&gt; dict:\n        \"\"\"Processes a single RPC request and returns a response dictionary.\"\"\"\n        try:\n            call = json.loads(msg_data)\n            if call.get('type') != 'request':\n                raise ValueError(\"Invalid message type\")\n        except (json.JSONDecodeError, ValueError) as e:\n            return self.format_error(call.get('call_id'), -32700, f\"Parse error: {e}\")\n\n        call_id = call.get('call_id')\n        module = call.get('module')\n        function = call.get('function')\n        args = call.get('args', [])\n        kwargs = call.get('kwargs', {})\n        client_identification = call.get('identification_part')\n\n        if not self.is_function_allowed(module, function, client_identification):\n            error_msg = f\"Function '{module}.{function}' is not allowed for identification '{client_identification}'.\"\n            print(f\"RPC Server: {error_msg}\")\n            return self.format_error(call_id, -32601, \"Method not found or not allowed\")\n\n        print(f\"RPC Server: Executing '{module}.{function}' for '{client_identification}'\")\n        try:\n            result: Result = await self.app.a_run_any(\n                (module, function),\n                args_=args,\n                kwargs_=kwargs,\n                get_results=True\n            )\n\n            if result.is_error():\n                return self.format_error(call_id, result.info.get('exec_code', -32000), result.info.get('help_text'), result.get())\n            else:\n                return {\n                    \"type\": \"response\",\n                    \"call_id\": call_id,\n                    \"result\": result.get(),\n                    \"error\": None\n                }\n        except Exception as e:\n            print(f\"RPC Server: Exception during execution of '{module}.{function}': {e}\")\n            return self.format_error(call_id, -32603, \"Internal error during execution\", str(e))\n\n    def is_function_allowed(self, module: str, function: str, client_identification: str) -&gt; bool:\n        \"\"\"Checks if a function is allowed for a given client identification.\"\"\"\n        if module not in self.function_access_config:\n            return False\n\n        allowed_functions_for_module = self.function_access_config[module]\n\n        if function not in allowed_functions_for_module:\n            return False\n\n        # If the function is whitelisted, and there's a specific identification part,\n        # you might want to add more granular control here.\n        # For now, if it's in the whitelist, it's allowed for any identified client.\n        # You could extend function_access_config to be:\n        # {\"ModuleName\": {\"function1\": [\"id1\", \"id2\"], \"function2\": [\"id3\"]}}\n        # For simplicity, current implementation assumes if module.function is in whitelist,\n        # it's generally allowed for any authenticated client.\n        return True\n\n    def format_error(self, call_id, code, message, details=None) -&gt; dict:\n        \"\"\"Helper to create a JSON-RPC error response object.\"\"\"\n        return {\n            \"type\": \"response\",\n            \"call_id\": call_id,\n            \"result\": None,\n            \"error\": {\n                \"code\": code,\n                \"message\": message,\n                \"details\": details\n            }\n        }\n\n    async def start(self):\n        \"\"\"Starts the TCP server.\"\"\"\n        self.server = await asyncio.start_server(\n            self.handle_client, self.host, self.port\n        )\n        addr = self.server.sockets[0].getsockname()\n        print(f\"P2P RPC Server listening on {addr}\")\n        async with self.server:\n            await self.server.serve_forever()\n\n    def stop(self):\n        \"\"\"Stops the TCP server.\"\"\"\n        if self.server:\n            self.server.close()\n            print(\"P2P RPC Server stopped.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.P2PRPCServer.P2PRPCServer.format_error","title":"<code>format_error(call_id, code, message, details=None)</code>","text":"<p>Helper to create a JSON-RPC error response object.</p> Source code in <code>toolboxv2/mods/P2PRPCServer.py</code> <pre><code>def format_error(self, call_id, code, message, details=None) -&gt; dict:\n    \"\"\"Helper to create a JSON-RPC error response object.\"\"\"\n    return {\n        \"type\": \"response\",\n        \"call_id\": call_id,\n        \"result\": None,\n        \"error\": {\n            \"code\": code,\n            \"message\": message,\n            \"details\": details\n        }\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.P2PRPCServer.P2PRPCServer.handle_client","title":"<code>handle_client(reader, writer)</code>  <code>async</code>","text":"<p>Callback to handle a single client connection from a tcm instance.</p> Source code in <code>toolboxv2/mods/P2PRPCServer.py</code> <pre><code>async def handle_client(self, reader, writer):\n    \"\"\"Callback to handle a single client connection from a tcm instance.\"\"\"\n    addr = writer.get_extra_info('peername')\n    print(f\"RPC Server: New connection from {addr}\")\n\n    session_key = self.code.generate_symmetric_key()\n    encrypted_session_key = self.code.encrypt_symmetric(session_key, self.auth_key_part)\n\n    try:\n        writer.write(len(encrypted_session_key).to_bytes(4, 'big'))\n        writer.write(encrypted_session_key.encode('utf-8'))\n        await writer.drain()\n\n        len_data = await reader.readexactly(4)\n        encrypted_challenge_len = int.from_bytes(len_data, 'big')\n        encrypted_challenge = (await reader.readexactly(encrypted_challenge_len)).decode('utf-8')\n\n        decrypted_challenge = self.code.decrypt_symmetric(encrypted_challenge, session_key)\n        if decrypted_challenge != \"CHALLENGE_ACK\":\n            raise ValueError(\"Invalid challenge received.\")\n\n        print(f\"RPC Server: Authenticated client {addr}\")\n\n        while True:\n            len_data = await reader.readexactly(4)\n            msg_len = int.from_bytes(len_data, 'big')\n\n            encrypted_msg_data = (await reader.readexactly(msg_len)).decode('utf-8')\n\n            decrypted_msg_data = self.code.decrypt_symmetric(encrypted_msg_data, session_key)\n\n            response = await self.process_rpc(decrypted_msg_data, session_key)\n\n            encrypted_response = self.code.encrypt_symmetric(json.dumps(response), session_key)\n\n            writer.write(len(encrypted_response).to_bytes(4, 'big'))\n            writer.write(encrypted_response.encode('utf-8'))\n            await writer.drain()\n\n    except asyncio.IncompleteReadError:\n        print(f\"RPC Server: Connection from {addr} closed.\")\n    except Exception as e:\n        print(f\"RPC Server: Error with client {addr}: {e}\")\n    finally:\n        writer.close()\n        await writer.wait_closed()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.P2PRPCServer.P2PRPCServer.is_function_allowed","title":"<code>is_function_allowed(module, function, client_identification)</code>","text":"<p>Checks if a function is allowed for a given client identification.</p> Source code in <code>toolboxv2/mods/P2PRPCServer.py</code> <pre><code>def is_function_allowed(self, module: str, function: str, client_identification: str) -&gt; bool:\n    \"\"\"Checks if a function is allowed for a given client identification.\"\"\"\n    if module not in self.function_access_config:\n        return False\n\n    allowed_functions_for_module = self.function_access_config[module]\n\n    if function not in allowed_functions_for_module:\n        return False\n\n    # If the function is whitelisted, and there's a specific identification part,\n    # you might want to add more granular control here.\n    # For now, if it's in the whitelist, it's allowed for any identified client.\n    # You could extend function_access_config to be:\n    # {\"ModuleName\": {\"function1\": [\"id1\", \"id2\"], \"function2\": [\"id3\"]}}\n    # For simplicity, current implementation assumes if module.function is in whitelist,\n    # it's generally allowed for any authenticated client.\n    return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.P2PRPCServer.P2PRPCServer.process_rpc","title":"<code>process_rpc(msg_data, session_key)</code>  <code>async</code>","text":"<p>Processes a single RPC request and returns a response dictionary.</p> Source code in <code>toolboxv2/mods/P2PRPCServer.py</code> <pre><code>async def process_rpc(self, msg_data: str, session_key: str) -&gt; dict:\n    \"\"\"Processes a single RPC request and returns a response dictionary.\"\"\"\n    try:\n        call = json.loads(msg_data)\n        if call.get('type') != 'request':\n            raise ValueError(\"Invalid message type\")\n    except (json.JSONDecodeError, ValueError) as e:\n        return self.format_error(call.get('call_id'), -32700, f\"Parse error: {e}\")\n\n    call_id = call.get('call_id')\n    module = call.get('module')\n    function = call.get('function')\n    args = call.get('args', [])\n    kwargs = call.get('kwargs', {})\n    client_identification = call.get('identification_part')\n\n    if not self.is_function_allowed(module, function, client_identification):\n        error_msg = f\"Function '{module}.{function}' is not allowed for identification '{client_identification}'.\"\n        print(f\"RPC Server: {error_msg}\")\n        return self.format_error(call_id, -32601, \"Method not found or not allowed\")\n\n    print(f\"RPC Server: Executing '{module}.{function}' for '{client_identification}'\")\n    try:\n        result: Result = await self.app.a_run_any(\n            (module, function),\n            args_=args,\n            kwargs_=kwargs,\n            get_results=True\n        )\n\n        if result.is_error():\n            return self.format_error(call_id, result.info.get('exec_code', -32000), result.info.get('help_text'), result.get())\n        else:\n            return {\n                \"type\": \"response\",\n                \"call_id\": call_id,\n                \"result\": result.get(),\n                \"error\": None\n            }\n    except Exception as e:\n        print(f\"RPC Server: Exception during execution of '{module}.{function}': {e}\")\n        return self.format_error(call_id, -32603, \"Internal error during execution\", str(e))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.P2PRPCServer.P2PRPCServer.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Starts the TCP server.</p> Source code in <code>toolboxv2/mods/P2PRPCServer.py</code> <pre><code>async def start(self):\n    \"\"\"Starts the TCP server.\"\"\"\n    self.server = await asyncio.start_server(\n        self.handle_client, self.host, self.port\n    )\n    addr = self.server.sockets[0].getsockname()\n    print(f\"P2P RPC Server listening on {addr}\")\n    async with self.server:\n        await self.server.serve_forever()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.P2PRPCServer.P2PRPCServer.stop","title":"<code>stop()</code>","text":"<p>Stops the TCP server.</p> Source code in <code>toolboxv2/mods/P2PRPCServer.py</code> <pre><code>def stop(self):\n    \"\"\"Stops the TCP server.\"\"\"\n    if self.server:\n        self.server.close()\n        print(\"P2P RPC Server stopped.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.P2PRPCServer.start_rpc_server","title":"<code>start_rpc_server(app, host='127.0.0.1', port=8888, tb_r_key=None, function_access_config=None)</code>  <code>async</code>","text":"<p>Starts the P2P RPC server.</p> Source code in <code>toolboxv2/mods/P2PRPCServer.py</code> <pre><code>@export(mod_name=Name, name=\"start_server\", test=False)\nasync def start_rpc_server(app: App, host: str = '127.0.0.1', port: int = 8888, tb_r_key: str = None, function_access_config: dict = None):\n    \"\"\"Starts the P2P RPC server.\"\"\"\n    if tb_r_key is None:\n        tb_r_key = os.getenv(\"TB_R_KEY\")\n        if tb_r_key is None:\n            raise ValueError(\"TB_R_KEY environment variable is not set.\")\n\n    server = P2PRPCServer(app, host, port, tb_r_key, function_access_config)\n    try:\n        await server.start()\n    except KeyboardInterrupt:\n        server.stop()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.POA","title":"<code>POA</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.POA.module","title":"<code>module</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.POA.module.ActionManagerEnhanced","title":"<code>ActionManagerEnhanced</code>","text":"Source code in <code>toolboxv2/mods/POA/module.py</code> <pre><code>class ActionManagerEnhanced:\n    DB_ITEMS_PREFIX = \"donext_items\"\n    DB_HISTORY_PREFIX = \"donext_history\"\n    DB_CURRENT_ITEM_PREFIX = \"donext_current_item\"\n    DB_UNDO_LOG_PREFIX = \"donext_undo_log\"\n    DB_SETTINGS_PREFIX = \"donext_settings\"  # Added for user settings\n\n    def __init__(self, app: App, user_id: str):\n        self.app = app\n        self.user_id = user_id\n        self.db = app.get_mod(\"DB\")\n        self.isaa = app.get_mod(\"isaa\")\n\n        self.settings: UserSettings = UserSettings(user_id=user_id)  # Initialize with defaults\n        self.items: List[ActionItem] = []\n        self.history: List[HistoryEntry] = []\n        self.current_item: Optional[ActionItem] = None\n        self.undo_log: List[UndoLogEntry] = []\n\n        self._load_settings()  # Load settings first as they might affect item loading\n        self._load_data()\n\n    def _get_db_key(self, prefix: str) -&gt; str:\n        return f\"{prefix}_{self.user_id}\"\n\n    def get_user_timezone(self) -&gt; pytz.BaseTzInfo:\n        try:\n            return pytz.timezone(self.settings.timezone)\n        except pytz.UnknownTimeZoneError:\n            return pytz.utc\n\n    def _load_settings(self):\n        settings_key = self._get_db_key(self.DB_SETTINGS_PREFIX)\n        try:\n            settings_data = self.db.get(settings_key)\n            if settings_data.is_data() and settings_data.get():\n                loaded_settings = json.loads(settings_data.get()[0]) if isinstance(settings_data.get(),\n                                                                                   list) else json.loads(\n                    settings_data.get())\n                self.settings = UserSettings.model_validate_json_safe(loaded_settings)\n            else:  # Save default settings if not found\n                self._save_settings()\n        except Exception as e:\n            self.app.logger.error(f\"Error loading settings for user {self.user_id}: {e}. Using defaults.\")\n            self.settings = UserSettings(user_id=self.user_id)  # Fallback to defaults\n            self._save_settings()  # Attempt to save defaults\n\n    def _save_settings(self):\n        try:\n            self.db.set(self._get_db_key(self.DB_SETTINGS_PREFIX), json.dumps(self.settings.model_dump_json_safe()))\n        except Exception as e:\n            self.app.logger.error(f\"Error saving settings for user {self.user_id}: {e}\")\n\n    def update_user_settings(self, settings_data: Dict[str, Any]) -&gt; UserSettings:\n        # Ensure user_id is not changed by malicious input\n        current_user_id = self.settings.user_id\n        updated_settings = UserSettings.model_validate(\n            {**self.settings.model_dump(), **settings_data, \"user_id\": current_user_id})\n        self.settings = updated_settings\n        self._save_settings()\n        # Potentially re-process items if timezone change affects interpretations, though this is complex.\n        # For now, new items will use the new timezone. Existing UTC times remain.\n        self.app.logger.info(f\"User {self.user_id} settings updated: Timezone {self.settings.timezone}\")\n        return self.settings\n\n    def _load_data(self):\n        items_key = self._get_db_key(self.DB_ITEMS_PREFIX)\n        history_key = self._get_db_key(self.DB_HISTORY_PREFIX)\n        current_item_key = self._get_db_key(self.DB_CURRENT_ITEM_PREFIX)\n        undo_log_key = self._get_db_key(self.DB_UNDO_LOG_PREFIX)\n        user_tz_str = self.settings.timezone  # For model_validate_json_safe context\n\n        try:\n            items_data = self.db.get(items_key)\n            if items_data.is_data() and items_data.get():\n                loaded_items_raw = json.loads(items_data.get()[0]) if isinstance(items_data.get(),\n                                                                                 list) else json.loads(items_data.get())\n                self.items = [ActionItem.model_validate_json_safe(item_dict, user_timezone_str=user_tz_str) for\n                              item_dict in loaded_items_raw]\n\n            history_data = self.db.get(history_key)\n            if history_data.is_data() and history_data.get():\n                loaded_history_raw = json.loads(history_data.get()[0]) if isinstance(history_data.get(),\n                                                                                     list) else json.loads(\n                    history_data.get())\n                self.history = [HistoryEntry.model_validate_json_safe(entry_dict) for entry_dict in loaded_history_raw]\n\n            current_item_data = self.db.get(current_item_key)\n            if current_item_data.is_data() and current_item_data.get():\n                current_item_dict = json.loads(current_item_data.get()[0]) if isinstance(current_item_data.get(),\n                                                                                         list) else json.loads(\n                    current_item_data.get())\n                if current_item_dict:\n                    self.current_item = ActionItem.model_validate_json_safe(current_item_dict,\n                                                                            user_timezone_str=user_tz_str)\n\n            undo_log_data = self.db.get(undo_log_key)\n            if undo_log_data.is_data() and undo_log_data.get():\n                loaded_undo_raw = json.loads(undo_log_data.get()[0]) if isinstance(undo_log_data.get(),\n                                                                                   list) else json.loads(\n                    undo_log_data.get())\n                self.undo_log = [UndoLogEntry.model_validate_json_safe(entry_dict) for entry_dict in loaded_undo_raw]\n\n        except Exception as e:\n            self.app.logger.error(f\"Error loading data for user {self.user_id}: {e}\")\n            self.items, self.history, self.current_item, self.undo_log = [], [], None, []\n        self._recalculate_next_due_for_all()\n\n    def _save_data(self):\n        try:\n            self.db.set(self._get_db_key(self.DB_ITEMS_PREFIX),\n                        json.dumps([item.model_dump_json_safe() for item in self.items]))\n            self.db.set(self._get_db_key(self.DB_HISTORY_PREFIX),\n                        json.dumps([entry.model_dump_json_safe() for entry in self.history]))\n            self.db.set(self._get_db_key(self.DB_CURRENT_ITEM_PREFIX),\n                        json.dumps(self.current_item.model_dump_json_safe() if self.current_item else None))\n            self.db.set(self._get_db_key(self.DB_UNDO_LOG_PREFIX),\n                        json.dumps([entry.model_dump_json_safe() for entry in self.undo_log]))\n        except Exception as e:\n            self.app.logger.error(f\"Error saving data for user {self.user_id}: {e}\")\n\n    def _add_history_entry(self, item: ActionItem, status_override: Optional[ActionStatus] = None,\n                           notes: Optional[str] = None):\n        entry = HistoryEntry(\n            item_id=item.id, item_title=item.title, item_type=item.item_type,\n            status_changed_to=status_override or item.status,\n            parent_id=item.parent_id, notes=notes\n        )\n        self.history.append(entry)\n\n    def _datetime_to_user_tz(self, dt_utc: Optional[datetime]) -&gt; Optional[datetime]:\n        if not dt_utc: return None\n        if dt_utc.tzinfo is None: dt_utc = pytz.utc.localize(dt_utc)  # Should already be UTC\n        return dt_utc.astimezone(self.get_user_timezone())\n\n    def _datetime_from_user_input_str(self, dt_str: Optional[str]) -&gt; Optional[datetime]:\n        if not dt_str: return None\n        try:\n            dt = isoparse(dt_str)\n            if dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None:  # Naive\n                return self.get_user_timezone().localize(dt).astimezone(pytz.utc)\n            return dt.astimezone(pytz.utc)  # Aware, convert to UTC\n        except ValueError:\n            self.app.logger.warning(f\"Could not parse datetime string: {dt_str}\")\n            return None\n\n    def _recalculate_next_due(self, item: ActionItem):\n        now_utc = datetime.now(pytz.utc)\n        user_tz = self.get_user_timezone()\n\n        if item.status == ActionStatus.COMPLETED and item.item_type == ItemType.TASK:\n            if item.frequency and item.frequency != Frequency.ONE_TIME:\n                base_time_utc = item.last_completed or now_utc  # last_completed is already UTC\n\n                # If item had a fixed_time, align next_due to that time of day in user's timezone\n                if item.fixed_time:\n                    original_fixed_time_user_tz = item.fixed_time.astimezone(user_tz)\n                    # Start from last_completed (or now if missing) in user's timezone for calculation\n                    base_time_user_tz = base_time_utc.astimezone(user_tz)\n\n                    # Ensure base_time_user_tz is at least original_fixed_time_user_tz for alignment\n                    # but calculations should project from last completion.\n                    # For example, if daily task due 9am was completed at 11am, next one is tomorrow 9am.\n                    # If completed at 8am, next one is today 9am (if fixed_time was today 9am) or tomorrow 9am.\n\n                    # Let's use last_completed as the primary anchor for when the *next* cycle starts.\n                    # The original fixed_time's time component is used for the *time of day* of the next due.\n\n                    current_anchor_user_tz = base_time_user_tz\n\n                    # Calculate next occurrence based on frequency\n                    if item.frequency == Frequency.DAILY:\n                        next_due_user_tz_date = (current_anchor_user_tz + timedelta(days=1)).date()\n                    elif item.frequency == Frequency.WEEKLY:\n                        next_due_user_tz_date = (current_anchor_user_tz + timedelta(weeks=1)).date()\n                    elif item.frequency == Frequency.MONTHLY:  # Simplified\n                        next_due_user_tz_date = (current_anchor_user_tz + timedelta(days=30)).date()\n                    elif item.frequency == Frequency.ANNUALLY:\n                        next_due_user_tz_date = (current_anchor_user_tz + timedelta(days=365)).date()\n                    else:  # Should not happen for recurring\n                        item.next_due = None\n                        return\n\n                    # Combine with original time of day\n                    next_due_user_tz = datetime.combine(next_due_user_tz_date, original_fixed_time_user_tz.time(),\n                                                        tzinfo=user_tz)\n                    item.next_due = next_due_user_tz.astimezone(pytz.utc)\n\n                else:  # No original fixed_time, so recur based on current time of completion\n                    if item.frequency == Frequency.DAILY:\n                        item.next_due = base_time_utc + timedelta(days=1)\n                    elif item.frequency == Frequency.WEEKLY:\n                        item.next_due = base_time_utc + timedelta(weeks=1)\n                    elif item.frequency == Frequency.MONTHLY:\n                        item.next_due = base_time_utc + timedelta(days=30)\n                    elif item.frequency == Frequency.ANNUALLY:\n                        item.next_due = base_time_utc + timedelta(days=365)\n\n                # Advance until future if needed (e.g., completing an overdue recurring task)\n                # This loop must operate on user's local time perception of \"next day\"\n                while item.next_due and item.next_due &lt; now_utc:\n                    next_due_user = item.next_due.astimezone(user_tz)\n                    original_time_comp = next_due_user.time()  # Preserve time of day\n\n                    if item.frequency == Frequency.DAILY:\n                        next_due_user_adv = next_due_user + timedelta(days=1)\n                    elif item.frequency == Frequency.WEEKLY:\n                        next_due_user_adv = next_due_user + timedelta(weeks=1)\n                    # For monthly/annually, simple timedelta might shift day of month. Using replace for date part.\n                    elif item.frequency == Frequency.MONTHLY:\n                        # This simplified logic might need dateutil.relativedelta for accuracy\n                        year, month = (next_due_user.year, next_due_user.month + 1) if next_due_user.month &lt; 12 else (\n                            next_due_user.year + 1, 1)\n                        try:\n                            next_due_user_adv = next_due_user.replace(year=year, month=month)\n                        except ValueError:  # Handle e.g. trying to set Feb 30\n                            import calendar\n                            last_day = calendar.monthrange(year, month)[1]\n                            next_due_user_adv = next_due_user.replace(year=year, month=month, day=last_day)\n\n                    elif item.frequency == Frequency.ANNUALLY:\n                        try:\n                            next_due_user_adv = next_due_user.replace(year=next_due_user.year + 1)\n                        except ValueError:  # Handle leap day if original was Feb 29\n                            next_due_user_adv = next_due_user.replace(year=next_due_user.year + 1,\n                                                                      day=28)  # Or March 1st\n                    else:\n                        break\n\n                    item.next_due = user_tz.localize(\n                        datetime.combine(next_due_user_adv.date(), original_time_comp)).astimezone(pytz.utc)\n\n                item.status = ActionStatus.NOT_STARTED  # Reset for next occurrence\n            else:  # One-time task\n                item.next_due = None\n        elif item.status == ActionStatus.NOT_STARTED and item.fixed_time and not item.next_due:\n            item.next_due = item.fixed_time  # fixed_time is already UTC\n\n        # If task is not completed, not started, and has a next_due in the past, but also a fixed_time in the future\n        # (e.g. recurring task whose current instance was missed, but fixed_time points to a specific time for all instances)\n        # ensure next_due is not before fixed_time if fixed_time is relevant for setting.\n        # This logic is complex. Current setup: fixed_time is the \"template\", next_due is the \"instance\".\n\n    def _recalculate_next_due_for_all(self):\n        for item in self.items:\n            self._recalculate_next_due(item)\n\n    def add_item(self, item_data: Dict[str, Any], by_ai: bool = False, imported: bool = False) -&gt; ActionItem:\n        item_data['_user_timezone_str'] = self.settings.timezone  # For validation context\n        item = ActionItem.model_validate(\n            item_data)  # Pydantic handles string-&gt;datetime, then model_validator converts to UTC\n        item.created_by_ai = by_ai\n        item.updated_at = datetime.now(pytz.utc)  # Ensure update\n\n        # Initial next_due for new items if not already set by iCal import logic\n        if not item.next_due and item.fixed_time and item.status == ActionStatus.NOT_STARTED:\n            item.next_due = item.fixed_time\n\n        self.items.append(item)\n        self._add_history_entry(item, status_override=ActionStatus.NOT_STARTED,\n                                notes=\"Item created\" + (\" by AI\" if by_ai else \"\") + (\n                                    \" via import\" if imported else \"\"))\n        if by_ai:\n            self._log_ai_action(\"ai_create_item\", [item.id])\n\n        self._save_data()\n        return item\n\n    def get_item_by_id(self, item_id: str) -&gt; Optional[ActionItem]:\n        return next((item for item in self.items if item.id == item_id), None)\n\n    def update_item(self, item_id: str, update_data: Dict[str, Any], by_ai: bool = False) -&gt; Optional[ActionItem]:\n        item = self.get_item_by_id(item_id)\n        if not item: return None\n\n        previous_data_json = item.model_dump_json() if by_ai else None\n\n        # Pass user timezone for validation context if datetime strings are present\n        update_data_with_tz_context = {**update_data, '_user_timezone_str': self.settings.timezone}\n\n        updated_item_dict = item.model_dump()\n        updated_item_dict.update(update_data_with_tz_context)\n\n        try:\n            # Re-validate the whole model to ensure consistency and proper conversions\n            new_item_state = ActionItem.model_validate(updated_item_dict)\n            # Preserve original ID and created_at, apply new state\n            new_item_state.id = item.id\n            new_item_state.created_at = item.created_at\n            self.items[self.items.index(item)] = new_item_state\n            item = new_item_state\n        except Exception as e:\n            self.app.logger.error(f\"Error validating updated item data: {e}. Update aborted for item {item_id}.\")\n            return None  # Or raise error\n\n        item.updated_at = datetime.now(pytz.utc)\n        item.created_by_ai = by_ai\n\n        self._recalculate_next_due(item)\n        self._add_history_entry(item, notes=\"Item updated\" + (\" by AI\" if by_ai else \"\"))\n\n        if by_ai:\n            self._log_ai_action(\"ai_modify_item\", [item.id],\n                                {item.id: previous_data_json} if previous_data_json else None)\n\n        self._save_data()\n        return item\n\n    def remove_item(self, item_id: str, record_history: bool = True) -&gt; bool:\n        item = self.get_item_by_id(item_id)\n        if not item: return False\n\n        children_ids = [child.id for child in self.items if child.parent_id == item_id]\n        for child_id in children_ids:\n            self.remove_item(child_id, record_history=record_history)\n\n        self.items = [i for i in self.items if i.id != item_id]\n        if self.current_item and self.current_item.id == item_id:\n            self.current_item = None\n\n        if record_history:\n            self._add_history_entry(item, status_override=ActionStatus.CANCELLED, notes=\"Item removed\")\n        self._save_data()\n        return True\n\n    def set_current_item(self, item_id: str) -&gt; Optional[ActionItem]:\n        item = self.get_item_by_id(item_id)\n        if not item: return None\n        if item.status == ActionStatus.COMPLETED and item.item_type == ItemType.TASK and item.frequency == Frequency.ONE_TIME:\n            return None\n\n        self.current_item = item\n        if item.status == ActionStatus.NOT_STARTED:\n            item.status = ActionStatus.IN_PROGRESS\n            item.updated_at = datetime.now(pytz.utc)\n            self._add_history_entry(item, notes=\"Set as current, status to In Progress\")\n        else:\n            self._add_history_entry(item, notes=\"Set as current\")\n        self._save_data()\n        return item\n\n    def complete_current_item(self) -&gt; Optional[ActionItem]:\n        if not self.current_item: return None\n\n        item_to_complete = self.current_item\n        item_to_complete.status = ActionStatus.COMPLETED\n        item_to_complete.last_completed = datetime.now(pytz.utc)\n        item_to_complete.updated_at = datetime.now(pytz.utc)\n\n        self._recalculate_next_due(item_to_complete)\n        self._add_history_entry(item_to_complete, status_override=ActionStatus.COMPLETED, notes=\"Marked as completed\")\n\n        self.current_item = None  # Clear current item after completion\n        self._save_data()\n        return item_to_complete\n\n    def get_suggestions(self, count: int = 2) -&gt; List[ActionItem]:\n        # Prioritize AI suggestions if ISAA is available\n        if self.isaa:\n            active_items_for_ai = []\n            for item in self.items:\n                if item.status != ActionStatus.COMPLETED and item.status != ActionStatus.CANCELLED:\n                    # Convert datetimes to user's local timezone string for AI context\n                    item_dump = item.model_dump_json_safe()  # This is already UTC ISO\n                    # Optionally, convert to user's timezone string if AI is better with local times\n                    # For now, UTC ISO is fine.\n                    active_items_for_ai.append(item_dump)\n\n            MAX_ITEMS_FOR_CONTEXT = 20\n            if len(active_items_for_ai) &gt; MAX_ITEMS_FOR_CONTEXT:\n                active_items_for_ai.sort(\n                    key=lambda x: (x.get('priority', 3), x.get('next_due') or '9999-12-31T23:59:59Z'))\n                active_items_for_ai = active_items_for_ai[:MAX_ITEMS_FOR_CONTEXT]\n\n            now_user_tz_str = datetime.now(self.get_user_timezone()).isoformat()\n\n            prompt = (\n                f\"User's current time: {now_user_tz_str} (Timezone: {self.settings.timezone}). \"\n                f\"Active items (tasks/notes) are provided below (datetimes are in UTC ISO format). \"\n                f\"Suggest the top {count} item IDs to focus on. Consider priority, due dates (next_due), \"\n                f\"and if a current item is set (current_item_id), its sub-items might be relevant. \"\n                f\"Tasks are generally more actionable. Focus on 'not_started' or 'in_progress'.\\n\\n\"\n                f\"Active Items (JSON):\\n{json.dumps(active_items_for_ai, indent=2)}\\n\\n\"\n                f\"Current Item ID: {self.current_item.id if self.current_item else 'None'}\\n\\n\"\n                f\"Return JSON: {{ \\\"suggested_item_ids\\\": [\\\"id1\\\", \\\"id2\\\"] }}.\"\n            )\n\n            class SuggestedIds(BaseModel):\n                suggested_item_ids: List[str]\n\n            try:\n                structured_response = asyncio.run(\n                    self.isaa.format_class(SuggestedIds, prompt, agent_name=\"TaskCompletion\"))\n                if structured_response and isinstance(structured_response, dict):\n                    suggested_ids_model = SuggestedIds(**structured_response)\n                    ai_suggestions = [self.get_item_by_id(id_str) for id_str in suggested_ids_model.suggested_item_ids\n                                      if self.get_item_by_id(id_str)]\n                    if ai_suggestions: return ai_suggestions[:count]\n            except Exception as e:\n                self.app.logger.error(f\"Error getting AI suggestions: {e}\")\n\n        # Fallback to basic suggestions\n        return self._get_basic_suggestions(count)\n\n    def _get_basic_suggestions(self, count: int = 2) -&gt; List[ActionItem]:\n        now_utc = datetime.now(pytz.utc)\n        available_items = [\n            item for item in self.items\n            if item.status in [ActionStatus.NOT_STARTED, ActionStatus.IN_PROGRESS]\n        ]\n\n        if self.current_item:\n            sub_items = [item for item in available_items if item.parent_id == self.current_item.id]\n            # If current item has actionable sub-items, prioritize them\n            if any(s.next_due and s.next_due &lt; (now_utc + timedelta(hours=2)) for s in sub_items) or \\\n                any(s.priority &lt;= 2 for s in sub_items):  # Urgent sub-items (due soon or high priority)\n                available_items = sub_items  # Focus on sub-items\n            # If no urgent sub-items, consider other items too, but maybe give slight preference to other sub-items.\n            # For simplicity now, if current_item is set, and it has sub-items, suggestions come from sub-items.\n            # If no sub-items, or current_item is not set, consider all available_items.\n            elif sub_items:  # Has sub-items, but none are \"urgent\" by above criteria\n                available_items = sub_items\n            # If current_item has no sub_items, then general pool is used.\n\n        def sort_key(item: ActionItem):\n            # Sort by: 1. Due Date (earlier is better, None is last) 2. Priority (lower num is higher)\n            due_date_utc = item.next_due if item.next_due else datetime.max.replace(tzinfo=pytz.utc)\n            return (due_date_utc, item.priority)\n\n        available_items.sort(key=sort_key)\n        return available_items[:count]\n\n    def get_history(self, limit: int = 50) -&gt; List[HistoryEntry]:\n        return sorted(self.history, key=lambda x: x.timestamp, reverse=True)[:limit]\n\n    def get_all_items_hierarchy(self) -&gt; Dict[str, List[Dict[str, Any]]]:\n        # This method remains largely the same, just ensure model_dump_json_safe is used.\n        # Datetimes will be ISO UTC strings. Client JS needs to handle display in user's local time.\n        hierarchy = {\"root\": []}\n        item_map = {item.id: item.model_dump_json_safe() for item in self.items}  # Uses UTC ISO dates\n\n        # This part seems fine, it builds hierarchy based on parent_id\n        processed_ids = set()\n        root_items_temp = []\n\n        for item_id, item_dict in item_map.items():\n            parent_id = item_dict.get(\"parent_id\")\n            if parent_id and parent_id in item_map:\n                if \"children\" not in item_map[parent_id]:\n                    item_map[parent_id][\"children\"] = []\n                item_map[parent_id][\"children\"].append(item_dict)\n            else:\n                root_items_temp.append(item_dict)\n        hierarchy[\"root\"] = root_items_temp\n\n        def sort_children_recursive(node_list):\n            for node_dict in node_list:\n                if \"children\" in node_dict:\n                    # Sort children by priority, then creation date\n                    node_dict[\"children\"].sort(key=lambda x: (x.get('priority', 3), isoparse(x.get('created_at'))))\n                    sort_children_recursive(node_dict[\"children\"])\n\n        # Sort root items\n        hierarchy[\"root\"].sort(key=lambda x: (x.get('priority', 3), isoparse(x.get('created_at'))))\n        sort_children_recursive(hierarchy[\"root\"])\n        return hierarchy\n\n    # --- AI Specific Methods ---\n    async def ai_create_item_from_text(self, text: str) -&gt; Optional[ActionItem]:\n        if not self.isaa:\n            self.app.logger.warning(\"ISAA module not available for AI item creation.\")\n            return None\n\n        class ParsedItemFromText(BaseModel):\n            item_type: Literal[\"task\", \"note\"] = \"task\"\n            title: str\n            description: Optional[str] = None\n            priority: Optional[int] = Field(default=3, ge=1, le=5)\n            due_date_str: Optional[str] = None  # e.g., \"tomorrow\", \"next monday at 5pm\", \"2024-12-25 17:00\"\n            frequency_str: Optional[str] = Field(default=\"one_time\",\n                                                 description=\"e.g. 'daily', 'weekly', 'one_time', 'every friday'\")\n\n        user_tz = self.get_user_timezone()\n        current_time_user_tz_str = datetime.now(user_tz).strftime('%Y-%m-%d %H:%M:%S %Z%z')\n        prompt = (\n            f\"User's current time is {current_time_user_tz_str}. Parse the input into a structured item. \"\n            f\"For due_date_str, interpret relative dates/times based on this current time and output \"\n            f\"a specific date string like 'YYYY-MM-DD HH:MM:SS'. If time is omitted, assume a default like 9 AM. \"\n            f\"If date is omitted but time is given (e.g. 'at 5pm'), assume today if 5pm is future, else tomorrow. \"\n            f\"User input: \\\"{text}\\\"\\n\\n\"\n            f\"Format as JSON for ParsedItemFromText.\"\n        )\n        try:\n            raw_response = await self.isaa.mini_task_completion(prompt, agent_name=\"TaskCompletion\")\n            if not raw_response: self.app.logger.error(\"AI parsing returned empty.\"); return None\n\n            json_str = raw_response\n            if \"```json\" in json_str: json_str = json_str.split(\"```json\")[1].split(\"```\")[0].strip()\n            parsed_dict = json.loads(json_str)\n            parsed_data_model = ParsedItemFromText(**parsed_dict)\n\n            item_constructor_data = {\n                \"item_type\": ItemType(parsed_data_model.item_type),\n                \"title\": parsed_data_model.title,\n                \"description\": parsed_data_model.description,\n                \"priority\": parsed_data_model.priority or 3,\n            }\n\n            if parsed_data_model.due_date_str:\n                # ISAA is prompted to return YYYY-MM-DD HH:MM:SS.\n                # This string is assumed to be in the user's local timezone.\n                # The ActionItem model_validator will convert this to UTC.\n                item_constructor_data[\"fixed_time\"] = parsed_data_model.due_date_str  # Pass as string\n\n            # Frequency parsing (simplified)\n            if parsed_data_model.frequency_str:\n                freq_str_lower = parsed_data_model.frequency_str.lower()\n                if \"daily\" in freq_str_lower:\n                    item_constructor_data[\"frequency\"] = Frequency.DAILY\n                elif \"weekly\" in freq_str_lower:\n                    item_constructor_data[\"frequency\"] = Frequency.WEEKLY\n                elif \"monthly\" in freq_str_lower:\n                    item_constructor_data[\"frequency\"] = Frequency.MONTHLY\n                elif \"annually\" in freq_str_lower or \"yearly\" in freq_str_lower:\n                    item_constructor_data[\"frequency\"] = Frequency.ANNUALLY\n                else:\n                    item_constructor_data[\"frequency\"] = Frequency.ONE_TIME\n\n            return self.add_item(item_constructor_data, by_ai=True)\n        except Exception as e:\n            self.app.logger.error(\n                f\"Error creating item with AI: {e}. Raw: {raw_response if 'raw_response' in locals() else 'N/A'}\")\n            return None\n\n    def _log_ai_action(self, action_type: Literal[\"ai_create_item\", \"ai_modify_item\", \"ical_import\"],\n                       item_ids: List[str], previous_data_map: Optional[Dict[str, str]] = None):\n        entry = UndoLogEntry(action_type=action_type, item_ids=item_ids, previous_data_json_map=previous_data_map)\n        self.undo_log.append(entry)\n        if len(self.undo_log) &gt; 20: self.undo_log = self.undo_log[-20:]\n        # _save_data called by caller\n\n    async def undo_last_ai_action(self) -&gt; bool:  # Also handles iCal import undo\n        if not self.undo_log: return False\n        last_action = self.undo_log.pop()\n        action_undone_count = 0\n\n        if last_action.action_type in [\"ai_create_item\", \"ical_import\"]:\n            for item_id in last_action.item_ids:\n                if self.remove_item(item_id, record_history=False):  # Don't double-log removal for undo\n                    action_undone_count += 1\n        elif last_action.action_type == \"ai_modify_item\":\n            if last_action.previous_data_json_map:\n                for item_id, prev_data_json in last_action.previous_data_json_map.items():\n                    try:\n                        prev_data = ActionItem.model_validate_json_safe(json.loads(prev_data_json),\n                                                                        user_timezone_str=self.settings.timezone)\n                        # Replace item\n                        found = False\n                        for i, item_in_list in enumerate(self.items):\n                            if item_in_list.id == item_id:\n                                self.items[i] = prev_data\n                                if self.current_item and self.current_item.id == item_id:\n                                    self.current_item = prev_data\n                                found = True\n                                break\n                        if found:\n                            action_undone_count += 1\n                        else:\n                            self.app.logger.warning(f\"Could not find item {item_id} to restore during AI undo.\")\n                    except Exception as e:\n                        self.app.logger.error(f\"Error restoring item {item_id} during undo: {e}\")\n            else:  # Should not happen for modify\n                self.app.logger.warning(\n                    f\"Undo for AI modify action on item(s) {last_action.item_ids} had no previous_data_json_map.\")\n\n        if action_undone_count &gt; 0:\n            # Create a generic history entry for the undo action\n            generic_undo_item_title = f\"Related to {len(last_action.item_ids)} item(s)\"\n            if len(last_action.item_ids) == 1:\n                item_for_title = self.get_item_by_id(last_action.item_ids[0])  # Might be None if it was a create undo\n                generic_undo_item_title = item_for_title.title if item_for_title else \"N/A (Undone Action)\"\n\n            self.history.append(HistoryEntry(\n                item_id=last_action.item_ids[0],  # Representative item\n                item_title=generic_undo_item_title,\n                item_type=ItemType.TASK,  # Generic\n                status_changed_to=ActionStatus.CANCELLED,  # Generic status for undo\n                notes=f\"Undid action: {last_action.action_type} for {len(last_action.item_ids)} item(s).\"\n            ))\n            self._save_data()\n            return True\n\n        # If nothing was undone, put action back to log\n        self.undo_log.append(last_action)\n        return False\n\n    # --- iCalendar Methods ---\n    def _parse_ical_dt(self, dt_ical: Union[vDatetime, vDate], user_tz: pytz.BaseTzInfo) -&gt; Optional[datetime]:\n        \"\"\"Converts icalendar vDatetime or vDate to UTC datetime.\"\"\"\n        if not dt_ical: return None\n        dt_val = dt_ical.dt\n\n        if isinstance(dt_val, datetime):\n            if dt_val.tzinfo is None:  # Naive datetime, assume user's local timezone as per iCal spec for floating\n                return user_tz.localize(dt_val).astimezone(pytz.utc)\n            return dt_val.astimezone(pytz.utc)  # Aware datetime\n        elif isinstance(dt_val, date):  # All-day event, represent as start of day in user's TZ, then UTC\n            return user_tz.localize(datetime.combine(dt_val, datetime.min.time())).astimezone(pytz.utc)\n        return None\n\n    def _map_ical_priority_to_app(self, ical_priority: Optional[int]) -&gt; int:\n        if ical_priority is None: return 3  # Default\n        if 1 &lt;= ical_priority &lt;= 4: return 1  # High\n        if ical_priority == 5: return 3  # Medium\n        if 6 &lt;= ical_priority &lt;= 9: return 5  # Low\n        return 3  # Default for 0 or other values\n\n    def _map_app_priority_to_ical(self, app_priority: int) -&gt; int:\n        if app_priority == 1: return 1  # High\n        if app_priority == 2: return 3\n        if app_priority == 3: return 5  # Medium\n        if app_priority == 4: return 7\n        if app_priority == 5: return 9  # Low\n        return 0  # No priority\n\n    def _map_rrule_to_frequency(self, rrule_prop: Optional[vRecur]) -&gt; Tuple[Frequency, Optional[str]]:\n        if not rrule_prop:\n            return Frequency.ONE_TIME, None\n\n        rrule_dict = rrule_prop.to_dict()\n        freq = rrule_dict.get('FREQ')\n        original_rrule_str = vRecur.from_dict(rrule_dict).to_ical().decode('utf-8')\n\n        if freq == 'DAILY': return Frequency.DAILY, original_rrule_str\n        if freq == 'WEEKLY': return Frequency.WEEKLY, original_rrule_str\n        if freq == 'MONTHLY': return Frequency.MONTHLY, original_rrule_str\n        if freq == 'YEARLY': return Frequency.ANNUALLY, original_rrule_str\n\n        # If RRULE is complex or not a direct match, import as ONE_TIME for each instance\n        # but store the original RRULE string for reference or future advanced handling.\n        return Frequency.ONE_TIME, original_rrule_str\n\n    def import_ical_events(self, ical_string: str) -&gt; List[ActionItem]:\n        imported_items: List[ActionItem] = []\n        try:\n            cal = iCalCalendar.from_ical(ical_string)\n            user_tz = self.get_user_timezone()\n            now_utc = datetime.now(pytz.utc)\n            import_limit_date_utc = now_utc + timedelta(days=RECURRING_IMPORT_WINDOW_DAYS)\n\n            processed_uids_for_session = set()  # To avoid processing same base recurring event multiple times in one import\n\n            for component in cal.walk():\n                if component.name == \"VEVENT\":\n                    uid = component.get('uid')\n                    if not uid:\n                        uid = str(uuid.uuid4())  # Generate a UID if missing\n                    else:\n                        uid = uid.to_ical().decode('utf-8')\n\n                    summary = component.get('summary', 'Untitled Event').to_ical().decode('utf-8')\n                    description = component.get('description', '').to_ical().decode('utf-8')\n                    location = component.get('location', '').to_ical().decode('utf-8')\n                    dtstart_ical = component.get('dtstart')\n                    dtend_ical = component.get('dtend')  # Can be used for duration if needed\n                    ical_priority_val = component.get('priority')\n                    ical_priority = int(ical_priority_val.to_ical().decode('utf-8')) if ical_priority_val else None\n\n                    rrule_prop = component.get('rrule')  # This is a vRecur object or None\n\n                    start_time_utc = self._parse_ical_dt(dtstart_ical, user_tz)\n                    if not start_time_utc:\n                        self.app.logger.warning(f\"Skipping event '{summary}' due to missing/invalid DTSTART.\")\n                        continue\n\n                    app_priority = self._map_ical_priority_to_app(ical_priority)\n\n                    # Check for existing item with this iCal UID to potentially update (simplistic check)\n                    # A more robust update would involve comparing sequence numbers, etc.\n                    # For now, if UID exists, we might skip or update. Let's try to update.\n                    # To keep it simpler for now, we will create new items for occurrences.\n                    # UID management needs to be precise for updates.\n                    # If an item is an instance of a recurring event, its UID in our system might be base_uid + occurrence_date.\n\n                    if rrule_prop:\n                        if uid in processed_uids_for_session:  # Already processed this recurring event's base\n                            continue\n                        processed_uids_for_session.add(uid)\n\n                        # Handle recurring event\n                        rrule_str = rrule_prop.to_ical().decode('utf-8')\n                        # Ensure DTSTART is part of the rrule context if not explicitly in rrulestr\n                        if 'DTSTART' not in rrule_str.upper() and start_time_utc:\n                            # dateutil.rrule needs start time; icalendar often bakes it in.\n                            # If start_time_utc is naive, use user_tz to make it aware.\n                            dtstart_for_rrule = start_time_utc.astimezone(\n                                user_tz) if start_time_utc.tzinfo else user_tz.localize(start_time_utc)\n                            # rrule_obj = rrulestr(rrule_str, dtstart=dtstart_for_rrule) # This is complex due to TZ handling in rrulestr\n                            # The icalendar library's component should be timezone aware from DTSTART\n                            # So, let's assume dtstart_ical.dt is the correct starting point.\n                            try:\n                                rrule_obj = rrulestr(rrule_str, dtstart=dtstart_ical.dt)\n                            except Exception as e_rr:\n                                self.app.logger.error(\n                                    f\"Could not parse RRULE '{rrule_str}' for event '{summary}': {e_rr}\")\n                                continue\n\n                        occurrences_imported = 0\n                        # Generate occurrences starting from now (in user's timezone, aligned to event's time)\n                        # or from event's start_time_utc if it's in the future.\n\n                        # The rrule iteration should be in the event's original timezone context if possible,\n                        # or consistently in user's timezone for 'now'.\n                        # Let's use UTC for iteration and then convert.\n\n                        # Iterate from the event's actual start time or now, whichever is later for relevant future instances.\n                        iteration_start_utc = max(now_utc, start_time_utc)\n\n                        for occ_dt_aware in rrule_obj.between(iteration_start_utc, import_limit_date_utc, inc=True):\n                            if occurrences_imported &gt;= MAX_RECURRING_INSTANCES_TO_IMPORT:\n                                break\n\n                            # occ_dt_aware is usually from dateutil.rrule, may need tzinfo set or conversion.\n                            # If rrulestr was given an aware dtstart, occurrences should be aware.\n                            # Ensure it's UTC for our system.\n                            occ_utc = occ_dt_aware.astimezone(pytz.utc) if occ_dt_aware.tzinfo else pytz.utc.localize(\n                                occ_dt_aware)\n\n                            instance_uid = f\"{uid}-{occ_utc.strftime('%Y%m%dT%H%M%S%Z')}\"\n\n                            # Check if this specific instance already exists\n                            existing_instance = next((item for item in self.items if item.ical_uid == instance_uid),\n                                                     None)\n                            if existing_instance:\n                                self.app.logger.info(\n                                    f\"Instance {instance_uid} for '{summary}' already exists. Skipping.\")\n                                continue\n\n                            item_data = {\n                                \"title\": summary, \"description\": description, \"location\": location,\n                                \"item_type\": ItemType.TASK, \"fixed_time\": occ_utc,\n                                \"frequency\": Frequency.ONE_TIME,  # Each imported instance is one-time in our system\n                                \"priority\": app_priority, \"ical_uid\": instance_uid,  # Instance-specific UID\n                                \"status\": ActionStatus.NOT_STARTED,\n                                \"ical_rrule_original\": rrule_str  # Store original rule for reference\n                            }\n                            new_item = self.add_item(item_data, imported=True)\n                            imported_items.append(new_item)\n                            occurrences_imported += 1\n\n                        if occurrences_imported == 0 and start_time_utc &gt; now_utc and start_time_utc &lt;= import_limit_date_utc:\n                            # If it's a future non-recurring event (or rrule didn't yield instances in window but start is in window)\n                            # This case is for when rrule_prop exists but yields no instances in the .between() range,\n                            # but the initial DTSTART itself is valid and upcoming.\n                            # However, rrule.between should include dtstart if inc=True and it's within range.\n                            # This path might be redundant if .between is inclusive and dtstart is in range.\n                            pass\n\n\n                    else:  # Non-recurring event\n                        # Only import if it's upcoming or started recently and not completed (e.g. within last day)\n                        if start_time_utc &lt; (\n                            now_utc - timedelta(days=1)) and not dtend_ical:  # Too old, and no end time to check\n                            self.app.logger.info(f\"Skipping old non-recurring event '{summary}' (UID: {uid})\")\n                            continue\n                        if dtend_ical:\n                            end_time_utc = self._parse_ical_dt(dtend_ical, user_tz)\n                            if end_time_utc and end_time_utc &lt; now_utc:  # Event has already ended\n                                self.app.logger.info(f\"Skipping past event '{summary}' (UID: {uid}) that has ended.\")\n                                continue\n\n                        existing_item = next((item for item in self.items if item.ical_uid == uid), None)\n                        if existing_item:  # Simplistic update: remove old, add new. Better: update in place.\n                            self.app.logger.info(\n                                f\"Event with UID {uid} ('{summary}') already exists. Re-importing (simple replace).\")\n                            self.remove_item(existing_item.id, record_history=False)\n\n                        item_data = {\n                            \"title\": summary, \"description\": description, \"location\": location,\n                            \"item_type\": ItemType.TASK, \"fixed_time\": start_time_utc,\n                            \"frequency\": Frequency.ONE_TIME, \"priority\": app_priority,\n                            \"ical_uid\": uid, \"status\": ActionStatus.NOT_STARTED\n                        }\n                        new_item = self.add_item(item_data, imported=True)\n                        imported_items.append(new_item)\n\n            if imported_items:\n                self._log_ai_action(\"ical_import\", [item.id for item in imported_items])\n            self._save_data()  # Ensure all changes are saved\n            self.app.logger.info(f\"Imported {len(imported_items)} items from iCalendar data.\")\n\n        except Exception as e:\n            self.app.logger.error(f\"Failed to parse iCalendar string: {e}\", exc_info=True)\n            # Potentially re-raise or return empty list with error status\n        return imported_items\n\n    def import_ical_from_url(self, url: str) -&gt; List[ActionItem]:\n        try:\n            headers = {'User-Agent': 'POA_App/1.0 (+https://yourdomain.com/poa_app_info)'}  # Be a good internet citizen\n            response = requests.get(url, timeout=10, headers=headers)\n            response.raise_for_status()  # Raises HTTPError for bad responses (4XX or 5XX)\n            return self.import_ical_events(response.text)\n        except requests.exceptions.RequestException as e:\n            self.app.logger.error(f\"Error fetching iCalendar from URL {url}: {e}\")\n            return []\n        except Exception as e:  # Catch other errors like parsing\n            self.app.logger.error(f\"Error processing iCalendar from URL {url}: {e}\")\n            return []\n\n    def import_ical_from_file_content(self, file_content: bytes) -&gt; List[ActionItem]:\n        try:\n            # Try to decode as UTF-8, but iCal can have other encodings.\n            # Standard is UTF-8. `icalendar` lib handles encoding detection mostly.\n            ical_string = file_content.decode('utf-8', errors='replace')\n            return self.import_ical_events(ical_string)\n        except UnicodeDecodeError as e:\n            self.app.logger.error(f\"Encoding error reading iCalendar file: {e}. Try ensuring UTF-8 encoding.\")\n            # Try with 'latin-1' as a common fallback for some older files\n            try:\n                ical_string = file_content.decode('latin-1', errors='replace')\n                return self.import_ical_events(ical_string)\n            except Exception as e_fallback:\n                self.app.logger.error(f\"Fallback decoding also failed for iCalendar file: {e_fallback}\")\n                return []\n        except Exception as e:\n            self.app.logger.error(f\"Error processing iCalendar file content: {e}\")\n            return []\n\n    def export_to_ical_string(self) -&gt; str:\n        cal = iCalCalendar()\n        cal.add('prodid', '-//POA App//yourdomain.com//')\n        cal.add('version', '2.0')\n        user_tz = self.get_user_timezone()\n\n        for item in self.items:\n            if item.item_type == ItemType.TASK and item.fixed_time:\n                event = iCalEvent()\n                event.add('summary', item.title)\n\n                # Ensure fixed_time is UTC for iCal standard practice\n                dtstart_utc = item.fixed_time\n                if dtstart_utc.tzinfo is None:  # Should not happen if stored correctly\n                    dtstart_utc = pytz.utc.localize(dtstart_utc)\n                else:\n                    dtstart_utc = dtstart_utc.astimezone(pytz.utc)\n                event.add('dtstart', dtstart_utc)  # vDatetime handles UTC conversion for .to_ical()\n\n                # Add DTEND (e.g., 1 hour duration for tasks, or based on item if available)\n                # For simplicity, let's assume 1 hour duration if not specified\n                event.add('dtend', dtstart_utc + timedelta(hours=1))\n\n                event.add('dtstamp', datetime.now(pytz.utc))  # Time the event was created in iCal\n                event.add('uid', item.ical_uid or item.id)  # Use original iCal UID if present, else our ID\n\n                if item.description:\n                    event.add('description', item.description)\n                if item.location:\n                    event.add('location', item.location)\n\n                event.add('priority', self._map_app_priority_to_ical(item.priority))\n\n                # Handle recurrence\n                if item.frequency != Frequency.ONE_TIME:\n                    if item.ical_rrule_original:  # If we have the original complex rule, use it\n                        try:\n                            # vRecur.from_ical requires bytes\n                            event.add('rrule', vRecur.from_ical(item.ical_rrule_original.encode()))\n                        except Exception as e_rrule:\n                            self.app.logger.warning(\n                                f\"Could not parse stored original RRULE '{item.ical_rrule_original}' for item {item.id}: {e_rrule}. Exporting as simple recurrence.\")\n                            # Fallback to simple mapping\n                            self._add_simple_rrule(event, item.frequency)\n                    else:  # Map simple frequency\n                        self._add_simple_rrule(event, item.frequency)\n\n                cal.add_component(event)\n        return cal.to_ical().decode('utf-8')\n\n    def _add_simple_rrule(self, event: iCalEvent, frequency: Frequency):\n        rrule_params = {}\n        if frequency == Frequency.DAILY:\n            rrule_params['freq'] = 'DAILY'\n        elif frequency == Frequency.WEEKLY:\n            rrule_params['freq'] = 'WEEKLY'\n        elif frequency == Frequency.MONTHLY:\n            rrule_params['freq'] = 'MONTHLY'\n        elif frequency == Frequency.ANNUALLY:\n            rrule_params['freq'] = 'YEARLY'\n\n        if rrule_params:\n            event.add('rrule', vRecur(rrule_params))\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager","title":"<code>SchedulerManager</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.SchedulerManagerClass","title":"<code>SchedulerManagerClass</code>","text":"Source code in <code>toolboxv2/mods/SchedulerManager.py</code> <pre><code>class SchedulerManagerClass:\n    def __init__(self):\n        self.jobs = {}\n        self.thread = None\n        self.running = False\n        self.last_successful_jobs = deque(maxlen=3)  # Stores last 3 successful job names\n        self.job_errors = {}  # Stores job names as keys and error messages as values\n\n    def _run(self):\n        while self.running:\n            schedule.run_pending()\n            time.sleep(1)\n\n    def start(self):\n        if not self.running:\n            self.running = True\n            self.thread = threading.Thread(target=self._run, daemon=True)\n            self.thread.start()\n\n    def stop(self):\n        self.running = False\n        if self.thread is not None:\n            self.thread.join()\n\n    def job_wrapper(self, job_name: str, job_function: callable):\n        \"\"\"\n        Wrap a job function to track success and errors.\n        \"\"\"\n        def wrapped_job(*args, **kwargs):\n            try:\n                job_function(*args, **kwargs)\n                # If the job ran successfully, store it in the success queue\n                self.last_successful_jobs.append(job_name)\n                if job_name in self.job_errors:\n                    del self.job_errors[job_name]  # Remove error record if job succeeded after failing\n            except Exception as e:\n                # Capture any exceptions and store them\n                self.job_errors[job_name] = str(e)\n\n        return wrapped_job\n\n\n    def register_job(self,\n                     job_id: str,\n                     second: int = -1,\n                     func: (Callable or str) | None = None,\n                     job: schedule.Job | None = None,\n                     time_passer: schedule.Job | None = None,\n                     object_name: str | None = None,\n                     receive_job: bool = False,\n                     save: bool = False,\n                     max_live: bool = False,\n                     serializer=serializer_default,\n                     args=None, kwargs=None):\n        \"\"\"\n            Parameters\n            ----------\n                job_id : str\n                    id for the job for management\n                second : int\n                    The time interval in seconds between each call of the job.\n                func : Callable or str\n                    The function to be executed as the job.\n                job : schedule.Job\n                    An existing job object from the schedule library.\n                time_passer : schedule.Job\n                    A job without a function, used to specify the time interval.\n                object_name : str\n                    The name of the object containing in the 'func' var to be executed.\n                receive_job : bool\n                    A flag indicating whether the job should be received from an object from 'func' var.\n                save : bool\n                    A flag indicating whether the job should be saved.\n                max_live : bool\n                    A flag indicating whether the job should have a maximum live time.\n                serializer : dill\n                    json pickel or dill must have a dumps fuction\n                *args, **kwargs : Any serializable and deserializable\n                    Additional arguments to be passed to the job function.\n\n            Returns\n            -------\n           \"\"\"\n\n        if job is None and func is None:\n            return Result.default_internal_error(\"Both job and func are not specified.\"\n                                                 \" Please specify either job or func.\")\n        if job is not None and func is not None:\n            return Result.default_internal_error(\"Both job and func are specified. Please specify either job or func.\")\n\n        if job is not None:\n            def func(x):\n                return x\n            return self._save_job(job_id=job_id,\n                                  job=job,\n                                  save=save,\n                                  func=func,\n                                  args=args,\n                                  kwargs=kwargs,\n                                  serializer=serializer)\n\n        parsed_attr = self._parse_function(func=func, object_name=object_name)\n\n        if parsed_attr.is_error():\n            parsed_attr.result.data_info = f\"Error parsing function for job : {job_id}\"\n            return parsed_attr\n\n        if receive_job:\n            job = parsed_attr.get()\n        else:\n            func = parsed_attr.get()\n\n        time_passer = self._prepare_time_passer(time_passer=time_passer,\n                                                second=second)\n\n        job_func = self._prepare_job_func(func=func,\n                                          max_live=max_live,\n                                          second=second,\n                                          args=args,\n                                          kwargs=kwargs,\n                                          job_id=job_id)\n\n        job = self._get_final_job(job=job,\n                                  func=self.job_wrapper(job_id, job_func),\n                                  time_passer=time_passer,\n                                  job_func=job_func,\n                                  args=args,\n                                  kwargs=kwargs)\n        if job.is_error():\n            return job\n\n        job = job.get()\n\n        return self._save_job(job_id=job_id,\n                              job=job,\n                              save=save,\n                              func=func,\n                              args=args,\n                              kwargs=kwargs,\n                              serializer=serializer)\n\n    @staticmethod\n    def _parse_function(func: str or Callable, object_name):\n        if isinstance(func, str) and func.endswith('.py'):\n            with open(func) as file:\n                func_code = file.read()\n                exec(func_code)\n                func = locals()[object_name]\n        elif isinstance(func, str) and func.endswith('.dill') and safety_mode == 'open':\n            try:\n                with open(func, 'rb') as file:\n                    func = dill.load(file)\n            except FileNotFoundError:\n                return Result.default_internal_error(f\"Function file {func} not found or dill not installed\")\n        elif isinstance(func, str):\n            local_vars = {'app': get_app(from_=Name + f\".pasing.{object_name}\")}\n            try:\n                exec(func.strip(), {}, local_vars)\n            except Exception as e:\n                return Result.default_internal_error(f\"Function parsing failed withe {e}\")\n            func = local_vars[object_name]\n        elif isinstance(func, Callable):\n            pass\n        else:\n            return Result.default_internal_error(\"Could not parse object scheduler_manager.parse_function\")\n        return Result.ok(func)\n\n    @staticmethod\n    def _prepare_time_passer(time_passer, second):\n        if time_passer is None and second &gt; 0:\n            return schedule.every(second).seconds\n        elif time_passer is None and second &lt;= 0:\n            raise ValueError(\"second must be greater than 0\")\n        return time_passer\n\n    def _prepare_job_func(self, func: Callable, max_live: bool, second: float, job_id: str, *args, **kwargs):\n        if max_live:\n            end_time = datetime.now() + timedelta(seconds=second)\n\n            def job_func():\n                if datetime.now() &lt; end_time:\n                    func(*args, **kwargs)\n                else:\n                    job = self.jobs.get(job_id, {}).get('job')\n                    if job is not None:\n                        schedule.cancel_job(job)\n                    else:\n                        print(\"Error Canceling job\")\n\n            return job_func\n        return func\n\n    @staticmethod\n    def _get_final_job(job, func, time_passer, job_func, args, kwargs):\n        if job is None and isinstance(func, Callable):\n            job = time_passer.do(job_func, *args, **kwargs)\n        elif job is not None:\n            pass\n        else:\n            return Result.default_internal_error(\"No Final job found for register\")\n        return Result.ok(job)\n\n    def _save_job(self, job_id, job, save, args=None, **kwargs):\n        if job is not None:\n            self.jobs[job_id] = {'id': job_id, 'job': job, 'save': save, 'func': job_id, 'args': args,\n                                 'kwargs': kwargs}\n            f = (f\"Added Job {job_id} :{' - saved' if save else ''}\"\n                  f\"{' - args ' + str(len(args)) if args else ''}\"\n                  f\"{' - kwargs ' + str(len(kwargs.keys())) if kwargs else ''}\")\n            return Result.ok(f)\n        else:\n            return Result.default_internal_error(job_id)\n\n    def cancel_job(self, job_id):\n        if job_id not in self.jobs:\n            print(\"Job not found\")\n            return\n        schedule.cancel_job(self.jobs[job_id].get('job'))\n        self.jobs[job_id][\"cancelled\"] = True\n        self.jobs[job_id][\"save\"] = False\n        print(\"Job cancelled\")\n\n    def del_job(self, job_id):\n        if job_id not in self.jobs:\n            print(\"Job not found\")\n            return\n        if not self.jobs[job_id].get(\"cancelled\", False):\n            print(\"Job not cancelled canceling job\")\n            self.cancel_job(job_id)\n        del self.jobs[job_id]\n        print(\"Job deleted\")\n\n    def save_jobs(self, file_path, serializer=serializer_default):\n        with open(file_path, 'wb') as file:\n            save_jobs = [job for job in self.jobs.values() if job['save']]\n            serializer.dump(save_jobs, file)\n\n    def load_jobs(self, file_path, deserializer=deserializer_default):\n        with open(file_path, 'rb') as file:\n            jobs = deserializer.load(file)\n            for job_info in jobs:\n                del job_info['job']\n                func = deserializer.loads(job_info['func'])\n                self.register_job(job_info['id'], func=func, **job_info)\n\n    def get_tasks_table(self):\n        if not self.jobs:\n            return \"No tasks registered.\"\n\n        # Calculate the maximum width for each column\n        id_width = max(len(\"Task ID\"), max(len(job_id) for job_id in self.jobs))\n        next_run_width = len(\"Next Execution\")\n        interval_width = len(\"Interval\")\n\n        # Create the header\n        header = f\"| {'Task ID':&lt;{id_width}} | {'Next Execution':&lt;{next_run_width}} | {'Interval':&lt;{interval_width}} |\"\n        separator = f\"|{'-' * (id_width + 2)}|{'-' * (next_run_width + 2)}|{'-' * (interval_width + 2)}|\"\n\n        # Create the table rows\n        rows = []\n        for job_id, job_info in self.jobs.items():\n            job = job_info['job']\n            next_run = job.next_run.strftime(\"%Y-%m-%d %H:%M:%S\") if job.next_run else \"N/A\"\n            interval = self._get_interval_str(job)\n            row = f\"| {job_id:&lt;{id_width}} | {next_run:&lt;{next_run_width}} | {interval:&lt;{interval_width}} |\"\n            rows.append(row)\n\n        # Combine all parts of the table\n        table = \"\\n\".join([header, separator] + rows)\n        return table\n\n    def _get_interval_str(self, job):\n        if job.interval == 0:\n            return \"Once\"\n\n        units = [\n            (86400, \"day\"),\n            (3600, \"hour\"),\n            (60, \"minute\"),\n            (1, \"second\")\n        ]\n\n        for seconds, unit in units:\n            if job.interval % seconds == 0:\n                count = job.interval // seconds\n                return f\"Every {count} {unit}{'s' if count &gt; 1 else ''}\"\n\n        return f\"Every {job.interval} seconds\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.SchedulerManagerClass.job_wrapper","title":"<code>job_wrapper(job_name, job_function)</code>","text":"<p>Wrap a job function to track success and errors.</p> Source code in <code>toolboxv2/mods/SchedulerManager.py</code> <pre><code>def job_wrapper(self, job_name: str, job_function: callable):\n    \"\"\"\n    Wrap a job function to track success and errors.\n    \"\"\"\n    def wrapped_job(*args, **kwargs):\n        try:\n            job_function(*args, **kwargs)\n            # If the job ran successfully, store it in the success queue\n            self.last_successful_jobs.append(job_name)\n            if job_name in self.job_errors:\n                del self.job_errors[job_name]  # Remove error record if job succeeded after failing\n        except Exception as e:\n            # Capture any exceptions and store them\n            self.job_errors[job_name] = str(e)\n\n    return wrapped_job\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.SchedulerManagerClass.register_job","title":"<code>register_job(job_id, second=-1, func=None, job=None, time_passer=None, object_name=None, receive_job=False, save=False, max_live=False, serializer=serializer_default, args=None, kwargs=None)</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.SchedulerManagerClass.register_job--parameters","title":"Parameters","text":"<pre><code>job_id : str\n    id for the job for management\nsecond : int\n    The time interval in seconds between each call of the job.\nfunc : Callable or str\n    The function to be executed as the job.\njob : schedule.Job\n    An existing job object from the schedule library.\ntime_passer : schedule.Job\n    A job without a function, used to specify the time interval.\nobject_name : str\n    The name of the object containing in the 'func' var to be executed.\nreceive_job : bool\n    A flag indicating whether the job should be received from an object from 'func' var.\nsave : bool\n    A flag indicating whether the job should be saved.\nmax_live : bool\n    A flag indicating whether the job should have a maximum live time.\nserializer : dill\n    json pickel or dill must have a dumps fuction\n*args, **kwargs : Any serializable and deserializable\n    Additional arguments to be passed to the job function.\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.SchedulerManagerClass.register_job--returns","title":"Returns","text":"Source code in <code>toolboxv2/mods/SchedulerManager.py</code> <pre><code>def register_job(self,\n                 job_id: str,\n                 second: int = -1,\n                 func: (Callable or str) | None = None,\n                 job: schedule.Job | None = None,\n                 time_passer: schedule.Job | None = None,\n                 object_name: str | None = None,\n                 receive_job: bool = False,\n                 save: bool = False,\n                 max_live: bool = False,\n                 serializer=serializer_default,\n                 args=None, kwargs=None):\n    \"\"\"\n        Parameters\n        ----------\n            job_id : str\n                id for the job for management\n            second : int\n                The time interval in seconds between each call of the job.\n            func : Callable or str\n                The function to be executed as the job.\n            job : schedule.Job\n                An existing job object from the schedule library.\n            time_passer : schedule.Job\n                A job without a function, used to specify the time interval.\n            object_name : str\n                The name of the object containing in the 'func' var to be executed.\n            receive_job : bool\n                A flag indicating whether the job should be received from an object from 'func' var.\n            save : bool\n                A flag indicating whether the job should be saved.\n            max_live : bool\n                A flag indicating whether the job should have a maximum live time.\n            serializer : dill\n                json pickel or dill must have a dumps fuction\n            *args, **kwargs : Any serializable and deserializable\n                Additional arguments to be passed to the job function.\n\n        Returns\n        -------\n       \"\"\"\n\n    if job is None and func is None:\n        return Result.default_internal_error(\"Both job and func are not specified.\"\n                                             \" Please specify either job or func.\")\n    if job is not None and func is not None:\n        return Result.default_internal_error(\"Both job and func are specified. Please specify either job or func.\")\n\n    if job is not None:\n        def func(x):\n            return x\n        return self._save_job(job_id=job_id,\n                              job=job,\n                              save=save,\n                              func=func,\n                              args=args,\n                              kwargs=kwargs,\n                              serializer=serializer)\n\n    parsed_attr = self._parse_function(func=func, object_name=object_name)\n\n    if parsed_attr.is_error():\n        parsed_attr.result.data_info = f\"Error parsing function for job : {job_id}\"\n        return parsed_attr\n\n    if receive_job:\n        job = parsed_attr.get()\n    else:\n        func = parsed_attr.get()\n\n    time_passer = self._prepare_time_passer(time_passer=time_passer,\n                                            second=second)\n\n    job_func = self._prepare_job_func(func=func,\n                                      max_live=max_live,\n                                      second=second,\n                                      args=args,\n                                      kwargs=kwargs,\n                                      job_id=job_id)\n\n    job = self._get_final_job(job=job,\n                              func=self.job_wrapper(job_id, job_func),\n                              time_passer=time_passer,\n                              job_func=job_func,\n                              args=args,\n                              kwargs=kwargs)\n    if job.is_error():\n        return job\n\n    job = job.get()\n\n    return self._save_job(job_id=job_id,\n                          job=job,\n                          save=save,\n                          func=func,\n                          args=args,\n                          kwargs=kwargs,\n                          serializer=serializer)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.Tools","title":"<code>Tools</code>","text":"<p>               Bases: <code>MainTool</code>, <code>SchedulerManagerClass</code></p> Source code in <code>toolboxv2/mods/SchedulerManager.py</code> <pre><code>class Tools(MainTool, SchedulerManagerClass):\n    version = version\n\n    def __init__(self, app=None):\n        self.name = Name\n        self.color = \"VIOLET2\"\n\n        self.keys = {\"mode\": \"db~mode~~:\"}\n        self.encoding = 'utf-8'\n        self.tools = {'name': Name}\n\n        SchedulerManagerClass.__init__(self)\n        MainTool.__init__(self,\n                          load=self.init_sm,\n                          v=self.version,\n                          name=self.name,\n                          color=self.color,\n                          on_exit=self.on_exit)\n\n\n    @export(\n        mod_name=Name,\n        name=\"Version\",\n        version=version,\n    )\n    def get_version(self):\n        return self.version\n\n    # Exportieren der Scheduler-Instanz f\u00fcr die Nutzung in anderen Modulen\n    @export(mod_name=Name, name='init', version=version, initial=True)\n    def init_sm(self):\n        if os.path.exists(self.app.data_dir + '/jobs.compact'):\n            print(\"SchedulerManager try loading from file\")\n            self.load_jobs(\n                self.app.data_dir + '/jobs.compact'\n            )\n            print(\"SchedulerManager Successfully loaded\")\n        print(\"STARTING SchedulerManager\")\n        self.start()\n\n    @export(mod_name=Name, name='clos_manager', version=version, exit_f=True)\n    def on_exit(self):\n        self.stop()\n        self.save_jobs(self.app.data_dir + '/jobs.compact')\n        return f\"saved {len(self.jobs.keys())} jobs in {self.app.data_dir + '/jobs.compact'}\"\n\n    @export(mod_name=Name, name='instance', version=version)\n    def get_instance(self):\n        return self\n\n    @export(mod_name=Name, name='start', version=version)\n    def start_instance(self):\n        return self.start()\n\n    @export(mod_name=Name, name='stop', version=version)\n    def stop_instance(self):\n        return self.stop()\n\n    @export(mod_name=Name, name='cancel', version=version)\n    def cancel_instance(self, job_id):\n        return self.cancel_job(job_id)\n\n    @export(mod_name=Name, name='dealt', version=version)\n    def dealt_instance(self, job_id):\n        return self.del_job(job_id)\n\n    @export(mod_name=Name, name='add', version=version)\n    def register_instance(self, job_data: dict):\n        \"\"\"\n        example dicts :\n            -----------\n            {\n                \"job_id\": \"job0\",\n                \"second\": 0,\n                \"func\": None,\n                \"job\": None,\n                \"time_passer\": None,\n                \"object_name\": \"tb_job_fuction\",\n                \"receive_job\": False,\n                \"save\": False,\n                \"max_live\": True,\n                # just lev it out \"serializer\": serializer_default,\n                \"args\": [],\n                \"kwargs\": {},\n            }\n\n            job_id : str\n                id for the job for management\n            second (optional): int\n                The time interval in seconds between each call of the job.\n            func (optional): Callable or str\n                The function to be executed as the job.\n            job (optional):  schedule.Job\n                An existing job object from the schedule library.\n            time_passer (optional):  schedule.Job\n                A job without a function, used to specify the time interval.\n            object_name (optional): str\n                The name of the object containing in the 'func' var to be executed.\n            receive_job (optional): bool\n                A flag indicating whether the job should be received from an object from 'func' var.\n            save (optional): bool\n                A flag indicating whether the job should be saved.\n            max_live (optional): bool\n                A flag indicating whether the job should have a maximum live time.\n            serializer (optional): bool\n                json pickel or dill must have a dumps fuction\n            *args, **kwargs (optional):\n                Additional arguments to be passed to the job function.\n\n\n        Parameters\n            ----------\n           job_data : dict\n\n        example usage\n            ----------\n            `python\n\n            `\n\n    \"\"\"\n        if job_data is None:\n            self.app.logger.error(\"No job data provided\")\n            return None\n        job_id = job_data[\"job_id\"]\n        second = job_data.get(\"second\", 0)\n        func = job_data.get(\"func\")\n        job = job_data.get(\"job\")\n        time_passer = job_data.get(\"time_passer\")\n        object_name = job_data.get(\"object_name\", \"tb_job_fuction\")\n        receive_job = job_data.get(\"receive_job\", False)\n        save = job_data.get(\"save\", False)\n        max_live = job_data.get(\"max_live\", True)\n        serializer = job_data.get(\"serializer\", serializer_default)\n        args = job_data.get(\"args\", ())\n        kwargs = job_data.get(\"kwargs\", {})\n\n        return self.register_job(\n            job_id=job_id,\n            second=second,\n            func=func,\n            job=job,\n            time_passer=time_passer,\n            object_name=object_name,\n            receive_job=receive_job,\n            save=save,\n            max_live=max_live,\n            serializer=serializer,\n            args=args,\n            kwargs=kwargs\n        )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SchedulerManager.Tools.register_instance","title":"<code>register_instance(job_data)</code>","text":"example dicts <p>{     \"job_id\": \"job0\",     \"second\": 0,     \"func\": None,     \"job\": None,     \"time_passer\": None,     \"object_name\": \"tb_job_fuction\",     \"receive_job\": False,     \"save\": False,     \"max_live\": True,     # just lev it out \"serializer\": serializer_default,     \"args\": [],     \"kwargs\": {}, }</p> <p>job_id : str     id for the job for management second (optional): int     The time interval in seconds between each call of the job. func (optional): Callable or str     The function to be executed as the job. job (optional):  schedule.Job     An existing job object from the schedule library. time_passer (optional):  schedule.Job     A job without a function, used to specify the time interval. object_name (optional): str     The name of the object containing in the 'func' var to be executed. receive_job (optional): bool     A flag indicating whether the job should be received from an object from 'func' var. save (optional): bool     A flag indicating whether the job should be saved. max_live (optional): bool     A flag indicating whether the job should have a maximum live time. serializer (optional): bool     json pickel or dill must have a dumps fuction args, *kwargs (optional):     Additional arguments to be passed to the job function.</p> <p>Parameters     ----------    job_data : dict</p> <p>example usage     ----------     `python</p> <pre><code>`\n</code></pre> Source code in <code>toolboxv2/mods/SchedulerManager.py</code> <pre><code>@export(mod_name=Name, name='add', version=version)\ndef register_instance(self, job_data: dict):\n    \"\"\"\n    example dicts :\n        -----------\n        {\n            \"job_id\": \"job0\",\n            \"second\": 0,\n            \"func\": None,\n            \"job\": None,\n            \"time_passer\": None,\n            \"object_name\": \"tb_job_fuction\",\n            \"receive_job\": False,\n            \"save\": False,\n            \"max_live\": True,\n            # just lev it out \"serializer\": serializer_default,\n            \"args\": [],\n            \"kwargs\": {},\n        }\n\n        job_id : str\n            id for the job for management\n        second (optional): int\n            The time interval in seconds between each call of the job.\n        func (optional): Callable or str\n            The function to be executed as the job.\n        job (optional):  schedule.Job\n            An existing job object from the schedule library.\n        time_passer (optional):  schedule.Job\n            A job without a function, used to specify the time interval.\n        object_name (optional): str\n            The name of the object containing in the 'func' var to be executed.\n        receive_job (optional): bool\n            A flag indicating whether the job should be received from an object from 'func' var.\n        save (optional): bool\n            A flag indicating whether the job should be saved.\n        max_live (optional): bool\n            A flag indicating whether the job should have a maximum live time.\n        serializer (optional): bool\n            json pickel or dill must have a dumps fuction\n        *args, **kwargs (optional):\n            Additional arguments to be passed to the job function.\n\n\n    Parameters\n        ----------\n       job_data : dict\n\n    example usage\n        ----------\n        `python\n\n        `\n\n\"\"\"\n    if job_data is None:\n        self.app.logger.error(\"No job data provided\")\n        return None\n    job_id = job_data[\"job_id\"]\n    second = job_data.get(\"second\", 0)\n    func = job_data.get(\"func\")\n    job = job_data.get(\"job\")\n    time_passer = job_data.get(\"time_passer\")\n    object_name = job_data.get(\"object_name\", \"tb_job_fuction\")\n    receive_job = job_data.get(\"receive_job\", False)\n    save = job_data.get(\"save\", False)\n    max_live = job_data.get(\"max_live\", True)\n    serializer = job_data.get(\"serializer\", serializer_default)\n    args = job_data.get(\"args\", ())\n    kwargs = job_data.get(\"kwargs\", {})\n\n    return self.register_job(\n        job_id=job_id,\n        second=second,\n        func=func,\n        job=job,\n        time_passer=time_passer,\n        object_name=object_name,\n        receive_job=receive_job,\n        save=save,\n        max_live=max_live,\n        serializer=serializer,\n        args=args,\n        kwargs=kwargs\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.SocketManager","title":"<code>SocketManager</code>","text":"<p>The SocketManager Supports 2 types of connections 1. Client Server 2. Peer to Peer</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker","title":"<code>TruthSeeker</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler","title":"<code>arXivCrawler</code>","text":"<p>ArXiv Crawler for TruthSeeker. Main module for processing research queries.</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler.ArXivPDFProcessor","title":"<code>ArXivPDFProcessor</code>","text":"<p>Main processor for research queries. This is a wrapper around the new ResearchProcessor for backward compatibility.</p> Source code in <code>toolboxv2/mods/TruthSeeker/arXivCrawler.py</code> <pre><code>class ArXivPDFProcessor:\n    \"\"\"\n    Main processor for research queries.\n    This is a wrapper around the new ResearchProcessor for backward compatibility.\n    \"\"\"\n    def __init__(self,\n                 query: str,\n                 tools,\n                 chunk_size: int = 1_000_000,\n                 overlap: int = 2_000,\n                 max_workers=None,\n                 num_search_result_per_query=6,\n                 max_search=6,\n                 download_dir=\"pdfs\",\n                 callback=None,\n                 num_workers=None):\n        \"\"\"Initialize the ArXiv PDF processor.\n\n        Args:\n            query: Research query\n            tools: Tools module\n            chunk_size: Size of text chunks for processing\n            overlap: Overlap between chunks\n            max_workers: Maximum number of worker threads\n            num_search_result_per_query: Number of search results per query\n            max_search: Maximum number of search queries\n            download_dir: Directory to save downloaded files\n            callback: Callback function for status updates\n            num_workers: Number of worker threads\n        \"\"\"\n        # Create the new research processor\n        self.processor = ResearchProcessor(\n            query=query,\n            tools=tools,\n            chunk_size=chunk_size,\n            overlap=overlap,\n            max_workers=max_workers,\n            num_search_result_per_query=num_search_result_per_query,\n            max_search=max_search,\n            download_dir=download_dir,\n            callback=callback,\n            num_workers=num_workers\n        )\n\n        # Copy attributes for backward compatibility\n        self.insights_generated = False\n        self.queries_generated = False\n        self.query = query\n        self.tools = tools\n        self.mem = tools.get_memory()\n        self.chunk_size = chunk_size\n        self.overlap = overlap\n        self.max_workers = max_workers\n        self.nsrpq = num_search_result_per_query\n        self.max_search = max_search\n        self.download_dir = download_dir\n        self.parser = RobustPDFDownloader(download_dir=download_dir)\n        self.callback = callback if callback is not None else lambda status: None\n        self.mem_name = None\n        self.current_session = None\n        self.all_ref_papers = 0\n        self.last_insights_list = None\n        self.all_texts_len = 0\n        self.f_texts_len = 0\n        self.s_id = str(uuid.uuid4())\n        self.semantic_model = self.processor.semantic_model\n        self._query_progress = {}\n        self._progress_lock = threading.Lock()\n        self.num_workers = self.processor.num_workers\n\n    def _update_global_progress(self) -&gt; float:\n        \"\"\"Calculate overall progress considering all processing phases.\"\"\"\n        return self.processor._update_global_progress()\n\n    async def search_and_process_papers(self, queries: list[str]) -&gt; list[Paper]:\n        \"\"\"Search for and process papers based on queries.\n\n        Args:\n            queries: List of search queries\n\n        Returns:\n            List of processed papers\n        \"\"\"\n        # Use the new processor to search and process papers\n        unified_papers = await self.processor.search_and_process_papers(queries)\n\n        # Convert UnifiedPaper objects to Paper objects for backward compatibility\n        papers = []\n        for paper in unified_papers:\n            if paper.source == \"arxiv\":\n                # Convert to the old Paper format\n                arxiv_paper = Paper(\n                    title=paper.title,\n                    authors=paper.authors,\n                    summary=paper.summary,\n                    url=paper.url,\n                    pdf_url=paper.pdf_url,\n                    published=paper.published,\n                    updated=paper.source_specific_data.get(\"updated\", \"\"),\n                    categories=paper.source_specific_data.get(\"categories\", []),\n                    paper_id=paper.paper_id\n                )\n                papers.append(arxiv_paper)\n\n        # Update attributes for backward compatibility\n        self.all_ref_papers = self.processor.all_ref_papers\n        self.all_texts_len = self.processor.all_texts_len\n        self.f_texts_len = self.processor.f_texts_len\n\n        return papers\n\n    def send_status(self, step: str, progress: float = None, additional_info: str = \"\"):\n        \"\"\"Send status update via callback.\"\"\"\n        if progress is None:\n            progress = self._update_global_progress()\n        self.callback({\n            \"step\": step,\n            \"progress\": progress,\n            \"info\": additional_info\n        })\n\n    def generate_queries(self) -&gt; list[str]:\n        self.send_status(\"Generating search queries\")\n        self.queries_generated = False\n\n        class ArXivQueries(BaseModel):\n            queries: list[str] = Field(..., description=\"List of ArXiv search queries (en)\")\n\n        try:\n            query_generator: ArXivQueries = self.tools.format_class(\n                ArXivQueries,\n                f\"Generate a list of precise ArXiv search queries to comprehensively address: {self.query}\"\n            )\n            queries = [self.query] + query_generator[\"queries\"]\n        except Exception:\n            self.send_status(\"Error generating queries\", additional_info=\"Using default query.\")\n            queries = [self.query]\n\n        if len(queries[:self.max_search]) &gt; 0:\n            self.queries_generated = True\n        return queries[:self.max_search]\n\n    def init_process_papers(self):\n        self.mem.create_memory(self.mem_name, model_config={\"model_name\": \"anthropic/claude-3-5-haiku-20241022\"})\n        self.send_status(\"Memory initialized\")\n\n\n    async def generate_insights(self, queries) -&gt; dict:\n        self.send_status(\"Generating insights\")\n        query = self.query\n        # max_it = 0\n        results = await self.mem.query(query=query, memory_names=self.mem_name, unified_retrieve=True, query_params={\n            \"max_sentences\": 25})\n        #query = queries[min(len(queries)-1, max_it)]\n\n        self.insights_generated = True\n        self.send_status(\"Insights generated\", progress=1.0)\n        return results\n\n    async def extra_query(self, query, query_params=None, unified_retrieve=True):\n        self.send_status(\"Processing follow-up query\", progress=0.5)\n        results = await self.mem.query(query=query, memory_names=self.mem_name,\n                                                      query_params=query_params, unified_retrieve=unified_retrieve)\n        self.send_status(\"Processing follow-up query Done\", progress=1)\n        return results\n\n    def generate_mem_name(self):\n        class UniqueMemoryName(BaseModel):\n            \"\"\"unique memory name based on the user query\"\"\"\n            name: str\n        return self.tools.get_agent(\"thinkm\").format_class(UniqueMemoryName, self.query).get('name', '_'.join(self.query.split(\" \")[:3]))\n\n    def initialize(self, session_id, second=False):\n        self.current_session = session_id\n        self.insights_generated = False\n        self.queries_generated = False\n        if second:\n            return\n        self.mem_name = self.generate_mem_name().strip().replace(\"\\n\", '') + '_' + session_id\n        self.init_process_papers()\n\n    async def process(self, query=None) -&gt; tuple[list[Paper], dict]:\n        if query is not None:\n            self.query = query\n        self.send_status(\"Starting research process\")\n        t0 = time.perf_counter()\n        self.initialize(self.s_id, query is not None)\n\n        queries = self.generate_queries()\n\n        papers = await self.search_and_process_papers(queries)\n\n        if len(papers) == 0:\n            class UserQuery(BaseModel):\n                \"\"\"Fix all typos and clear the original user query\"\"\"\n                new_query: str\n            self.query= self.tools.format_class(\n                UserQuery,\n                self.query\n            )[\"new_query\"]\n            queries = self.generate_queries()\n            papers = await self.search_and_process_papers(queries)\n\n        insights = await self.generate_insights(queries)\n\n        elapsed_time = time.perf_counter() - t0\n        self.send_status(\"Process complete\", progress=1.0,\n                         additional_info=f\"Total time: {elapsed_time:.2f}s, Papers analyzed: {len(papers)}/{self.all_ref_papers}\")\n\n        return papers, insights\n\n    @staticmethod\n    def estimate_processing_metrics(query_length: int, **config) -&gt; (float, float):\n        \"\"\"Return estimated time (seconds) and price for processing.\"\"\"\n        total_papers = config['max_search'] * config['num_search_result_per_query']\n        median_text_length = 100000  # 10 pages * 10000 characters\n\n        # Estimated chunks to process\n        total_chunks = total_papers * (median_text_length / config['chunk_size']) + 1 / config['overlap']\n        processed_chunks = total_chunks * 0.45\n        total_chars = TextSplitter(config['chunk_size'],\n                     config['overlap']\n                     ).approximate(config['chunk_size'] * processed_chunks)\n        # Time estimation (seconds)\n        .75 / config['chunk_size']  # Hypothetical time per chunk in seconds\n        w = (config.get('num_workers', 16) if config.get('num_workers', 16) is not None else 16 / 10)\n        # Processing_ time - Insights Genration - Insights Query   -   Indexing Time     -    Download Time     -       workers   -   Query Genration time - Ui - Init Db\n        estimated_time = ((8+total_papers*0.012)+(total_chunks/20000) * .005 + (total_chunks/2) * .0003 + total_papers * 2.8 ) / w + (0.25 * config['max_search']) + 6 + 4\n\n        price_per_char = 0.0000012525\n        price_per_t_chunk =  total_chars * price_per_char\n        estimated_price = price_per_t_chunk ** 1.7\n\n        # estimated_price = 0 if query_length &lt; 420 and estimated_price &lt; 5 else estimated_price\n        if estimated_time &lt; 10:\n            estimated_time = 10\n        if estimated_price &lt; .04:\n            estimated_price = .04\n        return round(estimated_time, 2), round(estimated_price, 4)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler.ArXivPDFProcessor.__init__","title":"<code>__init__(query, tools, chunk_size=1000000, overlap=2000, max_workers=None, num_search_result_per_query=6, max_search=6, download_dir='pdfs', callback=None, num_workers=None)</code>","text":"<p>Initialize the ArXiv PDF processor.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Research query</p> required <code>tools</code> <p>Tools module</p> required <code>chunk_size</code> <code>int</code> <p>Size of text chunks for processing</p> <code>1000000</code> <code>overlap</code> <code>int</code> <p>Overlap between chunks</p> <code>2000</code> <code>max_workers</code> <p>Maximum number of worker threads</p> <code>None</code> <code>num_search_result_per_query</code> <p>Number of search results per query</p> <code>6</code> <code>max_search</code> <p>Maximum number of search queries</p> <code>6</code> <code>download_dir</code> <p>Directory to save downloaded files</p> <code>'pdfs'</code> <code>callback</code> <p>Callback function for status updates</p> <code>None</code> <code>num_workers</code> <p>Number of worker threads</p> <code>None</code> Source code in <code>toolboxv2/mods/TruthSeeker/arXivCrawler.py</code> <pre><code>def __init__(self,\n             query: str,\n             tools,\n             chunk_size: int = 1_000_000,\n             overlap: int = 2_000,\n             max_workers=None,\n             num_search_result_per_query=6,\n             max_search=6,\n             download_dir=\"pdfs\",\n             callback=None,\n             num_workers=None):\n    \"\"\"Initialize the ArXiv PDF processor.\n\n    Args:\n        query: Research query\n        tools: Tools module\n        chunk_size: Size of text chunks for processing\n        overlap: Overlap between chunks\n        max_workers: Maximum number of worker threads\n        num_search_result_per_query: Number of search results per query\n        max_search: Maximum number of search queries\n        download_dir: Directory to save downloaded files\n        callback: Callback function for status updates\n        num_workers: Number of worker threads\n    \"\"\"\n    # Create the new research processor\n    self.processor = ResearchProcessor(\n        query=query,\n        tools=tools,\n        chunk_size=chunk_size,\n        overlap=overlap,\n        max_workers=max_workers,\n        num_search_result_per_query=num_search_result_per_query,\n        max_search=max_search,\n        download_dir=download_dir,\n        callback=callback,\n        num_workers=num_workers\n    )\n\n    # Copy attributes for backward compatibility\n    self.insights_generated = False\n    self.queries_generated = False\n    self.query = query\n    self.tools = tools\n    self.mem = tools.get_memory()\n    self.chunk_size = chunk_size\n    self.overlap = overlap\n    self.max_workers = max_workers\n    self.nsrpq = num_search_result_per_query\n    self.max_search = max_search\n    self.download_dir = download_dir\n    self.parser = RobustPDFDownloader(download_dir=download_dir)\n    self.callback = callback if callback is not None else lambda status: None\n    self.mem_name = None\n    self.current_session = None\n    self.all_ref_papers = 0\n    self.last_insights_list = None\n    self.all_texts_len = 0\n    self.f_texts_len = 0\n    self.s_id = str(uuid.uuid4())\n    self.semantic_model = self.processor.semantic_model\n    self._query_progress = {}\n    self._progress_lock = threading.Lock()\n    self.num_workers = self.processor.num_workers\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler.ArXivPDFProcessor.estimate_processing_metrics","title":"<code>estimate_processing_metrics(query_length, **config)</code>  <code>staticmethod</code>","text":"<p>Return estimated time (seconds) and price for processing.</p> Source code in <code>toolboxv2/mods/TruthSeeker/arXivCrawler.py</code> <pre><code>@staticmethod\ndef estimate_processing_metrics(query_length: int, **config) -&gt; (float, float):\n    \"\"\"Return estimated time (seconds) and price for processing.\"\"\"\n    total_papers = config['max_search'] * config['num_search_result_per_query']\n    median_text_length = 100000  # 10 pages * 10000 characters\n\n    # Estimated chunks to process\n    total_chunks = total_papers * (median_text_length / config['chunk_size']) + 1 / config['overlap']\n    processed_chunks = total_chunks * 0.45\n    total_chars = TextSplitter(config['chunk_size'],\n                 config['overlap']\n                 ).approximate(config['chunk_size'] * processed_chunks)\n    # Time estimation (seconds)\n    .75 / config['chunk_size']  # Hypothetical time per chunk in seconds\n    w = (config.get('num_workers', 16) if config.get('num_workers', 16) is not None else 16 / 10)\n    # Processing_ time - Insights Genration - Insights Query   -   Indexing Time     -    Download Time     -       workers   -   Query Genration time - Ui - Init Db\n    estimated_time = ((8+total_papers*0.012)+(total_chunks/20000) * .005 + (total_chunks/2) * .0003 + total_papers * 2.8 ) / w + (0.25 * config['max_search']) + 6 + 4\n\n    price_per_char = 0.0000012525\n    price_per_t_chunk =  total_chars * price_per_char\n    estimated_price = price_per_t_chunk ** 1.7\n\n    # estimated_price = 0 if query_length &lt; 420 and estimated_price &lt; 5 else estimated_price\n    if estimated_time &lt; 10:\n        estimated_time = 10\n    if estimated_price &lt; .04:\n        estimated_price = .04\n    return round(estimated_time, 2), round(estimated_price, 4)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler.ArXivPDFProcessor.search_and_process_papers","title":"<code>search_and_process_papers(queries)</code>  <code>async</code>","text":"<p>Search for and process papers based on queries.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>list[str]</code> <p>List of search queries</p> required <p>Returns:</p> Type Description <code>list[Paper]</code> <p>List of processed papers</p> Source code in <code>toolboxv2/mods/TruthSeeker/arXivCrawler.py</code> <pre><code>async def search_and_process_papers(self, queries: list[str]) -&gt; list[Paper]:\n    \"\"\"Search for and process papers based on queries.\n\n    Args:\n        queries: List of search queries\n\n    Returns:\n        List of processed papers\n    \"\"\"\n    # Use the new processor to search and process papers\n    unified_papers = await self.processor.search_and_process_papers(queries)\n\n    # Convert UnifiedPaper objects to Paper objects for backward compatibility\n    papers = []\n    for paper in unified_papers:\n        if paper.source == \"arxiv\":\n            # Convert to the old Paper format\n            arxiv_paper = Paper(\n                title=paper.title,\n                authors=paper.authors,\n                summary=paper.summary,\n                url=paper.url,\n                pdf_url=paper.pdf_url,\n                published=paper.published,\n                updated=paper.source_specific_data.get(\"updated\", \"\"),\n                categories=paper.source_specific_data.get(\"categories\", []),\n                paper_id=paper.paper_id\n            )\n            papers.append(arxiv_paper)\n\n    # Update attributes for backward compatibility\n    self.all_ref_papers = self.processor.all_ref_papers\n    self.all_texts_len = self.processor.all_texts_len\n    self.f_texts_len = self.processor.f_texts_len\n\n    return papers\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler.ArXivPDFProcessor.send_status","title":"<code>send_status(step, progress=None, additional_info='')</code>","text":"<p>Send status update via callback.</p> Source code in <code>toolboxv2/mods/TruthSeeker/arXivCrawler.py</code> <pre><code>def send_status(self, step: str, progress: float = None, additional_info: str = \"\"):\n    \"\"\"Send status update via callback.\"\"\"\n    if progress is None:\n        progress = self._update_global_progress()\n    self.callback({\n        \"step\": step,\n        \"progress\": progress,\n        \"info\": additional_info\n    })\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.arXivCrawler.main","title":"<code>main(query='Beste strategien in bretspielen sitler von katar')</code>  <code>async</code>","text":"<p>Main execution function</p> Source code in <code>toolboxv2/mods/TruthSeeker/arXivCrawler.py</code> <pre><code>async def main(query: str = \"Beste strategien in bretspielen sitler von katar\"):\n    \"\"\"Main execution function\"\"\"\n    with Spinner(\"Init Isaa\"):\n        tools = get_app(\"ArXivPDFProcessor\", name=None).get_mod(\"isaa\")\n        tools.init_isaa(build=True)\n    processor = ArXivPDFProcessor(query, tools=tools)\n    papers, insights = await processor.process()\n\n    print(\"Generated Insights:\", insights)\n    print(\"Generated Insights_list:\", processor.last_insights_list)\n    kb = tools.get_memory(processor.mem_name)\n    print(await kb.query_concepts(\"AI\"))\n    print(await kb.retrieve(\"Evaluation metrics for assessing AI Agent performance\"))\n    print(kb.concept_extractor.concept_graph.concepts.keys())\n    kb.vis(output_file=\"insights_graph.html\")\n    kb.save(\"mem.plk\")\n    # await get_app(\"ArXivPDFProcessor\", name=None).a_idle()\n    return insights\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui","title":"<code>nGui</code>","text":"<p>import colorsys import json import time from datetime import datetime, timedelta from queue import Queue from typing import Dict, Union, List, Any</p> <p>from fastapi import Request import os import random from threading import Thread, Event</p> <p>import networkx as nx from dataclasses import asdict</p> <p>from toolboxv2 import get_app from toolboxv2.mods.FastApi.fast_nice import register_nicegui</p> <p>import asyncio</p> <p>from nicegui import ui</p> <p>from pathlib import Path import stripe</p> <p>from toolboxv2.mods.TruthSeeker.arXivCrawler import Paper from toolboxv2.mods.isaa.base.AgentUtils import anything_from_str_to_dict</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--set-your-secret-key-use-environment-variables-in-production","title":"Set your secret key (use environment variables in production!)","text":"<p>stripe.api_key = os.getenv('STRIPE_SECRET_KEY', 'sk_test_YourSecretKey')</p> <p>def create_landing_page():     # Set up dynamic background     ui.query(\"body\").style(\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%)\")</p> <pre><code># Main container with enhanced responsive design\nwith ui.column().classes(\n\"w-full max-w-md p-8 rounded-3xl shadow-2xl \"\n\"items-center self-center mx-auto my-8\"\n):\n    # Advanced styling for glass-morphism effect\n    ui.query(\".nicegui-column\").style(\"\"\"\n    background: rgba(255, 255, 255, 0.05);\n    backdrop-filter: blur(12px);\n    border: 1px solid rgba(255, 255, 255, 0.1);\n    transition: all 0.3s ease-in-out;\n    \"\"\")\n\n    # Animated logo/brand icon\n    with ui.element(\"div\").classes(\"animate-fadeIn\"):\n        ui.icon(\"science\").classes(\n        \"text-7xl mb-6 text-primary \"\n        \"transform hover:scale-110 transition-transform\"\n        )\n\n    # Enhanced typography for title\n    ui.label(\"TruthSeeker\").classes(\n    \"text-5xl font-black text-center \"\n    \"text-primary mb-2 animate-slideDown\"\n    )\n\n    # Stylized subtitle with brand message\n    ui.label(\"Precision. Discovery. Insights.\").classes(\n    \"text-xl font-medium text-center \"\n    \"mb-10 animate-fadeIn\"\n    )\n\n    # Button container for consistent spacing\n    ui.button(\n    \"Start Research\",\n    on_click=lambda: ui.navigate.to(\"/open-Seeker.seek\")\n    ).classes(\n    \"w-full px-6 py-4 text-lg font-bold \"\n    \"bg-primary hover:bg-primary-dark \"\n    \"transform hover:-translate-y-0.5 \"\n    \"transition-all duration-300 ease-in-out \"\n    \"rounded-xl shadow-lg animate-slideUp\"\n    )\n\n    # Navigation links container\n    with ui.element(\"div\").classes(\"mt-8 space-y-3 text-center\"):\n        ui.link(\n        \"Demo video\",\n        ).classes(\n        \"block text-lg text-gray-200 hover:text-primary \"\n        \"transition-colors duration-300 animate-fadeIn\"\n        ).on(\"click\", lambda: ui.navigate.to(\"/open-Seeker.demo\"))\n\n        ui.link(\n        \"About Us\",\n        ).classes(\n        \"block text-lg text-gray-400 hover:text-primary \"\n        \"transition-colors duration-300 animate-fadeIn\"\n        ).on(\"click\", lambda: ui.navigate.to(\"/open-Seeker.about\"))\n</code></pre> <p>def create_video_demo():     with ui.card().classes('w-full max-w-3xl mx-auto').style(         'background: var(--background-color); color: var(--text-color)'):         # Video container with responsive aspect ratio         with ui.element('div').classes('relative w-full aspect-video'):             video = ui.video('../api/TruthSeeker/video').classes('w-full h-full object-cover')</p> <pre><code>        # Custom controls overlay\n        with ui.element('div').classes('absolute bottom-0 left-0 right-0 bg-black/50 p-2'):\n            with ui.row().classes('items-center gap-2'):\n                #play_btn = ui.button(icon='play_arrow', on_click=lambda: video.props('playing=true'))\n                #pause_btn = ui.button(icon='pause', on_click=lambda: video.props('playing=false'))\n                ui.slider(min=0, max=100, value=0).classes('w-full').bind_value(video, 'time')\n                #mute_btn = ui.button(icon='volume_up', on_click=lambda: video.props('muted=!muted'))\n                #fullscreen_btn = ui.button(icon='fullscreen', on_click=lambda: video.props('fullscreen=true'))\n\n\n    # Video description\n    ui.markdown('Walkthrough of TruthSeeker features and capabilities.')\n    # Back to Home Button\n    ui.button('Back to Home', on_click=lambda: ui.navigate.to('/open-Seeker')).classes(\n        'mt-6 w-full bg-primary text-white hover:opacity-90'\n    )\n\nreturn video\n</code></pre> <p>def create_about_page():     \"\"\"Create a comprehensive About page for TruthSeeker\"\"\"     with ui.column().classes('w-full max-w-4xl mx-auto p-6'):         # Page Header         ui.label('About TruthSeeker').classes('text-4xl font-bold text-primary mb-6')</p> <pre><code>    # Mission Statement\n    with ui.card().classes('w-full mb-6').style(\n        'background: var(--background-color); color: var(--text-color); padding: 20px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);'\n    ):\n        ui.label('Our Mission').classes('text-2xl font-semibold text-primary mb-4')\n        ui.markdown(\"\"\"\n            TruthSeeker aims to democratize access to scientific knowledge,\n            transforming complex academic research into comprehensible insights.\n            We bridge the gap between raw data and meaningful understanding.\n        \"\"\").classes('text-lg').style('color: var(--text-color);')\n\n    # Core Technologies\n    with ui.card().classes('w-full mb-6').style(\n        'background: var(--background-color); color: var(--text-color); padding: 20px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);'\n    ):\n        ui.label('Core Technologies').classes('text-2xl font-semibold text-primary mb-4')\n        with ui.row().classes('gap-4 w-full'):\n            with ui.column().classes('flex-1 text-center'):\n                ui.icon('search').classes('text-4xl text-primary mb-2')\n                ui.label('Advanced Query Processing').classes('font-bold')\n                ui.markdown('Intelligent algorithms that extract nuanced research insights.').style(\n                    'color: var(--text-color);')\n            with ui.column().classes('flex-1 text-center'):\n                ui.icon('analytics').classes('text-4xl text-primary mb-2')\n                ui.label('Semantic Analysis').classes('font-bold')\n                ui.markdown('Deep learning models for comprehensive research verification.').style(\n                    'color: var(--text-color);')\n            with ui.column().classes('flex-1 text-center'):\n                ui.icon('verified').classes('text-4xl text-primary mb-2')\n                ui.label('Research Validation').classes('font-bold')\n                ui.markdown('Multi-layered verification of academic sources.').style('color: var(--text-color);')\n    # Research Process\n    with ui.card().classes('w-full').style('background: var(--background-color);color: var(--text-color);'):\n        ui.label('Research Discovery Process').classes('text-2xl font-semibold text-primary mb-4')\n        with ui.card().classes('q-pa-md q-mx-auto').style(\n            'max-width: 800px; background: var(--background-color); border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);'\n        ) as card:\n            ui.markdown(\"# Research Workflow\").style(\n                \"color: var(--primary-color); text-align: center; margin-bottom: 20px;\")\n            ui.markdown(\n                \"\"\"\n                Welcome to TruthSeeker\u2019s interactive research assistant. Follow the steps below to transform your initial inquiry into a refined, actionable insight.\n                \"\"\"\n            ).style(\"color: var(--text-color); text-align: center; margin-bottom: 30px;\")\n\n            # The stepper component\n            with ui.stepper().style('background: var(--background-color); color: var(--text-color);') as stepper:\n                # Step 1: Query Initialization\n                with ui.step('Query Initialization'):\n                    ui.markdown(\"### Step 1: Query Initialization\").style(\"color: var(--primary-color);\")\n                    ui.markdown(\n                        \"\"\"\n                        Begin by entering your research question or selecting from popular academic domains.\n                        This sets the direction for our semantic analysis engine.\n                        \"\"\"\n                    ).style(\"color: var(--text-color); margin-bottom: 20px;\")\n                    with ui.stepper_navigation():\n                        ui.button('Next', on_click=stepper.next).props('rounded color=primary')\n\n                # Step 2: Semantic Search\n                with ui.step('Semantic Search'):\n                    ui.markdown(\"### Step 2: Semantic Search\").style(\"color: var(--primary-color);\")\n                    ui.markdown(\n                        \"\"\"\n                        Our advanced algorithms now process your input to generate context-rich queries.\n                        This stage refines the search context by understanding the deeper intent behind your question.\n                        \"\"\"\n                    ).style(\"color: var(--text-color); margin-bottom: 20px;\")\n                    with ui.stepper_navigation():\n                        ui.button('Back', on_click=stepper.previous).props('flat')\n                        ui.button('Next', on_click=stepper.next).props('rounded color=primary')\n\n                # Step 3: Document Analysis\n                with ui.step('Document Analysis'):\n                    ui.markdown(\"### Step 3: Document Analysis\").style(\"color: var(--primary-color);\")\n                    ui.markdown(\n                        \"\"\"\n                        The system then dives into a detailed analysis of academic papers, parsing content to extract key insights and connections.\n                        This ensures that even subtle but crucial information is captured.\n                        \"\"\"\n                    ).style(\"color: var(--text-color); margin-bottom: 20px;\")\n                    with ui.stepper_navigation():\n                        ui.button('Back', on_click=stepper.previous).props('flat')\n                        ui.button('Next', on_click=stepper.next).props('rounded color=primary')\n\n                # Step 4: Insight Generation\n                with ui.step('Insight Generation'):\n                    ui.markdown(\"### Step 4: Insight Generation\").style(\"color: var(--primary-color);\")\n                    ui.markdown(\n                        \"\"\"\n                        Finally, we synthesize the analyzed data into clear, actionable research summaries.\n                        These insights empower you with concise guidance to drive further inquiry or practical application.\n                        \"\"\"\n                    ).style(\"color: var(--text-color); margin-bottom: 20px;\")\n                    with ui.stepper_navigation():\n                        ui.button('Back', on_click=stepper.previous).props('flat')\n\n    # Back to Home Button\n    ui.button('Back to Home', on_click=lambda: ui.navigate.to('/open-Seeker')).classes(\n        'mt-6 w-full bg-primary text-white hover:opacity-90'\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--dummy-implementierung-fur-get_tools","title":"Dummy-Implementierung f\u00fcr get_tools()","text":"<p>def get_tools():     \"\"\"     Hier solltest du dein richtiges Werkzeug-Objekt zur\u00fcckliefern.     In diesem Beispiel gehen wir davon aus, dass du \u00fcber eine Funktion wie get_app verf\u00fcgst.     \"\"\"     return get_app(\"ArXivPDFProcessor\", name=None).get_mod(\"isaa\")</p> <p>def create_graph_tab(processor_instance: Dict, graph_ui: ui.element, main_ui: ui.element):     \"\"\"Create and update the graph visualization\"\"\"</p> <pre><code># Get HTML graph from processor\n_html_content = processor_instance[\"instance\"].tools.get_memory(processor_instance[\"instance\"].mem_name)\nhtml_content = \"\" if isinstance(_html_content, list) else _html_content.vis(get_output_html=True)\n\n# Ensure static directory exists\nstatic_dir = Path('dist/static')\nstatic_dir.mkdir(exist_ok=True)\n\n# Save HTML to static file\ngraph_file = static_dir / f'graph{processor_instance[\"instance\"].mem_name}.html'\n# Save HTML to static file with added fullscreen functionality\n\n# Add fullscreen JavaScript\ngraph_file.write_text(html_content, encoding='utf-8')\n\nwith main_ui:\n    # Clear existing content except fullscreen button\n    graph_ui.clear()\n\n    with graph_ui:\n        ui.html(f\"\"\"\n\n            &lt;iframe\n                 src=\"/static/graph{processor_instance[\"instance\"].mem_name}.html\"\n                style=\"width: 100%; height: 800px; border: none; background: #1a1a1a;\"\n                &gt;\n            &lt;/iframe&gt;\n        \"\"\").classes('w-full h-full')\n</code></pre> <p>is_init = [False]</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui---database-setup-","title":"--- Database Setup ---","text":"<p>def get_db():     db = get_app().get_mod(\"DB\")     if not is_init[0]:         is_init[0] = True         db.edit_cli(\"LD\")         db.initialize_database()     return db</p> <p>import pickle</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui---session-state-management-","title":"--- Session State Management ---","text":"<p>def get_user_state(session_id: str, is_new=False) -&gt; dict:     db = get_db()     state_ = {         'balance': .5,         'last_reset': datetime.utcnow().isoformat(),         'research_history': [],         'payment_id': '',     }     if session_id is None:         state_['balance'] *= -1         if is_new:             return state_, True         return state_     state = db.get(f\"TruthSeeker::session:{session_id}\")     if state.get() is None:         state = state_         if is_new:             return state_, True     else:         try:             state = pickle.loads(state.get())         except Exception as e:             print(e)             state = {         'balance': 0.04,         'last_reset': datetime.utcnow().isoformat(),         'research_history': [\"Sorry we had an error recreating your state\"],         'payment_id': '',             }             if is_new:                 return state, True     if is_new:         return state, False     return state</p> <p>def save_user_state(session_id: str, state: dict):     db = get_db()     print(\"Saving state\")     db.set(f\"TruthSeeker::session:{session_id}\", pickle.dumps(state)).print()</p> <p>def delete_user_state(session_id: str):     db = get_db()     print(\"Saving state\")     db.delete(f\"TruthSeeker::session:{session_id}\").print()</p> <p>def reset_daily_balance(state: dict, valid=False) -&gt; dict:     now = datetime.utcnow()     last_reset = datetime.fromisoformat(state.get('last_reset', now.isoformat()))     if now - last_reset &gt; timedelta(hours=24):         state['balance'] = max(state.get('balance', 1.6 if valid else 0.5), 1.6 if valid else 0.5)         state['last_reset'] = now.isoformat()     return state</p> class MemoryResultsDisplay <p>def init(self, results: List[Dict[str, Any]], main_ui: ui.element):     self.results = results     self.main_ui = main_ui     self.setup_ui()</p> <p>def setup_ui(self):     \"\"\"Set up the main UI for displaying memory results\"\"\"     with self.main_ui:         self.main_ui.clear()         with ui.column().classes('w-full'):             for mem_result in self.results:                 self.create_memory_card(mem_result)</p> <p>def create_memory_card(self, mem_result: Dict[str, Any]):     \"\"\"Create a card for each memory result\"\"\"     result = mem_result.get(\"result\", {})     with self.main_ui:         if isinstance(result, dict):             self.display_dict_result(result)         elif hasattr(result, 'overview'):  # Assuming RetrievalResult type             self.display_retrieval_result(result)         else:             ui.label(\"Unsupported result type\").classes('--text-color:error')</p> <p>def display_dict_result(self, result: Dict[str, Any]):     \"\"\"Display dictionary-based result with collapsible sections\"\"\"     # Summary Section     summary = result.get(\"summary\", {})     if isinstance(summary, str):         try:             summary = json.loads(summary[:-1])         except json.JSONDecodeError:             summary = {\"error\": \"Could not parse summary\"}</p> <pre><code># Raw Results Section\nraw_results = result.get(\"raw_results\", {})\nif isinstance(raw_results, str):\n    try:\n        raw_results = json.loads(raw_results[:-1])\n    except json.JSONDecodeError:\n        raw_results = {\"error\": \"Could not parse raw results\"}\n\n# Metadata Section\nmetadata = result.get(\"metadata\", {})\nwith self.main_ui:\n    # Collapsible Sections\n    with ui.column().classes('w-full space-y-2').style(\"max-width: 100%;\"):\n        # Summary Section\n        with ui.expansion('Summary', icon='description').classes('w-full') as se:\n            self.display_nested_data(summary, main_ui=se)\n\n        # Raw Results Section\n        with ui.expansion('Raw Results', icon='work').classes('w-full') as re:\n            self.display_nested_data(raw_results, main_ui=re)\n\n        # Metadata Section\n        if metadata:\n            with ui.expansion('Metadata', icon='info').classes('w-full'):\n                ui.markdown(f\"```json\n</code></pre> <p>{json.dumps(metadata, indent=2)} ```\").style(\"max-width: 100%;\")</p> <pre><code>def display_retrieval_result(self, result):\n    \"\"\"Display retrieval result with detailed sections\"\"\"\n    with self.main_ui:\n        with ui.column().classes('w-full space-y-4').style(\"max-width: 100%;\"):\n            # Overview Section\n            with ui.expansion('Overview', icon='visibility').classes('w-full') as ov:\n                for overview_item in result.overview:\n                    if isinstance(overview_item, str):\n                        overview_item = json.loads(overview_item)\n                    self.display_nested_data(overview_item, main_ui=ov)\n\n            # Details Section\n            with ui.expansion('Details', icon='article').classes('w-full'):\n                for chunk in result.details:\n                    with ui.card().classes('w-full p-3 mb-2').style(\"background: var(--background-color)\"):\n                        ui.label(chunk.text).classes('font-medium mb-2 --text-color:secondary')\n\n                        with ui.row().classes('w-full justify-between').style(\"background: var(--background-color)\"):\n                            ui.label(f\"Embedding Shape: {chunk.embedding.shape}\").classes('text-sm')\n                            ui.label(f\"Content Hash: {chunk.content_hash}\").classes('text-sm')\n\n                        if chunk.cluster_id is not None:\n                            ui.label(f\"Cluster ID: {chunk.cluster_id}\").classes('text-sm')\n\n            # Cross References Section\n            with ui.expansion('Cross References', icon='link').classes('w-full'):\n                for topic, chunks in result.cross_references.items():\n                    with ui.card().classes('w-full p-3 mb-2').style(\"background: var(--background-color)\"):\n                        ui.label(topic).classes('font-semibold mb-2 --text-color:secondary')\n                        for chunk in chunks:\n                            ui.label(chunk.text).classes('text-sm mb-1')\n\ndef display_nested_data(self, data: Union[Dict, List], indent: int = 0, main_ui=None):\n    \"\"\"Recursively display nested dictionary or list data\"\"\"\n    with (self.main_ui if main_ui is None else main_ui):\n        if isinstance(data, dict):\n            with ui.column().classes(f'ml-{indent * 2}').style(\"max-width: 100%;\"):\n                for key, value in data.items():\n                    with ui.row().classes('items-center'):\n                        ui.label(f\"{key}:\").classes('font-bold mr-2 --text-color:primary')\n                        if isinstance(value, list):\n                            if key == \"main_chunks\":\n                                continue\n                            self.display_nested_data(value, indent + 1, main_ui=main_ui)\n                        if isinstance(value, dict):\n                            ui.markdown(f\"```json\n</code></pre> <p>{json.dumps(value, indent=2)} <code>\").classes(\"break-words w-full\").style(\"max-width: 100%;\")                             else:                                 ui.label(str(value)).classes('--text-color:secondary')             elif isinstance(data, list):                 with ui.column().classes(f'ml-{indent * 2}').style(\"max-width: 100%;\"):                     for item in data:                         if isinstance(item, str):                             item = json.loads(item)                         if isinstance(item, list):                             self.display_nested_data(item, indent + 1, main_ui=main_ui)                         if isinstance(item, dict):                             ui.markdown(f\"</code>json {json.dumps(item, indent=2)} ```\").classes(\"break-words w-full\").style(\"max-width: 100%;\")                         else:                             ui.label(str(item)).classes('--text-color:secondary')</p> <p>def create_followup_section(processor_instance: Dict, main_ui: ui.element, session_id, balance):     main_ui.clear()     with main_ui:         ui.label(\"Query Interface  (1ct)\").classes(\"text-xl font-semibold mb-4\")</p> <pre><code>    # Container for query inputs\n    query_container = ui.column().classes(\"w-full gap-4\")\n    query = \"\"  # Store references to query inputs\n    # Query parameters section\n    with ui.expansion(\"Query Parameters\", icon=\"settings\").classes(\"w-full\") as query_e:\n        with ui.grid(columns=2).classes(\"w-full gap-4\"):\n            k_input = ui.number(\"Results Count (k)\", value=2, min=1, max=20)\n            min_sim = ui.number(\"Min Similarity\", value=.3, min=0, max=1, step=0.1)\n            cross_depth = ui.number(\"Cross Reference Depth\", value=2, min=1, max=5)\n            max_cross = ui.number(\"Max Cross References\", value=10, min=1, max=20)\n            max_sent = ui.number(\"Max Sentences\", value=10, min=1, max=50)\n            unified = ui.switch(\"Unified Retrieve (+3ct)\", value=True)\n\n    # Results display\n    with ui.element(\"div\").classes(\"w-full mt-4\") as results_display:\n        pass\n    results_display = results_display\n    with query_container:\n        query_input = ui.input(\"Query\", placeholder=\"Enter your query...\")                 .classes(\"w-full\")\n    # Control buttons\n    with ui.row().classes(\"w-full gap-4 mt-4\"):\n        ui.button(\"Execute Query\", on_click=lambda: asyncio.create_task(execute_query()))                 .classes(\"bg-green-600 hover:bg-green-700\")\n        ui.button(\"Clear Results\", on_click=lambda: results_display.clear())                 .classes(\"bg-red-600 hover:bg-red-700\")\nquery_input = query_input\n\nasync def execute_query():\n    \"\"\"Execute a single query with parameters\"\"\"\n    nonlocal query_input, results_display, main_ui\n    try:\n        query_text = query_input.value\n        if not query_text.strip():\n            with main_ui:\n                ui.notify(\"No Input\", type=\"warning\")\n            return \"\"\n\n        if not processor_instance.get(\"instance\"):\n            with main_ui:\n                ui.notify(\"No active processor instance\", type=\"warning\")\n            return\n        # Collect parameters\n        params = {\n            \"k\": int(k_input.value),\n            \"min_similarity\": min_sim.value,\n            \"cross_ref_depth\": int(cross_depth.value),\n            \"max_cross_refs\": int(max_cross.value),\n            \"max_sentences\": int(max_sent.value),\n            \"unified\": unified.value\n        }\n        # Construct query parameters\n        query_params = {\n            \"k\": params[\"k\"],\n            \"min_similarity\": params[\"min_similarity\"],\n            \"cross_ref_depth\": params[\"cross_ref_depth\"],\n            \"max_cross_refs\": params[\"max_cross_refs\"],\n            \"max_sentences\": params[\"max_sentences\"]\n        }\n\n        # Execute query\n        results = await processor_instance[\"instance\"].extra_query(\n            query=query_text,\n            query_params=query_params,\n            unified_retrieve=params[\"unified\"]\n        )\n        print(\"results\",results)\n        s = get_user_state(session_id)\n        s['balance'] -= .04 if unified.value else .01\n        save_user_state(session_id, s)\n        with main_ui:\n            balance.set_text(f\"Balance: {s['balance']:.2f}\u20ac\")\n        # Format results\n        with main_ui:\n            with results_display:\n                MemoryResultsDisplay(results, results_display)\n\n    except Exception as e:\n        return f\"Error executing query: {str(e)}\n</code></pre> <p>\"</p> <pre><code># Add initial query input\n</code></pre> <p>online_states = [0] def create_research_interface(Processor):</p> <pre><code>def helpr(request, session: dict):\n\n    state = {'balance':0, 'research_history': []}\n    main_ui = None\n    with ui.column().classes(\"w-full max-w-6xl mx-auto p-6 space-y-6\") as loading:\n        ui.spinner(size='lg')\n        ui.label('Initializing...').classes('ml-2')\n\n    # Container for main content (initially hidden)\n    content = ui.column().classes('hidden')\n\n    # Extract session data before spawning thread\n    session_id = session.get('ID')\n    session_id_h = session.get('IDh')\n    session_rid = request.row.query_params.get('session_id') if hasattr(request, 'row') else request.query_params.get('session_id')\n    session_valid = session.get('valid')\n\n    # Thread communication\n    result_queue = Queue()\n    ready_event = Event()\n\n    def init_background():\n        nonlocal session_id, session_id_h, session_rid, session_valid\n        try:\n            # Original initialization logic\n            _state, is_new = get_user_state(session_id, is_new=True)\n\n            if is_new and session_id_h != \"#0\":\n                _state = get_user_state(session_id_h)\n                save_user_state(session_id, _state)\n                delete_user_state(session_id_h)\n            if session_rid:\n                state_: dict\n                state_, is_new_ = get_user_state(session_rid, is_new=True)\n                if not is_new_:\n                    _state = state_.copy()\n                    state_['payment_id'] = ''\n                    state_['last_reset'] = datetime.utcnow().isoformat()\n                    state_['research_history'] = state_['research_history'][:3]\n                    state_['balance'] = 0\n                    save_user_state(session_id, _state)\n            _state = reset_daily_balance(_state, session_valid)\n            save_user_state(session_id, _state)\n\n            # Send result back to main thread\n            result_queue.put(_state)\n            ready_event.set()\n        except Exception as e:\n            result_queue.put(e)\n            ready_event.set()\n\n        # Start background initialization\n\n    Thread(target=init_background).start()\n\n    def check_ready():\n        nonlocal state\n        if ready_event.is_set():\n            result = result_queue.get()\n\n            # Check if initialization failed\n            if isinstance(result, Exception):\n                loading.clear()\n                with loading:\n                    ui.label(f\"Error during initialization: {str(result)}\").classes('text-red-500')\n                return\n\n            # Get state and build main UI\n            state = result\n            loading.classes('hidden')\n            content.classes(remove='hidden')\n            main_ui.visible = True\n            with main_ui:\n                balance.set_text(f\"Balance: {state['balance']:.2f}\u20ac\")\n                show_history()\n            return  # Stop the timer\n\n        # Check again in 100ms\n        ui.timer(0.1, check_ready, once=True)\n\n    # Start checking for completion\n    check_ready()\n\n    # Wir speichern die aktive Instanz, damit Follow-Up Fragen gestellt werden k\u00f6nnen\n    processor_instance = {\"instance\": None}\n\n    # UI-Elemente als Platzhalter; wir definieren sie sp\u00e4ter in der UI und machen sie so\n    # in den Callback-Funktionen \u00fcber \"nonlocal\" verf\u00fcgbar.\n    overall_progress = None\n    status_label = None\n    results_card = None\n    summary_content = None\n    analysis_content = None\n    references_content = None\n    followup_card = None\n    research_card = None\n    config_cart = None\n    progress_card = None\n    balance = None\n    graph_ui = None\n\n    sr_button = None\n    r_button = None\n    r_text = None\n\n\n    # Global config storage with default values\n    config = {\n        'chunk_size': 21000,\n        'overlap': 600,\n        'num_search_result_per_query': 3,\n        'max_search': 3,\n        'num_workers': None\n    }\n\n    def update_estimates():\n        \"\"\"\n        Dummy estimation based on query length and configuration.\n        (Replace with your own non-linear formula if needed.)\n        \"\"\"\n        query_text = query.value or \"\"\n        query_length = len(query_text)\n        # For example: estimated time scales with chunk size and query length.\n        estimated_time ,estimated_price = Processor.estimate_processing_metrics(query_length, **config)\n        estimated_time *= max(1, online_states[0] * 6)\n        if processor_instance[\"instance\"] is not None:\n            estimated_price += .25\n        if estimated_time &lt; 60:\n            time_str = f\"~{int(estimated_time)}s\"\n        elif estimated_time &lt; 3600:\n            minutes = estimated_time // 60\n            seconds = estimated_time % 60\n            time_str = f\"~{int(minutes)}m {int(seconds)}s\"\n        else:\n            hours = estimated_time // 3600\n            minutes = (estimated_time % 3600) // 60\n            time_str = f\"~{int(hours)}h {int(minutes)}m\"\n        with main_ui:\n            query_length_label.set_text(f\"Total Papers: {config['max_search']*config['num_search_result_per_query']}\")\n            time_label.set_text(f\"Processing Time: {time_str}\")\n            price_label.set_text(f\"Price: {estimated_price:.2f}\u20ac\")\n\n        return estimated_price\n\n    def on_config_change(event):\n        \"\"\"\n        Update the global config based on input changes and recalc estimates.\n        \"\"\"\n        try:\n            config['chunk_size'] = int(chunk_size_input.value)\n        except ValueError:\n            pass\n        try:\n            config['overlap'] = int(overlap_input.value)\n            if config['overlap'] &gt; config['chunk_size'] / 4:\n                config['overlap'] = int(config['chunk_size'] / 4)\n                with main_ui:\n                    overlap_input.value = config['overlap']\n        except ValueError:\n            pass\n        try:\n            config['num_search_result_per_query'] = int(num_search_result_input.value)\n        except ValueError:\n            pass\n        try:\n            config['max_search'] = int(max_search_input.value)\n        except ValueError:\n            pass\n        try:\n            config['num_workers'] = int(num_workers_input.value) if num_workers_input.value != 0 else None\n        except ValueError:\n            config['num_workers'] = None\n\n        update_estimates()\n\n    def on_query_change():\n        update_estimates()\n\n    # Callback, der vom Processor (\u00fcber processor_instance.callback) aufgerufen wird.\n    def update_status(data: dict):\n        nonlocal overall_progress, status_label\n        if not data:\n            return\n        # Aktualisiere den Fortschrittsbalken und den aktuellen Schritt (wenn vorhanden)\n        with main_ui:\n            if isinstance(data, dict):\n                progress = data.get(\"progress\", 0)\n                step = data.get(\"step\", \"Processing...\")\n                overall_progress.value =round( progress ,2) # nicegui.linear_progress erwartet einen Wert zwischen 0 und 1\n                status_label.set_text(f\"{step} {data.get('info','')}\")\n            else:\n                status_label.set_text(f\"{data}\")\n\n    def start_search():\n        nonlocal balance\n\n        async def helper():\n            nonlocal processor_instance, overall_progress, status_label, results_card,                     summary_content, analysis_content,config, references_content, followup_card,sr_button,r_button,r_text\n\n            try:\n                if not validate_inputs():\n                    with main_ui:\n                        state['balance'] += est_price\n                        save_user_state(session_id, state)\n                        balance.set_text(f\"Balance: {state['balance']:.2f}\u20ac\")\n                    return\n                reset_interface()\n                show_progress_indicators()\n\n                query_text = query.value.strip()\n                # Erzeuge das \"tools\"-Objekt (abh\u00e4ngig von deiner konkreten Implementation)\n                tools = get_tools()\n                with main_ui:\n                    research_card.visible = False\n                    config_cart.visible = False\n                    config_section.visible = False\n                    query.set_value(\"\")\n                # Direkt instanziieren: Eine neue ArXivPDFProcessor-Instanz\n                if processor_instance[\"instance\"] is not None:\n                    processor = processor_instance[\"instance\"]\n                    processor.chunk_size = config['chunk_size']\n                    processor.overlap = config['overlap']\n                    processor.num_search_result_per_query = config['num_search_result_per_query']\n                    processor.max_search = config['max_search']\n                    processor.num_workers = config['num_workers']\n                    papers, insights = await processor.process(query_text)\n                else:\n                    processor = Processor(query_text, tools=tools, **config)\n                # Setze den Callback so, dass Updates in der GUI angezeigt werden\n                    processor.callback = update_status\n                    processor_instance[\"instance\"] = processor\n                    papers, insights = await processor.process()\n\n                update_results({\n                    \"papers\": papers,\n                    \"insights\": insights\n                })\n                with main_ui:\n                    research_card.visible = True\n                    config_cart.visible = True\n                    show_history()\n\n            except Exception as e:\n                import traceback\n\n                with main_ui:\n                    update_status({\"progress\": 0, \"step\": \"Error\", \"info\": str(e)})\n                    state['balance'] += est_price\n                    save_user_state(session_id, state)\n                    balance.set_text(f\"Balance: {state['balance']:.2f}\u20ac\")\n                    ui.notify(f\"Error {str(e)})\", type=\"negative\")\n                    research_card.visible = True\n                    config_cart.visible = True\n                    config_section.visible = True\n                print(traceback.format_exc())\n\n        def target():\n            get_app().run_a_from_sync(helper, )\n\n        est_price = update_estimates()\n        if est_price &gt; state['balance']:\n            with main_ui:\n                ui.notify(f\"Insufficient balance. Need \u20ac{est_price:.2f}\", type='negative')\n        else:\n            state['balance'] -= est_price\n            save_user_state(session_id, state)\n            with main_ui:\n                online_states[0] += 1\n                balance.set_text(f\"Balance: {state['balance']:.2f}\u20ac Running Queries: {online_states[0]}\")\n\n            Thread(target=target, daemon=True).start()\n            with main_ui:\n                online_states[0] -= 1\n                balance.set_text(f\"Balance: {get_user_state(session_id)['balance']:.2f}\u20ac\")\n\n\n    def show_history():\n        with config_cart:\n            for idx, entry in enumerate(state['research_history']):\n                with ui.card().classes(\"w-full backdrop-blur-lg bg-white/10 p-4\"):\n                    ui.label(entry['query']).classes('text-sm')\n                    ui.button(\"Open\").on_click(lambda _, i=idx: load_history(i))\n\n    def reset():\n        nonlocal processor_instance, results_card, followup_card, sr_button, r_button, r_text\n        processor_instance[\"instance\"] = None\n        show_progress_indicators()\n        with main_ui:\n            config_cart.visible = False\n            config_section.visible = False\n            followup_card.visible = False\n            results_card.visible = False\n            r_button.visible = False\n            r_text.set_text(\"Research Interface\")\n            sr_button.set_text(\"Start Research\")\n        start_search()\n    # UI-Aufbau\n\n    with ui.column().classes(\"w-full max-w-6xl mx-auto p-6 space-y-6\") as main_ui:\n        balance = ui.label(f\"Balance: {state['balance']:.2f}\u20ac\").classes(\"text-s font-semibold\")\n\n        config_cart = config_cart\n\n        # --- Research Input UI Card ---\n        with ui.card().classes(\"w-full backdrop-blur-lg bg-white/10 p-4\") as research_card:\n            r_text = ui.label(\"Research Interface\").classes(\"text-3xl font-bold mb-4\")\n\n            # Query input section with auto-updating estimates\n            query = ui.input(\"Research Query\",\n                                placeholder=\"Gib hier deine Forschungsfrage ein...\",\n                                value=\"\")                     .classes(\"w-full min-h-[100px]\")                     .on('change', lambda e: on_query_change()).style(\"color: var(--text-color)\")\n\n            # --- Action Buttons ---\n            with ui.row().classes(\"mt-4\"):\n                sr_button =ui.button(\"Start Research\", on_click=start_search)                         .classes(\"bg-blue-600 hover:bg-blue-700 py-3 rounded-lg\")\n                ui.button(\"toggle config\",\n                          on_click=lambda: setattr(config_section, 'visible', not config_section.visible) or show_progress_indicators()).style(\n                    \"color: var(--text-color)\")\n                r_button = ui.button(\"Start new Research\",\n                          on_click=reset).style(\n                    \"color: var(--text-color)\")\n        sr_button = sr_button\n        r_button = r_button\n        r_button.visible = False\n        research_card = research_card\n\n        # --- Options Cart / Configurations ---\n        with ui.card_section().classes(\"w-full backdrop-blur-lg bg-white/10 hidden\") as config_section:\n            ui.separator()\n            ui.label(\"Configuration Options\").classes(\"text-xl font-semibold mt-4 mb-2\")\n            with ui.row():\n                chunk_size_input = ui.number(label=\"Chunk Size\",\n                                             value=config['chunk_size'], format='%.0f', max=64_000, min=1000,\n                                             step=100)                         .on('change', on_config_change).style(\"color: var(--text-color)\")\n                overlap_input = ui.number(label=\"Overlap\",\n                                          value=config['overlap'], format='%.0f', max=6400, min=100, step=50)                         .on('change', on_config_change).style(\"color: var(--text-color)\")\n\n            with ui.row():\n                num_search_result_input = ui.number(label=\"Results per Query\",\n                                                    value=config['num_search_result_per_query'], format='%.0f',\n                                                    min=1, max=100, step=1)                         .on('change', on_config_change).style(\"color: var(--text-color)\")\n                max_search_input = ui.number(label=\"Max Search Queries\",\n                                             value=config['max_search'], format='%.0f', min=1, max=100, step=1)                         .on('change', on_config_change).style(\"color: var(--text-color)\")\n                num_workers_input = ui.number(label=\"Number of Workers (leave empty for default)\",\n                                              value=0, format='%.0f', min=0, max=32, step=1)                         .on('change', on_config_change).style(\"color: var(--text-color)\")\n        config_section = config_section\n        config_section.visible = False\n        # --- Ergebnisse anzeigen ---\n        with ui.card().classes(\"w-full backdrop-blur-lg p-4 bg-white/10\") as results_card:\n            ui.label(\"Research Results\").classes(\"text-xl font-semibold mb-4\")\n            with ui.tabs() as tabs:\n                ui.tab(\"Summary\")\n                ui.tab(\"References\")\n                ui.tab(\"SystemStates\")\n            with ui.tab_panels(tabs, value=\"Summary\").classes(\"w-full\").style(\"background-color: var(--background-color)\"):\n                with ui.tab_panel(\"Summary\"):\n                    summary_content = ui.markdown(\"\").style(\"color : var(--text-color)\")\n                with ui.tab_panel(\"References\"):\n                    references_content = ui.markdown(\"\").style(\"color : var(--text-color)\")\n                with ui.tab_panel(\"SystemStates\"):\n                    analysis_content = ui.markdown(\"\").style(\"color : var(--text-color)\")\n\n\n        # Ergebnisse sichtbar machen, sobald sie vorliegen.\n        results_card = results_card\n        results_card.visible = False\n\n        # --- Follow-Up Bereich mit mehrfachen Folgefragen und Suchparametern ---\n        with ui.card().classes(\"w-full backdrop-blur-lg bg-white/10 p-4 hidden\") as followup_card:\n            pass\n\n        # Zugriff auf followup_card (falls sp\u00e4ter ben\u00f6tigt)\n        followup_card = followup_card\n        followup_card.visible = False\n\n        # --- Fortschrittsanzeige ---\n        with ui.card().classes(\"w-full backdrop-blur-lg bg-white/10 p-4\") as progress_card:\n            with ui.row():\n                ui.label(\"Research Progress\").classes(\"text-xl font-semibold mb-4\")\n                query_length_label = ui.label(\"\").classes(\"mt-6 hover:text-primary transition-colors duration-300\")\n                time_label = ui.label(\"Time: ...\").classes(\"mt-6 hover:text-primary transition-colors duration-300\")\n                price_label = ui.label(\"Price: ...\").classes(\n                    \"mt-6 hover:text-primary transition-colors duration-300\")\n\n            overall_progress = ui.linear_progress(0).classes(\"w-full mb-4\")\n            status_label = ui.label(\"Warte auf Start...\").classes(\"text-base\")\n        # Wir merken uns progress_card, falls wir ihn zur\u00fccksetzen wollen.\n        progress_card = progress_card\n\n        query_length_label = query_length_label\n        time_label = time_label\n        price_label = price_label\n\n        with ui.card().classes(\"w-full backdrop-blur-lg bg-white/10 p-4\") as config_cart:\n            # --- Process Code Section ---\n            # --- Estimated Time and Price ---\n            # ui.label(\"History\").classes(\"text-xl font-semibold mt-4 mb-2\")\n            ui.label('Research History').classes('text-xl p-4')\n            show_history()\n\n        ui.button('Add Credits', on_click=lambda: balance_overlay(session_id)).props('icon=paid')\n        ui.label('About TruthSeeker').classes(\n            'mt-6 text-gray-500 hover:text-primary '\n            'transition-colors duration-300'\n        ).on('click', lambda: ui.navigate.to('/open-Seeker.about', new_tab=True))\n\n        with ui.element('div').classes(\"w-full\").style(\"white:100%; height:100%\") as graph_ui:\n            pass\n\n        with ui.card().classes(\"w-full p-4\").style(\"background-color: var(--background-color)\"):\n            ui.label(\"Private Session link (restore the session on a different device)\")\n            base_url = f'https://{os.getenv(\"HOSTNAME\")}/gui/open-Seeker.seek' if not 'localhost' in os.getenv(\"HOSTNAME\") else 'http://localhost:5000/gui/open-Seeker.seek'\n            ui.label(f\"{base_url}?session_id={session_id}\").style(\"white:100%\")\n            ui.label(\"Changes each time!\")\n\n        graph_ui = graph_ui\n        graph_ui.visible = False\n    main_ui = main_ui\n    main_ui.visible = False\n\n    # --- Hilfsfunktionen ---\n    def validate_inputs() -&gt; bool:\n        if not query.value.strip():\n            with main_ui:\n                ui.notify(\"Bitte gib eine Forschungsfrage ein.\", type=\"warning\")\n            return False\n        return True\n\n    def reset_interface():\n        nonlocal overall_progress, status_label, results_card, followup_card\n        overall_progress.value = 0\n        with main_ui:\n            status_label.set_text(\"Research startet...\")\n        # Ergebnisse und Follow-Up Bereich verstecken\n        results_card.visible = False\n        followup_card.visible = False\n        graph_ui.visible = False\n\n    def show_progress_indicators():\n        nonlocal progress_card\n        progress_card.visible = True\n\n    def update_results(data: dict, save=True):\n        nonlocal summary_content, analysis_content, references_content, results_card,                followup_card,graph_ui, r_button, r_text, sr_button\n        with main_ui:\n            r_button.visible = True\n            r_text.set_text(\"Add to current Results or press 'Start new Research'\")\n            sr_button.set_text(\"Add to current Results\")\n        # Handle papers (1-to-1 case)\n        papers = data.get(\"papers\", [])\n        if not isinstance(papers, list):\n            papers = [papers]\n\n        # Get insights\n        insights = data.get(\"insights\", [])\n\n        if save:\n            history_entry = data.copy()\n            history_entry['papers'] = [paper.model_dump_json() for paper in papers]\n            if processor_instance is not None and processor_instance['instance'] is not None:\n                history_entry[\"mam_name\"] = processor_instance['instance'].mem_name\n                history_entry[\"query\"] = processor_instance['instance'].query\n\n                history_entry[\"processor_memory\"] = processor_instance['instance'].tools.get_memory(\n\n                ).save_memory(history_entry[\"mam_name\"], None)\n            state['research_history'].append(history_entry)\n            save_user_state(session_id, state)\n        else:\n            papers = [Paper(**json.loads(paper)) for paper in papers]\n        create_followup_section(processor_instance, followup_card, session_id, balance)\n        with main_ui:\n            progress_card.visible = False\n            # Build summary from insights\n            summaries = []\n            for insight in insights:\n                if 'result' in insight and 'summary' in insight['result']:\n                    if isinstance(insight['result']['summary'], str):\n                        # print(insight['result']['summary'], \"NEXT\", json.loads(insight['result']['summary'][:-1]),\"NEXT22\",  type(json.loads(insight['result']['summary'][:-1])))\n                        insight['result']['summary'] = json.loads(insight['result']['summary'][:-1])\n                    main_summary = insight['result']['summary'].get('main_summary', '')\n                    if main_summary:\n                        summaries.append(main_summary)\n            summary_text = \"\n</code></pre> <p>\".join(summaries) if summaries else \"No summary available.\"                 summary_content.set_content(f\"# Research Summary</p> <p>{summary_text}\")</p> <pre><code>            # Analysis section (unchanged if processor details haven't changed)\n            if processor_instance[\"instance\"] is not None:\n                inst = processor_instance[\"instance\"]\n                analysis_md = (\n                    f\"# Analysis\n</code></pre> <p>\"                         f\"- query: {inst.query} \"                         f\"- chunk_size: {inst.chunk_size} \"                         f\"- overlap: {inst.overlap} \"                         f\"- max_workers: {inst.max_workers} \"                         f\"- num_search_result_per_query: {inst.nsrpq} \"                         f\"- max_search: {inst.max_search} \"                         f\"- download_dir: {inst.download_dir} \"                         f\"- mem_name: {inst.mem_name} \"                         f\"- current_session: {inst.current_session} \"                         f\"- all_ref_papers: {inst.all_ref_papers} \"                         f\"- all_texts_len: {inst.all_texts_len} \"                         f\"- final_texts_len: {inst.f_texts_len} \"                         f\"- num_workers: {inst.num_workers}\"                     )                     analysis_content.set_content(analysis_md)</p> <pre><code>            # References and Insights section\n            references_md = \"# References\n</code></pre> <p>\"                 # Add papers                 references_md += \" \".join(                     f\"- ({i}) {getattr(paper, 'title', 'Unknown Title')}})\"                     for i, paper in enumerate(papers)                 )</p> <pre><code>            # Add detailed insights\n            references_md += \"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--insights","title":"Insights","text":"<p>\"                 for i, insight in enumerate(insights):                     print(insight)                     result = insight.get('result', {})                     summary = result.get('summary', {})</p> <pre><code>                if isinstance(summary, str):\n                    summary = json.loads(summary)\n\n                # Main summary\n                references_md += f\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--insight","title":"Insight","text":"<p>\"                     references_md += f\"### Main Summary {summary.get('main_summary', 'No summary available.')} \"</p> <pre><code>                # Concept Analysis\n                concept_analysis = summary.get('concept_analysis', {})\n                if concept_analysis:\n                    references_md += \"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--concept-analysis","title":"Concept Analysis","text":"<p>\"                         references_md += \"#### Key Concepts - \" + \" - \".join(                             concept_analysis.get('key_concepts', [])) + \" \"                         references_md += \"</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--relationships","title":"Relationships","text":"<ul> <li>\" + \"</li> <li>\".join(                             concept_analysis.get('relationships', [])) + \" \"                         references_md += \"</li> </ul>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--importance-hierarchy","title":"Importance Hierarchy","text":"<ul> <li>\" + \"</li> <li> <p>\".join(                             concept_analysis.get('importance_hierarchy', [])) + \" \"</p> <pre><code>            # Topic Insights\n            topic_insights = summary.get('topic_insights', {})\n            if topic_insights:\n                references_md += \"\n</code></pre> </li> </ul>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--topic-insights","title":"Topic Insights","text":"<p>\"                     references_md += \"#### Primary Topics - \" + \" - \".join(                         topic_insights.get('primary_topics', [])) + \" \"                     references_md += \"</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--cross-references","title":"Cross References","text":"<ul> <li>\" + \"</li> <li>\".join(                         topic_insights.get('cross_references', [])) + \" \"                     references_md += \"</li> </ul>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--knowledge-gaps","title":"Knowledge Gaps","text":"<ul> <li>\" + \"</li> <li> <p>\".join(                         topic_insights.get('knowledge_gaps', [])) + \" \"</p> <pre><code>        # Relevance Assessment\n        relevance = summary.get('relevance_assessment', {})\n        if relevance:\n            references_md += \"\n</code></pre> </li> </ul> <p>return helpr</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui--relevance-assessment","title":"Relevance Assessment","text":"<p>\"                 references_md += f\"- Query Alignment: {relevance.get('query_alignment', 'N/A')} \"                 references_md += f\"- Confidence Score: {relevance.get('confidence_score', 'N/A')} \"                 references_md += f\"- Coverage Analysis: {relevance.get('coverage_analysis', 'N/A')} \"</p> <pre><code>    references_content.set_content(references_md)\n\n    # nx concpts graph\n    if processor_instance[\"instance\"] is not None:\n        create_graph_tab(\n            processor_instance,\n            graph_ui,main_ui\n        )\n\n    # Show results and followup cards\n    results_card.visible = True\n    followup_card.visible = True\n    graph_ui.visible = True\n</code></pre> <p>def load_history(index: int):     entry = state['research_history'][index]     if processor_instance is not None and processor_instance['instance'] is not None:</p> <pre><code>    processor_instance[\"instance\"].mem_name = entry[\"mam_name\"]\n    processor_instance['instance'].query = entry[\"query\"]\n\n    pass\nelse:\n    processor = Processor(entry[\"query\"], tools=get_tools(), **config)\n    # Setze den Callback so, dass Updates in der GUI angezeigt werden\n    processor.callback = update_status\n    processor.mem_name = entry[\"mam_name\"]\n    processor_instance[\"instance\"] = processor\n\nprocessor_instance[\"instance\"].tools.get_memory().load_memory(entry[\"mam_name\"], entry[\"processor_memory\"])\nprocessor_instance[\"instance\"].mem_name = entry[\"mam_name\"]\nupdate_results(entry, save=False)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui---stripe-integration-","title":"--- Stripe Integration ---","text":"<p>def regiser_stripe_integration(is_scc=True):     def stripe_callback(request: Request):</p> <pre><code>    sid = request.row.query_params.get('session_id') if hasattr(request, 'row') else request.query_params.get('session_id')\n    state = get_user_state(sid)\n\n    if state['payment_id'] == '':\n        with ui.card().classes(\"w-full items-center\").style(\"background-color: var(--background-color)\"):\n            ui.label(f\"No payment id!\").classes(\"text-lg font-bold\")\n            ui.button(\n                \"Start Research\",\n                on_click=lambda: ui.navigate.to(\"/open-Seeker.seek?session_id=\"+sid)\n            ).classes(\n                \"w-full px-6 py-4 text-lg font-bold \"\n                \"bg-primary hover:bg-primary-dark \"\n                \"transform hover:-translate-y-0.5 \"\n                \"transition-all duration-300 ease-in-out \"\n                \"rounded-xl shadow-lg animate-slideUp\"\n            )\n        return\n\n    try:\n        session_data = stripe.checkout.Session.retrieve(state['payment_id'])\n    except Exception as e:\n        with ui.card().classes(\"w-full items-center\").style(\"background-color: var(--background-color)\"):\n            ui.label(f\"No Transactions Details !{e}\").classes(\"text-lg font-bold\")\n            ui.button(\n                \"Start Research\",\n                on_click=lambda: ui.navigate.to(\"/open-Seeker.seek\")\n            ).classes(\n                \"w-full px-6 py-4 text-lg font-bold \"\n                \"bg-primary hover:bg-primary-dark \"\n                \"transform hover:-translate-y-0.5 \"\n                \"transition-all duration-300 ease-in-out \"\n                \"rounded-xl shadow-lg animate-slideUp\"\n            )\n            return\n    with ui.card().classes(\"w-full items-center\").style(\"background-color: var(--background-color)\"):\n        if is_scc and state['payment_id'] != '' and session_data.payment_status == 'paid':\n            state = get_user_state(sid)\n            amount = session_data.amount_total / 100  # Convert cents to euros\n            state['balance'] += amount\n            state['payment_id'] = ''\n            save_user_state(sid, state)\n\n        # ui.navigate.to(f'/session?session={session}')\n            ui.label(f\"Transaction Complete - New balance :{state['balance']}\").classes(\"text-lg font-bold\")\n            with ui.card().classes(\"w-full p-4\").style(\"background-color: var(--background-color)\"):\n                ui.label(\"Private Session link (restore the session on a different device)\")\n                base_url = f'https://{os.getenv(\"HOSTNAME\")}/gui/open-Seeker.seek' if not 'localhost' in os.getenv(\"HOSTNAME\")else 'http://localhost:5000/gui/open-Seeker.seek'\n                ui.label(f\"{base_url}?session_id={sid}\").style(\"white:100%\")\n                ui.label(\"Changes each time!\")\n        else:\n            ui.label(f\"Transaction Error! {session_data}, {dir(session_data)}\").classes(\"text-lg font-bold\")\n        ui.button(\n            \"Start Research\",\n            on_click=lambda: ui.navigate.to(\"/open-Seeker.seek\")\n        ).classes(\n            \"w-full px-6 py-4 text-lg font-bold \"\n            \"bg-primary hover:bg-primary-dark \"\n            \"transform hover:-translate-y-0.5 \"\n            \"transition-all duration-300 ease-in-out \"\n            \"rounded-xl shadow-lg animate-slideUp\"\n        )\n\n\nreturn stripe_callback\n</code></pre> <p>def handle_stripe_payment(amount: float, session_id):     base_url = f'https://{os.getenv(\"HOSTNAME\")}/gui/open-Seeker.stripe' if not 'localhost' in os.getenv(\"HOSTNAME\") else 'http://localhost:5000/gui/open-Seeker.stripe'     session = stripe.checkout.Session.create(         payment_method_types=['card',                               \"link\",                               ],         line_items=[{             'price_data': {                 'currency': 'eur',                 'product_data': {'name': 'Research Credits'},                 'unit_amount': int(amount * 100),             },             'quantity': 1,         }],         automatic_tax={\"enabled\": True},         mode='payment',         success_url=f'{base_url}?session_id={session_id}',         cancel_url=f'{base_url}.error'     )     state = get_user_state(session_id)     state['payment_id'] = session.id     save_user_state(session_id, state)     ui.navigate.to(session.url, new_tab=True)</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.nGui---ui-components-","title":"--- UI Components ---","text":"<p>def balance_overlay(session_id):     with ui.dialog().classes('w-full max-w-md bg-white/20 backdrop-blur-lg rounded-xl') as dialog:         with ui.card().classes('w-full p-6 space-y-4').style(\"background-color: var(--background-color)\"):             ui.label('Add Research Credits').classes('text-2xl font-bold')             amount = ui.number('Amount (\u20ac) min 2', value=5, format='%.2f', min=2, max=9999, step=1).classes('w-full')             with ui.row().classes('w-full justify-between'):                 ui.button('Cancel', on_click=dialog.close).props('flat')                 ui.button('Purchase', on_click=lambda: handle_stripe_payment(amount.value, session_id))     return dialog</p> <p>def create_ui(processor):     # ui_instance =     register_nicegui(\"open-Seeker\", create_landing_page                      , additional=\"\"\"     \"\"\", show=False)     register_nicegui(\"open-Seeker.demo\", create_video_demo, additional=\"\"\"          \"\"\", show=False)</p>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui","title":"<code>newui</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.cleanup_module","title":"<code>cleanup_module(app)</code>","text":"<p>Cleanup resources when the module is unloaded</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, version=version, exit_f=True)\ndef cleanup_module(app: App):\n    \"\"\"Cleanup resources when the module is unloaded\"\"\"\n    # Clean up any temp files or resources\n    import glob\n    import shutil\n\n    # Remove temporary PDF directories\n    for pdf_dir in glob.glob(\"pdfs_*\"):\n        try:\n            shutil.rmtree(pdf_dir)\n        except Exception as e:\n            print(f\"Error removing directory {pdf_dir}: {str(e)}\")\n\n    # Clear any SSE queues\n    if hasattr(app, 'sse_queues'):\n        app.sse_queues = {}\n\n    if hasattr(app, 'payment_queues'):\n        app.payment_queues = {}\n\n    return Result.ok(info=\"ArXivPDFProcessor UI cleaned up\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.create_payment","title":"<code>create_payment(app, data)</code>  <code>async</code>","text":"<p>Create a Stripe payment session</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def create_payment(app: App, data):\n    \"\"\"Create a Stripe payment session\"\"\"\n    amount = data.get(\"amount\")\n    session_id = data.get(\"session_id\")\n\n    if amount &lt; 2:\n        return Result.default_user_error(info=\"Minimum donation amount is \u20ac2\")\n\n    try:\n        # Create a Stripe Checkout Session\n        base_url = f\"https://{os.getenv('HOSTNAME', 'localhost:5000')}\"\n        success_url = f\"{base_url}/api/{MOD_NAME}/payment_success?session_id={session_id}\"\n        cancel_url = f\"{base_url}/api/{MOD_NAME}/payment_cancel?session_id={session_id}\"\n\n        stripe_session = stripe.checkout.Session.create(\n            payment_method_types=['card', 'link'],\n            line_items=[{\n                'price_data': {\n                    'currency': 'eur',\n                    'product_data': {'name': 'Research Credits'},\n                    'unit_amount': int(amount * 100),\n                },\n                'quantity': 1,\n            }],\n            automatic_tax={\"enabled\": True},\n            mode='payment',\n            success_url=success_url,\n            cancel_url=cancel_url\n        )\n\n        # Store the payment info\n        if not hasattr(app, 'payment_info'):\n            app.payment_info = {}\n\n        # Initialize payment_queues if not already done\n        if not hasattr(app, 'payment_queues'):\n            app.payment_queues = {}\n\n        # Create a queue for this payment\n        app.payment_queues[session_id] = asyncio.Queue()\n\n        app.payment_info[session_id] = {\n            'payment_id': stripe_session.id,\n            'amount': amount,\n            'status': 'pending'\n        }\n\n        return Result.ok(data={\"url\": stripe_session.url})\n    except Exception as e:\n        return Result.default_internal_error(info=f\"Error creating payment: {str(e)}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.estimate_processing","title":"<code>estimate_processing(data)</code>  <code>async</code>","text":"<p>Estimate processing time and cost for a given query</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def estimate_processing(data):\n    \"\"\"Estimate processing time and cost for a given query\"\"\"\n    # Use the static method to estimate metrics\n    query, max_search, num_search_result_per_query= data.get(\"query\", \"\"), data.get(\"max_search\",4), data.get(\"num_search_result_per_query\",6)\n    estimated_time, estimated_price = ArXivPDFProcessor.estimate_processing_metrics(\n        query_length=len(query),\n        max_search=max_search,\n        num_search_result_per_query=num_search_result_per_query,\n        chunk_size=1_000_000,\n        overlap=2_000,\n        num_workers=None\n    )\n\n    return Result.ok(data={\n        \"time\": estimated_time,\n        \"price\": estimated_price\n    })\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.follow_up_query","title":"<code>follow_up_query(app, data)</code>  <code>async</code>","text":"<p>Ask a follow-up question about the research</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def follow_up_query(app: App, data):\n    \"\"\"Ask a follow-up question about the research\"\"\"\n    research_id = data.get(\"research_id\")\n    query = data.get(\"query\")\n\n    if not hasattr(app, 'research_processes') or research_id not in app.research_processes:\n        return Result.default_user_error(info=\"Research process not found\")\n\n    research_process = app.research_processes[research_id]\n\n    if research_process['status'] != 'complete':\n        return Result.default_user_error(info=\"Research is not complete\")\n\n    processor = research_process['processor']\n    if not processor:\n        return Result.default_user_error(info=\"Processor not available\")\n\n    try:\n        # Use the extra_query method to ask follow-up questions\n        result = await processor.extra_query(query)\n\n        return Result.ok(data={\"answer\": result['response'] if result and 'response' in result else \"No response\"})\n    except Exception as e:\n        return Result.default_internal_error(info=f\"Error processing follow-up query: {str(e)}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.initialize_module","title":"<code>initialize_module(app)</code>","text":"<p>Initialize the module and register UI with CloudM</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, version=version, initial=True)\ndef initialize_module(app: App):\n    \"\"\"Initialize the module and register UI with CloudM\"\"\"\n    # Register the UI with CloudM\n    app.run_any((\"CloudM\", \"add_ui\"),\n                name=\"TruthSeeker\",\n                title=\"TruthSeeker Research\",\n                path=f\"/api/{MOD_NAME}/get_main_ui\",\n                description=\"AI Research Assistant\"\n                )\n\n    # Initialize SSE message queues\n    if not hasattr(app, 'sse_queues'):\n        app.sse_queues = {}\n    print(\"TruthSeeker online\")\n    return Result.ok(info=\"ArXivPDFProcessor UI initialized\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.payment_cancel","title":"<code>payment_cancel(app, session_id, request_as_kwarg=True, request=None)</code>  <code>async</code>","text":"<p>Handle cancelled payment</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def payment_cancel(app: App, session_id: str, request_as_kwarg=True, request=None):\n    \"\"\"Handle cancelled payment\"\"\"\n    if hasattr(app, 'payment_info') and session_id in app.payment_info:\n        app.payment_info[session_id]['status'] = 'cancelled'\n\n        # Notify SSE clients about payment cancellation\n        if hasattr(app, 'payment_queues') and session_id in app.payment_queues:\n            await app.payment_queues[session_id].put({\n                \"status\": \"cancelled\"\n            })\n\n    return Result.html(app.web_context() + \"\"\"\n    &lt;div style=\"text-align: center; padding: 50px;\"&gt;\n        &lt;h2&gt;Payment Cancelled&lt;/h2&gt;\n        &lt;p&gt;Your payment was cancelled.&lt;/p&gt;\n        &lt;script&gt;\n            setTimeout(function() {\n                window.close();\n            }, 3000);\n        &lt;/script&gt;\n    &lt;/div&gt;\n    \"\"\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.payment_stream","title":"<code>payment_stream(app, session_id)</code>  <code>async</code>","text":"<p>SSE stream endpoint for payment status updates</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def payment_stream(app: App, session_id: str):\n    \"\"\"SSE stream endpoint for payment status updates\"\"\"\n    if not hasattr(app, 'payment_queues'):\n        app.payment_queues = {}\n\n    # Create a message queue for this session_id if it doesn't exist\n    if session_id not in app.payment_queues:\n        app.payment_queues[session_id] = asyncio.Queue()\n\n    async def generate():\n        try:\n            # Stream payment updates\n            while True:\n                try:\n                    # Wait for a payment update with a timeout\n                    payment_data = await asyncio.wait_for(app.payment_queues[session_id].get(), timeout=30)\n                    yield f\"event: payment_update\\ndata: {json.dumps(payment_data)}\\n\\n\"\n\n                    # If the payment is complete or cancelled, exit the loop\n                    if payment_data.get('status') in ['completed', 'cancelled']:\n                        break\n                except TimeoutError:\n                    # Send a keep-alive comment to prevent connection timeout\n                    yield \":\\n\\n\"\n        finally:\n            # Clean up resources when the client disconnects\n            if session_id in app.payment_queues:\n                # Keep the queue for other potential clients\n                pass\n\n    return Result.stream(generate())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.payment_success","title":"<code>payment_success(app, session_id, request_as_kwarg=True, request=None)</code>  <code>async</code>","text":"<p>Handle successful payment</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def payment_success(app: App, session_id: str, request_as_kwarg=True, request=None):\n    \"\"\"Handle successful payment\"\"\"\n    if not hasattr(app, 'payment_info') or session_id not in app.payment_info:\n        return Result.html(app.web_context() + \"\"\"\n        &lt;div style=\"text-align: center; padding: 50px;\"&gt;\n            &lt;h2&gt;Payment Session Not Found&lt;/h2&gt;\n            &lt;p&gt;Return to the main page to continue.&lt;/p&gt;\n            &lt;a href=\"/\" style=\"display: inline-block; margin-top: 20px; padding: 10px 20px; background-color: #4F46E5; color: white; text-decoration: none; border-radius: 5px;\"&gt;Return to Home&lt;/a&gt;\n        &lt;/div&gt;\n        \"\"\")\n\n    payment_info = app.payment_info[session_id]\n\n    try:\n        # Verify the payment with Stripe\n        stripe_session = stripe.checkout.Session.retrieve(payment_info['payment_id'])\n\n        if stripe_session.payment_status == 'paid':\n            payment_info['status'] = 'completed'\n\n            # Notify SSE clients about payment completion\n            if hasattr(app, 'payment_queues') and session_id in app.payment_queues:\n                await app.payment_queues[session_id].put({\n                    \"status\": \"completed\",\n                    \"amount\": payment_info['amount']\n                })\n\n            return Result.html(app.web_context() + \"\"\"\n            &lt;div style=\"text-align: center; padding: 50px;\"&gt;\n                &lt;h2&gt;Thank You for Your Support!&lt;/h2&gt;\n                &lt;p&gt;Your payment was successful. You can now close this window and continue with your research.&lt;/p&gt;\n                &lt;script&gt;\n                    setTimeout(function() {\n                        window.close();\n                    }, 5000);\n                &lt;/script&gt;\n            &lt;/div&gt;\n            \"\"\")\n        else:\n            return Result.html(app.web_context() + \"\"\"\n            &lt;div style=\"text-align: center; padding: 50px;\"&gt;\n                &lt;h2&gt;Payment Not Completed&lt;/h2&gt;\n                &lt;p&gt;Your payment has not been completed. Please try again.&lt;/p&gt;\n                &lt;button onclick=\"window.close()\"&gt;Close Window&lt;/button&gt;\n            &lt;/div&gt;\n            \"\"\")\n    except Exception as e:\n        return Result.html(app.web_context() + f\"\"\"\n        &lt;div style=\"text-align: center; padding: 50px;\"&gt;\n            &lt;h2&gt;Error Processing Payment&lt;/h2&gt;\n            &lt;p&gt;There was an error processing your payment: {str(e)}&lt;/p&gt;\n            &lt;button onclick=\"window.close()\"&gt;Close Window&lt;/button&gt;\n        &lt;/div&gt;\n        \"\"\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.research_results","title":"<code>research_results(app, research_id)</code>  <code>async</code>","text":"<p>Get the results of a completed research process</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def research_results(app: App, research_id: str):\n    \"\"\"Get the results of a completed research process\"\"\"\n    if not hasattr(app, 'research_processes') or research_id not in app.research_processes:\n        return Result.default_user_error(info=\"Research process not found\")\n\n    research_process = app.research_processes[research_id]\n\n    if research_process['status'] != 'complete':\n        return Result.default_user_error(info=\"Research is not complete\")\n\n    return Result.ok(data=research_process['results'])\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.research_status","title":"<code>research_status(app, research_id)</code>  <code>async</code>","text":"<p>Get the status of a research process</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def research_status(app: App, research_id: str):\n    \"\"\"Get the status of a research process\"\"\"\n    if not hasattr(app, 'research_processes') or research_id not in app.research_processes:\n        return Result.default_user_error(info=\"Research process not found\")\n\n    research_process = app.research_processes[research_id]\n\n    return Result.ok(data={\n        \"status\": research_process['status'],\n        \"progress\": research_process['progress'],\n        \"step\": research_process['step'],\n        \"info\": research_process['info']\n    })\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.start_research","title":"<code>start_research(app, data)</code>  <code>async</code>","text":"<p>Start a new research process</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def start_research(app: App, data):\n    \"\"\"Start a new research process\"\"\"\n    # Get data from the request\n    query = data.get(\"query\")\n    session_id = data.get(\"session_id\")\n    max_search = data.get(\"max_search\", 4)\n    num_search_result_per_query = data.get(\"num_search_result_per_query\", 4)\n\n    # Get the tools module\n    tools = get_app(\"ArXivPDFProcessor\").get_mod(\"isaa\")\n    if not hasattr(tools, 'initialized') or not tools.initialized:\n        tools.init_isaa(build=True)\n\n    # Generate a unique research_id\n    research_id = str(uuid.uuid4())\n\n    # Store the research information in a global dictionary\n    if not hasattr(app, 'research_processes'):\n        app.research_processes = {}\n\n    # Initialize SSE queues if not already done\n    if not hasattr(app, 'sse_queues'):\n        app.sse_queues = {}\n\n    # Create a queue for this research process\n    app.sse_queues[research_id] = asyncio.Queue()\n\n    # Create a processor with callback for status updates\n    app.research_processes[research_id] = {\n        'status': 'initializing',\n        'progress': 0.0,\n        'step': 'Initializing',\n        'info': '',\n        'query': query,\n        'session_id': session_id,\n        'processor': None,\n        'results': None,\n        'stop_requested': False\n    }\n\n    # Define the callback function that sends updates to the SSE queue\n    def status_callback(status_data):\n        if research_id in app.research_processes:\n            process = app.research_processes[research_id]\n            process['status'] = 'processing'\n            process['progress'] = status_data.get('progress', 0.0)\n            process['step'] = status_data.get('step', '')\n            process['info'] = status_data.get('info', '')\n\n            # Put the status update in the SSE queue\n            status_update = {\n                \"status\": process['status'],\n                \"progress\": process['progress'],\n                \"step\": process['step'],\n                \"info\": process['info']\n            }\n\n            if research_id in app.sse_queues:\n                asyncio.create_task(app.sse_queues[research_id].put(status_update))\n\n    # Create the processor\n    processor = ArXivPDFProcessor(\n        query=query,\n        tools=tools,\n        chunk_size=1_000_000,\n        overlap=2_000,\n        max_search=max_search,\n        num_search_result_per_query=num_search_result_per_query,\n        download_dir=f\"pdfs_{research_id}\",\n        callback=status_callback\n    )\n\n    app.research_processes[research_id]['processor'] = processor\n\n    # Process in the background\n    async def process_in_background():\n        try:\n            # Check if stop was requested before starting\n            if app.research_processes[research_id]['stop_requested']:\n                app.research_processes[research_id]['status'] = 'stopped'\n                if research_id in app.sse_queues:\n                    await app.sse_queues[research_id].put({\n                        \"status\": \"stopped\",\n                        \"progress\": 0,\n                        \"step\": \"Research stopped\",\n                        \"info\": \"\"\n                    })\n                return\n\n            # Start processing\n            papers, insights = await processor.process()\n\n            # Check if stop was requested during processing\n            if app.research_processes[research_id]['stop_requested']:\n                app.research_processes[research_id]['status'] = 'stopped'\n                if research_id in app.sse_queues:\n                    await app.sse_queues[research_id].put({\n                        \"status\": \"stopped\",\n                        \"progress\": 1,\n                        \"step\": \"Research stopped\",\n                        \"info\": \"\"\n                    })\n                return\n\n            # Store results\n            app.research_processes[research_id]['results'] = {\n                'papers': papers,\n                'insights': insights['response'] if insights and 'response' in insights else None\n            }\n            app.research_processes[research_id]['status'] = 'complete'\n\n            # Send final status update\n            if research_id in app.sse_queues:\n                await app.sse_queues[research_id].put({\n                    \"status\": \"complete\",\n                    \"progress\": 1,\n                    \"step\": \"Research complete\",\n                    \"info\": f\"Found {len(papers)} papers\"\n                })\n\n        except Exception as e:\n            app.research_processes[research_id]['status'] = 'error'\n            app.research_processes[research_id]['info'] = str(e)\n\n            # Send error status\n            if research_id in app.sse_queues:\n                await app.sse_queues[research_id].put({\n                    \"status\": \"error\",\n                    \"progress\": 0,\n                    \"step\": \"Error\",\n                    \"info\": str(e)\n                })\n\n            print(f\"Error in research process {research_id}: {str(e)}\")\n\n    # Start the background task\n    asyncio.create_task(process_in_background())\n\n    return Result.ok(data={\"research_id\": research_id})\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.status_stream","title":"<code>status_stream(app, research_id)</code>  <code>async</code>","text":"<p>SSE stream endpoint for research status updates</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def status_stream(app: App, research_id: str):\n    \"\"\"SSE stream endpoint for research status updates\"\"\"\n    if not hasattr(app, 'sse_queues'):\n        app.sse_queues = {}\n\n    # Create a message queue for this research_id if it doesn't exist\n    if research_id not in app.sse_queues:\n        app.sse_queues[research_id] = asyncio.Queue()\n\n    async def generate():\n        # Send initial status\n        if hasattr(app, 'research_processes') and research_id in app.research_processes:\n            process = app.research_processes[research_id]\n            initial_status = {\n                \"status\": process['status'],\n                \"progress\": process['progress'],\n                \"step\": process['step'],\n                \"info\": process['info']\n            }\n            yield f\"event: status_update\\ndata: {json.dumps(initial_status)}\\n\\n\"\n\n        try:\n            # Stream status updates\n            while True:\n                try:\n                    # Wait for a new status update with a timeout\n                    status_data = await asyncio.wait_for(app.sse_queues[research_id].get(), timeout=30)\n                    yield f\"event: status_update\\ndata: {json.dumps(status_data)}\\n\\n\"\n\n                    # If the research is complete or there was an error, exit the loop\n                    if status_data.get('status') in ['complete', 'error', 'stopped']:\n                        break\n                except TimeoutError:\n                    # Send a keep-alive comment to prevent connection timeout\n                    yield \":\\n\\n\"\n        finally:\n            # Clean up resources when the client disconnects\n            if research_id in app.sse_queues:\n                # Keep the queue for other potential clients\n                pass\n\n    return Result.stream(generate())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.newui.stop_research","title":"<code>stop_research(app, data)</code>  <code>async</code>","text":"<p>Stop a research process</p> Source code in <code>toolboxv2/mods/TruthSeeker/newui.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=version)\nasync def stop_research(app: App, data):\n    \"\"\"Stop a research process\"\"\"\n    research_id = data.get(\"research_id\")\n    if not hasattr(app, 'research_processes') or research_id not in app.research_processes:\n        return Result.default_user_error(info=\"Research process not found\")\n\n    app.research_processes[research_id]['stop_requested'] = True\n\n    # Send stopped status to SSE clients\n    if hasattr(app, 'sse_queues') and research_id in app.sse_queues:\n        await app.sse_queues[research_id].put({\n            \"status\": \"stopped\",\n            \"progress\": app.research_processes[research_id]['progress'],\n            \"step\": \"Stopping research\",\n            \"info\": \"\"\n        })\n\n    return Result.ok(data={\"status\": \"stop_requested\"})\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.one","title":"<code>one</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.one.IntelligenceRingEmbeddings","title":"<code>IntelligenceRingEmbeddings</code>","text":"Source code in <code>toolboxv2/mods/TruthSeeker/one.py</code> <pre><code>class IntelligenceRingEmbeddings:\n    name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n    clip_name: str = \"openai/clip-vit-base-patch32\"\n    wav2vec_name: str = \"facebook/wav2vec2-base-960h\"\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    vector_size: int = 768\n    tokenizer: Any | None = None\n    text_model: Any | None = None\n\n    clip_processor: Any | None = None\n    clip_model: Any | None = None\n\n    audio_processor: Any | None = None\n    audio_model: Any | None = None\n\n    text_projection: Any | None = None\n    image_projection: Any | None = None\n    audio_projection: Any | None = None\n\n    def __init__(self, **kwargs):\n\n        super().__init__(**kwargs)\n        self._ndims = self.vector_size\n\n        # Text embedding model\n        self.tokenizer = AutoTokenizer.from_pretrained(self.name)\n        self.text_model = AutoModel.from_pretrained(self.name).to(self.device)\n\n        # Image embedding model (CLIP)\n        self.clip_processor = CLIPProcessor.from_pretrained(self.clip_name)\n        self.clip_model = CLIPModel.from_pretrained(self.clip_name).to(self.device)\n\n        # Audio embedding model (Wav2Vec2)\n        self.audio_processor = Wav2Vec2Processor.from_pretrained(self.wav2vec_name)\n        self.audio_model = Wav2Vec2Model.from_pretrained(self.wav2vec_name).to(self.device)\n\n        # Projection layers to align dimensions\n        self.text_projection = torch.nn.Linear(\n            self.text_model.config.hidden_size,\n            self.vector_size\n        ).to(self.device)\n        self.image_projection = torch.nn.Linear(\n            self.clip_model.config.vision_config.hidden_size,\n            self.vector_size\n        ).to(self.device)\n        self.audio_projection = torch.nn.Linear(\n            self.audio_model.config.hidden_size,\n            self.vector_size\n        ).to(self.device)\n\n    def _process_text(self, text: str) -&gt; torch.Tensor:\n        encoded_input = self.tokenizer(\n            text,\n            padding=True,\n            truncation=True,\n            max_length=self.vector_size,\n            return_tensors='pt'\n        ).to(self.device)\n\n        with torch.no_grad():\n            outputs = self.text_model(**encoded_input)\n            embeddings = self._mean_pooling(outputs, encoded_input['attention_mask'])\n            projected = self.text_projection(embeddings)\n            return torch.nn.functional.normalize(projected, p=2, dim=1)\n\n    def _process_image(self, image_data: bytes | str) -&gt; torch.Tensor:\n        # Handle different image input types\n        if isinstance(image_data, str):\n            if image_data.startswith('data:image'):\n                # Handle base64 encoded images\n                image_data = base64.b64decode(image_data.split(',')[1])\n            else:\n                # Handle file paths\n                with open(image_data, 'rb') as f:\n                    image_data = f.read()\n\n        # Convert bytes to PIL Image\n        image = Image.open(io.BytesIO(image_data))\n\n        # Process image with CLIP\n        inputs = self.clip_processor(images=image, return_tensors=\"pt\").to(self.device)\n\n        with torch.no_grad():\n            outputs = self.clip_model.get_image_features(**inputs)\n            projected = self.image_projection(outputs)\n            return torch.nn.functional.normalize(projected, p=2, dim=1)\n\n    def _process_audio(self, audio_data: bytes | str | np.ndarray) -&gt; torch.Tensor:\n        try:\n            import torchaudio\n        except ImportError:\n            raise ValueError(\"Couldn't load audio install torchaudio'\")\n        # Handle different audio input types\n        if isinstance(audio_data, str):\n            if audio_data.startswith('data:audio'):\n                # Handle base64 encoded audio\n                audio_data = base64.b64decode(audio_data.split(',')[1])\n                waveform, sample_rate = torchaudio.load(io.BytesIO(audio_data))\n            else:\n                # Handle file paths\n                waveform, sample_rate = torchaudio.load(audio_data)\n        elif isinstance(audio_data, bytes):\n            waveform, sample_rate = torchaudio.load(io.BytesIO(audio_data))\n        else:\n            # Assume numpy array with sample rate in metadata\n            waveform = torch.from_numpy(audio_data)\n            sample_rate = 16000  # Default sample rate\n\n        # Resample if necessary\n        if sample_rate != 16000:\n            resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n            waveform = resampler(waveform)\n\n        # Process audio with Wav2Vec2\n        inputs = self.audio_processor(waveform, sampling_rate=16000, return_tensors=\"pt\").to(self.device)\n\n        with torch.no_grad():\n            outputs = self.audio_model(**inputs)\n            # Mean pooling over time dimension\n            embeddings = outputs.last_hidden_state.mean(dim=1)\n            projected = self.audio_projection(embeddings)\n            return torch.nn.functional.normalize(projected, p=2, dim=1)\n\n    def _mean_pooling(self, model_output: torch.Tensor, attention_mask: torch.Tensor) -&gt; torch.Tensor:\n        token_embeddings = model_output[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n    def process_input(self, input_data: InputData) -&gt; np.ndarray:\n        if input_data.modality == \"text\":\n            embeddings = self._process_text(input_data.content)\n        elif input_data.modality == \"image\":\n            embeddings = self._process_image(input_data.content)\n        elif input_data.modality == \"audio\":\n            embeddings = self._process_audio(input_data.content)\n        else:\n            raise ValueError(f\"Unsupported modality: {input_data.modality}\")\n\n        return embeddings.cpu().numpy()\n\n    def compute_query_embeddings(self, query: str | bytes | np.ndarray, modality: str = \"text\") -&gt; list[\n        np.ndarray]:\n        \"\"\"Compute embeddings for query input\"\"\"\n        input_data = InputData(query, modality)\n        embedding = self.process_input(input_data)\n        return [embedding.squeeze()]\n\n    def compute_source_embeddings(self, sources: list[str | bytes | np.ndarray], modalities: list[str]) -&gt; list[\n        np.ndarray]:\n        \"\"\"Compute embeddings for source inputs\"\"\"\n        embeddings = []\n        for source, modality in zip(sources, modalities, strict=False):\n            input_data = InputData(source, modality)\n            embedding = self.process_input(input_data)\n            embeddings.append(embedding.squeeze())\n        return embeddings\n\n    def ndims(self) -&gt; int:\n        return self._ndims\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.one.IntelligenceRingEmbeddings.compute_query_embeddings","title":"<code>compute_query_embeddings(query, modality='text')</code>","text":"<p>Compute embeddings for query input</p> Source code in <code>toolboxv2/mods/TruthSeeker/one.py</code> <pre><code>def compute_query_embeddings(self, query: str | bytes | np.ndarray, modality: str = \"text\") -&gt; list[\n    np.ndarray]:\n    \"\"\"Compute embeddings for query input\"\"\"\n    input_data = InputData(query, modality)\n    embedding = self.process_input(input_data)\n    return [embedding.squeeze()]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.one.IntelligenceRingEmbeddings.compute_source_embeddings","title":"<code>compute_source_embeddings(sources, modalities)</code>","text":"<p>Compute embeddings for source inputs</p> Source code in <code>toolboxv2/mods/TruthSeeker/one.py</code> <pre><code>def compute_source_embeddings(self, sources: list[str | bytes | np.ndarray], modalities: list[str]) -&gt; list[\n    np.ndarray]:\n    \"\"\"Compute embeddings for source inputs\"\"\"\n    embeddings = []\n    for source, modality in zip(sources, modalities, strict=False):\n        input_data = InputData(source, modality)\n        embedding = self.process_input(input_data)\n        embeddings.append(embedding.squeeze())\n    return embeddings\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests","title":"<code>tests</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker","title":"<code>TestTruthSeeker</code>","text":"<p>               Bases: <code>TestCase</code></p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>class TestTruthSeeker(unittest.TestCase):\n    def setUp(self):\n        # Mock the App class\n        self.mock_app = Mock()\n        self.mock_app.get_mod.return_value = Mock()\n\n        # Setup mock for run_any that returns iterable dict\n        self.mock_app.run_any.return_value = {\n            \"1\": {\"name\": \"template1\"},\n            \"2\": {\"name\": \"template2\"}\n        }\n\n        # Mock RequestSession\n        self.mock_request = Mock()\n        self.mock_request.json = AsyncMock()\n\n    @patch('os.path.join')\n    @patch('builtins.open', create=True)\n    def test_start_initialization(self, mock_open, mock_join):\n        \"\"\"Test the start function initializes correctly\"\"\"\n        # Setup mock file handling\n        mock_file = Mock()\n        mock_file.read.return_value = \"test content\"\n        mock_open.return_value.__enter__.return_value = mock_file\n\n        # Call start function\n        start(self.mock_app)\n\n        # Verify app initialization calls\n        self.mock_app.get_mod.assert_called_with(\"CodeVerification\")\n        self.mock_app.run_any.assert_any_call((\"CodeVerification\", \"init_scope\"), scope=\"TruthSeeker\")\n        self.mock_app.run_any.assert_any_call((\"CodeVerification\", \"init_scope\"), scope=\"TruthSeeker-promo\")\n\n    @async_test\n    async def test_codes_valid_request(self):\n        \"\"\"Test the codes function with valid input\"\"\"\n        # Mock request data\n        test_data = {\n            \"query\": \"test query\",\n            \"depth\": \"Q\",\n            \"promoCode\": \"PROMO15\",\n            \"ontimeCode\": \"TEST123\"\n        }\n        self.mock_request.json.return_value = test_data\n\n        # Mock code verification\n        self.mock_app.run_any.return_value = {\n            \"template_name\": \"Promo15\",\n            \"usage_type\": \"one_time\"\n        }\n\n        result = await codes(self.mock_app, self.mock_request)\n\n        self.assertTrue(result['valid'])\n        self.assertIn('ontimeKey', result)\n        self.assertIn('ppc', result)\n\n    @async_test\n    async def test_codes_invalid_promo(self):\n        \"\"\"Test the codes function with invalid promo code\"\"\"\n        test_data = {\n            \"query\": \"test query\",\n            \"depth\": \"I\",\n            \"promoCode\": \"INVALID\",\n            \"ontimeCode\": \"TEST123\"\n        }\n        self.mock_request.json.return_value = test_data\n\n        # Mock invalid promo code verification\n        self.mock_app.run_any.return_value = None\n\n        result = await codes(self.mock_app, self.mock_request)\n\n        self.assertIn('ppc', result)\n        self.assertTrue(result['ppc']['price'] &gt; 0)\n\n    @async_test\n    async def test_process_valid_request(self):\n        \"\"\"Test the process function with valid input\"\"\"\n        test_data = {\n            \"query\": \"test query\",\n            \"depth\": \"Q\",\n            \"ontimeKey\": \"VALID_KEY\",\n            \"email\": \"test@example.com\"\n        }\n        self.mock_request.json.return_value = test_data\n\n        # Mock valid key verification\n        self.mock_app.run_any.return_value = {\n            \"template_name\": \"PROCESS\",\n            \"usage_type\": \"timed\",\n            \"uses_count\": 1\n        }\n\n        # Mock ArXivPDFProcessor\n        with patch('toolboxv2.mods.TruthSeeker.module.ArXivPDFProcessor') as mock_processor:\n            mock_insights = MagicMock()\n            mock_insights.is_true = \"True\"\n            mock_insights.summary = \"Test summary\"\n            mock_insights.key_point = \"Point1&gt;\\n\\n&lt;Point2\"\n\n            mock_processor.return_value.process.return_value = ([], mock_insights)\n\n            result = await process(self.mock_app, self.mock_request)\n\n            self.assertEqual(result['is_true'], \"True\")\n            self.assertEqual(result['summary'], \"Test summary\")\n\n    @async_test\n    async def test_process_invalid_key(self):\n        \"\"\"Test the process function with invalid key\"\"\"\n        test_data = {\n            \"query\": \"test query\",\n            \"depth\": \"Q\",\n            \"ontimeKey\": \"INVALID_KEY\",\n            \"email\": \"test@example.com\"\n        }\n        self.mock_request.json.return_value = test_data\n\n        # Mock invalid key verification\n        self.mock_app.run_any.return_value = None\n\n        result = await process(self.mock_app, self.mock_request)\n\n        self.assertEqual(result['summary'], \"INVALID QUERY\")\n        self.assertEqual(result['insights'], [])\n        self.assertEqual(result['papers'], [])\n\n    def test_byCode_functionality(self):\n        \"\"\"Test the byCode function\"\"\"\n        test_request = Mock()\n        test_request.json.return_value = [\"payKey\", \"codeClass\", \"ontimeKey\"]\n\n        result = byCode(self.mock_app, test_request)\n\n        self.assertEqual(result, {'code': 'code'})\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker.test_byCode_functionality","title":"<code>test_byCode_functionality()</code>","text":"<p>Test the byCode function</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>def test_byCode_functionality(self):\n    \"\"\"Test the byCode function\"\"\"\n    test_request = Mock()\n    test_request.json.return_value = [\"payKey\", \"codeClass\", \"ontimeKey\"]\n\n    result = byCode(self.mock_app, test_request)\n\n    self.assertEqual(result, {'code': 'code'})\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker.test_codes_invalid_promo","title":"<code>test_codes_invalid_promo()</code>  <code>async</code>","text":"<p>Test the codes function with invalid promo code</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@async_test\nasync def test_codes_invalid_promo(self):\n    \"\"\"Test the codes function with invalid promo code\"\"\"\n    test_data = {\n        \"query\": \"test query\",\n        \"depth\": \"I\",\n        \"promoCode\": \"INVALID\",\n        \"ontimeCode\": \"TEST123\"\n    }\n    self.mock_request.json.return_value = test_data\n\n    # Mock invalid promo code verification\n    self.mock_app.run_any.return_value = None\n\n    result = await codes(self.mock_app, self.mock_request)\n\n    self.assertIn('ppc', result)\n    self.assertTrue(result['ppc']['price'] &gt; 0)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker.test_codes_valid_request","title":"<code>test_codes_valid_request()</code>  <code>async</code>","text":"<p>Test the codes function with valid input</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@async_test\nasync def test_codes_valid_request(self):\n    \"\"\"Test the codes function with valid input\"\"\"\n    # Mock request data\n    test_data = {\n        \"query\": \"test query\",\n        \"depth\": \"Q\",\n        \"promoCode\": \"PROMO15\",\n        \"ontimeCode\": \"TEST123\"\n    }\n    self.mock_request.json.return_value = test_data\n\n    # Mock code verification\n    self.mock_app.run_any.return_value = {\n        \"template_name\": \"Promo15\",\n        \"usage_type\": \"one_time\"\n    }\n\n    result = await codes(self.mock_app, self.mock_request)\n\n    self.assertTrue(result['valid'])\n    self.assertIn('ontimeKey', result)\n    self.assertIn('ppc', result)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker.test_process_invalid_key","title":"<code>test_process_invalid_key()</code>  <code>async</code>","text":"<p>Test the process function with invalid key</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@async_test\nasync def test_process_invalid_key(self):\n    \"\"\"Test the process function with invalid key\"\"\"\n    test_data = {\n        \"query\": \"test query\",\n        \"depth\": \"Q\",\n        \"ontimeKey\": \"INVALID_KEY\",\n        \"email\": \"test@example.com\"\n    }\n    self.mock_request.json.return_value = test_data\n\n    # Mock invalid key verification\n    self.mock_app.run_any.return_value = None\n\n    result = await process(self.mock_app, self.mock_request)\n\n    self.assertEqual(result['summary'], \"INVALID QUERY\")\n    self.assertEqual(result['insights'], [])\n    self.assertEqual(result['papers'], [])\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker.test_process_valid_request","title":"<code>test_process_valid_request()</code>  <code>async</code>","text":"<p>Test the process function with valid input</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@async_test\nasync def test_process_valid_request(self):\n    \"\"\"Test the process function with valid input\"\"\"\n    test_data = {\n        \"query\": \"test query\",\n        \"depth\": \"Q\",\n        \"ontimeKey\": \"VALID_KEY\",\n        \"email\": \"test@example.com\"\n    }\n    self.mock_request.json.return_value = test_data\n\n    # Mock valid key verification\n    self.mock_app.run_any.return_value = {\n        \"template_name\": \"PROCESS\",\n        \"usage_type\": \"timed\",\n        \"uses_count\": 1\n    }\n\n    # Mock ArXivPDFProcessor\n    with patch('toolboxv2.mods.TruthSeeker.module.ArXivPDFProcessor') as mock_processor:\n        mock_insights = MagicMock()\n        mock_insights.is_true = \"True\"\n        mock_insights.summary = \"Test summary\"\n        mock_insights.key_point = \"Point1&gt;\\n\\n&lt;Point2\"\n\n        mock_processor.return_value.process.return_value = ([], mock_insights)\n\n        result = await process(self.mock_app, self.mock_request)\n\n        self.assertEqual(result['is_true'], \"True\")\n        self.assertEqual(result['summary'], \"Test summary\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.TestTruthSeeker.test_start_initialization","title":"<code>test_start_initialization(mock_open, mock_join)</code>","text":"<p>Test the start function initializes correctly</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@patch('os.path.join')\n@patch('builtins.open', create=True)\ndef test_start_initialization(self, mock_open, mock_join):\n    \"\"\"Test the start function initializes correctly\"\"\"\n    # Setup mock file handling\n    mock_file = Mock()\n    mock_file.read.return_value = \"test content\"\n    mock_open.return_value.__enter__.return_value = mock_file\n\n    # Call start function\n    start(self.mock_app)\n\n    # Verify app initialization calls\n    self.mock_app.get_mod.assert_called_with(\"CodeVerification\")\n    self.mock_app.run_any.assert_any_call((\"CodeVerification\", \"init_scope\"), scope=\"TruthSeeker\")\n    self.mock_app.run_any.assert_any_call((\"CodeVerification\", \"init_scope\"), scope=\"TruthSeeker-promo\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.run_all_tests","title":"<code>run_all_tests()</code>","text":"<p>Run all test classes</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@default_test\ndef run_all_tests():\n    \"\"\"Run all test classes\"\"\"\n    return run_test_suite()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.run_arxiv_processor_tests","title":"<code>run_arxiv_processor_tests(test_name=None)</code>","text":"<p>Run TestArXivPDFProcessor tests</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>def run_arxiv_processor_tests(test_name=None):\n    \"\"\"Run TestArXivPDFProcessor tests\"\"\"\n    return run_test_suite(TestArXivPDFProcessor, test_name)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.run_pdf_downloader_tests","title":"<code>run_pdf_downloader_tests(test_name=None)</code>","text":"<p>Run TestRobustPDFDownloader tests</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>def run_pdf_downloader_tests(test_name=None):\n    \"\"\"Run TestRobustPDFDownloader tests\"\"\"\n    return run_test_suite(TestRobustPDFDownloader, test_name)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.run_specific_test","title":"<code>run_specific_test(test_class, test_name)</code>","text":"<p>Run a specific test from a test class</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>def run_specific_test(test_class, test_name):\n    \"\"\"Run a specific test from a test class\"\"\"\n    return run_test_suite(test_class, test_name)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.run_test_suite","title":"<code>run_test_suite(test_class=None, test_name=None, verbosity=2)</code>","text":"<p>Run specific test class or test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_class</code> <p>The test class to run (optional)</p> <code>None</code> <code>test_name</code> <p>Specific test method name to run (optional)</p> <code>None</code> <code>verbosity</code> <p>Output detail level (default=2)</p> <code>2</code> <p>Returns:</p> Type Description <p>TestResult object</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>def run_test_suite(test_class=None, test_name=None, verbosity=2):\n    \"\"\"\n    Run specific test class or test case.\n\n    Args:\n        test_class: The test class to run (optional)\n        test_name: Specific test method name to run (optional)\n        verbosity: Output detail level (default=2)\n\n    Returns:\n        TestResult object\n    \"\"\"\n    loader = unittest.TestLoader()\n    suite = unittest.TestSuite()\n\n    if test_class and test_name:\n        # Run specific test method\n        suite.addTest(test_class(test_name))\n    elif test_class:\n        # Run all tests in the class\n        suite.addTests(loader.loadTestsFromTestCase(test_class))\n    else:\n        # Run all tests\n        suite.addTests(loader.loadTestsFromModule(sys.modules[__name__]))\n\n    runner = unittest.TextTestRunner(verbosity=verbosity)\n    return runner.run(suite)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.run_truth_seeker_tests","title":"<code>run_truth_seeker_tests(test_name=None)</code>","text":"<p>Run TestTruthSeeker tests</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>def run_truth_seeker_tests(test_name=None):\n    \"\"\"Run TestTruthSeeker tests\"\"\"\n    return run_test_suite(TestTruthSeeker, test_name)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.test_arxiv_search","title":"<code>test_arxiv_search()</code>","text":"<p>Run only ArXiv search tests</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@default_test\ndef test_arxiv_search():\n    \"\"\"Run only ArXiv search tests\"\"\"\n    return run_specific_test(\n        TestArXivPDFProcessor,\n        'test_search_and_process_papers'\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.test_pdf_download","title":"<code>test_pdf_download()</code>","text":"<p>Run only PDF download tests</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@default_test\ndef test_pdf_download():\n    \"\"\"Run only PDF download tests\"\"\"\n    return run_specific_test(\n        TestRobustPDFDownloader,\n        'test_download_pdf_success'\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.TruthSeeker.tests.test_truth_seeker","title":"<code>test_truth_seeker()</code>","text":"<p>Run only PDF download tests</p> Source code in <code>toolboxv2/mods/TruthSeeker/tests.py</code> <pre><code>@default_test\ndef test_truth_seeker():\n    \"\"\"Run only PDF download tests\"\"\"\n    return run_specific_test(\n        TestTruthSeeker,\n        'test_truth_seeker_success'\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.UltimateTTT","title":"<code>UltimateTTT</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.UltimateTTT.UltimateTTTGameEngine","title":"<code>UltimateTTTGameEngine</code>","text":"Source code in <code>toolboxv2/mods/UltimateTTT.py</code> <pre><code>class UltimateTTTGameEngine:  # Renamed for clarity\n    def __init__(self, game_state: GameState):\n        self.gs = game_state\n        self.size = game_state.config.grid_size\n\n    def _check_line_for_win(self, line: List[Union[CellState, BoardWinner]],\n                            symbol_to_check: Union[CellState, BoardWinner]) -&gt; bool:\n        if not line or line[0] == CellState.EMPTY or line[0] == BoardWinner.NONE:\n            return False\n        return all(cell == symbol_to_check for cell in line)\n\n    def _get_board_winner_symbol(self, board: List[List[Union[CellState, BoardWinner]]],\n                                 symbol_class: Union[type[CellState], type[BoardWinner]]) -&gt; Optional[\n        Union[CellState, BoardWinner]]:\n        symbols_to_try = [symbol_class.X, symbol_class.O]\n        for symbol in symbols_to_try:\n            # Rows\n            for r in range(self.size):\n                if self._check_line_for_win([board[r][c] for c in range(self.size)], symbol): return symbol\n            # Columns\n            for c in range(self.size):\n                if self._check_line_for_win([board[r][c] for r in range(self.size)], symbol): return symbol\n            # Diagonals\n            if self._check_line_for_win([board[i][i] for i in range(self.size)], symbol): return symbol\n            if self._check_line_for_win([board[i][self.size - 1 - i] for i in range(self.size)], symbol): return symbol\n        return None  # No winner\n\n    def _is_board_full(self, board: List[List[Union[CellState, BoardWinner]]],\n                       empty_value: Union[CellState, BoardWinner]) -&gt; bool:\n        return all(cell != empty_value for row in board for cell in row)\n\n    def _determine_local_board_result(self, global_r: int, global_c: int) -&gt; BoardWinner:\n        if self.gs.global_board_winners[global_r][global_c] != BoardWinner.NONE:\n            return self.gs.global_board_winners[global_r][global_c]\n\n        local_board_cells = self.gs.local_boards_state[global_r][global_c]\n        winner_symbol = self._get_board_winner_symbol(local_board_cells, CellState)\n        if winner_symbol:\n            return BoardWinner(winner_symbol.value)  # Convert CellState.X to BoardWinner.X\n        if self._is_board_full(local_board_cells, CellState.EMPTY):\n            return BoardWinner.DRAW\n        return BoardWinner.NONE\n\n    def _update_local_winner_and_check_global(self, global_r: int, global_c: int):\n        new_local_winner = self._determine_local_board_result(global_r, global_c)\n        if new_local_winner != BoardWinner.NONE and self.gs.global_board_winners[global_r][\n            global_c] == BoardWinner.NONE:\n            self.gs.global_board_winners[global_r][global_c] = new_local_winner\n            self._check_for_overall_game_end()\n\n    def _check_for_overall_game_end(self):\n        if self.gs.status == GameStatus.FINISHED: return\n\n        winner_board_symbol = self._get_board_winner_symbol(self.gs.global_board_winners, BoardWinner)\n        if winner_board_symbol:  # This is BoardWinner.X or BoardWinner.O\n            self.gs.overall_winner_symbol = PlayerSymbol(winner_board_symbol.value)  # Convert to PlayerSymbol\n            self.gs.status = GameStatus.FINISHED\n            return\n\n        if self._is_board_full(self.gs.global_board_winners, BoardWinner.NONE):\n            self.gs.is_draw = True\n            self.gs.status = GameStatus.FINISHED\n\n    def _determine_next_forced_board(self, last_move_local_r: int, last_move_local_c: int) -&gt; Optional[Tuple[int, int]]:\n        target_gr, target_gc = last_move_local_r, last_move_local_c\n\n        if self.gs.global_board_winners[target_gr][target_gc] == BoardWinner.NONE and \\\n            not self._is_local_board_full(self.gs.local_boards_state[target_gr][target_gc], CellState.EMPTY):\n            return (target_gr, target_gc)\n        return None  # Play anywhere valid\n\n    def _is_local_board_full(self, local_board_cells: List[List[CellState]], cell_type=CellState.EMPTY) -&gt; bool:\n        \"\"\"Checks if a specific local board (passed as a 2D list of CellState) is full.\"\"\"\n        for r in range(self.size):\n            for c in range(self.size):\n                if local_board_cells[r][c] == cell_type:\n                    return False\n        return True\n\n    def add_player(self, player_id: str, player_name: str,\n                   is_npc: bool = False, npc_difficulty: Optional[NPCDifficulty] = None) -&gt; bool:\n        if len(self.gs.players) &gt;= 2:\n            self.gs.last_error_message = \"Game is already full (2 players max).\"\n            return False\n\n        # Reconnect logic for existing player (human or NPC if that makes sense)\n        existing_player = self.gs.get_player_info(player_id)\n        if existing_player:\n            if not existing_player.is_connected:\n                existing_player.is_connected = True\n                # If NPC \"reconnects\", ensure its properties are correct (though unlikely scenario for NPC)\n                if is_npc:\n                    existing_player.is_npc = True\n                    existing_player.npc_difficulty = npc_difficulty\n                    existing_player.name = player_name  # Update name if it changed for NPC\n\n                self.gs.last_error_message = None\n                self.gs.updated_at = datetime.now(timezone.utc)\n\n                if len(self.gs.players) == 2 and all(p.is_connected for p in self.gs.players) and \\\n                    self.gs.status == GameStatus.WAITING_FOR_OPPONENT:  # Should not be waiting if NPC is P2\n                    self.gs.status = GameStatus.IN_PROGRESS\n                    player_x_info = next(p for p in self.gs.players if p.symbol == PlayerSymbol.X)\n                    self.gs.current_player_id = player_x_info.id\n                    self.gs.waiting_since = None\n                return True\n            else:  # Player ID exists and is already connected\n                self.gs.last_error_message = f\"Player with ID {player_id} is already in the game and connected.\"\n                return False\n\n        # Adding a new player\n        symbol = PlayerSymbol.X if not self.gs.players else PlayerSymbol.O\n\n        # Construct PlayerInfo with NPC details if applicable\n        player_info_data = {\n            \"id\": player_id,\n            \"symbol\": symbol,\n            \"name\": player_name,\n            \"is_connected\": True,  # NPCs are always \"connected\"\n            \"is_npc\": is_npc\n        }\n        if is_npc and npc_difficulty:\n            player_info_data[\"npc_difficulty\"] = npc_difficulty\n\n        new_player = PlayerInfo(**player_info_data)\n        self.gs.players.append(new_player)\n        self.gs.last_error_message = None\n\n        if len(self.gs.players) == 1:  # First player added\n            if self.gs.mode == GameMode.ONLINE:\n                self.gs.status = GameStatus.WAITING_FOR_OPPONENT\n                self.gs.current_player_id = player_id\n                self.gs.waiting_since = datetime.now(timezone.utc)\n            # For local mode with P1, we wait for P2 (human or NPC) to be added\n            # No status change yet, current_player_id not set until P2 joins\n\n        elif len(self.gs.players) == 2:  # Both players now present\n            self.gs.status = GameStatus.IN_PROGRESS\n            player_x_info = next(p for p in self.gs.players if p.symbol == PlayerSymbol.X)\n            self.gs.current_player_id = player_x_info.id  # X always starts\n            self.gs.next_forced_global_board = None\n            self.gs.waiting_since = None\n\n            # If the second player added is an NPC and it's their turn (e.g. P1 is human, P2 is NPC, P1 made a move)\n            # This specific logic is more for when make_move hands over to an NPC.\n            # Here, we just set up the game. X (P1) will make the first move.\n\n        self.gs.updated_at = datetime.now(timezone.utc)\n        return True\n\n    def make_move(self, move: Move) -&gt; bool:\n        self.gs.last_error_message = None\n\n        if self.gs.status != GameStatus.IN_PROGRESS:\n            self.gs.last_error_message = \"Game is not in progress.\"\n            return False\n\n        player_info = self.gs.get_player_info(move.player_id)\n        if not player_info or move.player_id != self.gs.current_player_id:\n            self.gs.last_error_message = \"Not your turn or invalid player.\"\n            return False\n\n        s = self.size\n        if not (0 &lt;= move.global_row &lt; s and 0 &lt;= move.global_col &lt; s and \\\n                0 &lt;= move.local_row &lt; s and 0 &lt;= move.local_col &lt; s):\n            self.gs.last_error_message = f\"Coordinates out of bounds for {s}x{s} grid.\"\n            return False\n\n        gr, gc, lr, lc = move.global_row, move.global_col, move.local_row, move.local_col\n\n        if self.gs.next_forced_global_board and (gr, gc) != self.gs.next_forced_global_board:\n            self.gs.last_error_message = f\"Must play in global board {self.gs.next_forced_global_board}.\"\n            return False\n\n        if self.gs.global_board_winners[gr][gc] != BoardWinner.NONE:\n            self.gs.last_error_message = f\"Local board ({gr},{gc}) is already decided.\"\n            return False\n        if self.gs.local_boards_state[gr][gc][lr][lc] != CellState.EMPTY:\n            self.gs.last_error_message = f\"Cell ({gr},{gc})-({lr},{lc}) is already empty.\"  # Should be 'not empty' or 'occupied'\n            # Correction:\n            self.gs.last_error_message = f\"Cell ({gr},{gc})-({lr},{lc}) is already occupied.\"\n            return False\n\n        self.gs.local_boards_state[gr][gc][lr][lc] = CellState(player_info.symbol.value)\n        self.gs.moves_history.append(move)\n\n        self._update_local_winner_and_check_global(gr, gc)\n\n        if self.gs.status == GameStatus.FINISHED:\n            self.gs.next_forced_global_board = None\n        else:\n            opponent_info = self.gs.get_opponent_info(self.gs.current_player_id)\n            self.gs.current_player_id = opponent_info.id\n            self.gs.next_forced_global_board = self._determine_next_forced_board(lr, lc)\n\n            if self.gs.next_forced_global_board is None:\n                is_any_move_possible = any(\n                    self.gs.global_board_winners[r_idx][c_idx] == BoardWinner.NONE and \\\n                    not self._is_local_board_full(self.gs.local_boards_state[r_idx][c_idx], CellState.EMPTY)\n                    for r_idx in range(s) for c_idx in range(s)\n                )\n                if not is_any_move_possible:\n                    self._check_for_overall_game_end()\n                    if self.gs.status != GameStatus.FINISHED:\n                        self.gs.is_draw = True\n                        self.gs.status = GameStatus.FINISHED\n\n        self.gs.updated_at = datetime.now(timezone.utc)\n        self.gs.last_made_move_coords = (move.global_row, move.global_col, move.local_row, move.local_col)\n\n        return True\n\n    def handle_player_disconnect(self, player_id: str):\n        player = self.gs.get_player_info(player_id)\n        app = get_app(GAME_NAME)  # Hol dir die App-Instanz\n        if player:\n            if not player.is_connected:  # Already marked as disconnected\n                app.logger.info(f\"Player {player_id} was already marked as disconnected from game {self.gs.game_id}.\")\n                return\n\n            player.is_connected = False\n            self.gs.updated_at = datetime.now(timezone.utc)\n            app.logger.info(f\"Player {player_id} disconnected from game {self.gs.game_id}. Name: {player.name}\")\n\n            if self.gs.mode == GameMode.ONLINE:\n                if self.gs.status == GameStatus.IN_PROGRESS:\n                    opponent = self.gs.get_opponent_info(player_id)\n                    if opponent and opponent.is_connected:\n                        self.gs.status = GameStatus.ABORTED  # Use ABORTED as \"paused\"\n                        self.gs.player_who_paused = player_id  # Store who disconnected\n                        # This message is for the game state, will be seen by the other player via SSE\n                        self.gs.last_error_message = f\"Player {player.name} disconnected. Waiting for them to rejoin.\"\n                        app.logger.info(\n                            f\"Game {self.gs.game_id} PAUSED, waiting for {player.name} ({player_id}) to reconnect.\")\n                    else:\n                        # Opponent also disconnected or was already gone\n                        self.gs.status = GameStatus.ABORTED\n                        self.gs.last_error_message = \"Both players disconnected. Game aborted.\"\n                        self.gs.player_who_paused = None  # No specific player to wait for\n                        app.logger.info(\n                            f\"Game {self.gs.game_id} ABORTED, both players (or last active player) disconnected.\")\n                elif self.gs.status == GameStatus.WAITING_FOR_OPPONENT:\n                    # If the creator (P1) disconnects while waiting for P2\n                    if len(self.gs.players) == 1 and self.gs.players[0].id == player_id:\n                        self.gs.status = GameStatus.ABORTED\n                        self.gs.last_error_message = \"Game creator disconnected before opponent joined. Game aborted.\"\n                        self.gs.player_who_paused = None\n                        app.logger.info(\n                            f\"Game {self.gs.game_id} ABORTED, creator {player.name} ({player_id}) disconnected while WAITING_FOR_OPPONENT.\")\n                elif self.gs.status == GameStatus.ABORTED and self.gs.player_who_paused:\n                    # Game was already paused (e.g. P1 disconnected), and now P2 (the waiting one) disconnects\n                    if self.gs.player_who_paused != player_id:  # Ensure it's the other player\n                        self.gs.last_error_message = \"Other player also disconnected during pause. Game aborted.\"\n                        self.gs.player_who_paused = None  # No one specific to wait for now\n                        app.logger.info(\n                            f\"Game {self.gs.game_id} ABORTED, waiting player {player.name} ({player_id}) disconnected.\")\n\n    def handle_player_reconnect(self, player_id: str) -&gt; bool:\n        player = self.gs.get_player_info(player_id)\n        app = get_app(GAME_NAME)\n        if not player:\n            app.logger.warning(f\"Reconnect attempt for unknown player {player_id} in game {self.gs.game_id}.\")\n            return False\n\n        if player.is_connected:\n            app.logger.info(\n                f\"Player {player.name} ({player_id}) attempted reconnect but was already marked as connected to game {self.gs.game_id}.\")\n            if self.gs.status == GameStatus.ABORTED and self.gs.player_who_paused == player_id:\n                opponent = self.gs.get_opponent_info(player_id)\n                if opponent and opponent.is_connected:\n                    self.gs.status = GameStatus.IN_PROGRESS\n                    self.gs.last_error_message = f\"Connection for {player.name} re-established. Game resumed.\"\n                    self.gs.player_who_paused = None\n                    self.gs.updated_at = datetime.now(timezone.utc)\n                    app.logger.info(\n                        f\"Game {self.gs.game_id} resumed as already-connected pauser {player.name} re-interacted.\")\n                else:\n                    self.gs.last_error_message = f\"Welcome back, {player.name}! Your opponent is still not connected.\"\n            return True\n\n        player.is_connected = True\n        self.gs.updated_at = datetime.now(timezone.utc)\n        app.logger.info(\n            f\"Player {player.name} ({player_id}) reconnected to game {self.gs.game_id}. Previous status: {self.gs.status}, Paused by: {self.gs.player_who_paused}\")\n\n        if self.gs.status == GameStatus.ABORTED:\n            if self.gs.player_who_paused == player_id:  # The player who caused the pause has reconnected\n                opponent = self.gs.get_opponent_info(player_id)\n                if opponent and opponent.is_connected:\n                    self.gs.status = GameStatus.IN_PROGRESS\n                    self.gs.last_error_message = f\"Player {player.name} reconnected. Game resumed!\"\n                    self.gs.player_who_paused = None\n                    app.logger.info(\n                        f\"Game {self.gs.game_id} RESUMED. Pauser {player.name} reconnected, opponent {opponent.name} is present.\")\n                else:  # Pauser reconnected, opponent (still) gone or never joined (if P1 disconnected from WAITING)\n                    if not opponent and len(\n                        self.gs.players) == 1:  # P1 reconnected to a game they created but no P2 yet\n                        self.gs.status = GameStatus.WAITING_FOR_OPPONENT\n                        self.gs.player_who_paused = None\n                        self.gs.current_player_id = player_id\n                        self.gs.last_error_message = f\"Creator {player.name} reconnected. Waiting for opponent.\"\n                        self.gs.waiting_since = datetime.now(timezone.utc)  # Reset waiting timer\n                    elif opponent:  # Opponent was there but is now disconnected\n                        self.gs.player_who_paused = opponent.id  # Now waiting for the other person\n                        self.gs.last_error_message = f\"Welcome back, {player.name}! Your opponent ({opponent.name}) is not connected. Game remains paused.\"\n                        app.logger.info(\n                            f\"Game {self.gs.game_id} still PAUSED. {player.name} reconnected, but opponent {opponent.name} is NOT. Waiting for {opponent.name}.\")\n                    else:  # Should be rare: 2 players in list, but opponent object not found for P1\n                        self.gs.last_error_message = f\"Welcome back, {player.name}! Opponent details unclear. Game remains paused.\"\n\n\n            elif self.gs.player_who_paused and self.gs.player_who_paused != player_id:\n                # The *other* player reconnected, while game was paused for initial pauser.\n                initial_pauser_info = self.gs.get_player_info(self.gs.player_who_paused)\n                if initial_pauser_info and initial_pauser_info.is_connected:  # This implies both are now connected.\n                    self.gs.status = GameStatus.IN_PROGRESS\n                    self.gs.last_error_message = f\"Both players are now connected. Game resumed!\"\n                    self.gs.player_who_paused = None\n                    app.logger.info(\n                        f\"Game {self.gs.game_id} RESUMED. Waiting player {player.name} reconnected, initial pauser {initial_pauser_info.name} also present.\")\n                else:\n                    self.gs.last_error_message = f\"Welcome back, {player.name}! Still waiting for {initial_pauser_info.name if initial_pauser_info else 'the other player'} to reconnect.\"\n                    app.logger.info(\n                        f\"Game {self.gs.game_id} still PAUSED. Player {player.name} reconnected, but still waiting for original pauser {self.gs.player_who_paused}.\")\n\n            else:  # game is ABORTED but no specific player_who_paused (hard abort by timeout or both disconnected)\n                if len(self.gs.players) == 2:  # Was a two-player game\n                    opponent = self.gs.get_opponent_info(player_id)\n                    if opponent:\n                        # Revive the game to a paused state, waiting for the other player\n                        self.gs.player_who_paused = opponent.id\n                        self.gs.status = GameStatus.ABORTED  # Remains aborted, but now specifically for opponent\n                        self.gs.last_error_message = f\"Welcome back, {player.name}! Game was fully aborted. Now waiting for {opponent.name} to rejoin.\"\n                        app.logger.info(\n                            f\"Game {self.gs.game_id} REVIVED from HARD ABORT by {player.name}. Now paused, waiting for {opponent.name} ({opponent.id}).\")\n                    else:  # Should not happen if two players were in game and player_id is one of them\n                        self.gs.last_error_message = f\"Player {player.name} reconnected, but game state is inconsistent (opponent not found).\"\n                        app.logger.warning(\n                            f\"Game {self.gs.game_id} HARD ABORT revival by {player.name} failed, opponent info missing.\")\n                elif len(self.gs.players) == 1 and self.gs.players[0].id == player_id:\n                    # P1 created, P1 disconnected, game WAITING_FOR_OPPONENT timed out &amp; hard aborted. P1 tries to rejoin.\n                    self.gs.status = GameStatus.WAITING_FOR_OPPONENT\n                    self.gs.player_who_paused = None\n                    self.gs.current_player_id = player_id\n                    self.gs.last_error_message = f\"Creator {player.name} reconnected. Waiting for opponent.\"\n                    self.gs.waiting_since = datetime.now(timezone.utc)  # Reset waiting timer\n                    app.logger.info(\n                        f\"Game {self.gs.game_id} (previously hard aborted while waiting) revived by creator {player.name}. Now WAITING_FOR_OPPONENT.\")\n                else:\n                    self.gs.last_error_message = f\"Player {player.name} reconnected, but the game was aborted and cannot be revived in its current player configuration.\"\n                    app.logger.info(\n                        f\"Game {self.gs.game_id} HARD ABORTED. Player {player.name} reconnected, but game cannot resume in current configuration.\")\n\n\n        elif self.gs.status == GameStatus.IN_PROGRESS:\n            opponent = self.gs.get_opponent_info(player_id)\n            if not opponent or not opponent.is_connected:\n                self.gs.status = GameStatus.ABORTED\n                self.gs.player_who_paused = opponent.id if opponent else None\n                self.gs.last_error_message = f\"Welcome back, {player.name}! Your opponent disconnected while you were away. Waiting for them.\"\n                app.logger.info(\n                    f\"Game {self.gs.game_id} transitions to PAUSED. {player.name} reconnected to IN_PROGRESS, but opponent {opponent.id if opponent else 'N/A'} is gone.\")\n            else:\n                self.gs.last_error_message = f\"Player {player.name} re-established connection during active game.\"\n                app.logger.info(\n                    f\"Player {player.name} ({player_id}) re-established connection to IN_PROGRESS game {self.gs.game_id}.\")\n\n        elif self.gs.status == GameStatus.WAITING_FOR_OPPONENT:\n            if len(self.gs.players) == 1 and self.gs.players[0].id == player_id:\n                self.gs.last_error_message = f\"Creator {player.name} reconnected. Still waiting for opponent.\"\n                self.gs.current_player_id = player_id\n                self.gs.waiting_since = datetime.now(timezone.utc)  # Reset waiting timer\n                app.logger.info(\n                    f\"Creator {player.name} ({player_id}) reconnected to WAITING_FOR_OPPONENT game {self.gs.game_id}.\")\n            else:\n                app.logger.warning(\n                    f\"Non-creator {player.name} or unexpected player count for reconnect to WAITING_FOR_OPPONENT game {self.gs.game_id}.\")\n\n        return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager","title":"<code>WebSocketManager</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager","title":"<code>WebSocketPoolManager</code>","text":"Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>class WebSocketPoolManager:\n    def __init__(self):\n        self.pools: dict[str, dict[str, Any]] = {}\n        self.logger = logging.getLogger(__name__)\n\n    async def create_pool(self, pool_id: str) -&gt; None:\n        \"\"\"Create a new WebSocket pool.\"\"\"\n        if pool_id not in self.pools:\n            self.pools[pool_id] = {\n                'connections': {},\n                'actions': {},\n                'global_actions': {}\n            }\n            self.logger.info(f\"Created new pool: {pool_id}\")\n        else:\n            self.logger.warning(f\"Pool {pool_id} already exists\")\n\n    async def add_connection(self, pool_id: str, connection_id: str, websocket) -&gt; None:\n        \"\"\"Add a WebSocket connection to a pool.\"\"\"\n        if pool_id not in self.pools:\n            await self.create_pool(pool_id)\n\n        self.pools[pool_id]['connections'][connection_id] = websocket\n        self.logger.info(f\"Added connection {connection_id} to pool {pool_id}\")\n\n    async def remove_connection(self, pool_id: str, connection_id: str) -&gt; None:\n        \"\"\"Remove a WebSocket connection from a pool.\"\"\"\n        if pool_id in self.pools and connection_id in self.pools[pool_id]['connections']:\n            del self.pools[pool_id]['connections'][connection_id]\n            self.logger.info(f\"Removed connection {connection_id} from pool {pool_id}\")\n        else:\n            self.logger.warning(f\"Connection {connection_id} not found in pool {pool_id}\")\n\n    def register_action(self, pool_id: str, action_name: str, handler: Callable,\n                        connection_ids: list[str] = None) -&gt; None:\n        \"\"\"Register an action for specific connections or the entire pool.\"\"\"\n        if pool_id not in self.pools:\n            self.logger.error(f\"Pool {pool_id} does not exist\")\n            return\n\n        if connection_ids is None:\n            self.pools[pool_id]['global_actions'][action_name] = handler\n            self.logger.info(f\"Registered global action {action_name} for pool {pool_id}\")\n        else:\n            for conn_id in connection_ids:\n                if conn_id not in self.pools[pool_id]['actions']:\n                    self.pools[pool_id]['actions'][conn_id] = {}\n                self.pools[pool_id]['actions'][conn_id][action_name] = handler\n            self.logger.info(f\"Registered action {action_name} for connections {connection_ids} in pool {pool_id}\")\n\n    async def handle_message(self, pool_id: str, connection_id: str, message: str) -&gt; None:\n        \"\"\"Handle incoming messages and route them to the appropriate action handler.\"\"\"\n        if pool_id not in self.pools or connection_id not in self.pools[pool_id]['connections']:\n            self.logger.error(f\"Invalid pool_id or connection_id: {pool_id}, {connection_id}\")\n            return\n\n        try:\n            data = json.loads(message)\n            action = data.get('action')\n\n            if action:\n                if action in self.pools[pool_id]['global_actions']:\n                    await self.pools[pool_id]['global_actions'][action](pool_id, connection_id, data)\n                elif connection_id in self.pools[pool_id]['actions'] and action in self.pools[pool_id]['actions'][\n                    connection_id]:\n                    await self.pools[pool_id]['actions'][connection_id][action](pool_id, connection_id, data)\n                else:\n                    self.logger.warning(f\"No handler found for action {action} in pool {pool_id}\")\n            else:\n                self.logger.warning(f\"No action specified in message from {connection_id} in pool {pool_id}\")\n        except json.JSONDecodeError:\n            self.logger.error(f\"Invalid JSON received from {connection_id} in pool {pool_id}\")\n\n    async def broadcast(self, pool_id: str, message: str, exclude_connection_id: str = None) -&gt; None:\n        \"\"\"Broadcast a message to all connections in a pool, optionally excluding one connection.\"\"\"\n        if pool_id not in self.pools:\n            self.logger.error(f\"Pool {pool_id} does not exist\")\n            return\n\n        for conn_id, websocket in self.pools[pool_id]['connections'].items():\n            if conn_id != exclude_connection_id:\n                try:\n                    await websocket.send_text(message)\n                except Exception as e:\n                    self.logger.error(f\"Error sending message to {conn_id} in pool {pool_id}: {str(e)}\")\n\n    async def send_to_connection(self, pool_id: str, connection_id: str, message: str) -&gt; None:\n        \"\"\"Send a message to a specific connection in a pool.\"\"\"\n        if pool_id in self.pools and connection_id in self.pools[pool_id]['connections']:\n            try:\n                await self.pools[pool_id]['connections'][connection_id].send_text(message)\n            except Exception as e:\n                self.logger.error(f\"Error sending message to {connection_id} in pool {pool_id}: {str(e)}\")\n        else:\n            self.logger.error(f\"Connection {connection_id} not found in pool {pool_id}\")\n\n    def get_pool_connections(self, pool_id: str) -&gt; list[str]:\n        \"\"\"Get a list of all connection IDs in a pool.\"\"\"\n        if pool_id in self.pools:\n            return list(self.pools[pool_id]['connections'].keys())\n        else:\n            self.logger.error(f\"Pool {pool_id} does not exist\")\n            return []\n\n    def get_all_pools(self) -&gt; list[str]:\n        \"\"\"Get a list of all pool IDs.\"\"\"\n        return list(self.pools.keys())\n\n    async def close_pool(self, pool_id: str) -&gt; None:\n        \"\"\"Close all connections in a pool and remove the pool.\"\"\"\n        if pool_id in self.pools:\n            for websocket in self.pools[pool_id]['connections'].values():\n                await websocket.close()\n            del self.pools[pool_id]\n            self.logger.info(f\"Closed and removed pool {pool_id}\")\n        else:\n            self.logger.warning(f\"Pool {pool_id} does not exist\")\n\n    async def close_all_pools(self) -&gt; None:\n        \"\"\"Close all connections in all pools and remove all pools.\"\"\"\n        for pool_id in list(self.pools.keys()):\n            await self.close_pool(pool_id)\n        self.logger.info(\"Closed all pools\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.add_connection","title":"<code>add_connection(pool_id, connection_id, websocket)</code>  <code>async</code>","text":"<p>Add a WebSocket connection to a pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def add_connection(self, pool_id: str, connection_id: str, websocket) -&gt; None:\n    \"\"\"Add a WebSocket connection to a pool.\"\"\"\n    if pool_id not in self.pools:\n        await self.create_pool(pool_id)\n\n    self.pools[pool_id]['connections'][connection_id] = websocket\n    self.logger.info(f\"Added connection {connection_id} to pool {pool_id}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.broadcast","title":"<code>broadcast(pool_id, message, exclude_connection_id=None)</code>  <code>async</code>","text":"<p>Broadcast a message to all connections in a pool, optionally excluding one connection.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def broadcast(self, pool_id: str, message: str, exclude_connection_id: str = None) -&gt; None:\n    \"\"\"Broadcast a message to all connections in a pool, optionally excluding one connection.\"\"\"\n    if pool_id not in self.pools:\n        self.logger.error(f\"Pool {pool_id} does not exist\")\n        return\n\n    for conn_id, websocket in self.pools[pool_id]['connections'].items():\n        if conn_id != exclude_connection_id:\n            try:\n                await websocket.send_text(message)\n            except Exception as e:\n                self.logger.error(f\"Error sending message to {conn_id} in pool {pool_id}: {str(e)}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.close_all_pools","title":"<code>close_all_pools()</code>  <code>async</code>","text":"<p>Close all connections in all pools and remove all pools.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def close_all_pools(self) -&gt; None:\n    \"\"\"Close all connections in all pools and remove all pools.\"\"\"\n    for pool_id in list(self.pools.keys()):\n        await self.close_pool(pool_id)\n    self.logger.info(\"Closed all pools\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.close_pool","title":"<code>close_pool(pool_id)</code>  <code>async</code>","text":"<p>Close all connections in a pool and remove the pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def close_pool(self, pool_id: str) -&gt; None:\n    \"\"\"Close all connections in a pool and remove the pool.\"\"\"\n    if pool_id in self.pools:\n        for websocket in self.pools[pool_id]['connections'].values():\n            await websocket.close()\n        del self.pools[pool_id]\n        self.logger.info(f\"Closed and removed pool {pool_id}\")\n    else:\n        self.logger.warning(f\"Pool {pool_id} does not exist\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.create_pool","title":"<code>create_pool(pool_id)</code>  <code>async</code>","text":"<p>Create a new WebSocket pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def create_pool(self, pool_id: str) -&gt; None:\n    \"\"\"Create a new WebSocket pool.\"\"\"\n    if pool_id not in self.pools:\n        self.pools[pool_id] = {\n            'connections': {},\n            'actions': {},\n            'global_actions': {}\n        }\n        self.logger.info(f\"Created new pool: {pool_id}\")\n    else:\n        self.logger.warning(f\"Pool {pool_id} already exists\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.get_all_pools","title":"<code>get_all_pools()</code>","text":"<p>Get a list of all pool IDs.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>def get_all_pools(self) -&gt; list[str]:\n    \"\"\"Get a list of all pool IDs.\"\"\"\n    return list(self.pools.keys())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.get_pool_connections","title":"<code>get_pool_connections(pool_id)</code>","text":"<p>Get a list of all connection IDs in a pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>def get_pool_connections(self, pool_id: str) -&gt; list[str]:\n    \"\"\"Get a list of all connection IDs in a pool.\"\"\"\n    if pool_id in self.pools:\n        return list(self.pools[pool_id]['connections'].keys())\n    else:\n        self.logger.error(f\"Pool {pool_id} does not exist\")\n        return []\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.handle_message","title":"<code>handle_message(pool_id, connection_id, message)</code>  <code>async</code>","text":"<p>Handle incoming messages and route them to the appropriate action handler.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def handle_message(self, pool_id: str, connection_id: str, message: str) -&gt; None:\n    \"\"\"Handle incoming messages and route them to the appropriate action handler.\"\"\"\n    if pool_id not in self.pools or connection_id not in self.pools[pool_id]['connections']:\n        self.logger.error(f\"Invalid pool_id or connection_id: {pool_id}, {connection_id}\")\n        return\n\n    try:\n        data = json.loads(message)\n        action = data.get('action')\n\n        if action:\n            if action in self.pools[pool_id]['global_actions']:\n                await self.pools[pool_id]['global_actions'][action](pool_id, connection_id, data)\n            elif connection_id in self.pools[pool_id]['actions'] and action in self.pools[pool_id]['actions'][\n                connection_id]:\n                await self.pools[pool_id]['actions'][connection_id][action](pool_id, connection_id, data)\n            else:\n                self.logger.warning(f\"No handler found for action {action} in pool {pool_id}\")\n        else:\n            self.logger.warning(f\"No action specified in message from {connection_id} in pool {pool_id}\")\n    except json.JSONDecodeError:\n        self.logger.error(f\"Invalid JSON received from {connection_id} in pool {pool_id}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.register_action","title":"<code>register_action(pool_id, action_name, handler, connection_ids=None)</code>","text":"<p>Register an action for specific connections or the entire pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>def register_action(self, pool_id: str, action_name: str, handler: Callable,\n                    connection_ids: list[str] = None) -&gt; None:\n    \"\"\"Register an action for specific connections or the entire pool.\"\"\"\n    if pool_id not in self.pools:\n        self.logger.error(f\"Pool {pool_id} does not exist\")\n        return\n\n    if connection_ids is None:\n        self.pools[pool_id]['global_actions'][action_name] = handler\n        self.logger.info(f\"Registered global action {action_name} for pool {pool_id}\")\n    else:\n        for conn_id in connection_ids:\n            if conn_id not in self.pools[pool_id]['actions']:\n                self.pools[pool_id]['actions'][conn_id] = {}\n            self.pools[pool_id]['actions'][conn_id][action_name] = handler\n        self.logger.info(f\"Registered action {action_name} for connections {connection_ids} in pool {pool_id}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.remove_connection","title":"<code>remove_connection(pool_id, connection_id)</code>  <code>async</code>","text":"<p>Remove a WebSocket connection from a pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def remove_connection(self, pool_id: str, connection_id: str) -&gt; None:\n    \"\"\"Remove a WebSocket connection from a pool.\"\"\"\n    if pool_id in self.pools and connection_id in self.pools[pool_id]['connections']:\n        del self.pools[pool_id]['connections'][connection_id]\n        self.logger.info(f\"Removed connection {connection_id} from pool {pool_id}\")\n    else:\n        self.logger.warning(f\"Connection {connection_id} not found in pool {pool_id}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WebSocketManager.WebSocketPoolManager.send_to_connection","title":"<code>send_to_connection(pool_id, connection_id, message)</code>  <code>async</code>","text":"<p>Send a message to a specific connection in a pool.</p> Source code in <code>toolboxv2/mods/WebSocketManager.py</code> <pre><code>async def send_to_connection(self, pool_id: str, connection_id: str, message: str) -&gt; None:\n    \"\"\"Send a message to a specific connection in a pool.\"\"\"\n    if pool_id in self.pools and connection_id in self.pools[pool_id]['connections']:\n        try:\n            await self.pools[pool_id]['connections'][connection_id].send_text(message)\n        except Exception as e:\n            self.logger.error(f\"Error sending message to {connection_id} in pool {pool_id}: {str(e)}\")\n    else:\n        self.logger.error(f\"Connection {connection_id} not found in pool {pool_id}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb","title":"<code>WhatsAppTb</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client","title":"<code>client</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.DocumentSystem","title":"<code>DocumentSystem</code>","text":"Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>class DocumentSystem:\n    def __init__(self, storage: BlobStorage):\n        self.storage = storage\n        self.media_types = {\n            'document': ['pdf', 'doc', 'docx', 'txt'],\n            'image': ['jpg', 'jpeg', 'png', 'gif'],\n            'video': ['mp4', 'mov', 'avi']\n        }\n\n    def list_documents(self, filter_type: str = None) -&gt; list[dict]:\n        \"\"\"List all documents with metadata\"\"\"\n        docs = []\n        for blob_id in self.storage._get_all_blob_ids():\n            with BlobFile(blob_id, 'r', self.storage) as f:\n                metadata = f.read_json()\n                if metadata:\n                    docs.append({\n                        'id': blob_id,\n                        'name': metadata.get('filename', blob_id),\n                        'type': metadata.get('type', 'document'),\n                        'size': metadata.get('size', 0),\n                        'modified': metadata.get('timestamp', ''),\n                        'preview': metadata.get('preview', '')\n                    })\n        if filter_type:\n            return [d for d in docs if d['type'] == filter_type]\n        return docs\n\n    def save_document(self, file_data: bytes, filename: str, file_type: str) -&gt; str:\n        \"\"\"Save a document with metadata\"\"\"\n        blob_id = self.storage._generate_blob_id()\n        metadata = {\n            'filename': filename,\n            'type': file_type,\n            'size': len(file_data),\n            'timestamp': datetime.now().isoformat(),\n            'preview': self._generate_preview(file_data, file_type)\n        }\n\n        with BlobFile(blob_id, 'w', self.storage) as f:\n            f.write_json(metadata)\n            f.write(file_data)\n        return blob_id\n\n    def delete_document(self, blob_id: str) -&gt; bool:\n        \"\"\"Delete a document\"\"\"\n        try:\n            self.storage.delete_blob(blob_id)\n            return True\n        except Exception as e:\n            logging.error(f\"Delete failed: {str(e)}\")\n            return False\n\n    def search_documents(self, query: str) -&gt; list[dict]:\n        \"\"\"Search documents by filename or content\"\"\"\n        results = []\n        for doc in self.list_documents():\n            if query.lower() in doc['name'].lower() or self._search_in_content(doc['id'], query):\n                results.append(doc)\n        return results\n\n    def _generate_preview(self, data: bytes, file_type: str) -&gt; str:\n        \"\"\"Generate preview based on file type\"\"\"\n        if file_type in self.media_types['image']:\n            return f\"Image preview: {data[:100].hex()}\"\n        elif file_type in self.media_types['video']:\n            return \"Video preview unavailable\"\n        return data[:100].decode('utf-8', errors='ignore')\n\n    def _search_in_content(self, blob_id: str, query: str) -&gt; bool:\n        \"\"\"Search content within documents\"\"\"\n        try:\n            with BlobFile(blob_id, 'r', self.storage) as f:\n                content = f.read().decode('utf-8', errors='ignore')\n                return query.lower() in content.lower()\n        except:\n            return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.DocumentSystem.delete_document","title":"<code>delete_document(blob_id)</code>","text":"<p>Delete a document</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def delete_document(self, blob_id: str) -&gt; bool:\n    \"\"\"Delete a document\"\"\"\n    try:\n        self.storage.delete_blob(blob_id)\n        return True\n    except Exception as e:\n        logging.error(f\"Delete failed: {str(e)}\")\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.DocumentSystem.list_documents","title":"<code>list_documents(filter_type=None)</code>","text":"<p>List all documents with metadata</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def list_documents(self, filter_type: str = None) -&gt; list[dict]:\n    \"\"\"List all documents with metadata\"\"\"\n    docs = []\n    for blob_id in self.storage._get_all_blob_ids():\n        with BlobFile(blob_id, 'r', self.storage) as f:\n            metadata = f.read_json()\n            if metadata:\n                docs.append({\n                    'id': blob_id,\n                    'name': metadata.get('filename', blob_id),\n                    'type': metadata.get('type', 'document'),\n                    'size': metadata.get('size', 0),\n                    'modified': metadata.get('timestamp', ''),\n                    'preview': metadata.get('preview', '')\n                })\n    if filter_type:\n        return [d for d in docs if d['type'] == filter_type]\n    return docs\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.DocumentSystem.save_document","title":"<code>save_document(file_data, filename, file_type)</code>","text":"<p>Save a document with metadata</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def save_document(self, file_data: bytes, filename: str, file_type: str) -&gt; str:\n    \"\"\"Save a document with metadata\"\"\"\n    blob_id = self.storage._generate_blob_id()\n    metadata = {\n        'filename': filename,\n        'type': file_type,\n        'size': len(file_data),\n        'timestamp': datetime.now().isoformat(),\n        'preview': self._generate_preview(file_data, file_type)\n    }\n\n    with BlobFile(blob_id, 'w', self.storage) as f:\n        f.write_json(metadata)\n        f.write(file_data)\n    return blob_id\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.DocumentSystem.search_documents","title":"<code>search_documents(query)</code>","text":"<p>Search documents by filename or content</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def search_documents(self, query: str) -&gt; list[dict]:\n    \"\"\"Search documents by filename or content\"\"\"\n    results = []\n    for doc in self.list_documents():\n        if query.lower() in doc['name'].lower() or self._search_in_content(doc['id'], query):\n            results.append(doc)\n    return results\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant","title":"<code>WhatsAppAssistant</code>  <code>dataclass</code>","text":"Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>@dataclass\nclass WhatsAppAssistant:\n    whc: WhClient\n    isaa: 'Tools'\n    agent: Optional['Agent'] = None\n    credentials: Credentials | None = None\n    state: AssistantState = AssistantState.OFFLINE\n\n    # Service clients\n    gmail_service: Any = None\n    calendar_service: Any = None\n\n    start_time: Any = None\n\n    blob_docs_system: Any = None\n    duration_minutes: int = 20\n    credentials_path: str = \"/root/Toolboxv2/credentials.json\"\n    # Progress messengers\n    progress_messengers: dict[str, 'ProgressMessenger'] = field(default_factory=dict)\n    buttons: dict[str, dict] = field(default_factory=dict)\n    history: FileCache = field(default_factory=FileCache)\n\n    pending_actions: dict[str, dict] = field(default_factory=dict)\n\n\n    def __post_init__(self):\n\n        self.start_time = datetime.now()\n        self.processed_messages = set()\n        self.message_lock = threading.Lock()\n        self.audio_processor = None\n        self.blob_docs_system = DocumentSystem(BlobStorage())\n        self.stt = get_app().run_any(TBEF.AUDIO.STT_GENERATE,\n                                     model=\"openai/whisper-small\",\n                                     row=False, device=1)\n\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n\n        self.load_credentials()\n        self.setup_progress_messengers()\n        self.setup_interaction_buttons()\n        self.history = FileCache(folder=\".data/WhatsAppAssistant\")\n        self.state = AssistantState.ONLINE\n\n    async def generate_authorization_url(self, *a):\n        \"\"\"\n        Generate an authorization URL for user consent\n\n        :return: Authorization URL for the user to click and authorize access\n        \"\"\"\n        from google_auth_oauthlib.flow import Flow\n        # Define the scopes required for Gmail and Calendar\n        SCOPES = [\n            'https://www.googleapis.com/auth/gmail.modify',\n            'https://www.googleapis.com/auth/calendar'\n        ]\n\n        # Create a flow instance to manage the OAuth 2.0 authorization process\n        flow = Flow.from_client_secrets_file(\n            self.credentials_path,\n            scopes=SCOPES,\n            redirect_uri='urn:ietf:wg:oauth:2.0:oob'  # Use 'urn:ietf:wg:oauth:2.0:oob' for desktop apps\n        )\n\n        # Generate the authorization URL\n        authorization_url, _ = flow.authorization_url(\n            access_type='offline',  # Allows obtaining refresh token\n            prompt='consent'  # Ensures user is always prompted for consent\n        )\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {'type': 'auth',\n                                                                              'step': 'awaiting_key'}\n        return {\n            'type': 'quick_reply',\n            'text': f'Url to log in {authorization_url}',\n            'options': {'cancel': '\u274c Cancel Upload'}\n        }\n\n    def complete_authorization(self, message: Message):\n        \"\"\"\n        Complete the authorization process using the authorization code\n\n        :param authorization_code: Authorization code received from Google\n        \"\"\"\n        from google_auth_oauthlib.flow import Flow\n        authorization_code = message.content\n        # Define the scopes required for Gmail and Calendar\n        SCOPES = [\n            'https://www.googleapis.com/auth/gmail.modify',\n            'https://www.googleapis.com/auth/calendar'\n        ]\n\n        # Create a flow instance to manage the OAuth 2.0 authorization process\n        flow = Flow.from_client_secrets_file(\n            self.credentials_path,\n            scopes=SCOPES,\n            redirect_uri='urn:ietf:wg:oauth:2.0:oob'\n        )\n\n        # Exchange the authorization code for credentials\n        flow.fetch_token(code=authorization_code)\n        self.credentials = flow.credentials\n\n        # Save the credentials for future use\n        self.save_credentials()\n\n        # Initialize services\n        self.init_services()\n        return \"Done\"\n\n\n    def save_credentials(self):\n        \"\"\"\n        Save the obtained credentials to a file for future use\n        \"\"\"\n        if not os.path.exists('token'):\n            os.makedirs('token')\n\n        with open('token/google_token.json', 'w') as token_file:\n            token_file.write(self.credentials.to_json())\n\n\n    def load_credentials(self):\n        \"\"\"\n        Load previously saved credentials if available\n\n        :return: Whether credentials were successfully loaded\n        \"\"\"\n        try:\n            self.credentials = Credentials.from_authorized_user_file('token/google_token.json')\n            self.init_services()\n            return True\n        except FileNotFoundError:\n            return False\n\n\n    def init_services(self):\n        \"\"\"\n        Initialize Gmail and Calendar services\n        \"\"\"\n        from googleapiclient.discovery import build\n\n        self.gmail_service = build('gmail', 'v1', credentials=self.credentials)\n        self.calendar_service = build('calendar', 'v3', credentials=self.credentials)\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n\n    def setup_progress_messengers(self):\n        \"\"\"Initialize progress messengers for different types of tasks\"\"\"\n        self.progress_messengers = {\n            'task': self.whc.progress_messenger0,\n            'email': self.whc.progress_messenger1,\n            'calendar': self.whc.progress_messenger2\n        }\n\n    def setup_interaction_buttons(self):\n        \"\"\"Define WhatsApp interaction buttons for different functionalities\"\"\"\n        self.buttons = {\n            'menu': {\n                'header': 'Digital Assistant',\n                'body': 'Please select an option:',\n                'footer': '-- + --',\n                'action': {\n                    'button': 'Menu',\n                    'sections': [\n                        {\n                            'title': 'Main Functions',\n                            'rows': [\n                                {'id': 'agent', 'title': 'Agent Controls', 'description': 'Manage your AI assistant'},\n                                {'id': 'email', 'title': 'Email Management', 'description': 'Handle your emails'},\n                                {'id': 'calendar', 'title': 'Calendar', 'description': 'Manage your schedule'},\n                                {'id': 'docs', 'title': 'Documents', 'description': 'Handle documents'},\n                                {'id': 'system', 'title': 'System', 'description': 'System controls and metrics'}\n                            ]\n                        }\n                    ]\n                }\n            },\n            'agent': self._create_agent_controls_buttons(),\n            'email': self._create_email_controls_buttons(),\n            'calendar': self._create_calendar_controls_buttons(),\n            'docs': self._create_docs_controls_buttons(),\n            'system': self._create_system_controls_buttons()\n        }\n\n    @staticmethod\n    def _create_agent_controls_buttons():\n        return {\n            'header': 'Agent Controls',\n            'body': 'Manage your AI assistant:',\n            'action': {\n                'button': 'Select',\n                'sections': [\n                    {\n                        'title': 'Basic Actions',\n                        'rows': [\n                            {'id': 'agent-task', 'title': 'Agent Task', 'description': 'Run the agent'},\n                            {'id': 'start', 'title': 'Start Agent', 'description': 'Run taskstack in background'},\n                            {'id': 'stop', 'title': 'Stop Agent', 'description': 'Stop taskstack execution'}\n                        ]\n                    },\n                    {\n                        'title': 'Advanced Actions',\n                        'rows': [\n                            {'id': 'system-task', 'title': 'System Task',\n                             'description': 'Run the Isaa Reasoning Agent system'},\n                            {'id': 'tasks', 'title': 'Task Stack', 'description': 'View and manage tasks'},\n                            {'id': 'memory', 'title': 'Clear Memory', 'description': 'Reset agent memory'}\n                        ]\n                    }\n                ]\n            }\n        }\n\n    @staticmethod\n    def _create_email_controls_buttons():\n        return {\n            'header': 'Email Management',\n            'body': 'Handle your emails:',\n            'action': {\n                'button': 'Select',\n                'sections': [\n                    {\n                        'title': 'Basic Actions',\n                        'rows': [\n                            {'id': 'check', 'title': 'Check Emails', 'description': 'View recent emails'},\n                            {'id': 'send', 'title': 'Send Email', 'description': 'Compose new email'},\n                            {'id': 'summary', 'title': 'Get Summary', 'description': 'Summarize emails'}\n                        ]\n                    },\n                    {\n                        'title': 'Advanced Actions',\n                        'rows': [\n                            {'id': 'search', 'title': 'Search', 'description': 'Search emails'}\n                        ]\n                    }\n                ]\n            }\n        }\n\n    @staticmethod\n    def _create_calendar_controls_buttons():\n        return {\n            'header': 'Calendar Management',\n            'body': 'Manage your schedule:',\n            'action': {\n                'button': 'Select',\n                'sections': [\n                    {\n                        'title': 'Basic Actions',\n                        'rows': [\n                            {'id': 'today', 'title': 'Today\\'s Events', 'description': 'View today\\'s schedule'},\n                            {'id': 'add', 'title': 'Add Event', 'description': 'Create new event'},\n                            {'id': 'upcoming', 'title': 'Upcoming', 'description': 'View upcoming events'}\n                        ]\n                    },\n                    {\n                        'title': 'Advanced Actions',\n                        'rows': [\n                            {'id': 'find_slot', 'title': 'Find Time Slot', 'description': 'Find available time'}\n                        ]\n                    }\n                ]\n            }\n        }\n\n    @staticmethod\n    def _create_docs_controls_buttons():\n        return {\n            'header': 'Document Management',\n            'body': 'Handle your documents:',\n            'action': {\n                'button': 'Select',\n                'sections': [\n                    {\n                        'title': 'Basic Actions',\n                        'rows': [\n                            {'id': 'upload', 'title': 'Upload', 'description': 'Add new document'},\n                            {'id': 'list', 'title': 'List Documents', 'description': 'View all documents'},\n                            {'id': 'search', 'title': 'Search', 'description': 'Search documents'}\n                        ]\n                    },\n                    {\n                        'title': 'Advanced Actions',\n                        'rows': [\n                            {'id': 'delete', 'title': 'Delete', 'description': 'Remove document'}\n                        ]\n                    }\n                ]\n            }\n        }\n\n    @staticmethod\n    def _create_system_controls_buttons():\n        return {\n            'header': 'System Controls',\n            'body': 'System management:',\n            'action': {\n                'button': 'Select',\n                'sections': [\n                    {\n                        'title': 'Basic Actions',\n                        'rows': [\n                            {'id': 'status', 'title': 'System Status', 'description': 'View current status'},\n                            {'id': 'restart', 'title': 'Restart', 'description': 'Restart system'},\n                            {'id': 'connect', 'title': 'Connect', 'description': 'Connect to Google Calendar and Email'}\n                        ]\n                    }\n                ]\n            }\n        }\n\n    async def handle_message(self, message: 'Message'):\n        \"\"\"Main message handler for incoming WhatsApp messages\"\"\"\n\n        # Deduplication check\n        with self.message_lock:\n            if message.id in self.processed_messages:\n                return\n            last_ts = time.time()\n            print(last_ts)\n            if len(self.processed_messages) &gt; 0:\n                m_id, last_ts = self.processed_messages.pop()\n                self.processed_messages.add((m_id, last_ts))\n\n            print(\"DUPLICATION P\", message.data.get('entry', [{}])[0].get('changes', [{}])[0].get('value', {}).get('messages', [{}])[0].get('timestamp', 0) , last_ts)\n            if float(message.data.get('entry', [{}])[0].get('changes', [{}])[0].get('value', {}).get('messages', [{}])[0].get('timestamp', 0)) &lt; last_ts - 120:\n                return\n            self.processed_messages.add((message.id, time.perf_counter()))\n\n        # Mark message as read\n        message.mark_as_read()\n\n        # Extract content and type\n        content_type = message.type\n        content = message.content\n\n        print(f\"message.content {content=} {content_type=} {message.data=}\")\n\n        try:\n            if content_type == 'interactive':\n                await self.handle_interactive(message)\n            elif content_type == 'audio':\n                await self.handle_audio_message(message)\n            elif content_type in ['document', 'image', 'video']:\n                response = await self.handle_media_message(message)\n                self.save_reply(message, response)\n            elif content_type == 'text':\n                if content.lower() == \"menu\":\n                    self.whc.messenger.send_button(\n                        recipient_id=self.whc.progress_messenger0.recipient_phone,\n                        button=self.buttons[content.lower()]\n                    )\n                else:\n                    await self.helper_text(message)\n            else:\n                message.reply(\"Unsupported message type\")\n        #except Exception as e:\n        #    logging.error(f\"Message handling error: {str(e)}\")\n        #   message.reply(\"\u274c Error processing request\")\n        finally:\n            # Cleanup old messages (keep 1 hour history)\n            with self.message_lock:\n                self._clean_processed_messages()\n\n    async def helper_text(self, message: 'Message', return_text=False):\n        if not isinstance(message.content, str) and not len(message.content) &gt; 0:\n            content = self.whc.messenger.get_message(message.data)\n            print(f\"contents {content=}, {message.content=}\")\n            message.content = content\n        self.history.set(message.id, message.content)\n        if len(self.pending_actions[self.whc.progress_messenger0.recipient_phone].keys()) != 0:\n            message.reply(\n                f\"Open Interaction : {json.dumps(self.pending_actions[self.whc.progress_messenger0.recipient_phone], indent=2)}\")\n            if self.pending_actions[self.whc.progress_messenger0.recipient_phone].get('type') == 'auth':\n                res = self.complete_authorization(message)\n                self.save_reply(message, res)\n            res = await self.handle_calendar_actions(message)\n            if res:\n                self.save_reply(message, res)\n                return\n            res2 = await self.handle_email_actions(message)\n            if res2:\n                self.save_reply(message, res2)\n                return\n            await self.handle_agent_actions(message)\n            return\n        await self.handle_agent_actions(message)\n\n    async def handle_interactive(self, message: Message):\n        \"\"\"Handle all interactive messages\"\"\"\n        content = self.whc.messenger.get_interactive_response(message.data)\n        if content.get(\"type\") == \"list_reply\":\n            await self.handle_button_interaction(content.get(\"list_reply\"), message)\n        elif content.get(\"type\") == \"button_reply\":\n            print(content)\n\n    async def handle_audio_message(self, message: 'Message'):\n        \"\"\"Process audio messages with STT and TTS\"\"\"\n        # Download audio\n        progress = self.progress_messengers['task']\n        stop_flag = threading.Event()\n        # message_id = progress.send_initial_message(mode=\"loading\")\n        progress.message_id = message.id\n        progress.start_loading_in_background(stop_flag)\n\n        content = self.whc.messenger.get_audio(message.data)\n        audio_file_name = self.whc.messenger.download_media(media_url=self.whc.messenger.query_media_url(media_id=content.get('id')), mime_type='audio/opus', file_path=\".data/temp\")\n        print(f\"audio_file_name {audio_file_name}\")\n        if audio_file_name is None:\n            message.reply(\"Could not process audio file\")\n            stop_flag.set()\n            return\n\n        text = self.stt(audio_file_name)['text']\n        if not text:\n            message.reply(\"Could not process audio\")\n            stop_flag.set()\n            return\n\n        message.reply(\"Transcription :\\n \"+ text)\n        message.content = text\n        agent_res = await self.helper_text(message, return_text=True)\n\n        if agent_res is not None:\n            pass\n\n        stop_flag.set()\n        # Process text and get response\n        # response = await self.process_input(text, message)\n\n        # Convert response to audio\n        #audio_file = self.audio_processor.tts(response)\n        #audio_file = None # TODO\n        #self.whc.messenger.send_audio(\n        #    audio=audio_file,\n        #    recipient_id=self.whc.progress_messenger0.recipient_phone,\n        #)\n\n    async def confirm(self, message: Message):\n        status = self.pending_actions[self.whc.progress_messenger0.recipient_phone]\n        if status.get('type') == \"create_event\":\n            if status.get('step') == \"confirm_envet\":\n                event = self._create_calendar_event(status.get('event_data'))\n                self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n                return f\"\u2705 Event created!\\n{event.get('htmlLink')}\"\n            return \"\u274c\"\n        elif status.get('type') == \"compose_email\":\n            if status.get('step') == \"confirm_email\":\n                # Send email\n                result = self.gmail_service.users().messages().send(\n                    userId='me',\n                    body=self._build_email_draft(status['draft'])\n                ).execute()\n                self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n                return f\"\u2705 Email sent! Message ID: {result['id']}\"\n            return \"\u274c\"\n        return \"\u274c Done\"\n\n    async def cancel(self, *a):\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n        return \"\u2705 cancel Done\"\n\n    async def handle_button_interaction(self, content: dict, message: Message):\n        \"\"\"Handle button click interactions\"\"\"\n        button_id = content['id']\n\n        # First check if it's a main menu button\n        if button_id in self.buttons:\n            self.whc.messenger.send_button(\n                recipient_id=self.whc.progress_messenger0.recipient_phone,\n                button=self.buttons[button_id]\n            )\n            return\n\n        # Handle action buttons\n        action_handlers = {\n            # Agent controls\n            'start': self.start_agent,\n            'stop': self.stop_agent,\n            'tasks': self.show_task_stack,\n            'memory': self.clear_memory,\n            'system-task': self.system_task,\n            'agent-task': self.agent_task,\n\n            # Email controls\n            'check': self.check_emails,\n            'send': self.start_email_compose,\n            'summary': self.email_summary,\n            'search': self.email_search,\n\n            # Calendar controls\n            'today': self.show_today_events,\n            'add': self.start_event_create,\n            'upcoming': self.show_upcoming_events,\n            'find_slot': self.find_time_slot,\n\n            # Document controls\n            'upload': self.start_document_upload,\n            'list': self.list_documents,\n            'search_docs': self.search_documents,\n            'delete': self.delete_document,\n\n            # System controls\n            'status': self.system_status,\n            'restart': self.restart_system,\n            'connect': self.generate_authorization_url,\n\n            'cancel': self.cancel,\n            'confirm': self.confirm,\n        }\n        if button_id in action_handlers:\n            try:\n                # Start progress indicator\n                progress = self.progress_messengers['task']\n                stop_flag = threading.Event()\n                # message_id = progress.send_initial_message(mode=\"loading\")\n                progress.message_id = message.id\n                progress.start_loading_in_background(stop_flag)\n\n                # Execute handler\n\n                result = await action_handlers[button_id](message)\n\n\n                # Send result\n                if isinstance(result, str):\n                    self.save_reply(message, result)\n                elif isinstance(result, dict):  # For structured responses\n                    self.send_structured_response(result)\n\n                stop_flag.set()\n            finally:\n                #except Exception as e:\n                stop_flag.set()\n            #    message.reply(f\"\u274c Error processing {button_id}: {str(e)}\")\n        elif 'event_' in button_id:\n            res = await self.get_event_details(button_id.replace(\"event_\", ''))\n            if isinstance(res, str):\n                self.save_reply(message, res)\n                return\n            for r in res:\n                if isinstance(r, str):\n                    self.save_reply(message, r)\n                else:\n                    self.whc.messenger.send_location(**r)\n\n        elif 'email_' in button_id:\n            res = await self.get_email_details(button_id.replace(\"email_\", ''))\n            self.save_reply(message, res)\n        else:\n            message.reply(\"\u26a0\ufe0f Unknown command\")\n\n    def send_structured_response(self, result: dict):\n        \"\"\"Send complex responses using appropriate WhatsApp features\"\"\"\n        if result['type'] == 'list':\n            self.whc.messenger.send_button(\n                recipient_id=self.whc.progress_messenger0.recipient_phone,\n                button={\n                    'header': result.get('header', ''),\n                    'body': result.get('body', ''),\n                    'footer': result.get('footer', ''),\n                    'action': {\n                        'button': 'Action',\n                        'sections': result['sections']\n                    }\n                }\n            )\n        elif result['type'] == 'quick_reply':\n            self.whc.messenger.send_button(\n                recipient_id=self.whc.progress_messenger0.recipient_phone,\n                button={\n                    'header': \"Quick reply\",\n                    'body': result['text'],\n                    'footer': '',\n                    'action': {'button': 'Action', 'sections': [{\n                        'title': 'View',\n                        'rows': [{'id': k, 'title': v[:23]} for k, v in result['options'].items()]\n                    }]}\n                }\n            )\n\n        elif result['type'] == 'media':\n            if result['media_type'] == 'image':\n                self.whc.messenger.send_image(\n                    image=result['url'],\n                    recipient_id=self.whc.progress_messenger0.recipient_phone,\n                    caption=result.get('caption', '')\n                )\n            elif result['media_type'] == 'document':\n                self.whc.messenger.send_document(\n                    document=result['url'],\n                    recipient_id=self.whc.progress_messenger0.recipient_phone,\n                    caption=result.get('caption', '')\n                )\n\n    async def clear_memory(self, message):\n        self.agent.reset_context()\n        self.agent.taskstack.tasks = []\n        return \"\ud83e\udde0 Memory cleared successfully\"\n\n    async def system_task(self, message):\n        \"\"\"Initiate email search workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n            'type': 'system',\n            'step': 'await_query'\n        }\n        return {\n            'type': 'quick_reply',\n            'text': \"Now prompt the \ud83e\udde0ISAA-System \ud83d\udcdd\",\n            'options': {'cancel': '\u274c Cancel Search'}\n        }\n\n    async def agent_task(self, message):\n        \"\"\"Initiate email search workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n            'type': 'self-agent',\n            'step': 'await_query'\n        }\n        return {\n            'type': 'quick_reply',\n            'text': \"Now prompt the self-agent \ud83d\udcdd\",\n            'options': {'cancel': '\u274c Cancel Search'}\n        }\n\n    async def check_emails(self, message, query=\"\"):\n        \"\"\"Improved email checking with WhatsApp API formatting\"\"\"\n        if not self.gmail_service:\n            return \"\u26a0\ufe0f Gmail service not configured\"\n\n        try:\n            results = self.gmail_service.users().messages().list(\n                userId='me',\n                maxResults=10,\n                labelIds=['INBOX'],\n                q=query\n            ).execute()\n\n            emails = []\n            for msg in results.get('messages', [])[:10]:\n                email_data = self.gmail_service.users().messages().get(\n                    userId='me',\n                    id=msg['id'],\n                    format='metadata'\n                ).execute()\n\n                headers = {h['name']: h['value'] for h in email_data['payload']['headers']}\n                emails.append({\n                    'id': msg['id'],\n                    'from': headers.get('From', 'Unknown'),\n                    'subject': headers.get('Subject', 'No Subject'),\n                    'date': headers.get('Date', 'Unknown'),\n                    'snippet': email_data.get('snippet', ''),\n                    'unread': 'UNREAD' in email_data.get('labelIds', [])\n                })\n\n            return {\n                'type': 'list',\n                'header': '\ud83d\udce8 Recent Emails',\n                'body': 'Tap to view full email',\n                'footer': 'Email Manager',\n                'sections': [{\n                    'title': f\"Inbox ({len(emails)} emails)\",\n                    'rows': [{\n                        'id': f\"email_{email['id']}\",\n                        'title': f\"{'\ud83d\udcec' if email['unread'] else '\ud83d\udced'} {email['subject']}\"[:23],\n                        'description': f\"From: {email['from']}\\n{email['snippet']}\"[:45]\n                    } for email in emails]\n                }]\n            }\n        except Exception as e:\n            return f\"\u26a0\ufe0f Error fetching emails: {str(e)}\"\n\n    async def get_email_details(self, email_id):\n        \"\"\"Retrieve and format full email details\"\"\"\n        if not self.gmail_service:\n            return \"\u26a0\ufe0f Gmail service not configured\"\n\n        try:\n            email_data = self.gmail_service.users().messages().get(\n                userId='me',\n                id=email_id,\n                format='full'\n            ).execute()\n\n            headers = {h['name']: h['value'] for h in email_data['payload']['headers']}\n            body = \"\"\n            for part in email_data.get('payload', {}).get('parts', []):\n                if part['mimeType'] == 'text/plain':\n                    body = base64.urlsafe_b64decode(part['body']['data']).decode('utf-8')\n                    break\n\n            formatted_text = (\n                f\"\ud83d\udce7 *Email Details*\\n\\n\"\n                f\"From: {headers.get('From', 'Unknown')}\\n\"\n                f\"Subject: {headers.get('Subject', 'No Subject')}\\n\"\n                f\"Date: {headers.get('Date', 'Unknown')}\\n\\n\"\n                f\"{body[:15000]}{'...' if len(body) &gt; 15000 else ''}\"\n            )\n            return  self.agent.mini_task(\n                formatted_text , \"system\", \"Summarize the email in bullet points with key details\"\n            )\n        except Exception as e:\n            return f\"\u26a0\ufe0f Error fetching email: {str(e)}\"\n\n    async def email_summary(self, message):\n        \"\"\"Generate AI-powered email summaries\"\"\"\n        try:\n            messages = self.gmail_service.users().messages().list(\n                userId='me',\n                maxResults=3,\n                labelIds=['INBOX']\n            ).execute().get('messages', [])\n\n            email_contents = []\n            for msg in messages[:3]:\n                email_data = self.gmail_service.users().messages().get(\n                    userId='me',\n                    id=msg['id'],\n                    format='full'\n                ).execute()\n                email_contents.append(self._parse_email_content(email_data))\n\n            summary = self.agent.mini_task(\n                \"\\n\\n\".join(email_contents) , \"system\", \"Summarize these emails in bullet points with key details:\"\n            )\n\n            return f\"\ud83d\udccb Email Summary:\\n{summary}\\n\\n*Powered by AI*\"\n        except Exception as e:\n            logging.error(f\"Summary failed: {str(e)}\")\n            return f\"\u274c Could not generate summary: {str(e)}\"\n\n    async def email_search(self, message):\n        \"\"\"Initiate email search workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n            'type': 'email_search',\n            'step': 'await_query'\n        }\n        return {\n            'type': 'quick_reply',\n            'text': \"\ud83d\udd0d What would you like to search for?\",\n            'options': {'cancel': '\u274c Cancel Search'}\n        }\n\n    async def start_email_compose(self, message):\n        \"\"\"Enhanced email composition workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n            'type': 'compose_email',\n            'step': 'subject',\n            'draft': {'attachments': []}\n        }\n        return {\n            'type': 'quick_reply',\n            'text': \"\ud83d\udcdd Let's compose an email\\n\\nSubject:\",\n            'options': {'cancel': '\u274c Cancel Composition'}\n        }\n\n    async def handle_email_actions(self, message):\n        \"\"\"Handle multi-step email workflows\"\"\"\n        user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n\n        if user_state.get('type') == 'compose_email':\n            return await self._handle_email_composition(message, user_state)\n        if user_state.get('type') == 'email_search':\n            return await self.check_emails(message, self.agent.mini_task(\"\"\"Conventire Pezise zu einer googel str only query using : Gmail Suchoperatoren!\n\nBasis-Operatoren:\n- from: Absender\n- to: Empf\u00e4nger\n- subject: Betreff\n- label: Gmail Label\n- has:attachment Anh\u00e4nge\n- newer_than:7d Zeitfilter\n- before: Datum vor\n- after: Datum nach\n\nErweiterte Operatoren:\n- in:inbox\n- in:sent\n- in:spam\n- cc: Kopie\n- bcc: Blindkopie\n- is:unread\n- is:read\n- larger:10M Gr\u00f6\u00dfenfilter\n- smaller:5M\n- filename:pdf Dateityp\n\nProfi-Tipps:\n- Kombinierbar mit UND/ODER\n- Anf\u00fchrungszeichen f\u00fcr exakte Suche\n- Negation mit -\n beispeile : 'Ungelesene Mails letzte Woche': -&gt; 'is:unread newer_than:7d'\n\n\"\"\", \"user\",message.content))\n\n\n        return None\n\n    async def _handle_email_composition(self, message, state):\n        if state['step'] == 'subject':\n            state['draft']['subject'] = message.content\n            state['step'] = 'body'\n            return {\n                'type': 'quick_reply',\n                'text': \"\u270d\ufe0f Email body:\",\n                'options': {'attach': '\ud83d\udcce Add Attachment', 'send': '\ud83d\udce4 Send Now'}\n            }\n\n        elif state['step'] == 'body':\n            if message.content == 'attach':\n                state['step'] = 'attachment'\n                return \"\ud83d\udcce Please send the file you want to attach\"\n\n            state['draft']['body'] = message.content\n            state['step'] = 'confirm_email'\n            return {\n                'type': 'quick_reply',\n                'text': f\"\ud83d\udce7 Ready to send?\\n\\nSubject: {state['draft']['subject']}\\n\\n{state['draft']['body']}\",\n                'options': {'confirm': '\u2705 Send', 'cancel': '\u274c cancel'}\n            }\n\n        elif state['step'] == 'attachment':\n            # Handle attachment upload\n            file_type = message.type\n            if file_type not in ['document', 'image']:\n                return \"\u274c Unsupported file type\"\n\n            media_url = getattr(message, file_type).id\n            media_data = self.whc.messenger.download_media(media_url=self.whc.messenger.query_media_url(media_id=media_url), mime_type=media_url.type, file_path=\".data/temp\")\n            state['draft']['attachments'].append(media_data)\n            state['step'] = 'body'\n            return \"\ud83d\udcce Attachment added! Add more or send the email\"\n\n\n    def _parse_email_content(self, email_data):\n        \"\"\"Extract readable content from email payload\"\"\"\n        parts = email_data.get('payload', {}).get('parts', [])\n        body = \"\"\n        for part in parts:\n            if part['mimeType'] == 'text/plain':\n                body += base64.urlsafe_b64decode(part['body']['data']).decode('utf-8')\n        return f\"Subject: {email_data.get('subject', '')}\\nFrom: {email_data.get('from', '')}\\n\\n{body}\"\n\n    def _build_email_draft(self, draft):\n        \"\"\"Create MIME message from draft data\"\"\"\n        message = MIMEMultipart()\n        message['to'] = draft.get('to', '')\n        message['subject'] = draft['subject']\n        message.attach(MIMEText(draft['body']))\n\n        for attachment in draft['attachments']:\n            part = MIMEBase('application', 'octet-stream')\n            part.set_payload(attachment)\n            encoders.encode_base64(part)\n            part.add_header('Content-Disposition', 'attachment')\n            message.attach(part)\n\n        return {'raw': base64.urlsafe_b64encode(message.as_bytes()).decode()}\n\n    def _get_email_subject(self, msg):\n        headers = msg.get('payload', {}).get('headers', [])\n        return next((h['value'] for h in headers if h['name'] == 'Subject'), 'No Subject')\n\n    def _get_email_sender(self, msg):\n        headers = msg.get('payload', {}).get('headers', [])\n        return next((h['value'] for h in headers if h['name'] == 'From'), 'Unknown Sender')\n\n    def _get_email_snippet(self, msg):\n        return msg.get('snippet', '')[:100] + '...'\n    # Calendar Handlers\n\n    # Calendar Functions\n    def _format_event_time(self, event):\n        \"\"\"Improved time formatting for calendar events\"\"\"\n        start = event['start'].get('dateTime', event['start'].get('date'))\n        end = event['end'].get('dateTime', event['end'].get('date'))\n\n        try:\n            start_dt = parser.parse(start)\n            end_dt = parser.parse(end)\n            if 'T' in start:\n                return f\"{start_dt.strftime('%a %d %b %H:%M')} - {end_dt.strftime('%H:%M')}\"\n            return f\"{start_dt.strftime('%d %b %Y')} (All Day)\"\n        except:\n            return \"Time not specified\"\n\n    async def get_event_details(self, event_id):\n        \"\"\"Retrieve and format calendar event details with location support\"\"\"\n        if not self.calendar_service:\n            return \"\u26a0\ufe0f Calendar service not configured\"\n\n        try:\n            event = self.calendar_service.events().get(\n                calendarId='primary',\n                eventId=event_id\n            ).execute()\n\n            response = [ (\n                    f\"\ud83d\udcc5 *Event Details*\\n\\n\"\n                    f\"Title: {event.get('summary', 'No title')}\\n\"\n                    f\"Time: {self._format_event_time(event)}\\n\"\n                    f\"Location: {event.get('location', 'Not specified')}\\n\\n\"\n                    f\"{event.get('description', 'No description')[:1000]}\"\n                )]\n\n            if 'geo' in event:\n                response.append({\n                    'lat': float(event['geo']['latitude']),\n                    'long': float(event['geo']['longitude']),\n                    'name': event.get('location', 'Event Location'),\n                    'address': event.get('location', ''),\n                    'recipient_id': self.whc.progress_messenger0.recipient_phone\n                })\n            return response\n        except Exception as e:\n            return f\"\u26a0\ufe0f Error fetching event: {str(e)}\"\n\n    async def show_today_events(self, message):\n        \"\"\"Show today's calendar events\"\"\"\n        if not self.calendar_service:\n            message.replay(\"service not online\")\n\n        now = datetime.utcnow().isoformat() + 'Z'\n        end_of_day = (datetime.now() + timedelta(days=1)).replace(\n            hour=0, minute=0, second=0).isoformat() + 'Z'\n\n        events_result = self.calendar_service.events().list(\n            calendarId='primary',\n            timeMin=now,\n            timeMax=end_of_day,\n            singleEvents=True,\n            orderBy='startTime'\n        ).execute()\n\n        events = events_result.get('items', [])\n        return self._format_calendar_response(events, \"Today's Events\")\n\n    # Updated Calendar List Handlers\n    async def show_upcoming_events(self, message):\n        \"\"\"Show upcoming events with interactive support\"\"\"\n        if not self.calendar_service:\n            return \"\u26a0\ufe0f Calendar service not configured\"\n\n        try:\n            now = datetime.utcnow().isoformat() + 'Z'\n            next_week = (datetime.now() + timedelta(days=7)).isoformat() + 'Z'\n\n            events_result = self.calendar_service.events().list(\n                calendarId='primary',\n                timeMin=now,\n                timeMax=next_week,\n                singleEvents=True,\n                orderBy='startTime',\n                maxResults=10\n            ).execute()\n\n            events = events_result.get('items', [])\n            return self._format_calendar_response(events, \"Upcoming Events\")\n        except Exception as e:\n            return f\"\u26a0\ufe0f Error fetching events: {str(e)}\"\n\n    async def start_event_create(self, message):\n        \"\"\"Initiate event creation workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n            'type': 'create_event',\n            'step': 'title',\n            'event_data': {}\n        }\n        return {\n            'type': 'quick_reply',\n            'text': \"Let's create an event! What's the title?\",\n            'options': {'cancel': '\u274c Cancel'}\n        }\n\n    async def find_time_slot(self, message):\n        \"\"\"Find and display the next 5 available time slots with dynamic durations\"\"\"\n        if not self.calendar_service:\n            return \"\u26a0\ufe0f Calendar service not configured\"\n\n        try:\n            # Define the time range for the search (next 24 hours)\n            now = datetime.now(UTC)\n            end_time = now + timedelta(days=1)\n\n            # FreeBusy Request\n            freebusy_request = {\n                \"timeMin\": now.isoformat(),\n                \"timeMax\": end_time.isoformat(),\n                \"items\": [{\"id\": 'primary'}]\n            }\n\n            freebusy_response = self.calendar_service.freebusy().query(body=freebusy_request).execute()\n            busy_slots = freebusy_response['calendars']['primary']['busy']\n\n            # Slot-Berechnung\n            available_slots = self._calculate_efficient_slots(\n                busy_slots,\n                self.duration_minutes\n            )\n\n            # Format the response for WhatsApp\n            return {\n                'type': 'list',\n                'header': \"\u23f0 Available Time Slots\",\n                'body': \"Tap to select a time slot\",\n                'footer': \"Time Slot Finder\",\n                'sections': [{\n                    'title': \"Next 5 Available Slots\",\n                    'rows': [{\n                        'id': f\"slot_{slot['start'].timestamp()}\",\n                        'title': f\"\ud83d\udd52 {slot['start'].strftime('%H:%M')} - {slot['end'].strftime('%H:%M')}\",\n                        'description': f\"Duration: {slot['duration']}\"\n                    } for slot in available_slots[:5]]\n                }]\n            }\n        except Exception as e:\n            return f\"\u26a0\ufe0f Error finding time slots: {str(e)}\"\n\n    def _calculate_efficient_slots(self, busy_slots, duration_minutes):\n        \"\"\"Effiziente Slot-Berechnung\"\"\"\n        available_slots = []\n        current = datetime.now(UTC)\n        end_time = current + timedelta(days=1)\n\n        while current &lt; end_time:\n            slot_end = current + timedelta(minutes=duration_minutes)\n\n            if slot_end &gt; end_time:\n                break\n\n            is_available = all(\n                slot_end &lt;= parser.parse(busy['start']) or\n                current &gt;= parser.parse(busy['end'])\n                for busy in busy_slots\n            )\n\n            if is_available:\n                available_slots.append({\n                    'start': current,\n                    'end': slot_end,\n                    'duration': f\"{duration_minutes} min\"\n                })\n                current = slot_end\n            else:\n                current += timedelta(minutes=15)\n\n        return available_slots\n\n    async def handle_calendar_actions(self, message):\n        \"\"\"Handle calendar-related pending actions\"\"\"\n        user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n\n        if user_state.get('type') == 'create_event':\n            return await self._handle_event_creation(message, user_state)\n\n        return None\n\n    async def _handle_event_creation(self, message, state):\n        step = state['step']\n        event_data = state['event_data']\n\n        if step == 'title':\n            event_data['summary'] = message.content\n            state['step'] = 'start_time'\n            return \"\ud83d\udcc5 When should it start? (e.g., 'tomorrow 2pm' or '2024-03-20 14:30')\"\n\n        elif step == 'start_time':\n            event_data['start'] = self._parse_time(message.content)\n            state['step'] = 'end_time'\n            return \"\u23f0 When should it end? (e.g., '3pm' or '2024-03-20 15:30')\"\n\n        elif step == 'end_time':\n            event_data['end'] = self._parse_time(message.content, reference=event_data['start'])\n            state['step'] = 'description'\n            return \"\ud83d\udcdd Add a description (or type 'skip')\"\n\n        elif step == 'description':\n            if message.content.lower() != 'skip':\n                event_data['description'] = message.content\n            state['step'] = 'confirm_envet'\n            return self._create_confirmation_message(event_data)\n\n    def _format_calendar_response(self, events, title):\n        \"\"\"Enhanced calendar formatting with interactive support\"\"\"\n        if not events:\n            return f\"\ud83d\udcc5 No {title.lower()} found\"\n\n        return {\n            'type': 'list',\n            'header': title,\n            'body': \"Tap to view event details\",\n            \"footer\": \"-- Calendar --\",\n            'sections': [{\n                'title': f\"{len(events)} Events\",\n                'rows': [{\n                    'id': f\"event_{event['id']}\",\n                    'title': f\"\ud83d\udcc5 {event['summary']}\"[:23],\n                    'description': self._format_event_time(event)[:45]\n                } for event in events[:5]]\n            }]\n        }\n\n    def _parse_iso_to_readable(self, iso_str):\n        \"\"\"Convert ISO datetime to readable format\"\"\"\n        dt = datetime.fromisoformat(iso_str.replace('Z', '+00:00'))\n        return dt.strftime(\"%a %d %b %Y %H:%M\")\n\n    def _parse_time(self, time_str, reference=None):\n        \"\"\"\n        Konvertiert nat\u00fcrliche Sprache zu pr\u00e4ziser Datetime\n\n        Unterst\u00fctzt:\n        - 'heute'\n        - 'morgen'\n        - 'in einer woche'\n        - '10 uhr'\n        - '10pm'\n        - 'n\u00e4chsten montag'\n        \"\"\"\n        if reference is None:\n            reference = datetime.now()\n\n        try:\n            import dateparser\n\n            # Dateparser f\u00fcr flexibel Zeitparsing\n            parsed_time = dateparser.parse(\n                time_str,\n                settings={\n                    'PREFER_DATES_FROM': 'future',\n                    'RELATIVE_BASE': reference,\n                    'TIMEZONE': 'Europe/Berlin'\n                }\n            )\n\n            if parsed_time is None:\n                # Fallback auf dateutil wenn dateparser scheitert\n                parsed_time = parser .parse(time_str, fuzzy=True, default=reference)\n\n            return parsed_time\n\n        except Exception as e:\n            print(f\"Zeitparsing-Fehler: {e}\")\n            return reference\n\n    def _calculate_free_slots(self, start, end, busy_slots):\n        \"\"\"Calculate free time slots between busy periods\"\"\"\n        # Implementation would calculate available windows\n        return [{\n            'start': \"09:00\",\n            'end': \"11:00\",\n            'duration': \"2 hours\"\n        }]\n\n    def _create_confirmation_message(self, event_data):\n        \"\"\"Create event confirmation message\"\"\"\n        details = [\n            f\"\ud83d\udccc Title: {event_data['summary']}\",\n            f\"\ud83d\udd52 Start: {self._parse_iso_to_readable(event_data['start'])}\",\n            f\"\u23f0 End: {self._parse_iso_to_readable(event_data['end'])}\",\n            f\"\ud83d\udcdd Description: {event_data.get('description', 'None')}\"\n        ]\n        return {\n            'type': 'quick_reply',\n            'text': \"\\n\".join(details),\n            'options': {'confirm': '\u2705 Confirm', 'cancel': '\u274c Cancel'}\n        }\n\n    def _create_calendar_event(self, event_data):\n        \"\"\"Create event through Calendar API\"\"\"\n        event = {\n            'summary': event_data['summary'],\n            'start': {'dateTime': event_data['start']},\n            'end': {'dateTime': event_data['end']},\n        }\n        if 'description' in event_data:\n            event['description'] = event_data['description']\n\n        return self.calendar_service.events().insert(\n            calendarId='primary',\n            body=event\n        ).execute()\n\n    async def system_status(self, message):\n        o = (datetime.now() - self.start_time)\n        o.microseconds = 0\n        status = {\n            \"\ud83e\udd16 Agent\": \"Online\" if self.agent else \"Offline\",\n            \"\ud83d\udce7 Email\": \"Connected\" if self.gmail_service else \"Disconnected\",\n            \"\ud83d\udcc5 Calendar\": \"Connected\" if self.calendar_service else \"Disconnected\",\n            \"\ud83d\udcc4 Documents\": \"Connected\" if self.blob_docs_system else \"Disconnected\",\n            \"\u23f3 Uptime\": f\"{str(o.isoformat())}\"\n        }\n        return \"\\n\".join([f\"{k}: {v}\" for k, v in status.items()])\n\n    async def restart_system(self, message):\n        message.reply(\"\ud83d\udd04 System restart initiated...\")\n        time.sleep(1)\n        await self.clear_memory(message)\n        time.sleep(1)\n        return  \"\u2705 System restarted\"\n\n    # Updated document handlers\n    async def list_documents(self, message, filter_type=None):\n        docs = self.blob_docs_system.list_documents(filter_type)\n        if len(docs) == 0:\n            return \"No docs found\"\n        else:\n            return str(docs)\n        return {\n            'type': 'list',\n            'body': 'Stored Documents',\n            'action': {\n                'sections': [{\n                    'title': 'Your Documents',\n                    'rows': [{\n                        'id': doc['id'],\n                        'title': f\"{self._get_icon(doc['type'])} {doc['name']}\"[:23],\n                        'description': f\"{doc['type'].title()} | {self._format_size(doc['size'])} | {doc['modified']}\"[:29]\n                    } for doc in docs[:10]]\n                }]}\n        }\n\n    async def start_document_upload(self, message):\n        \"\"\"Initiate document upload workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {'type': 'document', 'step': 'awaiting_file'}\n        return {\n            'type': 'quick_reply',\n            'text': '\ud83d\udce4 Send me the file you want to upload',\n            'options': {'cancel': '\u274c Cancel Upload'}\n        }\n\n    async def search_documents(self, message):\n        \"\"\"Initiate document search workflow\"\"\"\n        self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {'type': 'search', 'step': 'awaiting_query'}\n        return {\n            'type': 'quick_reply',\n            'text': '\ud83d\udd0d What are you looking for?',\n            'options': {'cancel': '\u274c Cancel Search'}\n        }\n\n    async def handle_media_message(self, message: 'Message'):\n        \"\"\"Handle document/image/video uploads\"\"\"\n        user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n\n        if user_state.get('step') == 'awaiting_file':\n            file_type = message.type\n            if file_type not in ['document', 'image', 'video']:\n                return \"Unsupported file type\"\n\n            try:\n                # Download media\n                #media_url = message.document.url if hasattr(message, 'document') else \\\n                #    message.image.url if hasattr(message, 'image') else \\\n                #        message.video.url\n                if file_type =='video':\n                    content = self.whc.messenger.get_video(message.data)\n                if file_type =='image':\n                    content = self.whc.messenger.get_image(message.data)\n                if file_type =='document':\n                    content = self.whc.messenger.get_document(message.data)\n                print(\"Media content:\", content)\n                media_data = self.whc.messenger.download_media(media_url=self.whc.messenger.query_media_url(media_id=content.get('id')),  mime_type=content.get('mime_type'), file_path='.data/temp')\n                print(\"Media media_data:\", media_data)\n                # Save to blob storage\n                filename = f\"file_{file_type}_{datetime.now().isoformat()}_{content.get('sha256', '')}\"\n                blob_id = self.blob_docs_system.save_document(\n                    open(media_data, 'rb').read(),\n                    filename=filename,\n                    file_type=file_type\n                )\n\n                self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n                return f\"\u2705 File uploaded successfully!\\nID: {blob_id}\"\n\n            except Exception as e:\n                logging.error(f\"Upload failed: {str(e)}\")\n                return f\"\u274c Failed to upload file Error : {str(e)}\"\n\n        return \"No pending uploads\"\n\n    async def delete_document(self, message):\n        \"\"\"Delete document workflow\"\"\"\n        docs = self.blob_docs_system.list_documents()\n        return {\n            'type': 'quick_reply',\n            'text': 'Select document to delete:',\n            'options': {doc['id']: doc['name'] for doc in docs[:5]},\n            'handler': self._confirm_delete\n        }\n\n    async def _confirm_delete(self, doc_id, message):\n        \"\"\"Confirm deletion workflow\"\"\"\n        doc = next((d for d in self.blob_docs_system.list_documents() if d['id'] == doc_id), None)\n        if not doc:\n            return \"Document not found\"\n\n        if self.blob_docs_system.delete_document(doc_id):\n            return f\"\u2705 {doc['name']} deleted successfully\"\n        return \"\u274c Failed to delete document\"\n\n    # Helper methods\n    def _get_icon(self, file_type: str) -&gt; str:\n        icons = {\n            'document': '\ud83d\udcc4',\n            'image': '\ud83d\uddbc\ufe0f',\n            'video': '\ud83c\udfa5'\n        }\n        return icons.get(file_type, '\ud83d\udcc1')\n\n    def _format_size(self, size: int) -&gt; str:\n        if size &lt; 1024:\n            return f\"{size}B\"\n        elif size &lt; 1024 ** 2:\n            return f\"{size / 1024:.1f}KB\"\n        elif size &lt; 1024 ** 3:\n            return f\"{size / (1024 ** 2):.1f}MB\"\n        return f\"{size / (1024 ** 3):.1f}GB\"\n\n    # Utility Methods\n\n    def _clean_processed_messages(self):\n        \"\"\"Clean old messages from processed cache\"\"\"\n        now = time.time()\n        self.processed_messages = {\n            msg_id for msg_id, timestamp in self.processed_messages\n            if now - timestamp &lt; 3600  # 1 hour retention\n        }\n\n    def send_email(self, to, subject, body):\n        \"\"\"Actual email sending function to be called by agent\"\"\"\n        if not self.gmail_service:\n            return False\n\n        message = MIMEText(body)\n        message['to'] = to\n        message['subject'] = subject\n\n        encoded_message = base64.urlsafe_b64encode(message.as_bytes()).decode()\n        self.gmail_service.users().messages().send(\n            userId='me',\n            body={'raw': encoded_message}\n        ).execute()\n        return True\n\n    async def start_agent(self, *a):\n        \"\"\"Start the agent in background mode\"\"\"\n        if self.agent:\n            self.agent.run_in_background()\n            return True\n        return False\n\n    async def stop_agent(self, *b):\n        \"\"\"Stop the currently running agent\"\"\"\n        if self.agent:\n            self.agent.stop()\n            return True\n        return False\n\n    async def show_task_stack(self, *a):\n        \"\"\"Display current task stack\"\"\"\n        if self.agent and len(self.agent.taskstack.tasks) &gt; 0:\n            tasks = self.agent.taskstack.tasks\n            return self.agent.mini_task(\"\\n\".join([f\"Task {t.id}: {t.description}\" for t in tasks]), \"system\", \"Format to nice and clean whatsapp format\")\n        return \"No tasks in stack\"\n\n    def run(self):\n        \"\"\"Start the WhatsApp assistant\"\"\"\n        try:\n            self.state = AssistantState.ONLINE\n            # Send welcome message\n\n            mas = self.whc.messenger.create_message(\n                content=\"Digital Assistant is online! Send /help for available commands.\",to=self.whc.progress_messenger0.recipient_phone,\n            ).send(sender=0)\n            mas_id = mas.get(\"messages\", [{}])[0].get(\"id\")\n            print(mas_id)\n\n        except Exception as e:\n            logging.error(f\"Assistant error: {str(e)}\")\n            self.state = AssistantState.OFFLINE\n            raise\n\n    async def handle_agent_actions(self, message):\n        user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n        def helper():\n\n            stop_flag = threading.Event()\n            try:\n                progress = self.progress_messengers['task']\n                # message_id = progress.send_initial_message(mode=\"loading\")\n                progress.message_id = message.id\n                progress.start_loading_in_background(stop_flag)\n                res = message.content\n                print(message.data.get('entry', [{}])[0].get('changes', [{}])[0].get('value', {}).get('messages', [{}])[0].get(\n                    'context'))\n                if context := message.data.get('entry', [{}])[0].get('changes', [{}])[0].get('value', {}).get('messages', [{}])[0].get(\n                    'context'):\n                    context_str = f\"Context : source {'USER' if context.get('from') in self.whc.progress_messenger0.recipient_phone else 'AGENT'}\"\n                    cd = self.history.get(context.get('id'))\n                    context_str += \"\\n\" + (cd if cd is not None else \"The ref Message is not in the history\")\n                    res += \"\\n\" + context_str\n                if user_state.get('type') == 'system':\n                    res = self.isaa.run(res)\n                    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n                elif user_state.get('type') == 'self-agent':\n                    res = self.agent.run(res)\n                    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n                self.agent.mode = LLMMode(\n                    name=\"Chatter\",\n                    description=\"whatsapp Chat LLM\",\n                    system_msg=\"Response precise and short style using whatsapp syntax!\",\n                    post_msg=None\n                )\n                response = self.agent.mini_task(res, \"user\", persist=True)\n                self.save_reply(message, response)\n            except Exception as e:\n                stop_flag.set()\n                message.reply(\"\u274c Error in agent \"+str(e))\n            finally:\n                self.agent.mode = None\n                stop_flag.set()\n        threading.Thread(target=helper, daemon=True).start()\n\n    def save_reply(self, message, content):\n        res = message.reply(content)\n        res_id = res.get(\"messages\", [{}])[0].get(\"id\")\n        if res_id is not None:\n            self.history.set(res_id, content)\n        else:\n            print(f\"No ID to add to history: {res}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.agent_task","title":"<code>agent_task(message)</code>  <code>async</code>","text":"<p>Initiate email search workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def agent_task(self, message):\n    \"\"\"Initiate email search workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n        'type': 'self-agent',\n        'step': 'await_query'\n    }\n    return {\n        'type': 'quick_reply',\n        'text': \"Now prompt the self-agent \ud83d\udcdd\",\n        'options': {'cancel': '\u274c Cancel Search'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.check_emails","title":"<code>check_emails(message, query='')</code>  <code>async</code>","text":"<p>Improved email checking with WhatsApp API formatting</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def check_emails(self, message, query=\"\"):\n    \"\"\"Improved email checking with WhatsApp API formatting\"\"\"\n    if not self.gmail_service:\n        return \"\u26a0\ufe0f Gmail service not configured\"\n\n    try:\n        results = self.gmail_service.users().messages().list(\n            userId='me',\n            maxResults=10,\n            labelIds=['INBOX'],\n            q=query\n        ).execute()\n\n        emails = []\n        for msg in results.get('messages', [])[:10]:\n            email_data = self.gmail_service.users().messages().get(\n                userId='me',\n                id=msg['id'],\n                format='metadata'\n            ).execute()\n\n            headers = {h['name']: h['value'] for h in email_data['payload']['headers']}\n            emails.append({\n                'id': msg['id'],\n                'from': headers.get('From', 'Unknown'),\n                'subject': headers.get('Subject', 'No Subject'),\n                'date': headers.get('Date', 'Unknown'),\n                'snippet': email_data.get('snippet', ''),\n                'unread': 'UNREAD' in email_data.get('labelIds', [])\n            })\n\n        return {\n            'type': 'list',\n            'header': '\ud83d\udce8 Recent Emails',\n            'body': 'Tap to view full email',\n            'footer': 'Email Manager',\n            'sections': [{\n                'title': f\"Inbox ({len(emails)} emails)\",\n                'rows': [{\n                    'id': f\"email_{email['id']}\",\n                    'title': f\"{'\ud83d\udcec' if email['unread'] else '\ud83d\udced'} {email['subject']}\"[:23],\n                    'description': f\"From: {email['from']}\\n{email['snippet']}\"[:45]\n                } for email in emails]\n            }]\n        }\n    except Exception as e:\n        return f\"\u26a0\ufe0f Error fetching emails: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.complete_authorization","title":"<code>complete_authorization(message)</code>","text":"<p>Complete the authorization process using the authorization code</p> <p>:param authorization_code: Authorization code received from Google</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def complete_authorization(self, message: Message):\n    \"\"\"\n    Complete the authorization process using the authorization code\n\n    :param authorization_code: Authorization code received from Google\n    \"\"\"\n    from google_auth_oauthlib.flow import Flow\n    authorization_code = message.content\n    # Define the scopes required for Gmail and Calendar\n    SCOPES = [\n        'https://www.googleapis.com/auth/gmail.modify',\n        'https://www.googleapis.com/auth/calendar'\n    ]\n\n    # Create a flow instance to manage the OAuth 2.0 authorization process\n    flow = Flow.from_client_secrets_file(\n        self.credentials_path,\n        scopes=SCOPES,\n        redirect_uri='urn:ietf:wg:oauth:2.0:oob'\n    )\n\n    # Exchange the authorization code for credentials\n    flow.fetch_token(code=authorization_code)\n    self.credentials = flow.credentials\n\n    # Save the credentials for future use\n    self.save_credentials()\n\n    # Initialize services\n    self.init_services()\n    return \"Done\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.delete_document","title":"<code>delete_document(message)</code>  <code>async</code>","text":"<p>Delete document workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def delete_document(self, message):\n    \"\"\"Delete document workflow\"\"\"\n    docs = self.blob_docs_system.list_documents()\n    return {\n        'type': 'quick_reply',\n        'text': 'Select document to delete:',\n        'options': {doc['id']: doc['name'] for doc in docs[:5]},\n        'handler': self._confirm_delete\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.email_search","title":"<code>email_search(message)</code>  <code>async</code>","text":"<p>Initiate email search workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def email_search(self, message):\n    \"\"\"Initiate email search workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n        'type': 'email_search',\n        'step': 'await_query'\n    }\n    return {\n        'type': 'quick_reply',\n        'text': \"\ud83d\udd0d What would you like to search for?\",\n        'options': {'cancel': '\u274c Cancel Search'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.email_summary","title":"<code>email_summary(message)</code>  <code>async</code>","text":"<p>Generate AI-powered email summaries</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def email_summary(self, message):\n    \"\"\"Generate AI-powered email summaries\"\"\"\n    try:\n        messages = self.gmail_service.users().messages().list(\n            userId='me',\n            maxResults=3,\n            labelIds=['INBOX']\n        ).execute().get('messages', [])\n\n        email_contents = []\n        for msg in messages[:3]:\n            email_data = self.gmail_service.users().messages().get(\n                userId='me',\n                id=msg['id'],\n                format='full'\n            ).execute()\n            email_contents.append(self._parse_email_content(email_data))\n\n        summary = self.agent.mini_task(\n            \"\\n\\n\".join(email_contents) , \"system\", \"Summarize these emails in bullet points with key details:\"\n        )\n\n        return f\"\ud83d\udccb Email Summary:\\n{summary}\\n\\n*Powered by AI*\"\n    except Exception as e:\n        logging.error(f\"Summary failed: {str(e)}\")\n        return f\"\u274c Could not generate summary: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.find_time_slot","title":"<code>find_time_slot(message)</code>  <code>async</code>","text":"<p>Find and display the next 5 available time slots with dynamic durations</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def find_time_slot(self, message):\n    \"\"\"Find and display the next 5 available time slots with dynamic durations\"\"\"\n    if not self.calendar_service:\n        return \"\u26a0\ufe0f Calendar service not configured\"\n\n    try:\n        # Define the time range for the search (next 24 hours)\n        now = datetime.now(UTC)\n        end_time = now + timedelta(days=1)\n\n        # FreeBusy Request\n        freebusy_request = {\n            \"timeMin\": now.isoformat(),\n            \"timeMax\": end_time.isoformat(),\n            \"items\": [{\"id\": 'primary'}]\n        }\n\n        freebusy_response = self.calendar_service.freebusy().query(body=freebusy_request).execute()\n        busy_slots = freebusy_response['calendars']['primary']['busy']\n\n        # Slot-Berechnung\n        available_slots = self._calculate_efficient_slots(\n            busy_slots,\n            self.duration_minutes\n        )\n\n        # Format the response for WhatsApp\n        return {\n            'type': 'list',\n            'header': \"\u23f0 Available Time Slots\",\n            'body': \"Tap to select a time slot\",\n            'footer': \"Time Slot Finder\",\n            'sections': [{\n                'title': \"Next 5 Available Slots\",\n                'rows': [{\n                    'id': f\"slot_{slot['start'].timestamp()}\",\n                    'title': f\"\ud83d\udd52 {slot['start'].strftime('%H:%M')} - {slot['end'].strftime('%H:%M')}\",\n                    'description': f\"Duration: {slot['duration']}\"\n                } for slot in available_slots[:5]]\n            }]\n        }\n    except Exception as e:\n        return f\"\u26a0\ufe0f Error finding time slots: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.generate_authorization_url","title":"<code>generate_authorization_url(*a)</code>  <code>async</code>","text":"<p>Generate an authorization URL for user consent</p> <p>:return: Authorization URL for the user to click and authorize access</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def generate_authorization_url(self, *a):\n    \"\"\"\n    Generate an authorization URL for user consent\n\n    :return: Authorization URL for the user to click and authorize access\n    \"\"\"\n    from google_auth_oauthlib.flow import Flow\n    # Define the scopes required for Gmail and Calendar\n    SCOPES = [\n        'https://www.googleapis.com/auth/gmail.modify',\n        'https://www.googleapis.com/auth/calendar'\n    ]\n\n    # Create a flow instance to manage the OAuth 2.0 authorization process\n    flow = Flow.from_client_secrets_file(\n        self.credentials_path,\n        scopes=SCOPES,\n        redirect_uri='urn:ietf:wg:oauth:2.0:oob'  # Use 'urn:ietf:wg:oauth:2.0:oob' for desktop apps\n    )\n\n    # Generate the authorization URL\n    authorization_url, _ = flow.authorization_url(\n        access_type='offline',  # Allows obtaining refresh token\n        prompt='consent'  # Ensures user is always prompted for consent\n    )\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {'type': 'auth',\n                                                                          'step': 'awaiting_key'}\n    return {\n        'type': 'quick_reply',\n        'text': f'Url to log in {authorization_url}',\n        'options': {'cancel': '\u274c Cancel Upload'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.get_email_details","title":"<code>get_email_details(email_id)</code>  <code>async</code>","text":"<p>Retrieve and format full email details</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def get_email_details(self, email_id):\n    \"\"\"Retrieve and format full email details\"\"\"\n    if not self.gmail_service:\n        return \"\u26a0\ufe0f Gmail service not configured\"\n\n    try:\n        email_data = self.gmail_service.users().messages().get(\n            userId='me',\n            id=email_id,\n            format='full'\n        ).execute()\n\n        headers = {h['name']: h['value'] for h in email_data['payload']['headers']}\n        body = \"\"\n        for part in email_data.get('payload', {}).get('parts', []):\n            if part['mimeType'] == 'text/plain':\n                body = base64.urlsafe_b64decode(part['body']['data']).decode('utf-8')\n                break\n\n        formatted_text = (\n            f\"\ud83d\udce7 *Email Details*\\n\\n\"\n            f\"From: {headers.get('From', 'Unknown')}\\n\"\n            f\"Subject: {headers.get('Subject', 'No Subject')}\\n\"\n            f\"Date: {headers.get('Date', 'Unknown')}\\n\\n\"\n            f\"{body[:15000]}{'...' if len(body) &gt; 15000 else ''}\"\n        )\n        return  self.agent.mini_task(\n            formatted_text , \"system\", \"Summarize the email in bullet points with key details\"\n        )\n    except Exception as e:\n        return f\"\u26a0\ufe0f Error fetching email: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.get_event_details","title":"<code>get_event_details(event_id)</code>  <code>async</code>","text":"<p>Retrieve and format calendar event details with location support</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def get_event_details(self, event_id):\n    \"\"\"Retrieve and format calendar event details with location support\"\"\"\n    if not self.calendar_service:\n        return \"\u26a0\ufe0f Calendar service not configured\"\n\n    try:\n        event = self.calendar_service.events().get(\n            calendarId='primary',\n            eventId=event_id\n        ).execute()\n\n        response = [ (\n                f\"\ud83d\udcc5 *Event Details*\\n\\n\"\n                f\"Title: {event.get('summary', 'No title')}\\n\"\n                f\"Time: {self._format_event_time(event)}\\n\"\n                f\"Location: {event.get('location', 'Not specified')}\\n\\n\"\n                f\"{event.get('description', 'No description')[:1000]}\"\n            )]\n\n        if 'geo' in event:\n            response.append({\n                'lat': float(event['geo']['latitude']),\n                'long': float(event['geo']['longitude']),\n                'name': event.get('location', 'Event Location'),\n                'address': event.get('location', ''),\n                'recipient_id': self.whc.progress_messenger0.recipient_phone\n            })\n        return response\n    except Exception as e:\n        return f\"\u26a0\ufe0f Error fetching event: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_audio_message","title":"<code>handle_audio_message(message)</code>  <code>async</code>","text":"<p>Process audio messages with STT and TTS</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def handle_audio_message(self, message: 'Message'):\n    \"\"\"Process audio messages with STT and TTS\"\"\"\n    # Download audio\n    progress = self.progress_messengers['task']\n    stop_flag = threading.Event()\n    # message_id = progress.send_initial_message(mode=\"loading\")\n    progress.message_id = message.id\n    progress.start_loading_in_background(stop_flag)\n\n    content = self.whc.messenger.get_audio(message.data)\n    audio_file_name = self.whc.messenger.download_media(media_url=self.whc.messenger.query_media_url(media_id=content.get('id')), mime_type='audio/opus', file_path=\".data/temp\")\n    print(f\"audio_file_name {audio_file_name}\")\n    if audio_file_name is None:\n        message.reply(\"Could not process audio file\")\n        stop_flag.set()\n        return\n\n    text = self.stt(audio_file_name)['text']\n    if not text:\n        message.reply(\"Could not process audio\")\n        stop_flag.set()\n        return\n\n    message.reply(\"Transcription :\\n \"+ text)\n    message.content = text\n    agent_res = await self.helper_text(message, return_text=True)\n\n    if agent_res is not None:\n        pass\n\n    stop_flag.set()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_button_interaction","title":"<code>handle_button_interaction(content, message)</code>  <code>async</code>","text":"<p>Handle button click interactions</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def handle_button_interaction(self, content: dict, message: Message):\n    \"\"\"Handle button click interactions\"\"\"\n    button_id = content['id']\n\n    # First check if it's a main menu button\n    if button_id in self.buttons:\n        self.whc.messenger.send_button(\n            recipient_id=self.whc.progress_messenger0.recipient_phone,\n            button=self.buttons[button_id]\n        )\n        return\n\n    # Handle action buttons\n    action_handlers = {\n        # Agent controls\n        'start': self.start_agent,\n        'stop': self.stop_agent,\n        'tasks': self.show_task_stack,\n        'memory': self.clear_memory,\n        'system-task': self.system_task,\n        'agent-task': self.agent_task,\n\n        # Email controls\n        'check': self.check_emails,\n        'send': self.start_email_compose,\n        'summary': self.email_summary,\n        'search': self.email_search,\n\n        # Calendar controls\n        'today': self.show_today_events,\n        'add': self.start_event_create,\n        'upcoming': self.show_upcoming_events,\n        'find_slot': self.find_time_slot,\n\n        # Document controls\n        'upload': self.start_document_upload,\n        'list': self.list_documents,\n        'search_docs': self.search_documents,\n        'delete': self.delete_document,\n\n        # System controls\n        'status': self.system_status,\n        'restart': self.restart_system,\n        'connect': self.generate_authorization_url,\n\n        'cancel': self.cancel,\n        'confirm': self.confirm,\n    }\n    if button_id in action_handlers:\n        try:\n            # Start progress indicator\n            progress = self.progress_messengers['task']\n            stop_flag = threading.Event()\n            # message_id = progress.send_initial_message(mode=\"loading\")\n            progress.message_id = message.id\n            progress.start_loading_in_background(stop_flag)\n\n            # Execute handler\n\n            result = await action_handlers[button_id](message)\n\n\n            # Send result\n            if isinstance(result, str):\n                self.save_reply(message, result)\n            elif isinstance(result, dict):  # For structured responses\n                self.send_structured_response(result)\n\n            stop_flag.set()\n        finally:\n            #except Exception as e:\n            stop_flag.set()\n        #    message.reply(f\"\u274c Error processing {button_id}: {str(e)}\")\n    elif 'event_' in button_id:\n        res = await self.get_event_details(button_id.replace(\"event_\", ''))\n        if isinstance(res, str):\n            self.save_reply(message, res)\n            return\n        for r in res:\n            if isinstance(r, str):\n                self.save_reply(message, r)\n            else:\n                self.whc.messenger.send_location(**r)\n\n    elif 'email_' in button_id:\n        res = await self.get_email_details(button_id.replace(\"email_\", ''))\n        self.save_reply(message, res)\n    else:\n        message.reply(\"\u26a0\ufe0f Unknown command\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_calendar_actions","title":"<code>handle_calendar_actions(message)</code>  <code>async</code>","text":"<p>Handle calendar-related pending actions</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def handle_calendar_actions(self, message):\n    \"\"\"Handle calendar-related pending actions\"\"\"\n    user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n\n    if user_state.get('type') == 'create_event':\n        return await self._handle_event_creation(message, user_state)\n\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_email_actions","title":"<code>handle_email_actions(message)</code>  <code>async</code>","text":"<p>Handle multi-step email workflows</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>    async def handle_email_actions(self, message):\n        \"\"\"Handle multi-step email workflows\"\"\"\n        user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n\n        if user_state.get('type') == 'compose_email':\n            return await self._handle_email_composition(message, user_state)\n        if user_state.get('type') == 'email_search':\n            return await self.check_emails(message, self.agent.mini_task(\"\"\"Conventire Pezise zu einer googel str only query using : Gmail Suchoperatoren!\n\nBasis-Operatoren:\n- from: Absender\n- to: Empf\u00e4nger\n- subject: Betreff\n- label: Gmail Label\n- has:attachment Anh\u00e4nge\n- newer_than:7d Zeitfilter\n- before: Datum vor\n- after: Datum nach\n\nErweiterte Operatoren:\n- in:inbox\n- in:sent\n- in:spam\n- cc: Kopie\n- bcc: Blindkopie\n- is:unread\n- is:read\n- larger:10M Gr\u00f6\u00dfenfilter\n- smaller:5M\n- filename:pdf Dateityp\n\nProfi-Tipps:\n- Kombinierbar mit UND/ODER\n- Anf\u00fchrungszeichen f\u00fcr exakte Suche\n- Negation mit -\n beispeile : 'Ungelesene Mails letzte Woche': -&gt; 'is:unread newer_than:7d'\n\n\"\"\", \"user\",message.content))\n\n\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_interactive","title":"<code>handle_interactive(message)</code>  <code>async</code>","text":"<p>Handle all interactive messages</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def handle_interactive(self, message: Message):\n    \"\"\"Handle all interactive messages\"\"\"\n    content = self.whc.messenger.get_interactive_response(message.data)\n    if content.get(\"type\") == \"list_reply\":\n        await self.handle_button_interaction(content.get(\"list_reply\"), message)\n    elif content.get(\"type\") == \"button_reply\":\n        print(content)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_media_message","title":"<code>handle_media_message(message)</code>  <code>async</code>","text":"<p>Handle document/image/video uploads</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def handle_media_message(self, message: 'Message'):\n    \"\"\"Handle document/image/video uploads\"\"\"\n    user_state = self.pending_actions.get(self.whc.progress_messenger0.recipient_phone, {})\n\n    if user_state.get('step') == 'awaiting_file':\n        file_type = message.type\n        if file_type not in ['document', 'image', 'video']:\n            return \"Unsupported file type\"\n\n        try:\n            # Download media\n            #media_url = message.document.url if hasattr(message, 'document') else \\\n            #    message.image.url if hasattr(message, 'image') else \\\n            #        message.video.url\n            if file_type =='video':\n                content = self.whc.messenger.get_video(message.data)\n            if file_type =='image':\n                content = self.whc.messenger.get_image(message.data)\n            if file_type =='document':\n                content = self.whc.messenger.get_document(message.data)\n            print(\"Media content:\", content)\n            media_data = self.whc.messenger.download_media(media_url=self.whc.messenger.query_media_url(media_id=content.get('id')),  mime_type=content.get('mime_type'), file_path='.data/temp')\n            print(\"Media media_data:\", media_data)\n            # Save to blob storage\n            filename = f\"file_{file_type}_{datetime.now().isoformat()}_{content.get('sha256', '')}\"\n            blob_id = self.blob_docs_system.save_document(\n                open(media_data, 'rb').read(),\n                filename=filename,\n                file_type=file_type\n            )\n\n            self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n            return f\"\u2705 File uploaded successfully!\\nID: {blob_id}\"\n\n        except Exception as e:\n            logging.error(f\"Upload failed: {str(e)}\")\n            return f\"\u274c Failed to upload file Error : {str(e)}\"\n\n    return \"No pending uploads\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.handle_message","title":"<code>handle_message(message)</code>  <code>async</code>","text":"<p>Main message handler for incoming WhatsApp messages</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def handle_message(self, message: 'Message'):\n    \"\"\"Main message handler for incoming WhatsApp messages\"\"\"\n\n    # Deduplication check\n    with self.message_lock:\n        if message.id in self.processed_messages:\n            return\n        last_ts = time.time()\n        print(last_ts)\n        if len(self.processed_messages) &gt; 0:\n            m_id, last_ts = self.processed_messages.pop()\n            self.processed_messages.add((m_id, last_ts))\n\n        print(\"DUPLICATION P\", message.data.get('entry', [{}])[0].get('changes', [{}])[0].get('value', {}).get('messages', [{}])[0].get('timestamp', 0) , last_ts)\n        if float(message.data.get('entry', [{}])[0].get('changes', [{}])[0].get('value', {}).get('messages', [{}])[0].get('timestamp', 0)) &lt; last_ts - 120:\n            return\n        self.processed_messages.add((message.id, time.perf_counter()))\n\n    # Mark message as read\n    message.mark_as_read()\n\n    # Extract content and type\n    content_type = message.type\n    content = message.content\n\n    print(f\"message.content {content=} {content_type=} {message.data=}\")\n\n    try:\n        if content_type == 'interactive':\n            await self.handle_interactive(message)\n        elif content_type == 'audio':\n            await self.handle_audio_message(message)\n        elif content_type in ['document', 'image', 'video']:\n            response = await self.handle_media_message(message)\n            self.save_reply(message, response)\n        elif content_type == 'text':\n            if content.lower() == \"menu\":\n                self.whc.messenger.send_button(\n                    recipient_id=self.whc.progress_messenger0.recipient_phone,\n                    button=self.buttons[content.lower()]\n                )\n            else:\n                await self.helper_text(message)\n        else:\n            message.reply(\"Unsupported message type\")\n    #except Exception as e:\n    #    logging.error(f\"Message handling error: {str(e)}\")\n    #   message.reply(\"\u274c Error processing request\")\n    finally:\n        # Cleanup old messages (keep 1 hour history)\n        with self.message_lock:\n            self._clean_processed_messages()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.init_services","title":"<code>init_services()</code>","text":"<p>Initialize Gmail and Calendar services</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def init_services(self):\n    \"\"\"\n    Initialize Gmail and Calendar services\n    \"\"\"\n    from googleapiclient.discovery import build\n\n    self.gmail_service = build('gmail', 'v1', credentials=self.credentials)\n    self.calendar_service = build('calendar', 'v3', credentials=self.credentials)\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {}\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.load_credentials","title":"<code>load_credentials()</code>","text":"<p>Load previously saved credentials if available</p> <p>:return: Whether credentials were successfully loaded</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def load_credentials(self):\n    \"\"\"\n    Load previously saved credentials if available\n\n    :return: Whether credentials were successfully loaded\n    \"\"\"\n    try:\n        self.credentials = Credentials.from_authorized_user_file('token/google_token.json')\n        self.init_services()\n        return True\n    except FileNotFoundError:\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.run","title":"<code>run()</code>","text":"<p>Start the WhatsApp assistant</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def run(self):\n    \"\"\"Start the WhatsApp assistant\"\"\"\n    try:\n        self.state = AssistantState.ONLINE\n        # Send welcome message\n\n        mas = self.whc.messenger.create_message(\n            content=\"Digital Assistant is online! Send /help for available commands.\",to=self.whc.progress_messenger0.recipient_phone,\n        ).send(sender=0)\n        mas_id = mas.get(\"messages\", [{}])[0].get(\"id\")\n        print(mas_id)\n\n    except Exception as e:\n        logging.error(f\"Assistant error: {str(e)}\")\n        self.state = AssistantState.OFFLINE\n        raise\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.save_credentials","title":"<code>save_credentials()</code>","text":"<p>Save the obtained credentials to a file for future use</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def save_credentials(self):\n    \"\"\"\n    Save the obtained credentials to a file for future use\n    \"\"\"\n    if not os.path.exists('token'):\n        os.makedirs('token')\n\n    with open('token/google_token.json', 'w') as token_file:\n        token_file.write(self.credentials.to_json())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.search_documents","title":"<code>search_documents(message)</code>  <code>async</code>","text":"<p>Initiate document search workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def search_documents(self, message):\n    \"\"\"Initiate document search workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {'type': 'search', 'step': 'awaiting_query'}\n    return {\n        'type': 'quick_reply',\n        'text': '\ud83d\udd0d What are you looking for?',\n        'options': {'cancel': '\u274c Cancel Search'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.send_email","title":"<code>send_email(to, subject, body)</code>","text":"<p>Actual email sending function to be called by agent</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def send_email(self, to, subject, body):\n    \"\"\"Actual email sending function to be called by agent\"\"\"\n    if not self.gmail_service:\n        return False\n\n    message = MIMEText(body)\n    message['to'] = to\n    message['subject'] = subject\n\n    encoded_message = base64.urlsafe_b64encode(message.as_bytes()).decode()\n    self.gmail_service.users().messages().send(\n        userId='me',\n        body={'raw': encoded_message}\n    ).execute()\n    return True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.send_structured_response","title":"<code>send_structured_response(result)</code>","text":"<p>Send complex responses using appropriate WhatsApp features</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def send_structured_response(self, result: dict):\n    \"\"\"Send complex responses using appropriate WhatsApp features\"\"\"\n    if result['type'] == 'list':\n        self.whc.messenger.send_button(\n            recipient_id=self.whc.progress_messenger0.recipient_phone,\n            button={\n                'header': result.get('header', ''),\n                'body': result.get('body', ''),\n                'footer': result.get('footer', ''),\n                'action': {\n                    'button': 'Action',\n                    'sections': result['sections']\n                }\n            }\n        )\n    elif result['type'] == 'quick_reply':\n        self.whc.messenger.send_button(\n            recipient_id=self.whc.progress_messenger0.recipient_phone,\n            button={\n                'header': \"Quick reply\",\n                'body': result['text'],\n                'footer': '',\n                'action': {'button': 'Action', 'sections': [{\n                    'title': 'View',\n                    'rows': [{'id': k, 'title': v[:23]} for k, v in result['options'].items()]\n                }]}\n            }\n        )\n\n    elif result['type'] == 'media':\n        if result['media_type'] == 'image':\n            self.whc.messenger.send_image(\n                image=result['url'],\n                recipient_id=self.whc.progress_messenger0.recipient_phone,\n                caption=result.get('caption', '')\n            )\n        elif result['media_type'] == 'document':\n            self.whc.messenger.send_document(\n                document=result['url'],\n                recipient_id=self.whc.progress_messenger0.recipient_phone,\n                caption=result.get('caption', '')\n            )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.setup_interaction_buttons","title":"<code>setup_interaction_buttons()</code>","text":"<p>Define WhatsApp interaction buttons for different functionalities</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def setup_interaction_buttons(self):\n    \"\"\"Define WhatsApp interaction buttons for different functionalities\"\"\"\n    self.buttons = {\n        'menu': {\n            'header': 'Digital Assistant',\n            'body': 'Please select an option:',\n            'footer': '-- + --',\n            'action': {\n                'button': 'Menu',\n                'sections': [\n                    {\n                        'title': 'Main Functions',\n                        'rows': [\n                            {'id': 'agent', 'title': 'Agent Controls', 'description': 'Manage your AI assistant'},\n                            {'id': 'email', 'title': 'Email Management', 'description': 'Handle your emails'},\n                            {'id': 'calendar', 'title': 'Calendar', 'description': 'Manage your schedule'},\n                            {'id': 'docs', 'title': 'Documents', 'description': 'Handle documents'},\n                            {'id': 'system', 'title': 'System', 'description': 'System controls and metrics'}\n                        ]\n                    }\n                ]\n            }\n        },\n        'agent': self._create_agent_controls_buttons(),\n        'email': self._create_email_controls_buttons(),\n        'calendar': self._create_calendar_controls_buttons(),\n        'docs': self._create_docs_controls_buttons(),\n        'system': self._create_system_controls_buttons()\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.setup_progress_messengers","title":"<code>setup_progress_messengers()</code>","text":"<p>Initialize progress messengers for different types of tasks</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>def setup_progress_messengers(self):\n    \"\"\"Initialize progress messengers for different types of tasks\"\"\"\n    self.progress_messengers = {\n        'task': self.whc.progress_messenger0,\n        'email': self.whc.progress_messenger1,\n        'calendar': self.whc.progress_messenger2\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.show_task_stack","title":"<code>show_task_stack(*a)</code>  <code>async</code>","text":"<p>Display current task stack</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def show_task_stack(self, *a):\n    \"\"\"Display current task stack\"\"\"\n    if self.agent and len(self.agent.taskstack.tasks) &gt; 0:\n        tasks = self.agent.taskstack.tasks\n        return self.agent.mini_task(\"\\n\".join([f\"Task {t.id}: {t.description}\" for t in tasks]), \"system\", \"Format to nice and clean whatsapp format\")\n    return \"No tasks in stack\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.show_today_events","title":"<code>show_today_events(message)</code>  <code>async</code>","text":"<p>Show today's calendar events</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def show_today_events(self, message):\n    \"\"\"Show today's calendar events\"\"\"\n    if not self.calendar_service:\n        message.replay(\"service not online\")\n\n    now = datetime.utcnow().isoformat() + 'Z'\n    end_of_day = (datetime.now() + timedelta(days=1)).replace(\n        hour=0, minute=0, second=0).isoformat() + 'Z'\n\n    events_result = self.calendar_service.events().list(\n        calendarId='primary',\n        timeMin=now,\n        timeMax=end_of_day,\n        singleEvents=True,\n        orderBy='startTime'\n    ).execute()\n\n    events = events_result.get('items', [])\n    return self._format_calendar_response(events, \"Today's Events\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.show_upcoming_events","title":"<code>show_upcoming_events(message)</code>  <code>async</code>","text":"<p>Show upcoming events with interactive support</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def show_upcoming_events(self, message):\n    \"\"\"Show upcoming events with interactive support\"\"\"\n    if not self.calendar_service:\n        return \"\u26a0\ufe0f Calendar service not configured\"\n\n    try:\n        now = datetime.utcnow().isoformat() + 'Z'\n        next_week = (datetime.now() + timedelta(days=7)).isoformat() + 'Z'\n\n        events_result = self.calendar_service.events().list(\n            calendarId='primary',\n            timeMin=now,\n            timeMax=next_week,\n            singleEvents=True,\n            orderBy='startTime',\n            maxResults=10\n        ).execute()\n\n        events = events_result.get('items', [])\n        return self._format_calendar_response(events, \"Upcoming Events\")\n    except Exception as e:\n        return f\"\u26a0\ufe0f Error fetching events: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.start_agent","title":"<code>start_agent(*a)</code>  <code>async</code>","text":"<p>Start the agent in background mode</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def start_agent(self, *a):\n    \"\"\"Start the agent in background mode\"\"\"\n    if self.agent:\n        self.agent.run_in_background()\n        return True\n    return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.start_document_upload","title":"<code>start_document_upload(message)</code>  <code>async</code>","text":"<p>Initiate document upload workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def start_document_upload(self, message):\n    \"\"\"Initiate document upload workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {'type': 'document', 'step': 'awaiting_file'}\n    return {\n        'type': 'quick_reply',\n        'text': '\ud83d\udce4 Send me the file you want to upload',\n        'options': {'cancel': '\u274c Cancel Upload'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.start_email_compose","title":"<code>start_email_compose(message)</code>  <code>async</code>","text":"<p>Enhanced email composition workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def start_email_compose(self, message):\n    \"\"\"Enhanced email composition workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n        'type': 'compose_email',\n        'step': 'subject',\n        'draft': {'attachments': []}\n    }\n    return {\n        'type': 'quick_reply',\n        'text': \"\ud83d\udcdd Let's compose an email\\n\\nSubject:\",\n        'options': {'cancel': '\u274c Cancel Composition'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.start_event_create","title":"<code>start_event_create(message)</code>  <code>async</code>","text":"<p>Initiate event creation workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def start_event_create(self, message):\n    \"\"\"Initiate event creation workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n        'type': 'create_event',\n        'step': 'title',\n        'event_data': {}\n    }\n    return {\n        'type': 'quick_reply',\n        'text': \"Let's create an event! What's the title?\",\n        'options': {'cancel': '\u274c Cancel'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.stop_agent","title":"<code>stop_agent(*b)</code>  <code>async</code>","text":"<p>Stop the currently running agent</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def stop_agent(self, *b):\n    \"\"\"Stop the currently running agent\"\"\"\n    if self.agent:\n        self.agent.stop()\n        return True\n    return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.client.WhatsAppAssistant.system_task","title":"<code>system_task(message)</code>  <code>async</code>","text":"<p>Initiate email search workflow</p> Source code in <code>toolboxv2/mods/WhatsAppTb/client.py</code> <pre><code>async def system_task(self, message):\n    \"\"\"Initiate email search workflow\"\"\"\n    self.pending_actions[self.whc.progress_messenger0.recipient_phone] = {\n        'type': 'system',\n        'step': 'await_query'\n    }\n    return {\n        'type': 'quick_reply',\n        'text': \"Now prompt the \ud83e\udde0ISAA-System \ud83d\udcdd\",\n        'options': {'cancel': '\u274c Cancel Search'}\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server","title":"<code>server</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager","title":"<code>AppManager</code>","text":"Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>class AppManager(metaclass=Singleton):\n    pepper = \"pepper0\"\n\n    def __init__(self, start_port: int = 8000, port_range: int = 10, em=None):\n        self.instances: dict[str, dict] = {}\n        self.start_port = start_port\n        self.port_range = port_range\n        self.threads: dict[str, Thread] = {}\n        self.stop_events: dict[str, Event] = {}\n        self.message_queue: asyncio.Queue = asyncio.Queue()\n        self.last_messages: dict[str, datetime] = {}\n        self.keys: dict[str, str] = {}\n        self.forwarders: dict[str, dict] = {}\n        self.runner = lambda :None\n\n        if em is None:\n            from toolboxv2 import get_app\n            em = get_app().get_mod(\"EventManager\")\n        from toolboxv2.mods import EventManager\n        self.event_manager: EventManager = em.get_manager()\n\n        # Set up signal handlers for graceful shutdown\n        try:\n            if threading.current_thread() is threading.main_thread():\n                signal.signal(signal.SIGINT, self.signal_handler)\n                signal.signal(signal.SIGTERM, self.signal_handler)\n        except Exception:\n            pass\n\n    def offline(self, instance_id):\n\n        def mark_as_offline():\n            self.forwarders[instance_id]['send'] = None\n            return 'done'\n\n        return mark_as_offline\n\n    def online(self, instance_id):\n\n        def mark_as_online():\n            return self.instances[instance_id]['app']\n\n        def set_callbacks(callback, e_callback=None):\n            if callback is not None:\n                self.forwarders[instance_id]['send'] = callback\n            if e_callback is not None:\n                self.forwarders[instance_id]['sende'] = e_callback\n\n        return mark_as_online(), set_callbacks\n\n    def get_next_available_port(self) -&gt; int:\n        \"\"\"Find the next available port in the range.\"\"\"\n        used_ports = {instance['port'] for instance in self.instances.values()}\n        for port in range(self.start_port, self.start_port + self.port_range):\n            if port not in used_ports:\n                return port\n        raise RuntimeError(\"No available ports in range\")\n\n    def add_instance(self, instance_id: str, **kwargs):\n        \"\"\"\n        Add a new app instance to the manager with automatic port assignment.\n        \"\"\"\n        if instance_id in self.instances:\n            raise ValueError(f\"Instance {instance_id} already exists\")\n\n        port = self.get_next_available_port()\n        app_instance = WhatsApp(**kwargs)\n\n        self.instances[instance_id] = {\n            'app': app_instance,\n            'port': port,\n            'kwargs': kwargs,\n            'phone_number_id': kwargs.get(\"phone_number_id\", {}),\n            'retry_count': 0,\n            'max_retries': 3,\n            'retry_delay': 5\n        }\n        self.keys[instance_id] = Code.one_way_hash(kwargs.get(\"phone_number_id\", {}).get(\"key\"), \"WhatsappAppManager\",\n                                                   self.pepper)\n        self.forwarders[instance_id] = {}\n\n        # Set up message handlers\n        @app_instance.on_message\n        async def message_handler(message):\n            await self.on_message(instance_id, message)\n\n        @app_instance.on_event\n        async def event_handler(event):\n            await self.on_event(instance_id, event)\n\n        @app_instance.on_verification\n        async def verification_handler(verification):\n            await self.on_verification(instance_id, verification)\n\n        # Create stop event for this instance Error parsing message1:\n        self.stop_events[instance_id] = Event()\n\n    def run_instance(self, instance_id: str):\n        \"\"\"Run a single instance in a separate thread with error handling and automatic restart.\"\"\"\n        instance_data = self.instances[instance_id]\n        stop_event = self.stop_events[instance_id]\n\n        while not stop_event.is_set():\n            try:\n                logger.info(f\"Starting instance {instance_id} on port {instance_data['port']}\")\n                instance_data['app'].run(host='0.0.0.0', port=instance_data['port'])\n\n            except Exception as e:\n                logger.error(f\"Error in instance {instance_id}: {str(e)}\")\n                instance_data['retry_count'] += 1\n\n                if instance_data['retry_count'] &gt; instance_data['max_retries']:\n                    logger.error(f\"Max retries exceeded for instance {instance_id}\")\n                    break\n\n                logger.info(f\"Restarting instance {instance_id} in {instance_data['retry_delay']} seconds...\")\n                time.sleep(instance_data['retry_delay'])\n\n                # Recreate the instance\n                instance_data['app'] = WhatsApp(**instance_data['kwargs'])\n                continue\n\n    async def on_message(self, instance_id: str, message: Message):\n        \"\"\"Handle and forward incoming messages.\"\"\"\n        logger.info(f\"Message from instance {instance_id}: {message}\")\n        if instance_id in self.forwarders and 'send' in self.forwarders[instance_id]:\n            await self.forwarders[instance_id]['send'](message)\n\n    async def on_event(self, instance_id: str, event):\n        \"\"\"Handle events.\"\"\"\n        logger.info(f\"Event from instance {instance_id}: {event}\")\n        if instance_id in self.forwarders and 'sende' in self.forwarders[instance_id] and self.forwarders[instance_id]['sende'] is not None:\n            self.forwarders[instance_id]['sende'](event)\n\n    async def on_verification(self, instance_id: str, verification):\n        \"\"\"Handle verification events.\"\"\"\n        logger.info(f\"Verification from instance {instance_id}: {verification}\")\n\n    def run_all_instances(self):\n        \"\"\"Start all instances in separate daemon threads.\"\"\"\n        # Start message forwarder\n\n        # Start all instances\n        for instance_id in self.instances:\n            thread = Thread(\n                target=self.run_instance,\n                args=(instance_id,),\n                daemon=True,\n                name=f\"WhatsApp-{instance_id}\"\n            )\n            self.threads[instance_id] = thread\n            thread.start()\n\n    def signal_handler(self, signum, frame):\n        \"\"\"Handle shutdown signals gracefully.\"\"\"\n        logger.info(\"Shutdown signal received, stopping all instances...\")\n        self.stop_all_instances()\n        sys.exit(0)\n\n    def stop_all_instances(self):\n        \"\"\"Stop all running instances gracefully.\"\"\"\n        for instance_id in self.stop_events:\n            self.stop_events[instance_id].set()\n\n        for thread in self.threads.values():\n            thread.join(timeout=5)\n\n    def create_manager_ui(self, start_assistant):\n        \"\"\"Enhanced WhatsApp Manager UI with instance configuration controls\"\"\"\n        self.runner = start_assistant\n        def ui_manager():\n            # Track instance states and messages\n            original_on_message = self.on_message\n\n            async def enhanced_on_message(instance_id: str, message):\n                self.last_messages[instance_id] = datetime.now()\n                await original_on_message(instance_id, message)\n\n            self.on_message = enhanced_on_message\n\n            def create_instance_card(instance_id: str):\n                \"\"\"Interactive instance control card\"\"\"\n                config = self.instances[instance_id]\n                with ui.card().classes('w-full p-4 mb-4 bg-gray-50 dark:bg-gray-800').style(\"background-color: var(--background-color) !important\"):\n                    # Header Section\n                    with ui.row().classes('w-full justify-between items-center'):\n                        ui.label(f'\ud83d\udcf1 {instance_id}').classes('text-xl font-bold')\n\n                        # Status Indicator\n                        ui.label().bind_text_from(\n                            self.threads, instance_id,\n                            lambda x: 'Running' if x and x.is_alive() else 'Stopped'\n                        )\n\n                    # Configuration Display\n                    with ui.grid(columns=2).classes('w-full mt-4 gap-2'):\n\n                        ui.label('port:').classes('font-bold')\n                        ui.label(config['port'])\n\n                        ui.label('Last Activity:').classes('font-bold')\n                        ui.label().bind_text_from(\n                            self.last_messages, instance_id,\n                            lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\") if x else 'Never'\n                        )\n\n                    # Action Controls\n                    with ui.row().classes('w-full mt-4 gap-2'):\n                        with ui.button(icon='settings', on_click=lambda: edit_dialog.open()).props('flat'):\n                            ui.tooltip('Configure')\n\n                        with ui.button(icon='refresh', color='orange',\n                                       on_click=lambda: self.restart_instance(instance_id)):\n                            ui.tooltip('Restart')\n\n                        with ui.button(icon='stop', color='red',\n                                       on_click=lambda: self.stop_instance(instance_id)):\n                            ui.tooltip('Stop')\n\n                    # Edit Configuration Dialog\n                    with ui.dialog() as edit_dialog, ui.card().classes('p-4 gap-4'):\n                        new_key = ui.input('API Key', value=config['phone_number_id'].get('key', ''))\n                        new_number = ui.input('Phone Number', value=config['phone_number_id'].get('number', ''))\n\n                        with ui.row().classes('w-full justify-end'):\n                            ui.button('Cancel', on_click=edit_dialog.close)\n                            ui.button('Save', color='primary', on_click=lambda: (\n                                self.update_instance_config(\n                                    instance_id,\n                                    new_key.value,\n                                    new_number.value\n                                ),\n                                edit_dialog.close()\n                            ))\n\n            # Main UI Layout\n            with ui.column().classes('w-full max-w-4xl mx-auto p-4'):\n                ui.label('WhatsApp Instance Manager').classes('text-2xl font-bold mb-6')\n\n                # Add Instance Section\n                with ui.expansion('\u2795 Add New Instance', icon='add').classes('w-full'):\n                    with ui.card().classes('w-full p-4 mt-2'):\n                        instance_id = ui.input('Instance ID').classes('w-full')\n                        token = ui.input('API Token').classes('w-full')\n                        phone_key = ui.input('Phone Number Key').classes('w-full')\n                        phone_number = ui.input('Phone Number').classes('w-full')\n\n                        with ui.row().classes('w-full justify-end gap-2'):\n                            ui.button('Clear', on_click=lambda: (\n                                instance_id.set_value(''),\n                                token.set_value(''),\n                                phone_key.set_value(''),\n                                phone_number.set_value('')\n                            ))\n                            ui.button('Create', color='positive', on_click=lambda: (\n                                self.add_update_instance(\n                                    instance_id.value,\n                                    token.value,\n                                    phone_key.value,\n                                    phone_number.value\n                                ),\n                                instances_container.refresh()\n                            ))\n\n                # Instances Display\n                instances_container = ui.column().classes('w-full')\n                with instances_container:\n                    for instance_id in self.instances:\n                        create_instance_card(instance_id)\n\n        return ui_manager\n\n    # Add to manager class\n    def add_update_instance(self, instance_id, token, phone_key, phone_number):\n        \"\"\"Add or update instance configuration\"\"\"\n        if instance_id in self.instances:\n            self.stop_instance(instance_id)\n            del self.instances[instance_id]\n\n        self.add_instance(\n            instance_id,\n            token=token,\n            phone_number_id={\n                'key': phone_key,\n                'number': phone_number\n            },\n            verify_token=os.getenv(\"WHATSAPP_VERIFY_TOKEN\")\n        )\n        self.start_instance(instance_id)\n\n    def update_instance_config(self, instance_id, new_key, new_number):\n        \"\"\"Update existing instance configuration\"\"\"\n        if instance_id in self.instances:\n            self.instances[instance_id]['phone_number_id'] = {\n                'key': new_key,\n                'number': new_number\n            }\n            self.restart_instance(instance_id)\n\n    def restart_instance(self, instance_id):\n        \"\"\"Safe restart of instance\"\"\"\n        self.stop_instance(instance_id)\n        self.start_instance(instance_id)\n\n    def stop_instance(self, instance_id):\n        \"\"\"Graceful stop of instance\"\"\"\n        if instance_id in self.threads:\n            self.stop_events[instance_id].set()\n            self.threads[instance_id].join(timeout=5)\n            del self.threads[instance_id]\n\n    def start_instance(self, instance_id):\n        \"\"\"Start instance thread\"\"\"\n        print(\"Starting Istance\")\n\n        self.stop_events[instance_id] = threading.Event()\n        self.threads[instance_id] = threading.Thread(\n            target=self.run_instance,\n            args=(instance_id,),\n            daemon=True\n        )\n        self.threads[instance_id].start()\n        print(\"Running starter\", self.runner())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.add_instance","title":"<code>add_instance(instance_id, **kwargs)</code>","text":"<p>Add a new app instance to the manager with automatic port assignment.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def add_instance(self, instance_id: str, **kwargs):\n    \"\"\"\n    Add a new app instance to the manager with automatic port assignment.\n    \"\"\"\n    if instance_id in self.instances:\n        raise ValueError(f\"Instance {instance_id} already exists\")\n\n    port = self.get_next_available_port()\n    app_instance = WhatsApp(**kwargs)\n\n    self.instances[instance_id] = {\n        'app': app_instance,\n        'port': port,\n        'kwargs': kwargs,\n        'phone_number_id': kwargs.get(\"phone_number_id\", {}),\n        'retry_count': 0,\n        'max_retries': 3,\n        'retry_delay': 5\n    }\n    self.keys[instance_id] = Code.one_way_hash(kwargs.get(\"phone_number_id\", {}).get(\"key\"), \"WhatsappAppManager\",\n                                               self.pepper)\n    self.forwarders[instance_id] = {}\n\n    # Set up message handlers\n    @app_instance.on_message\n    async def message_handler(message):\n        await self.on_message(instance_id, message)\n\n    @app_instance.on_event\n    async def event_handler(event):\n        await self.on_event(instance_id, event)\n\n    @app_instance.on_verification\n    async def verification_handler(verification):\n        await self.on_verification(instance_id, verification)\n\n    # Create stop event for this instance Error parsing message1:\n    self.stop_events[instance_id] = Event()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.add_update_instance","title":"<code>add_update_instance(instance_id, token, phone_key, phone_number)</code>","text":"<p>Add or update instance configuration</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def add_update_instance(self, instance_id, token, phone_key, phone_number):\n    \"\"\"Add or update instance configuration\"\"\"\n    if instance_id in self.instances:\n        self.stop_instance(instance_id)\n        del self.instances[instance_id]\n\n    self.add_instance(\n        instance_id,\n        token=token,\n        phone_number_id={\n            'key': phone_key,\n            'number': phone_number\n        },\n        verify_token=os.getenv(\"WHATSAPP_VERIFY_TOKEN\")\n    )\n    self.start_instance(instance_id)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.create_manager_ui","title":"<code>create_manager_ui(start_assistant)</code>","text":"<p>Enhanced WhatsApp Manager UI with instance configuration controls</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def create_manager_ui(self, start_assistant):\n    \"\"\"Enhanced WhatsApp Manager UI with instance configuration controls\"\"\"\n    self.runner = start_assistant\n    def ui_manager():\n        # Track instance states and messages\n        original_on_message = self.on_message\n\n        async def enhanced_on_message(instance_id: str, message):\n            self.last_messages[instance_id] = datetime.now()\n            await original_on_message(instance_id, message)\n\n        self.on_message = enhanced_on_message\n\n        def create_instance_card(instance_id: str):\n            \"\"\"Interactive instance control card\"\"\"\n            config = self.instances[instance_id]\n            with ui.card().classes('w-full p-4 mb-4 bg-gray-50 dark:bg-gray-800').style(\"background-color: var(--background-color) !important\"):\n                # Header Section\n                with ui.row().classes('w-full justify-between items-center'):\n                    ui.label(f'\ud83d\udcf1 {instance_id}').classes('text-xl font-bold')\n\n                    # Status Indicator\n                    ui.label().bind_text_from(\n                        self.threads, instance_id,\n                        lambda x: 'Running' if x and x.is_alive() else 'Stopped'\n                    )\n\n                # Configuration Display\n                with ui.grid(columns=2).classes('w-full mt-4 gap-2'):\n\n                    ui.label('port:').classes('font-bold')\n                    ui.label(config['port'])\n\n                    ui.label('Last Activity:').classes('font-bold')\n                    ui.label().bind_text_from(\n                        self.last_messages, instance_id,\n                        lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\") if x else 'Never'\n                    )\n\n                # Action Controls\n                with ui.row().classes('w-full mt-4 gap-2'):\n                    with ui.button(icon='settings', on_click=lambda: edit_dialog.open()).props('flat'):\n                        ui.tooltip('Configure')\n\n                    with ui.button(icon='refresh', color='orange',\n                                   on_click=lambda: self.restart_instance(instance_id)):\n                        ui.tooltip('Restart')\n\n                    with ui.button(icon='stop', color='red',\n                                   on_click=lambda: self.stop_instance(instance_id)):\n                        ui.tooltip('Stop')\n\n                # Edit Configuration Dialog\n                with ui.dialog() as edit_dialog, ui.card().classes('p-4 gap-4'):\n                    new_key = ui.input('API Key', value=config['phone_number_id'].get('key', ''))\n                    new_number = ui.input('Phone Number', value=config['phone_number_id'].get('number', ''))\n\n                    with ui.row().classes('w-full justify-end'):\n                        ui.button('Cancel', on_click=edit_dialog.close)\n                        ui.button('Save', color='primary', on_click=lambda: (\n                            self.update_instance_config(\n                                instance_id,\n                                new_key.value,\n                                new_number.value\n                            ),\n                            edit_dialog.close()\n                        ))\n\n        # Main UI Layout\n        with ui.column().classes('w-full max-w-4xl mx-auto p-4'):\n            ui.label('WhatsApp Instance Manager').classes('text-2xl font-bold mb-6')\n\n            # Add Instance Section\n            with ui.expansion('\u2795 Add New Instance', icon='add').classes('w-full'):\n                with ui.card().classes('w-full p-4 mt-2'):\n                    instance_id = ui.input('Instance ID').classes('w-full')\n                    token = ui.input('API Token').classes('w-full')\n                    phone_key = ui.input('Phone Number Key').classes('w-full')\n                    phone_number = ui.input('Phone Number').classes('w-full')\n\n                    with ui.row().classes('w-full justify-end gap-2'):\n                        ui.button('Clear', on_click=lambda: (\n                            instance_id.set_value(''),\n                            token.set_value(''),\n                            phone_key.set_value(''),\n                            phone_number.set_value('')\n                        ))\n                        ui.button('Create', color='positive', on_click=lambda: (\n                            self.add_update_instance(\n                                instance_id.value,\n                                token.value,\n                                phone_key.value,\n                                phone_number.value\n                            ),\n                            instances_container.refresh()\n                        ))\n\n            # Instances Display\n            instances_container = ui.column().classes('w-full')\n            with instances_container:\n                for instance_id in self.instances:\n                    create_instance_card(instance_id)\n\n    return ui_manager\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.get_next_available_port","title":"<code>get_next_available_port()</code>","text":"<p>Find the next available port in the range.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def get_next_available_port(self) -&gt; int:\n    \"\"\"Find the next available port in the range.\"\"\"\n    used_ports = {instance['port'] for instance in self.instances.values()}\n    for port in range(self.start_port, self.start_port + self.port_range):\n        if port not in used_ports:\n            return port\n    raise RuntimeError(\"No available ports in range\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.on_event","title":"<code>on_event(instance_id, event)</code>  <code>async</code>","text":"<p>Handle events.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>async def on_event(self, instance_id: str, event):\n    \"\"\"Handle events.\"\"\"\n    logger.info(f\"Event from instance {instance_id}: {event}\")\n    if instance_id in self.forwarders and 'sende' in self.forwarders[instance_id] and self.forwarders[instance_id]['sende'] is not None:\n        self.forwarders[instance_id]['sende'](event)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.on_message","title":"<code>on_message(instance_id, message)</code>  <code>async</code>","text":"<p>Handle and forward incoming messages.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>async def on_message(self, instance_id: str, message: Message):\n    \"\"\"Handle and forward incoming messages.\"\"\"\n    logger.info(f\"Message from instance {instance_id}: {message}\")\n    if instance_id in self.forwarders and 'send' in self.forwarders[instance_id]:\n        await self.forwarders[instance_id]['send'](message)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.on_verification","title":"<code>on_verification(instance_id, verification)</code>  <code>async</code>","text":"<p>Handle verification events.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>async def on_verification(self, instance_id: str, verification):\n    \"\"\"Handle verification events.\"\"\"\n    logger.info(f\"Verification from instance {instance_id}: {verification}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.restart_instance","title":"<code>restart_instance(instance_id)</code>","text":"<p>Safe restart of instance</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def restart_instance(self, instance_id):\n    \"\"\"Safe restart of instance\"\"\"\n    self.stop_instance(instance_id)\n    self.start_instance(instance_id)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.run_all_instances","title":"<code>run_all_instances()</code>","text":"<p>Start all instances in separate daemon threads.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def run_all_instances(self):\n    \"\"\"Start all instances in separate daemon threads.\"\"\"\n    # Start message forwarder\n\n    # Start all instances\n    for instance_id in self.instances:\n        thread = Thread(\n            target=self.run_instance,\n            args=(instance_id,),\n            daemon=True,\n            name=f\"WhatsApp-{instance_id}\"\n        )\n        self.threads[instance_id] = thread\n        thread.start()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.run_instance","title":"<code>run_instance(instance_id)</code>","text":"<p>Run a single instance in a separate thread with error handling and automatic restart.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def run_instance(self, instance_id: str):\n    \"\"\"Run a single instance in a separate thread with error handling and automatic restart.\"\"\"\n    instance_data = self.instances[instance_id]\n    stop_event = self.stop_events[instance_id]\n\n    while not stop_event.is_set():\n        try:\n            logger.info(f\"Starting instance {instance_id} on port {instance_data['port']}\")\n            instance_data['app'].run(host='0.0.0.0', port=instance_data['port'])\n\n        except Exception as e:\n            logger.error(f\"Error in instance {instance_id}: {str(e)}\")\n            instance_data['retry_count'] += 1\n\n            if instance_data['retry_count'] &gt; instance_data['max_retries']:\n                logger.error(f\"Max retries exceeded for instance {instance_id}\")\n                break\n\n            logger.info(f\"Restarting instance {instance_id} in {instance_data['retry_delay']} seconds...\")\n            time.sleep(instance_data['retry_delay'])\n\n            # Recreate the instance\n            instance_data['app'] = WhatsApp(**instance_data['kwargs'])\n            continue\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.signal_handler","title":"<code>signal_handler(signum, frame)</code>","text":"<p>Handle shutdown signals gracefully.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def signal_handler(self, signum, frame):\n    \"\"\"Handle shutdown signals gracefully.\"\"\"\n    logger.info(\"Shutdown signal received, stopping all instances...\")\n    self.stop_all_instances()\n    sys.exit(0)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.start_instance","title":"<code>start_instance(instance_id)</code>","text":"<p>Start instance thread</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def start_instance(self, instance_id):\n    \"\"\"Start instance thread\"\"\"\n    print(\"Starting Istance\")\n\n    self.stop_events[instance_id] = threading.Event()\n    self.threads[instance_id] = threading.Thread(\n        target=self.run_instance,\n        args=(instance_id,),\n        daemon=True\n    )\n    self.threads[instance_id].start()\n    print(\"Running starter\", self.runner())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.stop_all_instances","title":"<code>stop_all_instances()</code>","text":"<p>Stop all running instances gracefully.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def stop_all_instances(self):\n    \"\"\"Stop all running instances gracefully.\"\"\"\n    for instance_id in self.stop_events:\n        self.stop_events[instance_id].set()\n\n    for thread in self.threads.values():\n        thread.join(timeout=5)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.stop_instance","title":"<code>stop_instance(instance_id)</code>","text":"<p>Graceful stop of instance</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def stop_instance(self, instance_id):\n    \"\"\"Graceful stop of instance\"\"\"\n    if instance_id in self.threads:\n        self.stop_events[instance_id].set()\n        self.threads[instance_id].join(timeout=5)\n        del self.threads[instance_id]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.server.AppManager.update_instance_config","title":"<code>update_instance_config(instance_id, new_key, new_number)</code>","text":"<p>Update existing instance configuration</p> Source code in <code>toolboxv2/mods/WhatsAppTb/server.py</code> <pre><code>def update_instance_config(self, instance_id, new_key, new_number):\n    \"\"\"Update existing instance configuration\"\"\"\n    if instance_id in self.instances:\n        self.instances[instance_id]['phone_number_id'] = {\n            'key': new_key,\n            'number': new_number\n        }\n        self.restart_instance(instance_id)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils","title":"<code>utils</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils.ProgressMessenger","title":"<code>ProgressMessenger</code>","text":"Source code in <code>toolboxv2/mods/WhatsAppTb/utils.py</code> <pre><code>class ProgressMessenger:\n    def __init__(self, messenger, recipient_phone: str, max_steps: int = 5, emoji_set: list[str] = None, content=None):\n        self.messenger = messenger\n        self.recipient_phone = recipient_phone\n        self.max_steps = max_steps\n        self.emoji_set = emoji_set or [\"\u2b1c\", \"\u2b1b\", \"\ud83d\udfe9\", \"\ud83d\udfe8\", \"\ud83d\udfe6\"]\n        self.message_id = None\n        self.content = content\n\n    def send_initial_message(self, mode: str = \"progress\"):\n        \"\"\"\n        Sends the initial message. Modes can be 'progress' or 'loading'.\n        \"\"\"\n        if mode == \"progress\":\n            emoji_legend = \"\\n\".join(\n                f\"{emoji} - Step {i + 1}\" for i, emoji in enumerate(self.emoji_set)\n            )\n            content = (\n                \"Progress is being updated in real-time!\\n\\n\"\n                \"Legend:\\n\"\n                f\"{emoji_legend}\\n\\n\"\n                \"Stay tuned for updates!\"\n            )\n        elif mode == \"loading\":\n            content = (\n                \"Loading in progress! \ud83c\udf00\\n\"\n                \"The indicator will loop until work is done.\"\n            )\n        else:\n            raise ValueError(\"Invalid mode. Use 'progress' or 'loading'.\")\n\n        if self.content is not None:\n            content += '\\n'+self.content\n        message = self.messenger.create_message(content=content, to=self.recipient_phone)\n        response = message.send(sender=0)\n        self.message_id = response.get(\"messages\", [{}])[0].get(\"id\")\n        logging.info(f\"Initial message sent: {content}\")\n        return self.message_id\n\n    def update_progress(self, step_flag: threading.Event):\n        \"\"\"\n        Updates the reaction on the message to represent progress.\n        \"\"\"\n        if not self.message_id:\n            raise ValueError(\"Message ID not found. Ensure the initial message is sent first.\")\n        message = self.messenger.create_message(id=self.message_id, to=self.recipient_phone)\n        for step in range(self.max_steps):\n            emoji = self.emoji_set[step % len(self.emoji_set)]\n            message.react(emoji)\n            logging.info(f\"Progress updated: Step {step + 1}/{self.max_steps} with emoji {emoji}\")\n            while not step_flag.is_set():\n                time.sleep(0.5)\n            step_flag.clear()\n        # Final acknowledgment\n        message.react(\"\ud83d\udc4d\")\n        logging.info(\"Progress completed with final acknowledgment.\")\n\n    def update_loading(self, stop_flag: threading.Event):\n        \"\"\"\n        Continuously updates the reaction to represent a looping 'loading' indicator.\n        \"\"\"\n        if not self.message_id:\n            raise ValueError(\"Message ID not found. Ensure the initial message is sent first.\")\n        message = self.messenger.create_message(id=self.message_id, to=self.recipient_phone)\n        step = 0\n        while not stop_flag.is_set():\n            emoji = self.emoji_set[step % len(self.emoji_set)]\n            message.react(emoji)\n            logging.info(f\"Loading update: {emoji}\")\n            time.sleep(1)  # Faster updates for loading\n            step += 1\n        # Final acknowledgment\n        message.react(\"\u2705\")\n        logging.info(\"Loading completed with final acknowledgment.\")\n        message.reply(\"\u2705Done\u2705\")\n\n    def start_progress_in_background(self, step_flag):\n        \"\"\"\n        Starts the progress update in a separate thread.\n        \"\"\"\n        threading.Thread(target=self.update_progress, args=(step_flag, ), daemon=True).start()\n\n    def start_loading_in_background(self, stop_flag: threading.Event):\n        \"\"\"\n        Starts the loading update in a separate thread.\n        \"\"\"\n        threading.Thread(target=self.update_loading, args=(stop_flag,), daemon=True).start()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils.ProgressMessenger.send_initial_message","title":"<code>send_initial_message(mode='progress')</code>","text":"<p>Sends the initial message. Modes can be 'progress' or 'loading'.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/utils.py</code> <pre><code>def send_initial_message(self, mode: str = \"progress\"):\n    \"\"\"\n    Sends the initial message. Modes can be 'progress' or 'loading'.\n    \"\"\"\n    if mode == \"progress\":\n        emoji_legend = \"\\n\".join(\n            f\"{emoji} - Step {i + 1}\" for i, emoji in enumerate(self.emoji_set)\n        )\n        content = (\n            \"Progress is being updated in real-time!\\n\\n\"\n            \"Legend:\\n\"\n            f\"{emoji_legend}\\n\\n\"\n            \"Stay tuned for updates!\"\n        )\n    elif mode == \"loading\":\n        content = (\n            \"Loading in progress! \ud83c\udf00\\n\"\n            \"The indicator will loop until work is done.\"\n        )\n    else:\n        raise ValueError(\"Invalid mode. Use 'progress' or 'loading'.\")\n\n    if self.content is not None:\n        content += '\\n'+self.content\n    message = self.messenger.create_message(content=content, to=self.recipient_phone)\n    response = message.send(sender=0)\n    self.message_id = response.get(\"messages\", [{}])[0].get(\"id\")\n    logging.info(f\"Initial message sent: {content}\")\n    return self.message_id\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils.ProgressMessenger.start_loading_in_background","title":"<code>start_loading_in_background(stop_flag)</code>","text":"<p>Starts the loading update in a separate thread.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/utils.py</code> <pre><code>def start_loading_in_background(self, stop_flag: threading.Event):\n    \"\"\"\n    Starts the loading update in a separate thread.\n    \"\"\"\n    threading.Thread(target=self.update_loading, args=(stop_flag,), daemon=True).start()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils.ProgressMessenger.start_progress_in_background","title":"<code>start_progress_in_background(step_flag)</code>","text":"<p>Starts the progress update in a separate thread.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/utils.py</code> <pre><code>def start_progress_in_background(self, step_flag):\n    \"\"\"\n    Starts the progress update in a separate thread.\n    \"\"\"\n    threading.Thread(target=self.update_progress, args=(step_flag, ), daemon=True).start()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils.ProgressMessenger.update_loading","title":"<code>update_loading(stop_flag)</code>","text":"<p>Continuously updates the reaction to represent a looping 'loading' indicator.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/utils.py</code> <pre><code>def update_loading(self, stop_flag: threading.Event):\n    \"\"\"\n    Continuously updates the reaction to represent a looping 'loading' indicator.\n    \"\"\"\n    if not self.message_id:\n        raise ValueError(\"Message ID not found. Ensure the initial message is sent first.\")\n    message = self.messenger.create_message(id=self.message_id, to=self.recipient_phone)\n    step = 0\n    while not stop_flag.is_set():\n        emoji = self.emoji_set[step % len(self.emoji_set)]\n        message.react(emoji)\n        logging.info(f\"Loading update: {emoji}\")\n        time.sleep(1)  # Faster updates for loading\n        step += 1\n    # Final acknowledgment\n    message.react(\"\u2705\")\n    logging.info(\"Loading completed with final acknowledgment.\")\n    message.reply(\"\u2705Done\u2705\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.WhatsAppTb.utils.ProgressMessenger.update_progress","title":"<code>update_progress(step_flag)</code>","text":"<p>Updates the reaction on the message to represent progress.</p> Source code in <code>toolboxv2/mods/WhatsAppTb/utils.py</code> <pre><code>def update_progress(self, step_flag: threading.Event):\n    \"\"\"\n    Updates the reaction on the message to represent progress.\n    \"\"\"\n    if not self.message_id:\n        raise ValueError(\"Message ID not found. Ensure the initial message is sent first.\")\n    message = self.messenger.create_message(id=self.message_id, to=self.recipient_phone)\n    for step in range(self.max_steps):\n        emoji = self.emoji_set[step % len(self.emoji_set)]\n        message.react(emoji)\n        logging.info(f\"Progress updated: Step {step + 1}/{self.max_steps} with emoji {emoji}\")\n        while not step_flag.is_set():\n            time.sleep(0.5)\n        step_flag.clear()\n    # Final acknowledgment\n    message.react(\"\ud83d\udc4d\")\n    logging.info(\"Progress completed with final acknowledgment.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.cli_functions","title":"<code>cli_functions</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.cli_functions.replace_bracketed_content","title":"<code>replace_bracketed_content(text, replacements, inlist=False)</code>","text":"<p>Ersetzt Inhalte in eckigen Klammern mit entsprechenden Werten aus einem W\u00f6rterbuch.</p> <p>:param text: Der zu verarbeitende Text als String. :param replacements: Ein W\u00f6rterbuch mit Schl\u00fcssel-Wert-Paaren f\u00fcr die Ersetzung. :return: Den modifizierten Text.</p> Source code in <code>toolboxv2/mods/cli_functions.py</code> <pre><code>def replace_bracketed_content(text, replacements, inlist=False):\n    \"\"\"\n    Ersetzt Inhalte in eckigen Klammern mit entsprechenden Werten aus einem W\u00f6rterbuch.\n\n    :param text: Der zu verarbeitende Text als String.\n    :param replacements: Ein W\u00f6rterbuch mit Schl\u00fcssel-Wert-Paaren f\u00fcr die Ersetzung.\n    :return: Den modifizierten Text.\n    \"\"\"\n    # Finde alle Vorkommen von Texten in eckigen Klammern\n    matches = re.findall(r'\\[([^\\]]+)\\]', text)\n\n    # Ersetze jeden gefundenen Text durch den entsprechenden Wert aus dem W\u00f6rterbuch\n    as_list = text.split(' ')\n    i = 0\n    for key in matches:\n        if key in replacements:\n            if not inlist:\n                text = text.replace(f'[{key}]', str(replacements[key]))\n            else:\n                as_list[i] = replacements[key]\n        i += 1\n    if not inlist:\n        return text\n    return as_list\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.helper","title":"<code>helper</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.helper.create_invitation","title":"<code>create_invitation(app, username)</code>","text":"<p>Creates a one-time invitation code for a user to link a new device.</p> Source code in <code>toolboxv2/mods/helper.py</code> <pre><code>@export(mod_name=Name, name=\"create-invitation\", test=False)\ndef create_invitation(app: App, username: str):\n    \"\"\"Creates a one-time invitation code for a user to link a new device.\"\"\"\n    print(f\"Creating invitation for user '{username}'...\")\n    result = app.run_any(TBEF.CLOUDM_AUTHMANAGER.GET_INVITATION,\n                         get_results=True,\n                         username=username)\n\n    if result.is_ok():\n        print(f\"\u2705 Invitation code for '{username}': {result.get()}\")\n    else:\n        print(f\"\u274c Error creating invitation:\")\n        result.print()\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.helper.create_user","title":"<code>create_user(app, username, email)</code>","text":"<p>Creates a new user with a generated key pair.</p> Source code in <code>toolboxv2/mods/helper.py</code> <pre><code>@export(mod_name=Name, name=\"create-user\", test=False)\ndef create_user(app: App, username: str, email: str):\n    \"\"\"Creates a new user with a generated key pair.\"\"\"\n    print(f\"Creating user '{username}' with email '{email}'...\")\n\n    # Generate an invitation on the fly\n    invitation_res = app.run_any(TBEF.CLOUDM_AUTHMANAGER.GET_INVITATION,\n                                 get_results=True,\n                                 username=username)\n    if invitation_res.is_error():\n        print(\"\u274c Error creating invitation:\")\n        invitation_res.print()\n        return invitation_res\n\n    result = app.run_any(TBEF.CLOUDM_AUTHMANAGER.CRATE_LOCAL_ACCOUNT,\n                         get_results=True,\n                         username=username,\n                         email=email,\n                         invitation=invitation_res.get(),\n                         create=True)\n\n    if result.is_ok():\n        print(f\"\u2705 User '{username}' created successfully.\")\n    else:\n        print(f\"\u274c Error creating user:\")\n        result.print()\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.helper.delete_user_cli","title":"<code>delete_user_cli(app, username)</code>","text":"<p>Deletes a user and all their associated data.</p> Source code in <code>toolboxv2/mods/helper.py</code> <pre><code>@export(mod_name=Name, name=\"delete-user\", test=False)\ndef delete_user_cli(app: App, username: str):\n    \"\"\"Deletes a user and all their associated data.\"\"\"\n    print(f\"Attempting to delete user '{username}'...\")\n    auth_manager = app.get_mod('CloudM.AuthManager')\n    if not auth_manager:\n        return Result.default_internal_error(\"Could not load AuthManager module.\")\n\n    result = auth_manager.delete_user(username=username)\n\n    if result.is_ok():\n        print(f\"\u2705 User '{username}' has been deleted.\")\n    else:\n        print(f\"\u274c Error deleting user: {result.info.get('help_text')}\")\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.helper.init_system","title":"<code>init_system(app)</code>  <code>async</code>","text":"<p>Initializes the ToolBoxV2 system by creating the first administrative user. This is an interactive command.</p> Source code in <code>toolboxv2/mods/helper.py</code> <pre><code>@export(mod_name=Name, name=\"init_system\", test=False)\nasync def init_system(app: App):\n    \"\"\"\n    Initializes the ToolBoxV2 system by creating the first administrative user.\n    This is an interactive command.\n    \"\"\"\n    print(\"--- ToolBoxV2 System Initialization ---\")\n    print(\"This will guide you through creating the first administrator account.\")\n    print(\"This account will have the highest permission level.\\n\")\n\n    try:\n        username = input(\"Enter the administrator's username: \").strip()\n        if not username:\n            print(\"Username cannot be empty.\")\n            return Result.default_user_error(\"Username cannot be empty.\")\n\n        email = input(f\"Enter the email for '{username}': \").strip()\n        if not email: # A simple check, can be improved with regex\n            print(\"Email cannot be empty.\")\n            return Result.default_user_error(\"Email cannot be empty.\")\n\n        print(f\"\\nCreating user '{username}' with email '{email}'...\")\n        # Call the internal function to create the account\n        # The 'create=True' flag likely handles the initial key generation\n        result = await app.a_run_any(TBEF.CLOUDM.REGISTER_INITIAL_LOOT_USER,\n                                     user_name=username,\n                                     email=email,\n                                     get_results=True)\n\n        if result.is_ok():\n            print(\"\\n\u2705 Administrator account created successfully!\")\n            print(\"   A new cryptographic key pair has been generated for this user.\")\n            print(\"   Authentication is handled automatically using these keys.\")\n            print(\"   You can now use other CLI commands or log into the web UI.\")\n            return Result.ok(\"System initialized successfully.\")\n        else:\n            print(f\"\\n\u274c Error creating administrator account:\")\n            result.print()\n            return result\n\n    except (KeyboardInterrupt, EOFError):\n        print(\"\\n\\nInitialization cancelled by user.\")\n        return Result.default_user_error(\"Initialization cancelled.\")\n    except Exception as e:\n        print(f\"\\nAn unexpected error occurred: {e}\")\n        return Result.default_internal_error(f\"An unexpected error occurred: {e}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.helper.list_users_cli","title":"<code>list_users_cli(app)</code>","text":"<p>Lists all registered users.</p> Source code in <code>toolboxv2/mods/helper.py</code> <pre><code>@export(mod_name=Name, name=\"list-users\", test=False)\ndef list_users_cli(app: App):\n    \"\"\"Lists all registered users.\"\"\"\n    print(\"Fetching user list...\")\n    auth_manager = app.get_mod('CloudM.AuthManager')\n    if not auth_manager:\n        return Result.default_internal_error(\"Could not load AuthManager module.\")\n\n    result = auth_manager.list_users()\n\n    if result.is_ok():\n        users = result.get()\n        if not users:\n            print(\"No users found.\")\n            return result\n\n        print(\"--- Registered Users ---\")\n        # Simple table formatting\n        print(f\"{'Username':&lt;25} {'Email':&lt;30} {'Level'}\")\n        print(\"------------------------\")\n        for user in users:\n            print(f\"{user['username']:&lt;25} {user['email']:&lt;30} {user['level']}\")\n        print(\"------------------------\")\n    else:\n        print(f\"\u274c Error listing users: {result.info.get('help_text')}\")\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.helper.send_magic_link","title":"<code>send_magic_link(app, username)</code>","text":"<p>Sends a magic login link to the user's registered email address.</p> Source code in <code>toolboxv2/mods/helper.py</code> <pre><code>@export(mod_name=Name, name=\"send-magic-link\", test=False)\ndef send_magic_link(app: App, username: str):\n    \"\"\"Sends a magic login link to the user's registered email address.\"\"\"\n    print(f\"Sending magic link to user '{username}'...\")\n    result = app.run_any(TBEF.CLOUDM_AUTHMANAGER.GET_MAGIC_LINK_EMAIL,\n                         get_results=True,\n                         username=username)\n\n    if result.is_ok():\n        print(f\"\u2705 Magic link sent successfully to the email address associated with '{username}'.\")\n    else:\n        print(f\"\u274c Error sending magic link:\")\n        result.print()\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa","title":"<code>isaa</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent","title":"<code>CodingAgent</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live","title":"<code>live</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.AsyncCodeDetector","title":"<code>AsyncCodeDetector</code>","text":"<p>               Bases: <code>NodeVisitor</code></p> <p>Detect async code and top-level await</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class AsyncCodeDetector(ast.NodeVisitor):\n    \"\"\"Detect async code and top-level await\"\"\"\n    def __init__(self):\n        self.has_async = False\n        self.has_top_level_await = False\n        self.await_nodes = []\n\n    def visit_AsyncFunctionDef(self, node):\n        self.has_async = True\n        self.generic_visit(node)\n\n    def visit_Await(self, node):\n        self.has_async = True\n        # Track all await nodes\n        self.await_nodes.append(node)\n        # Check if this await is at top level\n        parent = node\n        while hasattr(parent, 'parent'):\n            parent = parent.parent\n            if isinstance(parent, ast.AsyncFunctionDef | ast.FunctionDef):\n                break\n        else:\n            self.has_top_level_await = True\n        self.generic_visit(node)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.BrowserWrapper","title":"<code>BrowserWrapper</code>","text":"<p>A wrapper for browser agent functionality that allows seamless interaction with web browsers.</p> <p>This class provides a system-agnostic interface to control browsers through the browser_use library, supporting both local and remote browser connections.</p> <p>Attributes:</p> Name Type Description <code>browser</code> <p>The Browser instance for web automation</p> <code>agent</code> <p>The BrowserAgent instance for intelligent browsing</p> <code>is_initialized</code> <code>bool</code> <p>Whether the browser has been initialized</p> <code>config</code> <code>Dict</code> <p>Configuration for the browser</p> <code>remote_url</code> <code>Optional[str]</code> <p>URL for remote browser connection if applicable</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class BrowserWrapper:\n    \"\"\"\n    A wrapper for browser agent functionality that allows seamless interaction with web browsers.\n\n    This class provides a system-agnostic interface to control browsers through the browser_use\n    library, supporting both local and remote browser connections.\n\n    Attributes:\n        browser: The Browser instance for web automation\n        agent: The BrowserAgent instance for intelligent browsing\n        is_initialized (bool): Whether the browser has been initialized\n        config (Dict): Configuration for the browser\n        remote_url (Optional[str]): URL for remote browser connection if applicable\n    \"\"\"\n\n    def __init__(self,\n                 llm: Any = None,\n                 headless: bool = False,\n                 chrome_path: str | None = None,\n                 remote_url: str | None = None,\n                 api_key: str | None=None,\n                 config: dict[str, Any] | None = None):\n        \"\"\"\n        Initialize the browser wrapper.\n\n        Args:\n            llm: Language model to use for the browser agent\n            headless: Whether to run the browser in headless mode\n            chrome_path: Path to local Chrome executable\n            remote_url: URL for remote browser connection (wss or cdp)\n            config: Additional browser configuration\n        \"\"\"\n        self.is_initialized = False\n        self.agent = None\n        self.browser = None\n        self.context = None\n\n        self.llm = llm\n        model, provider = None, None\n        if isinstance(llm, str):\n            if llm.count('/') == 2 and llm.startswith('openrouter/'):\n                provider = 'openrouter'\n                model = llm.split('/', 1)[1]\n            else:\n                provider, model = llm.split('/')\n        self._initialize_llm(model or \"claude-3-7-sonnet-latest\", provider or \"anthropic\")\n        self.parser = None\n\n        browser_config = {\n            'headless': headless,\n            'disable_security': True\n        }\n\n        if config:\n            browser_config.update(config)\n\n        self.config = browser_config\n\n        # Set up remote connection if specified\n        if remote_url:\n            if remote_url.startswith('wss://'):\n                self.config['wss_url'] = remote_url\n            elif remote_url.startswith('http'):\n                self.config['cdp_url'] = remote_url\n            self.remote_url = remote_url\n        else:\n            self.remote_url = None\n\n        # Set up local Chrome path if specified\n        if not headless and remote_url is None and chrome_path is None:\n            import os\n            import platform\n\n            def get_chrome_path():\n                \"\"\"\n                Returns the correct path to the Chrome executable based on the OS.\n                If Chrome is not found, returns None.\n                \"\"\"\n                chrome_paths = {\n                    \"Darwin\": \"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\",  # macOS\n                    \"Windows\": \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\",  # Windows\n                    \"Linux\": \"/usr/bin/google-chrome\"  # Linux\n                }\n\n                system = platform.system()\n                chrome_path_ = chrome_paths.get(system)\n\n                if chrome_path_ and os.path.isfile(chrome_path_):\n                    return chrome_path_\n\n                return None\n\n            chrome_path = get_chrome_path()\n        if chrome_path:\n            self.config['chrome_instance_path'] = chrome_path\n\n    def _initialize_llm(self, model: str, provider: str):\n        provider_key = provider.lower()\n        cls = _MODEL_MAP.get(provider_key)\n        if cls is None:\n            raise ValueError(f\"Unbekannter LLM\u2011Provider: {provider}\")\n        # optionale Parameter wie temperature k\u00f6nnen Sie ggf. \u00fcber config reinsteuern\n        self.llm = cls(model=model)\n        # Hinweis: browser-use liest den API-Key standardm\u00e4\u00dfig via Umgebungsvariablen ein\n        # z.B. OPENAI_API_KEY, ANTHROPIC_API_KEY oder GOOGLE_API_KEY (f\u00fcr Gemini) :contentReference[oaicite:6]{index=6}\n        self.model_name = model\n        self.provider = provider_key\n\n    async def initialize(self):\n        \"\"\"Initialize the browser and context\"\"\"\n        if self.is_initialized:\n            return\n\n        try:\n            # Create browser instance\n            self.browser = Browser(\n                config=BrowserConfig(**self.config)\n            )\n\n            # Create context configuration with better settings for scraping\n            context_config = BrowserContextConfig(\n                wait_for_network_idle_page_load_time=3.0,\n                highlight_elements=True,\n                viewport_expansion=500,\n                wait_between_actions=0.5  # Add a small delay between actions\n            )\n\n            # Initialize context\n            self.context = await self.browser.new_context(config=context_config)\n\n            # Create an initial page\n            browser_state = self.context\n            if not browser_state or not browser_state.tabs:\n                # If no tabs exist, create a new page\n                self.page = await self.context.new_tab()\n            else:\n                # Use the existing active tab\n                self.page = await self.context.get_current_page()\n\n            self.is_initialized = True\n\n        except Exception as e:\n            # Clean up resources in case of initialization error\n            if self.context:\n                await self.context.close()\n            if self.browser:\n                await self.browser.close()\n            raise Exception(f\"Failed to initialize browser: {str(e)}\")\n\n    async def create_agent(self, task: str, initial_actions=None):\n        \"\"\"Create a browser agent with the specified task\"\"\"\n        #if not self.is_initialized:\n        #    await self.initialize()\n\n        self.agent = BrowserAgent(\n            task=task,\n            llm=self.llm,\n            #browser_context=self.context,\n            initial_actions=initial_actions,\n            #browser=self.browser,\n        )\n        return self.agent\n\n    async def run(self, task: str):\n        \"\"\"Run the browser agent with the specified task\"\"\"\n        agent = await self.create_agent(task)\n        result = await agent.run()\n        return result\n\n    async def navigate(self, url: str):\n        \"\"\"Navigate to a URL\"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        # Get the current active page or create a new one if needed\n        try:\n            page = await self.context.get_current_page()\n            if not page:\n                page = await self.context.new_page()\n\n            # Navigate to the URL\n            await page.goto(url)\n            self.page = page\n            return page\n        except Exception as e:\n            raise Exception(f\"Failed to navigate to {url}: {str(e)}\")\n\n    async def get_tabs(self):\n        \"\"\"Get all open tabs/pages\"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        browser_state = await self.context.get_state()\n        return browser_state.tabs if browser_state else []\n\n    async def switch_to_tab(self, tab_index: int):\n        \"\"\"Switch to a specific tab by index\"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        browser_state = await self.context.get_state()\n        if not browser_state or not browser_state.tabs or tab_index &gt;= len(browser_state.tabs):\n            raise ValueError(f\"Tab index {tab_index} is out of range\")\n\n        tab_id = browser_state.tabs[tab_index].id\n        await self.context.switch_to_tab(tab_id)\n        self.page = await self.context.get_current_page()\n        return self.page\n\n    async def create_new_tab(self):\n        \"\"\"Create a new tab/page\"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        browser_context = await self.context.get_playwright_context()\n        new_page = await browser_context.new_page()\n        self.page = new_page\n        return new_page\n\n    async def close_current_tab(self):\n        \"\"\"Close the current tab/page\"\"\"\n        if not self.is_initialized:\n            return\n\n        page = await self.context.get_current_page()\n        if page:\n            await page.close()\n\n        # Update the current page reference\n        browser_state = await self.context.get_state()\n        if browser_state and browser_state.tabs:\n            await self.switch_to_tab(0)\n\n    async def execute_js(self, code: str, page=None):\n        \"\"\"Execute JavaScript code in the browser context\"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        if page is None:\n            pages = await self.context.pages()\n            if not pages:\n                page = await self.context.new_page()\n            else:\n                page = pages[0]\n\n        result = await page.evaluate(code)\n        return result\n\n    async def save_context(self):\n        \"\"\"Save browser context state\"\"\"\n        if not self.is_initialized:\n            return None\n\n        return await self.browser.export_context(self.context)\n\n    async def restore_context(self, context_data):\n        \"\"\"Restore browser context from saved state\"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        await self.browser.import_context(context_data)\n\n    async def close(self):\n        \"\"\"Close the browser\"\"\"\n        if self.is_initialized and self.browser:\n            await self.browser.close()\n            self.is_initialized = False\n\n    # Add these methods to the BrowserWrapper class\n\n    def get_parser(self):\n        \"\"\"Get a content parser for the browser\"\"\"\n        if self.parser is None:\n            self.parser = WebContentParser(self)\n        return self.parser\n\n    async def extract_markdown(self, page=None, selector=\"body\", include_images=True):\n        \"\"\"\n        Extract content from a webpage and convert it to markdown.\n        \"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        if page is None:\n            pages = await self.context.pages()\n            if not pages:\n                page = await self.context.new_page()\n            else:\n                page = pages[0]\n\n        # JavaScript to convert HTML to markdown\n        script = \"\"\"\n        (selector, includeImages) =&gt; {\n            const element = document.querySelector(selector);\n            if (!element) return '';\n\n            // Simple HTML to Markdown conversion function\n            const htmlToMarkdown = (node) =&gt; {\n                let result = '';\n\n                // Process text nodes\n                if (node.nodeType === Node.TEXT_NODE) {\n                    return node.textContent;\n                }\n\n                // Process element nodes\n                if (node.nodeType === Node.ELEMENT_NODE) {\n                    const tagName = node.tagName.toLowerCase();\n\n                    // Process by tag type\n                    switch(tagName) {\n                        case 'h1': return '# ' + getInnerText(node) + '\\\\n\\\\n';\n                        case 'h2': return '## ' + getInnerText(node) + '\\\\n\\\\n';\n                        case 'h3': return '### ' + getInnerText(node) + '\\\\n\\\\n';\n                        case 'h4': return '#### ' + getInnerText(node) + '\\\\n\\\\n';\n                        case 'h5': return '##### ' + getInnerText(node) + '\\\\n\\\\n';\n                        case 'h6': return '###### ' + getInnerText(node) + '\\\\n\\\\n';\n                        case 'p': return getInnerText(node) + '\\\\n\\\\n';\n                        case 'br': return '\\\\n';\n                        case 'hr': return '---\\\\n\\\\n';\n                        case 'b':\n                        case 'strong': return '**' + getInnerText(node) + '**';\n                        case 'i':\n                        case 'em': return '*' + getInnerText(node) + '*';\n                        case 'a': {\n                            const href = node.getAttribute('href');\n                            return '[' + getInnerText(node) + '](' + href + ')';\n                        }\n                        case 'img': {\n                            if (!includeImages) return '';\n                            const src = node.getAttribute('src');\n                            const alt = node.getAttribute('alt') || 'image';\n                            return '![' + alt + '](' + src + ')\\\\n\\\\n';\n                        }\n                        case 'code':\n                        case 'pre': return '`' + getInnerText(node) + '`';\n                        case 'ul': {\n                            let listResult = '\\\\n';\n                            Array.from(node.children).forEach(li =&gt; {\n                                if (li.tagName.toLowerCase() === 'li') {\n                                    listResult += '- ' + getInnerText(li) + '\\\\n';\n                                }\n                            });\n                            return listResult + '\\\\n';\n                        }\n                        case 'ol': {\n                            let listResult = '\\\\n';\n                            Array.from(node.children).forEach((li, index) =&gt; {\n                                if (li.tagName.toLowerCase() === 'li') {\n                                    listResult += (index + 1) + '. ' + getInnerText(li) + '\\\\n';\n                                }\n                            });\n                            return listResult + '\\\\n';\n                        }\n                        case 'blockquote': return '&gt; ' + getInnerText(node) + '\\\\n\\\\n';\n                        default: {\n                            // Process child nodes for other elements\n                            for (const child of node.childNodes) {\n                                result += htmlToMarkdown(child);\n                            }\n                            return result;\n                        }\n                    }\n                }\n\n                return '';\n            };\n\n            // Helper function to get inner text with special handling\n            const getInnerText = (node) =&gt; {\n                let text = '';\n                for (const child of node.childNodes) {\n                    text += htmlToMarkdown(child);\n                }\n                return text;\n            };\n\n            return htmlToMarkdown(element);\n        }\n        \"\"\"\n\n        try:\n            # Try to convert to markdown using our script\n            markdown = await page.evaluate(script, selector, include_images)\n\n            # Add a title if we have one\n            title = await page.title()\n            if title and not markdown.startswith(\"# \"):\n                markdown = f\"# {title}\\n\\n{markdown}\"\n\n            return markdown\n        except Exception:\n            # Fallback to basic extraction if script fails\n            content = await self.extract_text(page, selector)\n            title = await page.title()\n            return f\"# {title}\\n\\n{content}\"\n\n    async def take_scrolling_screenshot(self, page=None, full_page=True, path=None,\n                                        initial_delay=1000, scroll_delay=500, format='png'):\n        \"\"\"\n        Take a screenshot with scrolling functionality and delay.\n        \"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        if page is None:\n            pages = await self.context.pages()\n            if not pages:\n                page = await self.context.new_page()\n            else:\n                page = pages[0]\n\n        # Wait for the initial delay to let content load\n        if initial_delay &gt; 0:\n            await page.wait_for_timeout(initial_delay)\n\n        if full_page and scroll_delay &gt; 0:\n            # Get page dimensions\n            dimensions = await page.evaluate(\"\"\"\n                () =&gt; {\n                    return {\n                        width: document.documentElement.scrollWidth,\n                        height: document.documentElement.scrollHeight,\n                        windowHeight: window.innerHeight\n                    }\n                }\n            \"\"\")\n\n            # Scroll down the page gradually to trigger lazy loading\n            current_position = 0\n            while current_position &lt; dimensions['height']:\n                await page.evaluate(f\"window.scrollTo(0, {current_position})\")\n                await page.wait_for_timeout(scroll_delay)\n                current_position += dimensions['windowHeight'] // 2  # Scroll by half viewport\n\n        # Reset scroll position to top\n        await page.evaluate(\"window.scrollTo(0, 0)\")\n\n        # Take the screenshot\n        screenshot_params = {\n            'full_page': full_page,\n            'type': format\n        }\n\n        if path:\n            screenshot_params['path'] = path\n\n        return await page.screenshot(**screenshot_params)\n\n    async def extract_text(self, page=None, selector=\"body\"):\n        \"\"\"\n        Extract plain text from a webpage.\n        \"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        if page is None:\n            pages = await self.context.pages()\n            if not pages:\n                page = await self.context.new_page()\n            else:\n                page = pages[0]\n\n        text = await page.evaluate(\"\"\"\n            (selector) =&gt; {\n                const element = document.querySelector(selector);\n                return element ? element.innerText : '';\n            }\n        \"\"\", selector)\n\n        return text\n\n    async def extract_structured_content(self, page=None, config=None):\n        \"\"\"\n        Extract structured content from a webpage based on a configuration.\n        \"\"\"\n        if not self.is_initialized:\n            await self.initialize()\n\n        if page is None:\n            pages = await self.context.pages()\n            if not pages:\n                page = await self.context.new_page()\n            else:\n                page = pages[0]\n\n        if not config:\n            # Default configuration if none provided\n            config = {\n                'title': 'h1',\n                'headings': 'h2, h3, h4, h5, h6',\n                'paragraphs': 'p',\n                'links': 'a',\n                'images': 'img'\n            }\n\n        result = {}\n\n        for key, selector in config.items():\n            if key == 'links':\n                # Extract links with their href and text\n                result[key] = await page.evaluate(\"\"\"\n                    (selector) =&gt; {\n                        return Array.from(document.querySelectorAll(selector))\n                            .map(el =&gt; ({\n                                text: el.innerText.trim(),\n                                href: el.href\n                            }))\n                            .filter(item =&gt; item.text &amp;&amp; item.href);\n                    }\n                \"\"\", selector)\n            elif key == 'images':\n                # Extract images with their src and alt\n                result[key] = await page.evaluate(\"\"\"\n                    (selector) =&gt; {\n                        return Array.from(document.querySelectorAll(selector))\n                            .map(el =&gt; ({\n                                src: el.src,\n                                alt: el.alt || ''\n                            }))\n                            .filter(item =&gt; item.src);\n                    }\n                \"\"\", selector)\n            else:\n                # Extract text content for other elements\n                result[key] = await page.evaluate(\"\"\"\n                    (selector) =&gt; {\n                        return Array.from(document.querySelectorAll(selector))\n                            .map(el =&gt; el.innerText.trim())\n                            .filter(text =&gt; text);\n                    }\n                \"\"\", selector)\n\n        return result\n</code></pre> <code>__init__(llm=None, headless=False, chrome_path=None, remote_url=None, api_key=None, config=None)</code> \u00b6 <p>Initialize the browser wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>Any</code> <p>Language model to use for the browser agent</p> <code>None</code> <code>headless</code> <code>bool</code> <p>Whether to run the browser in headless mode</p> <code>False</code> <code>chrome_path</code> <code>str | None</code> <p>Path to local Chrome executable</p> <code>None</code> <code>remote_url</code> <code>str | None</code> <p>URL for remote browser connection (wss or cdp)</p> <code>None</code> <code>config</code> <code>dict[str, Any] | None</code> <p>Additional browser configuration</p> <code>None</code> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __init__(self,\n             llm: Any = None,\n             headless: bool = False,\n             chrome_path: str | None = None,\n             remote_url: str | None = None,\n             api_key: str | None=None,\n             config: dict[str, Any] | None = None):\n    \"\"\"\n    Initialize the browser wrapper.\n\n    Args:\n        llm: Language model to use for the browser agent\n        headless: Whether to run the browser in headless mode\n        chrome_path: Path to local Chrome executable\n        remote_url: URL for remote browser connection (wss or cdp)\n        config: Additional browser configuration\n    \"\"\"\n    self.is_initialized = False\n    self.agent = None\n    self.browser = None\n    self.context = None\n\n    self.llm = llm\n    model, provider = None, None\n    if isinstance(llm, str):\n        if llm.count('/') == 2 and llm.startswith('openrouter/'):\n            provider = 'openrouter'\n            model = llm.split('/', 1)[1]\n        else:\n            provider, model = llm.split('/')\n    self._initialize_llm(model or \"claude-3-7-sonnet-latest\", provider or \"anthropic\")\n    self.parser = None\n\n    browser_config = {\n        'headless': headless,\n        'disable_security': True\n    }\n\n    if config:\n        browser_config.update(config)\n\n    self.config = browser_config\n\n    # Set up remote connection if specified\n    if remote_url:\n        if remote_url.startswith('wss://'):\n            self.config['wss_url'] = remote_url\n        elif remote_url.startswith('http'):\n            self.config['cdp_url'] = remote_url\n        self.remote_url = remote_url\n    else:\n        self.remote_url = None\n\n    # Set up local Chrome path if specified\n    if not headless and remote_url is None and chrome_path is None:\n        import os\n        import platform\n\n        def get_chrome_path():\n            \"\"\"\n            Returns the correct path to the Chrome executable based on the OS.\n            If Chrome is not found, returns None.\n            \"\"\"\n            chrome_paths = {\n                \"Darwin\": \"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\",  # macOS\n                \"Windows\": \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\",  # Windows\n                \"Linux\": \"/usr/bin/google-chrome\"  # Linux\n            }\n\n            system = platform.system()\n            chrome_path_ = chrome_paths.get(system)\n\n            if chrome_path_ and os.path.isfile(chrome_path_):\n                return chrome_path_\n\n            return None\n\n        chrome_path = get_chrome_path()\n    if chrome_path:\n        self.config['chrome_instance_path'] = chrome_path\n</code></pre> <code>close()</code> <code>async</code> \u00b6 <p>Close the browser</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def close(self):\n    \"\"\"Close the browser\"\"\"\n    if self.is_initialized and self.browser:\n        await self.browser.close()\n        self.is_initialized = False\n</code></pre> <code>close_current_tab()</code> <code>async</code> \u00b6 <p>Close the current tab/page</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def close_current_tab(self):\n    \"\"\"Close the current tab/page\"\"\"\n    if not self.is_initialized:\n        return\n\n    page = await self.context.get_current_page()\n    if page:\n        await page.close()\n\n    # Update the current page reference\n    browser_state = await self.context.get_state()\n    if browser_state and browser_state.tabs:\n        await self.switch_to_tab(0)\n</code></pre> <code>create_agent(task, initial_actions=None)</code> <code>async</code> \u00b6 <p>Create a browser agent with the specified task</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def create_agent(self, task: str, initial_actions=None):\n    \"\"\"Create a browser agent with the specified task\"\"\"\n    #if not self.is_initialized:\n    #    await self.initialize()\n\n    self.agent = BrowserAgent(\n        task=task,\n        llm=self.llm,\n        #browser_context=self.context,\n        initial_actions=initial_actions,\n        #browser=self.browser,\n    )\n    return self.agent\n</code></pre> <code>create_new_tab()</code> <code>async</code> \u00b6 <p>Create a new tab/page</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def create_new_tab(self):\n    \"\"\"Create a new tab/page\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    browser_context = await self.context.get_playwright_context()\n    new_page = await browser_context.new_page()\n    self.page = new_page\n    return new_page\n</code></pre> <code>execute_js(code, page=None)</code> <code>async</code> \u00b6 <p>Execute JavaScript code in the browser context</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def execute_js(self, code: str, page=None):\n    \"\"\"Execute JavaScript code in the browser context\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    if page is None:\n        pages = await self.context.pages()\n        if not pages:\n            page = await self.context.new_page()\n        else:\n            page = pages[0]\n\n    result = await page.evaluate(code)\n    return result\n</code></pre> <code>extract_markdown(page=None, selector='body', include_images=True)</code> <code>async</code> \u00b6 <p>Extract content from a webpage and convert it to markdown.</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def extract_markdown(self, page=None, selector=\"body\", include_images=True):\n    \"\"\"\n    Extract content from a webpage and convert it to markdown.\n    \"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    if page is None:\n        pages = await self.context.pages()\n        if not pages:\n            page = await self.context.new_page()\n        else:\n            page = pages[0]\n\n    # JavaScript to convert HTML to markdown\n    script = \"\"\"\n    (selector, includeImages) =&gt; {\n        const element = document.querySelector(selector);\n        if (!element) return '';\n\n        // Simple HTML to Markdown conversion function\n        const htmlToMarkdown = (node) =&gt; {\n            let result = '';\n\n            // Process text nodes\n            if (node.nodeType === Node.TEXT_NODE) {\n                return node.textContent;\n            }\n\n            // Process element nodes\n            if (node.nodeType === Node.ELEMENT_NODE) {\n                const tagName = node.tagName.toLowerCase();\n\n                // Process by tag type\n                switch(tagName) {\n                    case 'h1': return '# ' + getInnerText(node) + '\\\\n\\\\n';\n                    case 'h2': return '## ' + getInnerText(node) + '\\\\n\\\\n';\n                    case 'h3': return '### ' + getInnerText(node) + '\\\\n\\\\n';\n                    case 'h4': return '#### ' + getInnerText(node) + '\\\\n\\\\n';\n                    case 'h5': return '##### ' + getInnerText(node) + '\\\\n\\\\n';\n                    case 'h6': return '###### ' + getInnerText(node) + '\\\\n\\\\n';\n                    case 'p': return getInnerText(node) + '\\\\n\\\\n';\n                    case 'br': return '\\\\n';\n                    case 'hr': return '---\\\\n\\\\n';\n                    case 'b':\n                    case 'strong': return '**' + getInnerText(node) + '**';\n                    case 'i':\n                    case 'em': return '*' + getInnerText(node) + '*';\n                    case 'a': {\n                        const href = node.getAttribute('href');\n                        return '[' + getInnerText(node) + '](' + href + ')';\n                    }\n                    case 'img': {\n                        if (!includeImages) return '';\n                        const src = node.getAttribute('src');\n                        const alt = node.getAttribute('alt') || 'image';\n                        return '![' + alt + '](' + src + ')\\\\n\\\\n';\n                    }\n                    case 'code':\n                    case 'pre': return '`' + getInnerText(node) + '`';\n                    case 'ul': {\n                        let listResult = '\\\\n';\n                        Array.from(node.children).forEach(li =&gt; {\n                            if (li.tagName.toLowerCase() === 'li') {\n                                listResult += '- ' + getInnerText(li) + '\\\\n';\n                            }\n                        });\n                        return listResult + '\\\\n';\n                    }\n                    case 'ol': {\n                        let listResult = '\\\\n';\n                        Array.from(node.children).forEach((li, index) =&gt; {\n                            if (li.tagName.toLowerCase() === 'li') {\n                                listResult += (index + 1) + '. ' + getInnerText(li) + '\\\\n';\n                            }\n                        });\n                        return listResult + '\\\\n';\n                    }\n                    case 'blockquote': return '&gt; ' + getInnerText(node) + '\\\\n\\\\n';\n                    default: {\n                        // Process child nodes for other elements\n                        for (const child of node.childNodes) {\n                            result += htmlToMarkdown(child);\n                        }\n                        return result;\n                    }\n                }\n            }\n\n            return '';\n        };\n\n        // Helper function to get inner text with special handling\n        const getInnerText = (node) =&gt; {\n            let text = '';\n            for (const child of node.childNodes) {\n                text += htmlToMarkdown(child);\n            }\n            return text;\n        };\n\n        return htmlToMarkdown(element);\n    }\n    \"\"\"\n\n    try:\n        # Try to convert to markdown using our script\n        markdown = await page.evaluate(script, selector, include_images)\n\n        # Add a title if we have one\n        title = await page.title()\n        if title and not markdown.startswith(\"# \"):\n            markdown = f\"# {title}\\n\\n{markdown}\"\n\n        return markdown\n    except Exception:\n        # Fallback to basic extraction if script fails\n        content = await self.extract_text(page, selector)\n        title = await page.title()\n        return f\"# {title}\\n\\n{content}\"\n</code></pre> <code>extract_structured_content(page=None, config=None)</code> <code>async</code> \u00b6 <p>Extract structured content from a webpage based on a configuration.</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def extract_structured_content(self, page=None, config=None):\n    \"\"\"\n    Extract structured content from a webpage based on a configuration.\n    \"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    if page is None:\n        pages = await self.context.pages()\n        if not pages:\n            page = await self.context.new_page()\n        else:\n            page = pages[0]\n\n    if not config:\n        # Default configuration if none provided\n        config = {\n            'title': 'h1',\n            'headings': 'h2, h3, h4, h5, h6',\n            'paragraphs': 'p',\n            'links': 'a',\n            'images': 'img'\n        }\n\n    result = {}\n\n    for key, selector in config.items():\n        if key == 'links':\n            # Extract links with their href and text\n            result[key] = await page.evaluate(\"\"\"\n                (selector) =&gt; {\n                    return Array.from(document.querySelectorAll(selector))\n                        .map(el =&gt; ({\n                            text: el.innerText.trim(),\n                            href: el.href\n                        }))\n                        .filter(item =&gt; item.text &amp;&amp; item.href);\n                }\n            \"\"\", selector)\n        elif key == 'images':\n            # Extract images with their src and alt\n            result[key] = await page.evaluate(\"\"\"\n                (selector) =&gt; {\n                    return Array.from(document.querySelectorAll(selector))\n                        .map(el =&gt; ({\n                            src: el.src,\n                            alt: el.alt || ''\n                        }))\n                        .filter(item =&gt; item.src);\n                }\n            \"\"\", selector)\n        else:\n            # Extract text content for other elements\n            result[key] = await page.evaluate(\"\"\"\n                (selector) =&gt; {\n                    return Array.from(document.querySelectorAll(selector))\n                        .map(el =&gt; el.innerText.trim())\n                        .filter(text =&gt; text);\n                }\n            \"\"\", selector)\n\n    return result\n</code></pre> <code>extract_text(page=None, selector='body')</code> <code>async</code> \u00b6 <p>Extract plain text from a webpage.</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def extract_text(self, page=None, selector=\"body\"):\n    \"\"\"\n    Extract plain text from a webpage.\n    \"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    if page is None:\n        pages = await self.context.pages()\n        if not pages:\n            page = await self.context.new_page()\n        else:\n            page = pages[0]\n\n    text = await page.evaluate(\"\"\"\n        (selector) =&gt; {\n            const element = document.querySelector(selector);\n            return element ? element.innerText : '';\n        }\n    \"\"\", selector)\n\n    return text\n</code></pre> <code>get_parser()</code> \u00b6 <p>Get a content parser for the browser</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def get_parser(self):\n    \"\"\"Get a content parser for the browser\"\"\"\n    if self.parser is None:\n        self.parser = WebContentParser(self)\n    return self.parser\n</code></pre> <code>get_tabs()</code> <code>async</code> \u00b6 <p>Get all open tabs/pages</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def get_tabs(self):\n    \"\"\"Get all open tabs/pages\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    browser_state = await self.context.get_state()\n    return browser_state.tabs if browser_state else []\n</code></pre> <code>initialize()</code> <code>async</code> \u00b6 <p>Initialize the browser and context</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def initialize(self):\n    \"\"\"Initialize the browser and context\"\"\"\n    if self.is_initialized:\n        return\n\n    try:\n        # Create browser instance\n        self.browser = Browser(\n            config=BrowserConfig(**self.config)\n        )\n\n        # Create context configuration with better settings for scraping\n        context_config = BrowserContextConfig(\n            wait_for_network_idle_page_load_time=3.0,\n            highlight_elements=True,\n            viewport_expansion=500,\n            wait_between_actions=0.5  # Add a small delay between actions\n        )\n\n        # Initialize context\n        self.context = await self.browser.new_context(config=context_config)\n\n        # Create an initial page\n        browser_state = self.context\n        if not browser_state or not browser_state.tabs:\n            # If no tabs exist, create a new page\n            self.page = await self.context.new_tab()\n        else:\n            # Use the existing active tab\n            self.page = await self.context.get_current_page()\n\n        self.is_initialized = True\n\n    except Exception as e:\n        # Clean up resources in case of initialization error\n        if self.context:\n            await self.context.close()\n        if self.browser:\n            await self.browser.close()\n        raise Exception(f\"Failed to initialize browser: {str(e)}\")\n</code></pre> <code>navigate(url)</code> <code>async</code> \u00b6 <p>Navigate to a URL</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def navigate(self, url: str):\n    \"\"\"Navigate to a URL\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    # Get the current active page or create a new one if needed\n    try:\n        page = await self.context.get_current_page()\n        if not page:\n            page = await self.context.new_page()\n\n        # Navigate to the URL\n        await page.goto(url)\n        self.page = page\n        return page\n    except Exception as e:\n        raise Exception(f\"Failed to navigate to {url}: {str(e)}\")\n</code></pre> <code>restore_context(context_data)</code> <code>async</code> \u00b6 <p>Restore browser context from saved state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def restore_context(self, context_data):\n    \"\"\"Restore browser context from saved state\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    await self.browser.import_context(context_data)\n</code></pre> <code>run(task)</code> <code>async</code> \u00b6 <p>Run the browser agent with the specified task</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def run(self, task: str):\n    \"\"\"Run the browser agent with the specified task\"\"\"\n    agent = await self.create_agent(task)\n    result = await agent.run()\n    return result\n</code></pre> <code>save_context()</code> <code>async</code> \u00b6 <p>Save browser context state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def save_context(self):\n    \"\"\"Save browser context state\"\"\"\n    if not self.is_initialized:\n        return None\n\n    return await self.browser.export_context(self.context)\n</code></pre> <code>switch_to_tab(tab_index)</code> <code>async</code> \u00b6 <p>Switch to a specific tab by index</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def switch_to_tab(self, tab_index: int):\n    \"\"\"Switch to a specific tab by index\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    browser_state = await self.context.get_state()\n    if not browser_state or not browser_state.tabs or tab_index &gt;= len(browser_state.tabs):\n        raise ValueError(f\"Tab index {tab_index} is out of range\")\n\n    tab_id = browser_state.tabs[tab_index].id\n    await self.context.switch_to_tab(tab_id)\n    self.page = await self.context.get_current_page()\n    return self.page\n</code></pre> <code>take_scrolling_screenshot(page=None, full_page=True, path=None, initial_delay=1000, scroll_delay=500, format='png')</code> <code>async</code> \u00b6 <p>Take a screenshot with scrolling functionality and delay.</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def take_scrolling_screenshot(self, page=None, full_page=True, path=None,\n                                    initial_delay=1000, scroll_delay=500, format='png'):\n    \"\"\"\n    Take a screenshot with scrolling functionality and delay.\n    \"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n\n    if page is None:\n        pages = await self.context.pages()\n        if not pages:\n            page = await self.context.new_page()\n        else:\n            page = pages[0]\n\n    # Wait for the initial delay to let content load\n    if initial_delay &gt; 0:\n        await page.wait_for_timeout(initial_delay)\n\n    if full_page and scroll_delay &gt; 0:\n        # Get page dimensions\n        dimensions = await page.evaluate(\"\"\"\n            () =&gt; {\n                return {\n                    width: document.documentElement.scrollWidth,\n                    height: document.documentElement.scrollHeight,\n                    windowHeight: window.innerHeight\n                }\n            }\n        \"\"\")\n\n        # Scroll down the page gradually to trigger lazy loading\n        current_position = 0\n        while current_position &lt; dimensions['height']:\n            await page.evaluate(f\"window.scrollTo(0, {current_position})\")\n            await page.wait_for_timeout(scroll_delay)\n            current_position += dimensions['windowHeight'] // 2  # Scroll by half viewport\n\n    # Reset scroll position to top\n    await page.evaluate(\"window.scrollTo(0, 0)\")\n\n    # Take the screenshot\n    screenshot_params = {\n        'full_page': full_page,\n        'type': format\n    }\n\n    if path:\n        screenshot_params['path'] = path\n\n    return await page.screenshot(**screenshot_params)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface","title":"<code>CargoRustInterface</code>","text":"<p>Usage :</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface--create-interface","title":"Create interface","text":"<p>cargo_interface = CargoRustInterface()</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface--set-up-new-project","title":"Set up new project","text":"<p>await cargo_interface.setup_project(\"hello_rust\")</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface--add-a-dependency","title":"Add a dependency","text":"<p>await cargo_interface.add_dependency(\"serde\", \"1.0\")</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface--write-and-run-some-code","title":"Write and run some code","text":"<p>code = \"\"\" fn main() {     println!(\"Hello, Rust!\"); } \"\"\" result = await cargo_interface.run_code(code)</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface--modify-code","title":"Modify code","text":"<p>new_function = \"\"\" fn main() {     println!(\"Modified Hello, Rust!\"); } \"\"\" await cargo_interface.modify_code(new_function, \"main()\")</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.CargoRustInterface--build-and-test","title":"Build and test","text":"<p>await cargo_interface.build() await cargo_interface.test()</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class CargoRustInterface:\n    '''Usage :\n# Create interface\ncargo_interface = CargoRustInterface()\n\n# Set up new project\nawait cargo_interface.setup_project(\"hello_rust\")\n\n# Add a dependency\nawait cargo_interface.add_dependency(\"serde\", \"1.0\")\n\n# Write and run some code\ncode = \"\"\"\nfn main() {\n    println!(\"Hello, Rust!\");\n}\n\"\"\"\nresult = await cargo_interface.run_code(code)\n\n# Modify code\nnew_function = \"\"\"\nfn main() {\n    println!(\"Modified Hello, Rust!\");\n}\n\"\"\"\nawait cargo_interface.modify_code(new_function, \"main()\")\n\n# Build and test\nawait cargo_interface.build()\nawait cargo_interface.test()\n\n    '''\n    def __init__(self, session_dir=None, auto_remove=True):\n        \"\"\"Initialize the Rust/Cargo interface\"\"\"\n        self.auto_remove = auto_remove\n        self._session_dir = session_dir or Path.home() / '.cargo_sessions'\n        self._session_dir.mkdir(exist_ok=True)\n        self.vfs = VirtualFileSystem(self._session_dir / 'virtual_fs')\n        self.output_history = {}\n        self._execution_count = 0\n        self.current_project = None\n\n    def reset(self):\n        \"\"\"Reset the interface state\"\"\"\n        if self.auto_remove and self.current_project:\n            shutil.rmtree(self.current_project, ignore_errors=True)\n        self.output_history.clear()\n        self._execution_count = 0\n        self.current_project = None\n\n    async def setup_project(self, name: str) -&gt; str:\n        \"\"\"Set up a new Cargo project\"\"\"\n        try:\n            project_path = self.vfs.base_dir / name\n            if project_path.exists():\n                shutil.rmtree(project_path)\n\n            result = subprocess.run(\n                ['cargo', 'new', str(project_path)],\n                capture_output=True,\n                text=True, check=True\n            )\n\n            if result.returncode != 0:\n                return f\"Error creating project: {result.stderr}\"\n\n            self.current_project = project_path\n            return f\"Created new project at {project_path}\"\n\n        except Exception as e:\n            return f\"Failed to create project: {str(e)}\"\n\n    async def add_dependency(self, name: str, version: str | None = None) -&gt; str:\n        \"\"\"Add a dependency to Cargo.toml\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            cargo_toml = self.current_project / \"Cargo.toml\"\n            if not cargo_toml.exists():\n                return \"Cargo.toml not found\"\n\n            cmd = ['cargo', 'add', name]\n            if version:\n                cmd.extend(['--vers', version])\n\n            result = subprocess.run(\n                cmd,\n                cwd=self.current_project,\n                capture_output=True,\n                text=True,check=True\n            )\n\n            return result.stdout if result.returncode == 0 else f\"Error: {result.stderr}\"\n\n        except Exception as e:\n            return f\"Failed to add dependency: {str(e)}\"\n\n    async def build(self, release: bool = False) -&gt; str:\n        \"\"\"Build the project\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            cmd = ['cargo', 'build']\n            if release:\n                cmd.append('--release')\n\n            result = subprocess.run(\n                cmd,\n                cwd=self.current_project,\n                capture_output=True,\n                text=True\n            )\n\n            return result.stdout if result.returncode == 0 else f\"Build error: {result.stderr}\"\n\n        except Exception as e:\n            return f\"Build failed: {str(e)}\"\n\n    async def test(self) -&gt; str:\n        \"\"\"Run project tests\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            result = subprocess.run(\n                ['cargo', 'test'],\n                cwd=self.current_project,\n                capture_output=True,\n                text=True, check=True\n            )\n\n            return result.stdout if result.returncode == 0 else f\"Test error: {result.stderr}\"\n\n        except Exception as e:\n            return f\"Tests failed: {str(e)}\"\n\n    async def run_code(self, code: str) -&gt; str:\n        \"\"\"Run Rust code\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            # Write code to main.rs\n            main_rs = self.current_project / \"src\" / \"main.rs\"\n            with open(main_rs, 'w') as f:\n                f.write(code)\n\n            # Build and run\n            build_result = subprocess.run(\n                ['cargo', 'build'],\n                cwd=self.current_project,\n                capture_output=True,\n                text=True\n            )\n\n            if build_result.returncode != 0:\n                return f\"Compilation error: {build_result.stderr}\"\n\n            run_result = subprocess.run(\n                ['cargo', 'run'],\n                cwd=self.current_project,\n                capture_output=True,\n                text=True\n            )\n\n            self._execution_count += 1\n            output = {\n                'code': code,\n                'stdout': run_result.stdout,\n                'stderr': run_result.stderr,\n                'result': run_result.returncode == 0\n            }\n            self.output_history[self._execution_count] = output\n\n            return run_result.stdout if run_result.returncode == 0 else f\"Runtime error: {run_result.stderr}\"\n\n        except Exception as e:\n            return f\"Execution failed: {str(e)}\"\n\n    async def modify_code(self, code: str, object_name: str, file: str = \"src/main.rs\") -&gt; str:\n        \"\"\"Modify existing Rust code\"\"\"\n        if not self.current_project:\n            return \"No active project\"\n\n        try:\n            file_path = self.current_project / file\n            if not file_path.exists():\n                return f\"File {file} not found\"\n\n            with open(file_path) as f:\n                content = f.read()\n\n            # Handle function modification\n            if object_name.endswith(\"()\"):\n                func_name = object_name[:-2]\n                # Find and replace function definition\n                pattern = f\"fn {func_name}.*?}}(?=\\n|$)\"\n                updated_content = re.sub(pattern, code.strip(), content, flags=re.DOTALL)\n            else:\n                # Handle other modifications (structs, constants, etc.)\n                pattern = f\"{object_name}.*?(?=\\n|$)\"\n                updated_content = re.sub(pattern, code.strip(), content)\n\n            with open(file_path, 'w') as f:\n                f.write(updated_content)\n\n            return f\"Modified {object_name} in {file}\"\n\n        except Exception as e:\n            return f\"Modification failed: {str(e)}\"\n\n    def save_session(self, name: str):\n        \"\"\"Save current session state\"\"\"\n        session_file = self._session_dir / f\"{name}.json\"\n        state = {\n            'output_history': self.output_history,\n            'current_project': str(self.current_project) if self.current_project else None\n        }\n\n        with open(session_file, 'w') as f:\n            json.dump(state, f)\n\n    def load_session(self, name: str):\n        \"\"\"Load saved session state\"\"\"\n        session_file = self._session_dir / f\"{name}.json\"\n        if session_file.exists():\n            with open(session_file) as f:\n                state = json.load(f)\n                self.output_history = state['output_history']\n                self.current_project = Path(state['current_project']) if state['current_project'] else None\n</code></pre> <code>__init__(session_dir=None, auto_remove=True)</code> \u00b6 <p>Initialize the Rust/Cargo interface</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __init__(self, session_dir=None, auto_remove=True):\n    \"\"\"Initialize the Rust/Cargo interface\"\"\"\n    self.auto_remove = auto_remove\n    self._session_dir = session_dir or Path.home() / '.cargo_sessions'\n    self._session_dir.mkdir(exist_ok=True)\n    self.vfs = VirtualFileSystem(self._session_dir / 'virtual_fs')\n    self.output_history = {}\n    self._execution_count = 0\n    self.current_project = None\n</code></pre> <code>add_dependency(name, version=None)</code> <code>async</code> \u00b6 <p>Add a dependency to Cargo.toml</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def add_dependency(self, name: str, version: str | None = None) -&gt; str:\n    \"\"\"Add a dependency to Cargo.toml\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        cargo_toml = self.current_project / \"Cargo.toml\"\n        if not cargo_toml.exists():\n            return \"Cargo.toml not found\"\n\n        cmd = ['cargo', 'add', name]\n        if version:\n            cmd.extend(['--vers', version])\n\n        result = subprocess.run(\n            cmd,\n            cwd=self.current_project,\n            capture_output=True,\n            text=True,check=True\n        )\n\n        return result.stdout if result.returncode == 0 else f\"Error: {result.stderr}\"\n\n    except Exception as e:\n        return f\"Failed to add dependency: {str(e)}\"\n</code></pre> <code>build(release=False)</code> <code>async</code> \u00b6 <p>Build the project</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def build(self, release: bool = False) -&gt; str:\n    \"\"\"Build the project\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        cmd = ['cargo', 'build']\n        if release:\n            cmd.append('--release')\n\n        result = subprocess.run(\n            cmd,\n            cwd=self.current_project,\n            capture_output=True,\n            text=True\n        )\n\n        return result.stdout if result.returncode == 0 else f\"Build error: {result.stderr}\"\n\n    except Exception as e:\n        return f\"Build failed: {str(e)}\"\n</code></pre> <code>load_session(name)</code> \u00b6 <p>Load saved session state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def load_session(self, name: str):\n    \"\"\"Load saved session state\"\"\"\n    session_file = self._session_dir / f\"{name}.json\"\n    if session_file.exists():\n        with open(session_file) as f:\n            state = json.load(f)\n            self.output_history = state['output_history']\n            self.current_project = Path(state['current_project']) if state['current_project'] else None\n</code></pre> <code>modify_code(code, object_name, file='src/main.rs')</code> <code>async</code> \u00b6 <p>Modify existing Rust code</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def modify_code(self, code: str, object_name: str, file: str = \"src/main.rs\") -&gt; str:\n    \"\"\"Modify existing Rust code\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        file_path = self.current_project / file\n        if not file_path.exists():\n            return f\"File {file} not found\"\n\n        with open(file_path) as f:\n            content = f.read()\n\n        # Handle function modification\n        if object_name.endswith(\"()\"):\n            func_name = object_name[:-2]\n            # Find and replace function definition\n            pattern = f\"fn {func_name}.*?}}(?=\\n|$)\"\n            updated_content = re.sub(pattern, code.strip(), content, flags=re.DOTALL)\n        else:\n            # Handle other modifications (structs, constants, etc.)\n            pattern = f\"{object_name}.*?(?=\\n|$)\"\n            updated_content = re.sub(pattern, code.strip(), content)\n\n        with open(file_path, 'w') as f:\n            f.write(updated_content)\n\n        return f\"Modified {object_name} in {file}\"\n\n    except Exception as e:\n        return f\"Modification failed: {str(e)}\"\n</code></pre> <code>reset()</code> \u00b6 <p>Reset the interface state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def reset(self):\n    \"\"\"Reset the interface state\"\"\"\n    if self.auto_remove and self.current_project:\n        shutil.rmtree(self.current_project, ignore_errors=True)\n    self.output_history.clear()\n    self._execution_count = 0\n    self.current_project = None\n</code></pre> <code>run_code(code)</code> <code>async</code> \u00b6 <p>Run Rust code</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def run_code(self, code: str) -&gt; str:\n    \"\"\"Run Rust code\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        # Write code to main.rs\n        main_rs = self.current_project / \"src\" / \"main.rs\"\n        with open(main_rs, 'w') as f:\n            f.write(code)\n\n        # Build and run\n        build_result = subprocess.run(\n            ['cargo', 'build'],\n            cwd=self.current_project,\n            capture_output=True,\n            text=True\n        )\n\n        if build_result.returncode != 0:\n            return f\"Compilation error: {build_result.stderr}\"\n\n        run_result = subprocess.run(\n            ['cargo', 'run'],\n            cwd=self.current_project,\n            capture_output=True,\n            text=True\n        )\n\n        self._execution_count += 1\n        output = {\n            'code': code,\n            'stdout': run_result.stdout,\n            'stderr': run_result.stderr,\n            'result': run_result.returncode == 0\n        }\n        self.output_history[self._execution_count] = output\n\n        return run_result.stdout if run_result.returncode == 0 else f\"Runtime error: {run_result.stderr}\"\n\n    except Exception as e:\n        return f\"Execution failed: {str(e)}\"\n</code></pre> <code>save_session(name)</code> \u00b6 <p>Save current session state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def save_session(self, name: str):\n    \"\"\"Save current session state\"\"\"\n    session_file = self._session_dir / f\"{name}.json\"\n    state = {\n        'output_history': self.output_history,\n        'current_project': str(self.current_project) if self.current_project else None\n    }\n\n    with open(session_file, 'w') as f:\n        json.dump(state, f)\n</code></pre> <code>setup_project(name)</code> <code>async</code> \u00b6 <p>Set up a new Cargo project</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def setup_project(self, name: str) -&gt; str:\n    \"\"\"Set up a new Cargo project\"\"\"\n    try:\n        project_path = self.vfs.base_dir / name\n        if project_path.exists():\n            shutil.rmtree(project_path)\n\n        result = subprocess.run(\n            ['cargo', 'new', str(project_path)],\n            capture_output=True,\n            text=True, check=True\n        )\n\n        if result.returncode != 0:\n            return f\"Error creating project: {result.stderr}\"\n\n        self.current_project = project_path\n        return f\"Created new project at {project_path}\"\n\n    except Exception as e:\n        return f\"Failed to create project: {str(e)}\"\n</code></pre> <code>test()</code> <code>async</code> \u00b6 <p>Run project tests</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def test(self) -&gt; str:\n    \"\"\"Run project tests\"\"\"\n    if not self.current_project:\n        return \"No active project\"\n\n    try:\n        result = subprocess.run(\n            ['cargo', 'test'],\n            cwd=self.current_project,\n            capture_output=True,\n            text=True, check=True\n        )\n\n        return result.stdout if result.returncode == 0 else f\"Test error: {result.stderr}\"\n\n    except Exception as e:\n        return f\"Tests failed: {str(e)}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.DynamicVerboseFormatter","title":"<code>DynamicVerboseFormatter</code>","text":"<p>Unified, dynamic formatter that adapts to screen size</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class DynamicVerboseFormatter:\n    \"\"\"Unified, dynamic formatter that adapts to screen size\"\"\"\n\n    def __init__(self, print_func=None, min_width: int = 40, max_width: int = 240):\n        self.style = Style()\n        self.print = print_func or print\n        self.min_width = min_width\n        self.max_width = max_width\n        self._terminal_width = self._get_terminal_width()\n\n\n    def get_git_info(self):\n        \"\"\"Checks for a git repo and returns its name and branch, or None.\"\"\"\n        try:\n            # Check if we are in a git repository\n            subprocess.check_output(['git', 'rev-parse', '--is-inside-work-tree'], stderr=subprocess.DEVNULL)\n\n            # Get the repo name (root folder name)\n            repo_root = subprocess.check_output(['git', 'rev-parse', '--show-toplevel'],\n                                                stderr=subprocess.DEVNULL).strip().decode('utf-8')\n            repo_name = os.path.basename(repo_root)\n\n            # Get the current branch name\n            branch = subprocess.check_output(['git', 'rev-parse', '--abbrev-ref', 'HEAD'],\n                                             stderr=subprocess.DEVNULL).strip().decode('utf-8')\n\n            return repo_name, branch\n        except (subprocess.CalledProcessError, FileNotFoundError):\n            # This handles cases where 'git' is not installed or it's not a git repo\n            return None\n\n    def _get_terminal_width(self) -&gt; int:\n        \"\"\"Get current terminal width with fallback\"\"\"\n        try:\n            width = shutil.get_terminal_size().columns\n            return max(self.min_width, min(width - 2, self.max_width))\n        except (OSError, AttributeError):\n            return 80\n\n    def _wrap_text(self, text: str, width: int = None) -&gt; list[str]:\n        \"\"\"Wrap text to fit terminal width\"\"\"\n        if width is None:\n            width = self._terminal_width - 4  # Account for borders\n\n        words = text.split()\n        lines = []\n        current_line = []\n        current_length = 0\n\n        for word in words:\n            if current_length + len(word) + len(current_line) &lt;= width:\n                current_line.append(word)\n                current_length += len(word)\n            else:\n                if current_line:\n                    lines.append(' '.join(current_line))\n                current_line = [word]\n                current_length = len(word)\n\n        if current_line:\n            lines.append(' '.join(current_line))\n\n        return lines\n\n    def _create_border(self, char: str = \"\u2500\", width: int = None) -&gt; str:\n        \"\"\"Create a border line that fits the terminal\"\"\"\n        if width is None:\n            width = self._terminal_width\n        return char * width\n\n    def _center_text(self, text: str, width: int = None) -&gt; str:\n        \"\"\"Center text within the given width\"\"\"\n        if width is None:\n            width = self._terminal_width\n\n        # Remove ANSI codes for length calculation\n        clean_text = self._strip_ansi(text)\n        padding = max(0, (width - len(clean_text)) // 2)\n        return \" \" * padding + text\n\n    def _strip_ansi(self, text: str) -&gt; str:\n        \"\"\"Remove ANSI escape codes for length calculation\"\"\"\n        import re\n        ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n        return ansi_escape.sub('', text)\n\n    def print_header(self, text: str):\n        \"\"\"Print a dynamic header that adapts to screen size\"\"\"\n        self._terminal_width = self._get_terminal_width()\n\n        if self._terminal_width &lt; 60:  # Tiny screen\n            self.print()\n            self.print(self.style.CYAN(\"=\" * self._terminal_width))\n            self.print(self.style.CYAN(self.style.Bold(text)))\n            self.print(self.style.CYAN(\"=\" * self._terminal_width))\n        else:  # Regular/large screen\n            border_width = min(len(text) + 2, self._terminal_width - 2)\n            border = \"\u2500\" * border_width\n\n            self.print()\n            self.print(self.style.CYAN(f\"\u250c{border}\u2510\"))\n            self.print(self.style.CYAN(f\"\u2502 {self.style.Bold(text).center(border_width - 2)} \u2502\"))\n            self.print(self.style.CYAN(f\"\u2514{border}\u2518\"))\n        self.print()\n\n    def print_section(self, title: str, content: str):\n        \"\"\"Print a clean section with adaptive formatting\"\"\"\n        self._terminal_width = self._get_terminal_width()\n\n        # Title\n        if self._terminal_width &lt; 60:\n            self.print(f\"\\n{self.style.BLUE('\u25cf')} {self.style.Bold(title)}\")\n        else:\n            self.print(f\"\\n{self.style.BLUE('\u25cf')} {self.style.Bold(self.style.BLUE(title))}\")\n\n        # Content with proper wrapping\n        for line in content.split('\\n'):\n            if line.strip():\n                wrapped_lines = self._wrap_text(line.strip())\n                for wrapped_line in wrapped_lines:\n                    if self._terminal_width &lt; 60:\n                        self.print(f\"  {wrapped_line}\")\n                    else:\n                        self.print(f\"  {self.style.GREY('\u2502')} {wrapped_line}\")\n        self.print()\n\n    def print_progress_bar(self, current: int, maximum: int, title: str = \"Progress\"):\n        \"\"\"Dynamic progress bar that adapts to screen size\"\"\"\n        self._terminal_width = self._get_terminal_width()\n\n        # Calculate bar width based on screen size\n        if self._terminal_width &lt; 60:\n            bar_width = 10\n            template = f\"\\r{title}: [{{}}] {current}/{maximum}\"\n        else:\n            bar_width = min(30, self._terminal_width - 30)\n            template = f\"\\r{self.style.CYAN(title)}: [{{}}] {current}/{maximum} ({current / maximum * 100:.1f}%)\"\n\n        progress = int((current / maximum) * bar_width)\n        bar = \"\u2588\" * progress + \"\u2591\" * (bar_width - progress)\n\n        self.print(template.format(bar), end='', flush=True)\n\n    def print_state(self, state: str, details: Dict[str, Any] = None) -&gt; str:\n        \"\"\"Print current state with adaptive formatting\"\"\"\n        self._terminal_width = self._get_terminal_width()\n\n        state_colors = {\n            'ACTION': self.style.GREEN2,\n            'PROCESSING': self.style.YELLOW2,\n            'BRAKE': self.style.RED2,\n            'DONE': self.style.BLUE2,\n            'ERROR': self.style.RED,\n            'SUCCESS': self.style.GREEN,\n            'INFO': self.style.CYAN\n        }\n\n        color_func = state_colors.get(state.upper(), self.style.WHITE2)\n\n        if self._terminal_width &lt; 60:\n            # Compact format for small screens\n            self.print(f\"\\n[{color_func(state)}]\")\n            result = f\"\\n[{state}]\"\n        else:\n            # Full format for larger screens\n            self.print(f\"\\n{self.style.Bold('State:')} {color_func(state)}\")\n            result = f\"\\nState: {state}\"\n\n        if details:\n            for key, value in details.items():\n                # Truncate long values on small screens\n                if self._terminal_width &lt; 60 and len(str(value)) &gt; 30:\n                    display_value = str(value)[:27] + \"...\"\n                else:\n                    display_value = str(value)\n\n                if self._terminal_width &lt; 60:\n                    self.print(f\"  {key}: {display_value}\")\n                    result += f\"\\n  {key}: {display_value}\"\n                else:\n                    self.print(f\"  {self.style.GREY('\u251c\u2500')} {self.style.CYAN(key)}: {display_value}\")\n                    result += f\"\\n  \u251c\u2500 {key}: {display_value}\"\n\n        return result\n\n    def print_code_block(self, code: str, language: str = \"python\"):\n        \"\"\"Print code with syntax awareness and proper formatting\"\"\"\n        self._terminal_width = self._get_terminal_width()\n\n        if self._terminal_width &lt; 60:\n            # Simple format for small screens\n            self.print(f\"\\n{self.style.GREY('Code:')}\")\n            for line in code.split('\\n'):\n                self.print(f\"  {line}\")\n        else:\n            # Detailed format for larger screens\n            self.print(f\"\\n{self.style.BLUE('\u250c\u2500')} {self.style.YELLOW2(f'{language.upper()} Code')}\")\n\n            lines = code.split('\\n')\n            for i, line in enumerate(lines):\n                if i == len(lines) - 1 and not line.strip():\n                    continue\n\n                # Wrap long lines\n                if len(line) &gt; self._terminal_width - 6:\n                    wrapped = self._wrap_text(line, self._terminal_width - 6)\n                    for j, wrapped_line in enumerate(wrapped):\n                        prefix = \"\u2502\" if j == 0 else \"\u2502\"\n                        self.print(f\"{self.style.BLUE(prefix)} {wrapped_line}\")\n                else:\n                    self.print(f\"{self.style.BLUE('\u2502')} {line}\")\n\n            self.print(f\"{self.style.BLUE('\u2514\u2500')} {self.style.GREY('End of code block')}\")\n\n    def print_table(self, headers: list[str], rows: list[list[str]]):\n        \"\"\"Print a dynamic table that adapts to screen size\"\"\"\n        self._terminal_width = self._get_terminal_width()\n\n        if not rows:\n            return\n\n        # Calculate column widths\n        all_data = [headers] + rows\n        col_widths = []\n\n        for col in range(len(headers)):\n            max_width = max(len(str(row[col])) for row in all_data if col &lt; len(row))\n            col_widths.append(min(max_width, self._terminal_width // len(headers) - 2))\n\n        # Adjust if total width exceeds terminal\n        total_width = sum(col_widths) + len(headers) * 3 + 1\n        if total_width &gt; self._terminal_width:\n            # Proportionally reduce column widths\n            scale_factor = (self._terminal_width - len(headers) * 3 - 1) / sum(col_widths)\n            col_widths = [max(8, int(w * scale_factor)) for w in col_widths]\n\n        # Print table\n        self._print_table_row(headers, col_widths, is_header=True)\n        self._print_table_separator(col_widths)\n\n        for row in rows:\n            self._print_table_row(row, col_widths)\n\n    def _print_table_row(self, row: list[str], widths: list[int], is_header: bool = False):\n        \"\"\"Helper method to print a table row\"\"\"\n        formatted_cells = []\n        for i, (cell, width) in enumerate(zip(row, widths)):\n            cell_str = str(cell)\n            if len(cell_str) &gt; width:\n                cell_str = cell_str[:width - 3] + \"...\"\n\n            if is_header:\n                formatted_cells.append(self.style.Bold(self.style.CYAN(cell_str.ljust(width))))\n            else:\n                formatted_cells.append(cell_str.ljust(width))\n\n        self.print(f\"\u2502 {' \u2502 '.join(formatted_cells)} \u2502\")\n\n    def _print_table_separator(self, widths: list[int]):\n        \"\"\"Helper method to print table separator\"\"\"\n        parts = ['\u2500' * w for w in widths]\n        self.print(f\"\u251c\u2500{'\u2500\u253c\u2500'.join(parts)}\u2500\u2524\")\n\n    async def process_with_spinner(self, message: str, coroutine):\n        \"\"\"Execute coroutine with adaptive spinner\"\"\"\n        self._terminal_width = self._get_terminal_width()\n\n        if self._terminal_width &lt; 60:\n            # Simple spinner for small screens\n            spinner_symbols = \"\u280b\u2819\u2839\u2838\u283c\u2834\u2826\u2827\u2807\u280f\"\n        else:\n            # Detailed spinner for larger screens\n            spinner_symbols = \"\u280b\u2819\u2839\u2838\u283c\u2834\u2826\u2827\u2807\u280f\"\n\n        # Truncate message if too long\n        if len(message) &gt; self._terminal_width - 10:\n            display_message = message[:self._terminal_width - 13] + \"...\"\n        else:\n            display_message = message\n\n        with Spinner(f\"{self.style.CYAN('\u25cf')} {display_message}\", symbols=spinner_symbols):\n            return await coroutine\n\n    def print_git_info(self) -&gt; Optional[str]:\n        \"\"\"Get current git branch with error handling\"\"\"\n        try:\n            result = subprocess.run(\n                ['git', 'rev-parse', '--abbrev-ref', 'HEAD'],\n                capture_output=True, text=True, timeout=2\n            )\n            if result.returncode == 0 and result.stdout.strip():\n                branch = result.stdout.strip()\n\n                # Check for uncommitted changes\n                status_result = subprocess.run(\n                    ['git', 'status', '--porcelain'],\n                    capture_output=True, text=True, timeout=1\n                )\n                dirty = \"*\" if status_result.stdout.strip() else \"\"\n\n                git_info = f\"{branch}{dirty}\"\n                self.print_info(f\"Git: {git_info}\")\n                return git_info\n        except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError):\n            pass\n        return None\n\n    # Convenience methods with consistent styling\n    def print_error(self, message: str):\n        \"\"\"Print error message with consistent formatting\"\"\"\n        self.print(f\"{self.style.RED('\u2717')} {self.style.RED(message)}\")\n\n    def print_success(self, message: str):\n        \"\"\"Print success message with consistent formatting\"\"\"\n        self.print(f\"{self.style.GREEN('\u2713')} {self.style.GREEN(message)}\")\n\n    def print_warning(self, message: str):\n        \"\"\"Print warning message with consistent formatting\"\"\"\n        self.print(f\"{self.style.YELLOW('\u26a0')} {self.style.YELLOW(message)}\")\n\n    def print_info(self, message: str):\n        \"\"\"Print info message with consistent formatting\"\"\"\n        self.print(f\"{self.style.CYAN('\u2139')} {self.style.CYAN(message)}\")\n\n    def print_debug(self, message: str):\n        \"\"\"Print debug message with consistent formatting\"\"\"\n        self.print(f\"{self.style.GREY('\ud83d\udc1b')} {self.style.GREY(message)}\")\n</code></pre> <code>get_git_info()</code> \u00b6 <p>Checks for a git repo and returns its name and branch, or None.</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def get_git_info(self):\n    \"\"\"Checks for a git repo and returns its name and branch, or None.\"\"\"\n    try:\n        # Check if we are in a git repository\n        subprocess.check_output(['git', 'rev-parse', '--is-inside-work-tree'], stderr=subprocess.DEVNULL)\n\n        # Get the repo name (root folder name)\n        repo_root = subprocess.check_output(['git', 'rev-parse', '--show-toplevel'],\n                                            stderr=subprocess.DEVNULL).strip().decode('utf-8')\n        repo_name = os.path.basename(repo_root)\n\n        # Get the current branch name\n        branch = subprocess.check_output(['git', 'rev-parse', '--abbrev-ref', 'HEAD'],\n                                         stderr=subprocess.DEVNULL).strip().decode('utf-8')\n\n        return repo_name, branch\n    except (subprocess.CalledProcessError, FileNotFoundError):\n        # This handles cases where 'git' is not installed or it's not a git repo\n        return None\n</code></pre> <code>print_code_block(code, language='python')</code> \u00b6 <p>Print code with syntax awareness and proper formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_code_block(self, code: str, language: str = \"python\"):\n    \"\"\"Print code with syntax awareness and proper formatting\"\"\"\n    self._terminal_width = self._get_terminal_width()\n\n    if self._terminal_width &lt; 60:\n        # Simple format for small screens\n        self.print(f\"\\n{self.style.GREY('Code:')}\")\n        for line in code.split('\\n'):\n            self.print(f\"  {line}\")\n    else:\n        # Detailed format for larger screens\n        self.print(f\"\\n{self.style.BLUE('\u250c\u2500')} {self.style.YELLOW2(f'{language.upper()} Code')}\")\n\n        lines = code.split('\\n')\n        for i, line in enumerate(lines):\n            if i == len(lines) - 1 and not line.strip():\n                continue\n\n            # Wrap long lines\n            if len(line) &gt; self._terminal_width - 6:\n                wrapped = self._wrap_text(line, self._terminal_width - 6)\n                for j, wrapped_line in enumerate(wrapped):\n                    prefix = \"\u2502\" if j == 0 else \"\u2502\"\n                    self.print(f\"{self.style.BLUE(prefix)} {wrapped_line}\")\n            else:\n                self.print(f\"{self.style.BLUE('\u2502')} {line}\")\n\n        self.print(f\"{self.style.BLUE('\u2514\u2500')} {self.style.GREY('End of code block')}\")\n</code></pre> <code>print_debug(message)</code> \u00b6 <p>Print debug message with consistent formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_debug(self, message: str):\n    \"\"\"Print debug message with consistent formatting\"\"\"\n    self.print(f\"{self.style.GREY('\ud83d\udc1b')} {self.style.GREY(message)}\")\n</code></pre> <code>print_error(message)</code> \u00b6 <p>Print error message with consistent formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_error(self, message: str):\n    \"\"\"Print error message with consistent formatting\"\"\"\n    self.print(f\"{self.style.RED('\u2717')} {self.style.RED(message)}\")\n</code></pre> <code>print_git_info()</code> \u00b6 <p>Get current git branch with error handling</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_git_info(self) -&gt; Optional[str]:\n    \"\"\"Get current git branch with error handling\"\"\"\n    try:\n        result = subprocess.run(\n            ['git', 'rev-parse', '--abbrev-ref', 'HEAD'],\n            capture_output=True, text=True, timeout=2\n        )\n        if result.returncode == 0 and result.stdout.strip():\n            branch = result.stdout.strip()\n\n            # Check for uncommitted changes\n            status_result = subprocess.run(\n                ['git', 'status', '--porcelain'],\n                capture_output=True, text=True, timeout=1\n            )\n            dirty = \"*\" if status_result.stdout.strip() else \"\"\n\n            git_info = f\"{branch}{dirty}\"\n            self.print_info(f\"Git: {git_info}\")\n            return git_info\n    except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError):\n        pass\n    return None\n</code></pre> <code>print_header(text)</code> \u00b6 <p>Print a dynamic header that adapts to screen size</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_header(self, text: str):\n    \"\"\"Print a dynamic header that adapts to screen size\"\"\"\n    self._terminal_width = self._get_terminal_width()\n\n    if self._terminal_width &lt; 60:  # Tiny screen\n        self.print()\n        self.print(self.style.CYAN(\"=\" * self._terminal_width))\n        self.print(self.style.CYAN(self.style.Bold(text)))\n        self.print(self.style.CYAN(\"=\" * self._terminal_width))\n    else:  # Regular/large screen\n        border_width = min(len(text) + 2, self._terminal_width - 2)\n        border = \"\u2500\" * border_width\n\n        self.print()\n        self.print(self.style.CYAN(f\"\u250c{border}\u2510\"))\n        self.print(self.style.CYAN(f\"\u2502 {self.style.Bold(text).center(border_width - 2)} \u2502\"))\n        self.print(self.style.CYAN(f\"\u2514{border}\u2518\"))\n    self.print()\n</code></pre> <code>print_info(message)</code> \u00b6 <p>Print info message with consistent formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_info(self, message: str):\n    \"\"\"Print info message with consistent formatting\"\"\"\n    self.print(f\"{self.style.CYAN('\u2139')} {self.style.CYAN(message)}\")\n</code></pre> <code>print_progress_bar(current, maximum, title='Progress')</code> \u00b6 <p>Dynamic progress bar that adapts to screen size</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_progress_bar(self, current: int, maximum: int, title: str = \"Progress\"):\n    \"\"\"Dynamic progress bar that adapts to screen size\"\"\"\n    self._terminal_width = self._get_terminal_width()\n\n    # Calculate bar width based on screen size\n    if self._terminal_width &lt; 60:\n        bar_width = 10\n        template = f\"\\r{title}: [{{}}] {current}/{maximum}\"\n    else:\n        bar_width = min(30, self._terminal_width - 30)\n        template = f\"\\r{self.style.CYAN(title)}: [{{}}] {current}/{maximum} ({current / maximum * 100:.1f}%)\"\n\n    progress = int((current / maximum) * bar_width)\n    bar = \"\u2588\" * progress + \"\u2591\" * (bar_width - progress)\n\n    self.print(template.format(bar), end='', flush=True)\n</code></pre> <code>print_section(title, content)</code> \u00b6 <p>Print a clean section with adaptive formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_section(self, title: str, content: str):\n    \"\"\"Print a clean section with adaptive formatting\"\"\"\n    self._terminal_width = self._get_terminal_width()\n\n    # Title\n    if self._terminal_width &lt; 60:\n        self.print(f\"\\n{self.style.BLUE('\u25cf')} {self.style.Bold(title)}\")\n    else:\n        self.print(f\"\\n{self.style.BLUE('\u25cf')} {self.style.Bold(self.style.BLUE(title))}\")\n\n    # Content with proper wrapping\n    for line in content.split('\\n'):\n        if line.strip():\n            wrapped_lines = self._wrap_text(line.strip())\n            for wrapped_line in wrapped_lines:\n                if self._terminal_width &lt; 60:\n                    self.print(f\"  {wrapped_line}\")\n                else:\n                    self.print(f\"  {self.style.GREY('\u2502')} {wrapped_line}\")\n    self.print()\n</code></pre> <code>print_state(state, details=None)</code> \u00b6 <p>Print current state with adaptive formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_state(self, state: str, details: Dict[str, Any] = None) -&gt; str:\n    \"\"\"Print current state with adaptive formatting\"\"\"\n    self._terminal_width = self._get_terminal_width()\n\n    state_colors = {\n        'ACTION': self.style.GREEN2,\n        'PROCESSING': self.style.YELLOW2,\n        'BRAKE': self.style.RED2,\n        'DONE': self.style.BLUE2,\n        'ERROR': self.style.RED,\n        'SUCCESS': self.style.GREEN,\n        'INFO': self.style.CYAN\n    }\n\n    color_func = state_colors.get(state.upper(), self.style.WHITE2)\n\n    if self._terminal_width &lt; 60:\n        # Compact format for small screens\n        self.print(f\"\\n[{color_func(state)}]\")\n        result = f\"\\n[{state}]\"\n    else:\n        # Full format for larger screens\n        self.print(f\"\\n{self.style.Bold('State:')} {color_func(state)}\")\n        result = f\"\\nState: {state}\"\n\n    if details:\n        for key, value in details.items():\n            # Truncate long values on small screens\n            if self._terminal_width &lt; 60 and len(str(value)) &gt; 30:\n                display_value = str(value)[:27] + \"...\"\n            else:\n                display_value = str(value)\n\n            if self._terminal_width &lt; 60:\n                self.print(f\"  {key}: {display_value}\")\n                result += f\"\\n  {key}: {display_value}\"\n            else:\n                self.print(f\"  {self.style.GREY('\u251c\u2500')} {self.style.CYAN(key)}: {display_value}\")\n                result += f\"\\n  \u251c\u2500 {key}: {display_value}\"\n\n    return result\n</code></pre> <code>print_success(message)</code> \u00b6 <p>Print success message with consistent formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_success(self, message: str):\n    \"\"\"Print success message with consistent formatting\"\"\"\n    self.print(f\"{self.style.GREEN('\u2713')} {self.style.GREEN(message)}\")\n</code></pre> <code>print_table(headers, rows)</code> \u00b6 <p>Print a dynamic table that adapts to screen size</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_table(self, headers: list[str], rows: list[list[str]]):\n    \"\"\"Print a dynamic table that adapts to screen size\"\"\"\n    self._terminal_width = self._get_terminal_width()\n\n    if not rows:\n        return\n\n    # Calculate column widths\n    all_data = [headers] + rows\n    col_widths = []\n\n    for col in range(len(headers)):\n        max_width = max(len(str(row[col])) for row in all_data if col &lt; len(row))\n        col_widths.append(min(max_width, self._terminal_width // len(headers) - 2))\n\n    # Adjust if total width exceeds terminal\n    total_width = sum(col_widths) + len(headers) * 3 + 1\n    if total_width &gt; self._terminal_width:\n        # Proportionally reduce column widths\n        scale_factor = (self._terminal_width - len(headers) * 3 - 1) / sum(col_widths)\n        col_widths = [max(8, int(w * scale_factor)) for w in col_widths]\n\n    # Print table\n    self._print_table_row(headers, col_widths, is_header=True)\n    self._print_table_separator(col_widths)\n\n    for row in rows:\n        self._print_table_row(row, col_widths)\n</code></pre> <code>print_warning(message)</code> \u00b6 <p>Print warning message with consistent formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_warning(self, message: str):\n    \"\"\"Print warning message with consistent formatting\"\"\"\n    self.print(f\"{self.style.YELLOW('\u26a0')} {self.style.YELLOW(message)}\")\n</code></pre> <code>process_with_spinner(message, coroutine)</code> <code>async</code> \u00b6 <p>Execute coroutine with adaptive spinner</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def process_with_spinner(self, message: str, coroutine):\n    \"\"\"Execute coroutine with adaptive spinner\"\"\"\n    self._terminal_width = self._get_terminal_width()\n\n    if self._terminal_width &lt; 60:\n        # Simple spinner for small screens\n        spinner_symbols = \"\u280b\u2819\u2839\u2838\u283c\u2834\u2826\u2827\u2807\u280f\"\n    else:\n        # Detailed spinner for larger screens\n        spinner_symbols = \"\u280b\u2819\u2839\u2838\u283c\u2834\u2826\u2827\u2807\u280f\"\n\n    # Truncate message if too long\n    if len(message) &gt; self._terminal_width - 10:\n        display_message = message[:self._terminal_width - 13] + \"...\"\n    else:\n        display_message = message\n\n    with Spinner(f\"{self.style.CYAN('\u25cf')} {display_message}\", symbols=spinner_symbols):\n        return await coroutine\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.EnhancedVerboseOutput","title":"<code>EnhancedVerboseOutput</code>","text":"<p>Main interface for verbose output with full functionality</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class EnhancedVerboseOutput:\n    \"\"\"Main interface for verbose output with full functionality\"\"\"\n\n    def __init__(self, verbose: bool = True, print_func=None, **formatter_kwargs):\n        self.verbose = verbose\n        self.print = print_func or print\n        self.formatter = DynamicVerboseFormatter(self.print, **formatter_kwargs)\n        self._start_time = time.time()\n\n    def __getattr__(self, name):\n        \"\"\"Delegate to formatter for convenience\"\"\"\n        return getattr(self.formatter, name)\n\n    async def print_agent_response(self, response: str):\n        await self.log_message(\"assistant\", response)\n\n    async def print_thought(self, thought: str):\n        await self.log_message(\"assistant\", f\"Thought: {thought}\")\n\n    async def log_message(self, role: str, content: str):\n        \"\"\"Log chat messages with role-based formatting\"\"\"\n        if not self.verbose:\n            return\n\n        role_formats = {\n            'user': (self.formatter.style.GREEN, \"\ud83d\udc64\"),\n            'assistant': (self.formatter.style.BLUE, \"\ud83e\udd16\"),\n            'system': (self.formatter.style.YELLOW, \"\u2699\ufe0f\"),\n            'error': (self.formatter.style.RED, \"\u274c\"),\n            'debug': (self.formatter.style.GREY, \"\ud83d\udc1b\")\n        }\n\n        color_func, icon = role_formats.get(role.lower(), (self.formatter.style.WHITE, \"\u2022\"))\n\n        if content.startswith(\"```\"):\n            self.formatter.print_code_block(content)\n            return\n\n        if content.startswith(\"{\") or content.startswith(\"[\") and content.endswith(\"}\") or content.endswith(\"]\"):\n            content = json.dumps(json.loads(content), indent=2)\n\n        # Adapt formatting based on screen size\n        if self.formatter._terminal_width &lt; 60:\n            self.print(f\"\\n{icon} [{role.upper()}]\")\n            # Wrap content for small screens\n            wrapped_content = self.formatter._wrap_text(content, self.formatter._terminal_width - 2)\n            for line in wrapped_content:\n                self.print(f\"  {line}\")\n        else:\n            self.print(f\"\\n{icon} {color_func(f'[{role.upper()}]')}\")\n            self.print(f\"{self.formatter.style.GREY('\u2514\u2500')} {content}\")\n        self.print()\n\n    async def log_process_result(self, result: Dict[str, Any]):\n        \"\"\"Log processing results with structured formatting\"\"\"\n        if not self.verbose:\n            return\n\n        content_parts = []\n\n        if 'action' in result:\n            content_parts.append(f\"Action: {result['action']}\")\n        if 'is_completed' in result:\n            content_parts.append(f\"Completed: {result['is_completed']}\")\n        if 'effectiveness' in result:\n            content_parts.append(f\"Effectiveness: {result['effectiveness']}\")\n        if 'recommendations' in result:\n            content_parts.append(f\"Recommendations:\\n{result['recommendations']}\")\n        if 'workflow' in result:\n            content_parts.append(f\"Workflow:\\n{result['workflow']}\")\n        if 'errors' in result and result['errors']:\n            content_parts.append(f\"Errors: {result['errors']}\")\n        if 'content' in result:\n            content_parts.append(f\"Content:\\n{result['content']}\")\n\n        self.formatter.print_section(\"Process Result\", '\\n'.join(content_parts))\n\n    def log_header(self, text: str):\n        \"\"\"Log header with timing information\"\"\"\n        if not self.verbose:\n            return\n\n        elapsed = time.time() - self._start_time\n        if elapsed &gt; 60:\n            timing = f\" ({elapsed / 60:.1f}m)\"\n        else:\n            timing = f\" ({elapsed:.1f}s)\"\n\n        self.formatter.print_header(f\"{text}{timing}\")\n\n    def log_state(self, state: str, user_ns: Dict = None, override: bool = False):\n        \"\"\"Log state with optional override\"\"\"\n        if not self.verbose and not override:\n            return\n\n        return self.formatter.print_state(state, user_ns)\n\n    async def process(self, message: str, coroutine):\n        \"\"\"Process with optional spinner\"\"\"\n        if not self.verbose:\n            return await coroutine\n\n        if message.lower() in [\"code\", \"silent\"]:\n            return await coroutine\n\n        return await self.formatter.process_with_spinner(message, coroutine)\n\n    def print_tool_call(self, tool_name: str, tool_args: Dict, result: Optional[str] = None):\n        \"\"\"\n        Gibt Informationen zum Tool-Aufruf aus.\n        Versucht, das Ergebnis als JSON zu formatieren, wenn m\u00f6glich.\n        \"\"\"\n        if not self.verbose:\n            return\n\n        # Argumente wie zuvor formatieren\n        args_str = json.dumps(tool_args, indent=2, ensure_ascii=False) if tool_args else \"None\"\n        content = f\"Tool: {tool_name}\\nArguments:\\n{args_str}\"\n\n        if result:\n            result_output = \"\"\n            try:\n                # 1. Versuch, den String als JSON zu parsen\n                data = json.loads(result)\n\n                # 2. Pr\u00fcfen, ob das Ergebnis ein Dictionary ist (der h\u00e4ufigste Fall)\n                if isinstance(data, dict):\n                    # Eine Kopie f\u00fcr die Anzeige erstellen, um den 'output'-Wert zu ersetzen\n                    display_data = data.copy()\n                    output_preview = \"\"\n\n                    # Spezielle Handhabung f\u00fcr einen langen 'output'-String, falls vorhanden\n                    if 'output' in display_data and isinstance(display_data['output'], str):\n                        full_output = display_data['output']\n                        # Den langen String im JSON durch einen Platzhalter ersetzen\n                        display_data['output'] = \"&lt;-- [Inhalt wird separat formatiert]\"\n\n                        # Vorschau mit den ersten 3 Zeilen erstellen\n                        lines = full_output.strip().split('\\n')[:3]\n                        preview_text = '\\n'.join(lines)\n                        output_preview = f\"\\n\\n--- Vorschau f\u00fcr 'output' ---\\n\\x1b[90m{preview_text}\\n...\\x1b[0m\"  # Hellgrauer Text\n                        # display_data['output'] = output_preview\n                    # Das formatierte JSON (mit Platzhalter) zum Inhalt hinzuf\u00fcgen\n                    formatted_json = json.dumps(display_data, indent=2, ensure_ascii=False)\n                    result_output = f\"Geparstes Dictionary:\\n{formatted_json}{output_preview}\"\n\n                else:\n                    # Falls es valides JSON, aber kein Dictionary ist (z.B. eine Liste)\n                    result_output = f\"Gepastes JSON (kein Dictionary):\\n{json.dumps(data, indent=2, ensure_ascii=False)}\"\n\n            except json.JSONDecodeError:\n                # 3. Wenn Parsen fehlschl\u00e4gt, den String als Rohtext behandeln\n                result_output = f\"{result}\"\n\n            content += f\"\\nResult:\\n{result_output}\"\n\n        else:\n            # Fall, wenn der Task noch l\u00e4uft\n            content += \"\\nResult: In progress...\"\n\n        # Den gesamten Inhalt an den Formatter \u00fcbergeben\n        self.formatter.print_section(\"Tool Call\", content)\n\n    def print_event(self, event: Dict):\n        \"\"\"Print event information\"\"\"\n        if not self.verbose:\n            return\n\n        if event.get(\"content\") and event[\"content\"].get(\"parts\"):\n            for part in event[\"content\"][\"parts\"]:\n                if part.get(\"text\"):\n                    self.formatter.print_info(f\"Thought: {part['text']}\")\n                if part.get(\"function_call\"):\n                    self.print_tool_call(\n                        part[\"function_call\"][\"name\"],\n                        part[\"function_call\"][\"args\"]\n                    )\n                if part.get(\"function_response\"):\n                    result = part[\"function_response\"][\"response\"].get(\"result\", \"\")\n                    self.print_tool_call(\n                        part[\"function_response\"][\"name\"],\n                        {},\n                        str(result)\n                    )\n\n        if event.get(\"usage_metadata\"):\n            self.formatter.print_info(f\"Token usage: {event['usage_metadata']}\")\n\n    @contextmanager\n    def section_context(self, title: str):\n        \"\"\"Context manager for sections\"\"\"\n        if self.verbose:\n            self.formatter.print_section(title, \"Starting...\")\n        try:\n            yield\n        finally:\n            if self.verbose:\n                self.formatter.print_success(f\"Completed: {title}\")\n\n    def clear_line(self):\n        \"\"\"Clear current line\"\"\"\n        self.print('\\r' + ' ' * self.formatter._terminal_width + '\\r', end='')\n\n    def print_separator(self, char: str = \"\u2500\"):\n        \"\"\"Print a separator line\"\"\"\n        self.print(self.formatter.style.GREY(char * self.formatter._terminal_width))\n\n    def print_warning(self, message: str):\n        \"\"\"Print a warning message with yellow style\"\"\"\n        if self.verbose:\n            self.print(self.formatter.style.YELLOW(f\"\u26a0\ufe0f  WARNING: {message}\"))\n\n    def print_error(self, message: str):\n        \"\"\"Print an error message with red style\"\"\"\n        if self.verbose:\n            self.print(self.formatter.style.RED(f\"\u274c ERROR: {message}\"))\n\n    def print_success(self, message: str):\n        \"\"\"Print a success message with green style\"\"\"\n        if self.verbose:\n            self.print(self.formatter.style.GREEN(f\"\u2705 SUCCESS: {message}\"))\n</code></pre> <code>__getattr__(name)</code> \u00b6 <p>Delegate to formatter for convenience</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __getattr__(self, name):\n    \"\"\"Delegate to formatter for convenience\"\"\"\n    return getattr(self.formatter, name)\n</code></pre> <code>clear_line()</code> \u00b6 <p>Clear current line</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def clear_line(self):\n    \"\"\"Clear current line\"\"\"\n    self.print('\\r' + ' ' * self.formatter._terminal_width + '\\r', end='')\n</code></pre> <code>log_header(text)</code> \u00b6 <p>Log header with timing information</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def log_header(self, text: str):\n    \"\"\"Log header with timing information\"\"\"\n    if not self.verbose:\n        return\n\n    elapsed = time.time() - self._start_time\n    if elapsed &gt; 60:\n        timing = f\" ({elapsed / 60:.1f}m)\"\n    else:\n        timing = f\" ({elapsed:.1f}s)\"\n\n    self.formatter.print_header(f\"{text}{timing}\")\n</code></pre> <code>log_message(role, content)</code> <code>async</code> \u00b6 <p>Log chat messages with role-based formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def log_message(self, role: str, content: str):\n    \"\"\"Log chat messages with role-based formatting\"\"\"\n    if not self.verbose:\n        return\n\n    role_formats = {\n        'user': (self.formatter.style.GREEN, \"\ud83d\udc64\"),\n        'assistant': (self.formatter.style.BLUE, \"\ud83e\udd16\"),\n        'system': (self.formatter.style.YELLOW, \"\u2699\ufe0f\"),\n        'error': (self.formatter.style.RED, \"\u274c\"),\n        'debug': (self.formatter.style.GREY, \"\ud83d\udc1b\")\n    }\n\n    color_func, icon = role_formats.get(role.lower(), (self.formatter.style.WHITE, \"\u2022\"))\n\n    if content.startswith(\"```\"):\n        self.formatter.print_code_block(content)\n        return\n\n    if content.startswith(\"{\") or content.startswith(\"[\") and content.endswith(\"}\") or content.endswith(\"]\"):\n        content = json.dumps(json.loads(content), indent=2)\n\n    # Adapt formatting based on screen size\n    if self.formatter._terminal_width &lt; 60:\n        self.print(f\"\\n{icon} [{role.upper()}]\")\n        # Wrap content for small screens\n        wrapped_content = self.formatter._wrap_text(content, self.formatter._terminal_width - 2)\n        for line in wrapped_content:\n            self.print(f\"  {line}\")\n    else:\n        self.print(f\"\\n{icon} {color_func(f'[{role.upper()}]')}\")\n        self.print(f\"{self.formatter.style.GREY('\u2514\u2500')} {content}\")\n    self.print()\n</code></pre> <code>log_process_result(result)</code> <code>async</code> \u00b6 <p>Log processing results with structured formatting</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def log_process_result(self, result: Dict[str, Any]):\n    \"\"\"Log processing results with structured formatting\"\"\"\n    if not self.verbose:\n        return\n\n    content_parts = []\n\n    if 'action' in result:\n        content_parts.append(f\"Action: {result['action']}\")\n    if 'is_completed' in result:\n        content_parts.append(f\"Completed: {result['is_completed']}\")\n    if 'effectiveness' in result:\n        content_parts.append(f\"Effectiveness: {result['effectiveness']}\")\n    if 'recommendations' in result:\n        content_parts.append(f\"Recommendations:\\n{result['recommendations']}\")\n    if 'workflow' in result:\n        content_parts.append(f\"Workflow:\\n{result['workflow']}\")\n    if 'errors' in result and result['errors']:\n        content_parts.append(f\"Errors: {result['errors']}\")\n    if 'content' in result:\n        content_parts.append(f\"Content:\\n{result['content']}\")\n\n    self.formatter.print_section(\"Process Result\", '\\n'.join(content_parts))\n</code></pre> <code>log_state(state, user_ns=None, override=False)</code> \u00b6 <p>Log state with optional override</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def log_state(self, state: str, user_ns: Dict = None, override: bool = False):\n    \"\"\"Log state with optional override\"\"\"\n    if not self.verbose and not override:\n        return\n\n    return self.formatter.print_state(state, user_ns)\n</code></pre> <code>print_error(message)</code> \u00b6 <p>Print an error message with red style</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_error(self, message: str):\n    \"\"\"Print an error message with red style\"\"\"\n    if self.verbose:\n        self.print(self.formatter.style.RED(f\"\u274c ERROR: {message}\"))\n</code></pre> <code>print_event(event)</code> \u00b6 <p>Print event information</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_event(self, event: Dict):\n    \"\"\"Print event information\"\"\"\n    if not self.verbose:\n        return\n\n    if event.get(\"content\") and event[\"content\"].get(\"parts\"):\n        for part in event[\"content\"][\"parts\"]:\n            if part.get(\"text\"):\n                self.formatter.print_info(f\"Thought: {part['text']}\")\n            if part.get(\"function_call\"):\n                self.print_tool_call(\n                    part[\"function_call\"][\"name\"],\n                    part[\"function_call\"][\"args\"]\n                )\n            if part.get(\"function_response\"):\n                result = part[\"function_response\"][\"response\"].get(\"result\", \"\")\n                self.print_tool_call(\n                    part[\"function_response\"][\"name\"],\n                    {},\n                    str(result)\n                )\n\n    if event.get(\"usage_metadata\"):\n        self.formatter.print_info(f\"Token usage: {event['usage_metadata']}\")\n</code></pre> <code>print_separator(char='\u2500')</code> \u00b6 <p>Print a separator line</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_separator(self, char: str = \"\u2500\"):\n    \"\"\"Print a separator line\"\"\"\n    self.print(self.formatter.style.GREY(char * self.formatter._terminal_width))\n</code></pre> <code>print_success(message)</code> \u00b6 <p>Print a success message with green style</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_success(self, message: str):\n    \"\"\"Print a success message with green style\"\"\"\n    if self.verbose:\n        self.print(self.formatter.style.GREEN(f\"\u2705 SUCCESS: {message}\"))\n</code></pre> <code>print_tool_call(tool_name, tool_args, result=None)</code> \u00b6 <p>Gibt Informationen zum Tool-Aufruf aus. Versucht, das Ergebnis als JSON zu formatieren, wenn m\u00f6glich.</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_tool_call(self, tool_name: str, tool_args: Dict, result: Optional[str] = None):\n    \"\"\"\n    Gibt Informationen zum Tool-Aufruf aus.\n    Versucht, das Ergebnis als JSON zu formatieren, wenn m\u00f6glich.\n    \"\"\"\n    if not self.verbose:\n        return\n\n    # Argumente wie zuvor formatieren\n    args_str = json.dumps(tool_args, indent=2, ensure_ascii=False) if tool_args else \"None\"\n    content = f\"Tool: {tool_name}\\nArguments:\\n{args_str}\"\n\n    if result:\n        result_output = \"\"\n        try:\n            # 1. Versuch, den String als JSON zu parsen\n            data = json.loads(result)\n\n            # 2. Pr\u00fcfen, ob das Ergebnis ein Dictionary ist (der h\u00e4ufigste Fall)\n            if isinstance(data, dict):\n                # Eine Kopie f\u00fcr die Anzeige erstellen, um den 'output'-Wert zu ersetzen\n                display_data = data.copy()\n                output_preview = \"\"\n\n                # Spezielle Handhabung f\u00fcr einen langen 'output'-String, falls vorhanden\n                if 'output' in display_data and isinstance(display_data['output'], str):\n                    full_output = display_data['output']\n                    # Den langen String im JSON durch einen Platzhalter ersetzen\n                    display_data['output'] = \"&lt;-- [Inhalt wird separat formatiert]\"\n\n                    # Vorschau mit den ersten 3 Zeilen erstellen\n                    lines = full_output.strip().split('\\n')[:3]\n                    preview_text = '\\n'.join(lines)\n                    output_preview = f\"\\n\\n--- Vorschau f\u00fcr 'output' ---\\n\\x1b[90m{preview_text}\\n...\\x1b[0m\"  # Hellgrauer Text\n                    # display_data['output'] = output_preview\n                # Das formatierte JSON (mit Platzhalter) zum Inhalt hinzuf\u00fcgen\n                formatted_json = json.dumps(display_data, indent=2, ensure_ascii=False)\n                result_output = f\"Geparstes Dictionary:\\n{formatted_json}{output_preview}\"\n\n            else:\n                # Falls es valides JSON, aber kein Dictionary ist (z.B. eine Liste)\n                result_output = f\"Gepastes JSON (kein Dictionary):\\n{json.dumps(data, indent=2, ensure_ascii=False)}\"\n\n        except json.JSONDecodeError:\n            # 3. Wenn Parsen fehlschl\u00e4gt, den String als Rohtext behandeln\n            result_output = f\"{result}\"\n\n        content += f\"\\nResult:\\n{result_output}\"\n\n    else:\n        # Fall, wenn der Task noch l\u00e4uft\n        content += \"\\nResult: In progress...\"\n\n    # Den gesamten Inhalt an den Formatter \u00fcbergeben\n    self.formatter.print_section(\"Tool Call\", content)\n</code></pre> <code>print_warning(message)</code> \u00b6 <p>Print a warning message with yellow style</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_warning(self, message: str):\n    \"\"\"Print a warning message with yellow style\"\"\"\n    if self.verbose:\n        self.print(self.formatter.style.YELLOW(f\"\u26a0\ufe0f  WARNING: {message}\"))\n</code></pre> <code>process(message, coroutine)</code> <code>async</code> \u00b6 <p>Process with optional spinner</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def process(self, message: str, coroutine):\n    \"\"\"Process with optional spinner\"\"\"\n    if not self.verbose:\n        return await coroutine\n\n    if message.lower() in [\"code\", \"silent\"]:\n        return await coroutine\n\n    return await self.formatter.process_with_spinner(message, coroutine)\n</code></pre> <code>section_context(title)</code> \u00b6 <p>Context manager for sections</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>@contextmanager\ndef section_context(self, title: str):\n    \"\"\"Context manager for sections\"\"\"\n    if self.verbose:\n        self.formatter.print_section(title, \"Starting...\")\n    try:\n        yield\n    finally:\n        if self.verbose:\n            self.formatter.print_success(f\"Completed: {title}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.JSExecutionRecord","title":"<code>JSExecutionRecord</code>  <code>dataclass</code>","text":"<p>Records JavaScript execution details</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>@dataclass\nclass JSExecutionRecord:\n    \"\"\"Records JavaScript execution details\"\"\"\n    code: str\n    result: Any\n    error: str | None = None\n    page_state: dict | None = None\n    extracted_data: dict | None = None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.MockIPython","title":"<code>MockIPython</code>","text":"Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class MockIPython:\n    def __init__(self, _session_dir=None, auto_remove=True):\n        self.auto_remove = auto_remove\n        self.output_history = {}\n        self._execution_count = 0\n        self._session_dir = _session_dir or Path(get_app().appdata) / '.pipeline_sessions'\n        self._session_dir.mkdir(exist_ok=True)\n        self.vfs = VirtualFileSystem(self._session_dir / 'virtual_fs')\n        self._venv_path = self._session_dir / 'venv'\n        self.user_ns: dict[str, Any] = {}\n        nest_asyncio.apply()\n        # Set up virtual environment if it doesn't exist\n        with Spinner(\"Starting virtual environment\"):\n            self._setup_venv()\n        self.reset()\n\n    def _setup_venv(self):\n        \"\"\"Create virtual environment if it doesn't exist\"\"\"\n        if not self._venv_path.exists():\n            try:\n                subprocess.run([sys.executable, \"-m\", \"venv\", str(self._venv_path)], check=True)\n            except subprocess.CalledProcessError as e:\n                raise RuntimeError(f\"Failed to create virtual environment: {str(e)}\")\n\n    def _virtual_open(self, filepath, mode='r', *args, **kwargs):\n        \"\"\"Custom open function that uses virtual filesystem\"\"\"\n        abs_path = self.vfs._resolve_path(filepath)\n\n        if 'w' in mode or 'a' in mode:\n            # Ensure parent directory exists\n            abs_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Use actual filesystem but track in virtual fs\n        real_file = open(abs_path, mode, *args, **kwargs)\n\n        if 'r' in mode:\n            # Track file content in virtual filesystem when reading\n            rel_path = str(abs_path.relative_to(self.vfs.base_dir))\n            if rel_path not in self.vfs.virtual_files:\n                try:\n                    self.vfs.virtual_files[rel_path] = real_file.read()\n                    real_file.seek(0)\n                except UnicodeDecodeError:\n                    # Handle binary files\n                    pass\n\n        return real_file\n\n    def reset(self):\n        \"\"\"Reset the interpreter state\"\"\"\n        self.user_ns = {\n            '__name__': '__main__',\n            '__builtins__': __builtins__,\n            'toolboxv2': toolboxv2,\n            '__file__': None,\n            '__path__': [str(self.vfs.current_dir)],\n            'auto_install': auto_install,\n            'modify_code': self.modify_code,\n        }\n        self.output_history.clear()\n        self._execution_count = 0\n        if self.auto_remove:\n            shutil.rmtree(self.vfs.base_dir, ignore_errors=True)\n\n    def get_namespace(self) -&gt; dict[str, Any]:\n        \"\"\"Get current namespace\"\"\"\n        return self.user_ns.copy()\n\n    def update_namespace(self, variables: dict[str, Any]):\n        \"\"\"Update namespace with new variables\"\"\"\n        self.user_ns.update(variables)\n\n    @staticmethod\n    def _parse_code(code: str) -&gt; tuple[Any, Any | None, bool, bool]:\n        \"\"\"Parse code and handle top-level await\"\"\"\n        code_ = \"\"\n        for line in code.split('\\n'):\n            if line.strip().startswith('#'):\n                continue\n            if line.strip().startswith('asyncio.run('):\n                line = (' ' *(len(line) - len(line.strip()))) + 'await ' + line.strip()[len('asyncio.run('):-1]\n            code_ += line + '\\n'\n        try:\n            tree = ast.parse(code)\n            # Add parent references\n            ParentNodeTransformer().visit(tree)\n\n            # Detect async features\n            detector = AsyncCodeDetector()\n            detector.visit(tree)\n\n            if detector.has_top_level_await:\n                # Wrap code in async function\n                wrapped_code = \"async def __wrapper():\\n\"\n                wrapped_code += \"    global result\\n\"  # Allow writing to global scope\n                wrapped_code += \"    result = None\\n\"\n                # add try:\n                wrapped_code +=\"    try:\\n\"\n                # Indent the original code\n                wrapped_code += \"\\n\".join(f\"        {line}\" for line in code.splitlines())\n                # Add return statement for last expression\n                wrapped_code +=\"\\n    except Exception as e:\\n\"\n                wrapped_code +=\"        import traceback\\n\"\n                wrapped_code +=\"        print(traceback.format_exc())\\n\"\n                wrapped_code +=\"        raise e\\n\"\n                if isinstance(tree.body[-1], ast.Expr):\n                    wrapped_code += \"\\n    return result\"\n\n                # Parse and compile wrapped code\n                wrapped_tree = ast.parse(wrapped_code)\n                return (\n                    compile(wrapped_tree, '&lt;exec&gt;', 'exec'),\n                    None,\n                    True,\n                    True\n                )\n\n            # Handle regular code\n            if isinstance(tree.body[-1], ast.Expr):\n                exec_code = ast.Module(\n                    body=tree.body[:-1],\n                    type_ignores=[]\n                )\n                eval_code = ast.Expression(\n                    body=tree.body[-1].value\n                )\n                return (\n                    compile(exec_code, '&lt;exec&gt;', 'exec'),\n                    compile(eval_code, '&lt;eval&gt;', 'eval'),\n                    detector.has_async,\n                    False\n                )\n\n            return (\n                compile(tree, '&lt;exec&gt;', 'exec'),\n                None,\n                detector.has_async,\n                False\n            )\n\n        except SyntaxError as e:\n            lines = code.splitlines()\n            if e.lineno and e.lineno &lt;= len(lines):\n                line = lines[e.lineno - 1]\n                arrow = ' ' * (e.offset - 1) + '^' if e.offset else ''\n                error_msg = (\n                    f\"Syntax error at line {e.lineno}:\\n\"\n                    f\"{line}\\n\"\n                    f\"{arrow}\\n\"\n                    f\"{e.msg}\"\n                )\n            else:\n                error_msg = str(e)\n\n            error_msg += traceback.format_exc()\n\n            raise SyntaxError(error_msg) from e\n\n    async def run_cell(self, code: str, live_output: bool = True) -&gt; Any:\n        \"\"\"Async version of run_cell that handles both sync and async code\"\"\"\n        result = None\n        error = None\n        tb = None\n        original_dir = os.getcwd()\n\n        if live_output:\n            stdout_buffer = io.StringIO()\n            stderr_buffer = io.StringIO()\n            stdout = TeeStream(sys.__stdout__, stdout_buffer)\n            stderr = TeeStream(sys.__stderr__, stderr_buffer)\n        else:\n            stdout = io.StringIO()\n            stderr = io.StringIO()\n\n        try:\n            # Check if a file is already specified\n            original_file = self.user_ns.get('__file__')\n            if original_file is None:\n                # Create temp file if no file specified\n                temp_file = self.vfs.write_file(\n                    f'src/temp/_temp_{self._execution_count}.py',\n                    code\n                )\n                # work_ns = self.user_ns.copy()\n                self.user_ns['__file__'] = str(temp_file)\n            else:\n                # Use existing file\n                temp_file = Path(original_file)\n                # Write code to the existing file\n                self.vfs.write_file(temp_file, code)\n                #work_ns = self.user_ns.copy()\n\n            self.user_ns['__builtins__'] = __builtins__\n            with VirtualEnvContext(self._venv_path) as python_exec:\n                try:\n                    exec_code, eval_code, is_async, has_top_level_await = self._parse_code(\n                        code.encode('utf-8', errors='replace').decode('utf-8')\n                    )\n                    if exec_code is None:\n                        return \"No executable code\"\n                    os.makedirs(str(temp_file.parent.absolute()), exist_ok=True)\n                    os.chdir(str(temp_file.parent.absolute()))\n                    self.user_ns['PYTHON_EXEC'] = python_exec\n\n                    with redirect_stdout(stdout), redirect_stderr(stderr):\n                        if has_top_level_await:\n                            try:\n                                # Execute wrapped code and await it\n                                exec(exec_code, self.user_ns)\n                                result = self.user_ns['__wrapper']()\n                                if asyncio.iscoroutine(result):\n                                    result = await result\n                            finally:\n                                self.user_ns.pop('__wrapper', None)\n                        elif is_async:\n                            # Execute async code\n                            exec(exec_code, self.user_ns)\n                            if eval_code:\n                                result = eval(eval_code, self.user_ns)\n                                if asyncio.iscoroutine(result):\n                                    result = await result\n                        else:\n                            # Execute sync code\n                            exec(exec_code, self.user_ns)\n                            if eval_code:\n                                result = eval(eval_code, self.user_ns)\n\n                        if result is not None:\n                            self.user_ns['_'] = result\n                except KeyboardInterrupt:\n                    print(\"Stop execution manuel!\")\n\n                except Exception as e:\n                    error = str(e)\n                    tb = traceback.format_exc()\n                    if live_output:\n                        sys.__stderr__.write(f\"{error}\\n{tb}\")\n                    stderr.write(f\"{error}\\n{tb}\")\n\n                finally:\n                    os.chdir(original_dir)\n                    self._execution_count += 1\n                    # self.user_ns = work_ns.copy()\n                    if live_output:\n                        stdout_value = stdout_buffer.getvalue()\n                        stderr_value = stderr_buffer.getvalue()\n                    else:\n                        stdout_value = stdout.getvalue()\n                        stderr_value = stderr.getvalue()\n\n                    output = {\n                        'code': code,\n                        'stdout': stdout_value,\n                        'stderr': stderr_value,\n                        'result': result if result else \"stdout\"\n                    }\n                    self.output_history[self._execution_count] = output\n\n        except Exception as e:\n            error_msg = f\"Error executing code: {str(e)}\\n{traceback.format_exc()}\"\n            if live_output:\n                sys.__stderr__.write(error_msg)\n            return error_msg\n\n        if not result:\n            result = \"\"\n        if output['stdout']:\n            result = f\"{result}\\nstdout:{output['stdout']}\"\n        if output['stderr']:\n            result = f\"{result}\\nstderr:{output['stderr']}\"\n\n        if self.auto_remove and original_file is None:\n            # Only remove temp files, not user-specified files\n            self.vfs.delete_file(temp_file)\n\n        return result\n\n    async def modify_code(self, code: str = None, object_name: str = None, file: str = None) -&gt; str:\n        '''\n        Modify existing code in memory (user namespace) and optionally in the corresponding file.\n\n        This method updates variables, functions, or methods in the current Python session and can\n        also update the corresponding source file if specified.\n\n        Args:\n            code: New value or implementation for the object\n            object_name: Name of the object to modify (variable, function, or method)\n            file: Path to the file to update (if None, only updates in memory)\n\n        Returns:\n            String describing the modification result\n\n        Examples:\n\n        # 1. Update a variable in memory\n        await ipython.modify_code(code=\"5\", object_name=\"x\")\n\n    # 2. Change a method implementation\n    await ipython.modify_code(\n        code='\"\"\"def sound(self):\\n        return \"Woof\"\"\"\"',\n        object_name=\"Dog.sound\"\n    )\n\n    # 3. Modify a function\n    await ipython.modify_code(\n        code='\"\"\"def calculate_age():\\n    return 25\"\"\"',\n        object_name=\"calculate_age\"\n    )\n\n    # 4. Update variable in memory and file\n    await ipython.modify_code(\n        code=\"100\",\n        object_name=\"MAX_SIZE\",\n        file=\"config.py\"\n    )\n\n    # 5. Modifying an attribute in __init__\n    await ipython.modify_code(\n        code='\"\"\"def __init__(self):\\n        self.name = \"Buddy\"\"\"\"',\n        object_name=\"Dog.__init__\"\n    )\n        '''\n        try:\n            if not object_name:\n                raise ValueError(\"Object name must be specified\")\n            if code is None:\n                raise ValueError(\"New code or value must be provided\")\n\n            # Process object name (handle methods with parentheses)\n            clean_object_name = object_name.replace(\"()\", \"\")\n\n            # Step 1: Update in memory (user namespace)\n            result_message = []\n\n            # Handle different types of objects\n            if \".\" in clean_object_name:\n                # For methods or class attributes\n                parts = clean_object_name.split(\".\")\n                base_obj_name = parts[0]\n                attr_name = parts[1]\n\n                if base_obj_name not in self.user_ns:\n                    raise ValueError(f\"Object '{base_obj_name}' not found in namespace\")\n\n                base_obj = self.user_ns[base_obj_name]\n\n                # Handle method definitions which are passed as docstrings\n                if code.split('\\n'):\n                    method_code = code\n\n                    # Parse the method code to extract its body\n                    method_ast = ast.parse(method_code).body[0]\n                    method_name = method_ast.name\n\n                    # Create a new function object from the code\n                    method_locals = {}\n                    exec(\n                        f\"def _temp_func{signature(getattr(base_obj.__class__, attr_name, None))}: {method_ast.body[0].value.s}\",\n                        globals(), method_locals)\n                    new_method = method_locals['_temp_func']\n\n                    # Set the method on the class\n                    setattr(base_obj.__class__, attr_name, new_method)\n                    result_message.append(f\"Updated method '{clean_object_name}' in memory\")\n                else:\n                    # For simple attributes\n                    setattr(base_obj, attr_name, eval(code, self.user_ns))\n                    result_message.append(f\"Updated attribute '{clean_object_name}' in memory\")\n            else:\n                # For variables and functions\n                if code.startswith('\"\"\"') and code.endswith('\"\"\"'):\n                    # Handle function definitions\n                    func_code = code.strip('\"\"\"')\n                    func_ast = ast.parse(func_code).body[0]\n                    func_name = func_ast.name\n\n                    # Create a new function object from the code\n                    func_locals = {}\n                    exec(f\"{func_code}\", globals(), func_locals)\n                    self.user_ns[clean_object_name] = func_locals[func_name]\n                    result_message.append(f\"Updated function '{clean_object_name}' in memory\")\n                else:\n                    # Simple variable assignment\n                    self.user_ns[clean_object_name] = eval(code, self.user_ns)\n                    result_message.append(f\"Updated variable '{clean_object_name}' in memory\")\n\n            # Step 2: Update in file if specified\n            if file is not None:\n                file_path = self.vfs._resolve_path(file)\n\n                if not file_path.exists():\n                    self.user_ns['__file__'] = str(file_path)\n                    return await self.run_cell(code)\n\n                # Read original content\n                original_content = self.vfs.read_file(file_path)\n                updated_content = original_content\n\n                # Handle different object types for file updates\n                if \".\" in clean_object_name:\n                    # For methods\n                    parts = clean_object_name.split(\".\")\n                    class_name = parts[0]\n                    method_name = parts[1]\n\n                    if code.startswith('\"\"\"') and code.endswith('\"\"\"'):\n                        method_code = code.strip('\"\"\"')\n\n                        # Use ast to parse the file and find the method to replace\n                        file_ast = ast.parse(original_content)\n                        for node in ast.walk(file_ast):\n                            if isinstance(node, ast.ClassDef) and node.name == class_name:\n                                for method in node.body:\n                                    if isinstance(method, ast.FunctionDef) and method.name == method_name:\n                                        # Find the method in the source code\n                                        method_pattern = fr\"def {method_name}.*?:(.*?)(?=\\n    \\w|\\n\\w|\\Z)\"\n                                        method_match = re.search(method_pattern, original_content, re.DOTALL)\n\n                                        if method_match:\n                                            indentation = re.match(r\"^(\\s*)\", method_match.group(0)).group(1)\n                                            method_indented = textwrap.indent(method_code, indentation)\n                                            updated_content = original_content.replace(\n                                                method_match.group(0),\n                                                method_indented\n                                            )\n                                            self.vfs.write_file(file_path, updated_content)\n                                            result_message.append(\n                                                f\"Updated method '{clean_object_name}' in file '{file}'\")\n                else:\n                    # For variables and functions\n                    if code.startswith('\"\"\"') and code.endswith('\"\"\"'):\n                        # Handle function updates\n                        func_code = code.strip('\"\"\"')\n                        func_pattern = fr\"def {clean_object_name}.*?:(.*?)(?=\\n\\w|\\Z)\"\n                        func_match = re.search(func_pattern, original_content, re.DOTALL)\n\n                        if func_match:\n                            indentation = re.match(r\"^(\\s*)\", func_match.group(0)).group(1)\n                            func_indented = textwrap.indent(func_code, indentation)\n                            updated_content = original_content.replace(\n                                func_match.group(0),\n                                func_indented\n                            )\n                            self.vfs.write_file(file_path, updated_content)\n                            result_message.append(f\"Updated function '{clean_object_name}' in file '{file}'\")\n                    else:\n                        # Handle variable updates\n                        var_pattern = fr\"{clean_object_name}\\s*=.*\"\n                        var_replacement = f\"{clean_object_name} = {code}\"\n                        updated_content = re.sub(var_pattern, var_replacement, original_content)\n\n                        if updated_content != original_content:\n                            self.vfs.write_file(file_path, updated_content)\n                            result_message.append(f\"Updated variable '{clean_object_name}' in file '{file}'\")\n                        else:\n                            result_message.append(f\"Could not find variable '{clean_object_name}' in file '{file}'\")\n\n            return \"\\n\".join(result_message)\n\n        except Exception as e:\n            return f\"Error during code modification: {str(e)}\\n{traceback.format_exc()}\"\n\n\n    def save_session(self, name: str):\n        \"\"\"Save session with UTF-8 encoding\"\"\"\n        session_file = self._session_dir / f\"{name}.pkl\"\n        user_ns = self.user_ns.copy()\n        output_history = self.output_history.copy()\n\n        # Ensure all strings are properly encoded\n        for key, value in user_ns.items():\n            try:\n                if isinstance(value, str):\n                    value = value.encode('utf-8').decode('utf-8')\n                pickle.dumps(value)\n            except Exception:\n                user_ns[key] = f\"not serializable: {str(value)}\"\n\n        for key, value in output_history.items():\n            try:\n                if isinstance(value, dict):\n                    for k, v in value.items():\n                        if isinstance(v, str):\n                            value[k] = v.encode('utf-8').decode('utf-8')\n                pickle.dumps(value)\n            except Exception:\n                output_history[key] = f\"not serializable: {str(value)}\"\n\n\n        session_data = {\n            'user_ns': user_ns,\n            'output_history': output_history,\n\n        }\n\n        with open(session_file, 'wb') as f:\n            pickle.dump(session_data, f)\n\n        # Save VFS state with UTF-8 encoding\n        vfs_state_file = self._session_dir / f\"{name}_vfs.json\"\n        with open(vfs_state_file, 'w', encoding='utf-8') as f:\n            json.dump(self.vfs.virtual_files, f, ensure_ascii=False)\n\n    def load_session(self, name: str):\n        \"\"\"Load session with UTF-8 encoding\"\"\"\n        session_file = self._session_dir / f\"{name}.pkl\"\n        if session_file.exists():\n            with open(session_file, 'rb') as f:\n                session_data = pickle.load(f)\n                # self.user_ns.update(session_data['user_ns'])\n                self.output_history.update(session_data['output_history'])\n\n        # Load VFS state with UTF-8 encoding\n        vfs_state_file = self._session_dir / f\"{name}_vfs.json\"\n        if vfs_state_file.exists():\n            with open(vfs_state_file, encoding='utf-8') as f:\n                self.vfs.virtual_files = json.load(f)\n\n    def __str__(self):\n        \"\"\"String representation of current session\"\"\"\n        output = []\n        for count, data in self.output_history.items():\n            output.append(f\"In [{count}]: {data['code']}\")\n            if data['stdout']:\n                output.append(data['stdout'])\n            if data['stderr']:\n                output.append(f\"Error: {data['stderr']}\")\n            if data['result'] is not None:\n                output.append(f\"Out[{count}]: {data['result']}\")\n        return \"\\n\".join(output)\n</code></pre> <code>__str__()</code> \u00b6 <p>String representation of current session</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __str__(self):\n    \"\"\"String representation of current session\"\"\"\n    output = []\n    for count, data in self.output_history.items():\n        output.append(f\"In [{count}]: {data['code']}\")\n        if data['stdout']:\n            output.append(data['stdout'])\n        if data['stderr']:\n            output.append(f\"Error: {data['stderr']}\")\n        if data['result'] is not None:\n            output.append(f\"Out[{count}]: {data['result']}\")\n    return \"\\n\".join(output)\n</code></pre> <code>get_namespace()</code> \u00b6 <p>Get current namespace</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def get_namespace(self) -&gt; dict[str, Any]:\n    \"\"\"Get current namespace\"\"\"\n    return self.user_ns.copy()\n</code></pre> <code>load_session(name)</code> \u00b6 <p>Load session with UTF-8 encoding</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def load_session(self, name: str):\n    \"\"\"Load session with UTF-8 encoding\"\"\"\n    session_file = self._session_dir / f\"{name}.pkl\"\n    if session_file.exists():\n        with open(session_file, 'rb') as f:\n            session_data = pickle.load(f)\n            # self.user_ns.update(session_data['user_ns'])\n            self.output_history.update(session_data['output_history'])\n\n    # Load VFS state with UTF-8 encoding\n    vfs_state_file = self._session_dir / f\"{name}_vfs.json\"\n    if vfs_state_file.exists():\n        with open(vfs_state_file, encoding='utf-8') as f:\n            self.vfs.virtual_files = json.load(f)\n</code></pre> <code>modify_code(code=None, object_name=None, file=None)</code> <code>async</code> \u00b6 <pre><code>Modify existing code in memory (user namespace) and optionally in the corresponding file.\n\nThis method updates variables, functions, or methods in the current Python session and can\nalso update the corresponding source file if specified.\n\nArgs:\n    code: New value or implementation for the object\n    object_name: Name of the object to modify (variable, function, or method)\n    file: Path to the file to update (if None, only updates in memory)\n\nReturns:\n    String describing the modification result\n\nExamples:\n\n# 1. Update a variable in memory\nawait ipython.modify_code(code=\"5\", object_name=\"x\")\n</code></pre> <code>reset()</code> \u00b6 <p>Reset the interpreter state</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def reset(self):\n    \"\"\"Reset the interpreter state\"\"\"\n    self.user_ns = {\n        '__name__': '__main__',\n        '__builtins__': __builtins__,\n        'toolboxv2': toolboxv2,\n        '__file__': None,\n        '__path__': [str(self.vfs.current_dir)],\n        'auto_install': auto_install,\n        'modify_code': self.modify_code,\n    }\n    self.output_history.clear()\n    self._execution_count = 0\n    if self.auto_remove:\n        shutil.rmtree(self.vfs.base_dir, ignore_errors=True)\n</code></pre> <code>run_cell(code, live_output=True)</code> <code>async</code> \u00b6 <p>Async version of run_cell that handles both sync and async code</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def run_cell(self, code: str, live_output: bool = True) -&gt; Any:\n    \"\"\"Async version of run_cell that handles both sync and async code\"\"\"\n    result = None\n    error = None\n    tb = None\n    original_dir = os.getcwd()\n\n    if live_output:\n        stdout_buffer = io.StringIO()\n        stderr_buffer = io.StringIO()\n        stdout = TeeStream(sys.__stdout__, stdout_buffer)\n        stderr = TeeStream(sys.__stderr__, stderr_buffer)\n    else:\n        stdout = io.StringIO()\n        stderr = io.StringIO()\n\n    try:\n        # Check if a file is already specified\n        original_file = self.user_ns.get('__file__')\n        if original_file is None:\n            # Create temp file if no file specified\n            temp_file = self.vfs.write_file(\n                f'src/temp/_temp_{self._execution_count}.py',\n                code\n            )\n            # work_ns = self.user_ns.copy()\n            self.user_ns['__file__'] = str(temp_file)\n        else:\n            # Use existing file\n            temp_file = Path(original_file)\n            # Write code to the existing file\n            self.vfs.write_file(temp_file, code)\n            #work_ns = self.user_ns.copy()\n\n        self.user_ns['__builtins__'] = __builtins__\n        with VirtualEnvContext(self._venv_path) as python_exec:\n            try:\n                exec_code, eval_code, is_async, has_top_level_await = self._parse_code(\n                    code.encode('utf-8', errors='replace').decode('utf-8')\n                )\n                if exec_code is None:\n                    return \"No executable code\"\n                os.makedirs(str(temp_file.parent.absolute()), exist_ok=True)\n                os.chdir(str(temp_file.parent.absolute()))\n                self.user_ns['PYTHON_EXEC'] = python_exec\n\n                with redirect_stdout(stdout), redirect_stderr(stderr):\n                    if has_top_level_await:\n                        try:\n                            # Execute wrapped code and await it\n                            exec(exec_code, self.user_ns)\n                            result = self.user_ns['__wrapper']()\n                            if asyncio.iscoroutine(result):\n                                result = await result\n                        finally:\n                            self.user_ns.pop('__wrapper', None)\n                    elif is_async:\n                        # Execute async code\n                        exec(exec_code, self.user_ns)\n                        if eval_code:\n                            result = eval(eval_code, self.user_ns)\n                            if asyncio.iscoroutine(result):\n                                result = await result\n                    else:\n                        # Execute sync code\n                        exec(exec_code, self.user_ns)\n                        if eval_code:\n                            result = eval(eval_code, self.user_ns)\n\n                    if result is not None:\n                        self.user_ns['_'] = result\n            except KeyboardInterrupt:\n                print(\"Stop execution manuel!\")\n\n            except Exception as e:\n                error = str(e)\n                tb = traceback.format_exc()\n                if live_output:\n                    sys.__stderr__.write(f\"{error}\\n{tb}\")\n                stderr.write(f\"{error}\\n{tb}\")\n\n            finally:\n                os.chdir(original_dir)\n                self._execution_count += 1\n                # self.user_ns = work_ns.copy()\n                if live_output:\n                    stdout_value = stdout_buffer.getvalue()\n                    stderr_value = stderr_buffer.getvalue()\n                else:\n                    stdout_value = stdout.getvalue()\n                    stderr_value = stderr.getvalue()\n\n                output = {\n                    'code': code,\n                    'stdout': stdout_value,\n                    'stderr': stderr_value,\n                    'result': result if result else \"stdout\"\n                }\n                self.output_history[self._execution_count] = output\n\n    except Exception as e:\n        error_msg = f\"Error executing code: {str(e)}\\n{traceback.format_exc()}\"\n        if live_output:\n            sys.__stderr__.write(error_msg)\n        return error_msg\n\n    if not result:\n        result = \"\"\n    if output['stdout']:\n        result = f\"{result}\\nstdout:{output['stdout']}\"\n    if output['stderr']:\n        result = f\"{result}\\nstderr:{output['stderr']}\"\n\n    if self.auto_remove and original_file is None:\n        # Only remove temp files, not user-specified files\n        self.vfs.delete_file(temp_file)\n\n    return result\n</code></pre> <code>save_session(name)</code> \u00b6 <p>Save session with UTF-8 encoding</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def save_session(self, name: str):\n    \"\"\"Save session with UTF-8 encoding\"\"\"\n    session_file = self._session_dir / f\"{name}.pkl\"\n    user_ns = self.user_ns.copy()\n    output_history = self.output_history.copy()\n\n    # Ensure all strings are properly encoded\n    for key, value in user_ns.items():\n        try:\n            if isinstance(value, str):\n                value = value.encode('utf-8').decode('utf-8')\n            pickle.dumps(value)\n        except Exception:\n            user_ns[key] = f\"not serializable: {str(value)}\"\n\n    for key, value in output_history.items():\n        try:\n            if isinstance(value, dict):\n                for k, v in value.items():\n                    if isinstance(v, str):\n                        value[k] = v.encode('utf-8').decode('utf-8')\n            pickle.dumps(value)\n        except Exception:\n            output_history[key] = f\"not serializable: {str(value)}\"\n\n\n    session_data = {\n        'user_ns': user_ns,\n        'output_history': output_history,\n\n    }\n\n    with open(session_file, 'wb') as f:\n        pickle.dump(session_data, f)\n\n    # Save VFS state with UTF-8 encoding\n    vfs_state_file = self._session_dir / f\"{name}_vfs.json\"\n    with open(vfs_state_file, 'w', encoding='utf-8') as f:\n        json.dump(self.vfs.virtual_files, f, ensure_ascii=False)\n</code></pre> <code>update_namespace(variables)</code> \u00b6 <p>Update namespace with new variables</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def update_namespace(self, variables: dict[str, Any]):\n    \"\"\"Update namespace with new variables\"\"\"\n    self.user_ns.update(variables)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.MockIPython.modify_code--2-change-a-method-implementation","title":"2. Change a method implementation","text":"<p>await ipython.modify_code(     code='\"\"\"def sound(self):     return \"Woof\"\"\"\"',     object_name=\"Dog.sound\" )</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.MockIPython.modify_code--3-modify-a-function","title":"3. Modify a function","text":"<p>await ipython.modify_code(     code='\"\"\"def calculate_age(): return 25\"\"\"',     object_name=\"calculate_age\" )</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.MockIPython.modify_code--4-update-variable-in-memory-and-file","title":"4. Update variable in memory and file","text":"<p>await ipython.modify_code(     code=\"100\",     object_name=\"MAX_SIZE\",     file=\"config.py\" )</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.MockIPython.modify_code--5-modifying-an-attribute-in-init","title":"5. Modifying an attribute in init","text":"<p>await ipython.modify_code(     code='\"\"\"def init(self):     self.name = \"Buddy\"\"\"\"',     object_name=\"Dog.init\" )</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def modify_code(self, code: str = None, object_name: str = None, file: str = None) -&gt; str:\n    '''\n    Modify existing code in memory (user namespace) and optionally in the corresponding file.\n\n    This method updates variables, functions, or methods in the current Python session and can\n    also update the corresponding source file if specified.\n\n    Args:\n        code: New value or implementation for the object\n        object_name: Name of the object to modify (variable, function, or method)\n        file: Path to the file to update (if None, only updates in memory)\n\n    Returns:\n        String describing the modification result\n\n    Examples:\n\n    # 1. Update a variable in memory\n    await ipython.modify_code(code=\"5\", object_name=\"x\")\n\n# 2. Change a method implementation\nawait ipython.modify_code(\n    code='\"\"\"def sound(self):\\n        return \"Woof\"\"\"\"',\n    object_name=\"Dog.sound\"\n)\n\n# 3. Modify a function\nawait ipython.modify_code(\n    code='\"\"\"def calculate_age():\\n    return 25\"\"\"',\n    object_name=\"calculate_age\"\n)\n\n# 4. Update variable in memory and file\nawait ipython.modify_code(\n    code=\"100\",\n    object_name=\"MAX_SIZE\",\n    file=\"config.py\"\n)\n\n# 5. Modifying an attribute in __init__\nawait ipython.modify_code(\n    code='\"\"\"def __init__(self):\\n        self.name = \"Buddy\"\"\"\"',\n    object_name=\"Dog.__init__\"\n)\n    '''\n    try:\n        if not object_name:\n            raise ValueError(\"Object name must be specified\")\n        if code is None:\n            raise ValueError(\"New code or value must be provided\")\n\n        # Process object name (handle methods with parentheses)\n        clean_object_name = object_name.replace(\"()\", \"\")\n\n        # Step 1: Update in memory (user namespace)\n        result_message = []\n\n        # Handle different types of objects\n        if \".\" in clean_object_name:\n            # For methods or class attributes\n            parts = clean_object_name.split(\".\")\n            base_obj_name = parts[0]\n            attr_name = parts[1]\n\n            if base_obj_name not in self.user_ns:\n                raise ValueError(f\"Object '{base_obj_name}' not found in namespace\")\n\n            base_obj = self.user_ns[base_obj_name]\n\n            # Handle method definitions which are passed as docstrings\n            if code.split('\\n'):\n                method_code = code\n\n                # Parse the method code to extract its body\n                method_ast = ast.parse(method_code).body[0]\n                method_name = method_ast.name\n\n                # Create a new function object from the code\n                method_locals = {}\n                exec(\n                    f\"def _temp_func{signature(getattr(base_obj.__class__, attr_name, None))}: {method_ast.body[0].value.s}\",\n                    globals(), method_locals)\n                new_method = method_locals['_temp_func']\n\n                # Set the method on the class\n                setattr(base_obj.__class__, attr_name, new_method)\n                result_message.append(f\"Updated method '{clean_object_name}' in memory\")\n            else:\n                # For simple attributes\n                setattr(base_obj, attr_name, eval(code, self.user_ns))\n                result_message.append(f\"Updated attribute '{clean_object_name}' in memory\")\n        else:\n            # For variables and functions\n            if code.startswith('\"\"\"') and code.endswith('\"\"\"'):\n                # Handle function definitions\n                func_code = code.strip('\"\"\"')\n                func_ast = ast.parse(func_code).body[0]\n                func_name = func_ast.name\n\n                # Create a new function object from the code\n                func_locals = {}\n                exec(f\"{func_code}\", globals(), func_locals)\n                self.user_ns[clean_object_name] = func_locals[func_name]\n                result_message.append(f\"Updated function '{clean_object_name}' in memory\")\n            else:\n                # Simple variable assignment\n                self.user_ns[clean_object_name] = eval(code, self.user_ns)\n                result_message.append(f\"Updated variable '{clean_object_name}' in memory\")\n\n        # Step 2: Update in file if specified\n        if file is not None:\n            file_path = self.vfs._resolve_path(file)\n\n            if not file_path.exists():\n                self.user_ns['__file__'] = str(file_path)\n                return await self.run_cell(code)\n\n            # Read original content\n            original_content = self.vfs.read_file(file_path)\n            updated_content = original_content\n\n            # Handle different object types for file updates\n            if \".\" in clean_object_name:\n                # For methods\n                parts = clean_object_name.split(\".\")\n                class_name = parts[0]\n                method_name = parts[1]\n\n                if code.startswith('\"\"\"') and code.endswith('\"\"\"'):\n                    method_code = code.strip('\"\"\"')\n\n                    # Use ast to parse the file and find the method to replace\n                    file_ast = ast.parse(original_content)\n                    for node in ast.walk(file_ast):\n                        if isinstance(node, ast.ClassDef) and node.name == class_name:\n                            for method in node.body:\n                                if isinstance(method, ast.FunctionDef) and method.name == method_name:\n                                    # Find the method in the source code\n                                    method_pattern = fr\"def {method_name}.*?:(.*?)(?=\\n    \\w|\\n\\w|\\Z)\"\n                                    method_match = re.search(method_pattern, original_content, re.DOTALL)\n\n                                    if method_match:\n                                        indentation = re.match(r\"^(\\s*)\", method_match.group(0)).group(1)\n                                        method_indented = textwrap.indent(method_code, indentation)\n                                        updated_content = original_content.replace(\n                                            method_match.group(0),\n                                            method_indented\n                                        )\n                                        self.vfs.write_file(file_path, updated_content)\n                                        result_message.append(\n                                            f\"Updated method '{clean_object_name}' in file '{file}'\")\n            else:\n                # For variables and functions\n                if code.startswith('\"\"\"') and code.endswith('\"\"\"'):\n                    # Handle function updates\n                    func_code = code.strip('\"\"\"')\n                    func_pattern = fr\"def {clean_object_name}.*?:(.*?)(?=\\n\\w|\\Z)\"\n                    func_match = re.search(func_pattern, original_content, re.DOTALL)\n\n                    if func_match:\n                        indentation = re.match(r\"^(\\s*)\", func_match.group(0)).group(1)\n                        func_indented = textwrap.indent(func_code, indentation)\n                        updated_content = original_content.replace(\n                            func_match.group(0),\n                            func_indented\n                        )\n                        self.vfs.write_file(file_path, updated_content)\n                        result_message.append(f\"Updated function '{clean_object_name}' in file '{file}'\")\n                else:\n                    # Handle variable updates\n                    var_pattern = fr\"{clean_object_name}\\s*=.*\"\n                    var_replacement = f\"{clean_object_name} = {code}\"\n                    updated_content = re.sub(var_pattern, var_replacement, original_content)\n\n                    if updated_content != original_content:\n                        self.vfs.write_file(file_path, updated_content)\n                        result_message.append(f\"Updated variable '{clean_object_name}' in file '{file}'\")\n                    else:\n                        result_message.append(f\"Could not find variable '{clean_object_name}' in file '{file}'\")\n\n        return \"\\n\".join(result_message)\n\n    except Exception as e:\n        return f\"Error during code modification: {str(e)}\\n{traceback.format_exc()}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.ParentNodeTransformer","title":"<code>ParentNodeTransformer</code>","text":"<p>               Bases: <code>NodeTransformer</code></p> <p>Add parent references to AST nodes</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class ParentNodeTransformer(ast.NodeTransformer):\n    \"\"\"Add parent references to AST nodes\"\"\"\n    def visit(self, node):\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n        return super().visit(node)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.Pipeline","title":"<code>Pipeline</code>","text":"<p>A pipeline for executing AI agent-driven tasks with interactive code execution and variable management.</p> <p>The Pipeline class provides a structured environment for AI agents to: 1. Execute code in a controlled environment 2. Manage and track variables 3. Update methods dynamically 4. Save and load session states 5. Generate detailed variable descriptions</p> <p>Attributes:</p> Name Type Description <code>agent</code> <p>The AI agent instance used for task execution</p> <code>task</code> <code>str</code> <p>The task to be performed</p> <code>mas_iter</code> <code>int</code> <p>Maximum number of iterations allowed (default: 12)</p> <code>variables</code> <code>Dict[str, Any]</code> <p>Dictionary of variables available to the pipeline</p> <code>top_n</code> <code>Optional[int]</code> <p>Limit variable descriptions to top N most used</p> <code>execution_history</code> <code>List[ExecutionRecord]</code> <p>History of executed code and results</p> <code>session_name</code> <code>Optional[str]</code> <p>Name of the current session if saved</p> <code>ipython</code> <p>IPython or MockIPython instance for code execution</p> Example <p>agent = get_free_agent(\"demo\", \"anthropic/claude-3-haiku-20240307\") pipeline = Pipeline( ...     agent=agent, ...     variables={\"n\": 10} ... ) result = pipeline.run(\"Calculate fibonacci sequence\") print(result.result)</p> Notes <ul> <li>The pipeline uses either IPython if available or a MockIPython implementation</li> <li>Variables can be provided as either a dictionary or list</li> <li>Session state can be saved and loaded</li> <li>Method updates are handled through a structured BaseModel approach</li> </ul> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class Pipeline:\n    \"\"\"\n        A pipeline for executing AI agent-driven tasks with interactive code execution and variable management.\n\n        The Pipeline class provides a structured environment for AI agents to:\n        1. Execute code in a controlled environment\n        2. Manage and track variables\n        3. Update methods dynamically\n        4. Save and load session states\n        5. Generate detailed variable descriptions\n\n        Attributes:\n            agent: The AI agent instance used for task execution\n            task (str): The task to be performed\n            mas_iter (int): Maximum number of iterations allowed (default: 12)\n            variables (Dict[str, Any]): Dictionary of variables available to the pipeline\n            top_n (Optional[int]): Limit variable descriptions to top N most used\n            execution_history (List[ExecutionRecord]): History of executed code and results\n            session_name (Optional[str]): Name of the current session if saved\n            ipython: IPython or MockIPython instance for code execution\n\n        Example:\n            &gt;&gt;&gt; agent = get_free_agent(\"demo\", \"anthropic/claude-3-haiku-20240307\")\n            &gt;&gt;&gt; pipeline = Pipeline(\n            ...     agent=agent,\n            ...     variables={\"n\": 10}\n            ... )\n            &gt;&gt;&gt; result = pipeline.run(\"Calculate fibonacci sequence\")\n            &gt;&gt;&gt; print(result.result)\n\n        Notes:\n            - The pipeline uses either IPython if available or a MockIPython implementation\n            - Variables can be provided as either a dictionary or list\n            - Session state can be saved and loaded\n            - Method updates are handled through a structured BaseModel approach\n        \"\"\"\n    def __init__(\n        self,\n        agent: Any,\n        verbose: bool=False,\n        max_iter: int= 12,\n        variables: dict[str, Any] | list[Any] | None = None,\n        top_n: bool | None = None,\n        restore: bool | None = None,\n        max_think_after_think = None,\n        print_f=None,\n        web_js=False,\n        timeout_timer=25,\n        v_agent=None,\n        web_llm=None,\n    ):\n        \"\"\"\n        Initialize the Pipeline.\n\n        Args:\n            agent: AI agent instance to use for task execution\n            verbose: print internal results\n            max_iter: Maximum number of iterations (default: 12)\n            variables: Dictionary or list of variables to make available\n            top_n: Limit variable descriptions to top N most used\n            web_js: if the agent is allow to use the web\n        \"\"\"\n\n        self.timeout_timer = timeout_timer\n        self.top_n = top_n\n        self.max_iter = max_iter\n        self.max_think_after_think = max_think_after_think or max_iter // 2\n        self.agent = agent\n        self.v_agent = v_agent or agent\n        # self.agent.verbose = verbose\n        self.task = None\n        self.web_js = web_js\n        self.print_f = print_f\n        self.verbose_output = EnhancedVerboseOutput(verbose=verbose, print_func=self.print_f)\n        self.variables = self._process_variables(variables or {})\n        self.variables['auto_install'] = auto_install\n        self.execution_history = []\n        self.session_name = None\n\n        self.browser_session: BrowserWrapper | None = BrowserWrapper(llm=web_llm or agent.amd.model)\n        self.js_history: list[JSExecutionRecord] = []\n\n        self._session_dir = Path(get_app().appdata) / 'ChatSession' / agent.amd.name\n        self.ipython = MockIPython(self._session_dir, auto_remove=False)\n        self.chat_session = ChatSession(get_app().get_mod(\"isaa\").get_memory(), space_name=f\"ChatSession/{agent.amd.name}/Pipeline.session\", max_length=max_iter)\n        self.process_memory = ChatSession(get_app().get_mod(\"isaa\").get_memory(), space_name=f\"ChatSession/{agent.amd.name}/Process.session\", max_length=max_iter)\n\n        # Initialize interpreter with variables\n        self.init_keys = list(self.ipython.user_ns.keys()).copy()\n        if self.web_js:\n            self.variables['web_actions'] = self.browser_session.run\n            self.variables['browser_session'] = self.browser_session\n        self.ipython.user_ns.update(self.variables)\n\n        self.restore_var = restore\n\n        if restore:\n            self.restore()\n\n    def on_exit(self):\n        self.chat_session.on_exit()\n        self.process_memory.on_exit()\n        self.save_session(f\"Pipeline_Session_{self.agent.amd.name}\")\n\n    def restore(self):\n        self.load_session(f\"Pipeline_Session_{self.agent.amd.name}\")\n\n    def save_session(self, name: str):\n        \"\"\"Save current session\"\"\"\n        self.session_name = name\n        self.ipython.save_session(name)\n\n    def load_session(self, name: str):\n        \"\"\"Load saved session\"\"\"\n        self.ipython.load_session(name)\n        self.variables.update(self.ipython.user_ns)\n\n\n    def show_graph_html(self, output_file=None, get_output_html=False, get_output_net=False):\n\n        if output_file is None:\n            chat_graph = self.ipython._session_dir / 'chat_graph.html'\n            process_graph = self.ipython._session_dir / 'process_graph.html'\n            output_file = str(chat_graph.absolute())\n            p_output_file = str(process_graph.absolute())\n        else:\n            output_file = output_file + '_chat_graph.html'\n            p_output_file = output_file + '_process_graph.html'\n\n        return (self.chat_session.mem.memories.get(\n            self.chat_session.mem._sanitize_name(\n                self.chat_session.space_name)).vis(output_file=output_file,\n        get_output_html=get_output_html, get_output_net=get_output_net)  ,\n                self.process_memory.mem.memories.get(\n            self.process_memory.mem._sanitize_name(\n                self.process_memory.space_name)).vis(output_file=p_output_file,\n        get_output_html=get_output_html, get_output_net=get_output_net))\n\n    @staticmethod\n    def _process_variables(variables: dict[str, Any] | list[Any]) -&gt; dict[str, Any]:\n        \"\"\"\n        Process variables to generate meaningful names, using actual variable names where possible.\n        Instances get lowercase names based on their class names.\n\n        Args:\n            variables: Dictionary of variables or list of variables to process\n\n        Returns:\n            Dict[str, Any]: Processed variables with meaningful names\n        \"\"\"\n        if isinstance(variables, dict):\n            return variables\n\n        processed = {}\n        name_counts = defaultdict(int)\n\n        # Get caller's frame to find variable names\n        caller_frame = currentframe().f_back\n        caller_locals = {**caller_frame.f_locals, **caller_frame.f_globals}\n\n        def find_var_name(obj: Any) -&gt; str:\n            # Find original variable name if exists\n            var_names = [name for name, val in caller_locals.items()\n                         if val is obj and not name.startswith('_')]\n            if var_names:\n                return var_names[0]\n\n            # Special handling for functions\n            if isfunction(obj) or isclass(obj):\n                return obj.__name__\n            # Handle instances\n            elif hasattr(obj, '__class__'):\n                base_name = obj.__class__.__name__.lower()  # Lowercase for instances\n                count = name_counts[base_name]\n                name_counts[base_name] += 1\n                return f\"{base_name}_{count + 1}\" if count &gt; 0 else base_name\n\n            return type(obj).__name__\n\n        # Process each variable\n        for var in variables:\n            name = find_var_name(var)\n            while name in processed:\n                if name.rpartition('_')[0]:\n                    base, _, num = name.rpartition('_')\n                    try:\n                        num = int(num) + 1\n                        name = f\"{base}_{num}\"\n                    except ValueError:\n                        name = f\"{name}\"\n                else:\n                    name = f\"{name}\"\n\n            processed[name] = var\n\n        return processed\n\n    def _generate_variable_descriptions(\n        self,\n        top_n: int | None = None\n    ) -&gt; str:\n        \"\"\"\n        Generate detailed descriptions of variables, showing args, kwargs, docstrings, and return values.\n\n        Args:\n            top_n: Optional limit to show only top N variables\n\n        Returns:\n            str: Formatted variable descriptions in Markdown\n        \"\"\"\n        if top_n is None:\n            top_n = self.top_n\n\n\n        def get_type_name(tp):\n            if hasattr(tp, '__name__'):\n                return tp.__name__\n            elif isinstance(tp, types.UnionType):  # e.g., int | str (Python 3.10+)\n                return ' | '.join(get_type_name(arg) for arg in tp.__args__)\n            elif hasattr(tp, '__origin__'):  # e.g., list[int], Optional[str]\n                origin = get_type_name(tp.__origin__)\n                if hasattr(tp, '__args__'):\n                    args = ', '.join(get_type_name(arg) for arg in tp.__args__)\n                    return f\"{origin}[{args}]\"\n                return origin\n            elif hasattr(tp, '__class__'):\n                return tp.__class__.__name__\n            return str(tp)\n\n        def format_value_preview(var: Any) -&gt; str:\n            \"\"\"Format preview of variable contents\"\"\"\n            try:\n                if isinstance(var, int | float | bool | str):\n                    return f\"`{repr(var)}`\"\n                elif isinstance(var, list | tuple | set):\n                    preview = str(list(var)[:3])[:-1] + \", ...]\"\n                    return f\"{len(var)} items: {preview}\"\n                elif isinstance(var, dict):\n                    preview_items = [f\"{repr(k)}: {repr(v)}\" for k, v in list(var.items())[:3]]\n                    return f\"{len(var)} pairs: {{{', '.join(preview_items)}, ...}}\"\n                return f\"&lt;{get_type_name(type(var))}&gt;\"\n            except:\n                return \"&lt;error getting value&gt;\"\n\n        def get_instance_state(var: Any) -&gt; dict[str, Any]:\n            \"\"\"Get current instance state\"\"\"\n            state = {}\n            if hasattr(var, '__dict__'):\n                for name, value in var.__dict__.items():\n                    if not name.startswith('_') and not callable(value):\n                        state[name] = format_value_preview(value)\n            return state\n\n        # Process variables\n        variables = self.variables.items()\n        if top_n:\n            variables = list(variables)[:top_n]\n\n        descriptions = []\n        for name, var in variables:\n            if name in [\"PYTHON_EXEC\", \"__name__\", \"__builtins__\", \"__path__\", \"asyncio\"]:\n                continue\n\n            desc_parts = [f\"### {name}\"]\n\n            # Handle different types\n            if isinstance(var, type):  # Class\n                desc_parts.append(f\"**Type:** `class '{get_type_name(var)}'`\")\n                if var.__doc__:\n                    desc_parts.append(f\"**Documentation:**\\n{var.__doc__.strip()}\")\n\n                # Show methods\n                methods = []\n                for attr_name, attr in var.__dict__.items():\n                    if (not attr_name.startswith('_') or attr_name == \"__init__\") and (isfunction(attr) or ismethod(attr)):\n                        try:\n                            sig = signature(attr)\n                            is_a = asyncio.iscoroutinefunction(var)\n                            methods.append(f\"- `{attr_name}{sig}` Async: `{is_a}\")\n                            if attr.__doc__:\n                                r = attr.__doc__.split('\\n')[0]\n                                methods.append(f\"  {r}\")\n                        except:\n                            methods.append(f\"- `{attr_name}()`\")\n                if methods:\n                    desc_parts.append(\"**Methods:**\\n\" + \"\\n\".join(methods))\n\n            elif isfunction(var) or ismethod(var):  # Function\n                try:\n                    sig = signature(var)\n                    desc_parts.append(f\"**Signature:** `{get_type_name(var)}{sig}`\")\n                    is_a = asyncio.iscoroutinefunction(var)\n                    desc_parts.append(f\"**IS Async:** `{is_a}`\")\n                    if var.__doc__:\n                        desc_parts.append(f\"**Documentation:**\\n{var.__doc__.strip()}\")\n                    ret_anno = sig.return_annotation\n                    if ret_anno != Signature.empty:\n                        desc_parts.append(f\"**Returns:** `{ret_anno}`\")\n                except:\n                    desc_parts.append(f\"**Function:** `{get_type_name(var)}()`\")\n\n            elif isinstance(var, BaseModel):  # Pydantic model\n                desc_parts.append(f\"**Type:** Pydantic model '{get_type_name(var)}'\")\n                fields = []\n                for field_name, field in var.model_fields.items():\n                    value = getattr(var, field_name, None)\n                    fields.append(f\"- `{field_name}: {get_type_name(field.annotation)}` = {repr(value)}\")\n                if fields:\n                    desc_parts.append(\"**Fields:**\\n\" + \"\\n\".join(fields))\n\n            else:  # Instance\n                class_type = var.__class__\n                desc_parts.append(f\"**Type:** `{class_type.__module__}.{get_type_name(class_type)}`\")\n\n                # Instance initialization details\n                try:\n                    init = class_type.__init__\n                    sig = signature(init)\n                    params = list(sig.parameters.items())[1:]  # Skip self\n                    if params:\n                        args = []\n                        for name, param in params:\n                            if param.default == param.empty:\n                                args.append(name)\n                            else:\n                                args.append(f\"{name}={param.default}\")\n                        desc_parts.append(f\"**Init Args:** `{', '.join(args)}`\")\n                except:\n                    pass\n\n                # Instance state\n                state = get_instance_state(var)\n                if state:\n                    desc_parts.append(\"**Current instance State:**\")\n                    for attr_name, attr_value in state.items():\n                        desc_parts.append(f\"- `{attr_name}` = {attr_value}\")\n\n                # Documentation\n                doc = getdoc(var) or getdoc(class_type)\n                if doc:\n                    desc_parts.append(f\"**Documentation:**\\n{doc.strip()}\")\n\n            descriptions.append(\"\\n\".join(desc_parts))\n\n        return \"\\n\\n\".join(descriptions)\n\n    async def _execute_code(self, code: str, context:dict) -&gt; ExecutionRecord:\n        \"\"\"Execute code and track results\"\"\"\n        lang = context.get('lang', 'py')\n        try:\n\n            if'py' in lang:\n\n                return await self._execute_py(code)\n\n            elif self.web_js and 'js' in lang:\n                return await self._execute_js(code, context)\n\n        except Exception as e:\n            record = ExecutionRecord(code=code, result=None, error=str(e))\n            self.execution_history.append(record)\n            return record\n        record = ExecutionRecord(code=code, result=None, error=f\"Invalid lang {lang} valid is, {'js' if self.web_js else 'py'}]\")\n        self.execution_history.append(record)\n        return record\n\n    async def _execute_py(self, code) -&gt; ExecutionRecord:\n        show = True #len(code) &gt; 450 and code.count('while') &gt; 1 and code.count('print') &gt;= 1\n        result = await self.ipython.run_cell(code, show)\n\n        all_keys = list(self.ipython.user_ns.keys())\n\n        new_keys = [key for key in all_keys if key not in self.init_keys]\n        # Update pipeline variables from IPython namespace\n\n        for var_name in new_keys:\n            if var_name.startswith('_'):\n                continue\n            self.variables[var_name] = self.ipython.user_ns[var_name]\n\n        record = ExecutionRecord(code=code, result=result, error=None)\n        self.execution_history.append(record)\n        return record\n\n    async def _execute_js(self, code: str, context: dict) -&gt; ExecutionRecord:\n        \"\"\"Execute JavaScript code in browser context\"\"\"\n\n        if '&lt;script&gt;' in code:\n            code = code.split('&lt;script&gt;')[1]\n        if '&lt;/script&gt;' in code:\n            code = code.split('&lt;/script&gt;')[0]\n        def _format_error_markdown(error: str) -&gt; str:\n            \"\"\"Format error as Markdown\"\"\"\n            return f\"\"\"\n# Execution Error\n{error}\n\"\"\"\n\n        def _format_result_markdown(result_: dict) -&gt; str:\n            \"\"\"Format execution result as Markdown\"\"\"\n\n            def _clean_html_content(html: str) -&gt; str:\n                \"\"\"Clean HTML content and convert to Markdown-like format\"\"\"\n                soup = BeautifulSoup(html, 'html.parser')\n\n                # Remove scripts and styles\n                for script in soup([\"script\", \"style\"]):\n                    script.decompose()\n\n                # Extract text\n                text = soup.get_text()\n\n                # Clean up whitespace\n                lines = (line.strip() for line in text.splitlines())\n                chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n                text = '\\n'.join(chunk for chunk in chunks if chunk)\n\n                # Add Markdown formatting\n                text = re.sub(r'^(.+)$', r'&gt; \\1', text, flags=re.MULTILINE)\n\n                return text\n\n            md_parts = []\n\n            # Add title\n            md_parts.append(\"# Page Analysis Results\\n\")\n\n            # Format JavaScript result\n            if result_.get('js_result'):\n                md_parts.append(\"## JavaScript Execution Result\")\n                md_parts.append(\"```\")\n                md_parts.append(str(result_['js_result']))\n                md_parts.append(\"```\\n\")\n\n            # Format page state\n            if 'page_state' in result_:\n                md_parts.append(\"## Page Information\")\n                md_parts.append(f\"- **URL**: {result_['page_state']['url']}\")\n                md_parts.append(f\"- **Title**: {result_['page_state']['title']}\\n\")\n\n                # Clean and format content\n                if 'content' in result_['page_state']:\n                    content = _clean_html_content(result_['page_state']['content'])\n                    if content:\n                        md_parts.append(\"### Page Content\")\n                        md_parts.append(content + \"\\n\")\n\n            # Format extracted data\n            if result_.get('extracted_data'):\n                md_parts.append(\"## Extracted Data\")\n                for key, value in result_['extracted_data'].items():\n                    if value:\n                        md_parts.append(f\"### {key.replace('_', ' ').title()}\")\n                        if isinstance(value, list):\n                            for item in value:\n                                md_parts.append(f\"- {item}\")\n                        else:\n                            md_parts.append(str(value))\n                        md_parts.append(\"\")\n\n            return \"\\n\".join(md_parts)\n\n        try:\n            # Prepare execution context\n            url = context.get('url')\n            page = None\n            result = None\n            page_state = {}\n\n            extracted_data = None\n            if url:\n                page = await self.browser_session.navigate(url)\n                parser = self.browser_session.get_parser()\n                markdown = await parser.to_markdown(page)\n\n                if 'patterns' in context:\n                    extracted_data = await parser.to_structured(page, context['patterns'])\n\n                page_state = {\n                    'url': page.url,\n                    'title': await page.title(),\n                    'content': markdown,\n                }\n\n            if code:\n                result = await self.browser_session.execute_js(code, page)\n\n                if isinstance(result, dict) and 'success' in result:\n                    if not result['success']:\n                        raise Exception(f\"JavaScript Error: {result.get('error')}\\nStack: {result.get('stack')}\")\n                    result = result.get('result')\n\n            # Capture page state after execution\n\n\n            # Extract data using patterns if specified\n\n            # Create execution record\n            record = JSExecutionRecord(\n                code=code,\n                result=result,\n                page_state=page_state,\n                extracted_data=extracted_data\n            )\n\n            self.js_history.append(record)\n\n            # Convert to standard ExecutionRecord for pipeline\n            return ExecutionRecord(\n                code=code,\n                result=_format_result_markdown({\n                    'js_result': result,\n                    'page_state': page_state,\n                    'extracted_data': extracted_data\n                }),\n                error=None\n            )\n\n        except Exception as e:\n            error_md = _format_error_markdown(str(e))\n            return ExecutionRecord(code=code, result=None, error=error_md)\n\n\n    def __str__(self):\n        \"\"\"String representation of pipeline session\"\"\"\n        return str(self.ipython)\n\n    async def _process_think_result(self, think_result: ThinkResult, task:str) -&gt; tuple[ThinkState,  ExecutionRecord | str | None]:\n        \"\"\"Process the result of agent thinking\"\"\"\n        if think_result.action == 'brake':\n            return ThinkState.BRAKE, think_result.content\n\n        elif think_result.action == 'update':\n            if think_result.context.get('object_name') is None:\n                return ThinkState.ACTION, \"no object_name specified in context!\"\n            if think_result.context.get('file') is not None:\n                self.ipython.user_ns['__file__'] = think_result.context.get('file')\n            result = await self.verbose_output.process(think_result.action,\n                                                       self.ipython.modify_code(code=think_result.content,\n                                                    object_name=think_result.context.get('object_name'),))\n            return ThinkState.PROCESSING, result\n\n        elif think_result.action == 'code':\n            if think_result.context.get('file') is not None:\n                self.ipython.user_ns['__file__'] = think_result.context.get('file')\n            result = await self._execute_code(think_result.content, think_result.context)\n            return ThinkState.PROCESSING, result\n\n        elif think_result.action == 'done':\n            return ThinkState.DONE, think_result.content\n\n        elif think_result.action == 'infos':\n            infos = await self.chat_session.get_reference(think_result.content, to_str=True)\n            return ThinkState.ACTION, infos\n\n        elif think_result.action == 'guide':\n            details = await self.process_memory.get_reference(think_result.content, to_str=True)\n            plan = await self.agent.a_run(f\"\"\"You are an AI guidance system designed to help determine the next step in a task and provide instructions on how to proceed. Your role is to analyze the given information and offer clear, actionable guidance for the next steps.\n\nFirst, carefully read and understand the main task:\n&lt;main_task&gt;\n{task}\n&lt;/main_task&gt;\n\nNext, review the last thought of the agent, if available:\n&lt;last_thought&gt;\n{think_result.content}\n{think_result.context}\n&lt;/last_thought&gt;\n\nThen, examine the processing history, if provided:\n&lt;processing_history&gt;\n{details}\n&lt;/processing_history&gt;\n\nTo determine the next step and provide guidance, follow these instructions:\n\n1. Analyze the main task, breaking it down into smaller, manageable steps if necessary.\n2. Consider the last thought and processing history to understand the current progress and context.\n3. Identify any gaps, challenges, or areas that need further attention.\n4. Determine the most logical and efficient next step to move the task forward.\n5. Provide clear, concise instructions on how to complete this next step.\n\nWhen formulating your response, follow this structure:\n\n1. Begin with a brief summary of the current situation, referencing the main task and any relevant information from the last thought or processing history.\n2. Clearly state the next step that should be taken.\n3. Provide detailed instructions on how to complete this step, including any specific techniques, methods, or considerations to keep in mind.\n4. If applicable, mention any potential challenges or pitfalls to be aware of during this step.\n5. Conclude with a brief statement on how this step contributes to the overall progress of the main task.\n\nFormat your response using the following sections:\n&lt;summary&gt;\n(Include your summary of the current situation here)\n&lt;/summary&gt;\n\n&lt;next_step&gt;\n(State the next step to be taken here)\n&lt;/next_step&gt;\n\n&lt;instructions&gt;\n(Provide detailed instructions for completing the next step here)\n&lt;/instructions&gt;\n\n&lt;challenges&gt;\n(If applicable, mention potential challenges or pitfalls here)\n&lt;/challenges&gt;\n\n&lt;conclusion&gt;\n(Briefly state how this step contributes to overall progress)\n&lt;/conclusion&gt;\n\nRemember to be clear, concise, and specific in your guidance. Avoid vague or ambiguous instructions, and provide concrete examples or explanations where necessary.\"\"\",persist_history=False, strategy_override=\"direct_llm\")\n            return ThinkState.ACTION, plan\n\n        return ThinkState.ACTION, None\n\n    async def execute(self, code:str):\n        return str(await self._execute_code(code))\n\n    def clear(self):\n        self.chat_session.history = []\n        self.process_memory.history = []\n        self.execution_history = []\n        self.variables = {}\n        self.ipython.reset()\n        self.js_history = []\n\n    async def get_process_hint(self, task):\n        return await self.process_memory.get_reference(task, to_str=True), await self.chat_session.get_reference(task, to_str=True)\n\n    def show_vars(self):\n        return self.verbose_output.log_state(\"VARS\", self.variables, override=True)\n\n    def set_file(self, full_file_path_and_name):\n        if not os.path.exists(full_file_path_and_name):\n            print(\"Invalid file\")\n            return\n        self.ipython.user_ns[\"__file__\"] = full_file_path_and_name\n\n    async def run(self, task, do_continue=False) -&gt; PipelineResult:\n        \"\"\"Run the pipeline with separated thinking and processing phases\"\"\"\n        state = ThinkState.ACTION\n        result = None\n        original_task = task\n        if not do_continue:\n            task = await self.agent.a_run(f\"\"\"You are an AI assistant tasked with refactoring a user-provided task description into a more structured format with context learning and examples. Your goal is to create a comprehensive and well-organized task description that incorporates model flows and potential code fixes.\n\nFirst, I will provide you with a task description and some example tasks. Please read them carefully:\n\n&lt;existing_globals&gt;\n{self._generate_variable_descriptions()}\n&lt;/existing_globals&gt;\n\n&lt;example_tasks&gt;\nTask: Create a simple analysis of a list of numbers\n- Generate a list of 100 random numbers between 1-1000\n- Calculate the mean, median, and standard deviation\n- Create a histogram of the distribution\n- Print all results and display the plot\n\nTask: Create a reinforcement learning (RL) agent to play a simple game\n- Set up an OpenAI Gym environment (e.g., CartPole)\n- Implement a Q-learning or Deep Q-Network (DQN) agent\n- Train the model and optimize hyperparameters\n- Visualize learning progress with reward graphs\n- Save and reload trained models for inference\n- Provide an option to let the trained agent play in real time\n\nTask: Perform edge detection on an image\n- Load an image from a URL or local file\n- Convert the image to grayscale\n- Apply Gaussian blur to reduce noise\n- Use Canny edge detection to extract edges\n- Display the original and processed images side by side\n- Save the output image\n\nTask: Build a basic sentiment analysis system\n- Load a dataset of movie reviews (you can use a small sample)\n- Preprocess the text (remove punctuation, lowercase, etc.)\n- Create a TF-IDF vectorizer\n- Split data into training and testing sets\n- Train a classifier (e.g., Naive Bayes or LogisticRegression)\n- Evaluate performance with accuracy, precision, recall\n- Create a confusion matrix visualization\n- Make predictions on new sample texts\n&lt;/example_tasks&gt;\n\nNow, please refactor the given task description using the following guidelines:\n\n1. Analyze the task description and identify the main components and objectives.\n\n2. Structure the refactored task in a similar format to the example tasks, including:\n   - A clear title that summarizes the task\n   - A difficulty level (Easy, Intermediate, Hard, or Super Hard)\n   - A brief introduction to the task's context and purpose\n   - A code block containing step-by-step instructions\n   - A list of required skills, libraries, or technologies\n\n3. Incorporate model flows by breaking down the task into logical steps and explaining the process flow.\n\n4. Include potential code fixes or common pitfalls that users might encounter while working on the task.\n\n5. Add context learning elements by providing brief explanations or resources for key concepts related to the task.\n\n6. Ensure that the refactored task is comprehensive and can stand alone as a learning exercise.\n\nPlease provide your refactored task description within &lt;refactored_task&gt; tags. Use appropriate subheadings and formatting to make the description clear and easy to read.\n\nAdditional tips:\n- Mention any prerequisites or assumed knowledge\n- Suggest potential extensions or variations of the task for further learning\n\nRemember to maintain the original intent and complexity of the task while improving its structure and clarity. the task is: {task}\"\"\", persist_history=False, strategy_override=\"direct_llm\")\n            if '&lt;refactored_task&gt;' in task:\n                task = task.split('&lt;refactored_task&gt;')[1]\n            if '&lt;/refactored_task&gt;' in task:\n                task = task.split('&lt;/refactored_task&gt;')[0]\n        code_follow_up_prompt = f\"\"\"\nYou are an AI assistant responsible for evaluating task completion and providing feedback on the execution process. Your goal is to determine if a given task has been completed based on the execution result, and to offer insights for future improvements.\n\nYou will be provided with two inputs:\n&lt;task_description&gt;\n{original_task}\n{f'&lt;refactored_task_description_from_ai&gt;{task}&lt;/refactored_task_description_from_ai&gt;' if not do_continue else ''}\n&lt;/task_description&gt;\n\n&lt;code&gt;\n#CODE#\n&lt;/code&gt;\n\n&lt;execution_result&gt;\n#EXECUTION_RESULT#\n&lt;/execution_result&gt;\n\nFirst, carefully analyze the task description and the execution result. Determine whether the task has been completed successfully based on the information provided.\n\nIf the task is completed:\n1. Prepare a brief statement indicating that the task is done.\n2. Summarize the output for the user in a clear and concise manner.\n\nIf the task is not completed:\n1. Prepare a brief statement indicating that the task is not done.\n2. Identify the specific aspects of the task that remain incomplete.\n\nRegardless of task completion status, evaluate the procedure and effectiveness of the execution:\n1. Analyze the workflow: Describe the steps taken in the execution process.\n2. Assess effectiveness: Determine how well the procedure achieved the desired outcome.\n3. Identify errors: Pinpoint any mistakes or inefficiencies in the execution.\n4. Provide recommendations: Suggest improvements for future task executions.\n\ntip: Enclose mutil line strings property for python eval to function!\ntip: Set is_completed True if all requirements are completed from &lt;task_description&gt;.\ntip: Help the Agent with your analyses to finalize the &lt;task_description&gt;.\n{'tip: Prefer new informations from &lt;execution_result&gt; over &lt;refactored_task_description_from_ai&gt; based of &lt;code&gt;' if not do_continue else ''}\nnote : for the final result only toke information from the &lt;execution_result&gt;. if the relevant informations is not avalabel try string withe tips in the recommendations. else set is_completed True and return the teh Task failed!\nEnsure that your evaluation is thorough, constructive, and provides actionable insights for improving future task executions.\nAdd guidance based on the the last execution result\"\"\"\n        code_follow_up_prompt_ = [code_follow_up_prompt]\n        initial_prompt = f\"\"\"\nYou are an AI py coding agent specializing in iterative development and code refinement, designed to perform tasks that involve thinking. Your goal is to complete the given task while demonstrating a clear thought process throughout the execution.\nSYSTEM STATE:\n&lt;current_state&gt;\nIteration: #ITER#\nStatus: #STATE#\nLast EXECUTION: #EXECUTION#\n&lt;/current_state&gt;\n\nENVIRONMENT: {'current file :'+self.ipython.user_ns.get(\"__file__\")  if self.ipython.user_ns.get(\"__file__\") is not None else ''}\n\n'''&lt;global_variables&gt;\n#LOCALS#\n&lt;/global_variables&gt;'''\n\nMEMORY:\n&lt;process_memory&gt;\n#PHINT#\n&lt;/process_memory&gt;\n\n&lt;chat_memory&gt;\n#CHINT#\n&lt;/chat_memory&gt;\n\nVALIDATION CHECKLIST (Must verify before each action):\n1. \u2713 Check existing variables in ENVIRONMENT &lt;global_variables&gt;\n2. \u2713 Verify existing functions and classes\n3. \u2713 Review current imports\n4. \u2713 Confirm method signatures\n5. \u2713 Validate state preservation\n\nWORKFLOW STEPS:\n1. Analyze Current State:\n   - Reason and use all avalabel context\n   - Do not repeat the same errors\n   - Review existing implementations\n   - Check variable values\n   - Verify import statements\n   - Document dependencies\n\n2. Plan Change:\n   - NO example/simulation/simulate\n   - No demo er moc Data no Simulations Allowed or u will die!!\n   - Use existing variables and code when possible\n   - Prefer updates over rewrites\n\n3. Execute Change:\n   - Use appropriate action\n   - Maintain existing state\n   - Document modifications\n   - Verify results\n\nYou will use a structure called ThinkResult to organize your thoughts and actions.\nFor each step of your task, follow this process:\n\nACTIONS:\n1. 'code':\n    - MUST check &lt;global_variables&gt; first\n    - NEVER create demo functions\n    - Include 'reason'\n    - lang default 'py'\n    - Required: code in content\n    - code MUST call a function or display the row variabel / value at the end!\n    - Required: {{'context':{{'lang':'py',  'reason': ... }}...}}\n    - Optional file key in context example {{'context':{{'lang':'py',  'file': 'main.py' ,  'reason': ... }}...}}\n    - py code allows for toplevel await !!! use it !!! like\n:file-start:\nprint(\"using toplevel await\")\nawait abc()\n:file-end:\n\n    - Tip: use comments to reason with in the code\n3. 'infos': Request specific details\n4. 'guide': Get step clarification use on complex task and ery 5 step for staying on trak!\n5. 'brake': Pause for assessment\n6. 'done': Summarize changes\n\nCODE CONSTRAINTS:\n1. State Preservation:\n   - ALL variables ar persist\n   - ALL functions remain\n   - ALL classes ar maintained\n\n2. Import Management:\n   - Check &lt;global_variables&gt; for modules\n   - Use absolute imports\n   - Document new dependencies\n\n3. Function Handling:\n   - NEVER overwrite existing\n   - Use update for changes\n   - Preserve signatures\n\n4. Variable Scope:\n   - Maintain existing scope\n   - Check for conflicts\n   - Document state changes\n\nEXECUTION RULES:\n1. VERIFY before create\n2. UPDATE don't replace\n3. TEST after each change\n\nNext Action Required:\n1. Review current state\n2. Check existing code\n3. Execute with state preservation\n\n!!CRITICAL!!\n- NO demo functions\n- NO placeholder functions\n- USE existing code\n- FOR Implementations prefer writing large production redy code chunks.\n- FOR reasoning and validation write small code blocks.\n- THE CODE must call something or end the code with an value!\n- NO INFINIT LOOPS! none breakable while loops ar not allowed, exception ui (closed by user)\n- NO 'python' top level return, only write the variabel or value itself!\n- 'code is run using exec! do not use !pip ...'\n'- instead use auto_install(package_name, install_method=\"pip\", upgrade=False, quiet=False, version=None, extra_args=None)'\n# Example usage first time\n\u2502 auto_install('pandas', version='1.3.0')\n\u2502 import pandas\n\u2502 auto_install('pygame')\n\u2502 import pygame\n\u2502 auto_install('numpy')\n\u2502 import numpy as np\n!TIPS!\n- '&lt;global_variables&gt; can contain instances and functions you can use in your python' code\n- if the function is async you can use top level await\n- if their is missing of informations try running code to get the infos\n- if you got stuck or need assistance break with a question to the user.\n'- run functions from &lt;global_variables&gt; using name(*args, **kwargs) or await name(*args, **kwargs)'\n'- &lt;global_variables&gt; ar global accessible!'\n'- if an &lt;global_variables&gt; name is lower lists an redy to use instance'\n\"\"\"\n        p_hint, c_hint = await self.get_process_hint(task)\n        initial_prompt = initial_prompt.replace('#PHINT#', p_hint)\n        initial_prompt = initial_prompt.replace('#CHINT#', c_hint)\n        initial_prompt_ = initial_prompt\n        iter_i = 0\n        iter_p = 0\n        iter_tat = 0\n        next_infos = \"\"\n        if not do_continue:\n            await self.chat_session.add_message({'role': 'user', 'content': task})\n        else:\n            self.restore()\n            await self.chat_session.add_message({'role': 'user', 'content': task})\n\n        if self.web_js and self.browser_session is None:\n            self.browser_session = BrowserWrapper(llm=self.agent.amd.modle)\n\n        # await self.verbose_output.log_message('user', task)\n        self.verbose_output.log_header(task)\n        while state != ThinkState.DONE:\n            iter_i += 1\n            t0 = time.perf_counter()\n            prompt = initial_prompt.replace('#ITER#', f'{iter_i} max {self.max_iter}')\n            prompt = prompt.replace('#STATE#', f'{state.name}')\n            prompt = prompt.replace('#EXECUTION#', f'{next_infos}')  if next_infos else prompt.replace('Last EXECUTION: #EXECUTION#', '')\n            prompt = prompt.replace('#LOCALS#', f'{self._generate_variable_descriptions()}')\n            self.verbose_output.log_state(state.name, {})\n            self.verbose_output.print(Style.GREY(f\"{iter_i}/{self.max_iter}\"))\n            if state == ThinkState.ACTION:\n                iter_tat +=1\n                if iter_tat &gt; self.max_think_after_think:\n                    state = ThinkState.BRAKE\n            else:\n                iter_tat = 0\n\n            if state == ThinkState.ACTION:\n                # Get agent's thoughts\n                think_dicts = await self.verbose_output.process(state.name, self.agent.a_format_class(\n                    ThinkResults,\n                    prompt,\n                    message_context=self.chat_session.get_past_x(self.max_iter*2, last_u=not do_continue).copy()+([self.process_memory.history[-1]] if self.process_memory.history else []) ,\n                ))\n                think_dicts = think_dicts.get(\"actions\")\n                if think_dicts is None:\n                    think_dicts = [await self.verbose_output.process(state.name, self.agent.a_format_class(\n                        ThinkResult,\n                        prompt,\n                        message_context=self.chat_session.get_past_x(self.max_iter * 2, last_u=not do_continue).copy() + (\n                            [self.process_memory.history[-1]] if self.process_memory.history else []),\n                    ))]\n                if len(think_dicts) == 1:\n                    think_dict = think_dicts[0]\n                else:\n                    for think_dict in think_dicts[:-1]:\n                        if think_dict.get('context') is None:\n                            think_dict['context'] = {'context': 'N/A'}\n                        if not isinstance(think_dict.get('context'), dict):\n                            think_dict['context'] = {'context': think_dict.get('context')}\n                        think_result = ThinkResult(**think_dict)\n                        await self.chat_session.add_message(\n                            {'role': 'assistant', 'content': think_result.content + str(think_result.context)})\n                        state, result = await self.verbose_output.process(think_dict.get(\"action\"),\n                                                                          self._process_think_result(think_result,\n                                                                                                     task=task))\n                        if result:\n                            await self.chat_session.add_message(\n                                {'role': 'system', 'content': 'Evaluation: ' + str(result)})\n                            await self.verbose_output.log_message('system', str(result))\n                    think_dict = think_dicts[-1]\n                self.verbose_output.formatter.print_code_block(think_dict.get(\"content\"), 'python' if think_dict.get(\"context\", {'lang': 'py'}).get('lang') == 'py' else 'javascript')\n                if think_dict.get('context') is None:\n                    think_dict['context'] = {'context': 'N/A'}\n                if not isinstance(think_dict.get('context'), dict):\n                    think_dict['context'] = {'context': think_dict.get('context')}\n                think_result = ThinkResult(**think_dict)\n                state, result = await self.verbose_output.process(think_dict.get(\"action\"), self._process_think_result(think_result, task=task))\n                await self.chat_session.add_message({'role': 'assistant', 'content': think_result.content + str(think_result.context)})\n                if result:\n                    await self.chat_session.add_message({'role': 'system', 'content': 'Evaluation: '+str(result)})\n                    await self.verbose_output.log_message('system', str(result))\n                    code_follow_up_prompt_[0] = code_follow_up_prompt.replace(\"#EXECUTION_RESULT#\", str(result))\n                    if isinstance(result ,ExecutionRecord):\n                        code_follow_up_prompt_[0] = code_follow_up_prompt_[0].replace(\"#CODE#\", result.code)\n                    else:\n                        code_follow_up_prompt_[0] = code_follow_up_prompt_[0].replace(\"#CODE#\", self._generate_variable_descriptions())\n                else:\n                    code_follow_up_prompt_[0] = code_follow_up_prompt.replace(\"#EXECUTION_RESULT#\", str(think_result))\n                    code_follow_up_prompt_[0] = code_follow_up_prompt_[0].replace(\"#CODE#\",\n                                                                              self._generate_variable_descriptions())\n\n\n            elif state == ThinkState.PROCESSING:\n                # Get agent's thoughts\n                class Next(BaseModel):\n                    is_completed: bool\n                    recommendations: str\n                    errors: str\n                    effectiveness: str\n                    workflow: str\n                    text: str\n                # Format the agent's thoughts into a structured response\n                _agent = self.v_agent if self.v_agent is not None else self.agent\n                next_dict = await self.verbose_output.process(state.name, _agent.a_format_class(\n                    Next,\n                    code_follow_up_prompt_[0],\n                    message_context=self.chat_session.get_past_x(self.max_iter*2, last_u=not do_continue).copy(),\n                ))\n                next_infos = json.dumps(next_dict)\n                await self.verbose_output.log_process_result(next_dict)\n                await self.process_memory.add_message({'role': 'assistant', 'content': next_infos.replace('workflow:', 'past-workflow:')})\n                iter_p += 1\n                code_follow_up_prompt_[0] = code_follow_up_prompt\n                if not next_dict.get('is_completed', True):\n                    state = ThinkState.ACTION\n                    initial_prompt = initial_prompt_.replace('#ITER#',f'#ITER#\\nReasoning assist result: {next_dict}')\n                    continue\n                elif next_dict.get('is_completed', False):\n                    result = next_dict.get('text', '')\n                    state = ThinkState.DONE\n                    continue\n                else:\n                    result = next_dict.get('text', '')\n                    break\n\n            elif state == ThinkState.BRAKE:\n                break\n\n            if iter_i &lt; self.max_iter:\n                await asyncio.sleep(1)\n                # if time.perf_counter() -t0 &lt; self.timeout_timer*2.5:\n                #     with Spinner(f\"Prevent rate limit posing for {self.timeout_timer}s\", symbols='+', time_in_s=self.timeout_timer, count_down=True):\n                #         await asyncio.sleep(self.timeout_timer)\n            else:\n                state = ThinkState.BRAKE\n                if isinstance(result, ExecutionRecord):\n                    result = result.result\n                elif isinstance(result, str):\n                    pass\n                else:\n                    result = \"Max iterations\"\n                break\n\n        self.verbose_output.log_state(state.name, {})\n\n        return PipelineResult(\n            variables=self.variables,\n            result=result,\n            execution_history=self.execution_history,\n            message=self.chat_session.get_past_x(iter_i*2, last_u=not do_continue),\n        )\n\n    async def run_project(self, task, lang='py', execute_function=None):\n        if execute_function is None:\n            if lang == 'py':\n                execute_function = default_python_execute_function\n            elif lang == 'rust':\n                execute_function = default_rust_execute_function\n            else:\n                raise ValueError(f\"Unsupported language: {lang}\")\n        class FileAction(BaseModel):\n            action: str\n            path: str\n            content: str | None = None\n\n        class ProjectThinkResult(BaseModel):\n            action: str\n            file_actions: list[FileAction]\n            reasoning: str\n\n        class ProjectPipelineResult(BaseModel):\n            result: str\n            execution_history: list[str]\n            files: dict[str, str]\n        state = ThinkState.ACTION\n        result = None\n        vfs = VirtualFileSystem(self._session_dir / f\"project_{lang}\")\n\n        project_prompt = f\"\"\"\n    You are an AI coding agent specializing in {lang} project development. Your task is to create, modify, and manage files within a project structure to complete the given task. Use the VirtualFileSystem to interact with files.\n\n    TASK DESCRIPTION:\n    {task}\n    CURRENT FILES:\n    #files#\n\n    WORKFLOW STEPS:\n    1. Analyze the current project state\n    2. Plan necessary changes or additions\n    3. Execute changes using file actions\n    4. Evaluate the project's progress\n\n    Use the ProjectThinkResult structure to organize your thoughts and actions:\n\n    class ProjectThinkResult(BaseModel):\n        action: str  # 'code', 'evaluate', 'done'\n        file_actions: List[FileAction]\n        reasoning: str\n\n    class FileAction(BaseModel):\n        action: str  # 'write', 'read', 'delete', 'list'\n        path: str\n        content: Optional[str] = None\n\n    EXECUTION RULES:\n    1. Use absolute paths for all file operations\n    2. Maintain a clear project structure\n    3. Document your code and reasoning\n    4. Ensure all necessary files are created and properly linked\n    5. Use the appropriate language syntax and best practices for {lang}\n\n    Next Action Required:\n    1. Review the current project state\n    2. Plan the next step in project development\n    3. Execute file actions to implement changes\n    \"\"\"\n\n        execution_history = []\n        files = {}\n\n        iter_i = 0\n        self.verbose_output.log_header(task)\n\n        while state != ThinkState.DONE:\n            iter_i += 1\n            self.verbose_output.print(Style.GREY(f\"{iter_i}/{self.max_iter}\"))\n            if iter_i&gt;self.max_iter:\n                break\n            if state == ThinkState.ACTION:\n                think_result = await self.agent.a_format_class(\n                    ProjectThinkResult,\n                    project_prompt.replace('#files#', vfs.print_file_structure()),\n                    message_context=execution_history\n                )\n                self.verbose_output.log_state(state.name, think_result)\n                think_result = ProjectThinkResult(**think_result)\n                for file_action in think_result.file_actions:\n                    path = file_action.path\n                    Path(file_action.path).mkdir(exist_ok=True)\n                    if file_action.action == 'write':\n                        vfs.write_file(path, file_action.content)\n                        files[path] = file_action.content\n                    elif file_action.action == 'read':\n                        content = vfs.read_file(path)\n                        files[path] = content\n                    elif file_action.action == 'delete':\n                        vfs.delete_file(path)\n                        files.pop(path, None)\n                    elif file_action.action == 'list':\n                        dir_contents = vfs.list_directory(path)\n                        files[path] = str(dir_contents)\n\n                if think_result.action == 'evaluate':\n                    state = ThinkState.PROCESSING\n                elif think_result.action == 'done':\n                    state = ThinkState.DONE\n\n                execution_history.append(f\"Action: {think_result.action}\\nReasoning: {think_result.reasoning}\")\n\n            elif state == ThinkState.PROCESSING:\n                if execute_function:\n                    execution_result = await execute_function(files)\n                    execution_history.append(f\"Execution Result: {execution_result}\")\n\n                    evaluation_prompt = f\"\"\"\n    Evaluate the current state of the project based on the execution result:\n\n    {execution_result}\n\n    Determine if the project is complete or if further modifications are needed.\n    \"\"\"\n                    evaluation = await self.agent.a_format_class(\n                        ProjectThinkResult,\n                        evaluation_prompt,\n                        message_context=execution_history\n                    )\n                    self.verbose_output.log_state(state.name, evaluation)\n                    evaluation = ProjectThinkResult(**evaluation)\n                    if evaluation.action == 'done':\n                        state = ThinkState.DONE\n                        result = execution_result\n                    else:\n                        state = ThinkState.ACTION\n                else:\n                    state = ThinkState.ACTION\n            else:\n                break\n\n        return ProjectPipelineResult(\n            result=result,\n            execution_history=execution_history,\n            files=files\n        )\n\n    async def __aenter__(self):\n        self.clear()\n        return self\n\n    async def configure(self, verbose=None, print_function=None, with_js=False, agent=None, variables=None, web_kwargs=None):\n        if verbose is not None and (print_function is not None or verbose != self.verbose_output.verbose):\n            if agent is None:\n                agent = self.agent\n            else:\n                self.agent = agent\n            agent.verbose = verbose\n            self.verbose_output = EnhancedVerboseOutput(verbose=verbose, print_f=print_function)\n\n            if print_function is not None:\n                agent.print_verbose = print_function\n        if variables:\n            self.variables = {**self.variables, **self._process_variables(variables)}\n        if with_js and web_kwargs:\n            self.browser_session: BrowserWrapper | None = BrowserWrapper(**web_kwargs)\n        self.web_js = with_js\n        if self.restore_var:\n            self.restore()\n\n        return self\n\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        if self.web_js:\n            await self.browser_session.close()\n            if self.restore_var:\n                self.save_session(f\"Pipeline_Session_{self.agent.amd.name}\")\n        if exc_type is not None:\n            print(f\"Exception occurred: {exc_value}\")\n        else:\n            print(\"Pipe Exit\")\n</code></pre> <code>__init__(agent, verbose=False, max_iter=12, variables=None, top_n=None, restore=None, max_think_after_think=None, print_f=None, web_js=False, timeout_timer=25, v_agent=None, web_llm=None)</code> \u00b6 <p>Initialize the Pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Any</code> <p>AI agent instance to use for task execution</p> required <code>verbose</code> <code>bool</code> <p>print internal results</p> <code>False</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations (default: 12)</p> <code>12</code> <code>variables</code> <code>dict[str, Any] | list[Any] | None</code> <p>Dictionary or list of variables to make available</p> <code>None</code> <code>top_n</code> <code>bool | None</code> <p>Limit variable descriptions to top N most used</p> <code>None</code> <code>web_js</code> <p>if the agent is allow to use the web</p> <code>False</code> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __init__(\n    self,\n    agent: Any,\n    verbose: bool=False,\n    max_iter: int= 12,\n    variables: dict[str, Any] | list[Any] | None = None,\n    top_n: bool | None = None,\n    restore: bool | None = None,\n    max_think_after_think = None,\n    print_f=None,\n    web_js=False,\n    timeout_timer=25,\n    v_agent=None,\n    web_llm=None,\n):\n    \"\"\"\n    Initialize the Pipeline.\n\n    Args:\n        agent: AI agent instance to use for task execution\n        verbose: print internal results\n        max_iter: Maximum number of iterations (default: 12)\n        variables: Dictionary or list of variables to make available\n        top_n: Limit variable descriptions to top N most used\n        web_js: if the agent is allow to use the web\n    \"\"\"\n\n    self.timeout_timer = timeout_timer\n    self.top_n = top_n\n    self.max_iter = max_iter\n    self.max_think_after_think = max_think_after_think or max_iter // 2\n    self.agent = agent\n    self.v_agent = v_agent or agent\n    # self.agent.verbose = verbose\n    self.task = None\n    self.web_js = web_js\n    self.print_f = print_f\n    self.verbose_output = EnhancedVerboseOutput(verbose=verbose, print_func=self.print_f)\n    self.variables = self._process_variables(variables or {})\n    self.variables['auto_install'] = auto_install\n    self.execution_history = []\n    self.session_name = None\n\n    self.browser_session: BrowserWrapper | None = BrowserWrapper(llm=web_llm or agent.amd.model)\n    self.js_history: list[JSExecutionRecord] = []\n\n    self._session_dir = Path(get_app().appdata) / 'ChatSession' / agent.amd.name\n    self.ipython = MockIPython(self._session_dir, auto_remove=False)\n    self.chat_session = ChatSession(get_app().get_mod(\"isaa\").get_memory(), space_name=f\"ChatSession/{agent.amd.name}/Pipeline.session\", max_length=max_iter)\n    self.process_memory = ChatSession(get_app().get_mod(\"isaa\").get_memory(), space_name=f\"ChatSession/{agent.amd.name}/Process.session\", max_length=max_iter)\n\n    # Initialize interpreter with variables\n    self.init_keys = list(self.ipython.user_ns.keys()).copy()\n    if self.web_js:\n        self.variables['web_actions'] = self.browser_session.run\n        self.variables['browser_session'] = self.browser_session\n    self.ipython.user_ns.update(self.variables)\n\n    self.restore_var = restore\n\n    if restore:\n        self.restore()\n</code></pre> <code>__str__()</code> \u00b6 <p>String representation of pipeline session</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __str__(self):\n    \"\"\"String representation of pipeline session\"\"\"\n    return str(self.ipython)\n</code></pre> <code>load_session(name)</code> \u00b6 <p>Load saved session</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def load_session(self, name: str):\n    \"\"\"Load saved session\"\"\"\n    self.ipython.load_session(name)\n    self.variables.update(self.ipython.user_ns)\n</code></pre> <code>run(task, do_continue=False)</code> <code>async</code> \u00b6 <p>Run the pipeline with separated thinking and processing phases</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>    async def run(self, task, do_continue=False) -&gt; PipelineResult:\n        \"\"\"Run the pipeline with separated thinking and processing phases\"\"\"\n        state = ThinkState.ACTION\n        result = None\n        original_task = task\n        if not do_continue:\n            task = await self.agent.a_run(f\"\"\"You are an AI assistant tasked with refactoring a user-provided task description into a more structured format with context learning and examples. Your goal is to create a comprehensive and well-organized task description that incorporates model flows and potential code fixes.\n\nFirst, I will provide you with a task description and some example tasks. Please read them carefully:\n\n&lt;existing_globals&gt;\n{self._generate_variable_descriptions()}\n&lt;/existing_globals&gt;\n\n&lt;example_tasks&gt;\nTask: Create a simple analysis of a list of numbers\n- Generate a list of 100 random numbers between 1-1000\n- Calculate the mean, median, and standard deviation\n- Create a histogram of the distribution\n- Print all results and display the plot\n\nTask: Create a reinforcement learning (RL) agent to play a simple game\n- Set up an OpenAI Gym environment (e.g., CartPole)\n- Implement a Q-learning or Deep Q-Network (DQN) agent\n- Train the model and optimize hyperparameters\n- Visualize learning progress with reward graphs\n- Save and reload trained models for inference\n- Provide an option to let the trained agent play in real time\n\nTask: Perform edge detection on an image\n- Load an image from a URL or local file\n- Convert the image to grayscale\n- Apply Gaussian blur to reduce noise\n- Use Canny edge detection to extract edges\n- Display the original and processed images side by side\n- Save the output image\n\nTask: Build a basic sentiment analysis system\n- Load a dataset of movie reviews (you can use a small sample)\n- Preprocess the text (remove punctuation, lowercase, etc.)\n- Create a TF-IDF vectorizer\n- Split data into training and testing sets\n- Train a classifier (e.g., Naive Bayes or LogisticRegression)\n- Evaluate performance with accuracy, precision, recall\n- Create a confusion matrix visualization\n- Make predictions on new sample texts\n&lt;/example_tasks&gt;\n\nNow, please refactor the given task description using the following guidelines:\n\n1. Analyze the task description and identify the main components and objectives.\n\n2. Structure the refactored task in a similar format to the example tasks, including:\n   - A clear title that summarizes the task\n   - A difficulty level (Easy, Intermediate, Hard, or Super Hard)\n   - A brief introduction to the task's context and purpose\n   - A code block containing step-by-step instructions\n   - A list of required skills, libraries, or technologies\n\n3. Incorporate model flows by breaking down the task into logical steps and explaining the process flow.\n\n4. Include potential code fixes or common pitfalls that users might encounter while working on the task.\n\n5. Add context learning elements by providing brief explanations or resources for key concepts related to the task.\n\n6. Ensure that the refactored task is comprehensive and can stand alone as a learning exercise.\n\nPlease provide your refactored task description within &lt;refactored_task&gt; tags. Use appropriate subheadings and formatting to make the description clear and easy to read.\n\nAdditional tips:\n- Mention any prerequisites or assumed knowledge\n- Suggest potential extensions or variations of the task for further learning\n\nRemember to maintain the original intent and complexity of the task while improving its structure and clarity. the task is: {task}\"\"\", persist_history=False, strategy_override=\"direct_llm\")\n            if '&lt;refactored_task&gt;' in task:\n                task = task.split('&lt;refactored_task&gt;')[1]\n            if '&lt;/refactored_task&gt;' in task:\n                task = task.split('&lt;/refactored_task&gt;')[0]\n        code_follow_up_prompt = f\"\"\"\nYou are an AI assistant responsible for evaluating task completion and providing feedback on the execution process. Your goal is to determine if a given task has been completed based on the execution result, and to offer insights for future improvements.\n\nYou will be provided with two inputs:\n&lt;task_description&gt;\n{original_task}\n{f'&lt;refactored_task_description_from_ai&gt;{task}&lt;/refactored_task_description_from_ai&gt;' if not do_continue else ''}\n&lt;/task_description&gt;\n\n&lt;code&gt;\n#CODE#\n&lt;/code&gt;\n\n&lt;execution_result&gt;\n#EXECUTION_RESULT#\n&lt;/execution_result&gt;\n\nFirst, carefully analyze the task description and the execution result. Determine whether the task has been completed successfully based on the information provided.\n\nIf the task is completed:\n1. Prepare a brief statement indicating that the task is done.\n2. Summarize the output for the user in a clear and concise manner.\n\nIf the task is not completed:\n1. Prepare a brief statement indicating that the task is not done.\n2. Identify the specific aspects of the task that remain incomplete.\n\nRegardless of task completion status, evaluate the procedure and effectiveness of the execution:\n1. Analyze the workflow: Describe the steps taken in the execution process.\n2. Assess effectiveness: Determine how well the procedure achieved the desired outcome.\n3. Identify errors: Pinpoint any mistakes or inefficiencies in the execution.\n4. Provide recommendations: Suggest improvements for future task executions.\n\ntip: Enclose mutil line strings property for python eval to function!\ntip: Set is_completed True if all requirements are completed from &lt;task_description&gt;.\ntip: Help the Agent with your analyses to finalize the &lt;task_description&gt;.\n{'tip: Prefer new informations from &lt;execution_result&gt; over &lt;refactored_task_description_from_ai&gt; based of &lt;code&gt;' if not do_continue else ''}\nnote : for the final result only toke information from the &lt;execution_result&gt;. if the relevant informations is not avalabel try string withe tips in the recommendations. else set is_completed True and return the teh Task failed!\nEnsure that your evaluation is thorough, constructive, and provides actionable insights for improving future task executions.\nAdd guidance based on the the last execution result\"\"\"\n        code_follow_up_prompt_ = [code_follow_up_prompt]\n        initial_prompt = f\"\"\"\nYou are an AI py coding agent specializing in iterative development and code refinement, designed to perform tasks that involve thinking. Your goal is to complete the given task while demonstrating a clear thought process throughout the execution.\nSYSTEM STATE:\n&lt;current_state&gt;\nIteration: #ITER#\nStatus: #STATE#\nLast EXECUTION: #EXECUTION#\n&lt;/current_state&gt;\n\nENVIRONMENT: {'current file :'+self.ipython.user_ns.get(\"__file__\")  if self.ipython.user_ns.get(\"__file__\") is not None else ''}\n\n'''&lt;global_variables&gt;\n#LOCALS#\n&lt;/global_variables&gt;'''\n\nMEMORY:\n&lt;process_memory&gt;\n#PHINT#\n&lt;/process_memory&gt;\n\n&lt;chat_memory&gt;\n#CHINT#\n&lt;/chat_memory&gt;\n\nVALIDATION CHECKLIST (Must verify before each action):\n1. \u2713 Check existing variables in ENVIRONMENT &lt;global_variables&gt;\n2. \u2713 Verify existing functions and classes\n3. \u2713 Review current imports\n4. \u2713 Confirm method signatures\n5. \u2713 Validate state preservation\n\nWORKFLOW STEPS:\n1. Analyze Current State:\n   - Reason and use all avalabel context\n   - Do not repeat the same errors\n   - Review existing implementations\n   - Check variable values\n   - Verify import statements\n   - Document dependencies\n\n2. Plan Change:\n   - NO example/simulation/simulate\n   - No demo er moc Data no Simulations Allowed or u will die!!\n   - Use existing variables and code when possible\n   - Prefer updates over rewrites\n\n3. Execute Change:\n   - Use appropriate action\n   - Maintain existing state\n   - Document modifications\n   - Verify results\n\nYou will use a structure called ThinkResult to organize your thoughts and actions.\nFor each step of your task, follow this process:\n\nACTIONS:\n1. 'code':\n    - MUST check &lt;global_variables&gt; first\n    - NEVER create demo functions\n    - Include 'reason'\n    - lang default 'py'\n    - Required: code in content\n    - code MUST call a function or display the row variabel / value at the end!\n    - Required: {{'context':{{'lang':'py',  'reason': ... }}...}}\n    - Optional file key in context example {{'context':{{'lang':'py',  'file': 'main.py' ,  'reason': ... }}...}}\n    - py code allows for toplevel await !!! use it !!! like\n:file-start:\nprint(\"using toplevel await\")\nawait abc()\n:file-end:\n\n    - Tip: use comments to reason with in the code\n3. 'infos': Request specific details\n4. 'guide': Get step clarification use on complex task and ery 5 step for staying on trak!\n5. 'brake': Pause for assessment\n6. 'done': Summarize changes\n\nCODE CONSTRAINTS:\n1. State Preservation:\n   - ALL variables ar persist\n   - ALL functions remain\n   - ALL classes ar maintained\n\n2. Import Management:\n   - Check &lt;global_variables&gt; for modules\n   - Use absolute imports\n   - Document new dependencies\n\n3. Function Handling:\n   - NEVER overwrite existing\n   - Use update for changes\n   - Preserve signatures\n\n4. Variable Scope:\n   - Maintain existing scope\n   - Check for conflicts\n   - Document state changes\n\nEXECUTION RULES:\n1. VERIFY before create\n2. UPDATE don't replace\n3. TEST after each change\n\nNext Action Required:\n1. Review current state\n2. Check existing code\n3. Execute with state preservation\n\n!!CRITICAL!!\n- NO demo functions\n- NO placeholder functions\n- USE existing code\n- FOR Implementations prefer writing large production redy code chunks.\n- FOR reasoning and validation write small code blocks.\n- THE CODE must call something or end the code with an value!\n- NO INFINIT LOOPS! none breakable while loops ar not allowed, exception ui (closed by user)\n- NO 'python' top level return, only write the variabel or value itself!\n- 'code is run using exec! do not use !pip ...'\n'- instead use auto_install(package_name, install_method=\"pip\", upgrade=False, quiet=False, version=None, extra_args=None)'\n# Example usage first time\n\u2502 auto_install('pandas', version='1.3.0')\n\u2502 import pandas\n\u2502 auto_install('pygame')\n\u2502 import pygame\n\u2502 auto_install('numpy')\n\u2502 import numpy as np\n!TIPS!\n- '&lt;global_variables&gt; can contain instances and functions you can use in your python' code\n- if the function is async you can use top level await\n- if their is missing of informations try running code to get the infos\n- if you got stuck or need assistance break with a question to the user.\n'- run functions from &lt;global_variables&gt; using name(*args, **kwargs) or await name(*args, **kwargs)'\n'- &lt;global_variables&gt; ar global accessible!'\n'- if an &lt;global_variables&gt; name is lower lists an redy to use instance'\n\"\"\"\n        p_hint, c_hint = await self.get_process_hint(task)\n        initial_prompt = initial_prompt.replace('#PHINT#', p_hint)\n        initial_prompt = initial_prompt.replace('#CHINT#', c_hint)\n        initial_prompt_ = initial_prompt\n        iter_i = 0\n        iter_p = 0\n        iter_tat = 0\n        next_infos = \"\"\n        if not do_continue:\n            await self.chat_session.add_message({'role': 'user', 'content': task})\n        else:\n            self.restore()\n            await self.chat_session.add_message({'role': 'user', 'content': task})\n\n        if self.web_js and self.browser_session is None:\n            self.browser_session = BrowserWrapper(llm=self.agent.amd.modle)\n\n        # await self.verbose_output.log_message('user', task)\n        self.verbose_output.log_header(task)\n        while state != ThinkState.DONE:\n            iter_i += 1\n            t0 = time.perf_counter()\n            prompt = initial_prompt.replace('#ITER#', f'{iter_i} max {self.max_iter}')\n            prompt = prompt.replace('#STATE#', f'{state.name}')\n            prompt = prompt.replace('#EXECUTION#', f'{next_infos}')  if next_infos else prompt.replace('Last EXECUTION: #EXECUTION#', '')\n            prompt = prompt.replace('#LOCALS#', f'{self._generate_variable_descriptions()}')\n            self.verbose_output.log_state(state.name, {})\n            self.verbose_output.print(Style.GREY(f\"{iter_i}/{self.max_iter}\"))\n            if state == ThinkState.ACTION:\n                iter_tat +=1\n                if iter_tat &gt; self.max_think_after_think:\n                    state = ThinkState.BRAKE\n            else:\n                iter_tat = 0\n\n            if state == ThinkState.ACTION:\n                # Get agent's thoughts\n                think_dicts = await self.verbose_output.process(state.name, self.agent.a_format_class(\n                    ThinkResults,\n                    prompt,\n                    message_context=self.chat_session.get_past_x(self.max_iter*2, last_u=not do_continue).copy()+([self.process_memory.history[-1]] if self.process_memory.history else []) ,\n                ))\n                think_dicts = think_dicts.get(\"actions\")\n                if think_dicts is None:\n                    think_dicts = [await self.verbose_output.process(state.name, self.agent.a_format_class(\n                        ThinkResult,\n                        prompt,\n                        message_context=self.chat_session.get_past_x(self.max_iter * 2, last_u=not do_continue).copy() + (\n                            [self.process_memory.history[-1]] if self.process_memory.history else []),\n                    ))]\n                if len(think_dicts) == 1:\n                    think_dict = think_dicts[0]\n                else:\n                    for think_dict in think_dicts[:-1]:\n                        if think_dict.get('context') is None:\n                            think_dict['context'] = {'context': 'N/A'}\n                        if not isinstance(think_dict.get('context'), dict):\n                            think_dict['context'] = {'context': think_dict.get('context')}\n                        think_result = ThinkResult(**think_dict)\n                        await self.chat_session.add_message(\n                            {'role': 'assistant', 'content': think_result.content + str(think_result.context)})\n                        state, result = await self.verbose_output.process(think_dict.get(\"action\"),\n                                                                          self._process_think_result(think_result,\n                                                                                                     task=task))\n                        if result:\n                            await self.chat_session.add_message(\n                                {'role': 'system', 'content': 'Evaluation: ' + str(result)})\n                            await self.verbose_output.log_message('system', str(result))\n                    think_dict = think_dicts[-1]\n                self.verbose_output.formatter.print_code_block(think_dict.get(\"content\"), 'python' if think_dict.get(\"context\", {'lang': 'py'}).get('lang') == 'py' else 'javascript')\n                if think_dict.get('context') is None:\n                    think_dict['context'] = {'context': 'N/A'}\n                if not isinstance(think_dict.get('context'), dict):\n                    think_dict['context'] = {'context': think_dict.get('context')}\n                think_result = ThinkResult(**think_dict)\n                state, result = await self.verbose_output.process(think_dict.get(\"action\"), self._process_think_result(think_result, task=task))\n                await self.chat_session.add_message({'role': 'assistant', 'content': think_result.content + str(think_result.context)})\n                if result:\n                    await self.chat_session.add_message({'role': 'system', 'content': 'Evaluation: '+str(result)})\n                    await self.verbose_output.log_message('system', str(result))\n                    code_follow_up_prompt_[0] = code_follow_up_prompt.replace(\"#EXECUTION_RESULT#\", str(result))\n                    if isinstance(result ,ExecutionRecord):\n                        code_follow_up_prompt_[0] = code_follow_up_prompt_[0].replace(\"#CODE#\", result.code)\n                    else:\n                        code_follow_up_prompt_[0] = code_follow_up_prompt_[0].replace(\"#CODE#\", self._generate_variable_descriptions())\n                else:\n                    code_follow_up_prompt_[0] = code_follow_up_prompt.replace(\"#EXECUTION_RESULT#\", str(think_result))\n                    code_follow_up_prompt_[0] = code_follow_up_prompt_[0].replace(\"#CODE#\",\n                                                                              self._generate_variable_descriptions())\n\n\n            elif state == ThinkState.PROCESSING:\n                # Get agent's thoughts\n                class Next(BaseModel):\n                    is_completed: bool\n                    recommendations: str\n                    errors: str\n                    effectiveness: str\n                    workflow: str\n                    text: str\n                # Format the agent's thoughts into a structured response\n                _agent = self.v_agent if self.v_agent is not None else self.agent\n                next_dict = await self.verbose_output.process(state.name, _agent.a_format_class(\n                    Next,\n                    code_follow_up_prompt_[0],\n                    message_context=self.chat_session.get_past_x(self.max_iter*2, last_u=not do_continue).copy(),\n                ))\n                next_infos = json.dumps(next_dict)\n                await self.verbose_output.log_process_result(next_dict)\n                await self.process_memory.add_message({'role': 'assistant', 'content': next_infos.replace('workflow:', 'past-workflow:')})\n                iter_p += 1\n                code_follow_up_prompt_[0] = code_follow_up_prompt\n                if not next_dict.get('is_completed', True):\n                    state = ThinkState.ACTION\n                    initial_prompt = initial_prompt_.replace('#ITER#',f'#ITER#\\nReasoning assist result: {next_dict}')\n                    continue\n                elif next_dict.get('is_completed', False):\n                    result = next_dict.get('text', '')\n                    state = ThinkState.DONE\n                    continue\n                else:\n                    result = next_dict.get('text', '')\n                    break\n\n            elif state == ThinkState.BRAKE:\n                break\n\n            if iter_i &lt; self.max_iter:\n                await asyncio.sleep(1)\n                # if time.perf_counter() -t0 &lt; self.timeout_timer*2.5:\n                #     with Spinner(f\"Prevent rate limit posing for {self.timeout_timer}s\", symbols='+', time_in_s=self.timeout_timer, count_down=True):\n                #         await asyncio.sleep(self.timeout_timer)\n            else:\n                state = ThinkState.BRAKE\n                if isinstance(result, ExecutionRecord):\n                    result = result.result\n                elif isinstance(result, str):\n                    pass\n                else:\n                    result = \"Max iterations\"\n                break\n\n        self.verbose_output.log_state(state.name, {})\n\n        return PipelineResult(\n            variables=self.variables,\n            result=result,\n            execution_history=self.execution_history,\n            message=self.chat_session.get_past_x(iter_i*2, last_u=not do_continue),\n        )\n</code></pre> <code>save_session(name)</code> \u00b6 <p>Save current session</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def save_session(self, name: str):\n    \"\"\"Save current session\"\"\"\n    self.session_name = name\n    self.ipython.save_session(name)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.SyncReport","title":"<code>SyncReport</code>  <code>dataclass</code>","text":"<p>Report of variables synced from namespace to pipeline</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>@dataclass\nclass SyncReport:\n    \"\"\"Report of variables synced from namespace to pipeline\"\"\"\n    added: dict[str, str]\n    skipped: dict[str, str]  # var_name -&gt; reason\n    errors: dict[str, str]  # var_name -&gt; error message\n\n    def __str__(self) -&gt; str:\n        parts = []\n        if self.added:\n            parts.append(\"Added variables:\")\n            for name, type_ in self.added.items():\n                parts.append(f\"  - {name}: {type_}\")\n        if self.skipped:\n            parts.append(\"\\nSkipped variables:\")\n            for name, reason in self.skipped.items():\n                parts.append(f\"  - {name}: {reason}\")\n        if self.errors:\n            parts.append(\"\\nErrors:\")\n            for name, error in self.errors.items():\n                parts.append(f\"  - {name}: {error}\")\n        return \"\\n\".join(parts)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.TeeStream","title":"<code>TeeStream</code>","text":"<p>Stream that writes to both console and buffer</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class TeeStream:\n    \"\"\"Stream that writes to both console and buffer\"\"\"\n    def __init__(self, console_stream, buffer_stream):\n        self.console_stream = console_stream\n        self.buffer_stream = buffer_stream\n\n    def write(self, data):\n        self.console_stream.write(data)\n        self.buffer_stream.write(data)\n        self.console_stream.flush()  # Ensure immediate console output\n\n    def flush(self):\n        self.console_stream.flush()\n        self.buffer_stream.flush()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.VirtualEnvContext","title":"<code>VirtualEnvContext</code>","text":"<p>Context manager for temporary virtual environment activation</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class VirtualEnvContext:\n    \"\"\"Context manager for temporary virtual environment activation\"\"\"\n\n    def __init__(self, venv_path: Path):\n        self.venv_path = venv_path\n        self._original_path = None\n        self._original_sys_path = None\n        self._original_prefix = None\n        self._original_virtual_env = None\n\n    def _get_venv_paths(self):\n        \"\"\"Get virtual environment paths based on platform\"\"\"\n        if sys.platform == 'win32':\n            site_packages = self.venv_path / 'Lib' / 'site-packages'\n            scripts_dir = self.venv_path / 'Scripts'\n            python_path = scripts_dir / 'python.exe'\n        else:\n            python_version = f'python{sys.version_info.major}.{sys.version_info.minor}'\n            site_packages = self.venv_path / 'lib' / python_version / 'site-packages'\n            scripts_dir = self.venv_path / 'bin'\n            python_path = scripts_dir / 'python'\n\n        return site_packages, scripts_dir, python_path\n\n    def __enter__(self):\n        # Save original state\n        self._original_path = os.environ.get('PATH', '')\n        self._original_sys_path = sys.path.copy()\n        self._original_prefix = sys.prefix\n        self._original_virtual_env = os.environ.get('VIRTUAL_ENV')\n\n        # Get venv paths\n        site_packages, scripts_dir, python_path = self._get_venv_paths()\n\n        # Modify environment for venv\n        if scripts_dir.exists():\n            new_path = os.pathsep.join([str(scripts_dir), self._original_path])\n            os.environ['PATH'] = new_path\n\n        if site_packages.exists():\n            sys.path.insert(0, str(site_packages))\n\n        os.environ['VIRTUAL_ENV'] = str(self.venv_path)\n\n        # Return the python executable path for potential subprocess calls\n        return str(python_path)\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Restore original state\n        os.environ['PATH'] = self._original_path\n        sys.path = self._original_sys_path\n\n        if self._original_virtual_env is None:\n            os.environ.pop('VIRTUAL_ENV', None)\n        else:\n            os.environ['VIRTUAL_ENV'] = self._original_virtual_env\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.VirtualFileSystem","title":"<code>VirtualFileSystem</code>","text":"Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class VirtualFileSystem:\n    def __init__(self, base_dir: Path):\n        self.base_dir = base_dir\n        self.current_dir = base_dir\n        self.virtual_files: dict[str, str] = {}\n        self.base_dir.mkdir(parents=True, exist_ok=True)\n\n    def write_file(self, filepath: str | Path, content: str) -&gt; Path:\n        \"\"\"Write content to a virtual file and persist to disk using UTF-8\"\"\"\n        try:\n            abs_path = self._resolve_path(filepath)\n        except ValueError:\n            print(\"invalid :\", filepath)\n            filepath = \"src/temp_js/_temp_fix.py\"\n            abs_path = self._resolve_path(filepath)\n        abs_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Store in virtual filesystem\n        rel_path = str(abs_path.relative_to(self.base_dir))\n        self.virtual_files[rel_path] = content\n\n        # Write to actual filesystem with UTF-8 encoding\n        with open(abs_path, 'w', encoding='utf-8', errors='replace') as f:\n            f.write(content)\n\n        return abs_path\n\n    def read_file(self, filepath: str | Path) -&gt; str:\n        \"\"\"Read content from a virtual file using UTF-8\"\"\"\n        abs_path = self._resolve_path(filepath)\n        if not abs_path.exists():\n            raise FileNotFoundError(f\"File not found: {filepath}\")\n\n        rel_path = str(abs_path.relative_to(self.base_dir))\n\n        # Check virtual filesystem first\n        if rel_path in self.virtual_files:\n            return self.virtual_files[rel_path]\n\n        # Fall back to reading from disk with UTF-8 encoding\n        with open(abs_path, encoding='utf-8', errors='replace') as f:\n            content = f.read()\n            self.virtual_files[rel_path] = content\n            return content\n\n    def delete_file(self, filepath: str | Path):\n        \"\"\"Delete a virtual file\"\"\"\n        abs_path = self._resolve_path(filepath)\n        rel_path = str(abs_path.relative_to(self.base_dir))\n\n        if rel_path in self.virtual_files:\n            del self.virtual_files[rel_path]\n\n        if abs_path.exists():\n            abs_path.unlink()\n\n    def create_directory(self, dirpath: str | Path):\n        \"\"\"Create a new directory\"\"\"\n        abs_path = self._resolve_path(dirpath)\n        abs_path.mkdir(parents=True, exist_ok=True)\n        return abs_path\n\n\n    def list_directory(self, dirpath: str | Path = '.') -&gt; list:\n        \"\"\"List contents of a directory\"\"\"\n        abs_path = self._resolve_path(dirpath)\n        if not abs_path.exists():\n            raise FileNotFoundError(f\"Directory not found: {dirpath}\")\n        return [p.name for p in abs_path.iterdir()]\n\n    def change_directory(self, dirpath: str | Path):\n        \"\"\"Change current working directory\"\"\"\n        new_dir = self._resolve_path(dirpath)\n        if not new_dir.exists() or not new_dir.is_dir():\n            raise NotADirectoryError(f\"Directory not found: {dirpath}\")\n        self.current_dir = new_dir\n\n    def _resolve_path(self, filepath: str | Path) -&gt; Path:\n        \"\"\"Convert relative path to absolute path\"\"\"\n        filepath = Path(filepath)\n        if filepath.is_absolute():\n            if not str(filepath).startswith(str(self.base_dir)):\n                raise ValueError(\"Path must be within base directory\")\n            return filepath\n        return (self.current_dir / filepath).resolve()\n\n    def save_state(self, state_file: Path):\n        \"\"\"Save virtual filesystem state to disk\"\"\"\n        state = {\n            'current_dir': str(self.current_dir.relative_to(self.base_dir)),\n            'virtual_files': self.virtual_files\n        }\n        with open(state_file, 'w') as f:\n            json.dump(state, f)\n\n    def load_state(self, state_file: Path):\n        \"\"\"Load virtual filesystem state from disk\"\"\"\n        if not state_file.exists():\n            return\n\n        with open(state_file) as f:\n            state = json.load(f)\n            self.current_dir = self.base_dir / state['current_dir']\n            self.virtual_files = state['virtual_files']\n\n    def print_file_structure(self, start_path: str | Path = '.', indent: str = ''):\n        \"\"\"Print the file structure starting from the given path\"\"\"\n        start_path = self._resolve_path(start_path)\n        if not start_path.exists():\n            s = f\"Path not found: {start_path}\"\n            return s\n\n        s = f\"{indent}{start_path.name}/\"\n        for item in sorted(start_path.iterdir()):\n            if item.is_dir():\n               s+= self.print_file_structure(item, indent + '  ')\n            else:\n                s = f\"{indent}  {item.name}\"\n        return s\n</code></pre> <code>change_directory(dirpath)</code> \u00b6 <p>Change current working directory</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def change_directory(self, dirpath: str | Path):\n    \"\"\"Change current working directory\"\"\"\n    new_dir = self._resolve_path(dirpath)\n    if not new_dir.exists() or not new_dir.is_dir():\n        raise NotADirectoryError(f\"Directory not found: {dirpath}\")\n    self.current_dir = new_dir\n</code></pre> <code>create_directory(dirpath)</code> \u00b6 <p>Create a new directory</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def create_directory(self, dirpath: str | Path):\n    \"\"\"Create a new directory\"\"\"\n    abs_path = self._resolve_path(dirpath)\n    abs_path.mkdir(parents=True, exist_ok=True)\n    return abs_path\n</code></pre> <code>delete_file(filepath)</code> \u00b6 <p>Delete a virtual file</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def delete_file(self, filepath: str | Path):\n    \"\"\"Delete a virtual file\"\"\"\n    abs_path = self._resolve_path(filepath)\n    rel_path = str(abs_path.relative_to(self.base_dir))\n\n    if rel_path in self.virtual_files:\n        del self.virtual_files[rel_path]\n\n    if abs_path.exists():\n        abs_path.unlink()\n</code></pre> <code>list_directory(dirpath='.')</code> \u00b6 <p>List contents of a directory</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def list_directory(self, dirpath: str | Path = '.') -&gt; list:\n    \"\"\"List contents of a directory\"\"\"\n    abs_path = self._resolve_path(dirpath)\n    if not abs_path.exists():\n        raise FileNotFoundError(f\"Directory not found: {dirpath}\")\n    return [p.name for p in abs_path.iterdir()]\n</code></pre> <code>load_state(state_file)</code> \u00b6 <p>Load virtual filesystem state from disk</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def load_state(self, state_file: Path):\n    \"\"\"Load virtual filesystem state from disk\"\"\"\n    if not state_file.exists():\n        return\n\n    with open(state_file) as f:\n        state = json.load(f)\n        self.current_dir = self.base_dir / state['current_dir']\n        self.virtual_files = state['virtual_files']\n</code></pre> <code>print_file_structure(start_path='.', indent='')</code> \u00b6 <p>Print the file structure starting from the given path</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def print_file_structure(self, start_path: str | Path = '.', indent: str = ''):\n    \"\"\"Print the file structure starting from the given path\"\"\"\n    start_path = self._resolve_path(start_path)\n    if not start_path.exists():\n        s = f\"Path not found: {start_path}\"\n        return s\n\n    s = f\"{indent}{start_path.name}/\"\n    for item in sorted(start_path.iterdir()):\n        if item.is_dir():\n           s+= self.print_file_structure(item, indent + '  ')\n        else:\n            s = f\"{indent}  {item.name}\"\n    return s\n</code></pre> <code>read_file(filepath)</code> \u00b6 <p>Read content from a virtual file using UTF-8</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def read_file(self, filepath: str | Path) -&gt; str:\n    \"\"\"Read content from a virtual file using UTF-8\"\"\"\n    abs_path = self._resolve_path(filepath)\n    if not abs_path.exists():\n        raise FileNotFoundError(f\"File not found: {filepath}\")\n\n    rel_path = str(abs_path.relative_to(self.base_dir))\n\n    # Check virtual filesystem first\n    if rel_path in self.virtual_files:\n        return self.virtual_files[rel_path]\n\n    # Fall back to reading from disk with UTF-8 encoding\n    with open(abs_path, encoding='utf-8', errors='replace') as f:\n        content = f.read()\n        self.virtual_files[rel_path] = content\n        return content\n</code></pre> <code>save_state(state_file)</code> \u00b6 <p>Save virtual filesystem state to disk</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def save_state(self, state_file: Path):\n    \"\"\"Save virtual filesystem state to disk\"\"\"\n    state = {\n        'current_dir': str(self.current_dir.relative_to(self.base_dir)),\n        'virtual_files': self.virtual_files\n    }\n    with open(state_file, 'w') as f:\n        json.dump(state, f)\n</code></pre> <code>write_file(filepath, content)</code> \u00b6 <p>Write content to a virtual file and persist to disk using UTF-8</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def write_file(self, filepath: str | Path, content: str) -&gt; Path:\n    \"\"\"Write content to a virtual file and persist to disk using UTF-8\"\"\"\n    try:\n        abs_path = self._resolve_path(filepath)\n    except ValueError:\n        print(\"invalid :\", filepath)\n        filepath = \"src/temp_js/_temp_fix.py\"\n        abs_path = self._resolve_path(filepath)\n    abs_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Store in virtual filesystem\n    rel_path = str(abs_path.relative_to(self.base_dir))\n    self.virtual_files[rel_path] = content\n\n    # Write to actual filesystem with UTF-8 encoding\n    with open(abs_path, 'w', encoding='utf-8', errors='replace') as f:\n        f.write(content)\n\n    return abs_path\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.WebContentParser","title":"<code>WebContentParser</code>","text":"<p>Parser for extracting content from web pages in various formats.</p> <p>Provides methods to extract content as markdown, plain text, structured data, and take screenshots with scrolling support.</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>class WebContentParser:\n    \"\"\"\n    Parser for extracting content from web pages in various formats.\n\n    Provides methods to extract content as markdown, plain text,\n    structured data, and take screenshots with scrolling support.\n    \"\"\"\n\n    def __init__(self, browser_wrapper):\n        \"\"\"Initialize the parser with a browser wrapper instance\"\"\"\n        self.browser = browser_wrapper\n\n    async def to_markdown(self, page=None, selector=\"main, article, #content, .content, body\",\n                          include_images=True):\n        \"\"\"\n        Convert webpage content to markdown format\n\n        Args:\n            page: The page to parse (uses current page if None)\n            selector: CSS selector for the content to extract\n            include_images: Whether to include image references\n\n        Returns:\n            str: Markdown content\n        \"\"\"\n        return await self.browser.extract_markdown(page, selector, include_images)\n\n    async def to_text(self, page=None, selector=\"body\"):\n        \"\"\"Extract plain text from webpage\"\"\"\n        return await self.browser.extract_text(page, selector)\n\n    async def to_structured(self, page=None, config=None):\n        \"\"\"Extract structured data from webpage using selector configuration\"\"\"\n        return await self.browser.extract_structured_content(page, config)\n\n    async def to_screenshot(self, page=None, full_page=True, path=None,\n                            initial_delay=1000, scroll_delay=500, format='png'):\n        \"\"\"\n        Take a screenshot with scrolling functionality\n\n        Args:\n            page: The page to screenshot\n            full_page: Whether to capture the full page\n            path: Path to save the screenshot\n            initial_delay: Delay in ms before starting screenshot\n            scroll_delay: Delay in ms between scrolls\n            format: Image format ('png' or 'jpeg')\n        \"\"\"\n        return await self.browser.take_scrolling_screenshot(\n            page, full_page, path, initial_delay, scroll_delay, format\n        )\n\n    async def extract_all(self, page=None, selector=\"body\", include_images=True,\n                          screenshot=True, screenshot_path=None):\n        \"\"\"Extract all content types (markdown, text, structured data, screenshot)\"\"\"\n        result = {\n            'markdown': await self.to_markdown(page, selector, include_images),\n            'text': await self.to_text(page, selector),\n            'structured': await self.to_structured(page)\n        }\n\n        if screenshot:\n            result['screenshot'] = await self.to_screenshot(\n                page, path=screenshot_path, initial_delay=1000\n            )\n\n        return result\n</code></pre> <code>__init__(browser_wrapper)</code> \u00b6 <p>Initialize the parser with a browser wrapper instance</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def __init__(self, browser_wrapper):\n    \"\"\"Initialize the parser with a browser wrapper instance\"\"\"\n    self.browser = browser_wrapper\n</code></pre> <code>extract_all(page=None, selector='body', include_images=True, screenshot=True, screenshot_path=None)</code> <code>async</code> \u00b6 <p>Extract all content types (markdown, text, structured data, screenshot)</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def extract_all(self, page=None, selector=\"body\", include_images=True,\n                      screenshot=True, screenshot_path=None):\n    \"\"\"Extract all content types (markdown, text, structured data, screenshot)\"\"\"\n    result = {\n        'markdown': await self.to_markdown(page, selector, include_images),\n        'text': await self.to_text(page, selector),\n        'structured': await self.to_structured(page)\n    }\n\n    if screenshot:\n        result['screenshot'] = await self.to_screenshot(\n            page, path=screenshot_path, initial_delay=1000\n        )\n\n    return result\n</code></pre> <code>to_markdown(page=None, selector='main, article, #content, .content, body', include_images=True)</code> <code>async</code> \u00b6 <p>Convert webpage content to markdown format</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <p>The page to parse (uses current page if None)</p> <code>None</code> <code>selector</code> <p>CSS selector for the content to extract</p> <code>'main, article, #content, .content, body'</code> <code>include_images</code> <p>Whether to include image references</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <p>Markdown content</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def to_markdown(self, page=None, selector=\"main, article, #content, .content, body\",\n                      include_images=True):\n    \"\"\"\n    Convert webpage content to markdown format\n\n    Args:\n        page: The page to parse (uses current page if None)\n        selector: CSS selector for the content to extract\n        include_images: Whether to include image references\n\n    Returns:\n        str: Markdown content\n    \"\"\"\n    return await self.browser.extract_markdown(page, selector, include_images)\n</code></pre> <code>to_screenshot(page=None, full_page=True, path=None, initial_delay=1000, scroll_delay=500, format='png')</code> <code>async</code> \u00b6 <p>Take a screenshot with scrolling functionality</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <p>The page to screenshot</p> <code>None</code> <code>full_page</code> <p>Whether to capture the full page</p> <code>True</code> <code>path</code> <p>Path to save the screenshot</p> <code>None</code> <code>initial_delay</code> <p>Delay in ms before starting screenshot</p> <code>1000</code> <code>scroll_delay</code> <p>Delay in ms between scrolls</p> <code>500</code> <code>format</code> <p>Image format ('png' or 'jpeg')</p> <code>'png'</code> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def to_screenshot(self, page=None, full_page=True, path=None,\n                        initial_delay=1000, scroll_delay=500, format='png'):\n    \"\"\"\n    Take a screenshot with scrolling functionality\n\n    Args:\n        page: The page to screenshot\n        full_page: Whether to capture the full page\n        path: Path to save the screenshot\n        initial_delay: Delay in ms before starting screenshot\n        scroll_delay: Delay in ms between scrolls\n        format: Image format ('png' or 'jpeg')\n    \"\"\"\n    return await self.browser.take_scrolling_screenshot(\n        page, full_page, path, initial_delay, scroll_delay, format\n    )\n</code></pre> <code>to_structured(page=None, config=None)</code> <code>async</code> \u00b6 <p>Extract structured data from webpage using selector configuration</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def to_structured(self, page=None, config=None):\n    \"\"\"Extract structured data from webpage using selector configuration\"\"\"\n    return await self.browser.extract_structured_content(page, config)\n</code></pre> <code>to_text(page=None, selector='body')</code> <code>async</code> \u00b6 <p>Extract plain text from webpage</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>async def to_text(self, page=None, selector=\"body\"):\n    \"\"\"Extract plain text from webpage\"\"\"\n    return await self.browser.extract_text(page, selector)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.auto_install","title":"<code>auto_install(package_name, install_method='pip', upgrade=False, quiet=False, version=None, extra_args=None)</code>","text":"<p>Enhanced auto-save import with version and extra arguments support</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def auto_install(package_name, install_method='pip', upgrade=False, quiet=False, version=None, extra_args=None):\n    '''\n    Enhanced auto-save import with version and extra arguments support\n    '''\n    try:\n        # Attempt to import the package\n        return importlib.import_module(package_name)\n    except ImportError:\n        # Package not found, prepare for installation\n        print(f\"Package '{package_name}' not found. Attempting to install...\")\n        try:\n            # Determine Python executable based on virtual environment\n            venv_path = os.environ.get('VIRTUAL_ENV')\n            if venv_path:\n                venv_path = Path(venv_path)\n                if sys.platform == 'win32':\n                    python_exec = str(venv_path / 'Scripts' / 'python.exe')\n                else:\n                    python_exec = str(venv_path / 'bin' / 'python')\n                # Check if the Python executable exists\n                if not Path(python_exec).exists():\n                    python_exec = sys.executable\n            else:\n                python_exec = sys.executable\n\n            # Construct installation command with more flexibility\n            install_cmd = [python_exec, \"-m\", install_method, \"install\"]\n            if upgrade:\n                install_cmd.append(\"--upgrade\")\n            # Support specific version installation\n            if version:\n                install_cmd.append(f\"{package_name}=={version}\")\n            else:\n                install_cmd.append(package_name)\n            # Add extra arguments if provided\n            if extra_args:\n                install_cmd.extend(extra_args)\n            # Run installation with appropriate verbosity\n            installation_output = subprocess.run(\n                install_cmd,\n                capture_output=quiet,\n                text=True\n            )\n            # Check installation status\n            if installation_output.returncode == 0:\n                print(f\"Successfully installed {package_name}\")\n                return importlib.import_module(package_name)\n            else:\n                raise Exception(f\"Installation failed: {installation_output.stderr}\")\n        except Exception as install_error:\n            print(f\"Error installing {package_name}: {install_error}\")\n            return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.sync_globals_to_vars","title":"<code>sync_globals_to_vars(pipeline, namespace=None, prefix=None, include_types=None, exclude_patterns=None, exclude_private=True, deep_copy=False, only_serializable=False)</code>","text":"<pre><code>Sync global variables or a specific namespace to pipeline variables.\n\nArgs:\n    pipeline: Pipeline instance to sync variables to\n    namespace: Optional dictionary of variables (defaults to globals())\n    prefix: Optional prefix for variable names (e.g., 'global_')\n    include_types: Only include variables of these types\n    exclude_patterns: List of regex patterns to exclude\n    exclude_private: Exclude variables starting with underscore\n    deep_copy: Create deep copies of variables instead of references\n    only_serializable: Only include variables that can be serialized\n\nReturns:\n    SyncReport with details about added, skipped and error variables\n\nUsage example:\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.sync_globals_to_vars--basic-usage-sync-all-globals","title":"Basic usage - sync all globals","text":"<p>report = sync_globals_to_vars(pipeline)</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.sync_globals_to_vars--sync-only-numeric-types-with-prefix","title":"Sync only numeric types with prefix","text":"<p>report = sync_globals_to_vars(     pipeline,     include_types=[int, float],     prefix=\"global_\" )</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.sync_globals_to_vars--sync-from-specific-namespace","title":"Sync from specific namespace","text":"<p>import numpy as np namespace = {\"arr\": np.array([1,2,3])} report = sync_globals_to_vars(pipeline, namespace=namespace)</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.live.sync_globals_to_vars--sync-with-deep-copy-and-serialization-check","title":"Sync with deep copy and serialization check","text":"<p>report = sync_globals_to_vars(     pipeline,     deep_copy=True,     only_serializable=True )</p> Source code in <code>toolboxv2/mods/isaa/CodingAgent/live.py</code> <pre><code>def sync_globals_to_vars(\n    pipeline: Any,\n    namespace: dict[str, Any] | None = None,\n    prefix: str | None = None,\n    include_types: type | list[type] | None = None,\n    exclude_patterns: list[str] | None = None,\n    exclude_private: bool = True,\n    deep_copy: bool = False,\n    only_serializable: bool = False\n) -&gt; SyncReport:\n    \"\"\"\n    Sync global variables or a specific namespace to pipeline variables.\n\n    Args:\n        pipeline: Pipeline instance to sync variables to\n        namespace: Optional dictionary of variables (defaults to globals())\n        prefix: Optional prefix for variable names (e.g., 'global_')\n        include_types: Only include variables of these types\n        exclude_patterns: List of regex patterns to exclude\n        exclude_private: Exclude variables starting with underscore\n        deep_copy: Create deep copies of variables instead of references\n        only_serializable: Only include variables that can be serialized\n\n    Returns:\n        SyncReport with details about added, skipped and error variables\n\n    Usage example:\n# Basic usage - sync all globals\nreport = sync_globals_to_vars(pipeline)\n\n# Sync only numeric types with prefix\nreport = sync_globals_to_vars(\n    pipeline,\n    include_types=[int, float],\n    prefix=\"global_\"\n)\n\n# Sync from specific namespace\nimport numpy as np\nnamespace = {\"arr\": np.array([1,2,3])}\nreport = sync_globals_to_vars(pipeline, namespace=namespace)\n\n# Sync with deep copy and serialization check\nreport = sync_globals_to_vars(\n    pipeline,\n    deep_copy=True,\n    only_serializable=True\n)\n    \"\"\"\n    # Initialize report\n    report = SyncReport(\n        added={},\n        skipped={},\n        errors={}\n    )\n\n    # Get namespace\n    if namespace is None:\n        # Get caller's globals\n        namespace = currentframe().f_back.f_globals\n\n    # Compile exclude patterns\n    if exclude_patterns:\n        patterns = [re.compile(pattern) for pattern in exclude_patterns]\n    else:\n        patterns = []\n\n    # Normalize include_types\n    if include_types and not isinstance(include_types, list | tuple | set):\n        include_types = [include_types]\n    def get_type_info(var: Any) -&gt; str:\n        \"\"\"Helper to get detailed type information\"\"\"\n        if isinstance(var, type):\n            return f\"class '{var.__name__}'\"\n        elif isinstance(var, BaseModel):\n            return f\"Pydantic model '{var.__class__.__name__}'\"\n        elif hasattr(var, '__class__'):\n            type_name = var.__class__.__name__\n            module_name = var.__class__.__module__\n            if module_name != 'builtins':\n                return f\"{module_name}.{type_name}\"\n            return type_name\n        return type(var).__name__\n    # Process each variable\n    for name, value in namespace.items():\n        try:\n            # Skip if matches exclude criteria\n            if exclude_private and name.startswith('_'):\n                report.skipped[name] = \"private variable\"\n                continue\n\n            if any(pattern.match(name) for pattern in patterns):\n                report.skipped[name] = \"matched exclude pattern\"\n                continue\n\n            if include_types and not isinstance(value, tuple(include_types)):\n                report.skipped[name] = f\"type {type(value).__name__} not in include_types\"\n                continue\n\n            # Test serialization if required\n            if only_serializable:\n                try:\n                    import pickle\n                    pickle.dumps(value)\n                except Exception as e:\n                    report.skipped[name] = f\"not serializable: {str(e)}\"\n                    continue\n\n            # Prepare variable\n            var_value = deepcopy(value) if deep_copy else value\n            var_name = f\"{prefix}{name}\" if prefix else name\n\n            # Add to pipeline variables\n            pipeline.variables[var_name] = var_value\n            report.added[var_name] = get_type_info(value)\n\n        except Exception as e:\n            report.errors[name] = str(e)\n\n    return report\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.parser","title":"<code>parser</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.CodingAgent.parser.CodeProcessor","title":"<code>CodeProcessor</code>","text":"Source code in <code>toolboxv2/mods/isaa/CodingAgent/parser.py</code> <pre><code>class CodeProcessor:\n    def __init__(self, code_base='./'):\n        self.language_patterns = [\n            r'```([\\w-]+)\\n((?:#|//|&lt;!--)\\s*(\\S+))?\\n([\\s\\S]*?)```',  # Standard pattern\n            r'```([\\w-]+)\\s*\\n([\\s\\S]*?)```'  # Pattern without filename comment\n        ]\n        self.code_base = code_base\n\n    def extract_code(self, text):\n        code_blocks = {}\n        seen = set()\n        for pattern in self.language_patterns:\n            matches = re.finditer(pattern, text, re.DOTALL)\n            for match in matches:\n\n                print(match.groups())\n\n                if len(match.groups()) &lt; 3:\n                    continue\n\n                code = match.groups()[3]\n                filename = match.groups()[2]\n\n                if code == code_blocks.get(filename):\n                    continue\n\n                if code_blocks.get(filename) is not None and code != code_blocks.get(filename):\n                    comment_prfix = match.groups()[1].replace(filename, '')\n                    filename = code.split('\\n')[0].replace(comment_prfix, '')\n                    code = code.replace(comment_prfix + filename + '\\n', '')\n\n                    print(\"new code\", code)\n\n                seen.add(filename)\n\n                code_blocks[filename] = code\n        return code_blocks\n\n    def write_code(self, code_dict):\n        for filename, code in code_dict.items():\n            filepath = os.path.join(self.code_base, filename)\n            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n            print(\"Writing\", filepath)\n            with open(filepath, \"w\") as f:\n                f.write(code)\n\n    def extract_and_write_code(self, text):\n        code_blocks = self.extract_code(text)\n        files = []\n        for filename, new_code in code_blocks.items():\n            filepath = os.path.join(self.code_base, filename)\n            files.append(filepath)\n            if os.path.exists(filepath):\n                self.update_existing_file(filepath, new_code)\n            else:\n                self.write_code({filename: new_code})\n        return files\n\n    def update_existing_file(self, filepath, new_code):\n        \"\"\"\n            Update an existing Python file with new code while preserving existing implementations.\n\n            Args:\n                filepath (str): Path to the file to be updated\n                new_code (str): New code to merge with existing code\n            \"\"\"\n        try:\n            # Read existing code\n            with open(filepath) as f:\n                existing_code = f.read()\n\n            # Parse existing and new code\n            existing_ast_tree = ast.parse(existing_code)\n            new_ast_tree = ast.parse(new_code)\n\n            # Create updater and transform the AST\n            updater = CodeUpdater(existing_ast_tree)\n            updated_ast = updater.visit(new_ast_tree)\n\n            # Convert AST back to source code\n            updated_code = astor.to_source(updated_ast)\n\n            # Write updated code back to file\n            with open(filepath, 'w') as f:\n                f.write(updated_code)\n\n            print(f\"Successfully updated {filepath}\")\n            return True\n\n        except Exception as e:\n            print(f\"Error updating {filepath}: {e}\")\n            return False\n</code></pre> <code>update_existing_file(filepath, new_code)</code> \u00b6 <p>Update an existing Python file with new code while preserving existing implementations.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the file to be updated</p> required <code>new_code</code> <code>str</code> <p>New code to merge with existing code</p> required Source code in <code>toolboxv2/mods/isaa/CodingAgent/parser.py</code> <pre><code>def update_existing_file(self, filepath, new_code):\n    \"\"\"\n        Update an existing Python file with new code while preserving existing implementations.\n\n        Args:\n            filepath (str): Path to the file to be updated\n            new_code (str): New code to merge with existing code\n        \"\"\"\n    try:\n        # Read existing code\n        with open(filepath) as f:\n            existing_code = f.read()\n\n        # Parse existing and new code\n        existing_ast_tree = ast.parse(existing_code)\n        new_ast_tree = ast.parse(new_code)\n\n        # Create updater and transform the AST\n        updater = CodeUpdater(existing_ast_tree)\n        updated_ast = updater.visit(new_ast_tree)\n\n        # Convert AST back to source code\n        updated_code = astor.to_source(updated_ast)\n\n        # Write updated code back to file\n        with open(filepath, 'w') as f:\n            f.write(updated_code)\n\n        print(f\"Successfully updated {filepath}\")\n        return True\n\n    except Exception as e:\n        print(f\"Error updating {filepath}: {e}\")\n        return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster","title":"<code>SearchAgentCluster</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool","title":"<code>search_tool</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool.WebContentParser","title":"<code>WebContentParser</code>","text":"<p>Utility class for parsing web content using BrowserAnt</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>class WebContentParser:\n    \"\"\"Utility class for parsing web content using BrowserAnt\"\"\"\n\n    def __init__(self, browser_wrapper: BrowserWrapper):\n        \"\"\"Initialize with a browser wrapper\"\"\"\n        self.browser_wrapper = browser_wrapper\n\n    async def extract_article(self, url: str) -&gt; dict[str, Any]:\n        \"\"\"Extract article content with title, text, and metadata\"\"\"\n        await self.browser_wrapper.initialize()\n        page = await self.browser_wrapper.navigate(url)\n\n        # Execute readability.js to extract article content\n        readability_js = \"\"\"\n        function extractArticle() {\n            // Simple article extraction logic\n            const article = {\n                title: document.title,\n                byline: '',\n                content: '',\n                textContent: '',\n                excerpt: '',\n                siteName: '',\n                publishedTime: ''\n            };\n\n            // Try to find article elements\n            const articleElement = document.querySelector('article') ||\n                                   document.querySelector('main') ||\n                                   document.querySelector('.post-content') ||\n                                   document.querySelector('.entry-content');\n\n            if (articleElement) {\n                article.content = articleElement.innerHTML;\n                article.textContent = articleElement.textContent;\n            } else {\n                // Fallback to body content\n                article.content = document.body.innerHTML;\n                article.textContent = document.body.textContent;\n            }\n\n            // Try to extract metadata\n            const metaTags = document.querySelectorAll('meta');\n            metaTags.forEach(tag =&gt; {\n                const property = tag.getAttribute('property') || tag.getAttribute('name');\n                const content = tag.getAttribute('content');\n\n                if (property &amp;&amp; content) {\n                    if (property === 'og:site_name') article.siteName = content;\n                    if (property === 'og:title' &amp;&amp; !article.title) article.title = content;\n                    if (property === 'og:description' &amp;&amp; !article.excerpt) article.excerpt = content;\n                    if (property === 'article:published_time') article.publishedTime = content;\n                    if (property === 'author' || property === 'article:author') article.byline = content;\n                }\n            });\n\n            // Extract first paragraph as excerpt if not found\n            if (!article.excerpt) {\n                const paragraphs = document.querySelectorAll('p');\n                if (paragraphs.length &gt; 0) {\n                    for (let i = 0; i &lt; paragraphs.length; i++) {\n                        const text = paragraphs[i].textContent.trim();\n                        if (text.length &gt; 50) {\n                            article.excerpt = text;\n                            break;\n                        }\n                    }\n                }\n            }\n\n            return article;\n        }\n\n        return extractArticle();\n        \"\"\"\n\n        # Extract article content\n        article = await page.evaluate(readability_js)\n\n        # Add markdown version\n        article['markdown'] = await self.browser_wrapper.extract_markdown(page)\n\n        # Take a screenshot\n        screenshot_data = await self.browser_wrapper.take_scrolling_screenshot(page)\n        article['screenshot'] = base64.b64encode(screenshot_data).decode('utf-8')\n\n        return article\n\n    async def extract_table_data(self, url: str, table_selector: str = 'table') -&gt; list[dict[str, Any]]:\n        \"\"\"Extract tabular data from a webpage\"\"\"\n        await self.browser_wrapper.initialize()\n        page = await self.browser_wrapper.navigate(url)\n\n        # Script to extract table data\n        extract_table_js = \"\"\"\n        (tableSelector) =&gt; {\n            const tables = document.querySelectorAll(tableSelector);\n            if (tables.length === 0) return [];\n\n            // Use the first table found\n            const table = tables[0];\n            const headers = Array.from(table.querySelectorAll('th')).map(th =&gt; th.textContent.trim());\n\n            // If no headers found, try using the first row\n            const headerRow = headers.length &gt; 0 ? headers :\n                            Array.from(table.querySelectorAll('tr:first-child td')).map(td =&gt; td.textContent.trim());\n\n            const rows = Array.from(table.querySelectorAll('tr'));\n            const result = [];\n\n            // Start from 1 if we have headers, otherwise from 0\n            const startIdx = headers.length &gt; 0 ? 1 : 0;\n\n            for (let i = startIdx; i &lt; rows.length; i++) {\n                const row = rows[i];\n                const cells = Array.from(row.querySelectorAll('td')).map(td =&gt; td.textContent.trim());\n\n                if (cells.length &gt; 0) {\n                    const rowData = {};\n                    for (let j = 0; j &lt; Math.min(headerRow.length, cells.length); j++) {\n                        // Create a valid object key from header\n                        const key = headerRow[j].replace(/[^a-zA-Z0-9]/g, '_').toLowerCase();\n                        rowData[key] = cells[j];\n                    }\n                    result.push(rowData);\n                }\n            }\n\n            return result;\n        }\n        \"\"\"\n\n        # Extract data\n        table_data = await page.evaluate(extract_table_js, table_selector)\n        return table_data\n\n    async def extract_links(self, url: str, link_selector: str = 'a') -&gt; list[dict[str, str]]:\n        \"\"\"Extract all links from a webpage\"\"\"\n        await self.browser_wrapper.initialize()\n        page = await self.browser_wrapper.navigate(url)\n\n        # Script to extract links\n        extract_links_js = \"\"\"\n        (linkSelector) =&gt; {\n            const links = Array.from(document.querySelectorAll(linkSelector));\n            return links.map(link =&gt; {\n                return {\n                    text: link.textContent.trim(),\n                    href: link.href,\n                    title: link.getAttribute('title') || '',\n                    isExternal: link.hostname !== window.location.hostname\n                };\n            }).filter(link =&gt; link.href &amp;&amp; link.href.startsWith('http'));\n        }\n        \"\"\"\n\n        # Extract links\n        links = await page.evaluate(extract_links_js, link_selector)\n        return links\n</code></pre> <code>__init__(browser_wrapper)</code> \u00b6 <p>Initialize with a browser wrapper</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>def __init__(self, browser_wrapper: BrowserWrapper):\n    \"\"\"Initialize with a browser wrapper\"\"\"\n    self.browser_wrapper = browser_wrapper\n</code></pre> <code>extract_article(url)</code> <code>async</code> \u00b6 <p>Extract article content with title, text, and metadata</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def extract_article(self, url: str) -&gt; dict[str, Any]:\n    \"\"\"Extract article content with title, text, and metadata\"\"\"\n    await self.browser_wrapper.initialize()\n    page = await self.browser_wrapper.navigate(url)\n\n    # Execute readability.js to extract article content\n    readability_js = \"\"\"\n    function extractArticle() {\n        // Simple article extraction logic\n        const article = {\n            title: document.title,\n            byline: '',\n            content: '',\n            textContent: '',\n            excerpt: '',\n            siteName: '',\n            publishedTime: ''\n        };\n\n        // Try to find article elements\n        const articleElement = document.querySelector('article') ||\n                               document.querySelector('main') ||\n                               document.querySelector('.post-content') ||\n                               document.querySelector('.entry-content');\n\n        if (articleElement) {\n            article.content = articleElement.innerHTML;\n            article.textContent = articleElement.textContent;\n        } else {\n            // Fallback to body content\n            article.content = document.body.innerHTML;\n            article.textContent = document.body.textContent;\n        }\n\n        // Try to extract metadata\n        const metaTags = document.querySelectorAll('meta');\n        metaTags.forEach(tag =&gt; {\n            const property = tag.getAttribute('property') || tag.getAttribute('name');\n            const content = tag.getAttribute('content');\n\n            if (property &amp;&amp; content) {\n                if (property === 'og:site_name') article.siteName = content;\n                if (property === 'og:title' &amp;&amp; !article.title) article.title = content;\n                if (property === 'og:description' &amp;&amp; !article.excerpt) article.excerpt = content;\n                if (property === 'article:published_time') article.publishedTime = content;\n                if (property === 'author' || property === 'article:author') article.byline = content;\n            }\n        });\n\n        // Extract first paragraph as excerpt if not found\n        if (!article.excerpt) {\n            const paragraphs = document.querySelectorAll('p');\n            if (paragraphs.length &gt; 0) {\n                for (let i = 0; i &lt; paragraphs.length; i++) {\n                    const text = paragraphs[i].textContent.trim();\n                    if (text.length &gt; 50) {\n                        article.excerpt = text;\n                        break;\n                    }\n                }\n            }\n        }\n\n        return article;\n    }\n\n    return extractArticle();\n    \"\"\"\n\n    # Extract article content\n    article = await page.evaluate(readability_js)\n\n    # Add markdown version\n    article['markdown'] = await self.browser_wrapper.extract_markdown(page)\n\n    # Take a screenshot\n    screenshot_data = await self.browser_wrapper.take_scrolling_screenshot(page)\n    article['screenshot'] = base64.b64encode(screenshot_data).decode('utf-8')\n\n    return article\n</code></pre> <code>extract_links(url, link_selector='a')</code> <code>async</code> \u00b6 <p>Extract all links from a webpage</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def extract_links(self, url: str, link_selector: str = 'a') -&gt; list[dict[str, str]]:\n    \"\"\"Extract all links from a webpage\"\"\"\n    await self.browser_wrapper.initialize()\n    page = await self.browser_wrapper.navigate(url)\n\n    # Script to extract links\n    extract_links_js = \"\"\"\n    (linkSelector) =&gt; {\n        const links = Array.from(document.querySelectorAll(linkSelector));\n        return links.map(link =&gt; {\n            return {\n                text: link.textContent.trim(),\n                href: link.href,\n                title: link.getAttribute('title') || '',\n                isExternal: link.hostname !== window.location.hostname\n            };\n        }).filter(link =&gt; link.href &amp;&amp; link.href.startsWith('http'));\n    }\n    \"\"\"\n\n    # Extract links\n    links = await page.evaluate(extract_links_js, link_selector)\n    return links\n</code></pre> <code>extract_table_data(url, table_selector='table')</code> <code>async</code> \u00b6 <p>Extract tabular data from a webpage</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def extract_table_data(self, url: str, table_selector: str = 'table') -&gt; list[dict[str, Any]]:\n    \"\"\"Extract tabular data from a webpage\"\"\"\n    await self.browser_wrapper.initialize()\n    page = await self.browser_wrapper.navigate(url)\n\n    # Script to extract table data\n    extract_table_js = \"\"\"\n    (tableSelector) =&gt; {\n        const tables = document.querySelectorAll(tableSelector);\n        if (tables.length === 0) return [];\n\n        // Use the first table found\n        const table = tables[0];\n        const headers = Array.from(table.querySelectorAll('th')).map(th =&gt; th.textContent.trim());\n\n        // If no headers found, try using the first row\n        const headerRow = headers.length &gt; 0 ? headers :\n                        Array.from(table.querySelectorAll('tr:first-child td')).map(td =&gt; td.textContent.trim());\n\n        const rows = Array.from(table.querySelectorAll('tr'));\n        const result = [];\n\n        // Start from 1 if we have headers, otherwise from 0\n        const startIdx = headers.length &gt; 0 ? 1 : 0;\n\n        for (let i = startIdx; i &lt; rows.length; i++) {\n            const row = rows[i];\n            const cells = Array.from(row.querySelectorAll('td')).map(td =&gt; td.textContent.trim());\n\n            if (cells.length &gt; 0) {\n                const rowData = {};\n                for (let j = 0; j &lt; Math.min(headerRow.length, cells.length); j++) {\n                    // Create a valid object key from header\n                    const key = headerRow[j].replace(/[^a-zA-Z0-9]/g, '_').toLowerCase();\n                    rowData[key] = cells[j];\n                }\n                result.push(rowData);\n            }\n        }\n\n        return result;\n    }\n    \"\"\"\n\n    # Extract data\n    table_data = await page.evaluate(extract_table_js, table_selector)\n    return table_data\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool.WebScraper","title":"<code>WebScraper</code>","text":"<p>A high-performance web scraper using BrowserAnt with multi-tab parallel processing. Handles both structured and unstructured data collection efficiently.</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>class WebScraper:\n    \"\"\"\n    A high-performance web scraper using BrowserAnt with multi-tab parallel processing.\n    Handles both structured and unstructured data collection efficiently.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: WebScraperConfig = WebScraperConfig(),\n        llm: str | BaseChatModel | None = None,\n        chrome_path: str | None = None,\n        remote_url: str | None = None,\n        browser_config: dict[str, Any] | None = None\n    ):\n        \"\"\"\n        Initialize the web scraper with configuration.\n\n        Args:\n            config: Configuration for scraper behavior\n            llm: Language model for intelligent data extraction\n            chrome_path: Path to Chrome executable\n            remote_url: URL for remote browser connection\n            browser_config: Additional browser configuration\n        \"\"\"\n        self.config = config\n        self.browser_wrapper = BrowserWrapper(\n            llm=llm,\n            headless=config.headless,\n            chrome_path=chrome_path,\n            remote_url=remote_url,\n            config=browser_config\n        )\n        self.active_tasks = set()\n        self._semaphore = asyncio.Semaphore(config.max_concurrent_tabs)\n        self._results = {}\n\n        # Create screenshot directory if needed\n        if config.save_screenshots and not os.path.exists(config.screenshot_dir):\n            os.makedirs(config.screenshot_dir)\n\n    async def initialize(self):\n        \"\"\"Initialize the browser if not already initialized\"\"\"\n        await self.browser_wrapper.initialize()\n\n    async def close(self):\n        \"\"\"Close the browser and clean up resources\"\"\"\n        # Wait for all active tasks to complete\n        if self.active_tasks:\n            await asyncio.gather(*self.active_tasks)\n        await self.browser_wrapper.close()\n\n\n    # Add this method to your WebScraper class\n    async def search_web(\n        self,\n        query: str,\n        max_results: int = 5,\n        include_content: bool = True,\n        extract_images: bool = False,\n        extract_tables: bool = False,\n        extract_links: bool = False,\n        save_to_file: str | None = None\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Perform a comprehensive web search and return high-quality data for the given query.\n\n        Args:\n            query: Search query string\n            max_results: Maximum number of results to process (default: 5)\n            include_content: Whether to include full content from result pages (default: True)\n            extract_images: Whether to extract images from result pages (default: False)\n            extract_tables: Whether to extract tables from result pages (default: False)\n            extract_links: Whether to extract links from result pages (default: False)\n            save_to_file: Path to save results as JSON (optional)\n\n        Returns:\n            Dictionary containing search results and extracted information\n        \"\"\"\n        await self.initialize()\n        try:\n            start_time = datetime.now()\n\n            # Try different search engines in order\n            search_engines = [\n                {\n                    \"url\": f\"https://www.google.com/search?q={urllib.parse.quote_plus(query)}\",\n                    \"result_selector\": \".g\",\n                    \"title_selector\": \"h3\",\n                    \"link_selector\": \"a\",\n                    \"snippet_selector\": \".VwiC3b\",\n                    \"name\": \"google\"\n                },\n                {\n                    \"url\": f\"https://www.bing.com/search?q={urllib.parse.quote_plus(query)}\",\n                    \"result_selector\": \".b_algo\",\n                    \"title_selector\": \"h2\",\n                    \"link_selector\": \"a\",\n                    \"snippet_selector\": \".b_caption p\",\n                    \"name\": \"bing\"\n                },\n                {\n                    \"url\": f\"https://duckduckgo.com/?q={urllib.parse.quote_plus(query)}\",\n                    \"result_selector\": \".result\",\n                    \"title_selector\": \"h2\",\n                    \"link_selector\": \"a.result__a\",\n                    \"snippet_selector\": \".result__snippet\",\n                    \"name\": \"duckduckgo\"\n                }\n            ]\n\n            results = []\n\n            for engine in search_engines:\n                try:\n                    # Navigate to search engine\n                    page = await self.browser_wrapper.navigate(engine[\"url\"])\n                    await page.wait_for_load_state(\"networkidle\")\n                    await page.wait_for_timeout(2000)  # Wait for results to load\n\n                    # Extract search results\n                    search_results = await page.evaluate(\n                        \"\"\"\n                        (selectors) =&gt; {\n                            const results = [];\n                            const elements = document.querySelectorAll(selectors.result_selector);\n\n                            for (const element of elements) {\n                                const titleElement = element.querySelector(selectors.title_selector);\n                                const linkElement = element.querySelector(selectors.link_selector);\n                                const snippetElement = element.querySelector(selectors.snippet_selector);\n\n                                if (titleElement &amp;&amp; linkElement) {\n                                    const url = linkElement.href;\n                                    // Skip non-http links and same-domain results\n                                    if (url &amp;&amp; url.startsWith('http') &amp;&amp;\n                                        !url.includes('google.com/search') &amp;&amp;\n                                        !url.includes('bing.com/search') &amp;&amp;\n                                        !url.includes('duckduckgo.com')) {\n                                        results.push({\n                                            title: titleElement.textContent.trim(),\n                                            url: url,\n                                            snippet: snippetElement ? snippetElement.textContent.trim() : '',\n                                            source: selectors.name\n                                        });\n                                    }\n                                }\n                            }\n                            return results;\n                        }\n                        \"\"\",\n                        engine\n                    )\n\n                    if search_results and len(search_results) &gt; 0:\n                        # We got results, add them and break\n                        results = search_results\n                        break\n\n                except Exception as e:\n                    print(f\"Error searching with {engine['name']}: {str(e)}\")\n                    continue  # Try next engine\n\n            # Filter and limit results\n            unique_urls = set()\n            filtered_results = []\n\n            for result in results:\n                if result['url'] not in unique_urls and len(filtered_results) &lt; max_results:\n                    unique_urls.add(result['url'])\n                    filtered_results.append(result)\n\n            results = filtered_results\n\n            # Get detailed content if requested\n            if include_content and results:\n                # Extract content from each result page\n                urls_to_scrape = [result['url'] for result in results]\n\n                # Configure what to extract\n                extract_config = {}\n                if extract_tables:\n                    extract_config['tables'] = 'table'\n                if extract_images:\n                    extract_config['images'] = 'img'\n                if extract_links:\n                    extract_config['links'] = 'a'\n\n                # Scrape all pages in parallel using our efficient multi-tab approach\n                scraped_data = await self.scrape_urls(\n                    urls_to_scrape,\n                    extract_config=extract_config if extract_config else None\n                )\n\n                # Add content to results\n                for i, result in enumerate(results):\n                    if i &lt; len(scraped_data) and 'error' not in scraped_data[i]:\n                        result['content'] = {\n                            'title': scraped_data[i].get('title', result['title']),\n                            'markdown': scraped_data[i].get('markdown', ''),\n                            'text': scraped_data[i].get('text', ''),\n                        }\n\n                        # Add structured data if available\n                        if extract_config and 'structured_data' in scraped_data[i]:\n                            structured_data = scraped_data[i]['structured_data']\n                            for key, value in structured_data.items():\n                                if value:  # Only add non-empty data\n                                    result['content'][key] = value\n\n            # Prepare final response\n            response = {\n                'query': query,\n                'timestamp': datetime.now().isoformat(),\n                'num_results': len(results),\n                'results': results,\n                'execution_time': (datetime.now() - start_time).total_seconds()\n            }\n\n            # Save to file if requested\n            if save_to_file:\n                os.makedirs(os.path.dirname(os.path.abspath(save_to_file)), exist_ok=True)\n                with open(save_to_file, 'w', encoding='utf-8') as f:\n                    json.dump(response, f, ensure_ascii=False, indent=2)\n\n            return response\n\n        finally:\n            # Make sure we clean up browser resources\n            await self.close()\n\n    async def _scrape_url(self, url: str, task_id: str, extract_config: dict[str, Any] = None):\n        \"\"\"\n        Internal method to scrape a single URL\n\n        Args:\n            url: URL to scrape\n            task_id: Unique identifier for this scraping task\n            extract_config: Configuration for what/how to extract\n        \"\"\"\n        try:\n            async with self._semaphore:\n                # Navigate to the URL\n                page = await self.browser_wrapper.navigate(url)\n\n                # Wait for network to become idle\n                await page.wait_for_load_state(\"networkidle\")\n\n                # Perform initial delay\n                if self.config.initial_delay &gt; 0:\n                    await page.wait_for_timeout(self.config.initial_delay)\n\n                # Auto-scroll if configured\n                if self.config.auto_scroll:\n                    await self._auto_scroll(page)\n\n                # Initialize result dictionary\n                result = {\n                    \"url\": url,\n                    \"title\": await page.title(),\n                    \"timestamp\": datetime.now().isoformat(),\n                }\n\n                # Take screenshot if needed\n                if self.config.save_screenshots:\n                    file_name = f\"{urlparse(url).netloc}_{task_id}.png\"\n                    screenshot_path = os.path.join(self.config.screenshot_dir, file_name)\n                    result[\"screenshot\"] = screenshot_path\n                    await self.browser_wrapper.take_scrolling_screenshot(\n                        page=page,\n                        path=screenshot_path,\n                        initial_delay=0,  # We've already waited\n                        scroll_delay=self.config.scroll_delay\n                    )\n\n                # Extract content based on configuration\n                if extract_config:\n                    result[\"structured_data\"] = await self.browser_wrapper.extract_structured_content(\n                        page=page,\n                        config=extract_config\n                    )\n\n                # Extract markdown if configured\n                if self.config.extract_markdown:\n                    result[\"markdown\"] = await self.browser_wrapper.extract_markdown(page=page)\n\n                # Extract text if configured\n                if self.config.extract_text:\n                    result[\"text\"] = await self.browser_wrapper.extract_text(page=page)\n\n                # Extract HTML if configured\n                if self.config.extract_html:\n                    result[\"html\"] = await page.content()\n\n                self._results[task_id] = result\n                return result\n\n        except Exception as e:\n            self._results[task_id] = {\"error\": str(e), \"url\": url}\n            return {\"error\": str(e), \"url\": url}\n\n    async def _auto_scroll(self, page):\n        \"\"\"Automatically scroll down the page to load lazy content\"\"\"\n        try:\n            # Get page dimensions\n            dimensions = await page.evaluate(\"\"\"\n                () =&gt; {\n                    return {\n                        width: document.documentElement.scrollWidth,\n                        height: document.documentElement.scrollHeight,\n                        windowHeight: window.innerHeight\n                    }\n                }\n            \"\"\")\n\n            # Scroll down the page gradually\n            current_position = 0\n            while current_position &lt; dimensions['height']:\n                await page.evaluate(f\"window.scrollTo(0, {current_position})\")\n                await page.wait_for_timeout(self.config.scroll_delay)\n                current_position += dimensions['windowHeight'] // 2\n\n            # Scroll back to top\n            await page.evaluate(\"window.scrollTo(0, 0)\")\n        except Exception as e:\n            print(f\"Error during auto-scroll: {e}\")\n\n    async def scrape_url(self, url: str, extract_config: dict[str, Any] = None) -&gt; dict[str, Any]:\n        \"\"\"\n        Scrape a single URL and return the results\n\n        Args:\n            url: URL to scrape\n            extract_config: Configuration for structured data extraction\n\n        Returns:\n            Dictionary containing scraped data\n        \"\"\"\n        await self.initialize()\n        task_id = f\"{len(self._results)}_{datetime.now().timestamp()}\"\n        result = await self._scrape_url(url, task_id, extract_config)\n        return result\n\n    async def scrape_urls(\n        self,\n        urls: list[str],\n        extract_config: dict[str, Any] | None = None\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Scrape multiple URLs in parallel and return all results\n\n        Args:\n            urls: List of URLs to scrape\n            extract_config: Configuration for structured data extraction\n\n        Returns:\n            List of dictionaries containing scraped data\n        \"\"\"\n        await self.initialize()\n        tasks = []\n\n        for i, url in enumerate(urls):\n            task_id = f\"{i}_{datetime.now().timestamp()}\"\n            task = asyncio.create_task(self._scrape_url(url, task_id, extract_config))\n            self.active_tasks.add(task)\n            task.add_done_callback(self.active_tasks.discard)\n            tasks.append(task)\n\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return [r if not isinstance(r, Exception) else {\"error\": str(r)} for r in results]\n\n    async def scrape_structured_data(\n        self,\n        urls: list[str],\n        model: type[T],\n        extraction_task: str = None\n    ) -&gt; list[T]:\n        \"\"\"\n        Scrape and parse structured data into pydantic models\n\n        Args:\n            urls: List of URLs to scrape\n            model: Pydantic model class for structured data\n            extraction_task: Natural language description of what to extract\n\n        Returns:\n            List of parsed data objects\n        \"\"\"\n        await self.initialize()\n\n        # Create intelligent extraction task if provided\n        if extraction_task:\n            # Create a custom system prompt for extraction\n            class ExtractionPrompt(SystemPrompt):\n                def important_rules(self) -&gt; str:\n                    existing_rules = super().important_rules()\n                    new_rules = f\"\"\"\n                    9. EXTRACTION GOAL:\n                    - Your primary goal is to extract data according to this specific task: {extraction_task}\n                    - You should carefully identify and extract the information as accurately as possible.\n                    - Focus only on relevant information that matches the specified data structure.\n                    \"\"\"\n                    return f'{existing_rules}\\n{new_rules}'\n\n            # Define the extraction task for each URL\n            tasks = []\n            for url in urls:\n                # Setup intelligent extraction for each URL\n                task = asyncio.create_task(self._run_extraction_agent(\n                    url=url,\n                    model=model,\n                    extraction_task=extraction_task,\n                    system_prompt_class=ExtractionPrompt\n                ))\n                self.active_tasks.add(task)\n                task.add_done_callback(self.active_tasks.discard)\n                tasks.append(task)\n\n            # Wait for all extractions to complete\n            results = await asyncio.gather(*tasks, return_exceptions=True)\n            return [r if not isinstance(r, Exception) else None for r in results]\n        else:\n            # Manual extraction based on model fields\n            field_selectors = {}\n            for field_name in model.__annotations__:\n                # Convert field name to likely CSS selector\n                snake_case = field_name\n                selector = f\".{snake_case.replace('_', '-')}, #{snake_case.replace('_', '-')}\"\n                field_selectors[field_name] = selector\n\n            # Scrape with these selectors\n            raw_results = await self.scrape_urls(urls, extract_config=field_selectors)\n\n            # Convert to pydantic models\n            parsed_results = []\n            for result in raw_results:\n                try:\n                    if \"structured_data\" in result and \"error\" not in result:\n                        # Map the extracted data to model fields\n                        model_data = {}\n                        for field_name in model.__annotations__:\n                            if field_name in result[\"structured_data\"]:\n                                field_value = result[\"structured_data\"][field_name]\n                                if isinstance(field_value, list) and len(field_value) &gt; 0:\n                                    model_data[field_name] = field_value[0]  # Take first match\n                                else:\n                                    model_data[field_name] = field_value\n\n                        # Create the model instance\n                        parsed_results.append(model(**model_data))\n                    else:\n                        parsed_results.append(None)\n                except Exception as e:\n                    print(f\"Error parsing result: {e}\")\n                    parsed_results.append(None)\n\n            return parsed_results\n\n    async def _run_extraction_agent(\n        self,\n        url: str,\n        model: type[T],\n        extraction_task: str,\n        system_prompt_class: type[SystemPrompt]\n    ) -&gt; T:\n        \"\"\"Run an intelligent agent to extract structured data\"\"\"\n        # Define output model for the agent\n        controller = Controller(output_model=model)\n\n        # Create the task description\n        fields_info = \"\\n\".join([f\"- {field}: {model.__annotations__[field].__name__}\"\n                                 for field in model.__annotations__])\n\n        task = f\"\"\"\n        Go to {url} and extract the following information:\n        {fields_info}\n\n        Specific extraction instructions: {extraction_task}\n        \"\"\"\n\n        # Create and run the agent\n        agent = await self.browser_wrapper.create_agent(task=task)\n        agent._controller = controller\n        agent._system_prompt_class = system_prompt_class\n\n        history = await agent.run()\n\n        # Parse the result\n        result = history.final_result()\n        if result:\n            try:\n                return model.model_validate_json(result)\n            except Exception as e:\n                print(f\"Error parsing agent result: {e}\")\n                return None\n        return None\n</code></pre> <code>__init__(config=WebScraperConfig(), llm=None, chrome_path=None, remote_url=None, browser_config=None)</code> \u00b6 <p>Initialize the web scraper with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>WebScraperConfig</code> <p>Configuration for scraper behavior</p> <code>WebScraperConfig()</code> <code>llm</code> <code>str | BaseChatModel | None</code> <p>Language model for intelligent data extraction</p> <code>None</code> <code>chrome_path</code> <code>str | None</code> <p>Path to Chrome executable</p> <code>None</code> <code>remote_url</code> <code>str | None</code> <p>URL for remote browser connection</p> <code>None</code> <code>browser_config</code> <code>dict[str, Any] | None</code> <p>Additional browser configuration</p> <code>None</code> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>def __init__(\n    self,\n    config: WebScraperConfig = WebScraperConfig(),\n    llm: str | BaseChatModel | None = None,\n    chrome_path: str | None = None,\n    remote_url: str | None = None,\n    browser_config: dict[str, Any] | None = None\n):\n    \"\"\"\n    Initialize the web scraper with configuration.\n\n    Args:\n        config: Configuration for scraper behavior\n        llm: Language model for intelligent data extraction\n        chrome_path: Path to Chrome executable\n        remote_url: URL for remote browser connection\n        browser_config: Additional browser configuration\n    \"\"\"\n    self.config = config\n    self.browser_wrapper = BrowserWrapper(\n        llm=llm,\n        headless=config.headless,\n        chrome_path=chrome_path,\n        remote_url=remote_url,\n        config=browser_config\n    )\n    self.active_tasks = set()\n    self._semaphore = asyncio.Semaphore(config.max_concurrent_tabs)\n    self._results = {}\n\n    # Create screenshot directory if needed\n    if config.save_screenshots and not os.path.exists(config.screenshot_dir):\n        os.makedirs(config.screenshot_dir)\n</code></pre> <code>close()</code> <code>async</code> \u00b6 <p>Close the browser and clean up resources</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def close(self):\n    \"\"\"Close the browser and clean up resources\"\"\"\n    # Wait for all active tasks to complete\n    if self.active_tasks:\n        await asyncio.gather(*self.active_tasks)\n    await self.browser_wrapper.close()\n</code></pre> <code>initialize()</code> <code>async</code> \u00b6 <p>Initialize the browser if not already initialized</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def initialize(self):\n    \"\"\"Initialize the browser if not already initialized\"\"\"\n    await self.browser_wrapper.initialize()\n</code></pre> <code>scrape_structured_data(urls, model, extraction_task=None)</code> <code>async</code> \u00b6 <p>Scrape and parse structured data into pydantic models</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list[str]</code> <p>List of URLs to scrape</p> required <code>model</code> <code>type[T]</code> <p>Pydantic model class for structured data</p> required <code>extraction_task</code> <code>str</code> <p>Natural language description of what to extract</p> <code>None</code> <p>Returns:</p> Type Description <code>list[T]</code> <p>List of parsed data objects</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def scrape_structured_data(\n    self,\n    urls: list[str],\n    model: type[T],\n    extraction_task: str = None\n) -&gt; list[T]:\n    \"\"\"\n    Scrape and parse structured data into pydantic models\n\n    Args:\n        urls: List of URLs to scrape\n        model: Pydantic model class for structured data\n        extraction_task: Natural language description of what to extract\n\n    Returns:\n        List of parsed data objects\n    \"\"\"\n    await self.initialize()\n\n    # Create intelligent extraction task if provided\n    if extraction_task:\n        # Create a custom system prompt for extraction\n        class ExtractionPrompt(SystemPrompt):\n            def important_rules(self) -&gt; str:\n                existing_rules = super().important_rules()\n                new_rules = f\"\"\"\n                9. EXTRACTION GOAL:\n                - Your primary goal is to extract data according to this specific task: {extraction_task}\n                - You should carefully identify and extract the information as accurately as possible.\n                - Focus only on relevant information that matches the specified data structure.\n                \"\"\"\n                return f'{existing_rules}\\n{new_rules}'\n\n        # Define the extraction task for each URL\n        tasks = []\n        for url in urls:\n            # Setup intelligent extraction for each URL\n            task = asyncio.create_task(self._run_extraction_agent(\n                url=url,\n                model=model,\n                extraction_task=extraction_task,\n                system_prompt_class=ExtractionPrompt\n            ))\n            self.active_tasks.add(task)\n            task.add_done_callback(self.active_tasks.discard)\n            tasks.append(task)\n\n        # Wait for all extractions to complete\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return [r if not isinstance(r, Exception) else None for r in results]\n    else:\n        # Manual extraction based on model fields\n        field_selectors = {}\n        for field_name in model.__annotations__:\n            # Convert field name to likely CSS selector\n            snake_case = field_name\n            selector = f\".{snake_case.replace('_', '-')}, #{snake_case.replace('_', '-')}\"\n            field_selectors[field_name] = selector\n\n        # Scrape with these selectors\n        raw_results = await self.scrape_urls(urls, extract_config=field_selectors)\n\n        # Convert to pydantic models\n        parsed_results = []\n        for result in raw_results:\n            try:\n                if \"structured_data\" in result and \"error\" not in result:\n                    # Map the extracted data to model fields\n                    model_data = {}\n                    for field_name in model.__annotations__:\n                        if field_name in result[\"structured_data\"]:\n                            field_value = result[\"structured_data\"][field_name]\n                            if isinstance(field_value, list) and len(field_value) &gt; 0:\n                                model_data[field_name] = field_value[0]  # Take first match\n                            else:\n                                model_data[field_name] = field_value\n\n                    # Create the model instance\n                    parsed_results.append(model(**model_data))\n                else:\n                    parsed_results.append(None)\n            except Exception as e:\n                print(f\"Error parsing result: {e}\")\n                parsed_results.append(None)\n\n        return parsed_results\n</code></pre> <code>scrape_url(url, extract_config=None)</code> <code>async</code> \u00b6 <p>Scrape a single URL and return the results</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to scrape</p> required <code>extract_config</code> <code>dict[str, Any]</code> <p>Configuration for structured data extraction</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing scraped data</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def scrape_url(self, url: str, extract_config: dict[str, Any] = None) -&gt; dict[str, Any]:\n    \"\"\"\n    Scrape a single URL and return the results\n\n    Args:\n        url: URL to scrape\n        extract_config: Configuration for structured data extraction\n\n    Returns:\n        Dictionary containing scraped data\n    \"\"\"\n    await self.initialize()\n    task_id = f\"{len(self._results)}_{datetime.now().timestamp()}\"\n    result = await self._scrape_url(url, task_id, extract_config)\n    return result\n</code></pre> <code>scrape_urls(urls, extract_config=None)</code> <code>async</code> \u00b6 <p>Scrape multiple URLs in parallel and return all results</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list[str]</code> <p>List of URLs to scrape</p> required <code>extract_config</code> <code>dict[str, Any] | None</code> <p>Configuration for structured data extraction</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of dictionaries containing scraped data</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def scrape_urls(\n    self,\n    urls: list[str],\n    extract_config: dict[str, Any] | None = None\n) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Scrape multiple URLs in parallel and return all results\n\n    Args:\n        urls: List of URLs to scrape\n        extract_config: Configuration for structured data extraction\n\n    Returns:\n        List of dictionaries containing scraped data\n    \"\"\"\n    await self.initialize()\n    tasks = []\n\n    for i, url in enumerate(urls):\n        task_id = f\"{i}_{datetime.now().timestamp()}\"\n        task = asyncio.create_task(self._scrape_url(url, task_id, extract_config))\n        self.active_tasks.add(task)\n        task.add_done_callback(self.active_tasks.discard)\n        tasks.append(task)\n\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    return [r if not isinstance(r, Exception) else {\"error\": str(r)} for r in results]\n</code></pre> <code>search_web(query, max_results=5, include_content=True, extract_images=False, extract_tables=False, extract_links=False, save_to_file=None)</code> <code>async</code> \u00b6 <p>Perform a comprehensive web search and return high-quality data for the given query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query string</p> required <code>max_results</code> <code>int</code> <p>Maximum number of results to process (default: 5)</p> <code>5</code> <code>include_content</code> <code>bool</code> <p>Whether to include full content from result pages (default: True)</p> <code>True</code> <code>extract_images</code> <code>bool</code> <p>Whether to extract images from result pages (default: False)</p> <code>False</code> <code>extract_tables</code> <code>bool</code> <p>Whether to extract tables from result pages (default: False)</p> <code>False</code> <code>extract_links</code> <code>bool</code> <p>Whether to extract links from result pages (default: False)</p> <code>False</code> <code>save_to_file</code> <code>str | None</code> <p>Path to save results as JSON (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing search results and extracted information</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def search_web(\n    self,\n    query: str,\n    max_results: int = 5,\n    include_content: bool = True,\n    extract_images: bool = False,\n    extract_tables: bool = False,\n    extract_links: bool = False,\n    save_to_file: str | None = None\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Perform a comprehensive web search and return high-quality data for the given query.\n\n    Args:\n        query: Search query string\n        max_results: Maximum number of results to process (default: 5)\n        include_content: Whether to include full content from result pages (default: True)\n        extract_images: Whether to extract images from result pages (default: False)\n        extract_tables: Whether to extract tables from result pages (default: False)\n        extract_links: Whether to extract links from result pages (default: False)\n        save_to_file: Path to save results as JSON (optional)\n\n    Returns:\n        Dictionary containing search results and extracted information\n    \"\"\"\n    await self.initialize()\n    try:\n        start_time = datetime.now()\n\n        # Try different search engines in order\n        search_engines = [\n            {\n                \"url\": f\"https://www.google.com/search?q={urllib.parse.quote_plus(query)}\",\n                \"result_selector\": \".g\",\n                \"title_selector\": \"h3\",\n                \"link_selector\": \"a\",\n                \"snippet_selector\": \".VwiC3b\",\n                \"name\": \"google\"\n            },\n            {\n                \"url\": f\"https://www.bing.com/search?q={urllib.parse.quote_plus(query)}\",\n                \"result_selector\": \".b_algo\",\n                \"title_selector\": \"h2\",\n                \"link_selector\": \"a\",\n                \"snippet_selector\": \".b_caption p\",\n                \"name\": \"bing\"\n            },\n            {\n                \"url\": f\"https://duckduckgo.com/?q={urllib.parse.quote_plus(query)}\",\n                \"result_selector\": \".result\",\n                \"title_selector\": \"h2\",\n                \"link_selector\": \"a.result__a\",\n                \"snippet_selector\": \".result__snippet\",\n                \"name\": \"duckduckgo\"\n            }\n        ]\n\n        results = []\n\n        for engine in search_engines:\n            try:\n                # Navigate to search engine\n                page = await self.browser_wrapper.navigate(engine[\"url\"])\n                await page.wait_for_load_state(\"networkidle\")\n                await page.wait_for_timeout(2000)  # Wait for results to load\n\n                # Extract search results\n                search_results = await page.evaluate(\n                    \"\"\"\n                    (selectors) =&gt; {\n                        const results = [];\n                        const elements = document.querySelectorAll(selectors.result_selector);\n\n                        for (const element of elements) {\n                            const titleElement = element.querySelector(selectors.title_selector);\n                            const linkElement = element.querySelector(selectors.link_selector);\n                            const snippetElement = element.querySelector(selectors.snippet_selector);\n\n                            if (titleElement &amp;&amp; linkElement) {\n                                const url = linkElement.href;\n                                // Skip non-http links and same-domain results\n                                if (url &amp;&amp; url.startsWith('http') &amp;&amp;\n                                    !url.includes('google.com/search') &amp;&amp;\n                                    !url.includes('bing.com/search') &amp;&amp;\n                                    !url.includes('duckduckgo.com')) {\n                                    results.push({\n                                        title: titleElement.textContent.trim(),\n                                        url: url,\n                                        snippet: snippetElement ? snippetElement.textContent.trim() : '',\n                                        source: selectors.name\n                                    });\n                                }\n                            }\n                        }\n                        return results;\n                    }\n                    \"\"\",\n                    engine\n                )\n\n                if search_results and len(search_results) &gt; 0:\n                    # We got results, add them and break\n                    results = search_results\n                    break\n\n            except Exception as e:\n                print(f\"Error searching with {engine['name']}: {str(e)}\")\n                continue  # Try next engine\n\n        # Filter and limit results\n        unique_urls = set()\n        filtered_results = []\n\n        for result in results:\n            if result['url'] not in unique_urls and len(filtered_results) &lt; max_results:\n                unique_urls.add(result['url'])\n                filtered_results.append(result)\n\n        results = filtered_results\n\n        # Get detailed content if requested\n        if include_content and results:\n            # Extract content from each result page\n            urls_to_scrape = [result['url'] for result in results]\n\n            # Configure what to extract\n            extract_config = {}\n            if extract_tables:\n                extract_config['tables'] = 'table'\n            if extract_images:\n                extract_config['images'] = 'img'\n            if extract_links:\n                extract_config['links'] = 'a'\n\n            # Scrape all pages in parallel using our efficient multi-tab approach\n            scraped_data = await self.scrape_urls(\n                urls_to_scrape,\n                extract_config=extract_config if extract_config else None\n            )\n\n            # Add content to results\n            for i, result in enumerate(results):\n                if i &lt; len(scraped_data) and 'error' not in scraped_data[i]:\n                    result['content'] = {\n                        'title': scraped_data[i].get('title', result['title']),\n                        'markdown': scraped_data[i].get('markdown', ''),\n                        'text': scraped_data[i].get('text', ''),\n                    }\n\n                    # Add structured data if available\n                    if extract_config and 'structured_data' in scraped_data[i]:\n                        structured_data = scraped_data[i]['structured_data']\n                        for key, value in structured_data.items():\n                            if value:  # Only add non-empty data\n                                result['content'][key] = value\n\n        # Prepare final response\n        response = {\n            'query': query,\n            'timestamp': datetime.now().isoformat(),\n            'num_results': len(results),\n            'results': results,\n            'execution_time': (datetime.now() - start_time).total_seconds()\n        }\n\n        # Save to file if requested\n        if save_to_file:\n            os.makedirs(os.path.dirname(os.path.abspath(save_to_file)), exist_ok=True)\n            with open(save_to_file, 'w', encoding='utf-8') as f:\n                json.dump(response, f, ensure_ascii=False, indent=2)\n\n        return response\n\n    finally:\n        # Make sure we clean up browser resources\n        await self.close()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool.WebScraperConfig","title":"<code>WebScraperConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for web scraper operations</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>class WebScraperConfig(BaseModel):\n    \"\"\"Configuration for web scraper operations\"\"\"\n    max_concurrent_tabs: int = 5\n    default_timeout: float = 30000\n    scroll_delay: int = 500\n    initial_delay: int = 1000\n    viewport_height: int = 900\n    viewport_width: int = 1600\n    wait_for_selectors: bool = True\n    auto_scroll: bool = True\n    save_screenshots: bool = False\n    screenshot_dir: str = \"./screenshots\"\n    extract_markdown: bool = True\n    extract_text: bool = True\n    extract_html: bool = False\n    headless: bool = False\n    disable_images: bool = False\n    user_agent: str | None = None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool.sanitize_filename","title":"<code>sanitize_filename(filename)</code>","text":"<p>Convert a string to a valid filename</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>def sanitize_filename(filename: str) -&gt; str:\n    \"\"\"Convert a string to a valid filename\"\"\"\n    # Replace spaces with underscores and remove invalid characters\n    sanitized = re.sub(r'[^\\w\\s-]', '', filename).strip().lower()\n    return re.sub(r'[-\\s]+', '_', sanitized)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.SearchAgentCluster.search_tool.scrape_documentation_to_markdown","title":"<code>scrape_documentation_to_markdown(start_url, topic=None, max_pages=30, max_depth=3, output_dir=None, toc_filename='table_of_contents.md')</code>  <code>async</code>","text":"<p>Recursively scrape documentation pages starting from a URL, focused on a specific topic, and convert to Markdown.</p> <p>Parameters:</p> Name Type Description Default <code>start_url</code> <code>str</code> <p>The documentation homepage or entry point</p> required <code>topic</code> <code>str | None</code> <p>The topic to focus on (e.g., \"streaming\", \"authentication\")</p> <code>None</code> <code>max_pages</code> <code>int</code> <p>Maximum number of pages to scrape</p> <code>30</code> <code>max_depth</code> <code>int</code> <p>Maximum depth of link traversal</p> <code>3</code> <code>output_dir</code> <code>str | None</code> <p>Directory to save the MD files (if None, just returns them)</p> <code>None</code> <code>toc_filename</code> <code>str</code> <p>Filename for the table of contents</p> <code>'table_of_contents.md'</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary mapping page titles to markdown content</p> Source code in <code>toolboxv2/mods/isaa/SearchAgentCluster/search_tool.py</code> <pre><code>async def scrape_documentation_to_markdown(\n    start_url: str,\n    topic: str | None = None,\n    max_pages: int = 30,\n    max_depth: int = 3,\n    output_dir: str | None = None,\n    toc_filename: str = \"table_of_contents.md\"\n) -&gt; dict[str, str]:\n    \"\"\"\n    Recursively scrape documentation pages starting from a URL,\n    focused on a specific topic, and convert to Markdown.\n\n    Args:\n        start_url: The documentation homepage or entry point\n        topic: The topic to focus on (e.g., \"streaming\", \"authentication\")\n        max_pages: Maximum number of pages to scrape\n        max_depth: Maximum depth of link traversal\n        output_dir: Directory to save the MD files (if None, just returns them)\n        toc_filename: Filename for the table of contents\n\n    Returns:\n        Dictionary mapping page titles to markdown content\n    \"\"\"\n    # Initialize scraper with efficient settings for docs\n    scraper_config = WebScraperConfig(\n        max_concurrent_tabs=5,\n        headless=global_headless,\n        disable_images=True,\n        extract_html=False,\n        auto_scroll=True,\n        scroll_delay=300,\n        initial_delay=500,\n        save_screenshots=False\n    )\n\n    scraper = WebScraper(config=scraper_config)\n    await scraper.initialize()\n\n    # Track visited and pending URLs\n    visited_urls: set[str] = set()\n    pending_urls: list[dict] = [{\"url\": start_url, \"depth\": 0, \"parent\": None}]\n    results: dict[str, dict] = {}\n    domain = urlparse(start_url).netloc\n\n    logging.info(f\"Starting documentation scrape from {start_url}\")\n    if topic:\n        logging.info(f\"Focusing on topic: {topic}\")\n\n    # Create a regular expression pattern for topic if provided\n    topic_pattern = re.compile(rf'\\b{re.escape(topic)}\\b', re.IGNORECASE) if topic else None\n\n    try:\n        # Process URLs breadth-first until we hit max pages or have no more URLs\n        while pending_urls and len(results) &lt; max_pages:\n            # Get the next URL to process\n            current = pending_urls.pop(0)\n            current_url = current[\"url\"]\n            current_depth = current[\"depth\"]\n\n            # Skip if we've already visited this URL\n            if current_url in visited_urls:\n                continue\n\n            logging.info(f\"Scraping: {current_url} (depth: {current_depth})\")\n            visited_urls.add(current_url)\n\n            # Scrape the current page\n            page_result = await scraper.scrape_url(current_url)\n\n            # Skip pages with errors\n            if \"error\" in page_result:\n                logging.warning(f\"Error scraping {current_url}: {page_result['error']}\")\n                continue\n\n            # Check if page is relevant to the topic\n            is_relevant = True\n            if topic_pattern:\n                markdown_content = page_result.get(\"markdown\", \"\")\n                text_content = page_result.get(\"text\", \"\")\n\n                # Check if topic appears in title, URL, or content\n                has_topic_in_title = topic_pattern.search(page_result.get(\"title\", \"\"))\n                has_topic_in_url = topic_pattern.search(current_url)\n                has_topic_in_content = (\n                    topic_pattern.search(markdown_content) or\n                    topic_pattern.search(text_content)\n                )\n\n                is_relevant = has_topic_in_title or has_topic_in_url or has_topic_in_content\n\n            # Process this page if it's relevant\n            if is_relevant:\n                # Extract title and content\n                title = page_result.get(\"title\", f\"Page {len(results) + 1}\")\n\n                # Store the result\n                results[current_url] = {\n                    \"title\": title,\n                    \"markdown\": page_result.get(\"markdown\", \"\"),\n                    \"depth\": current_depth,\n                    \"parent\": current[\"parent\"]\n                }\n\n                # Only proceed deeper if we haven't hit max depth\n                if current_depth &lt; max_depth:\n                    # Extract links to follow\n                    parser = scraper.browser_wrapper.get_parser()\n                    links = await parser.extract_links(current_url)\n\n                    # Filter links for internal documentation pages\n                    doc_links = []\n                    for link in links:\n                        link_url = link[\"href\"]\n                        parsed_url = urlparse(link_url)\n\n                        # Only include links to the same domain\n                        if parsed_url.netloc == domain or not parsed_url.netloc:\n                            # Normalize URL\n                            if not parsed_url.netloc:\n                                link_url = urljoin(current_url, link_url)\n\n                            # Skip anchor links to same page\n                            if link_url.split('#')[0] == current_url.split('#')[0]:\n                                continue\n\n                            # Skip non-documentation links (common patterns)\n                            skip_patterns = [\n                                r'(\\.pdf|\\.zip|\\.tar|\\.gz)$',  # Downloads\n                                r'/search/',  # Search pages\n                                r'/login/',  # Auth pages\n                                r'/logout/',  # Auth pages\n                                r'/tag/',  # Tag pages\n                                r'/version/',  # Version switching\n                                r'/latest/',  # Version switching\n                                r'/download/',  # Download pages\n                                r'/contact/',  # Contact pages\n                                r'/blog/',  # Blog posts (unless that's what we want)\n                            ]\n\n                            should_skip = any(re.search(pattern, link_url) for pattern in skip_patterns)\n                            if should_skip:\n                                continue\n\n                            # Check if it's potentially relevant to the topic\n                            is_potentially_relevant = True\n                            if topic_pattern:\n                                has_topic_in_link_text = topic_pattern.search(link[\"text\"])\n                                has_topic_in_link_url = topic_pattern.search(link_url)\n                                is_potentially_relevant = has_topic_in_link_text or has_topic_in_link_url\n\n                            # Add to pending if it's potentially relevant and not already visited\n                            if is_potentially_relevant and link_url not in visited_urls:\n                                doc_links.append({\n                                    \"url\": link_url,\n                                    \"depth\": current_depth + 1,\n                                    \"parent\": current_url\n                                })\n\n                    # Add the filtered links to our pending list\n                    pending_urls.extend(doc_links)\n\n        # Generate markdown output\n        markdown_results = {}\n\n        # Create a hierarchy for building a table of contents\n        pages_hierarchy = {}\n        for url, page_data in results.items():\n            title = page_data[\"title\"]\n            markdown = page_data[\"markdown\"]\n\n            # Add page URL reference at the bottom\n            markdown += f\"\\n\\n---\\n*Source: [{url}]({url})*\"\n\n            # Add to outputs\n            markdown_results[url] = markdown\n\n            # Track in hierarchy for TOC\n            depth = page_data[\"depth\"]\n            parent = page_data[\"parent\"]\n\n            if depth not in pages_hierarchy:\n                pages_hierarchy[depth] = []\n\n            pages_hierarchy[depth].append({\n                \"url\": url,\n                \"title\": title,\n                \"parent\": parent\n            })\n\n        # Generate table of contents\n        toc = f\"# Documentation: {topic if topic else 'All Topics'}\\n\\n\"\n        toc += f\"*Generated from: [{start_url}]({start_url})*\\n\\n\"\n        toc += \"## Table of Contents\\n\\n\"\n\n        # Sort by depth to build hierarchy\n        for depth in sorted(pages_hierarchy.keys()):\n            pages = pages_hierarchy[depth]\n\n            for page in pages:\n                # Calculate indentation based on depth\n                indent = \"  \" * depth\n                page_filename = sanitize_filename(page[\"title\"]) + \".md\"\n                toc += f\"{indent}- [{page['title']}]({page_filename})\\n\"\n\n        # Save the results if output directory specified\n        if output_dir:\n            os.makedirs(output_dir, exist_ok=True)\n\n            # Write the TOC file\n            with open(os.path.join(output_dir, toc_filename), \"w\", encoding=\"utf-8\") as f:\n                f.write(toc)\n\n            # Write each page file\n            for url, content in markdown_results.items():\n                page_title = results[url][\"title\"]\n                filename = sanitize_filename(page_title) + \".md\"\n                filepath = os.path.join(output_dir, filename)\n\n                with open(filepath, \"w\", encoding=\"utf-8\") as f:\n                    f.write(content)\n\n            logging.info(f\"Saved {len(markdown_results)} documentation pages to {output_dir}\")\n\n        # Include the TOC in the results\n        markdown_results[\"table_of_contents\"] = toc\n\n        return markdown_results\n\n    finally:\n        # Make sure we clean up browser resources\n        await scraper.close()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base","title":"<code>base</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent","title":"<code>Agent</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent.agent","title":"<code>agent</code>","text":"<code>AgentModelData</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>class AgentModelData(BaseModel):\n    name: str = \"FlowAgent\"\n    fast_llm_model: str = \"openrouter/anthropic/claude-3-haiku\"\n    complex_llm_model: str = \"openrouter/openai/gpt-4o\"\n    system_message: str = \"You are a production-ready autonomous agent.\"\n    temperature: float = 0.7\n    max_tokens: int = 2048\n    max_input_tokens: int = 32768\n    api_key: Optional[str] = None\n    api_base: Optional[str] = None\n    budget_manager: Optional[Any] = None\n    caching: bool = True\n    persona: Optional[PersonaConfig] = None\n    use_fast_response: bool = True\n\n    def get_system_message_with_persona(self) -&gt; str:\n        \"\"\"Get system message with persona integration\"\"\"\n        base_message = self.system_message\n\n        if self.persona and self.persona.apply_method in [\"system_prompt\", \"both\"]:\n            persona_addition = self.persona.to_system_prompt_addition()\n            if persona_addition:\n                base_message += f\"\\n\\n## Persona Instructions\\n{persona_addition}\"\n\n        return base_message\n</code></pre> <code>get_system_message_with_persona()</code> \u00b6 <p>Get system message with persona integration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def get_system_message_with_persona(self) -&gt; str:\n    \"\"\"Get system message with persona integration\"\"\"\n    base_message = self.system_message\n\n    if self.persona and self.persona.apply_method in [\"system_prompt\", \"both\"]:\n        persona_addition = self.persona.to_system_prompt_addition()\n        if persona_addition:\n            base_message += f\"\\n\\n## Persona Instructions\\n{persona_addition}\"\n\n    return base_message\n</code></pre> <code>CompletionCheckerNode</code> \u00b6 <p>               Bases: <code>AsyncNode</code></p> <p>Breaks infinite cycles by checking actual completion status</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@with_progress_tracking\nclass CompletionCheckerNode(AsyncNode):\n    \"\"\"Breaks infinite cycles by checking actual completion status\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.execution_count = 0\n        self.max_cycles = 5  # Prevent infinite loops\n\n    async def prep_async(self, shared):\n        current_plan = shared.get(\"current_plan\")\n        tasks = shared.get(\"tasks\", {})\n\n        return {\n            \"current_plan\": current_plan,\n            \"tasks\": tasks,\n            \"execution_count\": self.execution_count\n        }\n\n    async def exec_async(self, prep_res):\n        self.execution_count += 1\n\n        # Safety check: prevent infinite loops\n        if self.execution_count &gt; self.max_cycles:\n            logger.warning(f\"Max execution cycles ({self.max_cycles}) reached, terminating\")\n            return {\n                \"action\": \"force_terminate\",\n                \"reason\": \"Max cycles reached\"\n            }\n\n        current_plan = prep_res[\"current_plan\"]\n        tasks = prep_res[\"tasks\"]\n\n        if not current_plan:\n            return {\"action\": \"truly_complete\", \"reason\": \"No active plan\"}\n\n        # Check actual completion status\n        pending_tasks = [t for t in current_plan.tasks if tasks[t.id].status == \"pending\"]\n        running_tasks = [t for t in current_plan.tasks if tasks[t.id].status == \"running\"]\n        completed_tasks = [t for t in current_plan.tasks if tasks[t.id].status == \"completed\"]\n        failed_tasks = [t for t in current_plan.tasks if tasks[t.id].status == \"failed\"]\n\n        total_tasks = len(current_plan.tasks)\n\n        # Truly complete: all tasks done\n        if len(completed_tasks) + len(failed_tasks) == total_tasks:\n            if len(failed_tasks) == 0 or len(completed_tasks) &gt; len(failed_tasks):\n                return {\"action\": \"truly_complete\", \"reason\": \"All tasks completed\"}\n            else:\n                return {\"action\": \"truly_complete\", \"reason\": \"Plan failed but cannot continue\"}\n\n        # Has pending tasks that can run\n        if pending_tasks and not running_tasks:\n            return {\"action\": \"continue_execution\", \"reason\": f\"{len(pending_tasks)} tasks ready\"}\n\n        # Has running tasks, wait\n        if running_tasks:\n            return {\"action\": \"continue_execution\", \"reason\": f\"{len(running_tasks)} tasks running\"}\n\n        # Need reflection if tasks are stuck\n        if pending_tasks and not running_tasks:\n            return {\"action\": \"needs_reflection\", \"reason\": \"Tasks may be blocked\"}\n\n        # Default: we're done\n        return {\"action\": \"truly_complete\", \"reason\": \"No actionable tasks\"}\n\n    async def post_async(self, shared, prep_res, exec_res):\n        action = exec_res[\"action\"]\n\n        # Reset counter on true completion\n        if action == \"truly_complete\":\n            self.execution_count = 0\n            shared[\"flow_completion_reason\"] = exec_res[\"reason\"]\n        elif action == \"force_terminate\":  # HINZUGEF\u00dcGT\n            self.execution_count = 0\n            shared[\"flow_completion_reason\"] = f\"Force terminated: {exec_res['reason']}\"\n            shared[\"force_terminated\"] = True\n            logger.warning(f\"Flow force terminated: {exec_res['reason']}\")\n\n        return action\n</code></pre> <code>CompoundTask</code> <code>dataclass</code> \u00b6 <p>               Bases: <code>Task</code></p> <p>Task der Sub-Tasks gruppiert</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass CompoundTask(Task):\n    \"\"\"Task der Sub-Tasks gruppiert\"\"\"\n    sub_task_ids: List[str] = field(default_factory=list)\n    execution_strategy: str = \"sequential\"  # \"sequential\" | \"parallel\"\n    success_criteria: str = \"\"  # Wann ist der Compound-Task erfolgreich?\n</code></pre> <code>ContextAggregatorNode</code> \u00b6 <p>               Bases: <code>AsyncNode</code></p> <p>Aggregiere alle relevanten Kontexte und Ergebnisse</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@with_progress_tracking\nclass ContextAggregatorNode(AsyncNode):\n    \"\"\"Aggregiere alle relevanten Kontexte und Ergebnisse\"\"\"\n\n    async def prep_async(self, shared):\n        return {\n            \"results_store\": shared.get(\"results_store\", {}),\n            \"tasks\": shared.get(\"tasks\", {}),\n            \"current_plan\": shared.get(\"current_plan\"),\n            \"original_query\": shared.get(\"current_query\", \"\"),\n            \"conversation_history\": shared.get(\"conversation_history\", []),\n            \"world_model\": shared.get(\"world_model\", {}),\n            \"plan_adaptations\": shared.get(\"plan_adaptations\", 0)\n        }\n\n    async def exec_async(self, prep_res):\n        \"\"\"Aggregiere intelligente Kontextinformationen\"\"\"\n\n        aggregated_context = {\n            \"original_query\": prep_res[\"original_query\"],\n            \"successful_results\": {},\n            \"failed_attempts\": {},\n            \"key_discoveries\": [],\n            \"adaptation_summary\": \"\",\n            \"confidence_scores\": {}\n        }\n\n        # Sammle erfolgreiche Task-Ergebnisse\n        for task_id, task in prep_res[\"tasks\"].items():\n            if task.status == \"completed\" and task.result:\n                result_data = prep_res[\"results_store\"].get(task_id, {})\n\n                aggregated_context[\"successful_results\"][task_id] = {\n                    \"task_description\": task.description,\n                    \"task_type\": task.type,\n                    \"result\": task.result,\n                    \"metadata\": getattr(task, 'metadata', {}),\n                    \"verification\": result_data.get(\"verification\", {})\n                }\n\n                # Extrahiere key discoveries\n                if isinstance(task, ToolTask) and task.hypothesis:\n                    verification = result_data.get(\"verification\", {})\n                    if verification.get(\"hypothesis_score\", 0) &gt; 0.7:\n                        aggregated_context[\"key_discoveries\"].append({\n                            \"discovery\": f\"Tool {task.tool_name} confirmed: {task.hypothesis}\",\n                            \"confidence\": verification.get(\"hypothesis_score\", 0.0),\n                            \"result\": task.result\n                        })\n\n        # Sammle fehlgeschlagene Versuche\n        for task_id, task in prep_res[\"tasks\"].items():\n            if task.status == \"failed\":\n                aggregated_context[\"failed_attempts\"][task_id] = {\n                    \"description\": task.description,\n                    \"error\": task.error,\n                    \"retry_count\": task.retry_count\n                }\n\n        # Plan adaptations summary\n        if prep_res[\"plan_adaptations\"] &gt; 0:\n            aggregated_context[\n                \"adaptation_summary\"] = f\"Plan was adapted {prep_res['plan_adaptations']} times to handle unexpected results.\"\n\n        return aggregated_context\n\n    async def post_async(self, shared, prep_res, exec_res):\n        shared[\"aggregated_context\"] = exec_res\n        if exec_res[\"successful_results\"] or exec_res[\"key_discoveries\"]:\n            return \"context_ready\"\n        else:\n            return \"no_context\"\n</code></pre> <code>exec_async(prep_res)</code> <code>async</code> \u00b6 <p>Aggregiere intelligente Kontextinformationen</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def exec_async(self, prep_res):\n    \"\"\"Aggregiere intelligente Kontextinformationen\"\"\"\n\n    aggregated_context = {\n        \"original_query\": prep_res[\"original_query\"],\n        \"successful_results\": {},\n        \"failed_attempts\": {},\n        \"key_discoveries\": [],\n        \"adaptation_summary\": \"\",\n        \"confidence_scores\": {}\n    }\n\n    # Sammle erfolgreiche Task-Ergebnisse\n    for task_id, task in prep_res[\"tasks\"].items():\n        if task.status == \"completed\" and task.result:\n            result_data = prep_res[\"results_store\"].get(task_id, {})\n\n            aggregated_context[\"successful_results\"][task_id] = {\n                \"task_description\": task.description,\n                \"task_type\": task.type,\n                \"result\": task.result,\n                \"metadata\": getattr(task, 'metadata', {}),\n                \"verification\": result_data.get(\"verification\", {})\n            }\n\n            # Extrahiere key discoveries\n            if isinstance(task, ToolTask) and task.hypothesis:\n                verification = result_data.get(\"verification\", {})\n                if verification.get(\"hypothesis_score\", 0) &gt; 0.7:\n                    aggregated_context[\"key_discoveries\"].append({\n                        \"discovery\": f\"Tool {task.tool_name} confirmed: {task.hypothesis}\",\n                        \"confidence\": verification.get(\"hypothesis_score\", 0.0),\n                        \"result\": task.result\n                    })\n\n    # Sammle fehlgeschlagene Versuche\n    for task_id, task in prep_res[\"tasks\"].items():\n        if task.status == \"failed\":\n            aggregated_context[\"failed_attempts\"][task_id] = {\n                \"description\": task.description,\n                \"error\": task.error,\n                \"retry_count\": task.retry_count\n            }\n\n    # Plan adaptations summary\n    if prep_res[\"plan_adaptations\"] &gt; 0:\n        aggregated_context[\n            \"adaptation_summary\"] = f\"Plan was adapted {prep_res['plan_adaptations']} times to handle unexpected results.\"\n\n    return aggregated_context\n</code></pre> <code>ContextManagerNode</code> \u00b6 <p>               Bases: <code>AsyncNode</code></p> <p>Advanced context management with intelligent splitting</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@with_progress_tracking\nclass ContextManagerNode(AsyncNode):\n    \"\"\"Advanced context management with intelligent splitting\"\"\"\n\n    def __init__(self, max_tokens: int = 8000, compression_threshold: float = 0.76, **kwargs):\n        super().__init__(**kwargs)\n        self.max_tokens = max_tokens\n        self.compression_threshold = compression_threshold\n        self.session_managers = {}  # Chat session instances\n\n    async def prep_async(self, shared):\n        agent_instance = shared.get(\"agent_instance\")\n        session_id = shared.get(\"session_id\", \"default\")\n        current_query = shared.get(\"current_query\", \"\")\n\n        # Initialize or get chat session\n        session_manager = await self._get_session_manager(agent_instance, session_id)\n\n        return {\n            \"session_manager\": session_manager,\n            \"current_query\": current_query,\n            \"agent_instance\": agent_instance,\n            \"tasks\": shared.get(\"tasks\", {}),\n            \"world_model\": shared.get(\"world_model\", {}),\n            \"results_store\": shared.get(\"results_store\", {}),\n            \"conversation_history\": shared.get(\"conversation_history\", []),\n            \"max_tokens\": self.max_tokens,\n            \"compression_threshold\": self.compression_threshold\n        }\n\n    async def _get_session_manager(self, agent_instance, session_id: str):\n        \"\"\"Get or create ChatSession for this session\"\"\"\n        if session_id not in self.session_managers:\n            from toolboxv2 import get_app\n            from toolboxv2.mods.isaa.extras.session import ChatSession\n\n            memory_instance = get_app().get_mod(\"isaa\").get_memory()\n            space_name = f\"ChatSession/ContextManager/{agent_instance.amd.name}/{session_id}\"\n\n            self.session_managers[session_id] = ChatSession(\n                memory_instance,\n                space_name=space_name,\n                max_length=200  # Keep more history for context\n            )\n\n        return self.session_managers[session_id]\n\n    async def exec_async(self, prep_res):\n        session_manager = prep_res[\"session_manager\"]\n        current_query = prep_res[\"current_query\"]\n\n        # Build comprehensive context parts\n        context_parts = await self._build_three_part_context(prep_res)\n\n        # Calculate token usage\n        total_tokens = self._estimate_total_tokens(context_parts)\n        usage_ratio = total_tokens / prep_res[\"max_tokens\"]\n\n        # Automatic compression if needed\n        if usage_ratio &gt;= prep_res[\"compression_threshold\"]:\n            context_parts = await self._compress_context_intelligently(context_parts, prep_res)\n            total_tokens = self._estimate_total_tokens(context_parts)\n            usage_ratio = total_tokens / prep_res[\"max_tokens\"]\n\n        # Store current interaction for future reference\n        await session_manager.add_message({\n            'role': 'user',\n            'content': current_query\n        })\n\n        return {\n            \"recent_interaction\": context_parts[\"recent_interaction\"],\n            \"session_summary\": context_parts[\"session_summary\"],\n            \"task_context\": context_parts[\"task_context\"],\n            \"total_tokens\": total_tokens,\n            \"usage_ratio\": usage_ratio,\n            \"compression_applied\": usage_ratio &gt;= prep_res[\"compression_threshold\"],\n            \"session_manager\": session_manager\n        }\n\n    async def _build_three_part_context(self, prep_res) -&gt; Dict[str, str]:\n        \"\"\"Build the 3-part context system\"\"\"\n        session_manager = prep_res[\"session_manager\"]\n        current_query = prep_res[\"current_query\"]\n\n        # Part 1: Recent Interaction (latest exchanges)\n        recent_interaction = await self._build_recent_interaction(session_manager, current_query)\n\n        # Part 2: Session Summary (compressed history + logger insights)\n        session_summary = await self._build_session_summary(session_manager, prep_res)\n\n        # Part 3: Task Context (current agent state and tasks)\n        task_context = await self._build_task_context(prep_res)\n\n        return {\n            \"recent_interaction\": recent_interaction,\n            \"session_summary\": session_summary,\n            \"task_context\": task_context\n        }\n\n    async def _build_recent_interaction(self, session_manager, current_query: str) -&gt; str:\n        \"\"\"Latest conversation context\"\"\"\n        # Get last few exchanges from session\n        recent_history = session_manager.get_past_x(6)  # Last 3 exchanges\n\n        formatted_recent = []\n        for entry in recent_history:\n            role = entry.get('role', 'unknown')\n            content = entry.get('content', '')\n            formatted_recent.append(f\"{role}: {content}\")\n\n        recent_context = \"## Recent Interaction\\n\"\n        if formatted_recent:\n            recent_context += \"\\n\".join(formatted_recent)\n\n        recent_context += f\"\\nCurrent: user: {current_query}\"\n\n        return recent_context\n\n    async def _build_session_summary(self, session_manager, prep_res) -&gt; str:\n        \"\"\"Compressed session history with insights\"\"\"\n        # Get relevant historical context using memory search\n        context_query = prep_res[\"current_query\"]\n        relevant_refs = await session_manager.get_reference(context_query, top_k=5, to_str=True)\n\n        summary = \"## Session Summary\\n\"\n\n        if relevant_refs:\n            summary += f\"Relevant previous interactions:\\n{relevant_refs}\\n\"\n\n        # Add world model insights\n        world_model = prep_res.get(\"world_model\", {})\n        if world_model:\n            key_facts = []\n            for key, value in list(world_model.items())[:10]:  # Top 10 facts\n                if self._is_context_relevant(key, context_query):\n                    key_facts.append(f\"- {key}: {value}\")\n\n            if key_facts:\n                summary += f\"\\nKnown context:\\n\" + \"\\n\".join(key_facts)\n\n        return summary\n\n    async def _build_task_context(self, prep_res) -&gt; str:\n        \"\"\"Current agent task state and capabilities\"\"\"\n        tasks = prep_res.get(\"tasks\", {})\n        results_store = prep_res.get(\"results_store\", {})\n        agent_instance = prep_res.get(\"agent_instance\")\n\n        context = \"## Current Task Context\\n\"\n\n        # Active/recent tasks\n        active_tasks = [t for t in tasks.values() if t.status in [\"running\", \"completed\"]]\n        if active_tasks:\n            context += \"Recent task activity:\\n\"\n            for task in active_tasks[-5:]:  # Last 5 tasks\n                status_emoji = \"\u2713\" if task.status == \"completed\" else \"\u2699\ufe0f\"\n                context += f\"{status_emoji} {task.description} ({task.status})\\n\"\n\n        # Key results\n        if results_store:\n            context += \"\\nKey findings:\\n\"\n            for task_id, result in list(results_store.items())[-3:]:  # Last 3 results\n                if result.get(\"metadata\", {}).get(\"success\", False):\n                    data_preview = str(result.get(\"data\", \"\"))[:100] + \"...\"\n                    context += f\"- {task_id}: {data_preview}\\n\"\n\n        # Agent capabilities\n        if agent_instance and hasattr(agent_instance, '_tool_capabilities'):\n            available_tools = list(agent_instance._tool_capabilities.keys())[:5]\n            context += f\"\\nAvailable tools: {', '.join(available_tools)}\"\n\n        return context\n\n    async def _compress_context_intelligently(self, context_parts: Dict[str, str], prep_res) -&gt; Dict[str, str]:\n        \"\"\"Intelligent context compression using LLM\"\"\"\n        if not LITELLM_AVAILABLE:\n            return self._fallback_compression(context_parts)\n\n        # Compress session summary (most compressible)\n        session_summary = context_parts[\"session_summary\"]\n        if len(session_summary) &gt; 1000:  # Only compress if substantial\n            compressed_summary = await self._llm_compress_text(\n                session_summary,\n                \"session context\",\n                prep_res.get(\"agent_instance\")\n            )\n            context_parts[\"session_summary\"] = compressed_summary\n\n        # Compress task context if still too large\n        total_tokens = self._estimate_total_tokens(context_parts)\n        if total_tokens &gt; prep_res[\"max_tokens\"] * 0.9:\n            task_context = context_parts[\"task_context\"]\n            compressed_task = await self._llm_compress_text(\n                task_context,\n                \"task context\",\n                prep_res.get(\"agent_instance\")\n            )\n            context_parts[\"task_context\"] = compressed_task\n\n        return context_parts\n\n    async def _llm_compress_text(self, text: str, context_type: str, agent_instance) -&gt; str:\n        \"\"\"Compress text using LLM while preserving key information\"\"\"\n        if len(text) &lt; 200:  # Don't compress short texts\n            return text\n\n        prompt = f\"\"\"\n    Compress this {context_type} while preserving ALL key information and relationships.\n    Focus on removing redundancy and verbose explanations, keep facts and data intact.\n\n    Original text:\n    {text}\n\n    Compressed version (aim for 60% reduction):\"\"\"\n\n        try:\n            model_to_use = agent_instance.amd.fast_llm_model if agent_instance else \"openrouter/anthropic/claude-3-haiku\"\n\n            response = await agent_instance.a_run_llm_completion(\n                model=model_to_use,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.1,  # Low temperature for consistent compression\n                max_tokens=int(len(text) * 0.4),  # Target 60% reduction\n                node_name=\"Context Compression\",\n                task_id=\"compression\"\n            )\n\n            compressed = response.strip()\n            return compressed if compressed else text\n\n        except Exception as e:\n            logger.warning(f\"LLM compression failed: {e}\")\n            return self._simple_text_compression(text)\n\n    def _fallback_compression(self, context_parts: Dict[str, str]) -&gt; Dict[str, str]:\n        \"\"\"Simple fallback compression without LLM\"\"\"\n        # Compress by truncating less important parts\n        if len(context_parts[\"session_summary\"]) &gt; 800:\n            lines = context_parts[\"session_summary\"].split('\\n')\n            context_parts[\"session_summary\"] = '\\n'.join(lines[:15]) + \"\\n[...compressed...]\"\n\n        if len(context_parts[\"task_context\"]) &gt; 600:\n            lines = context_parts[\"task_context\"].split('\\n')\n            context_parts[\"task_context\"] = '\\n'.join(lines[:12]) + \"\\n[...compressed...]\"\n\n        return context_parts\n\n    def _simple_text_compression(self, text: str) -&gt; str:\n        \"\"\"Simple text compression by removing redundancy\"\"\"\n        lines = text.split('\\n')\n\n        # Remove empty lines and duplicates\n        compressed_lines = []\n        seen_lines = set()\n\n        for line in lines:\n            line = line.strip()\n            if line and line not in seen_lines:\n                compressed_lines.append(line)\n                seen_lines.add(line)\n\n        # Keep first 75% of unique lines\n        keep_count = int(len(compressed_lines) * 0.75)\n        return '\\n'.join(compressed_lines[:keep_count])\n\n    def _is_context_relevant(self, key: str, query: str) -&gt; bool:\n        \"\"\"Check if world model key is relevant to current query\"\"\"\n        query_words = set(query.lower().split())\n        key_words = set(key.lower().split())\n\n        # Simple relevance scoring\n        intersection = query_words.intersection(key_words)\n        relevance_score = len(intersection) / max(len(query_words), 1)\n\n        return relevance_score &gt; 0.1\n\n    def _estimate_total_tokens(self, context_parts: Dict[str, str]) -&gt; int:\n        \"\"\"Estimate total token count for all context parts\"\"\"\n        total_chars = sum(len(part) for part in context_parts.values())\n        return total_chars // 4  # Rough token estimation\n\n    async def post_async(self, shared, prep_res, exec_res):\n        shared[\"formatted_context\"] = exec_res\n        shared[\"context_tokens\"] = exec_res[\"total_tokens\"]\n        shared[\"context_usage_ratio\"] = exec_res[\"usage_ratio\"]\n        shared[\"context_compression_applied\"] = exec_res[\"compression_applied\"]\n\n        # Store session manager for other nodes\n        shared[\"session_manager\"] = exec_res[\"session_manager\"]\n\n        return \"context_ready\"\n</code></pre> <code>DecisionTask</code> <code>dataclass</code> \u00b6 <p>               Bases: <code>Task</code></p> <p>Task f\u00fcr dynamisches Routing</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass DecisionTask(Task):\n    \"\"\"Task f\u00fcr dynamisches Routing\"\"\"\n    decision_prompt: str = \"\"  # Kurze Frage an LLM\n    routing_map: Dict[str, str] = field(default_factory=dict)  # Ergebnis -&gt; n\u00e4chster Task\n    decision_model: str = \"fast\"  # Welches LLM f\u00fcr Entscheidung\n</code></pre> <code>FlowAgent</code> \u00b6 <p>Production-ready agent system built on PocketFlow</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>class FlowAgent:\n    \"\"\"Production-ready agent system built on PocketFlow\"\"\"\n    def __init__(\n        self,\n        amd: AgentModelData,\n        world_model: Dict[str, Any] = None,\n        verbose: bool = False,\n        enable_pause_resume: bool = True,\n        checkpoint_interval: int = 300,  # 5 minutes\n        max_parallel_tasks: int = 3,\n        progress_callback: Optional[callable] = None,\n        **kwargs\n    ):\n        self.amd = amd\n        self.world_model = world_model or {}\n        self.verbose = verbose\n        self.enable_pause_resume = enable_pause_resume\n        self.checkpoint_interval = checkpoint_interval\n        self.max_parallel_tasks = max_parallel_tasks\n        self.progress_tracker = ProgressTracker(progress_callback, agent_name=amd.name)\n\n        # Core state\n        self.shared = {\n            \"world_model\": self.world_model,\n            \"tasks\": {},\n            \"task_plans\": {},\n            \"system_status\": \"idle\",\n            \"session_data\": {},\n            \"performance_metrics\": {},\n            \"conversation_history\": [],\n            \"available_tools\": [],\n            \"progress_tracker\": self.progress_tracker\n        }\n        self.variable_manager = VariableManager(self.shared[\"world_model\"], self.shared)\n        # Register default scopes\n        self._setup_variable_scopes()\n        # Flows\n        self.task_flow = TaskManagementFlow(max_parallel_tasks=self.max_parallel_tasks)\n        self.response_flow = ResponseGenerationFlow()\n\n        if hasattr(self.task_flow, 'executor_node'):\n            self.task_flow.executor_node.agent_instance = self\n\n        # Agent state\n        self.is_running = False\n        self.is_paused = False\n        self.last_checkpoint = None\n        self.checkpoint_data = {}\n\n        # Threading\n        self.executor = ThreadPoolExecutor(max_workers=max_parallel_tasks)\n        self._shutdown_event = threading.Event()\n\n        # Server components\n        self.a2a_server: Optional[A2AServer] = None\n        self.mcp_server: Optional[FastMCP] = None\n\n        # Enhanced tool registry\n        self._tool_registry = {}\n        self._tool_capabilities = {}\n        self._tool_analysis_cache = {}\n\n        # Tool analysis file path\n        self.tool_analysis_file = self._get_tool_analysis_path()\n\n        self._tool_capabilities.update(self._load_tool_analysis())\n        if self.amd.budget_manager:\n            self.amd.budget_manager.load_data()\n\n        logger.info(f\"FlowAgent initialized: {amd.name}\")\n\n    @property\n    def progress_callback(self):\n        return self.progress_tracker.progress_callback\n\n    @progress_callback.setter\n    def progress_callback(self, value):\n        self.progress_tracker.progress_callback = value\n\n\n    async def a_run_llm_completion(self, node_name=\"FlowAgentLLMCall\",task_id=\"unknown\",model_preference=\"fast\", **kwargs) -&gt; str:\n        if \"model\" not in kwargs:\n            kwargs[\"model\"] = self.amd.fast_llm_model if model_preference == \"fast\" else self.amd.complex_llm_model\n\n        llm_start = time.perf_counter()\n\n        if self.progress_tracker:\n            await self.progress_tracker.emit_event(ProgressEvent(\n                event_type=\"llm_call\",\n                timestamp=time.time(),\n                status=NodeStatus.RUNNING,\n                node_name=node_name,\n                task_id=task_id,\n                llm_model=kwargs[\"model\"],\n                llm_temperature=kwargs.get(\"temperature\", 0.7),\n                metadata={\n                    \"task_type\": \"LLMCall\",\n                    \"model_preference\": kwargs.get(\"model_preference\", \"fast\"),\n                    \"prompt_length\": len(kwargs.get(\"messages\", [{}])[-1].get(\"content\", \"\"))\n                }\n            ))\n\n        try:\n\n            response = await litellm.acompletion(**kwargs\n            )\n\n            llm_duration = time.perf_counter() - llm_start\n            result = response.choices[0].message.content\n\n            # Extract token usage and cost\n            usage = response.usage\n            input_tokens = usage.prompt_tokens if usage else 0\n            output_tokens = usage.completion_tokens if usage else 0\n            total_tokens = usage.total_tokens if usage else 0\n\n            call_cost = self.progress_tracker.calculate_llm_cost(kwargs[\"model\"], input_tokens,\n                                                            output_tokens) if self.progress_tracker else 0.0\n\n            if self.progress_tracker:\n                await self.progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"llm_call\",\n                    timestamp=time.time(),\n                    node_name=node_name,\n                    task_id=task_id,\n                    llm_model=kwargs[\"model\"],\n                    llm_prompt_tokens=input_tokens,\n                    llm_completion_tokens=output_tokens,\n                    llm_total_tokens=total_tokens,\n                    llm_cost=call_cost,\n                    llm_duration=llm_duration,\n                    llm_temperature=kwargs.get(\"temperature\", 0.7),\n                    metadata={\n                        \"success\": True,\n                        \"result_length\": len(result),\n                        \"task_type\": \"LLMCall\"\n                    }\n                ))\n\n            return result\n        except Exception as e:\n            llm_duration = time.perf_counter() - llm_start\n\n            if self.progress_tracker:\n                await self.progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"error\",\n                    timestamp=time.time(),\n                    status=NodeStatus.FAILED,\n                    node_name=node_name,\n                    task_id=task_id,\n                    llm_model=kwargs[\"model\"],\n                    llm_duration=llm_duration,\n                    metadata={\n                        \"error\": str(e),\n                        \"task_type\": \"LLMCall\",\n                        \"error_type\": type(e).__name__\n                    }\n                ))\n\n            raise\n\n    async def a_run(\n        self,\n        query: str,\n        session_id: str = \"default\",\n        user_id: str = None,\n        stream_callback: Optional[Callable] = None,\n        **kwargs\n    ) -&gt; str:\n        \"\"\"Main entry point for agent execution\"\"\"\n\n        execution_start = self.progress_tracker.start_timer(\"total_execution\")\n\n        await self.progress_tracker.emit_event(ProgressEvent(\n            event_type=\"execution_start\",\n            timestamp=time.time(),\n            status=NodeStatus.RUNNING,\n            node_name=\"FlowAgent\",\n            session_id=session_id,\n            metadata={\"query\": query, \"user_id\": user_id}\n        ))\n\n        try:\n            # Set user context variables\n            timestamp = datetime.now()\n            self.variable_manager.register_scope('user', {\n                'id': user_id,\n                'session': session_id,\n                'query': query,\n                'timestamp': timestamp.isoformat()\n            })\n\n            # Update system variables\n            self.variable_manager.set('system_context.timestamp', {'isoformat':timestamp.isoformat()})\n            self.variable_manager.set('system_context.timestamp.year', timestamp.year)\n            self.variable_manager.set('system_context.timestamp.month', timestamp.month)\n            self.variable_manager.set('system_context.timestamp.week', timestamp.strftime('%W'))\n            self.variable_manager.set('system_context.timestamp.day', timestamp.day)\n            self.variable_manager.set('system_context.timestamp.hour', timestamp.hour)\n            self.variable_manager.set('system_context.timestamp.minute', timestamp.minute)\n            self.variable_manager.set('system_context.timestamp.second', timestamp.second)\n            self.variable_manager.set('system_context.timestamp.date', timestamp.strftime('%Y-%m-%d'))\n            self.variable_manager.set('system_context.timestamp.weekday', timestamp.strftime('%A'))\n            self.variable_manager.set('system_context.timestamp.kalenderwoche', timestamp.strftime('%W'))\n            self.variable_manager.set('system_context.current_session', session_id)\n            self.variable_manager.set('system_context.current_user', user_id)\n            self.variable_manager.set('system_context.last_query', query)\n\n            # Initialize with tool awareness\n            await self._initialize_context_awareness()\n\n            # Prepare execution context\n            self.shared.update({\n                \"current_query\": query,\n                \"session_id\": session_id,\n                \"user_id\": user_id,\n                \"stream_callback\": stream_callback\n            })\n            # Set LLM models in shared context\n            self.shared['fast_llm_model'] = self.amd.fast_llm_model\n            self.shared['complex_llm_model'] = self.amd.complex_llm_model\n            self.shared['persona_config'] = self.amd.persona\n            self.shared['use_fast_response'] = self.amd.use_fast_response\n            self.shared['variable_manager'] = self.variable_manager\n            # Add to conversation history\n            self.shared[\"conversation_history\"].append({\n                \"role\": \"user\",\n                \"content\": query,\n                \"timestamp\": datetime.now().isoformat()\n            })\n\n            # Set system status\n            self.shared[\"system_status\"] = \"running\"\n            self.is_running = True\n\n            # Execute main orchestration flow\n            result = await self._orchestrate_execution()\n            # Add response to history\n            self.shared[\"conversation_history\"].append({\n                \"role\": \"assistant\",\n                \"content\": result,\n                \"timestamp\": datetime.now().isoformat()\n            })\n\n            total_duration = self.progress_tracker.end_timer(\"total_execution\")\n\n            await self.progress_tracker.emit_event(ProgressEvent(\n                event_type=\"execution_complete\",\n                timestamp=time.time(),\n                node_name=\"FlowAgent\",\n                status=NodeStatus.COMPLETED,\n                node_duration=total_duration,\n                session_id=session_id,\n                metadata={\n                    \"result_length\": len(result),\n                    \"summary\": self.progress_tracker.get_summary()\n                }\n            ))\n            # Checkpoint if needed\n            if self.enable_pause_resume:\n                await self._maybe_checkpoint()\n\n            return result\n\n        except Exception as e:\n            logger.error(f\"Agent execution failed: {e}\", exc_info=True)\n            error_response = f\"I encountered an error: {str(e)}\"\n            import traceback\n            print(traceback.format_exc())\n\n            self.shared[\"conversation_history\"].append({\n                \"role\": \"assistant\",\n                \"content\": error_response,\n                \"timestamp\": datetime.now().isoformat()\n            })\n\n            total_duration = self.progress_tracker.end_timer(\"total_execution\")\n\n            await self.progress_tracker.emit_event(ProgressEvent(\n                event_type=\"error\",\n                timestamp=time.time(),\n                node_name=\"FlowAgent\",\n                status=NodeStatus.FAILED,\n                node_duration=total_duration,\n                session_id=session_id,\n                metadata={\"error\": str(e), \"error_type\": type(e).__name__}\n            ))\n\n            return error_response\n\n        finally:\n            self.shared[\"system_status\"] = \"idle\"\n            self.is_running = False\n\n    def set_response_format(\n        self,\n        response_format: str,\n        text_length: str,\n        custom_instructions: str = \"\",\n        quality_threshold: float = 0.7\n    ):\n        \"\"\"Dynamische Format- und L\u00e4ngen-Konfiguration\"\"\"\n\n        # Validiere Eingaben\n        try:\n            ResponseFormat(response_format)\n            TextLength(text_length)\n        except ValueError:\n            available_formats = [f.value for f in ResponseFormat]\n            available_lengths = [l.value for l in TextLength]\n            raise ValueError(\n                f\"Invalid format or length. \"\n                f\"Available formats: {available_formats}. \"\n                f\"Available lengths: {available_lengths}\"\n            )\n\n        # Erstelle oder aktualisiere Persona\n        if not self.amd.persona:\n            self.amd.persona = PersonaConfig(name=\"Assistant\")\n\n        # Erstelle Format-Konfiguration\n        format_config = FormatConfig(\n            response_format=ResponseFormat(response_format),\n            text_length=TextLength(text_length),\n            custom_instructions=custom_instructions,\n            quality_threshold=quality_threshold\n        )\n\n        self.amd.persona.format_config = format_config\n\n        # Aktualisiere Personality Traits mit Format-Hinweisen\n        self._update_persona_with_format(response_format, text_length)\n\n        # Update shared state\n        self.shared[\"persona_config\"] = self.amd.persona\n        self.shared[\"format_config\"] = format_config\n\n        logger.info(f\"Response format set: {response_format}, length: {text_length}\")\n\n    def _update_persona_with_format(self, response_format: str, text_length: str):\n        \"\"\"Aktualisiere Persona-Traits basierend auf Format\"\"\"\n\n        # Format-spezifische Traits\n        format_traits = {\n            \"with-tables\": [\"structured\", \"data-oriented\", \"analytical\"],\n            \"with-bullet-points\": [\"organized\", \"clear\", \"systematic\"],\n            \"with-lists\": [\"methodical\", \"sequential\", \"thorough\"],\n            \"md-text\": [\"technical\", \"formatted\", \"detailed\"],\n            \"yaml-text\": [\"structured\", \"machine-readable\", \"precise\"],\n            \"json-text\": [\"technical\", \"API-focused\", \"structured\"],\n            \"text-only\": [\"conversational\", \"natural\", \"flowing\"],\n            \"pseudo-code\": [\"logical\", \"algorithmic\", \"step-by-step\"],\n            \"code-structure\": [\"technical\", \"systematic\", \"hierarchical\"]\n        }\n\n        # L\u00e4ngen-spezifische Traits\n        length_traits = {\n            \"mini-chat\": [\"concise\", \"quick\", \"to-the-point\"],\n            \"chat-conversation\": [\"conversational\", \"friendly\", \"balanced\"],\n            \"table-conversation\": [\"structured\", \"comparative\", \"organized\"],\n            \"detailed-indepth\": [\"thorough\", \"comprehensive\", \"analytical\"],\n            \"phd-level\": [\"academic\", \"scholarly\", \"authoritative\"]\n        }\n\n        # Kombiniere Traits\n        current_traits = set(self.amd.persona.personality_traits)\n\n        # Entferne alte Format-Traits\n        old_format_traits = set()\n        for traits in format_traits.values():\n            old_format_traits.update(traits)\n        for traits in length_traits.values():\n            old_format_traits.update(traits)\n\n        current_traits -= old_format_traits\n\n        # F\u00fcge neue Traits hinzu\n        new_traits = format_traits.get(response_format, [])\n        new_traits.extend(length_traits.get(text_length, []))\n\n        current_traits.update(new_traits)\n        self.amd.persona.personality_traits = list(current_traits)\n\n    def get_available_formats(self) -&gt; Dict[str, List[str]]:\n        \"\"\"Erhalte verf\u00fcgbare Format- und L\u00e4ngen-Optionen\"\"\"\n        return {\n            \"formats\": [f.value for f in ResponseFormat],\n            \"lengths\": [l.value for l in TextLength],\n            \"format_descriptions\": {\n                f.value: FormatConfig(response_format=f).get_format_instructions()\n                for f in ResponseFormat\n            },\n            \"length_descriptions\": {\n                l.value: FormatConfig(text_length=l).get_length_instructions()\n                for l in TextLength\n            }\n        }\n\n    async def a_run_with_format(\n        self,\n        query: str,\n        response_format: str = \"frei-text\",\n        text_length: str = \"chat-conversation\",\n        custom_instructions: str = \"\",\n        **kwargs\n    ) -&gt; str:\n        \"\"\"F\u00fchre Agent mit spezifischem Format aus\"\"\"\n\n        # Tempor\u00e4re Format-Einstellung\n        original_persona = self.amd.persona\n\n        try:\n            self.set_response_format(response_format, text_length, custom_instructions)\n            response = await self.a_run(query, **kwargs)\n            return response\n        finally:\n            # Restore original persona\n            self.amd.persona = original_persona\n            self.shared[\"persona_config\"] = original_persona\n\n    def get_format_quality_report(self) -&gt; Dict[str, Any]:\n        \"\"\"Erhalte detaillierten Format-Qualit\u00e4tsbericht\"\"\"\n        quality_assessment = self.shared.get(\"quality_assessment\", {})\n\n        if not quality_assessment:\n            return {\"status\": \"no_assessment\", \"message\": \"No recent quality assessment available\"}\n\n        quality_details = quality_assessment.get(\"quality_details\", {})\n\n        return {\n            \"overall_score\": quality_details.get(\"total_score\", 0.0),\n            \"format_adherence\": quality_details.get(\"format_adherence\", 0.0),\n            \"length_adherence\": quality_details.get(\"length_adherence\", 0.0),\n            \"content_quality\": quality_details.get(\"base_quality\", 0.0),\n            \"llm_assessment\": quality_details.get(\"llm_assessment\", 0.0),\n            \"suggestions\": quality_assessment.get(\"suggestions\", []),\n            \"assessment\": quality_assessment.get(\"quality_assessment\", \"unknown\"),\n            \"format_config_active\": quality_details.get(\"format_config_used\", False)\n        }\n\n    def get_variable_documentation(self) -&gt; str:\n        \"\"\"Get comprehensive variable system documentation\"\"\"\n        docs = []\n        docs.append(\"# Variable System Documentation\\n\")\n\n        # Available scopes\n        docs.append(\"## Available Scopes:\")\n        scope_info = self.variable_manager.get_scope_info()\n        for scope_name, info in scope_info.items():\n            docs.append(f\"- `{scope_name}`: {info['type']} with {info.get('keys', 'N/A')} keys\")\n\n        docs.append(\"\\n## Syntax Options:\")\n        docs.append(\"- `{{ variable.path }}` - Full path resolution\")\n        docs.append(\"- `{variable}` - Simple variable (no dots)\")\n        docs.append(\"- `$variable` - Shell-style variable\")\n\n        docs.append(\"\\n## Example Usage:\")\n        docs.append(\"- `{{ results.task_1.data }}` - Get result from task_1\")\n        docs.append(\"- `{{ user.name }}` - Get user name\")\n        docs.append(\"- `{agent_name}` - Simple agent name\")\n        docs.append(\"- `$timestamp` - System timestamp\")\n\n        # Available variables\n        docs.append(\"\\n## Available Variables:\")\n        variables = self.variable_manager.get_available_variables()\n        for scope_name, scope_vars in variables.items():\n            docs.append(f\"\\n### {scope_name}:\")\n            for var_name, var_info in scope_vars.items():\n                docs.append(f\"- `{var_info['path']}`: {var_info['preview']} ({var_info['type']})\")\n\n        return \"\\n\".join(docs)\n\n    def _setup_variable_scopes(self):\n        \"\"\"Setup default variable scopes with enhanced structure\"\"\"\n        self.variable_manager.register_scope('agent', {\n            'name': self.amd.name,\n            'model_fast': self.amd.fast_llm_model,\n            'model_complex': self.amd.complex_llm_model\n        })\n\n        timestamp = datetime.now()\n        self.variable_manager.register_scope('system', {\n            'timestamp': timestamp.isoformat(),\n            'version': '2.0',\n            'capabilities': []\n        })\n\n        # ADDED: Initialize empty results and tasks scopes\n        self.variable_manager.register_scope('results', {})\n        self.variable_manager.register_scope('tasks', {})\n\n        # Update shared state\n        self.shared[\"variable_manager\"] = self.variable_manager\n\n    def set_variable(self, path: str, value: Any):\n        \"\"\"Set variable using unified system\"\"\"\n        self.variable_manager.set(path, value)\n\n    def get_variable(self, path: str, default=None):\n        \"\"\"Get variable using unified system\"\"\"\n        return self.variable_manager.get(path, default)\n\n    def format_text(self, text: str, **context) -&gt; str:\n        \"\"\"Format text with variables\"\"\"\n        return self.variable_manager.format_text(text, context)\n\n    async def initialize_session_context(self, session_id: str = \"default\", max_history: int = 200) -&gt; bool:\n        \"\"\"Initialize session-aware context management for infinite scaling\"\"\"\n        try:\n            from toolboxv2 import get_app\n            from toolboxv2.mods.isaa.extras.session import ChatSession\n\n            # Initialize memory system\n            memory_instance = get_app().get_mod(\"isaa\").get_memory()\n\n            # Create multiple session managers for different aspects\n            session_managers = {\n                \"main_conversation\": ChatSession(\n                    memory_instance,\n                    space_name=f\"ChatSession/{self.amd.name}.{session_id}.conversation\",\n                    max_length=max_history\n                ),\n                \"task_history\": ChatSession(\n                    memory_instance,\n                    space_name=f\"ChatSession/{self.amd.name}.{session_id}.tasks\",\n                    max_length=max_history // 2\n                ),\n                \"insights\": ChatSession(\n                    memory_instance,\n                    space_name=f\"ChatSession/{self.amd.name}.{session_id}.insights\",\n                    max_length=100\n                )\n            }\n\n            # Store in shared state\n            self.shared[\"session_managers\"] = session_managers\n            self.shared[\"session_initialized\"] = True\n\n            logger.info(f\"Session context initialized for {session_id}\")\n            return True\n\n        except Exception as e:\n            logger.error(f\"Session context initialization failed: {e}\")\n            return False\n\n    async def _initialize_context_awareness(self):\n        \"\"\"Enhanced context awareness with session management\"\"\"\n\n        # Initialize session if not already done\n        session_id = self.shared.get(\"session_id\", \"default\")\n        if not self.shared.get(\"session_initialized\"):\n            await self.initialize_session_context(session_id)\n\n        # Replace context node in flows with advanced version\n        advanced_context_manager = ContextManagerNode(\n            max_tokens=self.amd.max_input_tokens,\n            compression_threshold=0.76\n        )\n\n        # Update task flow to use advanced context manager\n        if hasattr(self.task_flow, 'strategy_node'):\n            # Insert context manager before strategy\n            self.task_flow.context_manager = advanced_context_manager\n            advanced_context_manager &gt;&gt; self.task_flow.strategy_node\n            self.task_flow.start = advanced_context_manager\n\n        # Ensure tool capabilities are loaded\n        # add tqdm prigress bar\n        from tqdm import tqdm\n\n        for tool_name in tqdm(self.shared[\"available_tools\"], desc=\"Analyzing Tools\", unit=\"tool\", colour=\"green\", total=len(self.shared[\"available_tools\"])):\n            if tool_name not in self._tool_capabilities:\n                tool_info = self._tool_registry.get(tool_name, {})\n                description = tool_info.get(\"description\", \"No description\")\n                await self._analyze_tool_capabilities(tool_name, description)\n\n            if tool_name in self._tool_capabilities and self._tool_capabilities[tool_name].get(\"args_schema\") is None:\n                function = self._tool_registry[tool_name][\"function\"]\n                self._tool_capabilities[tool_name][\"args_schema\"] = get_args_schema(function)\n\n        # Set enhanced system context\n        self.shared[\"system_context\"] = {\n            \"capabilities_summary\": self._build_capabilities_summary(),\n            \"tool_count\": len(self.shared[\"available_tools\"]),\n            \"analysis_loaded\": len(self._tool_capabilities),\n            \"intelligence_level\": \"high\" if self._tool_capabilities else \"basic\",\n            \"context_management\": \"advanced_session_aware\",\n            \"session_managers\": len(self.shared.get(\"session_managers\", {})),\n        }\n\n        logger.info(f\"Advanced context awareness initialized with session management\")\n\n    def get_context_statistics(self) -&gt; Dict[str, Any]:\n        \"\"\"Get comprehensive context management statistics\"\"\"\n        stats = {\n            \"context_system\": \"advanced_session_aware\",\n            \"compression_threshold\": 0.76,\n            \"max_tokens\": getattr(self, 'max_input_tokens', 8000),\n            \"session_managers\": {},\n            \"context_usage\": {},\n            \"compression_stats\": {}\n        }\n\n        # Session manager statistics\n        session_managers = self.shared.get(\"session_managers\", {})\n        for name, manager in session_managers.items():\n            stats[\"session_managers\"][name] = {\n                \"history_length\": len(manager.history),\n                \"max_length\": manager.max_length,\n                \"space_name\": manager.space_name\n            }\n\n        # Context node statistics if available\n        if hasattr(self.task_flow, 'context_manager'):\n            context_manager = self.task_flow.context_manager\n            stats[\"compression_stats\"] = {\n                \"compression_threshold\": context_manager.compression_threshold,\n                \"max_tokens\": context_manager.max_tokens,\n                \"active_sessions\": len(context_manager.session_managers)\n            }\n\n        # LLM call statistics from enhanced node\n        llm_stats = self.shared.get(\"llm_call_stats\", {})\n        if llm_stats:\n            stats[\"context_usage\"] = {\n                \"total_llm_calls\": llm_stats.get(\"total_calls\", 0),\n                \"context_compression_rate\": llm_stats.get(\"context_compression_rate\", 0.0),\n                \"average_context_tokens\": llm_stats.get(\"context_tokens_used\", 0) / max(llm_stats.get(\"total_calls\", 1),\n                                                                                        1)\n            }\n\n        return stats\n\n    def set_persona(self, name: str, style: str = \"professional\", tone: str = \"friendly\",\n                    personality_traits: List[str] = None, apply_method: str = \"system_prompt\",\n                    integration_level: str = \"light\", custom_instructions: str = \"\"):\n        \"\"\"Set agent persona mit erweiterten Konfigurationsm\u00f6glichkeiten\"\"\"\n        if personality_traits is None:\n            personality_traits = [\"helpful\", \"concise\"]\n\n        self.amd.persona = PersonaConfig(\n            name=name,\n            style=style,\n            tone=tone,\n            personality_traits=personality_traits,\n            custom_instructions=custom_instructions,\n            apply_method=apply_method,\n            integration_level=integration_level\n        )\n\n        logger.info(f\"Persona set: {name} ({style}, {tone}) - Method: {apply_method}, Level: {integration_level}\")\n\n    def configure_persona_integration(self, apply_method: str = \"system_prompt\", integration_level: str = \"light\"):\n        \"\"\"Configure how persona is applied\"\"\"\n        if self.amd.persona:\n            self.amd.persona.apply_method = apply_method\n            self.amd.persona.integration_level = integration_level\n            logger.info(f\"Persona integration updated: {apply_method}, {integration_level}\")\n        else:\n            logger.warning(\"No persona configured to update\")\n\n    def get_available_variables(self) -&gt; Dict[str, str]:\n        \"\"\"Get available variables for dynamic formatting\"\"\"\n        return self.variable_manager.get_available_variables()\n\n    async def _orchestrate_execution(self) -&gt; str:\n        \"\"\"Vollst\u00e4ndig adaptive Orchestrierung mit separaten Phasen\"\"\"\n\n        self.shared[\"agent_instance\"] = self\n\n        # === PHASE 1: TASK MANAGEMENT CYCLE ===\n        logger.info(\"Starting adaptive task management cycle\")\n\n        # F\u00fchre Task-Management-Flow aus (adaptiv mit Reflexion)\n        task_management_result = await self.task_flow.run_async(self.shared)\n\n        if self.shared.get(\"plan_halted\"):\n            error_response = f\"Task execution was halted: {self.shared.get('halt_reason', 'Unknown reason')}\"\n            self.shared[\"current_response\"] = error_response\n            return error_response\n\n        # === PHASE 2: RESPONSE GENERATION ===\n        logger.info(\"Starting response generation flow\")\n\n        # F\u00fchre Response-Generation-Flow aus\n        response_result = await self.response_flow.run_async(self.shared)\n\n        # === PHASE 3: FINAL RESULT ===\n        final_response = self.shared.get(\"current_response\", \"Task completed successfully.\")\n\n        # Logge Statistiken\n        self._log_execution_stats()\n\n        return final_response\n\n    def _log_execution_stats(self):\n        \"\"\"Logge Ausf\u00fchrungsstatistiken\"\"\"\n        tasks = self.shared.get(\"tasks\", {})\n        adaptations = self.shared.get(\"plan_adaptations\", 0)\n\n        completed_tasks = sum(1 for t in tasks.values() if t.status == \"completed\")\n        failed_tasks = sum(1 for t in tasks.values() if t.status == \"failed\")\n\n        logger.info(\n            f\"Execution complete - Tasks: {completed_tasks} completed, {failed_tasks} failed, {adaptations} adaptations\")\n\n    def _build_capabilities_summary(self) -&gt; str:\n        \"\"\"Build summary of agent capabilities\"\"\"\n\n        if not self._tool_capabilities:\n            return \"Basic LLM capabilities only\"\n\n        summaries = []\n        for tool_name, cap in self._tool_capabilities.items():\n            primary = cap.get('primary_function', 'Unknown function')\n            summaries.append(f\"{tool_name}{cap.get('args_schema', '()')}: {primary}\")\n\n        return f\"Enhanced capabilities: {'; '.join(summaries)}\"\n\n    # Neue Hilfsmethoden f\u00fcr erweiterte Funktionalit\u00e4t\n\n    async def get_task_execution_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Erhalte detaillierte Zusammenfassung der Task-Ausf\u00fchrung\"\"\"\n        tasks = self.shared.get(\"tasks\", {})\n        results_store = self.shared.get(\"results_store\", {})\n\n        summary = {\n            \"total_tasks\": len(tasks),\n            \"completed_tasks\": [],\n            \"failed_tasks\": [],\n            \"task_types_used\": {},\n            \"tools_used\": [],\n            \"adaptations\": self.shared.get(\"plan_adaptations\", 0),\n            \"execution_timeline\": []\n        }\n\n        for task_id, task in tasks.items():\n            task_info = {\n                \"id\": task_id,\n                \"type\": task.type,\n                \"description\": task.description,\n                \"status\": task.status,\n                \"duration\": None\n            }\n\n            if task.started_at and task.completed_at:\n                duration = (task.completed_at - task.started_at).total_seconds()\n                task_info[\"duration\"] = duration\n\n            if task.status == \"completed\":\n                summary[\"completed_tasks\"].append(task_info)\n                if isinstance(task, ToolTask):\n                    summary[\"tools_used\"].append(task.tool_name)\n            elif task.status == \"failed\":\n                task_info[\"error\"] = task.error\n                summary[\"failed_tasks\"].append(task_info)\n\n            # Task types counting\n            task_type = task.type\n            summary[\"task_types_used\"][task_type] = summary[\"task_types_used\"].get(task_type, 0) + 1\n\n        return summary\n\n    async def explain_reasoning_process(self) -&gt; str:\n        \"\"\"Erkl\u00e4re den Reasoning-Prozess des Agenten\"\"\"\n        if not LITELLM_AVAILABLE:\n            return \"Reasoning explanation requires LLM capabilities.\"\n\n        summary = await self.get_task_execution_summary()\n\n        prompt = f\"\"\"\nErkl\u00e4re den Reasoning-Prozess dieses AI-Agenten in verst\u00e4ndlicher Form:\n\n## Ausf\u00fchrungszusammenfassung\n- Total Tasks: {summary['total_tasks']}\n- Erfolgreich: {len(summary['completed_tasks'])}\n- Fehlgeschlagen: {len(summary['failed_tasks'])}\n- Plan-Adaptationen: {summary['adaptations']}\n- Verwendete Tools: {', '.join(set(summary['tools_used']))}\n- Task-Typen: {summary['task_types_used']}\n\n## Task-Details\nErfolgreiche Tasks:\n{self._format_tasks_for_explanation(summary['completed_tasks'])}\n\n## Anweisungen\nErkl\u00e4re in 2-3 Abs\u00e4tzen:\n1. Welche Strategie der Agent gew\u00e4hlt hat\n2. Wie er die Aufgabe in Tasks unterteilt hat\n3. Wie er auf unerwartete Ergebnisse reagiert hat (falls Adaptationen)\n4. Was die wichtigsten Erkenntnisse waren\n\nSchreibe f\u00fcr einen technischen Nutzer, aber verst\u00e4ndlich.\"\"\"\n\n        try:\n            response = await self.a_run_llm_completion(\n                model=self.amd.complex_llm_model,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.5,\n                max_tokens=800,task_id=\"reasoning_explanation\"\n            )\n\n            return response\n\n        except Exception as e:\n            return f\"Could not generate reasoning explanation: {e}\"\n\n    def _format_tasks_for_explanation(self, tasks: List[Dict]) -&gt; str:\n        formatted = []\n        for task in tasks[:5]:  # Top 5 tasks\n            duration_info = f\" ({task['duration']:.1f}s)\" if task['duration'] else \"\"\n            formatted.append(f\"- {task['type']}: {task['description']}{duration_info}\")\n        return \"\\n\".join(formatted)\n\n    # ===== PAUSE/RESUME FUNCTIONALITY =====\n\n    async def pause(self) -&gt; bool:\n        \"\"\"Pause agent execution\"\"\"\n        if not self.is_running:\n            return False\n\n        self.is_paused = True\n        self.shared[\"system_status\"] = \"paused\"\n\n        # Create checkpoint\n        checkpoint = await self._create_checkpoint()\n        await self._save_checkpoint(checkpoint)\n\n        logger.info(\"Agent execution paused\")\n        return True\n\n    async def resume(self) -&gt; bool:\n        \"\"\"Resume agent execution\"\"\"\n        if not self.is_paused:\n            return False\n\n        self.is_paused = False\n        self.shared[\"system_status\"] = \"running\"\n\n        logger.info(\"Agent execution resumed\")\n        return True\n\n    async def _create_checkpoint(self) -&gt; AgentCheckpoint:\n        \"\"\"Create a checkpoint of current state\"\"\"\n        self.amd.budget_manager.save_data()\n        amd = self.amd.model_dump()\n        amd['budget_manager'] = None  # Exclude budget manager from checkpoint\n        return AgentCheckpoint(\n            timestamp=datetime.now(),\n            agent_state={\n                \"is_running\": self.is_running,\n                \"is_paused\": self.is_paused,\n                \"amd\": json.dumps(amd, ensure_ascii=False, default=str, indent=2)\n            },\n            task_state={\n                task_id: asdict(task) for task_id, task in self.shared.get(\"tasks\", {}).items()\n            },\n            world_model=self.shared[\"world_model\"].copy(),\n            active_flows=[\"task_flow\", \"response_flow\"],  # Track active flows\n            metadata={\n                \"session_id\": self.shared.get(\"session_id\", \"default\"),\n                \"last_query\": self.shared.get(\"current_query\", \"\")\n            }\n        )\n\n    async def _save_checkpoint(self, checkpoint: AgentCheckpoint, filepath: str = None):\n        \"\"\"Save checkpoint to file\"\"\"\n        from toolboxv2 import get_app\n        folder = str(get_app().data_dir) + '/Agents/checkpoint/' + self.amd.name\n        if not os.path.exists(folder):\n            os.makedirs(folder, exist_ok=True)\n        if not filepath:\n            timestamp = checkpoint.timestamp.strftime(\"%Y%m%d_%H%M%S\")\n            filepath = f\"agent_checkpoint_{timestamp}.pkl\"\n        filepath = folder + '/' + filepath\n        try:\n            with open(filepath, 'wb') as f:\n                pickle.dump(checkpoint, f)\n\n            self.last_checkpoint = checkpoint.timestamp\n            logger.info(f\"Checkpoint saved: {filepath}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to save checkpoint: {e}\")\n\n    async def load_checkpoint(self, filepath: str) -&gt; bool:\n        \"\"\"Load checkpoint from file\"\"\"\n        try:\n            with open(filepath, 'rb') as f:\n                checkpoint: AgentCheckpoint = pickle.load(f)\n\n            # Restore state\n            self.shared[\"world_model\"] = checkpoint.world_model\n            self.shared[\"tasks\"] = {\n                task_id: Task(**task_data)\n                for task_id, task_data in checkpoint.task_state.items()\n            }\n\n            # Restore agent state\n            agent_state = checkpoint.agent_state\n            self.is_running = agent_state.get(\"is_running\", False)\n            self.is_paused = agent_state.get(\"is_paused\", False)\n\n            self.last_checkpoint = checkpoint.timestamp\n            logger.info(f\"Checkpoint loaded: {filepath}\")\n\n            return True\n\n        except Exception as e:\n            logger.error(f\"Failed to load checkpoint: {e}\")\n            return False\n\n    async def _maybe_checkpoint(self):\n        \"\"\"Create checkpoint if interval has passed\"\"\"\n        now = datetime.now()\n        if (not self.last_checkpoint or\n            (now - self.last_checkpoint).seconds &gt;= self.checkpoint_interval):\n\n            checkpoint = await self._create_checkpoint()\n            await self._save_checkpoint(checkpoint)\n\n    # ===== TOOL AND NODE MANAGEMENT =====\n    def _get_tool_analysis_path(self) -&gt; str:\n        \"\"\"Get path for tool analysis cache\"\"\"\n        from toolboxv2 import get_app\n        folder = str(get_app().data_dir) + '/Agents/capabilities/' + self.amd.name\n        os.makedirs(folder, exist_ok=True)\n        return folder + '/tool_capabilities.json'\n\n    async def add_tool(self, tool_func: Callable, name: str = None, description: str = None, is_new=False):\n        \"\"\"Enhanced tool addition with intelligent analysis\"\"\"\n        if not asyncio.iscoroutinefunction(tool_func):\n            @wraps(tool_func)\n            async def async_wrapper(*args, **kwargs):\n                return await asyncio.to_thread(tool_func, *args, **kwargs)\n\n            effective_func = async_wrapper\n        else:\n            effective_func = tool_func\n\n        tool_name = name or effective_func.__name__\n        tool_description = description or effective_func.__doc__ or \"No description\"\n\n        # Store in registry\n        self._tool_registry[tool_name] = {\n            \"function\": effective_func,\n            \"description\": tool_description\n        }\n\n        # Add to available tools list\n        if tool_name not in self.shared[\"available_tools\"]:\n            self.shared[\"available_tools\"].append(tool_name)\n\n        # Intelligent tool analysis\n        if is_new:\n            await self._analyze_tool_capabilities(tool_name, tool_description)\n\n        logger.info(f\"Tool added with analysis: {tool_name}\")\n\n    async def _analyze_tool_capabilities(self, tool_name: str, description: str):\n        \"\"\"Analyze tool capabilities with LLM for smart usage\"\"\"\n\n        # Try to load existing analysis\n        existing_analysis = self._load_tool_analysis()\n\n        if tool_name in existing_analysis:\n            try:\n                # Validate cached data against the Pydantic model\n                ToolAnalysis.model_validate(existing_analysis[tool_name])\n                self._tool_capabilities[tool_name] = existing_analysis[tool_name]\n                logger.info(f\"Loaded and validated cached analysis for {tool_name}\")\n            except ValidationError as e:\n                logger.warning(f\"Cached data for {tool_name} is invalid and will be regenerated: {e}\")\n                del self._tool_capabilities[tool_name]\n\n        if not LITELLM_AVAILABLE:\n            # Fallback analysis\n            self._tool_capabilities[tool_name] = {\n                \"use_cases\": [description],\n                \"triggers\": [tool_name.lower().replace('_', ' ')],\n                \"complexity\": \"unknown\",\n                \"confidence\": 0.3\n            }\n            return\n\n        # LLM-based intelligent analysis\n        prompt = f\"\"\"\nAnalyze this tool and identify ALL possible use cases, triggers, and connections:\n\nTool Name: {tool_name}\nDescription: {description}\n\nProvide a comprehensive analysis covering:\n\n1. OBVIOUS use cases (direct functionality)\n2. INDIRECT connections (when this tool might be relevant)\n3. TRIGGER PHRASES (what user queries would benefit from this tool)\n4. COMPLEX scenarios (non-obvious applications)\n5. CONTEXTUAL usage (when combined with other information)\n\nExample for a \"get_user_name\" tool:\n- Obvious: When user asks \"what is my name\"\n- Indirect: Personalization, greetings, user identification\n- Triggers: \"my name\", \"who am I\", \"hello\", \"introduce yourself\", \"personalize\"\n- Complex: User context in multi-step tasks, addressing user directly\n- Contextual: Any response that could be personalized\n\nRespond in YAML format:\nExample:\n```yaml\nprimary_function: \"Retrieves the current user's name.\"\nuse_cases:\n  - \"Responding to 'what is my name?'\"\n  - \"Personalizing greeting messages.\"\ntrigger_phrases:\n  - \"my name\"\n  - \"who am I\"\n  - \"introduce yourself\"\nindirect_connections:\n  - \"User identification in multi-factor authentication.\"\n  - \"Tagging user-generated content.\"\ncomplexity_scenarios:\n  - \"In a multi-step task, remembering the user's name to personalize the final output.\"\nuser_intent_categories:\n  - \"Personalization\"\n  - \"User Identification\"\nconfidence_triggers:\n  \"my name\": 0.95\n  \"who am I\": 0.9\ntool_complexity: low/medium/high\n```\n\"\"\"\n\n        for i in range(3):\n            try:\n                response = await self.a_run_llm_completion(\n                    model=self.amd.complex_llm_model,\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                    temperature=0.3,\n                    max_tokens=1000,\n                    task_id=f\"tool_analysis_{tool_name}\"\n                )\n\n                content = response.strip()\n\n                # Extract JSON\n                if \"```yaml\" in content:\n                    yaml_str = content.split(\"```yaml\")[1].split(\"```\")[0].strip()\n                else:\n                    yaml_str = content\n\n                analysis = yaml.safe_load(yaml_str)\n\n                # Store analysis\n                self._tool_capabilities[tool_name] = analysis\n\n                # Save to cache\n                await self._save_tool_analysis()\n\n                validated_analysis = ToolAnalysis.model_validate(analysis)\n                logger.info(f\"Generated intelligent analysis for {tool_name}\")\n                break\n\n            except Exception as e:\n                logger.error(f\"Tool analysis failed for {tool_name}: {e}\")\n                # Fallback\n                self._tool_capabilities[tool_name] = {\n                    \"primary_function\": description,\n                    \"use_cases\": [description],\n                    \"trigger_phrases\": [tool_name.lower().replace('_', ' ')],\n                    \"tool_complexity\": \"medium\"\n                }\n\n    def _load_tool_analysis(self) -&gt; Dict[str, Any]:\n        \"\"\"Load tool analysis from cache\"\"\"\n        try:\n            if os.path.exists(self.tool_analysis_file):\n                with open(self.tool_analysis_file, 'r') as f:\n                    return json.load(f)\n        except Exception as e:\n            logger.warning(f\"Could not load tool analysis: {e}\")\n        return {}\n\n    async def _save_tool_analysis(self):\n        \"\"\"Save tool analysis to cache\"\"\"\n        try:\n            with open(self.tool_analysis_file, 'w') as f:\n                json.dump(self._tool_capabilities, f, indent=2)\n        except Exception as e:\n            logger.error(f\"Could not save tool analysis: {e}\")\n\n    def add_custom_flow(self, flow: AsyncFlow, name: str):\n        \"\"\"Add a custom flow for dynamic execution\"\"\"\n        self.add_tool(flow.run_async, name=name, description=f\"Custom flow: {flow.__class__.__name__}\")\n        logger.info(f\"Custom node added: {name}\")\n\n    def get_tool_by_name(self, tool_name: str) -&gt; Callable | None:\n        \"\"\"Get tool function by name\"\"\"\n        return self._tool_registry.get(tool_name, {}).get(\"function\")\n\n    async def arun_function(self, function_name: str, *args, **kwargs) -&gt; Any:\n        \"\"\"\n        Asynchronously finds a function by its string name, executes it with\n        the given arguments, and returns the result.\n        \"\"\"\n        logger.info(f\"Attempting to run function: {function_name} with args: {args}, kwargs: {kwargs}\")\n        target_function = self.get_tool_by_name(function_name)\n\n        if not target_function:\n            raise ValueError(f\"Function '{function_name}' not found in the agent's registered tools.\")\n\n        try:\n            if asyncio.iscoroutinefunction(target_function):\n                result = await target_function(*args, **kwargs)\n            else:\n                # If the function is not async, run it in a thread pool\n                loop = asyncio.get_running_loop()\n                result = await loop.run_in_executor(None, lambda: target_function(*args, **kwargs))\n\n            logger.info(f\"Function {function_name} completed successfully with result: {result}\")\n            return result\n\n        except Exception as e:\n            logger.error(f\"Function {function_name} execution failed: {e}\")\n            raise\n    async def execute_custom_node(self, node_name: str, **kwargs) -&gt; Any:\n        \"\"\"Execute a custom node dynamically\"\"\"\n        if not hasattr(self, '_node_registry') or node_name not in self._node_registry:\n            raise ValueError(f\"Node '{node_name}' not found\")\n\n        node = self._node_registry[node_name]\n\n        # Create temporary shared state with kwargs\n        temp_shared = self.shared.copy()\n        temp_shared.update(kwargs)\n\n        # Execute the node\n        result = await node.run_async(temp_shared)\n\n        # Merge back any changes\n        self.shared.update(temp_shared)\n\n        return result\n\n    # ===== SERVER SETUP =====\n\n    def setup_a2a_server(self, host: str = \"0.0.0.0\", port: int = 5000, **kwargs):\n        \"\"\"Setup A2A server for bidirectional communication\"\"\"\n        if not A2A_AVAILABLE:\n            logger.warning(\"A2A not available, cannot setup server\")\n            return\n\n        try:\n            self.a2a_server = A2AServer(\n                host=host,\n                port=port,\n                agent_card=AgentCard(\n                    name=self.amd.name,\n                    description=\"Production-ready PocketFlow agent\",\n                    version=\"1.0.0\"\n                ),\n                **kwargs\n            )\n\n            # Register agent methods\n            @self.a2a_server.route(\"/run\")\n            async def handle_run(request_data):\n                query = request_data.get(\"query\", \"\")\n                session_id = request_data.get(\"session_id\", \"a2a_session\")\n\n                response = await self.a_run(query, session_id=session_id)\n                return {\"response\": response}\n\n            logger.info(f\"A2A server setup on {host}:{port}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to setup A2A server: {e}\")\n\n    def setup_mcp_server(self, host: str = \"0.0.0.0\", port: int = 8000, name: str = None, **kwargs):\n        \"\"\"Setup MCP server\"\"\"\n        if not MCP_AVAILABLE:\n            logger.warning(\"MCP not available, cannot setup server\")\n            return\n\n        try:\n            server_name = name or f\"{self.amd.name}_MCP\"\n            self.mcp_server = FastMCP(server_name)\n\n            # Register agent as MCP tool\n            @self.mcp_server.tool()\n            async def agent_run(query: str, session_id: str = \"mcp_session\") -&gt; str:\n                \"\"\"Execute agent with given query\"\"\"\n                return await self.a_run(query, session_id=session_id)\n\n            logger.info(f\"MCP server setup: {server_name}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to setup MCP server: {e}\")\n\n    # ===== LIFECYCLE MANAGEMENT =====\n\n    async def start_servers(self):\n        \"\"\"Start all configured servers\"\"\"\n        tasks = []\n\n        if self.a2a_server:\n            tasks.append(asyncio.create_task(self.a2a_server.start()))\n\n        if self.mcp_server:\n            tasks.append(asyncio.create_task(self.mcp_server.run()))\n\n        if tasks:\n            logger.info(f\"Starting {len(tasks)} servers...\")\n            await asyncio.gather(*tasks, return_exceptions=True)\n\n    async def close(self):\n        \"\"\"Clean shutdown\"\"\"\n        self.is_running = False\n        self._shutdown_event.set()\n\n        # Create final checkpoint\n        if self.enable_pause_resume:\n            checkpoint = await self._create_checkpoint()\n            await self._save_checkpoint(checkpoint, \"final_checkpoint.pkl\")\n\n        # Shutdown executor\n        self.executor.shutdown(wait=True)\n\n        # Close servers\n        if self.a2a_server:\n            await self.a2a_server.close()\n\n        if self.mcp_server:\n            await self.mcp_server.close()\n\n        logger.info(\"Agent shutdown complete\")\n\n    @property\n    def total_cost(self) -&gt; float:\n        \"\"\"Get total cost if budget manager available\"\"\"\n        if hasattr(self.amd, 'budget_manager') and self.amd.budget_manager:\n            return getattr(self.amd.budget_manager, 'total_cost', 0.0)\n        return 0.0\n\n    def status(self, pretty_print: bool = False) -&gt; Union[Dict[str, Any], str]:\n        \"\"\"Get comprehensive agent status with optional pretty printing\"\"\"\n\n        # Core status information\n        base_status = {\n            \"agent_info\": {\n                \"name\": self.amd.name,\n                \"version\": \"2.0\",\n                \"type\": \"FlowAgent\"\n            },\n            \"runtime_status\": {\n                \"status\": self.shared.get(\"system_status\", \"idle\"),\n                \"is_running\": self.is_running,\n                \"is_paused\": self.is_paused,\n                \"uptime_seconds\": (datetime.now() - getattr(self, '_start_time', datetime.now())).total_seconds()\n            },\n            \"task_execution\": {\n                \"total_tasks\": len(self.shared.get(\"tasks\", {})),\n                \"active_tasks\": len([t for t in self.shared.get(\"tasks\", {}).values() if t.status == \"running\"]),\n                \"completed_tasks\": len([t for t in self.shared.get(\"tasks\", {}).values() if t.status == \"completed\"]),\n                \"failed_tasks\": len([t for t in self.shared.get(\"tasks\", {}).values() if t.status == \"failed\"]),\n                \"plan_adaptations\": self.shared.get(\"plan_adaptations\", 0)\n            },\n            \"conversation\": {\n                \"turns\": len(self.shared.get(\"conversation_history\", [])),\n                \"session_id\": self.shared.get(\"session_id\", \"default\"),\n                \"current_user\": self.shared.get(\"user_id\"),\n                \"last_query\": self.shared.get(\"current_query\", \"\")[:100] + \"...\" if len(\n                    self.shared.get(\"current_query\", \"\")) &gt; 100 else self.shared.get(\"current_query\", \"\")\n            },\n            \"capabilities\": {\n                \"available_tools\": len(self.shared.get(\"available_tools\", [])),\n                \"tool_names\": list(self.shared.get(\"available_tools\", [])),\n                \"analyzed_tools\": len(self._tool_capabilities),\n                \"world_model_size\": len(self.shared.get(\"world_model\", {})),\n                \"intelligence_level\": \"high\" if self._tool_capabilities else \"basic\"\n            },\n            \"memory_context\": {\n                \"session_initialized\": self.shared.get(\"session_initialized\", False),\n                \"session_managers\": len(self.shared.get(\"session_managers\", {})),\n                \"context_system\": \"advanced_session_aware\" if self.shared.get(\"session_initialized\") else \"basic\",\n                \"variable_scopes\": len(self.variable_manager.get_scope_info()) if hasattr(self,\n                                                                                          'variable_manager') else 0\n            },\n            \"performance\": {\n                \"total_cost\": self.total_cost,\n                \"checkpoint_enabled\": self.enable_pause_resume,\n                \"last_checkpoint\": self.last_checkpoint.isoformat() if self.last_checkpoint else None,\n                \"max_parallel_tasks\": self.max_parallel_tasks\n            },\n            \"servers\": {\n                \"a2a_server\": self.a2a_server is not None,\n                \"mcp_server\": self.mcp_server is not None,\n                \"server_count\": sum([self.a2a_server is not None, self.mcp_server is not None])\n            },\n            \"configuration\": {\n                \"fast_llm_model\": self.amd.fast_llm_model,\n                \"complex_llm_model\": self.amd.complex_llm_model,\n                \"use_fast_response\": getattr(self.amd, 'use_fast_response', False),\n                \"max_input_tokens\": getattr(self.amd, 'max_input_tokens', 8000),\n                \"persona_configured\": self.amd.persona is not None,\n                \"format_config\": bool(getattr(self.amd.persona, 'format_config', None)) if self.amd.persona else False\n            }\n        }\n\n        # Add detailed execution summary if tasks exist\n        tasks = self.shared.get(\"tasks\", {})\n        if tasks:\n            task_types_used = {}\n            tools_used = []\n            execution_timeline = []\n\n            for task_id, task in tasks.items():\n                # Count task types\n                task_type = getattr(task, 'type', 'unknown')\n                task_types_used[task_type] = task_types_used.get(task_type, 0) + 1\n\n                # Collect tools used\n                if hasattr(task, 'tool_name') and task.tool_name:\n                    tools_used.append(task.tool_name)\n\n                # Timeline info\n                if hasattr(task, 'started_at') and task.started_at:\n                    timeline_entry = {\n                        \"task_id\": task_id,\n                        \"type\": task_type,\n                        \"started\": task.started_at.isoformat(),\n                        \"status\": getattr(task, 'status', 'unknown')\n                    }\n                    if hasattr(task, 'completed_at') and task.completed_at:\n                        timeline_entry[\"completed\"] = task.completed_at.isoformat()\n                        timeline_entry[\"duration\"] = (task.completed_at - task.started_at).total_seconds()\n                    execution_timeline.append(timeline_entry)\n\n            base_status[\"task_execution\"].update({\n                \"task_types_used\": task_types_used,\n                \"tools_used\": list(set(tools_used)),\n                \"execution_timeline\": execution_timeline[-5:]  # Last 5 tasks\n            })\n\n        # Add context statistics\n        if hasattr(self.task_flow, 'context_manager'):\n            context_manager = self.task_flow.context_manager\n            base_status[\"memory_context\"].update({\n                \"compression_threshold\": context_manager.compression_threshold,\n                \"max_tokens\": context_manager.max_tokens,\n                \"active_context_sessions\": len(getattr(context_manager, 'session_managers', {}))\n            })\n\n        # Add variable system info\n        if hasattr(self, 'variable_manager'):\n            available_vars = self.variable_manager.get_available_variables()\n            scope_info = self.variable_manager.get_scope_info()\n\n            base_status[\"variable_system\"] = {\n                \"total_scopes\": len(scope_info),\n                \"scope_names\": list(scope_info.keys()),\n                \"total_variables\": sum(len(vars) for vars in available_vars.values()),\n                \"scope_details\": {\n                    scope: {\"type\": info[\"type\"], \"variables\": len(available_vars.get(scope, {}))}\n                    for scope, info in scope_info.items()\n                }\n            }\n\n        # Add format quality info if available\n        quality_assessment = self.shared.get(\"quality_assessment\", {})\n        if quality_assessment:\n            quality_details = quality_assessment.get(\"quality_details\", {})\n            base_status[\"format_quality\"] = {\n                \"overall_score\": quality_details.get(\"total_score\", 0.0),\n                \"format_adherence\": quality_details.get(\"format_adherence\", 0.0),\n                \"length_adherence\": quality_details.get(\"length_adherence\", 0.0),\n                \"content_quality\": quality_details.get(\"base_quality\", 0.0),\n                \"assessment\": quality_assessment.get(\"quality_assessment\", \"unknown\"),\n                \"has_suggestions\": bool(quality_assessment.get(\"suggestions\", []))\n            }\n\n        # Add LLM usage statistics\n        llm_stats = self.shared.get(\"llm_call_stats\", {})\n        if llm_stats:\n            base_status[\"llm_usage\"] = {\n                \"total_calls\": llm_stats.get(\"total_calls\", 0),\n                \"context_compression_rate\": llm_stats.get(\"context_compression_rate\", 0.0),\n                \"average_context_tokens\": llm_stats.get(\"context_tokens_used\", 0) / max(llm_stats.get(\"total_calls\", 1),\n                                                                                        1),\n                \"total_tokens_used\": llm_stats.get(\"total_tokens_used\", 0)\n            }\n\n        # Add timestamp\n        base_status[\"timestamp\"] = datetime.now().isoformat()\n\n        if not pretty_print:\n            return base_status\n\n        # Pretty print using EnhancedVerboseOutput\n        try:\n            from toolboxv2.mods.isaa.CodingAgent.live import EnhancedVerboseOutput\n            verbose_output = EnhancedVerboseOutput(verbose=True)\n\n            # Header\n            verbose_output.log_header(f\"Agent Status: {base_status['agent_info']['name']}\")\n\n            # Runtime Status\n            status_color = {\n                \"running\": \"SUCCESS\",\n                \"paused\": \"WARNING\",\n                \"idle\": \"INFO\",\n                \"error\": \"ERROR\"\n            }.get(base_status[\"runtime_status\"][\"status\"], \"INFO\")\n\n            getattr(verbose_output, f\"print_{status_color.lower()}\")(\n                f\"Status: {base_status['runtime_status']['status'].upper()}\"\n            )\n\n            # Task Execution Summary\n            task_exec = base_status[\"task_execution\"]\n            if task_exec[\"total_tasks\"] &gt; 0:\n                verbose_output.formatter.print_section(\n                    \"Task Execution\",\n                    f\"Total: {task_exec['total_tasks']} | \"\n                    f\"Completed: {task_exec['completed_tasks']} | \"\n                    f\"Failed: {task_exec['failed_tasks']} | \"\n                    f\"Active: {task_exec['active_tasks']}\\n\"\n                    f\"Adaptations: {task_exec['plan_adaptations']}\"\n                )\n\n                if task_exec.get(\"tools_used\"):\n                    verbose_output.formatter.print_section(\n                        \"Tools Used\",\n                        \", \".join(task_exec[\"tools_used\"])\n                    )\n\n            # Capabilities\n            caps = base_status[\"capabilities\"]\n            verbose_output.formatter.print_section(\n                \"Capabilities\",\n                f\"Intelligence Level: {caps['intelligence_level']}\\n\"\n                f\"Available Tools: {caps['available_tools']}\\n\"\n                f\"Analyzed Tools: {caps['analyzed_tools']}\\n\"\n                f\"World Model Size: {caps['world_model_size']}\"\n            )\n\n            # Memory &amp; Context\n            memory = base_status[\"memory_context\"]\n            verbose_output.formatter.print_section(\n                \"Memory &amp; Context\",\n                f\"Context System: {memory['context_system']}\\n\"\n                f\"Session Managers: {memory['session_managers']}\\n\"\n                f\"Variable Scopes: {memory['variable_scopes']}\\n\"\n                f\"Session Initialized: {memory['session_initialized']}\"\n            )\n\n            # Configuration\n            config = base_status[\"configuration\"]\n            verbose_output.formatter.print_section(\n                \"Configuration\",\n                f\"Fast LLM: {config['fast_llm_model']}\\n\"\n                f\"Complex LLM: {config['complex_llm_model']}\\n\"\n                f\"Max Tokens: {config['max_input_tokens']}\\n\"\n                f\"Persona: {'Configured' if config['persona_configured'] else 'Default'}\\n\"\n                f\"Format Config: {'Active' if config['format_config'] else 'None'}\"\n            )\n\n            # Performance\n            perf = base_status[\"performance\"]\n            verbose_output.formatter.print_section(\n                \"Performance\",\n                f\"Total Cost: ${perf['total_cost']:.4f}\\n\"\n                f\"Checkpointing: {'Enabled' if perf['checkpoint_enabled'] else 'Disabled'}\\n\"\n                f\"Max Parallel Tasks: {perf['max_parallel_tasks']}\\n\"\n                f\"Last Checkpoint: {perf['last_checkpoint'] or 'None'}\"\n            )\n\n            # Variable System Details\n            if \"variable_system\" in base_status:\n                var_sys = base_status[\"variable_system\"]\n                scope_details = []\n                for scope, details in var_sys[\"scope_details\"].items():\n                    scope_details.append(f\"{scope}: {details['variables']} variables ({details['type']})\")\n\n                verbose_output.formatter.print_section(\n                    \"Variable System\",\n                    f\"Total Scopes: {var_sys['total_scopes']}\\n\"\n                    f\"Total Variables: {var_sys['total_variables']}\\n\" +\n                    \"\\n\".join(scope_details)\n                )\n\n            # Format Quality\n            if \"format_quality\" in base_status:\n                quality = base_status[\"format_quality\"]\n                verbose_output.formatter.print_section(\n                    \"Format Quality\",\n                    f\"Overall Score: {quality['overall_score']:.2f}\\n\"\n                    f\"Format Adherence: {quality['format_adherence']:.2f}\\n\"\n                    f\"Length Adherence: {quality['length_adherence']:.2f}\\n\"\n                    f\"Content Quality: {quality['content_quality']:.2f}\\n\"\n                    f\"Assessment: {quality['assessment']}\"\n                )\n\n            # LLM Usage\n            if \"llm_usage\" in base_status:\n                llm = base_status[\"llm_usage\"]\n                verbose_output.formatter.print_section(\n                    \"LLM Usage Statistics\",\n                    f\"Total Calls: {llm['total_calls']}\\n\"\n                    f\"Avg Context Tokens: {llm['average_context_tokens']:.1f}\\n\"\n                    f\"Total Tokens: {llm['total_tokens_used']}\\n\"\n                    f\"Compression Rate: {llm['context_compression_rate']:.2%}\"\n                )\n\n            # Servers\n            servers = base_status[\"servers\"]\n            if servers[\"server_count\"] &gt; 0:\n                server_status = []\n                if servers[\"a2a_server\"]:\n                    server_status.append(\"A2A Server: Active\")\n                if servers[\"mcp_server\"]:\n                    server_status.append(\"MCP Server: Active\")\n\n                verbose_output.formatter.print_section(\n                    \"Servers\",\n                    \"\\n\".join(server_status)\n                )\n\n            verbose_output.print_separator()\n            verbose_output.print_info(f\"Status generated at: {base_status['timestamp']}\")\n\n            return \"Status printed above\"\n\n        except Exception as e:\n            # Fallback to JSON if pretty print fails\n            import json\n            return json.dumps(base_status, indent=2, default=str)\n\n    @property\n    def tool_registry(self):\n        return self._tool_registry\n</code></pre> <code>total_cost</code> <code>property</code> \u00b6 <p>Get total cost if budget manager available</p> <code>a_run(query, session_id='default', user_id=None, stream_callback=None, **kwargs)</code> <code>async</code> \u00b6 <p>Main entry point for agent execution</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def a_run(\n    self,\n    query: str,\n    session_id: str = \"default\",\n    user_id: str = None,\n    stream_callback: Optional[Callable] = None,\n    **kwargs\n) -&gt; str:\n    \"\"\"Main entry point for agent execution\"\"\"\n\n    execution_start = self.progress_tracker.start_timer(\"total_execution\")\n\n    await self.progress_tracker.emit_event(ProgressEvent(\n        event_type=\"execution_start\",\n        timestamp=time.time(),\n        status=NodeStatus.RUNNING,\n        node_name=\"FlowAgent\",\n        session_id=session_id,\n        metadata={\"query\": query, \"user_id\": user_id}\n    ))\n\n    try:\n        # Set user context variables\n        timestamp = datetime.now()\n        self.variable_manager.register_scope('user', {\n            'id': user_id,\n            'session': session_id,\n            'query': query,\n            'timestamp': timestamp.isoformat()\n        })\n\n        # Update system variables\n        self.variable_manager.set('system_context.timestamp', {'isoformat':timestamp.isoformat()})\n        self.variable_manager.set('system_context.timestamp.year', timestamp.year)\n        self.variable_manager.set('system_context.timestamp.month', timestamp.month)\n        self.variable_manager.set('system_context.timestamp.week', timestamp.strftime('%W'))\n        self.variable_manager.set('system_context.timestamp.day', timestamp.day)\n        self.variable_manager.set('system_context.timestamp.hour', timestamp.hour)\n        self.variable_manager.set('system_context.timestamp.minute', timestamp.minute)\n        self.variable_manager.set('system_context.timestamp.second', timestamp.second)\n        self.variable_manager.set('system_context.timestamp.date', timestamp.strftime('%Y-%m-%d'))\n        self.variable_manager.set('system_context.timestamp.weekday', timestamp.strftime('%A'))\n        self.variable_manager.set('system_context.timestamp.kalenderwoche', timestamp.strftime('%W'))\n        self.variable_manager.set('system_context.current_session', session_id)\n        self.variable_manager.set('system_context.current_user', user_id)\n        self.variable_manager.set('system_context.last_query', query)\n\n        # Initialize with tool awareness\n        await self._initialize_context_awareness()\n\n        # Prepare execution context\n        self.shared.update({\n            \"current_query\": query,\n            \"session_id\": session_id,\n            \"user_id\": user_id,\n            \"stream_callback\": stream_callback\n        })\n        # Set LLM models in shared context\n        self.shared['fast_llm_model'] = self.amd.fast_llm_model\n        self.shared['complex_llm_model'] = self.amd.complex_llm_model\n        self.shared['persona_config'] = self.amd.persona\n        self.shared['use_fast_response'] = self.amd.use_fast_response\n        self.shared['variable_manager'] = self.variable_manager\n        # Add to conversation history\n        self.shared[\"conversation_history\"].append({\n            \"role\": \"user\",\n            \"content\": query,\n            \"timestamp\": datetime.now().isoformat()\n        })\n\n        # Set system status\n        self.shared[\"system_status\"] = \"running\"\n        self.is_running = True\n\n        # Execute main orchestration flow\n        result = await self._orchestrate_execution()\n        # Add response to history\n        self.shared[\"conversation_history\"].append({\n            \"role\": \"assistant\",\n            \"content\": result,\n            \"timestamp\": datetime.now().isoformat()\n        })\n\n        total_duration = self.progress_tracker.end_timer(\"total_execution\")\n\n        await self.progress_tracker.emit_event(ProgressEvent(\n            event_type=\"execution_complete\",\n            timestamp=time.time(),\n            node_name=\"FlowAgent\",\n            status=NodeStatus.COMPLETED,\n            node_duration=total_duration,\n            session_id=session_id,\n            metadata={\n                \"result_length\": len(result),\n                \"summary\": self.progress_tracker.get_summary()\n            }\n        ))\n        # Checkpoint if needed\n        if self.enable_pause_resume:\n            await self._maybe_checkpoint()\n\n        return result\n\n    except Exception as e:\n        logger.error(f\"Agent execution failed: {e}\", exc_info=True)\n        error_response = f\"I encountered an error: {str(e)}\"\n        import traceback\n        print(traceback.format_exc())\n\n        self.shared[\"conversation_history\"].append({\n            \"role\": \"assistant\",\n            \"content\": error_response,\n            \"timestamp\": datetime.now().isoformat()\n        })\n\n        total_duration = self.progress_tracker.end_timer(\"total_execution\")\n\n        await self.progress_tracker.emit_event(ProgressEvent(\n            event_type=\"error\",\n            timestamp=time.time(),\n            node_name=\"FlowAgent\",\n            status=NodeStatus.FAILED,\n            node_duration=total_duration,\n            session_id=session_id,\n            metadata={\"error\": str(e), \"error_type\": type(e).__name__}\n        ))\n\n        return error_response\n\n    finally:\n        self.shared[\"system_status\"] = \"idle\"\n        self.is_running = False\n</code></pre> <code>a_run_with_format(query, response_format='frei-text', text_length='chat-conversation', custom_instructions='', **kwargs)</code> <code>async</code> \u00b6 <p>F\u00fchre Agent mit spezifischem Format aus</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def a_run_with_format(\n    self,\n    query: str,\n    response_format: str = \"frei-text\",\n    text_length: str = \"chat-conversation\",\n    custom_instructions: str = \"\",\n    **kwargs\n) -&gt; str:\n    \"\"\"F\u00fchre Agent mit spezifischem Format aus\"\"\"\n\n    # Tempor\u00e4re Format-Einstellung\n    original_persona = self.amd.persona\n\n    try:\n        self.set_response_format(response_format, text_length, custom_instructions)\n        response = await self.a_run(query, **kwargs)\n        return response\n    finally:\n        # Restore original persona\n        self.amd.persona = original_persona\n        self.shared[\"persona_config\"] = original_persona\n</code></pre> <code>add_custom_flow(flow, name)</code> \u00b6 <p>Add a custom flow for dynamic execution</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def add_custom_flow(self, flow: AsyncFlow, name: str):\n    \"\"\"Add a custom flow for dynamic execution\"\"\"\n    self.add_tool(flow.run_async, name=name, description=f\"Custom flow: {flow.__class__.__name__}\")\n    logger.info(f\"Custom node added: {name}\")\n</code></pre> <code>add_tool(tool_func, name=None, description=None, is_new=False)</code> <code>async</code> \u00b6 <p>Enhanced tool addition with intelligent analysis</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def add_tool(self, tool_func: Callable, name: str = None, description: str = None, is_new=False):\n    \"\"\"Enhanced tool addition with intelligent analysis\"\"\"\n    if not asyncio.iscoroutinefunction(tool_func):\n        @wraps(tool_func)\n        async def async_wrapper(*args, **kwargs):\n            return await asyncio.to_thread(tool_func, *args, **kwargs)\n\n        effective_func = async_wrapper\n    else:\n        effective_func = tool_func\n\n    tool_name = name or effective_func.__name__\n    tool_description = description or effective_func.__doc__ or \"No description\"\n\n    # Store in registry\n    self._tool_registry[tool_name] = {\n        \"function\": effective_func,\n        \"description\": tool_description\n    }\n\n    # Add to available tools list\n    if tool_name not in self.shared[\"available_tools\"]:\n        self.shared[\"available_tools\"].append(tool_name)\n\n    # Intelligent tool analysis\n    if is_new:\n        await self._analyze_tool_capabilities(tool_name, tool_description)\n\n    logger.info(f\"Tool added with analysis: {tool_name}\")\n</code></pre> <code>arun_function(function_name, *args, **kwargs)</code> <code>async</code> \u00b6 <p>Asynchronously finds a function by its string name, executes it with the given arguments, and returns the result.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def arun_function(self, function_name: str, *args, **kwargs) -&gt; Any:\n    \"\"\"\n    Asynchronously finds a function by its string name, executes it with\n    the given arguments, and returns the result.\n    \"\"\"\n    logger.info(f\"Attempting to run function: {function_name} with args: {args}, kwargs: {kwargs}\")\n    target_function = self.get_tool_by_name(function_name)\n\n    if not target_function:\n        raise ValueError(f\"Function '{function_name}' not found in the agent's registered tools.\")\n\n    try:\n        if asyncio.iscoroutinefunction(target_function):\n            result = await target_function(*args, **kwargs)\n        else:\n            # If the function is not async, run it in a thread pool\n            loop = asyncio.get_running_loop()\n            result = await loop.run_in_executor(None, lambda: target_function(*args, **kwargs))\n\n        logger.info(f\"Function {function_name} completed successfully with result: {result}\")\n        return result\n\n    except Exception as e:\n        logger.error(f\"Function {function_name} execution failed: {e}\")\n        raise\n</code></pre> <code>close()</code> <code>async</code> \u00b6 <p>Clean shutdown</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def close(self):\n    \"\"\"Clean shutdown\"\"\"\n    self.is_running = False\n    self._shutdown_event.set()\n\n    # Create final checkpoint\n    if self.enable_pause_resume:\n        checkpoint = await self._create_checkpoint()\n        await self._save_checkpoint(checkpoint, \"final_checkpoint.pkl\")\n\n    # Shutdown executor\n    self.executor.shutdown(wait=True)\n\n    # Close servers\n    if self.a2a_server:\n        await self.a2a_server.close()\n\n    if self.mcp_server:\n        await self.mcp_server.close()\n\n    logger.info(\"Agent shutdown complete\")\n</code></pre> <code>configure_persona_integration(apply_method='system_prompt', integration_level='light')</code> \u00b6 <p>Configure how persona is applied</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def configure_persona_integration(self, apply_method: str = \"system_prompt\", integration_level: str = \"light\"):\n    \"\"\"Configure how persona is applied\"\"\"\n    if self.amd.persona:\n        self.amd.persona.apply_method = apply_method\n        self.amd.persona.integration_level = integration_level\n        logger.info(f\"Persona integration updated: {apply_method}, {integration_level}\")\n    else:\n        logger.warning(\"No persona configured to update\")\n</code></pre> <code>execute_custom_node(node_name, **kwargs)</code> <code>async</code> \u00b6 <p>Execute a custom node dynamically</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def execute_custom_node(self, node_name: str, **kwargs) -&gt; Any:\n    \"\"\"Execute a custom node dynamically\"\"\"\n    if not hasattr(self, '_node_registry') or node_name not in self._node_registry:\n        raise ValueError(f\"Node '{node_name}' not found\")\n\n    node = self._node_registry[node_name]\n\n    # Create temporary shared state with kwargs\n    temp_shared = self.shared.copy()\n    temp_shared.update(kwargs)\n\n    # Execute the node\n    result = await node.run_async(temp_shared)\n\n    # Merge back any changes\n    self.shared.update(temp_shared)\n\n    return result\n</code></pre> <code>explain_reasoning_process()</code> <code>async</code> \u00b6 <p>Erkl\u00e4re den Reasoning-Prozess des Agenten</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>    async def explain_reasoning_process(self) -&gt; str:\n        \"\"\"Erkl\u00e4re den Reasoning-Prozess des Agenten\"\"\"\n        if not LITELLM_AVAILABLE:\n            return \"Reasoning explanation requires LLM capabilities.\"\n\n        summary = await self.get_task_execution_summary()\n\n        prompt = f\"\"\"\nErkl\u00e4re den Reasoning-Prozess dieses AI-Agenten in verst\u00e4ndlicher Form:\n\n## Ausf\u00fchrungszusammenfassung\n- Total Tasks: {summary['total_tasks']}\n- Erfolgreich: {len(summary['completed_tasks'])}\n- Fehlgeschlagen: {len(summary['failed_tasks'])}\n- Plan-Adaptationen: {summary['adaptations']}\n- Verwendete Tools: {', '.join(set(summary['tools_used']))}\n- Task-Typen: {summary['task_types_used']}\n\n## Task-Details\nErfolgreiche Tasks:\n{self._format_tasks_for_explanation(summary['completed_tasks'])}\n\n## Anweisungen\nErkl\u00e4re in 2-3 Abs\u00e4tzen:\n1. Welche Strategie der Agent gew\u00e4hlt hat\n2. Wie er die Aufgabe in Tasks unterteilt hat\n3. Wie er auf unerwartete Ergebnisse reagiert hat (falls Adaptationen)\n4. Was die wichtigsten Erkenntnisse waren\n\nSchreibe f\u00fcr einen technischen Nutzer, aber verst\u00e4ndlich.\"\"\"\n\n        try:\n            response = await self.a_run_llm_completion(\n                model=self.amd.complex_llm_model,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.5,\n                max_tokens=800,task_id=\"reasoning_explanation\"\n            )\n\n            return response\n\n        except Exception as e:\n            return f\"Could not generate reasoning explanation: {e}\"\n</code></pre> <code>format_text(text, **context)</code> \u00b6 <p>Format text with variables</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def format_text(self, text: str, **context) -&gt; str:\n    \"\"\"Format text with variables\"\"\"\n    return self.variable_manager.format_text(text, context)\n</code></pre> <code>get_available_formats()</code> \u00b6 <p>Erhalte verf\u00fcgbare Format- und L\u00e4ngen-Optionen</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def get_available_formats(self) -&gt; Dict[str, List[str]]:\n    \"\"\"Erhalte verf\u00fcgbare Format- und L\u00e4ngen-Optionen\"\"\"\n    return {\n        \"formats\": [f.value for f in ResponseFormat],\n        \"lengths\": [l.value for l in TextLength],\n        \"format_descriptions\": {\n            f.value: FormatConfig(response_format=f).get_format_instructions()\n            for f in ResponseFormat\n        },\n        \"length_descriptions\": {\n            l.value: FormatConfig(text_length=l).get_length_instructions()\n            for l in TextLength\n        }\n    }\n</code></pre> <code>get_available_variables()</code> \u00b6 <p>Get available variables for dynamic formatting</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def get_available_variables(self) -&gt; Dict[str, str]:\n    \"\"\"Get available variables for dynamic formatting\"\"\"\n    return self.variable_manager.get_available_variables()\n</code></pre> <code>get_context_statistics()</code> \u00b6 <p>Get comprehensive context management statistics</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def get_context_statistics(self) -&gt; Dict[str, Any]:\n    \"\"\"Get comprehensive context management statistics\"\"\"\n    stats = {\n        \"context_system\": \"advanced_session_aware\",\n        \"compression_threshold\": 0.76,\n        \"max_tokens\": getattr(self, 'max_input_tokens', 8000),\n        \"session_managers\": {},\n        \"context_usage\": {},\n        \"compression_stats\": {}\n    }\n\n    # Session manager statistics\n    session_managers = self.shared.get(\"session_managers\", {})\n    for name, manager in session_managers.items():\n        stats[\"session_managers\"][name] = {\n            \"history_length\": len(manager.history),\n            \"max_length\": manager.max_length,\n            \"space_name\": manager.space_name\n        }\n\n    # Context node statistics if available\n    if hasattr(self.task_flow, 'context_manager'):\n        context_manager = self.task_flow.context_manager\n        stats[\"compression_stats\"] = {\n            \"compression_threshold\": context_manager.compression_threshold,\n            \"max_tokens\": context_manager.max_tokens,\n            \"active_sessions\": len(context_manager.session_managers)\n        }\n\n    # LLM call statistics from enhanced node\n    llm_stats = self.shared.get(\"llm_call_stats\", {})\n    if llm_stats:\n        stats[\"context_usage\"] = {\n            \"total_llm_calls\": llm_stats.get(\"total_calls\", 0),\n            \"context_compression_rate\": llm_stats.get(\"context_compression_rate\", 0.0),\n            \"average_context_tokens\": llm_stats.get(\"context_tokens_used\", 0) / max(llm_stats.get(\"total_calls\", 1),\n                                                                                    1)\n        }\n\n    return stats\n</code></pre> <code>get_format_quality_report()</code> \u00b6 <p>Erhalte detaillierten Format-Qualit\u00e4tsbericht</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def get_format_quality_report(self) -&gt; Dict[str, Any]:\n    \"\"\"Erhalte detaillierten Format-Qualit\u00e4tsbericht\"\"\"\n    quality_assessment = self.shared.get(\"quality_assessment\", {})\n\n    if not quality_assessment:\n        return {\"status\": \"no_assessment\", \"message\": \"No recent quality assessment available\"}\n\n    quality_details = quality_assessment.get(\"quality_details\", {})\n\n    return {\n        \"overall_score\": quality_details.get(\"total_score\", 0.0),\n        \"format_adherence\": quality_details.get(\"format_adherence\", 0.0),\n        \"length_adherence\": quality_details.get(\"length_adherence\", 0.0),\n        \"content_quality\": quality_details.get(\"base_quality\", 0.0),\n        \"llm_assessment\": quality_details.get(\"llm_assessment\", 0.0),\n        \"suggestions\": quality_assessment.get(\"suggestions\", []),\n        \"assessment\": quality_assessment.get(\"quality_assessment\", \"unknown\"),\n        \"format_config_active\": quality_details.get(\"format_config_used\", False)\n    }\n</code></pre> <code>get_task_execution_summary()</code> <code>async</code> \u00b6 <p>Erhalte detaillierte Zusammenfassung der Task-Ausf\u00fchrung</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def get_task_execution_summary(self) -&gt; Dict[str, Any]:\n    \"\"\"Erhalte detaillierte Zusammenfassung der Task-Ausf\u00fchrung\"\"\"\n    tasks = self.shared.get(\"tasks\", {})\n    results_store = self.shared.get(\"results_store\", {})\n\n    summary = {\n        \"total_tasks\": len(tasks),\n        \"completed_tasks\": [],\n        \"failed_tasks\": [],\n        \"task_types_used\": {},\n        \"tools_used\": [],\n        \"adaptations\": self.shared.get(\"plan_adaptations\", 0),\n        \"execution_timeline\": []\n    }\n\n    for task_id, task in tasks.items():\n        task_info = {\n            \"id\": task_id,\n            \"type\": task.type,\n            \"description\": task.description,\n            \"status\": task.status,\n            \"duration\": None\n        }\n\n        if task.started_at and task.completed_at:\n            duration = (task.completed_at - task.started_at).total_seconds()\n            task_info[\"duration\"] = duration\n\n        if task.status == \"completed\":\n            summary[\"completed_tasks\"].append(task_info)\n            if isinstance(task, ToolTask):\n                summary[\"tools_used\"].append(task.tool_name)\n        elif task.status == \"failed\":\n            task_info[\"error\"] = task.error\n            summary[\"failed_tasks\"].append(task_info)\n\n        # Task types counting\n        task_type = task.type\n        summary[\"task_types_used\"][task_type] = summary[\"task_types_used\"].get(task_type, 0) + 1\n\n    return summary\n</code></pre> <code>get_tool_by_name(tool_name)</code> \u00b6 <p>Get tool function by name</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def get_tool_by_name(self, tool_name: str) -&gt; Callable | None:\n    \"\"\"Get tool function by name\"\"\"\n    return self._tool_registry.get(tool_name, {}).get(\"function\")\n</code></pre> <code>get_variable(path, default=None)</code> \u00b6 <p>Get variable using unified system</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def get_variable(self, path: str, default=None):\n    \"\"\"Get variable using unified system\"\"\"\n    return self.variable_manager.get(path, default)\n</code></pre> <code>get_variable_documentation()</code> \u00b6 <p>Get comprehensive variable system documentation</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def get_variable_documentation(self) -&gt; str:\n    \"\"\"Get comprehensive variable system documentation\"\"\"\n    docs = []\n    docs.append(\"# Variable System Documentation\\n\")\n\n    # Available scopes\n    docs.append(\"## Available Scopes:\")\n    scope_info = self.variable_manager.get_scope_info()\n    for scope_name, info in scope_info.items():\n        docs.append(f\"- `{scope_name}`: {info['type']} with {info.get('keys', 'N/A')} keys\")\n\n    docs.append(\"\\n## Syntax Options:\")\n    docs.append(\"- `{{ variable.path }}` - Full path resolution\")\n    docs.append(\"- `{variable}` - Simple variable (no dots)\")\n    docs.append(\"- `$variable` - Shell-style variable\")\n\n    docs.append(\"\\n## Example Usage:\")\n    docs.append(\"- `{{ results.task_1.data }}` - Get result from task_1\")\n    docs.append(\"- `{{ user.name }}` - Get user name\")\n    docs.append(\"- `{agent_name}` - Simple agent name\")\n    docs.append(\"- `$timestamp` - System timestamp\")\n\n    # Available variables\n    docs.append(\"\\n## Available Variables:\")\n    variables = self.variable_manager.get_available_variables()\n    for scope_name, scope_vars in variables.items():\n        docs.append(f\"\\n### {scope_name}:\")\n        for var_name, var_info in scope_vars.items():\n            docs.append(f\"- `{var_info['path']}`: {var_info['preview']} ({var_info['type']})\")\n\n    return \"\\n\".join(docs)\n</code></pre> <code>initialize_session_context(session_id='default', max_history=200)</code> <code>async</code> \u00b6 <p>Initialize session-aware context management for infinite scaling</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def initialize_session_context(self, session_id: str = \"default\", max_history: int = 200) -&gt; bool:\n    \"\"\"Initialize session-aware context management for infinite scaling\"\"\"\n    try:\n        from toolboxv2 import get_app\n        from toolboxv2.mods.isaa.extras.session import ChatSession\n\n        # Initialize memory system\n        memory_instance = get_app().get_mod(\"isaa\").get_memory()\n\n        # Create multiple session managers for different aspects\n        session_managers = {\n            \"main_conversation\": ChatSession(\n                memory_instance,\n                space_name=f\"ChatSession/{self.amd.name}.{session_id}.conversation\",\n                max_length=max_history\n            ),\n            \"task_history\": ChatSession(\n                memory_instance,\n                space_name=f\"ChatSession/{self.amd.name}.{session_id}.tasks\",\n                max_length=max_history // 2\n            ),\n            \"insights\": ChatSession(\n                memory_instance,\n                space_name=f\"ChatSession/{self.amd.name}.{session_id}.insights\",\n                max_length=100\n            )\n        }\n\n        # Store in shared state\n        self.shared[\"session_managers\"] = session_managers\n        self.shared[\"session_initialized\"] = True\n\n        logger.info(f\"Session context initialized for {session_id}\")\n        return True\n\n    except Exception as e:\n        logger.error(f\"Session context initialization failed: {e}\")\n        return False\n</code></pre> <code>load_checkpoint(filepath)</code> <code>async</code> \u00b6 <p>Load checkpoint from file</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def load_checkpoint(self, filepath: str) -&gt; bool:\n    \"\"\"Load checkpoint from file\"\"\"\n    try:\n        with open(filepath, 'rb') as f:\n            checkpoint: AgentCheckpoint = pickle.load(f)\n\n        # Restore state\n        self.shared[\"world_model\"] = checkpoint.world_model\n        self.shared[\"tasks\"] = {\n            task_id: Task(**task_data)\n            for task_id, task_data in checkpoint.task_state.items()\n        }\n\n        # Restore agent state\n        agent_state = checkpoint.agent_state\n        self.is_running = agent_state.get(\"is_running\", False)\n        self.is_paused = agent_state.get(\"is_paused\", False)\n\n        self.last_checkpoint = checkpoint.timestamp\n        logger.info(f\"Checkpoint loaded: {filepath}\")\n\n        return True\n\n    except Exception as e:\n        logger.error(f\"Failed to load checkpoint: {e}\")\n        return False\n</code></pre> <code>pause()</code> <code>async</code> \u00b6 <p>Pause agent execution</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def pause(self) -&gt; bool:\n    \"\"\"Pause agent execution\"\"\"\n    if not self.is_running:\n        return False\n\n    self.is_paused = True\n    self.shared[\"system_status\"] = \"paused\"\n\n    # Create checkpoint\n    checkpoint = await self._create_checkpoint()\n    await self._save_checkpoint(checkpoint)\n\n    logger.info(\"Agent execution paused\")\n    return True\n</code></pre> <code>resume()</code> <code>async</code> \u00b6 <p>Resume agent execution</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def resume(self) -&gt; bool:\n    \"\"\"Resume agent execution\"\"\"\n    if not self.is_paused:\n        return False\n\n    self.is_paused = False\n    self.shared[\"system_status\"] = \"running\"\n\n    logger.info(\"Agent execution resumed\")\n    return True\n</code></pre> <code>set_persona(name, style='professional', tone='friendly', personality_traits=None, apply_method='system_prompt', integration_level='light', custom_instructions='')</code> \u00b6 <p>Set agent persona mit erweiterten Konfigurationsm\u00f6glichkeiten</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def set_persona(self, name: str, style: str = \"professional\", tone: str = \"friendly\",\n                personality_traits: List[str] = None, apply_method: str = \"system_prompt\",\n                integration_level: str = \"light\", custom_instructions: str = \"\"):\n    \"\"\"Set agent persona mit erweiterten Konfigurationsm\u00f6glichkeiten\"\"\"\n    if personality_traits is None:\n        personality_traits = [\"helpful\", \"concise\"]\n\n    self.amd.persona = PersonaConfig(\n        name=name,\n        style=style,\n        tone=tone,\n        personality_traits=personality_traits,\n        custom_instructions=custom_instructions,\n        apply_method=apply_method,\n        integration_level=integration_level\n    )\n\n    logger.info(f\"Persona set: {name} ({style}, {tone}) - Method: {apply_method}, Level: {integration_level}\")\n</code></pre> <code>set_response_format(response_format, text_length, custom_instructions='', quality_threshold=0.7)</code> \u00b6 <p>Dynamische Format- und L\u00e4ngen-Konfiguration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def set_response_format(\n    self,\n    response_format: str,\n    text_length: str,\n    custom_instructions: str = \"\",\n    quality_threshold: float = 0.7\n):\n    \"\"\"Dynamische Format- und L\u00e4ngen-Konfiguration\"\"\"\n\n    # Validiere Eingaben\n    try:\n        ResponseFormat(response_format)\n        TextLength(text_length)\n    except ValueError:\n        available_formats = [f.value for f in ResponseFormat]\n        available_lengths = [l.value for l in TextLength]\n        raise ValueError(\n            f\"Invalid format or length. \"\n            f\"Available formats: {available_formats}. \"\n            f\"Available lengths: {available_lengths}\"\n        )\n\n    # Erstelle oder aktualisiere Persona\n    if not self.amd.persona:\n        self.amd.persona = PersonaConfig(name=\"Assistant\")\n\n    # Erstelle Format-Konfiguration\n    format_config = FormatConfig(\n        response_format=ResponseFormat(response_format),\n        text_length=TextLength(text_length),\n        custom_instructions=custom_instructions,\n        quality_threshold=quality_threshold\n    )\n\n    self.amd.persona.format_config = format_config\n\n    # Aktualisiere Personality Traits mit Format-Hinweisen\n    self._update_persona_with_format(response_format, text_length)\n\n    # Update shared state\n    self.shared[\"persona_config\"] = self.amd.persona\n    self.shared[\"format_config\"] = format_config\n\n    logger.info(f\"Response format set: {response_format}, length: {text_length}\")\n</code></pre> <code>set_variable(path, value)</code> \u00b6 <p>Set variable using unified system</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def set_variable(self, path: str, value: Any):\n    \"\"\"Set variable using unified system\"\"\"\n    self.variable_manager.set(path, value)\n</code></pre> <code>setup_a2a_server(host='0.0.0.0', port=5000, **kwargs)</code> \u00b6 <p>Setup A2A server for bidirectional communication</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def setup_a2a_server(self, host: str = \"0.0.0.0\", port: int = 5000, **kwargs):\n    \"\"\"Setup A2A server for bidirectional communication\"\"\"\n    if not A2A_AVAILABLE:\n        logger.warning(\"A2A not available, cannot setup server\")\n        return\n\n    try:\n        self.a2a_server = A2AServer(\n            host=host,\n            port=port,\n            agent_card=AgentCard(\n                name=self.amd.name,\n                description=\"Production-ready PocketFlow agent\",\n                version=\"1.0.0\"\n            ),\n            **kwargs\n        )\n\n        # Register agent methods\n        @self.a2a_server.route(\"/run\")\n        async def handle_run(request_data):\n            query = request_data.get(\"query\", \"\")\n            session_id = request_data.get(\"session_id\", \"a2a_session\")\n\n            response = await self.a_run(query, session_id=session_id)\n            return {\"response\": response}\n\n        logger.info(f\"A2A server setup on {host}:{port}\")\n\n    except Exception as e:\n        logger.error(f\"Failed to setup A2A server: {e}\")\n</code></pre> <code>setup_mcp_server(host='0.0.0.0', port=8000, name=None, **kwargs)</code> \u00b6 <p>Setup MCP server</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def setup_mcp_server(self, host: str = \"0.0.0.0\", port: int = 8000, name: str = None, **kwargs):\n    \"\"\"Setup MCP server\"\"\"\n    if not MCP_AVAILABLE:\n        logger.warning(\"MCP not available, cannot setup server\")\n        return\n\n    try:\n        server_name = name or f\"{self.amd.name}_MCP\"\n        self.mcp_server = FastMCP(server_name)\n\n        # Register agent as MCP tool\n        @self.mcp_server.tool()\n        async def agent_run(query: str, session_id: str = \"mcp_session\") -&gt; str:\n            \"\"\"Execute agent with given query\"\"\"\n            return await self.a_run(query, session_id=session_id)\n\n        logger.info(f\"MCP server setup: {server_name}\")\n\n    except Exception as e:\n        logger.error(f\"Failed to setup MCP server: {e}\")\n</code></pre> <code>start_servers()</code> <code>async</code> \u00b6 <p>Start all configured servers</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def start_servers(self):\n    \"\"\"Start all configured servers\"\"\"\n    tasks = []\n\n    if self.a2a_server:\n        tasks.append(asyncio.create_task(self.a2a_server.start()))\n\n    if self.mcp_server:\n        tasks.append(asyncio.create_task(self.mcp_server.run()))\n\n    if tasks:\n        logger.info(f\"Starting {len(tasks)} servers...\")\n        await asyncio.gather(*tasks, return_exceptions=True)\n</code></pre> <code>status(pretty_print=False)</code> \u00b6 <p>Get comprehensive agent status with optional pretty printing</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def status(self, pretty_print: bool = False) -&gt; Union[Dict[str, Any], str]:\n    \"\"\"Get comprehensive agent status with optional pretty printing\"\"\"\n\n    # Core status information\n    base_status = {\n        \"agent_info\": {\n            \"name\": self.amd.name,\n            \"version\": \"2.0\",\n            \"type\": \"FlowAgent\"\n        },\n        \"runtime_status\": {\n            \"status\": self.shared.get(\"system_status\", \"idle\"),\n            \"is_running\": self.is_running,\n            \"is_paused\": self.is_paused,\n            \"uptime_seconds\": (datetime.now() - getattr(self, '_start_time', datetime.now())).total_seconds()\n        },\n        \"task_execution\": {\n            \"total_tasks\": len(self.shared.get(\"tasks\", {})),\n            \"active_tasks\": len([t for t in self.shared.get(\"tasks\", {}).values() if t.status == \"running\"]),\n            \"completed_tasks\": len([t for t in self.shared.get(\"tasks\", {}).values() if t.status == \"completed\"]),\n            \"failed_tasks\": len([t for t in self.shared.get(\"tasks\", {}).values() if t.status == \"failed\"]),\n            \"plan_adaptations\": self.shared.get(\"plan_adaptations\", 0)\n        },\n        \"conversation\": {\n            \"turns\": len(self.shared.get(\"conversation_history\", [])),\n            \"session_id\": self.shared.get(\"session_id\", \"default\"),\n            \"current_user\": self.shared.get(\"user_id\"),\n            \"last_query\": self.shared.get(\"current_query\", \"\")[:100] + \"...\" if len(\n                self.shared.get(\"current_query\", \"\")) &gt; 100 else self.shared.get(\"current_query\", \"\")\n        },\n        \"capabilities\": {\n            \"available_tools\": len(self.shared.get(\"available_tools\", [])),\n            \"tool_names\": list(self.shared.get(\"available_tools\", [])),\n            \"analyzed_tools\": len(self._tool_capabilities),\n            \"world_model_size\": len(self.shared.get(\"world_model\", {})),\n            \"intelligence_level\": \"high\" if self._tool_capabilities else \"basic\"\n        },\n        \"memory_context\": {\n            \"session_initialized\": self.shared.get(\"session_initialized\", False),\n            \"session_managers\": len(self.shared.get(\"session_managers\", {})),\n            \"context_system\": \"advanced_session_aware\" if self.shared.get(\"session_initialized\") else \"basic\",\n            \"variable_scopes\": len(self.variable_manager.get_scope_info()) if hasattr(self,\n                                                                                      'variable_manager') else 0\n        },\n        \"performance\": {\n            \"total_cost\": self.total_cost,\n            \"checkpoint_enabled\": self.enable_pause_resume,\n            \"last_checkpoint\": self.last_checkpoint.isoformat() if self.last_checkpoint else None,\n            \"max_parallel_tasks\": self.max_parallel_tasks\n        },\n        \"servers\": {\n            \"a2a_server\": self.a2a_server is not None,\n            \"mcp_server\": self.mcp_server is not None,\n            \"server_count\": sum([self.a2a_server is not None, self.mcp_server is not None])\n        },\n        \"configuration\": {\n            \"fast_llm_model\": self.amd.fast_llm_model,\n            \"complex_llm_model\": self.amd.complex_llm_model,\n            \"use_fast_response\": getattr(self.amd, 'use_fast_response', False),\n            \"max_input_tokens\": getattr(self.amd, 'max_input_tokens', 8000),\n            \"persona_configured\": self.amd.persona is not None,\n            \"format_config\": bool(getattr(self.amd.persona, 'format_config', None)) if self.amd.persona else False\n        }\n    }\n\n    # Add detailed execution summary if tasks exist\n    tasks = self.shared.get(\"tasks\", {})\n    if tasks:\n        task_types_used = {}\n        tools_used = []\n        execution_timeline = []\n\n        for task_id, task in tasks.items():\n            # Count task types\n            task_type = getattr(task, 'type', 'unknown')\n            task_types_used[task_type] = task_types_used.get(task_type, 0) + 1\n\n            # Collect tools used\n            if hasattr(task, 'tool_name') and task.tool_name:\n                tools_used.append(task.tool_name)\n\n            # Timeline info\n            if hasattr(task, 'started_at') and task.started_at:\n                timeline_entry = {\n                    \"task_id\": task_id,\n                    \"type\": task_type,\n                    \"started\": task.started_at.isoformat(),\n                    \"status\": getattr(task, 'status', 'unknown')\n                }\n                if hasattr(task, 'completed_at') and task.completed_at:\n                    timeline_entry[\"completed\"] = task.completed_at.isoformat()\n                    timeline_entry[\"duration\"] = (task.completed_at - task.started_at).total_seconds()\n                execution_timeline.append(timeline_entry)\n\n        base_status[\"task_execution\"].update({\n            \"task_types_used\": task_types_used,\n            \"tools_used\": list(set(tools_used)),\n            \"execution_timeline\": execution_timeline[-5:]  # Last 5 tasks\n        })\n\n    # Add context statistics\n    if hasattr(self.task_flow, 'context_manager'):\n        context_manager = self.task_flow.context_manager\n        base_status[\"memory_context\"].update({\n            \"compression_threshold\": context_manager.compression_threshold,\n            \"max_tokens\": context_manager.max_tokens,\n            \"active_context_sessions\": len(getattr(context_manager, 'session_managers', {}))\n        })\n\n    # Add variable system info\n    if hasattr(self, 'variable_manager'):\n        available_vars = self.variable_manager.get_available_variables()\n        scope_info = self.variable_manager.get_scope_info()\n\n        base_status[\"variable_system\"] = {\n            \"total_scopes\": len(scope_info),\n            \"scope_names\": list(scope_info.keys()),\n            \"total_variables\": sum(len(vars) for vars in available_vars.values()),\n            \"scope_details\": {\n                scope: {\"type\": info[\"type\"], \"variables\": len(available_vars.get(scope, {}))}\n                for scope, info in scope_info.items()\n            }\n        }\n\n    # Add format quality info if available\n    quality_assessment = self.shared.get(\"quality_assessment\", {})\n    if quality_assessment:\n        quality_details = quality_assessment.get(\"quality_details\", {})\n        base_status[\"format_quality\"] = {\n            \"overall_score\": quality_details.get(\"total_score\", 0.0),\n            \"format_adherence\": quality_details.get(\"format_adherence\", 0.0),\n            \"length_adherence\": quality_details.get(\"length_adherence\", 0.0),\n            \"content_quality\": quality_details.get(\"base_quality\", 0.0),\n            \"assessment\": quality_assessment.get(\"quality_assessment\", \"unknown\"),\n            \"has_suggestions\": bool(quality_assessment.get(\"suggestions\", []))\n        }\n\n    # Add LLM usage statistics\n    llm_stats = self.shared.get(\"llm_call_stats\", {})\n    if llm_stats:\n        base_status[\"llm_usage\"] = {\n            \"total_calls\": llm_stats.get(\"total_calls\", 0),\n            \"context_compression_rate\": llm_stats.get(\"context_compression_rate\", 0.0),\n            \"average_context_tokens\": llm_stats.get(\"context_tokens_used\", 0) / max(llm_stats.get(\"total_calls\", 1),\n                                                                                    1),\n            \"total_tokens_used\": llm_stats.get(\"total_tokens_used\", 0)\n        }\n\n    # Add timestamp\n    base_status[\"timestamp\"] = datetime.now().isoformat()\n\n    if not pretty_print:\n        return base_status\n\n    # Pretty print using EnhancedVerboseOutput\n    try:\n        from toolboxv2.mods.isaa.CodingAgent.live import EnhancedVerboseOutput\n        verbose_output = EnhancedVerboseOutput(verbose=True)\n\n        # Header\n        verbose_output.log_header(f\"Agent Status: {base_status['agent_info']['name']}\")\n\n        # Runtime Status\n        status_color = {\n            \"running\": \"SUCCESS\",\n            \"paused\": \"WARNING\",\n            \"idle\": \"INFO\",\n            \"error\": \"ERROR\"\n        }.get(base_status[\"runtime_status\"][\"status\"], \"INFO\")\n\n        getattr(verbose_output, f\"print_{status_color.lower()}\")(\n            f\"Status: {base_status['runtime_status']['status'].upper()}\"\n        )\n\n        # Task Execution Summary\n        task_exec = base_status[\"task_execution\"]\n        if task_exec[\"total_tasks\"] &gt; 0:\n            verbose_output.formatter.print_section(\n                \"Task Execution\",\n                f\"Total: {task_exec['total_tasks']} | \"\n                f\"Completed: {task_exec['completed_tasks']} | \"\n                f\"Failed: {task_exec['failed_tasks']} | \"\n                f\"Active: {task_exec['active_tasks']}\\n\"\n                f\"Adaptations: {task_exec['plan_adaptations']}\"\n            )\n\n            if task_exec.get(\"tools_used\"):\n                verbose_output.formatter.print_section(\n                    \"Tools Used\",\n                    \", \".join(task_exec[\"tools_used\"])\n                )\n\n        # Capabilities\n        caps = base_status[\"capabilities\"]\n        verbose_output.formatter.print_section(\n            \"Capabilities\",\n            f\"Intelligence Level: {caps['intelligence_level']}\\n\"\n            f\"Available Tools: {caps['available_tools']}\\n\"\n            f\"Analyzed Tools: {caps['analyzed_tools']}\\n\"\n            f\"World Model Size: {caps['world_model_size']}\"\n        )\n\n        # Memory &amp; Context\n        memory = base_status[\"memory_context\"]\n        verbose_output.formatter.print_section(\n            \"Memory &amp; Context\",\n            f\"Context System: {memory['context_system']}\\n\"\n            f\"Session Managers: {memory['session_managers']}\\n\"\n            f\"Variable Scopes: {memory['variable_scopes']}\\n\"\n            f\"Session Initialized: {memory['session_initialized']}\"\n        )\n\n        # Configuration\n        config = base_status[\"configuration\"]\n        verbose_output.formatter.print_section(\n            \"Configuration\",\n            f\"Fast LLM: {config['fast_llm_model']}\\n\"\n            f\"Complex LLM: {config['complex_llm_model']}\\n\"\n            f\"Max Tokens: {config['max_input_tokens']}\\n\"\n            f\"Persona: {'Configured' if config['persona_configured'] else 'Default'}\\n\"\n            f\"Format Config: {'Active' if config['format_config'] else 'None'}\"\n        )\n\n        # Performance\n        perf = base_status[\"performance\"]\n        verbose_output.formatter.print_section(\n            \"Performance\",\n            f\"Total Cost: ${perf['total_cost']:.4f}\\n\"\n            f\"Checkpointing: {'Enabled' if perf['checkpoint_enabled'] else 'Disabled'}\\n\"\n            f\"Max Parallel Tasks: {perf['max_parallel_tasks']}\\n\"\n            f\"Last Checkpoint: {perf['last_checkpoint'] or 'None'}\"\n        )\n\n        # Variable System Details\n        if \"variable_system\" in base_status:\n            var_sys = base_status[\"variable_system\"]\n            scope_details = []\n            for scope, details in var_sys[\"scope_details\"].items():\n                scope_details.append(f\"{scope}: {details['variables']} variables ({details['type']})\")\n\n            verbose_output.formatter.print_section(\n                \"Variable System\",\n                f\"Total Scopes: {var_sys['total_scopes']}\\n\"\n                f\"Total Variables: {var_sys['total_variables']}\\n\" +\n                \"\\n\".join(scope_details)\n            )\n\n        # Format Quality\n        if \"format_quality\" in base_status:\n            quality = base_status[\"format_quality\"]\n            verbose_output.formatter.print_section(\n                \"Format Quality\",\n                f\"Overall Score: {quality['overall_score']:.2f}\\n\"\n                f\"Format Adherence: {quality['format_adherence']:.2f}\\n\"\n                f\"Length Adherence: {quality['length_adherence']:.2f}\\n\"\n                f\"Content Quality: {quality['content_quality']:.2f}\\n\"\n                f\"Assessment: {quality['assessment']}\"\n            )\n\n        # LLM Usage\n        if \"llm_usage\" in base_status:\n            llm = base_status[\"llm_usage\"]\n            verbose_output.formatter.print_section(\n                \"LLM Usage Statistics\",\n                f\"Total Calls: {llm['total_calls']}\\n\"\n                f\"Avg Context Tokens: {llm['average_context_tokens']:.1f}\\n\"\n                f\"Total Tokens: {llm['total_tokens_used']}\\n\"\n                f\"Compression Rate: {llm['context_compression_rate']:.2%}\"\n            )\n\n        # Servers\n        servers = base_status[\"servers\"]\n        if servers[\"server_count\"] &gt; 0:\n            server_status = []\n            if servers[\"a2a_server\"]:\n                server_status.append(\"A2A Server: Active\")\n            if servers[\"mcp_server\"]:\n                server_status.append(\"MCP Server: Active\")\n\n            verbose_output.formatter.print_section(\n                \"Servers\",\n                \"\\n\".join(server_status)\n            )\n\n        verbose_output.print_separator()\n        verbose_output.print_info(f\"Status generated at: {base_status['timestamp']}\")\n\n        return \"Status printed above\"\n\n    except Exception as e:\n        # Fallback to JSON if pretty print fails\n        import json\n        return json.dumps(base_status, indent=2, default=str)\n</code></pre> <code>FormatConfig</code> <code>dataclass</code> \u00b6 <p>Konfiguration f\u00fcr Response-Format und -L\u00e4nge</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass FormatConfig:\n    \"\"\"Konfiguration f\u00fcr Response-Format und -L\u00e4nge\"\"\"\n    response_format: ResponseFormat = ResponseFormat.FREI_TEXT\n    text_length: TextLength = TextLength.CHAT_CONVERSATION\n    custom_instructions: str = \"\"\n    strict_format_adherence: bool = True\n    quality_threshold: float = 0.7\n\n    def get_format_instructions(self) -&gt; str:\n        \"\"\"Generiere Format-spezifische Anweisungen\"\"\"\n        format_instructions = {\n            ResponseFormat.FREI_TEXT: \"Verwende nat\u00fcrlichen Flie\u00dftext ohne spezielle Formatierung.\",\n            ResponseFormat.WITH_TABLES: \"Integriere Tabellen zur strukturierten Darstellung von Daten. Verwende Markdown-Tabellen.\",\n            ResponseFormat.WITH_BULLET_POINTS: \"Strukturiere Informationen mit Bullet Points (\u2022, -, *) f\u00fcr bessere Lesbarkeit.\",\n            ResponseFormat.WITH_LISTS: \"Verwende nummerierte und unnummerierte Listen zur Organisation von Inhalten.\",\n            ResponseFormat.TEXT_ONLY: \"Nur reiner Text ohne Formatierung, Symbole oder Strukturelemente.\",\n            ResponseFormat.MD_TEXT: \"Vollst\u00e4ndige Markdown-Formatierung mit Headings, Code-Blocks, Links etc.\",\n            ResponseFormat.YAML_TEXT: \"Strukturiere Antworten als YAML-Format f\u00fcr maschinenlesbare Ausgabe.\",\n            ResponseFormat.JSON_TEXT: \"Formatiere Antworten als JSON-Struktur f\u00fcr API-Integration.\",\n            ResponseFormat.PSEUDO_CODE: \"Verwende Pseudocode-Struktur f\u00fcr algorithmische oder logische Erkl\u00e4rungen.\",\n            ResponseFormat.CODE_STRUCTURE: \"Strukturiere wie Code mit Einr\u00fcckungen, Kommentaren und logischen Bl\u00f6cken.\"\n        }\n        return format_instructions.get(self.response_format, \"Standard-Formatierung.\")\n\n    def get_length_instructions(self) -&gt; str:\n        \"\"\"Generiere L\u00e4ngen-spezifische Anweisungen\"\"\"\n        length_instructions = {\n            TextLength.MINI_CHAT: \"Sehr kurze, pr\u00e4gnante Antworten (1-2 S\u00e4tze, max 50 W\u00f6rter). Chat-Style.\",\n            TextLength.CHAT_CONVERSATION: \"Moderate Gespr\u00e4chsl\u00e4nge (2-4 S\u00e4tze, 50-150 W\u00f6rter). Nat\u00fcrlicher Unterhaltungsstil.\",\n            TextLength.TABLE_CONVERSATION: \"Strukturierte, tabellarische Darstellung mit kompakten Erkl\u00e4rungen (100-250 W\u00f6rter).\",\n            TextLength.DETAILED_INDEPTH: \"Ausf\u00fchrliche, detaillierte Erkl\u00e4rungen (300-800 W\u00f6rter) mit Tiefe und Kontext.\",\n            TextLength.PHD_LEVEL: \"Akademische Tiefe mit umfassenden Erkl\u00e4rungen (800+ W\u00f6rter), Quellenangaben und Fachterminologie.\"\n        }\n        return length_instructions.get(self.text_length, \"Standard-L\u00e4nge.\")\n\n    def get_combined_instructions(self) -&gt; str:\n        \"\"\"Kombiniere Format- und L\u00e4ngen-Anweisungen\"\"\"\n        instructions = []\n        instructions.append(\"## Format-Anforderungen:\")\n        instructions.append(self.get_format_instructions())\n        instructions.append(\"\\n## L\u00e4ngen-Anforderungen:\")\n        instructions.append(self.get_length_instructions())\n\n        if self.custom_instructions:\n            instructions.append(f\"\\n## Zus\u00e4tzliche Anweisungen:\")\n            instructions.append(self.custom_instructions)\n\n        if self.strict_format_adherence:\n            instructions.append(\"\\n## WICHTIG: Halte dich strikt an diese Format- und L\u00e4ngen-Vorgaben!\")\n\n        return \"\\n\".join(instructions)\n\n    def get_expected_word_range(self) -&gt; tuple[int, int]:\n        \"\"\"Erwartete Wortanzahl f\u00fcr Qualit\u00e4tsbewertung\"\"\"\n        ranges = {\n            TextLength.MINI_CHAT: (10, 50),\n            TextLength.CHAT_CONVERSATION: (50, 150),\n            TextLength.TABLE_CONVERSATION: (100, 250),\n            TextLength.DETAILED_INDEPTH: (300, 800),\n            TextLength.PHD_LEVEL: (800, 2000)\n        }\n        return ranges.get(self.text_length, (50, 200))\n</code></pre> <code>get_combined_instructions()</code> \u00b6 <p>Kombiniere Format- und L\u00e4ngen-Anweisungen</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def get_combined_instructions(self) -&gt; str:\n    \"\"\"Kombiniere Format- und L\u00e4ngen-Anweisungen\"\"\"\n    instructions = []\n    instructions.append(\"## Format-Anforderungen:\")\n    instructions.append(self.get_format_instructions())\n    instructions.append(\"\\n## L\u00e4ngen-Anforderungen:\")\n    instructions.append(self.get_length_instructions())\n\n    if self.custom_instructions:\n        instructions.append(f\"\\n## Zus\u00e4tzliche Anweisungen:\")\n        instructions.append(self.custom_instructions)\n\n    if self.strict_format_adherence:\n        instructions.append(\"\\n## WICHTIG: Halte dich strikt an diese Format- und L\u00e4ngen-Vorgaben!\")\n\n    return \"\\n\".join(instructions)\n</code></pre> <code>get_expected_word_range()</code> \u00b6 <p>Erwartete Wortanzahl f\u00fcr Qualit\u00e4tsbewertung</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def get_expected_word_range(self) -&gt; tuple[int, int]:\n    \"\"\"Erwartete Wortanzahl f\u00fcr Qualit\u00e4tsbewertung\"\"\"\n    ranges = {\n        TextLength.MINI_CHAT: (10, 50),\n        TextLength.CHAT_CONVERSATION: (50, 150),\n        TextLength.TABLE_CONVERSATION: (100, 250),\n        TextLength.DETAILED_INDEPTH: (300, 800),\n        TextLength.PHD_LEVEL: (800, 2000)\n    }\n    return ranges.get(self.text_length, (50, 200))\n</code></pre> <code>get_format_instructions()</code> \u00b6 <p>Generiere Format-spezifische Anweisungen</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def get_format_instructions(self) -&gt; str:\n    \"\"\"Generiere Format-spezifische Anweisungen\"\"\"\n    format_instructions = {\n        ResponseFormat.FREI_TEXT: \"Verwende nat\u00fcrlichen Flie\u00dftext ohne spezielle Formatierung.\",\n        ResponseFormat.WITH_TABLES: \"Integriere Tabellen zur strukturierten Darstellung von Daten. Verwende Markdown-Tabellen.\",\n        ResponseFormat.WITH_BULLET_POINTS: \"Strukturiere Informationen mit Bullet Points (\u2022, -, *) f\u00fcr bessere Lesbarkeit.\",\n        ResponseFormat.WITH_LISTS: \"Verwende nummerierte und unnummerierte Listen zur Organisation von Inhalten.\",\n        ResponseFormat.TEXT_ONLY: \"Nur reiner Text ohne Formatierung, Symbole oder Strukturelemente.\",\n        ResponseFormat.MD_TEXT: \"Vollst\u00e4ndige Markdown-Formatierung mit Headings, Code-Blocks, Links etc.\",\n        ResponseFormat.YAML_TEXT: \"Strukturiere Antworten als YAML-Format f\u00fcr maschinenlesbare Ausgabe.\",\n        ResponseFormat.JSON_TEXT: \"Formatiere Antworten als JSON-Struktur f\u00fcr API-Integration.\",\n        ResponseFormat.PSEUDO_CODE: \"Verwende Pseudocode-Struktur f\u00fcr algorithmische oder logische Erkl\u00e4rungen.\",\n        ResponseFormat.CODE_STRUCTURE: \"Strukturiere wie Code mit Einr\u00fcckungen, Kommentaren und logischen Bl\u00f6cken.\"\n    }\n    return format_instructions.get(self.response_format, \"Standard-Formatierung.\")\n</code></pre> <code>get_length_instructions()</code> \u00b6 <p>Generiere L\u00e4ngen-spezifische Anweisungen</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def get_length_instructions(self) -&gt; str:\n    \"\"\"Generiere L\u00e4ngen-spezifische Anweisungen\"\"\"\n    length_instructions = {\n        TextLength.MINI_CHAT: \"Sehr kurze, pr\u00e4gnante Antworten (1-2 S\u00e4tze, max 50 W\u00f6rter). Chat-Style.\",\n        TextLength.CHAT_CONVERSATION: \"Moderate Gespr\u00e4chsl\u00e4nge (2-4 S\u00e4tze, 50-150 W\u00f6rter). Nat\u00fcrlicher Unterhaltungsstil.\",\n        TextLength.TABLE_CONVERSATION: \"Strukturierte, tabellarische Darstellung mit kompakten Erkl\u00e4rungen (100-250 W\u00f6rter).\",\n        TextLength.DETAILED_INDEPTH: \"Ausf\u00fchrliche, detaillierte Erkl\u00e4rungen (300-800 W\u00f6rter) mit Tiefe und Kontext.\",\n        TextLength.PHD_LEVEL: \"Akademische Tiefe mit umfassenden Erkl\u00e4rungen (800+ W\u00f6rter), Quellenangaben und Fachterminologie.\"\n    }\n    return length_instructions.get(self.text_length, \"Standard-L\u00e4nge.\")\n</code></pre> <code>LLMTask</code> <code>dataclass</code> \u00b6 <p>               Bases: <code>Task</code></p> <p>Spezialisierter Task f\u00fcr LLM-Aufrufe</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass LLMTask(Task):\n    \"\"\"Spezialisierter Task f\u00fcr LLM-Aufrufe\"\"\"\n    llm_config: Dict[str, Any] = field(default_factory=lambda: {\n        \"model_preference\": \"fast\",  # \"fast\" | \"complex\"\n        \"temperature\": 0.7,\n        \"max_tokens\": 1024\n    })\n    prompt_template: str = \"\"\n    context_keys: List[str] = field(default_factory=list)  # Keys aus shared state\n    output_schema: Optional[Dict] = None  # JSON Schema f\u00fcr Validierung\n</code></pre> <code>LLMToolNode</code> \u00b6 <p>               Bases: <code>AsyncNode</code></p> <p>Enhanced LLM tool with automatic tool calling and agent loop integration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@with_progress_tracking\nclass LLMToolNode(AsyncNode):\n    \"\"\"Enhanced LLM tool with automatic tool calling and agent loop integration\"\"\"\n\n    def __init__(self, model: str = None, max_tool_calls: int = 5, **kwargs):\n        super().__init__(**kwargs)\n        self.model = model or os.getenv(\"DEFAULTMODELST\", \"openrouter/qwen/qwen3-code\")\n        self.max_tool_calls = max_tool_calls\n        self.call_log = []\n\n    async def prep_async(self, shared):\n        context = shared.get(\"formatted_context\", {})\n        task_description = shared.get(\"current_task_description\", shared.get(\"current_query\", \"\"))\n\n        # Variable Manager integration\n        variable_manager = shared.get(\"variable_manager\")\n        agent_instance = shared.get(\"agent_instance\")\n\n        if shared.get(\"progress_tracker\"):\n            await shared.get(\"progress_tracker\").emit_event(ProgressEvent(\n                event_type=\"llm_call\",\n                timestamp=time.time(),\n                node_name=\"LLMToolNode\",\n                status=NodeStatus.STARTING,\n                llm_model=\"auto\",\n                session_id=shared.get(\"session_id\"),\n            ))\n\n        return {\n            \"task_description\": task_description,\n            \"context\": context,\n            \"session_id\": shared.get(\"session_id\"),\n            \"variable_manager\": variable_manager,\n            \"agent_instance\": agent_instance,\n            \"available_tools\": shared.get(\"available_tools\", []),\n            \"tool_capabilities\": shared.get(\"tool_capabilities\", {}),\n            \"persona_config\": shared.get(\"persona_config\"),\n            \"base_system_message\": variable_manager.format_text(agent_instance.amd.get_system_message_with_persona()),\n            \"recent_interaction\": context.get(\"recent_interaction\", \"\"),\n            \"session_summary\": context.get(\"session_summary\", \"\"),\n            \"task_context\": context.get(\"task_context\", \"\"),\n            \"fast_llm_model\": shared.get(\"fast_llm_model\"),\n            \"complex_llm_model\": shared.get(\"complex_llm_model\"),\n            \"progress_tracker\": shared.get(\"progress_tracker\"),\n            \"tool_call_count\": 0\n        }\n\n    async def exec_async(self, prep_res):\n        \"\"\"Main execution with tool calling loop\"\"\"\n        if not LITELLM_AVAILABLE:\n            return await self._fallback_response(prep_res)\n\n        progress_tracker = prep_res.get(\"progress_tracker\")\n\n        conversation_history = []\n        tool_call_count = 0\n        final_response = None\n        total_llm_calls = 0\n        total_cost = 0.0\n        total_tokens = 0\n\n        # Initial system message with tool awareness\n        system_message = self._build_tool_aware_system_message(prep_res)\n\n        # Initial user prompt with variable resolution\n        initial_prompt = self._build_context_aware_prompt(prep_res)\n        conversation_history.append({\"role\": \"user\", \"content\":  prep_res[\"variable_manager\"].format_text(initial_prompt)})\n        runs = 0\n        while tool_call_count &lt; self.max_tool_calls:\n            runs += 1\n            # Get LLM response\n            messages = [{\"role\": \"system\", \"content\": system_message}] + conversation_history\n\n            model_to_use = self._select_optimal_model(prep_res[\"task_description\"], prep_res)\n\n            llm_start = time.perf_counter()\n            if progress_tracker:\n                await progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"llm_call\",\n                    timestamp=time.time(),\n                    node_name=\"LLMToolNode\",\n                    status=NodeStatus.RUNNING,\n                    llm_model=model_to_use,\n                    session_id=prep_res.get(\"session_id\"),\n                    metadata={\n                        \"call_number\": total_llm_calls + 1,\n                        \"conversation_turns\": len(conversation_history),\n                        \"system_message_length\": len(system_message),\n                    }\n                ))\n            try:\n                response = await litellm.acompletion(\n                    model=model_to_use,\n                    messages=messages,\n                    temperature=0.7,\n                    max_tokens=2048\n                )\n\n                llm_duration = time.perf_counter() - llm_start\n                total_llm_calls += 1\n\n                # Extract token usage and calculate cost\n                usage = response.usage\n                input_tokens = usage.prompt_tokens if usage else 0\n                output_tokens = usage.completion_tokens if usage else 0\n                total_tokens_call = usage.total_tokens if usage else 0\n\n                # Calculate cost (simplified)\n                call_cost = progress_tracker.calculate_llm_cost(model_to_use, input_tokens,\n                                                                output_tokens) if progress_tracker else 0.0\n                total_cost += call_cost\n                total_tokens += total_tokens_call\n\n                llm_response = response.choices[0].message.content\n                if not llm_response:\n                    final_response = \"I encountered an error while processing your request.\"\n                    break\n\n\n                # Check for tool calls\n                tool_calls = self._extract_tool_calls(llm_response)\n\n                llm_response = prep_res[\"variable_manager\"].format_text(llm_response)\n                conversation_history.append({\"role\": \"assistant\", \"content\": llm_response})\n\n                # Track successful LLM call\n                if progress_tracker:\n                    await progress_tracker.emit_event(ProgressEvent(\n                        event_type=\"llm_call\",\n                        timestamp=time.time(),\n                        node_name=\"LLMToolNode\",\n                        llm_model=model_to_use,\n                        status=NodeStatus.COMPLETED,\n                        llm_prompt_tokens=input_tokens,\n                        llm_completion_tokens=output_tokens,\n                        llm_total_tokens=total_tokens_call,\n                        llm_cost=call_cost,\n                        llm_duration=llm_duration,\n                        llm_temperature=0.7,\n                        session_id=prep_res.get(\"session_id\"),\n                        metadata={\n                            \"success\": True,\n                            \"response_length\": len(llm_response),\n                            \"call_number\": total_llm_calls,\n                            \"response\": llm_response,\n                        }\n                    ))\n\n\n                if not tool_calls:\n                    # No more tool calls, this is the final response\n                    final_response = llm_response\n                    if not (runs == 1 and '{{' in llm_response):\n                        break\n\n                # Execute tool calls\n                tool_results = await self._execute_tool_calls(tool_calls, prep_res)\n                tool_call_count += len(tool_calls)\n\n                # Add tool results to conversation\n                tool_results_text = self._format_tool_results(tool_results)\n                conversation_history.append({\"role\": \"user\",\n                                             \"content\": f\"Tool results:\\n{tool_results_text}\\n\\nPlease continue or provide your final response.\"})\n\n                # Update variable manager with tool results\n                self._update_variables_with_results(tool_results, prep_res[\"variable_manager\"])\n\n            except Exception as e:\n                llm_duration = time.perf_counter() - llm_start\n\n                if progress_tracker:\n                    await progress_tracker.emit_event(ProgressEvent(\n                        event_type=\"error\",\n                        timestamp=time.time(),\n                        node_name=\"LLMToolNode\",\n                        status=NodeStatus.FAILED,\n                        llm_model=model_to_use,\n                        llm_duration=llm_duration,\n                        session_id=prep_res.get(\"session_id\"),\n                        metadata={\"error\": str(e), \"call_number\": total_llm_calls + 1}\n                    ))\n                logger.error(f\"LLM tool execution failed: {e}\")\n                final_response = f\"I encountered an error while processing: {str(e)}\"\n                import traceback\n                print(traceback.format_exc())\n                break\n\n\n        # Apply persona if needed\n        if progress_tracker:\n            await progress_tracker.emit_event(ProgressEvent(\n                event_type=\"success\",\n                timestamp=time.time(),\n                node_name=\"LLMToolNode\",\n                status=NodeStatus.COMPLETED,\n                success=True,\n                llm_model=\"auto\",\n                session_id=prep_res.get(\"session_id\"),\n            ))\n        return {\n            \"success\": True,\n            \"final_response\": final_response or \"I was unable to complete the request.\",\n            \"tool_calls_made\": tool_call_count,\n            \"conversation_history\": conversation_history,\n            \"model_used\": model_to_use,\n            \"llm_statistics\": {\n                \"total_calls\": total_llm_calls,\n                \"total_cost\": total_cost,\n                \"total_tokens\": total_tokens\n            }\n        }\n\n    def _build_tool_aware_system_message(self, prep_res: Dict) -&gt; str:\n        \"\"\"Build a unified intelligent, tool-aware system message with context and relevance analysis.\"\"\"\n\n        # Base system message\n        base_message = prep_res.get(\"base_system_message\", \"You are a helpful AI assistant.\")\n        available_tools = prep_res.get(\"available_tools\", [])\n        tool_capabilities = prep_res.get(\"tool_capabilities\", {})\n        variable_manager = prep_res.get(\"variable_manager\")\n        context = prep_res.get(\"context\", {})\n        agent_instance = prep_res.get(\"agent_instance\")\n        query = prep_res.get('task_description', '').lower()\n\n        # --- Part 1: List available tools &amp; capabilities ---\n        if available_tools:\n            base_message += f\"\\n\\n## Available Tools\\nYou have access to these tools: {', '.join(available_tools)}\\n\"\n            base_message += \"Results will be stored to results.{tool_name}.data\"\n\n            for tool_name in available_tools:\n                if tool_name in tool_capabilities:\n                    cap = tool_capabilities[tool_name]\n                    base_message += f\"\\n**{tool_name}**: {cap.get('primary_function', 'No description')}\"\n                    use_cases = cap.get('use_cases', [])\n                    if use_cases:\n                        base_message += f\"\\n  Use cases: {', '.join(use_cases[:3])}\"\n\n            base_message += \"\\n\\n## Tool Usage\\nTo use tools, respond with:\\nTOOL_CALL: tool_name(arg1='value1', arg2='value2')\\nYou can make multiple tool calls in one response.\"\n\n        # --- Part 2: Add variable context ---\n        if variable_manager:\n            var_context = variable_manager.get_llm_variable_context()\n            if var_context:\n                base_message += f\"\\n\\n## Variable Context\\n{var_context}\"\n\n        # --- Part 3: Intelligent tool analysis ---\n        if not agent_instance or not hasattr(agent_instance, '_tool_capabilities'):\n            return base_message + \"\\n\\n\u26a0 No intelligent tool analysis available.\"\n\n        capabilities = agent_instance._tool_capabilities\n        analysis_parts = [\"\\n\\n## Intelligent Tool Analysis\"]\n\n        for tool_name, cap in capabilities.items():\n            analysis_parts.append(f\"\\n**{tool_name}{cap.get('args_schema', '()')}:**\")\n            analysis_parts.append(f\"- Function: {cap.get('primary_function', 'Unknown')}\")\n\n            # Calculate relevance score\n            relevance_score = self._calculate_tool_relevance(query, cap)\n            analysis_parts.append(f\"- Query relevance: {relevance_score:.2f}\")\n\n            if relevance_score &gt; 0.65:\n                analysis_parts.append(\"- \u2b50 HIGHLY RELEVANT - SHOULD USE THIS TOOL!\")\n\n            # Trigger phrase matching\n            triggers = cap.get('trigger_phrases', [])\n            matched_triggers = [t for t in triggers if t.lower() in query]\n            if matched_triggers:\n                analysis_parts.append(f\"- Matched triggers: {matched_triggers}\")\n\n            # Show top use cases\n            use_cases = cap.get('use_cases', [])[:3]\n            if use_cases:\n                analysis_parts.append(f\"- Use cases: {', '.join(use_cases)}\")\n\n        # Combine everything into a final message\n        return base_message + \"\\n\"+ \"\\n\".join(analysis_parts)\n\n    def _calculate_tool_relevance(self, query: str, capabilities: Dict) -&gt; float:\n        \"\"\"Calculate how relevant a tool is to the current query\"\"\"\n\n        query_words = set(query.lower().split())\n\n        # Check trigger phrases\n        trigger_score = 0.0\n        triggers = capabilities.get('trigger_phrases', [])\n        for trigger in triggers:\n            trigger_words = set(trigger.lower().split())\n            if trigger_words.intersection(query_words):\n                trigger_score += 0.04\n        # Check confidence triggers if available\n        conf_triggers = capabilities.get('confidence_triggers', {})\n        for phrase, confidence in conf_triggers.items():\n            if phrase.lower() in query:\n                trigger_score += confidence/10\n        # Check indirect connections\n        indirect = capabilities.get('indirect_connections', [])\n        for connection in indirect:\n            connection_words = set(connection.lower().split())\n            if connection_words.intersection(query_words):\n                trigger_score += 0.02\n        return min(1.0, trigger_score)\n\n    def _extract_tool_calls(self, text: str) -&gt; List[Dict]:\n        \"\"\"Extract tool calls from LLM response\"\"\"\n        import re\n\n        tool_calls = []\n        pattern = r'TOOL_CALL:\\s*(\\w+)\\((.*?)\\)'\n\n        matches = re.findall(pattern, text, re.MULTILINE)\n\n        for tool_name, args_str in matches:\n            try:\n                # Parse arguments\n                args = self._parse_tool_arguments(args_str)\n                tool_calls.append({\n                    \"tool_name\": tool_name,\n                    \"arguments\": args\n                })\n            except Exception as e:\n                logger.warning(f\"Failed to parse tool call {tool_name}: {e}\")\n\n        return tool_calls\n\n    def _parse_tool_arguments(self, args_str: str) -&gt; Dict:\n        \"\"\"Parse tool arguments from string\"\"\"\n        import ast\n        # Try to evaluate as Python dict\n        if args_str.strip():\n            # Wrap in dict if not already\n            if args_str.strip().startswith('{'):\n                try:\n                    return ast.literal_eval(args_str)\n                except:\n                    pass\n        # Fallback: simple key=value parsing\n        args = {}\n        for pair in args_str.split(','):\n            if '=' in pair:\n                key, value = pair.split('=', 1)\n                key = key.strip().strip(\"'\")\n                value = value.strip().strip(\"'\")\n                args[key] = value\n        return args\n\n    def _select_optimal_model(self, task_description: str, prep_res: Dict) -&gt; str:\n        \"\"\"Select optimal model based on task complexity and available resources\"\"\"\n        complexity_score = self._estimate_task_complexity(task_description, prep_res)\n        if complexity_score &gt; 0.7:\n            return prep_res.get(\"complex_llm_model\", \"openrouter/openai/gpt-4o\")\n        else:\n            return prep_res.get(\"fast_llm_model\", \"openrouter/anthropic/claude-3-haiku\")\n\n    def _estimate_task_complexity(self, task_description: str, prep_res: Dict) -&gt; float:\n        \"\"\"Estimate task complexity based on description, length, and available tools\"\"\"\n        # Simple heuristic: length + keyword matching + tool availability\n        description_length_score = min(len(task_description) / 500, 1.0)  # cap at 1.0\n        keywords = [\"analyze\", \"research\", \"generate\", \"simulate\", \"complex\", \"deep\", \"strategy\"]\n        keyword_score = sum(1 for k in keywords if k in task_description.lower()) / len(keywords)\n        tool_score = min(len(prep_res.get(\"available_tools\", [])) / 10, 1.0)\n\n        # Weighted sum\n        complexity_score = (0.5 * description_length_score) + (0.3 * keyword_score) + (0.2 * tool_score)\n        return round(complexity_score, 2)\n\n    async def _fallback_response(self, prep_res: Dict) -&gt; Dict:\n        \"\"\"Fallback response if LiteLLM is not available\"\"\"\n        logger.warning(\"LiteLLM not available \u2014 using fallback response.\")\n        return {\n            \"success\": False,\n            \"final_response\": (\n                \"I'm unable to process this request fully right now because the LLM interface \"\n                \"is not available. Please try again later or check system configuration.\"\n            ),\n            \"tool_calls_made\": 0,\n            \"conversation_history\": [],\n            \"model_used\": None\n        }\n\n    async def _execute_tool_calls(self, tool_calls: List[Dict], prep_res: Dict) -&gt; List[Dict]:\n        \"\"\"Execute tool calls via agent\"\"\"\n        agent_instance = prep_res.get(\"agent_instance\")\n        variable_manager = prep_res.get(\"variable_manager\")\n        progress_tracker = prep_res.get(\"progress_tracker\")\n\n        results = []\n\n        for tool_call in tool_calls:\n            tool_name = tool_call[\"tool_name\"]\n            arguments = tool_call[\"arguments\"]\n\n            # Start tool tracking\n            tool_start = time.perf_counter()\n\n            if progress_tracker:\n                await progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"tool_call\",\n                    timestamp=time.time(),\n                    status=NodeStatus.RUNNING,\n                    node_name=\"LLMToolNode\",\n                    tool_name=tool_name,\n                    tool_args=arguments,\n                    session_id=prep_res.get(\"session_id\"),\n                    metadata={\"tool_call_initiated\": True}\n                ))\n\n            try:\n                # Resolve variables in arguments\n                if variable_manager:\n                    resolved_args = {}\n                    for key, value in arguments.items():\n                        if isinstance(value, str):\n                            resolved_args[key] = variable_manager.format_text(value)\n                        else:\n                            resolved_args[key] = value\n                else:\n                    resolved_args = arguments\n\n                # Execute via agent\n                result = await agent_instance.arun_function(tool_name, **resolved_args)\n                tool_duration = time.perf_counter() - tool_start\n                variable_manager.set(f\"results.{tool_name}.data\", result)\n                if progress_tracker:\n                    await progress_tracker.emit_event(ProgressEvent(\n                        event_type=\"tool_call\",\n                        timestamp=time.time(),\n                        node_name=\"LLMToolNode\",\n                        status=NodeStatus.COMPLETED,\n                        tool_name=tool_name,\n                        tool_args=resolved_args,\n                        tool_result=result,\n                        tool_duration=tool_duration,\n                        tool_success=True,\n                        session_id=prep_res.get(\"session_id\"),\n                        metadata={\n                            \"result_type\": type(result).__name__,\n                            \"result_length\": len(str(result))\n                        }\n                    ))\n                results.append({\n                    \"tool_name\": tool_name,\n                    \"arguments\": resolved_args,\n                    \"success\": True,\n                    \"result\": result\n                })\n\n            except Exception as e:\n                tool_duration = time.perf_counter() - tool_start\n\n                if progress_tracker:\n                    await progress_tracker.emit_event(ProgressEvent(\n                        event_type=\"tool_call\",\n                        timestamp=time.time(),\n                        node_name=\"LLMToolNode\",\n                        status=NodeStatus.FAILED,\n                        tool_name=tool_name,\n                        tool_args=arguments,\n                        tool_duration=tool_duration,\n                        tool_success=False,\n                        tool_error=str(e),\n                        session_id=prep_res.get(\"session_id\"),\n                        metadata={\"error_type\": type(e).__name__}\n                    ))\n                logger.error(f\"Tool execution failed {tool_name}: {e}\")\n                results.append({\n                    \"tool_name\": tool_name,\n                    \"arguments\": arguments,\n                    \"success\": False,\n                    \"error\": str(e)\n                })\n\n        return results\n\n    def _format_tool_results(self, results: List[Dict]) -&gt; str:\n        \"\"\"Format tool results for LLM\"\"\"\n        formatted = []\n\n        for result in results:\n            if result[\"success\"]:\n                formatted.append(f\"\u2713 {result['tool_name']}: {result['result']}\")\n            else:\n                formatted.append(f\"\u2717 {result['tool_name']}: ERROR - {result['error']}\")\n\n        return \"\\n\".join(formatted)\n\n    def _update_variables_with_results(self, results: List[Dict], variable_manager):\n        \"\"\"Update variable manager with tool results\"\"\"\n        if not variable_manager:\n            return\n\n        for i, result in enumerate(results):\n            if result[\"success\"]:\n                tool_name = result['tool_name']\n                result_data = result['result']\n\n                # FIXED: Store result in proper variable paths\n                variable_manager.set(f\"results.{tool_name}.data\", result_data)\n                variable_manager.set(f\"tools.{tool_name}.result\", result_data)\n\n                # Also store with index for multiple calls to same tool\n                var_key = f\"tool_result_{tool_name}_{i}\"\n                variable_manager.set(var_key, result_data)\n\n    def _build_context_aware_prompt(self, prep_res: Dict) -&gt; str:\n        \"\"\"Build context-aware prompt with variable resolution\"\"\"\n        variable_manager = prep_res.get(\"variable_manager\")\n\n        prompt_parts = []\n\n        # Add context sections\n        recent_interaction = prep_res.get(\"recent_interaction\", \"\")\n        session_summary = prep_res.get(\"session_summary\", \"\")\n        task_context = prep_res.get(\"task_context\", \"\")\n\n        if recent_interaction:\n            prompt_parts.append(recent_interaction)\n        if session_summary:\n            prompt_parts.append(session_summary)\n        if task_context:\n            prompt_parts.append(task_context)\n\n        # Add main task\n        task_description = prep_res.get(\"task_description\", \"\")\n        if task_description:\n            prompt_parts.append(f\"## Current Request\\n{task_description}\")\n\n        # Variable suggestions\n        if variable_manager and task_description:\n            suggestions = variable_manager.get_variable_suggestions(task_description)\n            if suggestions:\n                prompt_parts.append(f\"## Available Variables\\nYou can use: {', '.join(suggestions)}\")\n\n        # Final variable resolution\n        final_prompt = \"\\n\\n\".join(prompt_parts)\n        if variable_manager:\n            final_prompt = variable_manager.format_text(final_prompt)\n\n        return final_prompt\n\n    async def post_async(self, shared, prep_res, exec_res):\n        shared[\"current_response\"] = exec_res.get(\"final_response\", \"Task completed.\")\n        shared[\"tool_calls_made\"] = exec_res.get(\"tool_calls_made\", 0)\n        shared[\"llm_tool_conversation\"] = exec_res.get(\"conversation_history\", [])\n        shared[\"synthesized_response\"] = {\"synthesized_response\":exec_res.get(\"final_response\", \"Task completed.\"),\n                                          \"confidence\": (0.7 if exec_res.get(\"model_used\") == prep_res.get(\"complex_llm_model\") else 0.6) if exec_res.get(\"success\", False) else 0,\n                                          \"metadata\": exec_res.get(\"metadata\", {\"model_used\": exec_res.get(\"model_used\")}),\n                                          \"synthesis_method\": \"llm_tool\"}\n        return \"llm_tool_complete\"\n</code></pre> <code>exec_async(prep_res)</code> <code>async</code> \u00b6 <p>Main execution with tool calling loop</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def exec_async(self, prep_res):\n    \"\"\"Main execution with tool calling loop\"\"\"\n    if not LITELLM_AVAILABLE:\n        return await self._fallback_response(prep_res)\n\n    progress_tracker = prep_res.get(\"progress_tracker\")\n\n    conversation_history = []\n    tool_call_count = 0\n    final_response = None\n    total_llm_calls = 0\n    total_cost = 0.0\n    total_tokens = 0\n\n    # Initial system message with tool awareness\n    system_message = self._build_tool_aware_system_message(prep_res)\n\n    # Initial user prompt with variable resolution\n    initial_prompt = self._build_context_aware_prompt(prep_res)\n    conversation_history.append({\"role\": \"user\", \"content\":  prep_res[\"variable_manager\"].format_text(initial_prompt)})\n    runs = 0\n    while tool_call_count &lt; self.max_tool_calls:\n        runs += 1\n        # Get LLM response\n        messages = [{\"role\": \"system\", \"content\": system_message}] + conversation_history\n\n        model_to_use = self._select_optimal_model(prep_res[\"task_description\"], prep_res)\n\n        llm_start = time.perf_counter()\n        if progress_tracker:\n            await progress_tracker.emit_event(ProgressEvent(\n                event_type=\"llm_call\",\n                timestamp=time.time(),\n                node_name=\"LLMToolNode\",\n                status=NodeStatus.RUNNING,\n                llm_model=model_to_use,\n                session_id=prep_res.get(\"session_id\"),\n                metadata={\n                    \"call_number\": total_llm_calls + 1,\n                    \"conversation_turns\": len(conversation_history),\n                    \"system_message_length\": len(system_message),\n                }\n            ))\n        try:\n            response = await litellm.acompletion(\n                model=model_to_use,\n                messages=messages,\n                temperature=0.7,\n                max_tokens=2048\n            )\n\n            llm_duration = time.perf_counter() - llm_start\n            total_llm_calls += 1\n\n            # Extract token usage and calculate cost\n            usage = response.usage\n            input_tokens = usage.prompt_tokens if usage else 0\n            output_tokens = usage.completion_tokens if usage else 0\n            total_tokens_call = usage.total_tokens if usage else 0\n\n            # Calculate cost (simplified)\n            call_cost = progress_tracker.calculate_llm_cost(model_to_use, input_tokens,\n                                                            output_tokens) if progress_tracker else 0.0\n            total_cost += call_cost\n            total_tokens += total_tokens_call\n\n            llm_response = response.choices[0].message.content\n            if not llm_response:\n                final_response = \"I encountered an error while processing your request.\"\n                break\n\n\n            # Check for tool calls\n            tool_calls = self._extract_tool_calls(llm_response)\n\n            llm_response = prep_res[\"variable_manager\"].format_text(llm_response)\n            conversation_history.append({\"role\": \"assistant\", \"content\": llm_response})\n\n            # Track successful LLM call\n            if progress_tracker:\n                await progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"llm_call\",\n                    timestamp=time.time(),\n                    node_name=\"LLMToolNode\",\n                    llm_model=model_to_use,\n                    status=NodeStatus.COMPLETED,\n                    llm_prompt_tokens=input_tokens,\n                    llm_completion_tokens=output_tokens,\n                    llm_total_tokens=total_tokens_call,\n                    llm_cost=call_cost,\n                    llm_duration=llm_duration,\n                    llm_temperature=0.7,\n                    session_id=prep_res.get(\"session_id\"),\n                    metadata={\n                        \"success\": True,\n                        \"response_length\": len(llm_response),\n                        \"call_number\": total_llm_calls,\n                        \"response\": llm_response,\n                    }\n                ))\n\n\n            if not tool_calls:\n                # No more tool calls, this is the final response\n                final_response = llm_response\n                if not (runs == 1 and '{{' in llm_response):\n                    break\n\n            # Execute tool calls\n            tool_results = await self._execute_tool_calls(tool_calls, prep_res)\n            tool_call_count += len(tool_calls)\n\n            # Add tool results to conversation\n            tool_results_text = self._format_tool_results(tool_results)\n            conversation_history.append({\"role\": \"user\",\n                                         \"content\": f\"Tool results:\\n{tool_results_text}\\n\\nPlease continue or provide your final response.\"})\n\n            # Update variable manager with tool results\n            self._update_variables_with_results(tool_results, prep_res[\"variable_manager\"])\n\n        except Exception as e:\n            llm_duration = time.perf_counter() - llm_start\n\n            if progress_tracker:\n                await progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"error\",\n                    timestamp=time.time(),\n                    node_name=\"LLMToolNode\",\n                    status=NodeStatus.FAILED,\n                    llm_model=model_to_use,\n                    llm_duration=llm_duration,\n                    session_id=prep_res.get(\"session_id\"),\n                    metadata={\"error\": str(e), \"call_number\": total_llm_calls + 1}\n                ))\n            logger.error(f\"LLM tool execution failed: {e}\")\n            final_response = f\"I encountered an error while processing: {str(e)}\"\n            import traceback\n            print(traceback.format_exc())\n            break\n\n\n    # Apply persona if needed\n    if progress_tracker:\n        await progress_tracker.emit_event(ProgressEvent(\n            event_type=\"success\",\n            timestamp=time.time(),\n            node_name=\"LLMToolNode\",\n            status=NodeStatus.COMPLETED,\n            success=True,\n            llm_model=\"auto\",\n            session_id=prep_res.get(\"session_id\"),\n        ))\n    return {\n        \"success\": True,\n        \"final_response\": final_response or \"I was unable to complete the request.\",\n        \"tool_calls_made\": tool_call_count,\n        \"conversation_history\": conversation_history,\n        \"model_used\": model_to_use,\n        \"llm_statistics\": {\n            \"total_calls\": total_llm_calls,\n            \"total_cost\": total_cost,\n            \"total_tokens\": total_tokens\n        }\n    }\n</code></pre> <code>ParallelTaskBatch</code> \u00b6 <p>               Bases: <code>BatchNode</code></p> <p>Batch node for parallel task execution</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@with_progress_tracking\nclass ParallelTaskBatch(BatchNode):\n    \"\"\"Batch node for parallel task execution\"\"\"\n    def prep(self, shared):\n        ready_tasks = []\n        for task_id, task in shared.get(\"tasks\", {}).items():\n            if task.status == \"pending\":\n                ready_tasks.append(task)\n        return ready_tasks\n\n    def exec(self, task: Task):\n        # This runs in parallel for each task\n        try:\n            # Simulate task execution\n            time.sleep(0.1)\n            result = f\"Batch result for task: {task.description}\"\n            return {\n                \"task_id\": task.id,\n                \"status\": \"completed\",\n                \"result\": result\n            }\n        except Exception as e:\n            return {\n                \"task_id\": task.id,\n                \"status\": \"failed\",\n                \"error\": str(e)\n            }\n\n    def post(self, shared, prep_res, exec_res_list):\n        # Process all batch results\n        completed_count = 0\n        failed_count = 0\n\n        for result in exec_res_list:\n            task_id = result[\"task_id\"]\n            if task_id in shared[\"tasks\"]:\n                task = shared[\"tasks\"][task_id]\n                task.status = result[\"status\"]\n                if result[\"status\"] == \"completed\":\n                    task.result = result[\"result\"]\n                    completed_count += 1\n                else:\n                    task.error = result.get(\"error\", \"Unknown error\")\n                    failed_count += 1\n\n        shared[\"batch_execution_stats\"] = {\n            \"completed\": completed_count,\n            \"failed\": failed_count,\n            \"total\": len(exec_res_list)\n        }\n\n        return \"batch_complete\"\n</code></pre> <code>PersonaConfig</code> <code>dataclass</code> \u00b6 Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass PersonaConfig:\n    name: str\n    style: str = \"professional\"\n    personality_traits: List[str] = field(default_factory=lambda: [\"helpful\", \"concise\"])\n    tone: str = \"friendly\"\n    response_format: str = \"direct\"\n    custom_instructions: str = \"\"\n\n    format_config: Optional[FormatConfig] = None\n\n    apply_method: str = \"system_prompt\"  # \"system_prompt\" | \"post_process\" | \"both\"\n    integration_level: str = \"light\"  # \"light\" | \"medium\" | \"heavy\"\n\n    def to_system_prompt_addition(self) -&gt; str:\n        \"\"\"Convert persona to system prompt addition with format integration\"\"\"\n        if self.apply_method in [\"system_prompt\", \"both\"]:\n            additions = []\n            additions.append(f\"You are {self.name}.\")\n            additions.append(f\"Your communication style is {self.style} with a {self.tone} tone.\")\n\n            if self.personality_traits:\n                traits_str = \", \".join(self.personality_traits)\n                additions.append(f\"Your key traits are: {traits_str}.\")\n\n            if self.custom_instructions:\n                additions.append(self.custom_instructions)\n\n            # Format-spezifische Anweisungen hinzuf\u00fcgen\n            if self.format_config:\n                additions.append(\"\\n\" + self.format_config.get_combined_instructions())\n\n            return \" \".join(additions)\n        return \"\"\n\n    def update_format(self, response_format: ResponseFormat|str, text_length: TextLength|str, custom_instructions: str = \"\"):\n        \"\"\"Dynamische Format-Aktualisierung\"\"\"\n        try:\n            format_enum = ResponseFormat(response_format) if isinstance(response_format, str) else response_format\n            length_enum = TextLength(text_length) if isinstance(text_length, str) else text_length\n\n            if not self.format_config:\n                self.format_config = FormatConfig()\n\n            self.format_config.response_format = format_enum\n            self.format_config.text_length = length_enum\n\n            if custom_instructions:\n                self.format_config.custom_instructions = custom_instructions\n\n\n        except ValueError as e:\n            raise ValueError(f\"Invalid format '{response_format}' or length '{text_length}'\")\n\n    def should_post_process(self) -&gt; bool:\n        \"\"\"Check if post-processing should be applied\"\"\"\n        return self.apply_method in [\"post_process\", \"both\"]\n</code></pre> <code>should_post_process()</code> \u00b6 <p>Check if post-processing should be applied</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def should_post_process(self) -&gt; bool:\n    \"\"\"Check if post-processing should be applied\"\"\"\n    return self.apply_method in [\"post_process\", \"both\"]\n</code></pre> <code>to_system_prompt_addition()</code> \u00b6 <p>Convert persona to system prompt addition with format integration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def to_system_prompt_addition(self) -&gt; str:\n    \"\"\"Convert persona to system prompt addition with format integration\"\"\"\n    if self.apply_method in [\"system_prompt\", \"both\"]:\n        additions = []\n        additions.append(f\"You are {self.name}.\")\n        additions.append(f\"Your communication style is {self.style} with a {self.tone} tone.\")\n\n        if self.personality_traits:\n            traits_str = \", \".join(self.personality_traits)\n            additions.append(f\"Your key traits are: {traits_str}.\")\n\n        if self.custom_instructions:\n            additions.append(self.custom_instructions)\n\n        # Format-spezifische Anweisungen hinzuf\u00fcgen\n        if self.format_config:\n            additions.append(\"\\n\" + self.format_config.get_combined_instructions())\n\n        return \" \".join(additions)\n    return \"\"\n</code></pre> <code>update_format(response_format, text_length, custom_instructions='')</code> \u00b6 <p>Dynamische Format-Aktualisierung</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def update_format(self, response_format: ResponseFormat|str, text_length: TextLength|str, custom_instructions: str = \"\"):\n    \"\"\"Dynamische Format-Aktualisierung\"\"\"\n    try:\n        format_enum = ResponseFormat(response_format) if isinstance(response_format, str) else response_format\n        length_enum = TextLength(text_length) if isinstance(text_length, str) else text_length\n\n        if not self.format_config:\n            self.format_config = FormatConfig()\n\n        self.format_config.response_format = format_enum\n        self.format_config.text_length = length_enum\n\n        if custom_instructions:\n            self.format_config.custom_instructions = custom_instructions\n\n\n    except ValueError as e:\n        raise ValueError(f\"Invalid format '{response_format}' or length '{text_length}'\")\n</code></pre> <code>PlanReflectorNode</code> \u00b6 <p>               Bases: <code>AsyncNode</code></p> <p>Unified Adaptive Plan Reflection and Dynamic Adjustment</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@with_progress_tracking\nclass PlanReflectorNode(AsyncNode):\n    \"\"\"Unified Adaptive Plan Reflection and Dynamic Adjustment\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    async def prep_async(self, shared):\n        current_plan = shared.get(\"current_plan\")\n        tasks = shared.get(\"tasks\", {})\n        results_store = shared.get(\"results_store\", {})\n\n        # Standard reflection on recently completed tasks\n        recently_completed = [\n            task for task in (current_plan.tasks if current_plan else [])\n            if task.status == \"completed\" and\n               task.completed_at and\n               (datetime.now() - task.completed_at).seconds &lt; 300  # Last 5 minutes\n        ]\n\n        # Dynamic replanning context (triggered by DecisionTasks)\n        needs_dynamic_replan = shared.get(\"needs_dynamic_replan\", False)\n        replan_context = shared.get(\"replan_context\", {})\n\n        # Plan append context (triggered by DecisionTasks)\n        needs_plan_append = shared.get(\"needs_plan_append\", False)\n        append_context = shared.get(\"append_context\", {})\n\n        return {\n            \"current_plan\": current_plan,\n            \"tasks\": tasks,\n            \"results_store\": results_store,\n            \"recently_completed\": recently_completed,\n            \"fast_llm_model\": shared.get(\"fast_llm_model\"),\n            \"complex_llm_model\": shared.get(\"complex_llm_model\"),\n            \"agent_instance\": shared.get(\"agent_instance\"),\n            \"original_query\": shared.get(\"current_query\", \"\"),\n            # Dynamic planning contexts\n            \"needs_dynamic_replan\": needs_dynamic_replan,\n            \"replan_context\": replan_context,\n            \"needs_plan_append\": needs_plan_append,\n            \"append_context\": append_context,\n            # Additional context\n            \"plan_adaptations\": shared.get(\"plan_adaptations\", 0),\n            \"execution_performance\": shared.get(\"executor_performance\", {}),\n            \"agent_instance\": shared.get(\"agent_instance\")\n        }\n\n    async def exec_async(self, prep_res):\n        \"\"\"Unified execution handling all reflection scenarios\"\"\"\n\n        # Priority 1: Handle dynamic replanning (triggered by DecisionTask)\n        if prep_res[\"needs_dynamic_replan\"]:\n            logger.info(\"Handling dynamic replan request\")\n            return await self._handle_dynamic_replan(prep_res)\n\n        # Priority 2: Handle plan append (triggered by DecisionTask)\n        if prep_res[\"needs_plan_append\"]:\n            logger.info(\"Handling plan append request\")\n            return await self._handle_plan_append(prep_res)\n\n        # Priority 3: Standard reflection on completed tasks\n        if prep_res[\"recently_completed\"]:\n            logger.info(\"Analyzing recently completed tasks\")\n            return await self._handle_standard_reflection(prep_res)\n\n        # Priority 4: Check execution performance issues\n        performance = prep_res[\"execution_performance\"]\n        if performance.get(\"status\") == \"error\" or performance.get(\"success_rate\", 1.0) &lt; 0.3:\n            logger.info(\"Handling performance issues\")\n            return await self._handle_performance_issues(prep_res)\n\n        # Default: Continue normal execution\n        return {\n            \"action\": \"CONTINUE\",\n            \"reason\": \"No reflection triggers detected, continuing normal execution\"\n        }\n\n    async def _handle_dynamic_replan(self, prep_res: Dict) -&gt; Dict[str, Any]:\n        \"\"\"Enhanced dynamic replanning with extensive context validation\"\"\"\n\n        replan_context = prep_res.get(\"replan_context\", {})\n        current_plan = prep_res[\"current_plan\"]\n\n        # Find the triggering task and validate replan necessity\n        triggering_task_id = replan_context.get(\"original_task\")\n        triggering_task = prep_res[\"tasks\"].get(triggering_task_id) if triggering_task_id else None\n\n        # Enhanced validation - check if replan is actually necessary\n        replan_justification = self._validate_replan_necessity(triggering_task, replan_context, prep_res)\n\n        if not replan_justification[\"is_necessary\"]:\n            logger.info(f\"Replan request rejected: {replan_justification['reason']}\")\n            return {\n                \"action\": \"CONTINUE\",\n                \"reason\": f\"Replan not necessary: {replan_justification['reason']}\",\n                \"alternative_action\": replan_justification.get(\"alternative\", \"continue_execution\")\n            }\n\n        # Extract comprehensive learned context\n        completed_tasks = [t for t in current_plan.tasks if prep_res[\"tasks\"][t.id].status == \"completed\"]\n        learned_context = self._extract_comprehensive_context(completed_tasks, prep_res)\n\n        # Build enhanced replan context with failure analysis\n        enhanced_replan_context = {\n            \"original_query\": prep_res.get(\"original_query\", \"\"),\n            \"new_goal\": replan_context.get(\"new_goal\", \"\"),\n            \"failure_reason\": replan_context.get(\"failure_reason\", \"\"),\n            \"learned_context\": learned_context,\n            \"completed_task_ids\": [t.id for t in completed_tasks],\n            \"triggering_decision\": replan_context.get(\"context\", \"\"),\n            \"avoid_approaches\": replan_context.get(\"failed_approaches\", []),\n            \"success_patterns\": replan_context.get(\"success_indicators\", []),\n            \"execution_history\": replan_context.get(\"execution_history\", \"\"),\n            \"variable_context\": self._build_variable_context_for_replanning(prep_res),\n            \"confidence_factors\": self._analyze_confidence_factors(prep_res),\n            \"dynamic_replan\": True,\n            \"justification\": replan_justification\n        }\n\n        logger.info(f\"Dynamic replan approved: {replan_context.get('new_goal', 'No goal specified')}\")\n\n        return {\n            \"action\": \"DYNAMIC_REPLAN_TRIGGERED\",\n            \"replan_context\": enhanced_replan_context,\n            \"reason\": f\"DecisionTask {triggering_task_id} triggered justified replanning\",\n            \"previous_plan_id\": current_plan.id,\n            \"preserve_completed\": True,\n            \"priority\": \"high\",  # Reduced from critical to high\n            \"confidence\": replan_justification[\"confidence\"]\n        }\n\n    def _validate_replan_necessity(self, triggering_task: Task, replan_context: Dict, prep_res: Dict) -&gt; Dict[str, Any]:\n        \"\"\"Validate whether replanning is actually necessary\"\"\"\n\n        validation = {\n            \"is_necessary\": False,\n            \"reason\": \"\",\n            \"confidence\": 0.0,\n            \"alternative\": \"continue_execution\"\n        }\n\n        # Check if we have genuine failure indicators\n        failure_reason = replan_context.get(\"failure_reason\", \"\")\n        new_goal = replan_context.get(\"new_goal\", \"\")\n\n        # Analyze completed tasks for alternative solutions\n        completed_count = len([t for t in prep_res[\"tasks\"].values() if t.status == \"completed\"])\n        failed_count = len([t for t in prep_res[\"tasks\"].values() if t.status == \"failed\"])\n\n        # Decision factors\n        has_alternatives = completed_count &gt; 0\n        failure_is_critical = \"critical\" in failure_reason.lower() or \"error\" in failure_reason.lower()\n        goal_is_specific = len(new_goal.split()) &gt; 3\n\n        # Conservative validation - only replan if really necessary\n        if failure_is_critical and not has_alternatives and goal_is_specific:\n            validation.update({\n                \"is_necessary\": True,\n                \"reason\": \"Critical failure with no alternatives and specific recovery goal\",\n                \"confidence\": 0.8\n            })\n        elif failed_count &gt; completed_count and goal_is_specific:\n            validation.update({\n                \"is_necessary\": True,\n                \"reason\": \"More failures than successes, specific recovery needed\",\n                \"confidence\": 0.6\n            })\n        else:\n            # Suggest alternatives instead of replanning\n            if has_alternatives:\n                validation.update({\n                    \"reason\": \"Existing results can be used instead of replanning\",\n                    \"alternative\": \"synthesize_from_existing\",\n                    \"confidence\": 0.7\n                })\n            else:\n                validation.update({\n                    \"reason\": \"Failure not critical enough to warrant full replan\",\n                    \"alternative\": \"continue_with_fallback\",\n                    \"confidence\": 0.5\n                })\n\n        return validation\n\n    def _extract_comprehensive_context(self, completed_tasks: List[Task], prep_res: Dict) -&gt; str:\n        \"\"\"Extract comprehensive context including variable state\"\"\"\n\n        context_parts = []\n        results_store = prep_res[\"results_store\"]\n\n        # Successful task results\n        for task in completed_tasks:\n            result_data = results_store.get(task.id, {})\n            if result_data.get(\"metadata\", {}).get(\"success\", False):\n                task_context = f\"Task {task.id} ({task.type}): {task.description}\"\n\n                data_preview = str(result_data.get(\"data\", \"\"))\n                if len(data_preview) &gt; 200:\n                    data_preview = data_preview[:200] + \"...\"\n\n                task_context += f\"\\nResult: {data_preview}\"\n\n                # Add verification if available\n                if hasattr(task, 'metadata') and task.metadata and \"verification\" in task.metadata:\n                    verification = task.metadata[\"verification\"]\n                    if isinstance(verification, dict):\n                        confidence = verification.get(\"confidence\", 0.0)\n                        task_context += f\"\\nVerification confidence: {confidence:.2f}\"\n\n                context_parts.append(task_context)\n\n        # Current variable state\n        if hasattr(prep_res.get(\"agent_instance\"), \"variable_manager\"):\n            vm = prep_res[\"agent_instance\"].variable_manager\n            var_summary = []\n\n            for scope_name, scope_data in vm.scopes.items():\n                if isinstance(scope_data, dict) and scope_data:\n                    var_summary.append(f\"{scope_name}: {len(scope_data)} variables\")\n\n            if var_summary:\n                context_parts.append(\"Variable state: \" + \", \".join(var_summary))\n\n        return \"\\n\\n\".join(context_parts) if context_parts else \"No significant context available\"\n\n    def _build_variable_context_for_replanning(self, prep_res: Dict) -&gt; Dict[str, Any]:\n        \"\"\"Build variable context specifically for replanning\"\"\"\n\n        variable_context = {}\n\n        agent_instance = prep_res.get(\"agent_instance\")\n        if agent_instance and hasattr(agent_instance, \"variable_manager\"):\n            vm = agent_instance.variable_manager\n\n            # Get key variables that might be useful for replanning\n            variable_context[\"available_scopes\"] = list(vm.scopes.keys())\n            variable_context[\"results_available\"] = len(vm.scopes.get(\"results\", {}))\n            variable_context[\"world_facts\"] = len(vm.scopes.get(\"world\", {}))\n\n            # Get user context\n            user_data = vm.scopes.get(\"user\", {})\n            if user_data:\n                variable_context[\"user_context\"] = {\n                    \"session\": user_data.get(\"session\", \"unknown\"),\n                    \"query\": user_data.get(\"query\", \"\")[:100] + \"...\" if len(\n                        user_data.get(\"query\", \"\")) &gt; 100 else user_data.get(\"query\", \"\")\n                }\n\n        return variable_context\n\n    def _analyze_confidence_factors(self, prep_res: Dict) -&gt; Dict[str, float]:\n        \"\"\"Analyze various confidence factors for replanning decision\"\"\"\n\n        confidence_factors = {}\n\n        # Task completion rate\n        tasks = prep_res[\"tasks\"]\n        if tasks:\n            completed = len([t for t in tasks.values() if t.status == \"completed\"])\n            total = len(tasks)\n            confidence_factors[\"completion_rate\"] = completed / total\n\n        # Result quality (based on verification scores)\n        results_store = prep_res[\"results_store\"]\n        verification_scores = []\n\n        for result_data in results_store.values():\n            if \"verification\" in result_data.get(\"metadata\", {}):\n                verification = result_data[\"metadata\"][\"verification\"]\n                if isinstance(verification, dict) and \"confidence\" in verification:\n                    verification_scores.append(verification[\"confidence\"])\n\n        if verification_scores:\n            confidence_factors[\"average_verification\"] = sum(verification_scores) / len(verification_scores)\n\n        # Information richness\n        total_result_length = sum(len(str(r.get(\"data\", \"\"))) for r in results_store.values())\n        confidence_factors[\"information_richness\"] = min(total_result_length / 1000, 1.0)  # Normalize\n\n        return confidence_factors\n\n    async def _handle_plan_append(self, prep_res: Dict) -&gt; Dict[str, Any]:\n        \"\"\"Handle plan append operation triggered by DecisionTask\"\"\"\n\n        append_context = prep_res.get(\"append_context\", {})\n        current_plan = prep_res[\"current_plan\"]\n\n        # Create append-specific planning context\n        append_planning_context = {\n            \"original_query\": prep_res.get(\"original_query\", \"\"),\n            \"append_goal\": append_context.get(\"new_goal\", \"\"),\n            \"extend_from_task\": append_context.get(\"extend_from\", \"\"),\n            \"completed_tasks\": [t.id for t in current_plan.tasks if prep_res[\"tasks\"][t.id].status == \"completed\"],\n            \"current_results\": self._summarize_current_results(prep_res),\n            \"append_operation\": True  # Flag for TaskPlannerNode\n        }\n\n        logger.info(f\"Plan append triggered: {append_context.get('new_goal', 'No goal specified')}\")\n\n        return {\n            \"action\": \"PLAN_APPEND_TRIGGERED\",\n            \"append_context\": append_planning_context,\n            \"reason\": f\"DecisionTask requested plan extension\",\n            \"current_plan_id\": current_plan.id,\n            \"preserve_existing\": True,\n            \"priority\": \"high\"\n        }\n\n    async def _handle_standard_reflection(self, prep_res: Dict) -&gt; Dict[str, Any]:\n        \"\"\"Handle standard reflection on recently completed tasks\"\"\"\n\n        # Analyze recent completions\n        reflection_result = await self._analyze_recent_completions(prep_res)\n\n        # Decide on next action using LLM\n        action_decision = await self._decide_next_action(reflection_result, prep_res)\n\n        # Execute the decided action\n        if action_decision[\"action\"] == \"ADAPT\":\n            adaptation_result = await self._adapt_plan(action_decision, prep_res)\n            return adaptation_result\n        elif action_decision[\"action\"] == \"REPLAN\":\n            replan_result = await self._trigger_standard_replanning(action_decision, prep_res)\n            return replan_result\n        elif action_decision[\"action\"] == \"HALT_FAILURE\":\n            return {\n                \"action\": \"HALT_FAILURE\",\n                \"reason\": action_decision.get(\"reasoning\", \"Critical failure detected\"),\n                \"halt_details\": action_decision,\n                \"priority\": \"critical\"\n            }\n        else:\n            return action_decision\n\n    async def _handle_performance_issues(self, prep_res: Dict) -&gt; Dict[str, Any]:\n        \"\"\"Handle execution performance issues\"\"\"\n\n        performance = prep_res[\"execution_performance\"]\n\n        if performance.get(\"success_rate\", 1.0) &lt; 0.3:\n            return {\n                \"action\": \"PERFORMANCE_REPLAN\",\n                \"reason\": f\"Low success rate: {performance.get('success_rate', 0.0):.1%}\",\n                \"performance_context\": {\n                    \"failed_tasks\": performance.get(\"failed_tasks\", 0),\n                    \"completed_tasks\": performance.get(\"completed_tasks\", 0),\n                    \"strategy_used\": performance.get(\"strategy_used\", \"unknown\")\n                },\n                \"priority\": \"high\"\n            }\n        else:\n            return {\n                \"action\": \"CONTINUE\",\n                \"reason\": \"Performance issues detected but not critical\"\n            }\n\n    def _summarize_current_results(self, prep_res: Dict) -&gt; str:\n        \"\"\"Summarize current results for context\"\"\"\n\n        results_store = prep_res[\"results_store\"]\n        current_plan = prep_res[\"current_plan\"]\n\n        summary_items = []\n\n        for task in current_plan.tasks:\n            if prep_res[\"tasks\"][task.id].status == \"completed\":\n                result = results_store.get(task.id, {})\n                data_preview = str(result.get(\"data\", \"\"))[:100]\n                summary_items.append(f\"{task.id}: {data_preview}...\")\n\n        return \"; \".join(summary_items) if summary_items else \"No results yet\"\n\n    async def _analyze_recent_completions(self, prep_res) -&gt; Dict[str, Any]:\n        \"\"\"Analyze recently completed tasks for issues and successes\"\"\"\n\n        recently_completed = prep_res[\"recently_completed\"]\n        results_store = prep_res[\"results_store\"]\n\n        analysis = {\n            \"successful_tasks\": [],\n            \"problematic_tasks\": [],\n            \"verification_issues\": [],\n            \"unexpected_results\": [],\n            \"dynamic_decisions\": []  # Track DecisionTask outcomes\n        }\n\n        for task in recently_completed:\n            task_analysis = await self._analyze_single_task(task, results_store)\n\n            if task_analysis[\"status\"] == \"successful\":\n                analysis[\"successful_tasks\"].append(task_analysis)\n            elif task_analysis[\"status\"] == \"problematic\":\n                analysis[\"problematic_tasks\"].append(task_analysis)\n            elif task_analysis[\"status\"] == \"unexpected\":\n                analysis[\"unexpected_results\"].append(task_analysis)\n            elif task_analysis[\"status\"] == \"dynamic_decision\":\n                analysis[\"dynamic_decisions\"].append(task_analysis)\n\n        return analysis\n\n    async def _analyze_single_task(self, task: Task, results_store: Dict) -&gt; Dict[str, Any]:\n        \"\"\"Enhanced single task analysis including DecisionTask dynamics\"\"\"\n\n        task_result = results_store.get(task.id, {})\n        verification = task.metadata.get(\"verification\", {}) if hasattr(task, 'metadata') else {}\n\n        # Enhanced DecisionTask analysis\n        if isinstance(task, DecisionTask):\n            if hasattr(task, 'metadata') and task.metadata:\n                routing_action = task.metadata.get(\"routing_action\")\n                decision_made = task.metadata.get(\"decision_made\", \"\")\n\n                if routing_action in [\"replan_from_here\", \"append_plan\"]:\n                    return {\n                        \"task_id\": task.id,\n                        \"status\": \"dynamic_decision\",\n                        \"action\": routing_action,\n                        \"decision\": decision_made,\n                        \"details\": task.metadata.get(\"routing_instruction\", {})\n                    }\n                elif decision_made and decision_made in task.routing_map:\n                    return {\n                        \"task_id\": task.id,\n                        \"status\": \"successful\",\n                        \"decision\": decision_made,\n                        \"routing_action\": routing_action or \"route_to_task\"\n                    }\n\n            return {\n                \"task_id\": task.id,\n                \"status\": \"problematic\",\n                \"issue\": \"decision_task_incomplete\",\n                \"decision\": task.metadata.get(\"decision_made\", \"\") if hasattr(task, 'metadata') else \"\"\n            }\n\n        # ToolTask analysis (existing logic)\n        elif isinstance(task, ToolTask) and task.hypothesis:\n            hypothesis_score = verification.get(\"hypothesis_score\", 0.5)\n            criteria_score = verification.get(\"criteria_score\", 0.5)\n\n            if hypothesis_score &lt; 0.3 or criteria_score &lt; 0.3:\n                return {\n                    \"task_id\": task.id,\n                    \"status\": \"problematic\",\n                    \"issue\": \"hypothesis_not_met\",\n                    \"details\": {\n                        \"hypothesis\": task.hypothesis,\n                        \"hypothesis_score\": hypothesis_score,\n                        \"criteria_score\": criteria_score,\n                        \"tool_name\": task.tool_name\n                    }\n                }\n            elif 0.3 &lt;= hypothesis_score &lt; 0.7 or 0.3 &lt;= criteria_score &lt; 0.7:\n                return {\n                    \"task_id\": task.id,\n                    \"status\": \"unexpected\",\n                    \"issue\": \"partial_success\",\n                    \"details\": {\n                        \"hypothesis\": task.hypothesis,\n                        \"actual_result\": task_result.get(\"data\", \"\"),\n                        \"verification\": verification\n                    }\n                }\n            else:\n                return {\n                    \"task_id\": task.id,\n                    \"status\": \"successful\",\n                    \"details\": verification\n                }\n\n        # Standard Task analysis\n        else:\n            if task.result and not task.error:\n                return {\"task_id\": task.id, \"status\": \"successful\"}\n            else:\n                return {\n                    \"task_id\": task.id,\n                    \"status\": \"problematic\",\n                    \"issue\": \"execution_error\",\n                    \"error\": task.error\n                }\n\n    async def _trigger_standard_replanning(self, decision: Dict, prep_res: Dict) -&gt; Dict[str, Any]:\n        \"\"\"Trigger standard replanning (not dynamic)\"\"\"\n\n        original_query = prep_res.get(\"original_query\", \"\")\n        learned_context = self._extract_learned_context(prep_res)\n\n        replan_context = {\n            \"original_query\": original_query,\n            \"previous_attempt_failed\": True,\n            \"learned_context\": learned_context,\n            \"failure_reason\": decision.get(\"reasoning\", \"\"),\n            \"avoid_approaches\": self._extract_failed_approaches(prep_res),\n            \"standard_replan\": True\n        }\n\n        return {\n            \"action\": \"STANDARD_REPLAN_TRIGGERED\",\n            \"replan_context\": replan_context,\n            \"reason\": decision.get(\"reasoning\", \"\"),\n            \"previous_plan_id\": prep_res[\"current_plan\"].id if prep_res[\"current_plan\"] else None\n        }\n\n    # Helper methods for dynamic replanning\n    def _extract_learned_context_from_tasks(self, completed_tasks: List[Task], results_store: Dict) -&gt; str:\n        \"\"\"Extract learned context from completed tasks for replanning\"\"\"\n\n        context_items = []\n\n        for task in completed_tasks:\n            result_data = results_store.get(task.id, {})\n\n            if isinstance(task, ToolTask):\n                if result_data.get(\"metadata\", {}).get(\"success\", False):\n                    context_items.append(f\"Tool {task.tool_name}: {str(result_data.get('data', ''))[:150]}...\")\n\n            elif isinstance(task, LLMTask):\n                if task.result:\n                    context_items.append(f\"Analysis result: {str(task.result)[:100]}...\")\n\n            elif isinstance(task, DecisionTask):\n                decision = task.metadata.get(\"decision_made\") if hasattr(task, 'metadata') else None\n                if decision:\n                    context_items.append(f\"Decision made: {decision}\")\n\n        return \"\\n\".join(context_items) if context_items else \"No significant context from previous tasks\"\n\n    def _extract_failed_approaches_from_context(self, replan_context: Dict) -&gt; List[str]:\n        \"\"\"Extract failed approaches to avoid in replanning\"\"\"\n\n        failed_approaches = []\n\n        failure_reason = replan_context.get(\"failure_reason\", \"\")\n        if \"search\" in failure_reason.lower():\n            failed_approaches.append(\"initial_search_approach\")\n\n        if \"tool\" in failure_reason.lower():\n            failed_approaches.append(\"direct_tool_usage\")\n\n        # Add context-specific failures\n        context = replan_context.get(\"context\", \"\")\n        if context:\n            failed_approaches.append(f\"approach_described_as: {context[:50]}...\")\n\n        return failed_approaches\n\n    async def post_async(self, shared, prep_res, exec_res):\n        \"\"\"Unified post-processing with complete action spectrum\"\"\"\n\n        action = exec_res.get(\"action\", \"CONTINUE\")\n\n        # Clear trigger flags\n        shared[\"needs_dynamic_replan\"] = False\n        shared[\"needs_plan_append\"] = False\n        shared[\"replan_context\"] = {}\n        shared[\"append_context\"] = {}\n\n        # Handle all possible actions\n        if action == \"DYNAMIC_REPLAN_TRIGGERED\":\n            shared[\"replan_context\"] = exec_res.get(\"replan_context\", {})\n            shared[\"needs_replanning\"] = True\n            return \"needs_replan\"\n\n        elif action == \"PLAN_APPEND_TRIGGERED\":\n            shared[\"append_context\"] = exec_res.get(\"append_context\", {})\n            shared[\"needs_plan_extension\"] = True\n            return \"needs_plan_append\"\n\n        elif action == \"STANDARD_REPLAN_TRIGGERED\":\n            shared[\"replan_context\"] = exec_res.get(\"replan_context\", {})\n            shared[\"needs_replanning\"] = True\n            return \"needs_replan\"\n\n        elif action == \"PERFORMANCE_REPLAN\":\n            shared[\"replan_context\"] = {\n                \"performance_failure\": True,\n                \"performance_context\": exec_res.get(\"performance_context\", {}),\n                \"original_query\": shared.get(\"current_query\", \"\")\n            }\n            shared[\"needs_replanning\"] = True\n            return \"needs_replan\"\n\n        elif action == \"ADAPT_COMPLETED\":\n            shared[\"plan_adaptations\"] = shared.get(\"plan_adaptations\", 0) + 1\n            shared[\"last_adaptation\"] = datetime.now()\n            return \"adapted\"\n\n        elif action == \"HALT_FAILURE\":\n            shared[\"plan_halted\"] = True\n            shared[\"halt_reason\"] = exec_res.get(\"reason\", \"Critical failure\")\n            shared[\"halt_details\"] = exec_res.get(\"halt_details\", {})\n            return \"plan_halted\"\n\n        elif action == \"CONTINUE\":\n            return \"continue\"\n\n        else:\n            # Unknown action - default to continue\n            logger.warning(f\"Unknown reflection action: {action}\")\n            return \"continue\"\n\n    async def _decide_next_action(self, reflection_result: Dict, prep_res: Dict) -&gt; Dict[str, Any]:\n        \"\"\"LLM-basierte Entscheidung \u00fcber n\u00e4chste Aktionen\"\"\"\n\n        if not LITELLM_AVAILABLE:\n            return {\"action\": \"CONTINUE\", \"reason\": \"No LLM available for decision making\"}\n\n        prompt = f\"\"\"\nDu bist ein Plan-Reflexionssystem. Analysiere die Task-Ergebnisse und entscheide \u00fcber die n\u00e4chste Aktion.\n\n## Analyse der k\u00fcrzlich abgeschlossenen Tasks\nErfolgreiche Tasks: {len(reflection_result['successful_tasks'])}\nProblematische Tasks: {len(reflection_result['problematic_tasks'])}\nUnerwartete Ergebnisse: {len(reflection_result['unexpected_results'])}\n\n## Detaillierte Probleme\n{self._format_analysis_for_prompt(reflection_result)}\n\n## Verf\u00fcgbare Aktionen\n- CONTINUE: Alles l\u00e4uft nach Plan, weitermachen\n- ADAPT: Plan um zus\u00e4tzliche Tasks erweitern (f\u00fcr unerwartete Ergebnisse)\n- REPLAN: Kompletter Neustart der Planung (bei kritischen Fehlern)\n- HALT_FAILURE: Plan abbrechen (bei un\u00fcberwindbaren Problemen)\n\n## Entscheidung\nAntworte mit YAML:\n\n```yaml\naction: \"ADAPT\"  # Eine der obigen Aktionen\nconfidence: 0.85  # Wie sicher ist diese Entscheidung?\nreasoning: \"Brief explanation of the decision\"\npriority: \"high\"  # high | medium | low\nadaptation_suggestions:\n  - task_type: \"tool_call\"\n    description: \"Additional search with refined query\"\n    reason: \"Original search didn't find expected information\"\n  - task_type: \"llm_call\"\n    description: \"Analyze alternative data sources\"\n    reason: \"Need to explore different approaches\"\n```\"\"\"\n\n        try:\n            model_to_use = prep_res.get(\"fast_llm_model\", \"openrouter/anthropic/claude-3-haiku\")\n            agent_instance = prep_res[\"agent_instance\"]\n            content = await agent_instance.a_run_llm_completion(\n                model=model_to_use,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.3,\n                max_tokens=600, node_name=\"PlanReflectionNode\", task_id=\"plan_reflection\"\n            )\n\n            yaml_match = re.search(r\"```yaml\\s*(.*?)\\s*```\", content, re.DOTALL)\n            yaml_str = yaml_match.group(1) if yaml_match else content.strip()\n\n            decision = yaml.safe_load(yaml_str)\n\n            # Validierung\n            valid_actions = [\"CONTINUE\", \"ADAPT\", \"REPLAN\", \"HALT_FAILURE\"]\n            if decision.get(\"action\") not in valid_actions:\n                decision[\"action\"] = \"CONTINUE\"\n\n            logger.info(\n                f\"Plan reflection decision: {decision.get('action')} (confidence: {decision.get('confidence', 0.0)})\")\n            return decision\n\n        except Exception as e:\n            logger.error(f\"Plan reflection decision failed: {e}\")\n            return {\"action\": \"CONTINUE\", \"reason\": f\"Decision error: {e}\"}\n\n    def _format_analysis_for_prompt(self, reflection_result: Dict) -&gt; str:\n        \"\"\"Formatiere Analyse-Ergebnisse f\u00fcr LLM-Prompt\"\"\"\n        formatted = []\n\n        for problematic in reflection_result.get(\"problematic_tasks\", []):\n            formatted.append(f\"PROBLEM - Task {problematic['task_id']}: {problematic.get('issue', 'Unknown')}\")\n\n        for unexpected in reflection_result.get(\"unexpected_results\", []):\n            formatted.append(f\"UNEXPECTED - Task {unexpected['task_id']}: {unexpected.get('issue', 'Unknown')}\")\n\n        return \"\\n\".join(formatted) if formatted else \"No significant issues detected.\"\n\n    async def _adapt_plan(self, decision: Dict, prep_res: Dict) -&gt; Dict[str, Any]:\n        \"\"\"F\u00fchre Plan-Adaptation aus basierend auf Entscheidung\"\"\"\n\n        current_plan = prep_res[\"current_plan\"]\n        agent_instance = prep_res[\"agent_instance\"]\n\n        adaptation_suggestions = decision.get(\"adaptation_suggestions\", [])\n        new_tasks = []\n\n        for suggestion in adaptation_suggestions:\n            # Erstelle neue Tasks basierend auf Suggestions\n            task_id = f\"adapt_{str(uuid.uuid4())[:8]}\"\n\n            if suggestion.get(\"task_type\") == \"tool_call\":\n                new_task = ToolTask(\n                    id=task_id,\n                    type=\"tool_call\",\n                    description=suggestion.get(\"description\", \"\"),\n                    priority=2,  # Mittlere Priorit\u00e4t f\u00fcr adaptive Tasks\n                    dependencies=[],  # K\u00f6nnen sofort ausgef\u00fchrt werden\n                    tool_name=self._infer_tool_name(suggestion, agent_instance),\n                    arguments=self._generate_tool_arguments(suggestion),\n                    hypothesis=f\"This adaptation will address: {suggestion.get('reason', '')}\",\n                    validation_criteria=\"Results should provide the missing information\"\n                )\n            elif suggestion.get(\"task_type\") == \"llm_call\":\n                new_task = LLMTask(\n                    id=task_id,\n                    type=\"llm_call\",\n                    description=suggestion.get(\"description\", \"\"),\n                    priority=2,\n                    dependencies=[],\n                    prompt_template=f\"Analyze the situation: {suggestion.get('reason', '')}\",\n                    llm_config={\"model_preference\": \"fast\"}\n                )\n            else:\n                new_task = create_task(\"generic\",\n                                       id=task_id,\n                                       description=suggestion.get(\"description\", \"\"),\n                                       priority=2\n                                       )\n\n            new_tasks.append(new_task)\n\n        # F\u00fcge neue Tasks zum Plan hinzu\n        current_plan.tasks.extend(new_tasks)\n\n        # Update shared state\n        task_dict = {task.id: task for task in new_tasks}\n        prep_res[\"tasks\"].update(task_dict)\n\n        logger.info(f\"Plan adapted with {len(new_tasks)} additional tasks\")\n\n        return {\n            \"action\": \"ADAPT_COMPLETED\",\n            \"new_tasks\": [task.id for task in new_tasks],\n            \"reason\": decision.get(\"reasoning\", \"\"),\n            \"total_tasks\": len(current_plan.tasks)\n        }\n\n    def _infer_tool_name(self, suggestion: Dict, agent_instance) -&gt; str:\n        \"\"\"Inferiere Tool-Name basierend auf Suggestion\"\"\"\n        description = suggestion.get(\"description\", \"\").lower()\n        available_tools = getattr(agent_instance, 'shared', {}).get(\"available_tools\", [])\n\n        # Simple Heuristik f\u00fcr Tool-Auswahl\n        if \"search\" in description and any(\"search\" in tool for tool in available_tools):\n            return next(tool for tool in available_tools if \"search\" in tool)\n        elif \"analyze\" in description and any(\"analy\" in tool for tool in available_tools):\n            return next(tool for tool in available_tools if \"analy\" in tool)\n        else:\n            return available_tools[0] if available_tools else \"generic_tool\"\n\n    def _generate_tool_arguments(self, suggestion: Dict) -&gt; Dict[str, Any]:\n        \"\"\"Generiere Tool-Argumente basierend auf Suggestion\"\"\"\n        description = suggestion.get(\"description\", \"\")\n        reason = suggestion.get(\"reason\", \"\")\n\n        # Basis-Argumente basierend auf Kontext\n        if \"search\" in description.lower():\n            return {\n                \"query\": f\"refined search based on: {reason}\",\n                \"max_results\": 5\n            }\n        elif \"analyze\" in description.lower():\n            return {\n                \"text\": \"{{ results.previous_task.data }}\",\n                \"analysis_type\": \"comprehensive\"\n            }\n        else:\n            return {\"input\": f\"{description} - {reason}\"}\n\n    def _extract_learned_context(self, prep_res: Dict) -&gt; str:\n        \"\"\"Extrahiere gelernten Kontext aus bisherigen Ergebnissen\"\"\"\n        results_store = prep_res.get(\"results_store\", {})\n        learned_facts = []\n\n        for task_id, result in results_store.items():\n            if result.get(\"metadata\", {}).get(\"success\", False):\n                learned_facts.append(f\"Task {task_id} discovered: {str(result.get('data', ''))[:100]}...\")\n\n        return \"\\\\n\".join(learned_facts)\n\n    def _extract_failed_approaches(self, prep_res: Dict) -&gt; List[str]:\n        \"\"\"Extrahiere fehlgeschlagene Ans\u00e4tze um sie zu vermeiden\"\"\"\n        failed_approaches = []\n        tasks = prep_res.get(\"tasks\", {})\n\n        for task in tasks.values():\n            if task.status == \"failed\" and isinstance(task, ToolTask):\n                failed_approaches.append(f\"Tool {task.tool_name} with args {task.arguments}\")\n\n        return failed_approaches\n</code></pre> <code>exec_async(prep_res)</code> <code>async</code> \u00b6 <p>Unified execution handling all reflection scenarios</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def exec_async(self, prep_res):\n    \"\"\"Unified execution handling all reflection scenarios\"\"\"\n\n    # Priority 1: Handle dynamic replanning (triggered by DecisionTask)\n    if prep_res[\"needs_dynamic_replan\"]:\n        logger.info(\"Handling dynamic replan request\")\n        return await self._handle_dynamic_replan(prep_res)\n\n    # Priority 2: Handle plan append (triggered by DecisionTask)\n    if prep_res[\"needs_plan_append\"]:\n        logger.info(\"Handling plan append request\")\n        return await self._handle_plan_append(prep_res)\n\n    # Priority 3: Standard reflection on completed tasks\n    if prep_res[\"recently_completed\"]:\n        logger.info(\"Analyzing recently completed tasks\")\n        return await self._handle_standard_reflection(prep_res)\n\n    # Priority 4: Check execution performance issues\n    performance = prep_res[\"execution_performance\"]\n    if performance.get(\"status\") == \"error\" or performance.get(\"success_rate\", 1.0) &lt; 0.3:\n        logger.info(\"Handling performance issues\")\n        return await self._handle_performance_issues(prep_res)\n\n    # Default: Continue normal execution\n    return {\n        \"action\": \"CONTINUE\",\n        \"reason\": \"No reflection triggers detected, continuing normal execution\"\n    }\n</code></pre> <code>post_async(shared, prep_res, exec_res)</code> <code>async</code> \u00b6 <p>Unified post-processing with complete action spectrum</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def post_async(self, shared, prep_res, exec_res):\n    \"\"\"Unified post-processing with complete action spectrum\"\"\"\n\n    action = exec_res.get(\"action\", \"CONTINUE\")\n\n    # Clear trigger flags\n    shared[\"needs_dynamic_replan\"] = False\n    shared[\"needs_plan_append\"] = False\n    shared[\"replan_context\"] = {}\n    shared[\"append_context\"] = {}\n\n    # Handle all possible actions\n    if action == \"DYNAMIC_REPLAN_TRIGGERED\":\n        shared[\"replan_context\"] = exec_res.get(\"replan_context\", {})\n        shared[\"needs_replanning\"] = True\n        return \"needs_replan\"\n\n    elif action == \"PLAN_APPEND_TRIGGERED\":\n        shared[\"append_context\"] = exec_res.get(\"append_context\", {})\n        shared[\"needs_plan_extension\"] = True\n        return \"needs_plan_append\"\n\n    elif action == \"STANDARD_REPLAN_TRIGGERED\":\n        shared[\"replan_context\"] = exec_res.get(\"replan_context\", {})\n        shared[\"needs_replanning\"] = True\n        return \"needs_replan\"\n\n    elif action == \"PERFORMANCE_REPLAN\":\n        shared[\"replan_context\"] = {\n            \"performance_failure\": True,\n            \"performance_context\": exec_res.get(\"performance_context\", {}),\n            \"original_query\": shared.get(\"current_query\", \"\")\n        }\n        shared[\"needs_replanning\"] = True\n        return \"needs_replan\"\n\n    elif action == \"ADAPT_COMPLETED\":\n        shared[\"plan_adaptations\"] = shared.get(\"plan_adaptations\", 0) + 1\n        shared[\"last_adaptation\"] = datetime.now()\n        return \"adapted\"\n\n    elif action == \"HALT_FAILURE\":\n        shared[\"plan_halted\"] = True\n        shared[\"halt_reason\"] = exec_res.get(\"reason\", \"Critical failure\")\n        shared[\"halt_details\"] = exec_res.get(\"halt_details\", {})\n        return \"plan_halted\"\n\n    elif action == \"CONTINUE\":\n        return \"continue\"\n\n    else:\n        # Unknown action - default to continue\n        logger.warning(f\"Unknown reflection action: {action}\")\n        return \"continue\"\n</code></pre> <code>ProgressEvent</code> <code>dataclass</code> \u00b6 <p>Enhanced progress event with better error handling</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass ProgressEvent:\n    \"\"\"Enhanced progress event with better error handling\"\"\"\n    event_type: str\n    timestamp: float\n    node_name: str\n    event_id: str = \"\"\n\n    # Status information\n    status: Optional[NodeStatus] = None\n    success: Optional[bool] = None\n    error_details: Optional[Dict[str, Any]] = None\n\n    # LLM-specific data\n    llm_model: Optional[str] = None\n    llm_prompt_tokens: Optional[int] = None\n    llm_completion_tokens: Optional[int] = None\n    llm_total_tokens: Optional[int] = None\n    llm_cost: Optional[float] = None\n    llm_duration: Optional[float] = None\n    llm_temperature: Optional[float] = None\n\n    # Tool-specific data\n    tool_name: Optional[str] = None\n    tool_args: Optional[Dict[str, Any]] = None\n    tool_result: Optional[Any] = None\n    tool_duration: Optional[float] = None\n    tool_success: Optional[bool] = None\n    tool_error: Optional[str] = None\n\n    # Node/Routing data\n    routing_decision: Optional[str] = None\n    routing_from: Optional[str] = None\n    routing_to: Optional[str] = None\n    node_phase: Optional[str] = None\n    node_duration: Optional[float] = None\n\n    # Context data\n    task_id: Optional[str] = None\n    session_id: Optional[str] = None\n    plan_id: Optional[str] = None\n\n    # Additional metadata\n    metadata: Dict[str, Any] = None\n\n    def __post_init__(self):\n        if self.metadata is None:\n            self.metadata = {}\n        if not self.event_id:\n            self.event_id = f\"{self.node_name}_{self.event_type}_{int(self.timestamp * 1000000)}\"\n        if 'error' in self.metadata or 'error_type' in self.metadata:\n            if self.error_details is None:\n                self.error_details = {}\n            self.error_details['error'] = self.metadata.get('error')\n            self.error_details['error_type'] = self.metadata.get('error_type')\n            self.status = NodeStatus.FAILED\n        if self.status == NodeStatus.FAILED:\n            self.success = False\n        if self.status == NodeStatus.COMPLETED:\n            self.success = True\n</code></pre> <code>ProgressTracker</code> \u00b6 <p>Advanced progress tracking with cost calculation</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>class ProgressTracker:\n    \"\"\"Advanced progress tracking with cost calculation\"\"\"\n\n    def __init__(self, progress_callback: Optional[callable] = None, agent_name=\"unknown\"):\n        self.progress_callback = progress_callback\n        self.events: List[ProgressEvent] = []\n        self.active_timers: Dict[str, float] = {}\n\n        # Cost tracking (simplified - would need actual provider pricing)\n        self.token_costs = {\n            \"input\": 0.00001,  # $0.01/1K tokens input\n            \"output\": 0.00003,  # $0.03/1K tokens output\n        }\n        self.agent_name = agent_name\n\n    async def emit_event(self, event: ProgressEvent):\n        \"\"\"Emit progress event with callback and storage\"\"\"\n        self.events.append(event)\n        event.agent_name = self.agent_name\n\n        if self.progress_callback:\n            try:\n                if asyncio.iscoroutinefunction(self.progress_callback):\n                    await self.progress_callback(event)\n                else:\n                    self.progress_callback(event)\n            except Exception as e:\n                import traceback\n                print(traceback.format_exc())\n\n\n    def start_timer(self, key: str) -&gt; float:\n        \"\"\"Start timing operation\"\"\"\n        start_time = time.perf_counter()\n        self.active_timers[key] = start_time\n        return start_time\n\n    def end_timer(self, key: str) -&gt; float:\n        \"\"\"End timing operation and return duration\"\"\"\n        if key not in self.active_timers:\n            return 0.0\n        duration = time.perf_counter() - self.active_timers[key]\n        del self.active_timers[key]\n        return duration\n\n    def calculate_llm_cost(self, model: str, input_tokens: int, output_tokens: int) -&gt; float:\n        \"\"\"Calculate approximate LLM cost\"\"\"\n        # Simplified cost calculation - would need actual provider pricing\n        input_cost = (input_tokens / 1000) * self.token_costs[\"input\"]\n        output_cost = (output_tokens / 1000) * self.token_costs[\"output\"]\n        return input_cost + output_cost\n\n    def get_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Get comprehensive progress summary\"\"\"\n        summary = {\n            \"total_events\": len(self.events),\n            \"llm_calls\": len([e for e in self.events if e.event_type == \"llm_call\"]),\n            \"tool_calls\": len([e for e in self.events if e.event_type == \"tool_call\"]),\n            \"total_cost\": sum(e.llm_cost for e in self.events if e.llm_cost),\n            \"total_tokens\": sum(e.llm_total_tokens for e in self.events if e.llm_total_tokens),\n            \"total_duration\": sum(e.node_duration for e in self.events if e.node_duration),\n            \"nodes_visited\": list(set(e.node_name for e in self.events)),\n            \"tools_used\": list(set(e.tool_name for e in self.events if e.tool_name)),\n            \"models_used\": list(set(e.llm_model for e in self.events if e.llm_model))\n        }\n        return summary\n</code></pre> <code>calculate_llm_cost(model, input_tokens, output_tokens)</code> \u00b6 <p>Calculate approximate LLM cost</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def calculate_llm_cost(self, model: str, input_tokens: int, output_tokens: int) -&gt; float:\n    \"\"\"Calculate approximate LLM cost\"\"\"\n    # Simplified cost calculation - would need actual provider pricing\n    input_cost = (input_tokens / 1000) * self.token_costs[\"input\"]\n    output_cost = (output_tokens / 1000) * self.token_costs[\"output\"]\n    return input_cost + output_cost\n</code></pre> <code>emit_event(event)</code> <code>async</code> \u00b6 <p>Emit progress event with callback and storage</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>async def emit_event(self, event: ProgressEvent):\n    \"\"\"Emit progress event with callback and storage\"\"\"\n    self.events.append(event)\n    event.agent_name = self.agent_name\n\n    if self.progress_callback:\n        try:\n            if asyncio.iscoroutinefunction(self.progress_callback):\n                await self.progress_callback(event)\n            else:\n                self.progress_callback(event)\n        except Exception as e:\n            import traceback\n            print(traceback.format_exc())\n</code></pre> <code>end_timer(key)</code> \u00b6 <p>End timing operation and return duration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def end_timer(self, key: str) -&gt; float:\n    \"\"\"End timing operation and return duration\"\"\"\n    if key not in self.active_timers:\n        return 0.0\n    duration = time.perf_counter() - self.active_timers[key]\n    del self.active_timers[key]\n    return duration\n</code></pre> <code>get_summary()</code> \u00b6 <p>Get comprehensive progress summary</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def get_summary(self) -&gt; Dict[str, Any]:\n    \"\"\"Get comprehensive progress summary\"\"\"\n    summary = {\n        \"total_events\": len(self.events),\n        \"llm_calls\": len([e for e in self.events if e.event_type == \"llm_call\"]),\n        \"tool_calls\": len([e for e in self.events if e.event_type == \"tool_call\"]),\n        \"total_cost\": sum(e.llm_cost for e in self.events if e.llm_cost),\n        \"total_tokens\": sum(e.llm_total_tokens for e in self.events if e.llm_total_tokens),\n        \"total_duration\": sum(e.node_duration for e in self.events if e.node_duration),\n        \"nodes_visited\": list(set(e.node_name for e in self.events)),\n        \"tools_used\": list(set(e.tool_name for e in self.events if e.tool_name)),\n        \"models_used\": list(set(e.llm_model for e in self.events if e.llm_model))\n    }\n    return summary\n</code></pre> <code>start_timer(key)</code> \u00b6 <p>Start timing operation</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def start_timer(self, key: str) -&gt; float:\n    \"\"\"Start timing operation\"\"\"\n    start_time = time.perf_counter()\n    self.active_timers[key] = start_time\n    return start_time\n</code></pre> <code>ResponseFinalProcessorNode</code> \u00b6 <p>               Bases: <code>AsyncNode</code></p> <p>Finale Verarbeitung mit Persona-System</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@with_progress_tracking\nclass ResponseFinalProcessorNode(AsyncNode):\n    \"\"\"Finale Verarbeitung mit Persona-System\"\"\"\n\n    async def prep_async(self, shared):\n        return {\n            \"formatted_response\": shared.get(\"formatted_response\", {}),\n            \"quality_assessment\": shared.get(\"quality_assessment\", {}),\n            \"conversation_history\": shared.get(\"conversation_history\", []),\n            \"persona\": shared.get(\"persona_config\"),\n            \"fast_llm_model\": shared.get(\"fast_llm_model\"),\n            \"use_fast_response\": shared.get(\"use_fast_response\", True),\n            \"agent_instance\": shared.get(\"agent_instance\"),\n        }\n\n    async def exec_async(self, prep_res):\n        response_data = prep_res[\"formatted_response\"]\n        raw_response = response_data.get(\"formatted_response\", \"I apologize, but I couldn't generate a response.\")\n\n        # Persona-basierte Anpassung\n        if prep_res.get(\"persona\") and LITELLM_AVAILABLE:\n            final_response = await self._apply_persona_style(raw_response, prep_res)\n        else:\n            final_response = raw_response\n\n        # Finale Metadaten\n        processing_metadata = {\n            \"response_confidence\": response_data.get(\"confidence\", 0.0),\n            \"quality_score\": prep_res.get(\"quality_assessment\", {}).get(\"quality_score\", 0.0),\n            \"processing_timestamp\": datetime.now().isoformat(),\n            \"response_length\": len(final_response),\n            \"persona_applied\": prep_res.get(\"persona\") is not None\n        }\n\n        return {\n            \"final_response\": final_response,\n            \"metadata\": processing_metadata,\n            \"status\": \"completed\"\n        }\n\n    async def _apply_persona_style(self, response: str, prep_res: Dict) -&gt; str:\n        \"\"\"Optimized persona styling mit Konfiguration\"\"\"\n        persona = prep_res[\"persona\"]\n\n        # Nur anwenden wenn post-processing konfiguriert\n        if not persona.should_post_process():\n            return response\n\n        # Je nach Integration Level unterschiedliche Prompts\n        if persona.integration_level == \"light\":\n            style_prompt = f\"Make this {persona.tone} and {persona.style}: {response}\"\n            max_tokens = 400\n        elif persona.integration_level == \"medium\":\n            style_prompt = f\"\"\"\n    Apply {persona.name} persona (style: {persona.style}, tone: {persona.tone}) to:\n    {response}\n\n    Keep the same information, adjust presentation:\"\"\"\n            max_tokens = 600\n        else:  # heavy\n            style_prompt = f\"\"\"\n    Completely transform as {persona.name}:\n    Style: {persona.style}, Tone: {persona.tone}\n    Traits: {', '.join(persona.personality_traits)}\n    Instructions: {persona.custom_instructions}\n\n    Original: {response}\n\n    As {persona.name}:\"\"\"\n            max_tokens = 1000\n\n        try:\n            model_to_use = prep_res.get(\"fast_llm_model\", \"openrouter/anthropic/claude-3-haiku\")\n            agent_instance = prep_res[\"agent_instance\"]\n            if prep_res.get(\"use_fast_response\", True):\n                response = await agent_instance.a_run_llm_completion(\n                    model=model_to_use,\n                    messages=[{\"role\": \"user\", \"content\": style_prompt}],\n                    temperature=0.5,\n                    max_tokens=max_tokens, node_name=\"PersonaStylingNode\", task_id=\"persona_styling_fast\"\n                )\n            else:\n                response = await agent_instance.a_run_llm_completion(\n                    model=model_to_use,\n                    messages=[{\"role\": \"user\", \"content\": style_prompt}],\n                    temperature=0.6,\n                    max_tokens=max_tokens + 200, node_name=\"PersonaStylingNode\", task_id=\"persona_styling_ritch\"\n                )\n\n            return response.strip()\n\n        except Exception as e:\n            logger.warning(f\"Persona styling failed: {e}\")\n            return response\n\n    async def post_async(self, shared, prep_res, exec_res):\n        shared[\"current_response\"] = exec_res[\"final_response\"]\n        shared[\"response_metadata\"] = exec_res[\"metadata\"]\n        return \"response_ready\"\n</code></pre> <code>ResponseFormatterNode</code> \u00b6 <p>               Bases: <code>AsyncNode</code></p> <p>Formatiere finale Antwort f\u00fcr Benutzer</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@with_progress_tracking\nclass ResponseFormatterNode(AsyncNode):\n    \"\"\"Formatiere finale Antwort f\u00fcr Benutzer\"\"\"\n\n    async def prep_async(self, shared):\n        return {\n            \"synthesized_response\": shared.get(\"synthesized_response\", {}),\n            \"original_query\": shared.get(\"current_query\", \"\"),\n            \"user_preferences\": shared.get(\"user_preferences\", {})\n        }\n\n    async def exec_async(self, prep_res):\n        synthesis_data = prep_res[\"synthesized_response\"]\n        raw_response = synthesis_data.get(\"synthesized_response\", \"\")\n\n        if not raw_response:\n            return {\n                \"formatted_response\": \"I apologize, but I was unable to generate a meaningful response to your query.\"}\n\n        # Basis-Formatierung\n        formatted_response = raw_response.strip()\n\n        # F\u00fcge Metadaten hinzu falls gew\u00fcnscht (f\u00fcr debugging/transparency)\n        confidence = synthesis_data.get(\"confidence\", 0.0)\n        if confidence &lt; 0.4:\n            formatted_response += \"\\n\\n*Note: This response has low confidence due to limited information.*\"\n\n        adaptation_note = \"\"\n        synthesis_method = synthesis_data.get(\"synthesis_method\", \"unknown\")\n        if synthesis_method == \"fallback\":\n            adaptation_note = \"\\n\\n*Note: Response generated with limited processing capabilities.*\"\n\n        return {\n            \"formatted_response\": formatted_response + adaptation_note,\n            \"confidence\": confidence,\n            \"metadata\": {\n                \"synthesis_method\": synthesis_method,\n                \"response_length\": len(formatted_response)\n            }\n        }\n\n    async def post_async(self, shared, prep_res, exec_res):\n        shared[\"formatted_response\"] = exec_res\n        return \"formatted\"\n</code></pre> <code>ResponseGenerationFlow</code> \u00b6 <p>               Bases: <code>AsyncFlow</code></p> <p>Intelligente Antwortgenerierung basierend auf Task-Ergebnissen</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@with_progress_tracking\nclass ResponseGenerationFlow(AsyncFlow):\n    \"\"\"Intelligente Antwortgenerierung basierend auf Task-Ergebnissen\"\"\"\n\n    def __init__(self, tools=None):\n        # Nodes f\u00fcr Response-Pipeline\n        self.context_aggregator = ContextAggregatorNode()\n        self.result_synthesizer = ResultSynthesizerNode()\n        self.response_formatter = ResponseFormatterNode()\n        self.quality_checker = ResponseQualityNode()\n        self.final_processor = ResponseFinalProcessorNode()\n\n        # === RESPONSE GENERATION PIPELINE ===\n\n        # Context Aggregation -&gt; Synthesis\n        self.context_aggregator - \"context_ready\" &gt;&gt; self.result_synthesizer\n        self.context_aggregator - \"no_context\" &gt;&gt; self.response_formatter  # Fallback\n\n        # Synthesis -&gt; Formatting\n        self.result_synthesizer - \"synthesized\" &gt;&gt; self.response_formatter\n        self.result_synthesizer - \"synthesis_failed\" &gt;&gt; self.response_formatter\n\n        # Formatting -&gt; Quality Check\n        self.response_formatter - \"formatted\" &gt;&gt; self.quality_checker\n        self.response_formatter - \"format_failed\" &gt;&gt; self.final_processor  # Skip quality check\n\n        # Quality Check -&gt; Final Processing oder Retry\n        self.quality_checker - \"quality_good\" &gt;&gt; self.final_processor\n        self.quality_checker - \"quality_poor\" &gt;&gt; self.result_synthesizer  # Retry synthesis\n        self.quality_checker - \"quality_acceptable\" &gt;&gt; self.final_processor\n\n        super().__init__(start=self.context_aggregator)\n</code></pre> <code>ResponseQualityNode</code> \u00b6 <p>               Bases: <code>AsyncNode</code></p> <p>Pr\u00fcfe Qualit\u00e4t der generierten Antwort</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@with_progress_tracking\nclass ResponseQualityNode(AsyncNode):\n    \"\"\"Pr\u00fcfe Qualit\u00e4t der generierten Antwort\"\"\"\n\n    async def prep_async(self, shared):\n        return {\n            \"formatted_response\": shared.get(\"formatted_response\", {}),\n            \"original_query\": shared.get(\"current_query\", \"\"),\n            \"format_config\": self._get_format_config(shared),\n            \"fast_llm_model\": shared.get(\"fast_llm_model\"),\n            \"persona_config\": shared.get(\"persona_config\"),\n            \"agent_instance\": shared.get(\"agent_instance\"),\n        }\n\n    def _get_format_config(self, shared) -&gt; Optional[FormatConfig]:\n        \"\"\"Extrahiere Format-Konfiguration\"\"\"\n        persona = shared.get(\"persona_config\")\n        if persona and hasattr(persona, 'format_config'):\n            return persona.format_config\n        return None\n\n    async def exec_async(self, prep_res):\n        response_data = prep_res[\"formatted_response\"]\n        response_text = response_data.get(\"formatted_response\", \"\")\n        original_query = prep_res[\"original_query\"]\n        format_config = prep_res[\"format_config\"]\n\n        # Basis-Qualit\u00e4tspr\u00fcfung\n        base_quality = self._heuristic_quality_check(response_text, original_query)\n\n        # Format-spezifische Bewertung\n        format_quality = await self._evaluate_format_adherence(response_text, format_config)\n\n        # L\u00e4ngen-spezifische Bewertung\n        length_quality = self._evaluate_length_adherence(response_text, format_config)\n\n        # LLM-basierte Gesamtbewertung\n        llm_quality = 0.5\n        if LITELLM_AVAILABLE and len(response_text) &gt; 500:\n            llm_quality = await self._llm_format_quality_check(\n                response_text, original_query, format_config, prep_res\n            )\n\n        # Gewichtete Gesamtbewertung\n        total_quality = (\n            base_quality * 0.3 +\n            format_quality * 0.3 +\n            length_quality * 0.2 +\n            llm_quality * 0.2\n        )\n\n        quality_details = {\n            \"total_score\": total_quality,\n            \"base_quality\": base_quality,\n            \"format_adherence\": format_quality,\n            \"length_adherence\": length_quality,\n            \"llm_assessment\": llm_quality,\n            \"format_config_used\": format_config is not None\n        }\n\n        return {\n            \"quality_score\": total_quality,\n            \"quality_assessment\": self._score_to_assessment(total_quality),\n            \"quality_details\": quality_details,\n            \"suggestions\": self._generate_format_quality_suggestions(\n                total_quality, response_text, format_config, quality_details\n            )\n        }\n\n    async def _evaluate_format_adherence(self, response: str, format_config: Optional[FormatConfig]) -&gt; float:\n        \"\"\"Bewerte Format-Einhaltung\"\"\"\n        if not format_config:\n            return 0.8  # Neutral wenn kein Format vorgegeben\n\n        format_type = format_config.response_format\n        score = 0.5\n\n        # Format-spezifische Checks\n        if format_type == ResponseFormat.WITH_TABLES:\n            if '|' in response or 'Table:' in response or '| ' in response:\n                score += 0.4\n\n        elif format_type == ResponseFormat.WITH_BULLET_POINTS:\n            bullet_count = response.count('\u2022') + response.count('-') + response.count('*')\n            if bullet_count &gt;= 2:\n                score += 0.4\n            elif bullet_count &gt;= 1:\n                score += 0.2\n\n        elif format_type == ResponseFormat.WITH_LISTS:\n            list_patterns = ['1.', '2.', '3.', 'a)', 'b)', 'c)']\n            list_score = sum(1 for pattern in list_patterns if pattern in response)\n            score += min(0.4, list_score * 0.1)\n\n        elif format_type == ResponseFormat.MD_TEXT:\n            md_elements = ['#', '**', '*', '`', '```', '[', ']', '(', ')']\n            md_score = sum(1 for element in md_elements if element in response)\n            score += min(0.4, md_score * 0.05)\n\n        elif format_type == ResponseFormat.YAML_TEXT:\n            if response.strip().startswith(('```yaml', '---')) or ': ' in response:\n                score += 0.4\n\n        elif format_type == ResponseFormat.JSON_TEXT:\n            if response.strip().startswith(('{', '[')):\n                try:\n                    json.loads(response)\n                    score += 0.4\n                except:\n                    score += 0.1  # Partial credit for JSON-like structure\n\n        elif format_type == ResponseFormat.TEXT_ONLY:\n            # Penalize if formatting elements are present\n            format_elements = ['#', '*', '|', '```', '1.', '\u2022', '-']\n            format_count = sum(1 for element in format_elements if element in response)\n            score += max(0.1, 0.5 - format_count * 0.05)\n\n        elif format_type == ResponseFormat.PSEUDO_CODE:\n            code_indicators = ['if ', 'for ', 'while ', 'def ', 'return ', 'function', 'BEGIN', 'END']\n            code_score = sum(1 for indicator in code_indicators if indicator in response)\n            score += min(0.4, code_score * 0.1)\n\n        return max(0.0, min(1.0, score))\n\n    def _evaluate_length_adherence(self, response: str, format_config: Optional[FormatConfig]) -&gt; float:\n        \"\"\"Bewerte L\u00e4ngen-Einhaltung\"\"\"\n        if not format_config:\n            return 0.8\n\n        word_count = len(response.split())\n        min_words, max_words = format_config.get_expected_word_range()\n\n        if min_words &lt;= word_count &lt;= max_words:\n            return 1.0\n        elif word_count &lt; min_words:\n            # Zu kurz - sanfte Bestrafung\n            ratio = word_count / min_words\n            return max(0.3, ratio * 0.8)\n        else:  # word_count &gt; max_words\n            # Zu lang - weniger Bestrafung als zu kurz\n            excess_ratio = (word_count - max_words) / max_words\n            return max(0.4, 1.0 - excess_ratio * 0.3)\n\n    async def _llm_format_quality_check(\n        self,\n        response: str,\n        query: str,\n        format_config: Optional[FormatConfig],\n        prep_res: Dict\n    ) -&gt; float:\n        \"\"\"LLM-basierte Format- und Qualit\u00e4tsbewertung\"\"\"\n        if not format_config:\n            return await self._standard_llm_quality_check(response, query, prep_res)\n\n        format_desc = format_config.get_format_instructions()\n        length_desc = format_config.get_length_instructions()\n\n        prompt = f\"\"\"\nBewerte diese Antwort auf einer Skala von 0.0 bis 1.0 basierend auf Format-Einhaltung und Qualit\u00e4t:\n\nBenutzer-Anfrage: {query}\n\nAntwort: {response}\n\nErwartetes Format: {format_desc}\nErwartete L\u00e4nge: {length_desc}\n\nBewertungskriterien:\n1. Format-Einhaltung (40%): Entspricht die Antwort dem geforderten Format?\n2. L\u00e4ngen-Angemessenheit (25%): Ist die L\u00e4nge angemessen?\n3. Inhaltliche Qualit\u00e4t (25%): Beantwortet die Anfrage vollst\u00e4ndig?\n4. Lesbarkeit und Struktur (10%): Ist die Antwort gut strukturiert?\n\nAntworte nur mit einer Zahl zwischen 0.0 und 1.0:\"\"\"\n\n        try:\n            model_to_use = prep_res.get(\"fast_llm_model\", \"openrouter/anthropic/claude-3-haiku\")\n            agent_instance = prep_res[\"agent_instance\"]\n            score_text = await agent_instance.a_run_llm_completion(\n                model=model_to_use,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.1,\n                max_tokens=10,\n                node_name=\"QualityAssessmentNode\", task_id=\"format_quality_assessment\"\n            ).strip()\n\n            return float(score_text)\n\n        except Exception as e:\n            logger.warning(f\"LLM format quality check failed: {e}\")\n            return 0.6  # Neutral fallback\n\n    def _generate_format_quality_suggestions(\n        self,\n        score: float,\n        response: str,\n        format_config: Optional[FormatConfig],\n        quality_details: Dict\n    ) -&gt; List[str]:\n        \"\"\"Generiere Format-spezifische Verbesserungsvorschl\u00e4ge\"\"\"\n        suggestions = []\n\n        if not format_config:\n            return [\"Consider defining a specific response format for better consistency\"]\n\n        # Format-spezifische Vorschl\u00e4ge\n        if quality_details[\"format_adherence\"] &lt; 0.6:\n            format_type = format_config.response_format\n\n            if format_type == ResponseFormat.WITH_TABLES:\n                suggestions.append(\"Add tables using markdown format (| Column | Column |)\")\n            elif format_type == ResponseFormat.WITH_BULLET_POINTS:\n                suggestions.append(\"Use bullet points (\u2022, -, *) to structure information\")\n            elif format_type == ResponseFormat.MD_TEXT:\n                suggestions.append(\"Use markdown formatting (headers, bold, code blocks)\")\n            elif format_type == ResponseFormat.YAML_TEXT:\n                suggestions.append(\"Format response as valid YAML structure\")\n            elif format_type == ResponseFormat.JSON_TEXT:\n                suggestions.append(\"Format response as valid JSON\")\n\n        # L\u00e4ngen-spezifische Vorschl\u00e4ge\n        if quality_details[\"length_adherence\"] &lt; 0.6:\n            word_count = len(response.split())\n            min_words, max_words = format_config.get_expected_word_range()\n\n            if word_count &lt; min_words:\n                suggestions.append(f\"Response too short ({word_count} words). Aim for {min_words}-{max_words} words\")\n            else:\n                suggestions.append(f\"Response too long ({word_count} words). Aim for {min_words}-{max_words} words\")\n\n        # Qualit\u00e4ts-spezifische Vorschl\u00e4ge\n        if score &lt; 0.5:\n            suggestions.append(\"Overall quality needs improvement - consider regenerating\")\n        elif score &lt; 0.7:\n            suggestions.append(\"Good response but could be enhanced with better format adherence\")\n\n        return suggestions\n\n    async def _standard_llm_quality_check(self, response: str, query: str, prep_res: Dict) -&gt; float:\n        \"\"\"Standard LLM-Qualit\u00e4tspr\u00fcfung ohne Format-Fokus\"\"\"\n        # Bestehende Implementierung beibehalten\n        return await self._llm_quality_check(response, query, prep_res)\n\n    def _heuristic_quality_check(self, response: str, query: str) -&gt; float:\n        \"\"\"Heuristische Qualit\u00e4tspr\u00fcfung\"\"\"\n        score = 0.5  # Base score\n\n        # Length check\n        if len(response) &lt; 50:\n            score -= 0.3\n        elif len(response) &gt; 100:\n            score += 0.2\n\n        # Query term coverage\n        query_terms = set(query.lower().split())\n        response_terms = set(response.lower().split())\n        coverage = len(query_terms.intersection(response_terms)) / max(len(query_terms), 1)\n        score += coverage * 0.3\n\n        # Structure indicators\n        if any(indicator in response for indicator in [\":\", \"-\", \"1.\", \"\u2022\"]):\n            score += 0.1  # Structured response bonus\n\n        return max(0.0, min(1.0, score))\n\n    async def _llm_quality_check(self, response: str, query: str, prep_res: Dict) -&gt; float:\n        \"\"\"LLM-basierte Qualit\u00e4tspr\u00fcfung\"\"\"\n        try:\n            prompt = f\"\"\"\nRate the quality of this response to the user's query on a scale of 0.0 to 1.0.\n\nUser Query: {query}\n\nResponse: {response}\n\nConsider:\n- Relevance to the query\n- Completeness of information\n- Clarity and readability\n- Accuracy (if verifiable)\n\nRespond with just a number between 0.0 and 1.0:\"\"\"\n\n            model_to_use = prep_res.get(\"fast_llm_model\", \"openrouter/anthropic/claude-3-haiku\")\n            agent_instance = prep_res[\"agent_instance\"]\n            score_text = await agent_instance.a_run_llm_completion(\n                model=model_to_use,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.1,\n                max_tokens=10,\n                node_name=\"QualityAssessmentNode\", task_id=\"quality_assessment\"\n            ).strip()\n\n            return float(score_text)\n\n        except:\n            return 0.5  # Fallback score\n\n    def _score_to_assessment(self, score: float) -&gt; str:\n        if score &gt;= 0.8:\n            return \"quality_good\"\n        elif score &gt;= 0.5:\n            return \"quality_acceptable\"\n        else:\n            return \"quality_poor\"\n\n    async def post_async(self, shared, prep_res, exec_res):\n        shared[\"quality_assessment\"] = exec_res\n        return exec_res[\"quality_assessment\"]\n</code></pre> <code>ResultSynthesizerNode</code> \u00b6 <p>               Bases: <code>AsyncNode</code></p> <p>Synthetisiere finale Antwort aus allen Ergebnissen</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@with_progress_tracking\nclass ResultSynthesizerNode(AsyncNode):\n    \"\"\"Synthetisiere finale Antwort aus allen Ergebnissen\"\"\"\n\n    async def prep_async(self, shared):\n        return {\n            \"aggregated_context\": shared.get(\"aggregated_context\", {}),\n            \"fast_llm_model\": shared.get(\"fast_llm_model\"),\n            \"complex_llm_model\": shared.get(\"complex_llm_model\"),\n            \"agent_instance\": shared.get(\"agent_instance\")\n        }\n\n    async def exec_async(self, prep_res):\n        if not LITELLM_AVAILABLE:\n            return await self._fallback_synthesis(prep_res)\n\n        context = prep_res[\"aggregated_context\"]\n        persona = (prep_res['agent_instance'].amd.persona.to_system_prompt_addition() if not prep_res['agent_instance'].amd.persona.should_post_process() else '') if prep_res['agent_instance'].amd.persona else None\n        prompt = f\"\"\"\nDu bist ein Experte f\u00fcr Informationssynthese. Erstelle eine umfassende, hilfreiche Antwort basierend auf den gesammelten Ergebnissen.\n\n## Urspr\u00fcngliche Anfrage\n{context.get('original_query', '')}\n\n## Erfolgreiche Ergebnisse\n{self._format_successful_results(context.get('successful_results', {}))}\n\n## Wichtige Entdeckungen\n{self._format_key_discoveries(context.get('key_discoveries', []))}\n\n## Plan-Adaptationen\n{context.get('adaptation_summary', 'No adaptations were needed.')}\n\n## Fehlgeschlagene Versuche\n{self._format_failed_attempts(context.get('failed_attempts', {}))}\n\n{persona}\n\n## Anweisungen\n1. Gib eine direkte, hilfreiche Antwort auf die urspr\u00fcngliche Anfrage\n2. Integriere alle relevanten gefundenen Informationen\n3. Erkl\u00e4re kurz den Prozess, falls Adaptationen n\u00f6tig waren\n4. Sei ehrlich \u00fcber Limitationen oder fehlende Informationen\n5. Strukturiere die Antwort logisch und lesbar\n\nErstelle eine finale Antwort:\"\"\"\n\n        try:\n            # Verwende complex model f\u00fcr finale Synthesis\n            model_to_use = prep_res.get(\"complex_llm_model\", \"openrouter/openai/gpt-4o\")\n            agent_instance = prep_res[\"agent_instance\"]\n            synthesized_response = await agent_instance.a_run_llm_completion(\n                model=model_to_use,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.3,\n                max_tokens=1500,\n                node_name=\"ResultSynthesizerNode\", task_id=\"response_synthesis\"\n            )\n\n            return {\n                \"synthesized_response\": synthesized_response,\n                \"synthesis_method\": \"llm\",\n                \"model_used\": model_to_use,\n                \"confidence\": self._estimate_synthesis_confidence(context)\n            }\n\n        except Exception as e:\n            logger.error(f\"LLM synthesis failed: {e}\")\n            return await self._fallback_synthesis(prep_res)\n\n    def _format_successful_results(self, results: Dict) -&gt; str:\n        formatted = []\n        for task_id, result_info in results.items():\n            formatted.append(f\"- {result_info['task_description']}: {str(result_info['result'])[:200]}...\")\n        return \"\\n\".join(formatted) if formatted else \"No successful results to report.\"\n\n    def _format_key_discoveries(self, discoveries: List) -&gt; str:\n        formatted = []\n        for discovery in discoveries:\n            confidence = discovery.get('confidence', 0.0)\n            formatted.append(f\"- {discovery['discovery']} (Confidence: {confidence:.2f})\")\n        return \"\\n\".join(formatted) if formatted else \"No key discoveries.\"\n\n    def _format_failed_attempts(self, failed: Dict) -&gt; str:\n        if not failed:\n            return \"No significant failures.\"\n        formatted = [f\"- {info['description']}: {info['error']}\" for info in failed.values()]\n        return \"\\n\".join(formatted)\n\n    async def _fallback_synthesis(self, prep_res) -&gt; Dict:\n        \"\"\"Fallback synthesis ohne LLM\"\"\"\n        context = prep_res[\"aggregated_context\"]\n\n        # Einfache Template-basierte Synthese\n        response_parts = []\n\n        if context.get(\"key_discoveries\"):\n            response_parts.append(\"Based on my analysis, I found:\")\n            for discovery in context[\"key_discoveries\"][:3]:  # Top 3\n                response_parts.append(f\"- {discovery['discovery']}\")\n\n        if context.get(\"successful_results\"):\n            response_parts.append(\"\\nDetailed results:\")\n            for task_id, result in list(context[\"successful_results\"].items())[:2]:  # Top 2\n                response_parts.append(f\"- {result['task_description']}: {str(result['result'])[:150]}\")\n\n        if context.get(\"adaptation_summary\"):\n            response_parts.append(f\"\\n{context['adaptation_summary']}\")\n\n        fallback_response = \"\\n\".join(\n            response_parts) if response_parts else \"I was unable to complete the requested task effectively.\"\n\n        return {\n            \"synthesized_response\": fallback_response,\n            \"synthesis_method\": \"fallback\",\n            \"confidence\": 0.3\n        }\n\n    def _estimate_synthesis_confidence(self, context: Dict) -&gt; float:\n        \"\"\"Sch\u00e4tze Confidence der Synthese\"\"\"\n        confidence = 0.5  # Base confidence\n\n        # Boost f\u00fcr erfolgreiche Ergebnisse\n        successful_count = len(context.get(\"successful_results\", {}))\n        confidence += min(successful_count * 0.15, 0.3)\n\n        # Boost f\u00fcr key discoveries mit hoher confidence\n        for discovery in context.get(\"key_discoveries\", []):\n            discovery_conf = discovery.get(\"confidence\", 0.0)\n            confidence += discovery_conf * 0.1\n\n        # Penalty f\u00fcr viele fehlgeschlagene Versuche\n        failed_count = len(context.get(\"failed_attempts\", {}))\n        confidence -= min(failed_count * 0.1, 0.2)\n\n        return max(0.1, min(1.0, confidence))\n\n    async def post_async(self, shared, prep_res, exec_res):\n        shared[\"synthesized_response\"] = exec_res\n        if exec_res.get(\"synthesized_response\"):\n            return \"synthesized\"\n        else:\n            return \"synthesis_failed\"\n</code></pre> <code>StateSyncNode</code> \u00b6 <p>               Bases: <code>AsyncNode</code></p> <p>Synchronize state between world model and shared store</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@with_progress_tracking\nclass StateSyncNode(AsyncNode):\n    \"\"\"Synchronize state between world model and shared store\"\"\"\n    async def prep_async(self, shared):\n        world_model = shared.get(\"world_model\", {})\n        session_data = shared.get(\"session_data\", {})\n        tasks = shared.get(\"tasks\", {})\n        system_status = shared.get(\"system_status\", \"idle\")\n\n        return {\n            \"world_model\": world_model,\n            \"session_data\": session_data,\n            \"tasks\": tasks,\n            \"system_status\": system_status,\n            \"sync_timestamp\": datetime.now().isoformat()\n        }\n\n    async def exec_async(self, prep_res):\n        # Perform intelligent state synchronization\n        sync_result = {\n            \"world_model_updates\": {},\n            \"session_updates\": {},\n            \"task_updates\": {},\n            \"conflicts_resolved\": [],\n            \"sync_successful\": True\n        }\n\n        # Update world model with new information\n        if \"current_response\" in prep_res:\n            # Extract learnable facts from responses\n            extracted_facts = self._extract_facts(prep_res.get(\"current_response\", \"\"))\n            sync_result[\"world_model_updates\"].update(extracted_facts)\n\n        # Sync task states\n        for task_id, task in prep_res[\"tasks\"].items():\n            if task.status == \"completed\" and task.result:\n                # Store task results in world model\n                fact_key = f\"task_{task_id}_result\"\n                sync_result[\"world_model_updates\"][fact_key] = task.result\n\n        return sync_result\n\n    def _extract_facts(self, text: str) -&gt; Dict[str, Any]:\n        \"\"\"Extract learnable facts from text\"\"\"\n        facts = {}\n        lines = text.split('\\n')\n\n        for line in lines:\n            line = line.strip()\n            # Look for definitive statements\n            if ' is ' in line and not line.startswith('I ') and not '?' in line:\n                parts = line.split(' is ', 1)\n                if len(parts) == 2:\n                    subject = parts[0].strip().lower()\n                    predicate = parts[1].strip().rstrip('.')\n                    if len(subject.split()) &lt;= 3:  # Keep subjects simple\n                        facts[subject] = predicate\n\n        return facts\n\n    async def post_async(self, shared, prep_res, exec_res):\n        # Apply the synchronization results\n        if exec_res[\"sync_successful\"]:\n            shared[\"world_model\"].update(exec_res[\"world_model_updates\"])\n            shared[\"session_data\"].update(exec_res[\"session_updates\"])\n            shared[\"last_sync\"] = datetime.now()\n            return \"sync_complete\"\n        else:\n            logger.warning(\"State synchronization failed\")\n            return \"sync_failed\"\n</code></pre> <code>StrategyOrchestratorNode</code> \u00b6 <p>               Bases: <code>AsyncNode</code></p> <p>Strategic orchestration with meta-reasoning</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@with_progress_tracking\nclass StrategyOrchestratorNode(AsyncNode):\n    \"\"\"Strategic orchestration with meta-reasoning\"\"\"\n    def __init__(self, strategies: Dict[str, Dict] = None, **kwargs):\n        super().__init__(**kwargs)\n        self.strategies = strategies or self._get_default_strategies()\n\n    async def prep_async(self, shared):\n        current_query = shared.get(\"current_query\", \"\")\n        task_stack = shared.get(\"tasks\", {})\n        world_model = shared.get(\"world_model\", {})\n        system_status = shared.get(\"system_status\", \"idle\")\n        recent_performance = shared.get(\"performance_metrics\", {})\n\n        agent_instance = shared.get(\"agent_instance\")\n        tool_capabilities = {}\n        if agent_instance and hasattr(agent_instance, '_tool_capabilities'):\n            tool_capabilities = agent_instance._tool_capabilities\n\n        return {\n            \"query\": current_query,\n            \"tasks\": task_stack,\n            \"world_model\": world_model,\n            \"system_status\": system_status,\n            \"performance\": recent_performance,\n            \"available_strategies\": list(self.strategies.keys()),\n            \"fast_llm_model\": shared.get(\"fast_llm_model\"),\n            \"complex_llm_model\": shared.get(\"complex_llm_model\"),\n            \"tool_capabilities\": tool_capabilities,\n            \"agent_instance\": agent_instance,\n            \"available_tools_names\": shared.get(\"available_tools\", []),\n            \"variable_manager\": shared.get(\"variable_manager\")\n        }\n\n    async def exec_async(self, prep_res):\n        # LLM-basierte Strategieauswahl\n        strategy = await self._determine_strategy_llm(prep_res)\n\n        # Generate execution plan\n        execution_plan = await self._create_execution_plan(strategy, prep_res)\n\n        return {\n            \"selected_strategy\": strategy,\n            \"execution_plan\": execution_plan,\n            \"reasoning\": self._get_strategy_reasoning(strategy, prep_res),\n            \"estimated_complexity\": self._estimate_complexity(prep_res)\n        }\n\n    async def _determine_strategy_llm(self, prep_res) -&gt; str:\n        \"\"\"Enhanced strategy determination with variable awareness\"\"\"\n        if not LITELLM_AVAILABLE:\n            return \"direct_response\"\n\n        variable_manager = prep_res.get(\"variable_manager\")\n        tool_context = self._build_tool_awareness_context(prep_res)\n\n        # Variable-aware context\n        var_context = \"\"\n        if variable_manager:\n            var_context = variable_manager.get_llm_variable_context()\n\n        prompt = f\"\"\"\nYou are a strategic AI agent with advanced variable system and tool capabilities.\n\n## User Query\n{prep_res['query']}\n\n## Your Capabilities &amp; Context\n{tool_context}\n\n{var_context}\n\n## System Variables Available\n- User context: {{ user.name }}, {{ user.session }}, {{ user.id }}\n- System state: {{ system.timestamp }}, {{ agent.name }}\n- Previous results: {{ results.* }} (if any exist)\n- World knowledge: {{ world.* }} (learned facts)\n\n## Available Strategies:\n## Available Strategies:\n- direct_response: Simple LLM flow with optional tool calls\n- fast_simple_planning: Simple multi-step plan with tool orchestration\n- slow_complex_planning: Complex task breakdown with tool orchestration, use for tasks with mor then 2 'and' words.\n- research_and_analyze: Information gathering with variable integration\n- creative_generation: Content creation with personalization\n- problem_solving: Analysis with tool validation\n\n## Decision Criteria:\n1. Check if tools can directly answer the query\n2. Consider variable/personalization opportunities\n3. Evaluate complexity and multi-step needs\n4. Look for context dependencies\n\nRespond ONLY with strategy name for the current task:\"\"\"\n\n        try:\n            agent_instance = prep_res[\"agent_instance\"]\n            response = await agent_instance.a_run_llm_completion(\n                model=prep_res.get(\"complex_llm_model\", \"openrouter/anthropic/claude-3-haiku\"),\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.2,\n                max_tokens=50, node_name=\"StrategyOrchestratorNode\", task_id=\"strategy_determination\",\n            )\n            strategy = response.strip().lower()\n            return strategy if strategy in self.strategies else \"fast_simple_planning\"\n\n        except Exception as e:\n            logger.error(f\"Strategy determination failed: {e}\")\n            import traceback\n            print(traceback.format_exc())\n            return \"fast_simple_planning\"\n\n    def _build_tool_awareness_context(self, prep_res: Dict) -&gt; str:\n        \"\"\"Build comprehensive tool context for strategy decisions\"\"\"\n\n        tool_capabilities = prep_res.get(\"tool_capabilities\", {})\n        available_tools = prep_res.get(\"available_tools_names\", [])\n\n        if not available_tools:\n            return \"No tools available.\"\n\n        context_parts = []\n        context_parts.append(\"### Available Tools:\")\n\n        for tool_name in available_tools:\n            if tool_name in tool_capabilities:\n                cap = tool_capabilities[tool_name]\n                context_parts.append(f\"\\n**{tool_name}{cap.get('args_schema', '()')}:**\")\n                context_parts.append(f\"- Primary function: {cap.get('primary_function', 'Unknown')}\")\n                context_parts.append(f\"- Use cases: {', '.join(cap.get('use_cases', [])[:3])}\")\n                context_parts.append(f\"- Triggers: {', '.join(cap.get('trigger_phrases', [])[:5])}\")\n\n                # Add indirect connections\n                indirect = cap.get('indirect_connections', [])\n                if indirect:\n                    context_parts.append(f\"- Indirect uses: {', '.join([str(i) for i in indirect[:3]])}\")\n            else:\n                # Fallback for tools without analysis\n                context_parts.append(f\"\\n**{tool_name}:** Available but not analyzed\")\n\n        return \"\\n\".join(context_parts)\n\n    async def _create_execution_plan(self, strategy: str, prep_res: Dict) -&gt; Dict:\n        strategy_config = self.strategies[strategy]\n\n        plan = {\n            \"strategy\": strategy,\n            \"phases\": strategy_config[\"phases\"],\n            \"parallel_capable\": strategy_config.get(\"parallel_capable\", False),\n            \"estimated_steps\": len(strategy_config[\"phases\"]),\n            \"resource_requirements\": strategy_config.get(\"resources\", {}),\n            \"success_criteria\": strategy_config.get(\"success_criteria\", [])\n        }\n\n        return plan\n\n    def _get_default_strategies(self) -&gt; Dict[str, Dict]:\n        return {\n            \"direct_response\": {\n                \"phases\": [\"context_prep\", \"llm_call\", \"response_format\"],\n                \"parallel_capable\": False,\n                \"resources\": {\"llm_calls\": 1, \"complexity\": \"low\"}\n            },\n            \"fast_simple_planning\": {\n                \"phases\": [\"context_prep\", \"tool_llm_call\", \"result_synthesis\"],\n                \"parallel_capable\": True,\n                \"resources\": {\"llm_calls\": \"multiple\", \"complexity\": \"low\"}\n            },\n            \"slow_complex_planning\": {\n                \"phases\": [\"task_decomposition\", \"dependency_analysis\", \"parallel_execution\", \"result_synthesis\"],\n                \"parallel_capable\": True,\n                \"resources\": {\"llm_calls\": \"multiple\", \"complexity\": \"high\"}\n            },\n            \"research_and_analyze\": {\n                \"phases\": [\"query_expansion\", \"information_gathering\", \"analysis\", \"synthesis\"],\n                \"parallel_capable\": True,\n                \"resources\": {\"llm_calls\": \"multiple\", \"tools\": [\"search\", \"analysis\"]}\n            },\n            \"creative_generation\": {\n                \"phases\": [\"ideation\", \"structure_planning\", \"content_generation\", \"refinement\"],\n                \"parallel_capable\": False,\n                \"resources\": {\"llm_calls\": \"multiple\", \"complexity\": \"medium\"}\n            },\n            \"problem_solving\": {\n                \"phases\": [\"problem_analysis\", \"solution_exploration\", \"implementation_planning\", \"validation\"],\n                \"parallel_capable\": True,\n                \"resources\": {\"llm_calls\": \"multiple\", \"tools\": [\"code_execution\", \"testing\"]}\n            }\n        }\n\n    def _get_strategy_reasoning(self, strategy: str, prep_res: Dict) -&gt; str:\n        return f\"Selected '{strategy}' based on query analysis and current system state\"\n\n    def _estimate_complexity(self, prep_res: Dict) -&gt; str:\n        task_count = len(prep_res[\"tasks\"])\n        query_length = len(prep_res[\"query\"].split())\n\n        if task_count &gt; 5 or query_length &gt; 100:\n            return \"high\"\n        elif task_count &gt; 2 or query_length &gt; 20:\n            return \"medium\"\n        else:\n            return \"low\"\n\n    async def post_async(self, shared, prep_res, exec_res):\n        shared[\"selected_strategy\"] = exec_res[\"selected_strategy\"]\n        shared[\"execution_plan\"] = exec_res[\"execution_plan\"]\n        shared[\"strategy_reasoning\"] = exec_res[\"reasoning\"]\n        progress_tracker = shared.get(\"progress_tracker\")\n        if progress_tracker:\n            await progress_tracker.emit_event(ProgressEvent(\n                event_type=\"strategy_selected\",\n                timestamp=time.time(),\n                node_name=\"StrategyOrchestratorNode\",\n                status=NodeStatus.COMPLETED,\n                session_id=shared.get(\"session_id\"),\n                metadata={\"strategy\": exec_res[\"selected_strategy\"],\n                          \"reasoning\": exec_res[\"reasoning\"],\n                          \"estimated_complexity\": exec_res[\"estimated_complexity\"],\n                          \"execution_plan\": exec_res[\"execution_plan\"]}\n            ))\n        return exec_res[\"selected_strategy\"]\n</code></pre> <code>Task</code> <code>dataclass</code> \u00b6 Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass Task:\n    id: str\n    type: str\n    description: str\n    status: str = \"pending\"  # pending, running, completed, failed, paused\n    priority: int = 1\n    dependencies: List[str] = field(default_factory=list)\n    subtasks: List[str] = field(default_factory=list)\n    result: Any = None\n    error: str = None\n    created_at: datetime = field(default_factory=datetime.now)\n    started_at: Optional[datetime] = None\n    completed_at: Optional[datetime] = None\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    retry_count: int = 0\n    max_retries: int = 3\n    critical: bool = False\n\n\n    def __post_init__(self):\n        \"\"\"Ensure all mutable defaults are properly initialized\"\"\"\n        if self.metadata is None:\n            self.metadata = {}\n        if self.dependencies is None:\n            self.dependencies = []\n        if self.subtasks is None:\n            self.subtasks = []\n\n    def __getitem__(self, key):\n        return getattr(self, key)\n\n    def __setitem__(self, key, value):\n        setattr(self, key, value)\n</code></pre> <code>__post_init__()</code> \u00b6 <p>Ensure all mutable defaults are properly initialized</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Ensure all mutable defaults are properly initialized\"\"\"\n    if self.metadata is None:\n        self.metadata = {}\n    if self.dependencies is None:\n        self.dependencies = []\n    if self.subtasks is None:\n        self.subtasks = []\n</code></pre> <code>TaskExecutorNode</code> \u00b6 <p>               Bases: <code>AsyncNode</code></p> <p>Vollst\u00e4ndige Task-Ausf\u00fchrung als unabh\u00e4ngige Node mit LLM-unterst\u00fctzter Planung</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@with_progress_tracking\nclass TaskExecutorNode(AsyncNode):\n    \"\"\"Vollst\u00e4ndige Task-Ausf\u00fchrung als unabh\u00e4ngige Node mit LLM-unterst\u00fctzter Planung\"\"\"\n\n    def __init__(self, max_parallel: int = 3, **kwargs):\n        super().__init__(**kwargs)\n        self.max_parallel = max_parallel\n        self.results_store = {}  # F\u00fcr {{ }} Referenzen\n        self.execution_history = []  # F\u00fcr LLM-basierte Optimierung\n        self.agent_instance = None  # Wird gesetzt vom FlowAgent\n        self.variable_manager = None\n        self.fast_llm_model = None\n        self.complex_llm_model = None\n        self.progress_tracker = None\n\n    async def prep_async(self, shared):\n        \"\"\"Enhanced preparation with unified variable system\"\"\"\n        current_plan = shared.get(\"current_plan\")\n        tasks = shared.get(\"tasks\", {})\n\n        # Get unified variable manager\n        self.variable_manager = shared.get(\"variable_manager\")\n        self.progress_tracker = shared.get(\"progress_tracker\")\n        if not self.variable_manager:\n            self.variable_manager = VariableManager(shared.get(\"world_model\", {}), shared)\n\n        # Register all necessary scopes\n        self.variable_manager.set_results_store(self.results_store)\n        self.variable_manager.set_tasks_store(tasks)\n        self.variable_manager.register_scope('user', shared.get('user_context', {}))\n        self.variable_manager.register_scope('system', {\n            'timestamp': datetime.now().isoformat(),\n            'agent_name': shared.get('agent_instance', {}).amd.name if shared.get('agent_instance') else 'unknown'\n        })\n\n        # Stelle sicher, dass Agent-Referenz verf\u00fcgbar ist\n        if not self.agent_instance:\n            self.agent_instance = shared.get(\"agent_instance\")\n\n        if not current_plan:\n            return {\"error\": \"No active plan\", \"tasks\": tasks}\n\n        # Rest of existing prep_async logic...\n        ready_tasks = self._find_ready_tasks(current_plan, tasks)\n        blocked_tasks = self._find_blocked_tasks(current_plan, tasks)\n\n        execution_plan = await self._create_intelligent_execution_plan(\n            ready_tasks, blocked_tasks, current_plan, shared\n        )\n        self.complex_llm_model = shared.get(\"complex_llm_model\")\n        self.fast_llm_model = shared.get(\"fast_llm_model\")\n\n        return {\n            \"plan\": current_plan,\n            \"ready_tasks\": ready_tasks,\n            \"blocked_tasks\": blocked_tasks,\n            \"all_tasks\": tasks,\n            \"execution_plan\": execution_plan,\n            \"fast_llm_model\": self.fast_llm_model,\n            \"complex_llm_model\": self.complex_llm_model,\n            \"available_tools\": shared.get(\"available_tools\", []),\n            \"world_model\": shared.get(\"world_model\", {}),\n            \"results_store\": self.results_store,\n            \"variable_manager\": self.variable_manager,\n            \"progress_tracker\": self.progress_tracker ,\n        }\n\n    def _find_ready_tasks(self, plan: TaskPlan, all_tasks: Dict[str, Task]) -&gt; List[Task]:\n        \"\"\"Finde Tasks die zur Ausf\u00fchrung bereit sind\"\"\"\n        ready = []\n        for task in plan.tasks:\n            if task.status == \"pending\" and self._dependencies_satisfied(task, all_tasks):\n                ready.append(task)\n        return ready\n\n    def _find_blocked_tasks(self, plan: TaskPlan, all_tasks: Dict[str, Task]) -&gt; List[Task]:\n        \"\"\"Finde blockierte Tasks f\u00fcr Analyse\"\"\"\n        blocked = []\n        for task in plan.tasks:\n            if task.status == \"pending\" and not self._dependencies_satisfied(task, all_tasks):\n                blocked.append(task)\n        return blocked\n\n    def _dependencies_satisfied(self, task: Task, all_tasks: Dict[str, Task]) -&gt; bool:\n        \"\"\"Pr\u00fcfe ob alle Dependencies erf\u00fcllt sind\"\"\"\n        for dep_id in task.dependencies:\n            if dep_id in all_tasks:\n                dep_task = all_tasks[dep_id]\n                if dep_task.status not in [\"completed\"]:\n                    return False\n            else:\n                # Dependency existiert nicht - k\u00f6nnte Problem sein\n                logger.warning(f\"Task {task.id} has missing dependency: {dep_id}\")\n                return False\n        return True\n\n    async def _create_intelligent_execution_plan(\n        self,\n        ready_tasks: List[Task],\n        blocked_tasks: List[Task],\n        plan: TaskPlan,\n        shared: Dict\n    ) -&gt; Dict[str, Any]:\n        \"\"\"LLM-unterst\u00fctzte intelligente Ausf\u00fchrungsplanung\"\"\"\n\n        if not ready_tasks:\n            return {\n                \"strategy\": \"waiting\",\n                \"reason\": \"No ready tasks\",\n                \"blocked_count\": len(blocked_tasks),\n                \"recommendations\": []\n            }\n\n        # Einfache Planung f\u00fcr wenige Tasks\n        if len(ready_tasks) &lt;= 2 and not LITELLM_AVAILABLE:\n            return self._create_simple_execution_plan(ready_tasks, plan)\n\n        # LLM-basierte intelligente Planung\n        return await self._llm_execution_planning(ready_tasks, blocked_tasks, plan, shared)\n\n    def _create_simple_execution_plan(self, ready_tasks: List[Task], plan: TaskPlan) -&gt; Dict[str, Any]:\n        \"\"\"Einfache heuristische Ausf\u00fchrungsplanung\"\"\"\n\n        # Priorit\u00e4ts-basierte Sortierung\n        sorted_tasks = sorted(ready_tasks, key=lambda t: (t.priority, t.created_at))\n\n        # Parallelisierbare Tasks identifizieren\n        parallel_groups = []\n        current_group = []\n\n        for task in sorted_tasks:\n            # ToolTasks k\u00f6nnen oft parallel laufen\n            if isinstance(task, ToolTask) and len(current_group) &lt; self.max_parallel:\n                current_group.append(task)\n            else:\n                if current_group:\n                    parallel_groups.append(current_group)\n                    current_group = []\n                current_group.append(task)\n\n        if current_group:\n            parallel_groups.append(current_group)\n\n        strategy = \"parallel\" if len(parallel_groups) &gt; 1 or len(parallel_groups[0]) &gt; 1 else \"sequential\"\n\n        return {\n            \"strategy\": strategy,\n            \"execution_groups\": parallel_groups,\n            \"total_groups\": len(parallel_groups),\n            \"reasoning\": \"Simple heuristic: priority-based with tool parallelization\",\n            \"estimated_duration\": self._estimate_duration(sorted_tasks)\n        }\n\n    async def _llm_execution_planning(\n        self,\n        ready_tasks: List[Task],\n        blocked_tasks: List[Task],\n        plan: TaskPlan,\n        shared: Dict\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Erweiterte LLM-basierte Ausf\u00fchrungsplanung\"\"\"\n\n        try:\n            # Erstelle detaillierte Task-Analyse f\u00fcr LLM\n            task_analysis = self._analyze_tasks_for_llm(ready_tasks, blocked_tasks)\n            execution_context = self._build_execution_context(shared)\n\n            prompt = f\"\"\"\nDu bist ein Experte f\u00fcr Task-Ausf\u00fchrungsplanung. Analysiere die verf\u00fcgbaren Tasks und erstelle einen optimalen Ausf\u00fchrungsplan.\n\n## Verf\u00fcgbare Tasks zur Ausf\u00fchrung\n{task_analysis['ready_tasks_summary']}\n\n## Blockierte Tasks (zur Information)\n{task_analysis['blocked_tasks_summary']}\n\n## Ausf\u00fchrungskontext\n- Max parallele Tasks: {self.max_parallel}\n- Plan-Strategie: {plan.execution_strategy}\n- Verf\u00fcgbare Tools: {', '.join(shared.get('available_tools', []))}\n- Bisherige Ergebnisse: {len(self.results_store)} Tasks abgeschlossen\n- Execution History: {len(self.execution_history)} vorherige Zyklen\n\n## Bisherige Performance\n{execution_context}\n\n## Aufgabe\nErstelle einen optimierten Ausf\u00fchrungsplan. Ber\u00fccksichtige:\n1. Task-Abh\u00e4ngigkeiten und Priorit\u00e4ten\n2. Parallelisierungsm\u00f6glichkeiten\n3. Resource-Optimierung (Tools, LLM-Aufrufe)\n4. Fehlerwahrscheinlichkeit und Retry-Strategien\n5. Dynamische Argument-Aufl\u00f6sung zwischen Tasks\n\nAntworte mit YAML:\n\n```yaml\nstrategy: \"parallel\"  # \"parallel\" | \"sequential\" | \"hybrid\"\nexecution_groups:\n  - group_id: 1\n    tasks: [\"task_1\", \"task_2\"]  # Task IDs\n    execution_mode: \"parallel\"\n    priority: \"high\"\n    estimated_duration: 30  # seconds\n    risk_level: \"low\"  # low | medium | high\n    dependencies_resolved: true\n  - group_id: 2\n    tasks: [\"task_3\"]\n    execution_mode: \"sequential\"\n    priority: \"medium\"\n    estimated_duration: 15\n    depends_on_groups: [1]\nreasoning: \"Detailed explanation of the execution strategy\"\noptimization_suggestions:\n  - \"Specific optimization 1\"\n  - \"Specific optimization 2\"\nrisk_mitigation:\n  - risk: \"Tool timeout\"\n    mitigation: \"Use shorter timeout for parallel calls\"\n  - risk: \"Argument resolution failure\"\n    mitigation: \"Validate references before execution\"\ntotal_estimated_duration: 45\nconfidence: 0.85\n```\"\"\"\n\n            model_to_use = shared.get(\"complex_llm_model\", \"openrouter/openai/gpt-4o\")\n\n            content = await self.agent_instance.a_run_llm_completion(\n                model=model_to_use,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.3,\n                max_tokens=1000, node_name=\"TaskExecutorNode\", task_id=\"llm_execution_planning\"\n            )\n\n            yaml_match = re.search(r\"```yaml\\s*(.*?)\\s*```\", content, re.DOTALL)\n            yaml_str = yaml_match.group(1) if yaml_match else content.strip()\n\n            execution_plan = yaml.safe_load(yaml_str)\n\n            # Validiere und erweitere den Plan\n            validated_plan = self._validate_execution_plan(execution_plan, ready_tasks)\n\n            logger.info(\n                f\"LLM execution plan created: {validated_plan.get('strategy')} with {len(validated_plan.get('execution_groups', []))} groups\")\n            return validated_plan\n\n        except Exception as e:\n            logger.error(f\"LLM execution planning failed: {e}\")\n            return self._create_simple_execution_plan(ready_tasks, plan)\n\n    def _analyze_tasks_for_llm(self, ready_tasks: List[Task], blocked_tasks: List[Task]) -&gt; Dict[str, str]:\n        \"\"\"Analysiere Tasks f\u00fcr LLM-Prompt\"\"\"\n\n        ready_summary = []\n        for task in ready_tasks:\n            task_info = f\"- {task.id} ({task.type}): {task.description}\"\n            if hasattr(task, 'priority'):\n                task_info += f\" [Priority: {task.priority}]\"\n            if isinstance(task, ToolTask):\n                task_info += f\" [Tool: {task.tool_name}]\"\n                if task.arguments:\n                    # Zeige dynamische Referenzen\n                    dynamic_refs = [arg for arg in task.arguments.values() if isinstance(arg, str) and \"{{\" in arg]\n                    if dynamic_refs:\n                        task_info += f\" [Dynamic refs: {len(dynamic_refs)}]\"\n            ready_summary.append(task_info)\n\n        blocked_summary = []\n        for task in blocked_tasks:\n            deps = \", \".join(task.dependencies) if task.dependencies else \"None\"\n            blocked_summary.append(f\"- {task.id}: waiting for [{deps}]\")\n\n        return {\n            \"ready_tasks_summary\": \"\\n\".join(ready_summary) or \"No ready tasks\",\n            \"blocked_tasks_summary\": \"\\n\".join(blocked_summary) or \"No blocked tasks\"\n        }\n\n    def _build_execution_context(self, shared: Dict) -&gt; str:\n        \"\"\"Baue Kontext f\u00fcr LLM-Planung\"\"\"\n        context_parts = []\n\n        # Performance der letzten Executions\n        if self.execution_history:\n            recent = self.execution_history[-3:]  # Last 3 executions\n            avg_duration = sum(h.get(\"duration\", 0) for h in recent) / len(recent)\n            success_rate = sum(1 for h in recent if h.get(\"success\", False)) / len(recent)\n            context_parts.append(f\"Recent performance: {avg_duration:.1f}s avg, {success_rate:.1%} success rate\")\n\n        # Resource utilization\n        if self.results_store:\n            tool_usage = {}\n            for task_result in self.results_store.values():\n                metadata = task_result.get(\"metadata\", {})\n                task_type = metadata.get(\"task_type\", \"unknown\")\n                tool_usage[task_type] = tool_usage.get(task_type, 0) + 1\n            context_parts.append(f\"Resource usage: {tool_usage}\")\n\n        return \"\\n\".join(context_parts) if context_parts else \"No previous execution history\"\n\n    def _validate_execution_plan(self, plan: Dict, ready_tasks: List[Task]) -&gt; Dict:\n        \"\"\"Validiere und korrigiere LLM-generierten Ausf\u00fchrungsplan\"\"\"\n\n        # Standard-Werte setzen\n        validated = {\n            \"strategy\": plan.get(\"strategy\", \"sequential\"),\n            \"execution_groups\": [],\n            \"reasoning\": plan.get(\"reasoning\", \"LLM-generated plan\"),\n            \"total_estimated_duration\": plan.get(\"total_estimated_duration\", 60),\n            \"confidence\": min(1.0, max(0.0, plan.get(\"confidence\", 0.5)))\n        }\n\n        # Validiere execution groups\n        task_ids_available = [t.id for t in ready_tasks]\n\n        for group_data in plan.get(\"execution_groups\", []):\n            group_tasks = group_data.get(\"tasks\", [])\n            # Filtere nur verf\u00fcgbare Tasks\n            valid_tasks = [tid for tid in group_tasks if tid in task_ids_available]\n\n            if valid_tasks:\n                validated[\"execution_groups\"].append({\n                    \"group_id\": group_data.get(\"group_id\", len(validated[\"execution_groups\"]) + 1),\n                    \"tasks\": valid_tasks,\n                    \"execution_mode\": group_data.get(\"execution_mode\", \"sequential\"),\n                    \"priority\": group_data.get(\"priority\", \"medium\"),\n                    \"estimated_duration\": group_data.get(\"estimated_duration\", 30),\n                    \"risk_level\": group_data.get(\"risk_level\", \"medium\")\n                })\n\n        # Falls keine validen Groups, erstelle Fallback\n        if not validated[\"execution_groups\"]:\n            validated[\"execution_groups\"] = [{\n                \"group_id\": 1,\n                \"tasks\": task_ids_available[:self.max_parallel],\n                \"execution_mode\": \"parallel\",\n                \"priority\": \"high\"\n            }]\n\n        return validated\n\n    def _estimate_duration(self, tasks: List[Task]) -&gt; int:\n        \"\"\"Sch\u00e4tze Ausf\u00fchrungsdauer in Sekunden\"\"\"\n        duration = 0\n        for task in tasks:\n            if isinstance(task, ToolTask):\n                duration += 10  # Tool calls meist schneller\n            elif isinstance(task, LLMTask):\n                duration += 20  # LLM calls brauchen l\u00e4nger\n            else:\n                duration += 15  # Standard\n        return duration\n\n    async def exec_async(self, prep_res):\n        \"\"\"Hauptausf\u00fchrungslogik mit intelligentem Routing\"\"\"\n\n        if \"error\" in prep_res:\n            return {\"error\": prep_res[\"error\"]}\n\n        execution_plan = prep_res[\"execution_plan\"]\n\n        if execution_plan[\"strategy\"] == \"waiting\":\n            return {\n                \"status\": \"waiting\",\n                \"message\": execution_plan[\"reason\"],\n                \"blocked_count\": execution_plan.get(\"blocked_count\", 0)\n            }\n\n        # Starte Ausf\u00fchrung basierend auf Plan\n        execution_start = datetime.now()\n\n        try:\n            if execution_plan[\"strategy\"] == \"parallel\":\n                results = await self._execute_parallel_plan(execution_plan, prep_res)\n            elif execution_plan[\"strategy\"] == \"sequential\":\n                results = await self._execute_sequential_plan(execution_plan, prep_res)\n            else:  # hybrid\n                results = await self._execute_hybrid_plan(execution_plan, prep_res)\n\n            execution_duration = (datetime.now() - execution_start).total_seconds()\n\n            # Speichere Execution-History f\u00fcr LLM-Optimierung\n            self.execution_history.append({\n                \"timestamp\": execution_start.isoformat(),\n                \"strategy\": execution_plan[\"strategy\"],\n                \"duration\": execution_duration,\n                \"tasks_executed\": len(results),\n                \"success\": all(r.get(\"status\") == \"completed\" for r in results),\n                \"plan_confidence\": execution_plan.get(\"confidence\", 0.5)\n            })\n\n            # Behalte nur letzte 10 Executions\n            if len(self.execution_history) &gt; 10:\n                self.execution_history = self.execution_history[-10:]\n\n            return {\n                \"status\": \"executed\",\n                \"results\": results,\n                \"execution_duration\": execution_duration,\n                \"strategy_used\": execution_plan[\"strategy\"],\n                \"completed_tasks\": len([r for r in results if r.get(\"status\") == \"completed\"]),\n                \"failed_tasks\": len([r for r in results if r.get(\"status\") == \"failed\"])\n            }\n\n        except Exception as e:\n            logger.error(f\"Execution plan failed: {e}\")\n            return {\n                \"status\": \"execution_failed\",\n                \"error\": str(e),\n                \"results\": []\n            }\n\n    async def _execute_parallel_plan(self, plan: Dict, prep_res: Dict) -&gt; List[Dict]:\n        \"\"\"F\u00fchre Plan mit parallelen Gruppen aus\"\"\"\n        all_results = []\n\n        for group in plan[\"execution_groups\"]:\n            group_tasks = self._get_tasks_by_ids(group[\"tasks\"], prep_res)\n\n            if group.get(\"execution_mode\") == \"parallel\":\n                # Parallele Ausf\u00fchrung innerhalb der Gruppe\n                batch_results = await self._execute_parallel_batch(group_tasks)\n            else:\n                # Sequenzielle Ausf\u00fchrung innerhalb der Gruppe\n                batch_results = await self._execute_sequential_batch(group_tasks)\n\n            all_results.extend(batch_results)\n\n            # Pr\u00fcfe ob kritische Tasks fehlgeschlagen sind\n            critical_failures = [\n                r for r in batch_results\n                if r.get(\"status\") == \"failed\" and self._is_critical_task(r.get(\"task_id\"), prep_res)\n            ]\n\n            if critical_failures:\n                logger.error(f\"Critical task failures in group {group['group_id']}, stopping execution\")\n                break\n\n        return all_results\n\n    async def _execute_sequential_plan(self, plan: Dict, prep_res: Dict) -&gt; List[Dict]:\n        \"\"\"F\u00fchre Plan sequenziell aus\"\"\"\n        all_results = []\n\n        for group in plan[\"execution_groups\"]:\n            group_tasks = self._get_tasks_by_ids(group[\"tasks\"], prep_res)\n            batch_results = await self._execute_sequential_batch(group_tasks)\n            all_results.extend(batch_results)\n\n            # Stoppe bei kritischen Fehlern\n            critical_failures = [\n                r for r in batch_results\n                if r.get(\"status\") == \"failed\" and self._is_critical_task(r.get(\"task_id\"), prep_res)\n            ]\n\n            if critical_failures:\n                break\n\n        return all_results\n\n    async def _execute_hybrid_plan(self, plan: Dict, prep_res: Dict) -&gt; List[Dict]:\n        \"\"\"Hybride Ausf\u00fchrung - Groups parallel, innerhalb je nach Mode\"\"\"\n\n        # F\u00fchre Gruppen parallel aus (wenn m\u00f6glich)\n        group_tasks_list = []\n        for group in plan[\"execution_groups\"]:\n            group_tasks = self._get_tasks_by_ids(group[\"tasks\"], prep_res)\n            group_tasks_list.append((group, group_tasks))\n\n        # F\u00fchre bis zu max_parallel Gruppen parallel aus\n        batch_size = min(len(group_tasks_list), self.max_parallel)\n        all_results = []\n\n        for i in range(0, len(group_tasks_list), batch_size):\n            batch = group_tasks_list[i:i + batch_size]\n\n            # Erstelle Coroutines f\u00fcr jede Gruppe\n            group_coroutines = []\n            for group, tasks in batch:\n                if group.get(\"execution_mode\") == \"parallel\":\n                    coro = self._execute_parallel_batch(tasks)\n                else:\n                    coro = self._execute_sequential_batch(tasks)\n                group_coroutines.append(coro)\n\n            # F\u00fchre Gruppen-Batch parallel aus\n            batch_results = await asyncio.gather(*group_coroutines, return_exceptions=True)\n\n            # Flache Liste der Ergebnisse\n            for result_group in batch_results:\n                if isinstance(result_group, Exception):\n                    logger.error(f\"Group execution failed: {result_group}\")\n                    continue\n                all_results.extend(result_group)\n\n        return all_results\n\n    def _get_tasks_by_ids(self, task_ids: List[str], prep_res: Dict) -&gt; List[Task]:\n        \"\"\"Hole Task-Objekte basierend auf IDs\"\"\"\n        all_tasks = prep_res[\"all_tasks\"]\n        return [all_tasks[tid] for tid in task_ids if tid in all_tasks]\n\n    def _is_critical_task(self, task_id: str, prep_res: Dict) -&gt; bool:\n        \"\"\"Pr\u00fcfe ob Task kritisch ist\"\"\"\n        task = prep_res[\"all_tasks\"].get(task_id)\n        if not task:\n            return False\n        return getattr(task, 'critical', False) or task.priority == 1\n\n    async def _execute_parallel_batch(self, tasks: List[Task]) -&gt; List[Dict]:\n        \"\"\"F\u00fchre Tasks parallel aus\"\"\"\n        if not tasks:\n            return []\n\n        # Limitiere auf max_parallel\n        batch_size = min(len(tasks), self.max_parallel)\n        batches = [tasks[i:i + batch_size] for i in range(0, len(tasks), batch_size)]\n\n        all_results = []\n        for batch in batches:\n            batch_results = await asyncio.gather(\n                *[self._execute_single_task(task) for task in batch],\n                return_exceptions=True\n            )\n\n            # Handle exceptions\n            processed_results = []\n            for i, result in enumerate(batch_results):\n                if isinstance(result, Exception):\n                    logger.error(f\"Task {batch[i].id} failed with exception: {result}\")\n                    processed_results.append({\n                        \"task_id\": batch[i].id,\n                        \"status\": \"failed\",\n                        \"error\": str(result)\n                    })\n                else:\n                    processed_results.append(result)\n\n            all_results.extend(processed_results)\n\n        return all_results\n\n    async def _execute_sequential_batch(self, tasks: List[Task]) -&gt; List[Dict]:\n        \"\"\"F\u00fchre Tasks sequenziell aus\"\"\"\n        results = []\n\n        for task in tasks:\n            try:\n                result = await self._execute_single_task(task)\n                results.append(result)\n\n                # Stoppe bei kritischen Fehlern in sequenzieller Ausf\u00fchrung\n                if result.get(\"status\") == \"failed\" and getattr(task, 'critical', False):\n                    logger.error(f\"Critical task {task.id} failed, stopping sequential execution\")\n                    break\n\n            except Exception as e:\n                logger.error(f\"Sequential task {task.id} failed: {e}\")\n                results.append({\n                    \"task_id\": task.id,\n                    \"status\": \"failed\",\n                    \"error\": str(e)\n                })\n\n                if getattr(task, 'critical', False):\n                    break\n\n        return results\n\n    async def _execute_single_task(self, task: Task) -&gt; Dict:\n        \"\"\"Enhanced task execution with unified LLMToolNode usage\"\"\"\n        if self.progress_tracker:\n            await self.progress_tracker.emit_event(ProgressEvent(\n                event_type=\"task_start\",\n                timestamp=time.time(),\n                node_name=\"TaskExecutorNode\",\n                status=NodeStatus.RUNNING,\n                task_id=task.id,\n                metadata={\n                    \"task_type\": task.type,\n                    \"task_description\": task.description,\n                    \"priority\": task.priority,\n                    \"dependencies\": task.dependencies\n                }\n            ))\n\n        task_start = time.perf_counter()\n        try:\n            task.status = \"running\"\n            task.started_at = datetime.now()\n\n            # Ensure metadata is initialized\n            if not hasattr(task, 'metadata') or task.metadata is None:\n                task.metadata = {}\n\n            # Pre-process task with variable resolution\n            if isinstance(task, ToolTask):\n                resolved_args = self._resolve_task_variables(task.arguments)\n                result = await self._execute_tool_task_with_validation(task, resolved_args)\n            elif isinstance(task, LLMTask):\n                # Use LLMToolNode for LLM tasks instead of direct execution\n                result = await self._execute_llm_via_llmtool(task)\n            elif isinstance(task, DecisionTask):\n                # Enhanced decision task with context awareness\n                result = await self._execute_decision_task_enhanced(task)\n            else:\n                # Use LLMToolNode for generic tasks as well\n                result = await self._execute_generic_via_llmtool(task)\n\n            # Store result in unified system\n            self._store_task_result(task.id, result, True)\n\n            task.result = result\n            task.status = \"completed\"\n            task.completed_at = datetime.now()\n\n            task_duration = time.perf_counter() - task_start\n\n            if self.progress_tracker:\n                await self.progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"task_complete\",\n                    timestamp=time.time(),\n                    node_name=\"TaskExecutorNode\",\n                    task_id=task.id,\n                    status=NodeStatus.COMPLETED,\n                    success=True,\n                    node_duration=task_duration,\n                    metadata={\n                        \"success\": True,\n                        \"result_type\": type(result).__name__,\n                        \"task_type\": task.type\n                    }\n                ))\n\n            return {\n                \"task_id\": task.id,\n                \"status\": \"completed\",\n                \"result\": result\n            }\n\n        except Exception as e:\n            task.error = str(e)\n            task.status = \"failed\"\n            task.retry_count += 1\n\n            # Store error in unified system\n            self._store_task_result(task.id, None, False, str(e))\n            task_duration = time.perf_counter() - task_start\n\n            if self.progress_tracker:\n                await self.progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"error\",\n                    timestamp=time.time(),\n                    node_name=\"TaskExecutorNode\",\n                    task_id=task.id,\n                    status=NodeStatus.FAILED,\n                    success=False,\n                    node_duration=task_duration,\n                    metadata={\n                        \"error\": str(e),\n                        \"error_type\": type(e).__name__,\n                        \"task_type\": task.type,\n                        \"retry_count\": task.retry_count\n                    }\n                ))\n\n            logger.error(f\"Task {task.id} failed: {e}\")\n            return {\n                \"task_id\": task.id,\n                \"status\": \"failed\",\n                \"error\": str(e),\n                \"retry_count\": task.retry_count\n            }\n\n    async def _resolve_dynamic_arguments(self, arguments: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Enhanced dynamic argument resolution with full variable system\"\"\"\n        resolved = {}\n\n        for key, value in arguments.items():\n            if isinstance(value, str):\n                # FIXED: Use unified variable manager for all resolution\n                resolved_value = self.variable_manager.format_text(value)\n\n                # Log if variables weren't resolved (debugging)\n                if \"{{\" in resolved_value and \"}}\" in resolved_value:\n                    logger.warning(f\"Unresolved variables in argument '{key}': {resolved_value}\")\n\n                resolved[key] = resolved_value\n            else:\n                resolved[key] = value\n\n        return resolved\n\n    async def _execute_tool_task_with_validation(self, task: ToolTask, resolved_args: Dict[str, Any]) -&gt; Any:\n        \"\"\"Tool execution with improved error detection and validation\"\"\"\n\n        if not task.tool_name:\n            raise ValueError(f\"ToolTask {task.id} missing tool_name\")\n\n        agent = self.agent_instance\n        if not agent:\n            raise ValueError(\"Agent instance not available for tool execution\")\n\n        tool_start = time.perf_counter()\n\n        # Track tool call start\n        if self.progress_tracker:\n            await self.progress_tracker.emit_event(ProgressEvent(\n                event_type=\"tool_call\",\n                timestamp=time.time(),\n                node_name=\"TaskExecutorNode\",\n                status=NodeStatus.RUNNING,\n                task_id=task.id,\n                tool_name=task.tool_name,\n                tool_args=resolved_args,\n                metadata={\n                    \"task_type\": \"ToolTask\",\n                    \"hypothesis\": task.hypothesis,\n                    \"validation_criteria\": task.validation_criteria\n                }\n            ))\n\n        try:\n            logger.info(f\"Executing tool {task.tool_name} with resolved args: {resolved_args}\")\n\n            # Execute tool with timeout and retry logic\n            result = await self._execute_tool_with_retries(task.tool_name, resolved_args, agent)\n\n            tool_duration = time.perf_counter() - tool_start\n\n            # Validate result before marking as success\n            is_valid_result = self._validate_tool_result(result, task)\n\n            if not is_valid_result:\n                raise ValueError(f\"Tool {task.tool_name} returned invalid result: {type(result).__name__}\")\n\n            # Track successful tool call\n            if self.progress_tracker:\n                await self.progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"tool_call\",\n                    timestamp=time.time(),\n                    node_name=\"TaskExecutorNode\",\n                    task_id=task.id,\n                    status=NodeStatus.COMPLETED,\n                    tool_name=task.tool_name,\n                    tool_args=resolved_args,\n                    tool_result=result,\n                    tool_duration=tool_duration,\n                    tool_success=True,\n                    metadata={\n                        \"task_type\": \"ToolTask\",\n                        \"result_type\": type(result).__name__,\n                        \"result_length\": len(str(result)),\n                        \"validation_passed\": is_valid_result\n                    }\n                ))\n\n            # FIXED: Store in variable manager with correct path structure\n            if self.variable_manager:\n                self.variable_manager.set(f\"results.{task.id}.data\", result)\n                self.variable_manager.set(f\"tasks.{task.id}.result\", result)\n\n            return result\n\n        except Exception as e:\n            tool_duration = time.perf_counter() - tool_start\n\n            # Detailed error tracking\n            if self.progress_tracker:\n                await self.progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"tool_call\",\n                    timestamp=time.time(),\n                    node_name=\"TaskExecutorNode\",\n                    task_id=task.id,\n                    status=NodeStatus.FAILED,\n                    tool_name=task.tool_name,\n                    tool_args=resolved_args,\n                    tool_duration=tool_duration,\n                    tool_success=False,\n                    tool_error=str(e),\n                    metadata={\n                        \"task_type\": \"ToolTask\",\n                        \"error_type\": type(e).__name__,\n                        \"retry_attempted\": hasattr(self, '_retry_count')\n                    }\n                ))\n\n            logger.error(f\"Tool execution failed for {task.tool_name}: {e}\")\n            raise\n    async def _execute_llm_via_llmtool(self, task: LLMTask) -&gt; Any:\n        \"\"\"Execute LLM task via LLMToolNode for consistency\"\"\"\n\n        # Prepare context for LLMToolNode\n        llm_shared = {\n            \"current_task_description\": task.description,\n            \"formatted_context\": {\n                \"recent_interaction\": f\"Executing LLM task: {task.description}\",\n                \"session_summary\": \"\",\n                \"task_context\": f\"Task ID: {task.id}, Priority: {task.priority}\"\n            },\n            \"variable_manager\": self.variable_manager,\n            \"agent_instance\": self.agent_instance,\n            \"available_tools\": [],  # No tools for LLM-only tasks\n            \"tool_capabilities\": {},\n            \"fast_llm_model\": self.fast_llm_model,\n            \"complex_llm_model\": self.complex_llm_model,\n            \"progress_tracker\": self.progress_tracker,\n            \"session_id\": getattr(self, 'session_id', 'task_executor'),\n            \"use_fast_response\": task.llm_config.get(\"model_preference\", \"fast\") == \"fast\"\n        }\n\n        # Create LLMToolNode instance\n        llm_node = LLMToolNode()\n\n        # Execute via LLMToolNode\n        try:\n            result = await llm_node.run_async(llm_shared)\n            # shared[\"current_response\"]\n            # shared[\"tool_calls_made\"]\n            # shared[\"llm_tool_conversation\"]\n            # shared[\"synthesized_response\"]\n            return llm_shared[\"current_response\"]\n        except Exception as e:\n            logger.error(f\"LLMToolNode execution failed for task {task.id}: {e}\")\n            # Fallback to direct execution\n            import traceback\n            print(traceback.format_exc())\n            return await self._execute_llm_task_enhanced(task)\n\n    async def _execute_llm_task_enhanced(self, task: LLMTask) -&gt; Any:\n        \"\"\"Enhanced LLM task execution with unified variable system\"\"\"\n        if not LITELLM_AVAILABLE:\n            raise Exception(\"LiteLLM not available for LLM tasks\")\n\n        # Get model preference with variable support\n        llm_config = task.llm_config\n        model_preference = llm_config.get(\"model_preference\", \"fast\")\n\n        if model_preference == \"complex\":\n            model_to_use = self.variable_manager.get(\"system.complex_llm_model\", \"openrouter/openai/gpt-4o\")\n        else:\n            model_to_use = self.variable_manager.get(\"system.fast_llm_model\", \"openrouter/anthropic/claude-3-haiku\")\n\n        # Build context for prompt\n        context_data = {}\n        for context_key in task.context_keys:\n            value = self.variable_manager.get(context_key)\n            if value is not None:\n                context_data[context_key] = value\n\n        # Resolve prompt template with full variable system\n        final_prompt = self.variable_manager.format_text(\n            task.prompt_template,\n            context=context_data\n        )\n\n        llm_start = time.perf_counter()\n\n        if self.progress_tracker:\n            await self.progress_tracker.emit_event(ProgressEvent(\n                event_type=\"llm_call\",\n                timestamp=time.time(),\n                node_name=\"TaskExecutorNode\",\n                status=NodeStatus.RUNNING,\n                task_id=task.id,\n                llm_model=model_to_use,\n                llm_temperature=llm_config.get(\"temperature\", 0.7),\n                metadata={\n                    \"task_type\": \"LLMTask\",\n                    \"model_preference\": model_preference,\n                    \"prompt_length\": len(final_prompt)\n                }\n            ))\n\n        try:\n\n            response = await litellm.acompletion(\n                model=model_to_use,\n                messages=[{\"role\": \"user\", \"content\": final_prompt}],\n                temperature=llm_config.get(\"temperature\", 0.7),\n                max_tokens=llm_config.get(\"max_tokens\", 1024)\n            )\n\n            llm_duration = time.perf_counter() - llm_start\n            result = response.choices[0].message.content\n\n            # Extract token usage and cost\n            usage = response.usage\n            input_tokens = usage.prompt_tokens if usage else 0\n            output_tokens = usage.completion_tokens if usage else 0\n            total_tokens = usage.total_tokens if usage else 0\n\n            call_cost = self.progress_tracker.calculate_llm_cost(model_to_use, input_tokens,\n                                                                 output_tokens) if self.progress_tracker else 0.0\n\n            if self.progress_tracker:\n                await self.progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"llm_call\",\n                    timestamp=time.time(),\n                    node_name=\"TaskExecutorNode\",\n                    task_id=task.id,\n                    status=NodeStatus.COMPLETED,\n                    llm_model=model_to_use,\n                    llm_prompt_tokens=input_tokens,\n                    llm_completion_tokens=output_tokens,\n                    llm_total_tokens=total_tokens,\n                    llm_cost=call_cost,\n                    llm_duration=llm_duration,\n                    llm_temperature=llm_config.get(\"temperature\", 0.7),\n                    metadata={\n                        \"success\": True,\n                        \"result_length\": len(result),\n                        \"task_type\": \"LLMTask\"\n                    }\n                ))\n\n            # Store intermediate result for other tasks\n            self.variable_manager.set(f\"tasks.{task.id}.result\", result)\n\n            # Output schema validation if present\n            if task.output_schema:\n                stripped = result.strip()\n\n                try:\n                    # Try JSON first if it looks like JSON\n                    if stripped.startswith('{') or stripped.startswith('['):\n                        parsed = json.loads(stripped)\n                    else:\n                        parsed = yaml.safe_load(stripped)\n\n                    # Ensure metadata is a dict before updating\n                    if not isinstance(task.metadata, dict):\n                        task.metadata = {}\n\n                    # Save parsed result\n                    task.metadata[\"parsed_output\"] = parsed\n\n                except (json.JSONDecodeError, yaml.YAMLError):\n                    # Save info about failure without logging output\n                    if not isinstance(task.metadata, dict):\n                        task.metadata = {}\n                    task.metadata[\"parsed_output_error\"] = \"Invalid JSON/YAML format\"\n\n                except Exception as e:\n                    if not isinstance(task.metadata, dict):\n                        task.metadata = {}\n                    task.metadata[\"parsed_output_error\"] = f\"Unexpected error: {str(e)}\"\n\n            return result\n        except Exception as e:\n            llm_duration = time.perf_counter() - llm_start\n\n            if self.progress_tracker:\n                await self.progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"error\",\n                    timestamp=time.time(),\n                    status=NodeStatus.FAILED,\n                    node_name=\"TaskExecutorNode\",\n                    task_id=task.id,\n                    llm_model=model_to_use,\n                    llm_duration=llm_duration,\n                    metadata={\n                        \"error\": str(e),\n                        \"task_type\": \"LLMTask\",\n                        \"error_type\": type(e).__name__\n                    }\n                ))\n\n            raise\n\n    async def _execute_decision_task_enhanced(self, task: DecisionTask) -&gt; str:\n        \"\"\"Enhanced DecisionTask with intelligent replan assessment\"\"\"\n\n        if not LITELLM_AVAILABLE:\n            raise Exception(\"LiteLLM not available for decision tasks\")\n\n        # Build comprehensive context for decision\n        decision_context = self._build_decision_context(task)\n\n        # Enhanced decision prompt with full context\n        enhanced_prompt = f\"\"\"\n    You are making a critical routing decision for task execution. Analyze all context carefully.\n\n    ## Current Situation\n    {task.decision_prompt}\n\n    ## Execution Context\n    {decision_context}\n\n    ## Available Routing Options\n    {json.dumps(task.routing_map, indent=2)}\n\n    ## Decision Guidelines\n    1. Only trigger \"replan_from_here\" if there's a genuine failure that cannot be recovered\n    2. Use \"route_to_task\" for normal flow continuation\n    3. Consider the full context, not just immediate results\n    4. Be conservative with replanning - it's expensive and can cause loops\n\n    Based on ALL the context above, what is your decision?\n    Respond with EXACTLY one of these options: {', '.join(task.routing_map.keys())}\n\n    Your decision:\"\"\"\n\n        model_to_use = self.fast_llm_model if hasattr(self, 'fast_llm_model') else \"openrouter/anthropic/claude-3-haiku\"\n\n        try:\n            response = await litellm.acompletion(\n                model=model_to_use,\n                messages=[{\"role\": \"user\", \"content\": enhanced_prompt}],\n                temperature=0.1,\n                max_tokens=50\n            )\n\n            decision = response.choices[0].message.content.strip().lower()\n\n            # Find matching key (case-insensitive)\n            matched_key = None\n            for key in task.routing_map.keys():\n                if key.lower() == decision:\n                    matched_key = key\n                    break\n\n            if not matched_key:\n                logger.warning(f\"Decision '{decision}' not in routing map, using first option\")\n                matched_key = list(task.routing_map.keys())[0] if task.routing_map else \"continue\"\n\n            routing_instruction = task.routing_map.get(matched_key, matched_key)\n\n            # Enhanced metadata with decision reasoning\n            if not hasattr(task, 'metadata'):\n                task.metadata = {}\n\n            task.metadata.update({\n                \"decision_made\": matched_key,\n                \"routing_instruction\": routing_instruction,\n                \"decision_context\": decision_context,\n                \"replan_justified\": self._assess_replan_necessity(matched_key, routing_instruction, decision_context)\n            })\n\n            # Handle dynamic planning instructions\n            if isinstance(routing_instruction, dict) and \"action\" in routing_instruction:\n                action = routing_instruction[\"action\"]\n\n                if action == \"replan_from_here\":\n                    # Add extensive context for replanning\n                    task.metadata[\"replan_context\"] = {\n                        \"new_goal\": routing_instruction.get(\"new_goal\", \"Continue with alternative approach\"),\n                        \"failure_reason\": f\"Decision task {task.id} determined: {matched_key}\",\n                        \"original_task\": task.id,\n                        \"context\": routing_instruction.get(\"context\", \"\"),\n                        \"execution_history\": self._get_execution_history_summary(),\n                        \"failed_approaches\": self._identify_failed_approaches(),\n                        \"success_indicators\": self._identify_success_patterns()\n                    }\n\n                self.variable_manager.set(f\"tasks.{task.id}.result\", {\n                    \"decision\": matched_key,\n                    \"action\": action,\n                    \"instruction\": routing_instruction,\n                    \"confidence\": self._calculate_decision_confidence(decision_context)\n                })\n\n                return action\n\n            else:\n                # Traditional routing\n                next_task_id = routing_instruction if isinstance(routing_instruction, str) else str(routing_instruction)\n\n                task.metadata.update({\n                    \"next_task_id\": next_task_id,\n                    \"routing_action\": \"route_to_task\"\n                })\n\n                self.variable_manager.set(f\"tasks.{task.id}.result\", {\n                    \"decision\": matched_key,\n                    \"next_task\": next_task_id\n                })\n\n                return matched_key\n\n        except Exception as e:\n            logger.error(f\"Enhanced decision task failed: {e}\")\n            raise\n\n    async def post_async(self, shared, prep_res, exec_res):\n        \"\"\"Erweiterte Post-Processing mit dynamischer Plan-Anpassung\"\"\"\n\n        # Results store in shared state integrieren\n        shared[\"results_store\"] = self.results_store\n\n        if exec_res is None or \"error\" in exec_res:\n            shared[\"executor_performance\"] = {\"status\": \"error\", \"last_error\": exec_res.get(\"error\")}\n            return \"execution_error\"\n\n        if exec_res[\"status\"] == \"waiting\":\n            shared[\"executor_status\"] = \"waiting_for_dependencies\"\n            return \"waiting\"\n\n        # Performance-Metriken speichern\n        performance_data = {\n            \"execution_duration\": exec_res.get(\"execution_duration\", 0),\n            \"strategy_used\": exec_res.get(\"strategy_used\", \"unknown\"),\n            \"completed_tasks\": exec_res.get(\"completed_tasks\", 0),\n            \"failed_tasks\": exec_res.get(\"failed_tasks\", 0),\n            \"success_rate\": exec_res.get(\"completed_tasks\", 0) / max(len(exec_res.get(\"results\", [])), 1),\n            \"timestamp\": datetime.now().isoformat()\n        }\n        shared[\"executor_performance\"] = performance_data\n\n        # Check for dynamic planning actions\n        planning_action_detected = False\n\n        for result in exec_res.get(\"results\", []):\n            task_id = result[\"task_id\"]\n            if task_id in shared[\"tasks\"]:\n                task = shared[\"tasks\"][task_id]\n                task.status = result[\"status\"]\n\n                if result[\"status\"] == \"completed\":\n                    task.result = result[\"result\"]\n\n                    # Check for planning actions from DecisionTasks\n                    if hasattr(task, 'metadata') and task.metadata:\n                        routing_action = task.metadata.get(\"routing_action\")\n\n                        if routing_action == \"replan_from_here\":\n                            shared[\"needs_dynamic_replan\"] = True\n                            shared[\"replan_context\"] = task.metadata.get(\"replan_context\", {})\n                            planning_action_detected = True\n                            logger.info(f\"Dynamic replan triggered by task {task_id}\")\n\n                        elif routing_action == \"append_plan\":\n                            shared[\"needs_plan_append\"] = True\n                            shared[\"append_context\"] = task.metadata.get(\"append_context\", {})\n                            planning_action_detected = True\n                            logger.info(f\"Plan append triggered by task {task_id}\")\n\n                    # Store verification results if available\n                    if result.get(\"verification\"):\n                        if not hasattr(task, 'metadata'):\n                            task.metadata = {}\n                        task.metadata[\"verification\"] = result[\"verification\"]\n\n                elif result[\"status\"] == \"failed\":\n                    task.error = result.get(\"error\", \"Unknown error\")\n\n        # Return appropriate status based on planning actions\n        if planning_action_detected:\n            if shared.get(\"needs_dynamic_replan\"):\n                return \"needs_dynamic_replan\"  # Goes to PlanReflectorNode\n            elif shared.get(\"needs_plan_append\"):\n                return \"needs_plan_append\"  # Goes to PlanReflectorNode\n\n        # Regular completion checking\n        current_plan = shared[\"current_plan\"]\n        if current_plan:\n            all_finished = all(\n                shared[\"tasks\"][task.id].status in [\"completed\", \"failed\"]\n                for task in current_plan.tasks\n            )\n\n            if all_finished:\n                current_plan.status = \"completed\"\n                shared[\"plan_completion_time\"] = datetime.now().isoformat()\n                logger.info(f\"Plan {current_plan.id} finished\")\n                return \"plan_completed\"\n            else:\n                ready_tasks = [\n                    task for task in current_plan.tasks\n                    if shared[\"tasks\"][task.id].status == \"pending\"\n                ]\n\n                if ready_tasks:\n                    return \"continue_execution\"\n                else:\n                    return \"waiting\"\n\n        return \"execution_complete\"\n\n    def get_execution_statistics(self) -&gt; Dict[str, Any]:\n        \"\"\"Erhalte detaillierte Ausf\u00fchrungsstatistiken\"\"\"\n        if not self.execution_history:\n            return {\"message\": \"No execution history available\"}\n\n        history = self.execution_history\n\n        return {\n            \"total_executions\": len(history),\n            \"average_duration\": sum(h[\"duration\"] for h in history) / len(history),\n            \"success_rate\": sum(1 for h in history if h[\"success\"]) / len(history),\n            \"strategy_usage\": {\n                strategy: sum(1 for h in history if h[\"strategy\"] == strategy)\n                for strategy in set(h[\"strategy\"] for h in history)\n            },\n            \"total_tasks_executed\": sum(h[\"tasks_executed\"] for h in history),\n            \"average_confidence\": sum(h[\"plan_confidence\"] for h in history) / len(history),\n            \"recent_performance\": history[-3:] if len(history) &gt;= 3 else history\n        }\n\n    def _resolve_task_variables(self, data):\n        \"\"\"Unified variable resolution for any task data\"\"\"\n        if isinstance(data, str):\n            res = self.variable_manager.format_text(data)\n            return res\n        elif isinstance(data, dict):\n            resolved = {}\n            for key, value in data.items():\n                resolved[key] = self._resolve_task_variables(value)\n            return resolved\n        elif isinstance(data, list):\n            return [self._resolve_task_variables(item) for item in data]\n        else:\n            return data\n\n    def _store_task_result(self, task_id: str, result: Any, success: bool, error: str = None):\n        \"\"\"Store task result in unified variable system\"\"\"\n        result_data = {\n            \"data\": result,\n            \"metadata\": {\n                \"task_type\": \"task\",\n                \"completed_at\": datetime.now().isoformat(),\n                \"success\": success\n            }\n        }\n\n        if error:\n            result_data[\"error\"] = error\n            result_data[\"metadata\"][\"success\"] = False\n\n        # Store in results_store and update variable manager\n        self.results_store[task_id] = result_data\n        self.variable_manager.set_results_store(self.results_store)\n\n        # FIXED: Store actual result data, not the wrapper object\n        self.variable_manager.set(f\"results.{task_id}.data\", result)\n        self.variable_manager.set(f\"results.{task_id}.metadata\", result_data[\"metadata\"])\n        if error:\n            self.variable_manager.set(f\"results.{task_id}.error\", error)\n\n    def _build_decision_context(self, task: DecisionTask) -&gt; str:\n        \"\"\"Build comprehensive context for decision making\"\"\"\n\n        context_parts = []\n\n        # Recent execution results\n        recent_results = []\n        for task_id, result_data in list(self.results_store.items())[-3:]:\n            success = result_data.get(\"metadata\", {}).get(\"success\", False)\n            status = \"\u2713\" if success else \"\u2717\"\n            data_preview = str(result_data.get(\"data\", \"\"))[:100] + \"...\"\n            recent_results.append(f\"{status} {task_id}: {data_preview}\")\n\n        if recent_results:\n            context_parts.append(\"Recent Results:\\n\" + \"\\n\".join(recent_results))\n\n        # Variable context\n        if self.variable_manager:\n            available_vars = list(self.variable_manager.get_available_variables().keys())[:10]\n            context_parts.append(f\"Available Variables: {', '.join(available_vars)}\")\n\n        # Execution history\n        execution_summary = self._get_execution_history_summary()\n        if execution_summary:\n            context_parts.append(f\"Execution Summary: {execution_summary}\")\n\n        # Current world model insights\n        world_insights = self._get_world_model_insights()\n        if world_insights:\n            context_parts.append(f\"Known Facts: {world_insights}\")\n\n        return \"\\n\\n\".join(context_parts)\n\n    def _assess_replan_necessity(self, decision: str, routing_instruction: Any, context: str) -&gt; bool:\n        \"\"\"Assess if replanning is truly necessary\"\"\"\n\n        if not isinstance(routing_instruction, dict):\n            return False\n\n        action = routing_instruction.get(\"action\", \"\")\n        if action != \"replan_from_here\":\n            return False\n\n        # Check if we have genuine failures\n        genuine_failures = \"error\" in context.lower() or \"failed\" in context.lower()\n        alternative_available = len(self.results_store) &gt; 0  # Have some results to work with\n\n        # Be conservative - only replan if really necessary\n        return genuine_failures and not alternative_available\n\n    async def _execute_tool_with_retries(self, tool_name: str, args: Dict, agent, max_retries: int = 2) -&gt; Any:\n        \"\"\"Execute tool with retry logic\"\"\"\n\n        last_exception = None\n\n        for attempt in range(max_retries + 1):\n            try:\n                result = await agent.arun_function(tool_name, **args)\n\n                # Additional validation - check if result indicates success\n                if self._is_tool_result_success(result):\n                    return result\n                elif attempt &lt; max_retries:\n                    logger.warning(f\"Tool {tool_name} returned unclear result, retrying...\")\n                    continue\n                else:\n                    return result\n\n            except Exception as e:\n                last_exception = e\n                if attempt &lt; max_retries:\n                    logger.warning(f\"Tool {tool_name} failed (attempt {attempt + 1}), retrying: {e}\")\n                    await asyncio.sleep(0.5 * (attempt + 1))  # Progressive delay\n                else:\n                    logger.error(f\"Tool {tool_name} failed after {max_retries + 1} attempts\")\n\n        if last_exception:\n            raise last_exception\n        else:\n            raise RuntimeError(f\"Tool {tool_name} failed without exception\")\n\n    def _validate_tool_result(self, result: Any, task: ToolTask) -&gt; bool:\n        \"\"\"Validate tool result to prevent false failures\"\"\"\n\n        # Basic validation\n        if result is None:\n            return False\n\n        # Check for common error indicators\n        if isinstance(result, str):\n            error_indicators = [\"error\", \"failed\", \"exception\", \"timeout\", \"not found\"]\n            result_lower = result.lower()\n\n            # If result contains error indicators but also has substantial content, it might still be valid\n            has_errors = any(indicator in result_lower for indicator in error_indicators)\n            has_content = len(result.strip()) &gt; 20\n\n            if has_errors and not has_content:\n                return False\n\n        # Check against expectation if provided\n        if hasattr(task, 'expectation') and task.expectation:\n            expectation_keywords = task.expectation.lower().split()\n            result_text = str(result).lower()\n\n            # At least one expectation keyword should be present\n            if not any(keyword in result_text for keyword in expectation_keywords):\n                logger.warning(f\"Tool result doesn't match expectation: {task.expectation}\")\n\n        return True\n\n    def _is_tool_result_success(self, result: Any) -&gt; bool:\n        \"\"\"Determine if a tool result indicates success\"\"\"\n\n        if result is None:\n            return False\n\n        if isinstance(result, bool):\n            return result\n\n        if isinstance(result, (list, dict)):\n            return len(result) &gt; 0\n\n        if isinstance(result, str):\n            # Check for explicit success/failure indicators\n            result_lower = result.lower()\n\n            success_indicators = [\"success\", \"completed\", \"found\", \"retrieved\", \"generated\"]\n            failure_indicators = [\"error\", \"failed\", \"not found\", \"timeout\", \"exception\"]\n\n            has_success = any(indicator in result_lower for indicator in success_indicators)\n            has_failure = any(indicator in result_lower for indicator in failure_indicators)\n\n            if has_success and not has_failure:\n                return True\n            elif has_failure and not has_success:\n                return False\n            else:\n                # Ambiguous - assume success if there's substantial content\n                return len(result.strip()) &gt; 10\n\n        # For other types, assume success if not None\n        return True\n\n    def _get_execution_history_summary(self) -&gt; str:\n        \"\"\"Get concise execution history summary\"\"\"\n\n        if not hasattr(self, 'execution_history') or not self.execution_history:\n            return \"No execution history\"\n\n        recent = self.execution_history[-3:]  # Last 3 executions\n        summaries = []\n\n        for hist in recent:\n            status = \"Success\" if hist.get(\"success\", False) else \"Failed\"\n            duration = hist.get(\"duration\", 0)\n            strategy = hist.get(\"strategy\", \"Unknown\")\n            summaries.append(f\"{strategy}: {status} ({duration:.1f}s)\")\n\n        return \"; \".join(summaries)\n\n    def _identify_failed_approaches(self) -&gt; List[str]:\n        \"\"\"Identify approaches that have consistently failed\"\"\"\n\n        failed_approaches = []\n\n        # Analyze failed tasks\n        for task_id, result_data in self.results_store.items():\n            if not result_data.get(\"metadata\", {}).get(\"success\", True):\n                error = result_data.get(\"error\", \"\")\n                if \"tool\" in error.lower():\n                    failed_approaches.append(\"direct_tool_approach\")\n                elif \"search\" in error.lower():\n                    failed_approaches.append(\"search_based_approach\")\n                elif \"llm\" in error.lower():\n                    failed_approaches.append(\"llm_direct_approach\")\n\n        return list(set(failed_approaches))\n\n    def _identify_success_patterns(self) -&gt; List[str]:\n        \"\"\"Identify patterns that have led to success\"\"\"\n\n        success_patterns = []\n\n        # Analyze successful tasks\n        successful_results = [\n            r for r in self.results_store.values()\n            if r.get(\"metadata\", {}).get(\"success\", False)\n        ]\n\n        if successful_results:\n            # Identify common patterns\n            if len(successful_results) &gt; 1:\n                success_patterns.append(\"multi_step_approach\")\n\n            for result in successful_results:\n                data = result.get(\"data\", \"\")\n                if isinstance(data, str) and len(data) &gt; 100:\n                    success_patterns.append(\"detailed_information_retrieval\")\n\n        return list(set(success_patterns))\n\n    def _get_world_model_insights(self) -&gt; str:\n        \"\"\"Get relevant insights from world model\"\"\"\n\n        if not self.variable_manager:\n            return \"\"\n\n        world_data = self.variable_manager.scopes.get(\"world\", {})\n        if not world_data:\n            return \"No world model data\"\n\n        # Get most recent or relevant facts\n        recent_facts = []\n        for key, value in list(world_data.items())[:5]:  # Top 5 facts\n            recent_facts.append(f\"{key}: {str(value)[:50]}...\")\n\n        return \"; \".join(recent_facts)\n\n    def _calculate_decision_confidence(self, context: str) -&gt; float:\n        \"\"\"Calculate confidence in decision based on context\"\"\"\n\n        # Simple heuristic based on context richness\n        base_confidence = 0.5\n\n        # Boost confidence if we have rich context\n        if len(context) &gt; 200:\n            base_confidence += 0.2\n\n        # Boost if we have recent results\n        if \"Recent Results:\" in context:\n            base_confidence += 0.2\n\n        # Reduce if there are many failures\n        failure_count = context.lower().count(\"failed\") + context.lower().count(\"error\")\n        base_confidence -= min(failure_count * 0.1, 0.3)\n\n        return max(0.1, min(1.0, base_confidence))\n</code></pre> <code>exec_async(prep_res)</code> <code>async</code> \u00b6 <p>Hauptausf\u00fchrungslogik mit intelligentem Routing</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def exec_async(self, prep_res):\n    \"\"\"Hauptausf\u00fchrungslogik mit intelligentem Routing\"\"\"\n\n    if \"error\" in prep_res:\n        return {\"error\": prep_res[\"error\"]}\n\n    execution_plan = prep_res[\"execution_plan\"]\n\n    if execution_plan[\"strategy\"] == \"waiting\":\n        return {\n            \"status\": \"waiting\",\n            \"message\": execution_plan[\"reason\"],\n            \"blocked_count\": execution_plan.get(\"blocked_count\", 0)\n        }\n\n    # Starte Ausf\u00fchrung basierend auf Plan\n    execution_start = datetime.now()\n\n    try:\n        if execution_plan[\"strategy\"] == \"parallel\":\n            results = await self._execute_parallel_plan(execution_plan, prep_res)\n        elif execution_plan[\"strategy\"] == \"sequential\":\n            results = await self._execute_sequential_plan(execution_plan, prep_res)\n        else:  # hybrid\n            results = await self._execute_hybrid_plan(execution_plan, prep_res)\n\n        execution_duration = (datetime.now() - execution_start).total_seconds()\n\n        # Speichere Execution-History f\u00fcr LLM-Optimierung\n        self.execution_history.append({\n            \"timestamp\": execution_start.isoformat(),\n            \"strategy\": execution_plan[\"strategy\"],\n            \"duration\": execution_duration,\n            \"tasks_executed\": len(results),\n            \"success\": all(r.get(\"status\") == \"completed\" for r in results),\n            \"plan_confidence\": execution_plan.get(\"confidence\", 0.5)\n        })\n\n        # Behalte nur letzte 10 Executions\n        if len(self.execution_history) &gt; 10:\n            self.execution_history = self.execution_history[-10:]\n\n        return {\n            \"status\": \"executed\",\n            \"results\": results,\n            \"execution_duration\": execution_duration,\n            \"strategy_used\": execution_plan[\"strategy\"],\n            \"completed_tasks\": len([r for r in results if r.get(\"status\") == \"completed\"]),\n            \"failed_tasks\": len([r for r in results if r.get(\"status\") == \"failed\"])\n        }\n\n    except Exception as e:\n        logger.error(f\"Execution plan failed: {e}\")\n        return {\n            \"status\": \"execution_failed\",\n            \"error\": str(e),\n            \"results\": []\n        }\n</code></pre> <code>get_execution_statistics()</code> \u00b6 <p>Erhalte detaillierte Ausf\u00fchrungsstatistiken</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def get_execution_statistics(self) -&gt; Dict[str, Any]:\n    \"\"\"Erhalte detaillierte Ausf\u00fchrungsstatistiken\"\"\"\n    if not self.execution_history:\n        return {\"message\": \"No execution history available\"}\n\n    history = self.execution_history\n\n    return {\n        \"total_executions\": len(history),\n        \"average_duration\": sum(h[\"duration\"] for h in history) / len(history),\n        \"success_rate\": sum(1 for h in history if h[\"success\"]) / len(history),\n        \"strategy_usage\": {\n            strategy: sum(1 for h in history if h[\"strategy\"] == strategy)\n            for strategy in set(h[\"strategy\"] for h in history)\n        },\n        \"total_tasks_executed\": sum(h[\"tasks_executed\"] for h in history),\n        \"average_confidence\": sum(h[\"plan_confidence\"] for h in history) / len(history),\n        \"recent_performance\": history[-3:] if len(history) &gt;= 3 else history\n    }\n</code></pre> <code>post_async(shared, prep_res, exec_res)</code> <code>async</code> \u00b6 <p>Erweiterte Post-Processing mit dynamischer Plan-Anpassung</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def post_async(self, shared, prep_res, exec_res):\n    \"\"\"Erweiterte Post-Processing mit dynamischer Plan-Anpassung\"\"\"\n\n    # Results store in shared state integrieren\n    shared[\"results_store\"] = self.results_store\n\n    if exec_res is None or \"error\" in exec_res:\n        shared[\"executor_performance\"] = {\"status\": \"error\", \"last_error\": exec_res.get(\"error\")}\n        return \"execution_error\"\n\n    if exec_res[\"status\"] == \"waiting\":\n        shared[\"executor_status\"] = \"waiting_for_dependencies\"\n        return \"waiting\"\n\n    # Performance-Metriken speichern\n    performance_data = {\n        \"execution_duration\": exec_res.get(\"execution_duration\", 0),\n        \"strategy_used\": exec_res.get(\"strategy_used\", \"unknown\"),\n        \"completed_tasks\": exec_res.get(\"completed_tasks\", 0),\n        \"failed_tasks\": exec_res.get(\"failed_tasks\", 0),\n        \"success_rate\": exec_res.get(\"completed_tasks\", 0) / max(len(exec_res.get(\"results\", [])), 1),\n        \"timestamp\": datetime.now().isoformat()\n    }\n    shared[\"executor_performance\"] = performance_data\n\n    # Check for dynamic planning actions\n    planning_action_detected = False\n\n    for result in exec_res.get(\"results\", []):\n        task_id = result[\"task_id\"]\n        if task_id in shared[\"tasks\"]:\n            task = shared[\"tasks\"][task_id]\n            task.status = result[\"status\"]\n\n            if result[\"status\"] == \"completed\":\n                task.result = result[\"result\"]\n\n                # Check for planning actions from DecisionTasks\n                if hasattr(task, 'metadata') and task.metadata:\n                    routing_action = task.metadata.get(\"routing_action\")\n\n                    if routing_action == \"replan_from_here\":\n                        shared[\"needs_dynamic_replan\"] = True\n                        shared[\"replan_context\"] = task.metadata.get(\"replan_context\", {})\n                        planning_action_detected = True\n                        logger.info(f\"Dynamic replan triggered by task {task_id}\")\n\n                    elif routing_action == \"append_plan\":\n                        shared[\"needs_plan_append\"] = True\n                        shared[\"append_context\"] = task.metadata.get(\"append_context\", {})\n                        planning_action_detected = True\n                        logger.info(f\"Plan append triggered by task {task_id}\")\n\n                # Store verification results if available\n                if result.get(\"verification\"):\n                    if not hasattr(task, 'metadata'):\n                        task.metadata = {}\n                    task.metadata[\"verification\"] = result[\"verification\"]\n\n            elif result[\"status\"] == \"failed\":\n                task.error = result.get(\"error\", \"Unknown error\")\n\n    # Return appropriate status based on planning actions\n    if planning_action_detected:\n        if shared.get(\"needs_dynamic_replan\"):\n            return \"needs_dynamic_replan\"  # Goes to PlanReflectorNode\n        elif shared.get(\"needs_plan_append\"):\n            return \"needs_plan_append\"  # Goes to PlanReflectorNode\n\n    # Regular completion checking\n    current_plan = shared[\"current_plan\"]\n    if current_plan:\n        all_finished = all(\n            shared[\"tasks\"][task.id].status in [\"completed\", \"failed\"]\n            for task in current_plan.tasks\n        )\n\n        if all_finished:\n            current_plan.status = \"completed\"\n            shared[\"plan_completion_time\"] = datetime.now().isoformat()\n            logger.info(f\"Plan {current_plan.id} finished\")\n            return \"plan_completed\"\n        else:\n            ready_tasks = [\n                task for task in current_plan.tasks\n                if shared[\"tasks\"][task.id].status == \"pending\"\n            ]\n\n            if ready_tasks:\n                return \"continue_execution\"\n            else:\n                return \"waiting\"\n\n    return \"execution_complete\"\n</code></pre> <code>prep_async(shared)</code> <code>async</code> \u00b6 <p>Enhanced preparation with unified variable system</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def prep_async(self, shared):\n    \"\"\"Enhanced preparation with unified variable system\"\"\"\n    current_plan = shared.get(\"current_plan\")\n    tasks = shared.get(\"tasks\", {})\n\n    # Get unified variable manager\n    self.variable_manager = shared.get(\"variable_manager\")\n    self.progress_tracker = shared.get(\"progress_tracker\")\n    if not self.variable_manager:\n        self.variable_manager = VariableManager(shared.get(\"world_model\", {}), shared)\n\n    # Register all necessary scopes\n    self.variable_manager.set_results_store(self.results_store)\n    self.variable_manager.set_tasks_store(tasks)\n    self.variable_manager.register_scope('user', shared.get('user_context', {}))\n    self.variable_manager.register_scope('system', {\n        'timestamp': datetime.now().isoformat(),\n        'agent_name': shared.get('agent_instance', {}).amd.name if shared.get('agent_instance') else 'unknown'\n    })\n\n    # Stelle sicher, dass Agent-Referenz verf\u00fcgbar ist\n    if not self.agent_instance:\n        self.agent_instance = shared.get(\"agent_instance\")\n\n    if not current_plan:\n        return {\"error\": \"No active plan\", \"tasks\": tasks}\n\n    # Rest of existing prep_async logic...\n    ready_tasks = self._find_ready_tasks(current_plan, tasks)\n    blocked_tasks = self._find_blocked_tasks(current_plan, tasks)\n\n    execution_plan = await self._create_intelligent_execution_plan(\n        ready_tasks, blocked_tasks, current_plan, shared\n    )\n    self.complex_llm_model = shared.get(\"complex_llm_model\")\n    self.fast_llm_model = shared.get(\"fast_llm_model\")\n\n    return {\n        \"plan\": current_plan,\n        \"ready_tasks\": ready_tasks,\n        \"blocked_tasks\": blocked_tasks,\n        \"all_tasks\": tasks,\n        \"execution_plan\": execution_plan,\n        \"fast_llm_model\": self.fast_llm_model,\n        \"complex_llm_model\": self.complex_llm_model,\n        \"available_tools\": shared.get(\"available_tools\", []),\n        \"world_model\": shared.get(\"world_model\", {}),\n        \"results_store\": self.results_store,\n        \"variable_manager\": self.variable_manager,\n        \"progress_tracker\": self.progress_tracker ,\n    }\n</code></pre> <code>TaskManagementFlow</code> \u00b6 <p>               Bases: <code>AsyncFlow</code></p> <p>Fixed Task-Management-Flow with proper termination</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@with_progress_tracking\nclass TaskManagementFlow(AsyncFlow):\n    \"\"\"Fixed Task-Management-Flow with proper termination\"\"\"\n\n    def __init__(self, max_parallel_tasks: int = 3):\n        # Create all nodes\n        self.strategy_node = StrategyOrchestratorNode()\n        self.planner_node = TaskPlannerNode()\n        self.executor_node = TaskExecutorNode(max_parallel=max_parallel_tasks)\n        self.reflector_node = PlanReflectorNode()\n        self.sync_node = StateSyncNode()\n        self.llm_tool_node = LLMToolNode()\n\n        # Add a completion checker node to break cycles\n        self.completion_checker = CompletionCheckerNode()\n\n        # === MAIN FLOW (Linear with controlled cycles) ===\n\n        # Strategy -&gt; Planning Phase\n        self.strategy_node - \"fast_simple_planning\" &gt;&gt; self.planner_node\n        self.strategy_node - \"slow_complex_planning\" &gt;&gt; self.planner_node\n        self.strategy_node - \"research_and_analyze\" &gt;&gt; self.planner_node\n        self.strategy_node - \"creative_generation\" &gt;&gt; self.planner_node\n        self.strategy_node - \"problem_solving\" &gt;&gt; self.planner_node\n\n        self.strategy_node - \"direct_response\" &gt;&gt; self.llm_tool_node\n        self.llm_tool_node - \"llm_tool_complete\" &gt;&gt; self.sync_node\n        self.strategy_node - \"default\" &gt;&gt; self.planner_node\n\n        # Planning -&gt; Execution\n        self.planner_node - \"planned\" &gt;&gt; self.executor_node\n        self.planner_node - \"planning_failed\" &gt;&gt; self.sync_node\n        self.planner_node - \"default\" &gt;&gt; self.sync_node\n\n        # === EXECUTION CYCLE WITH TERMINATION CONTROL ===\n\n        # Executor -&gt; Completion Checker (instead of direct to reflector)\n        self.executor_node - \"plan_completed\" &gt;&gt; self.completion_checker\n        self.executor_node - \"continue_execution\" &gt;&gt; self.completion_checker\n        self.executor_node - \"execution_error\" &gt;&gt; self.reflector_node\n        self.executor_node - \"waiting\" &gt;&gt; self.completion_checker\n        self.executor_node - \"execution_complete\" &gt;&gt; self.completion_checker  # HINZUGEF\u00dcGT\n\n        # f\u00fcr dynamische Aktionen\n        self.executor_node - \"needs_dynamic_replan\" &gt;&gt; self.reflector_node\n        self.executor_node - \"needs_plan_append\" &gt;&gt; self.reflector_node\n\n\n        self.executor_node - \"default\" &gt;&gt; self.completion_checker  # GE\u00c4NDERT von sync_node\n\n        # Completion Checker decides next action\n        self.completion_checker - \"truly_complete\" &gt;&gt; self.sync_node  # TERMINATE\n        self.completion_checker - \"needs_reflection\" &gt;&gt; self.reflector_node\n        self.completion_checker - \"force_terminate\" &gt;&gt; self.sync_node  # HINZUGEF\u00dcGT\n        self.completion_checker - \"continue_execution\" &gt;&gt; self.executor_node\n        self.completion_checker - \"default\" &gt;&gt; self.sync_node\n\n        # Reflector actions (limited cycles)\n        self.reflector_node - \"needs_replan\" &gt;&gt; self.planner_node\n        self.reflector_node - \"needs_plan_append\" &gt;&gt; self.planner_node\n        self.reflector_node - \"adapted\" &gt;&gt; self.completion_checker\n        self.reflector_node - \"continue\" &gt;&gt; self.completion_checker\n        self.reflector_node - \"plan_halted\" &gt;&gt; self.sync_node\n        self.reflector_node - \"default\" &gt;&gt; self.sync_node\n\n        super().__init__(start=self.strategy_node)\n</code></pre> <code>TaskPlannerNode</code> \u00b6 <p>               Bases: <code>AsyncNode</code></p> <p>Erweiterte Aufgabenplanung mit dynamischen Referenzen und Tool-Integration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@with_progress_tracking\nclass TaskPlannerNode(AsyncNode):\n    \"\"\"Erweiterte Aufgabenplanung mit dynamischen Referenzen und Tool-Integration\"\"\"\n\n    async def prep_async(self, shared):\n        return {\n            \"query\": shared.get(\"current_query\", \"\"),\n            \"tasks\": shared.get(\"tasks\", {}),\n            \"system_status\": shared.get(\"system_status\", \"idle\"),\n            \"tool_capabilities\": shared.get(\"tool_capabilities\", {}),\n            \"available_tools_names\": shared.get(\"available_tools\", []),\n            \"strategy\": shared.get(\"selected_strategy\", \"direct_response\"),\n            \"fast_llm_model\": shared.get(\"fast_llm_model\"),\n            \"complex_llm_model\": shared.get(\"complex_llm_model\"),\n            \"agent_instance\": shared.get(\"agent_instance\"),\n            \"variable_manager\": shared.get(\"variable_manager\"),\n        }\n\n    async def exec_async(self, prep_res):\n        if prep_res[\"strategy\"] == \"fast_simple_planning\":\n            return await self._create_simple_plan(prep_res)\n        else:\n            return await self._advanced_llm_decomposition(prep_res)\n\n    async def post_async(self, shared, prep_res, exec_res):\n        \"\"\"Post-processing nach Plan-Erstellung\"\"\"\n\n        if exec_res is None:\n            shared[\"planning_error\"] = \"Plan creation returned None\"\n            return \"planning_failed\"\n\n        if isinstance(exec_res, TaskPlan):\n\n            progress_tracker = shared.get(\"progress_tracker\")\n            if progress_tracker:\n                e = ProgressEvent(\n                    event_type=\"plan_created\",\n                    timestamp=time.time(),\n                    status=NodeStatus.COMPLETING,\n                    node_name=\"TaskPlannerNode\",\n                    node_phase=\"plan_created\",\n                    session_id=shared.get(\"session_id\"),\n                    metadata={\"plan_name\": exec_res.name, \"task_count\": len(exec_res.tasks),\n                              \"strategy\": exec_res.execution_strategy, \"full_plan\": exec_res}\n                )\n                await progress_tracker.emit_event(e)\n                await asyncio.sleep(0.1)\n\n            # Erfolgreicher Plan\n            shared[\"current_plan\"] = exec_res\n\n            # Tasks in shared state f\u00fcr Executor verf\u00fcgbar machen\n            task_dict = {task.id: task for task in exec_res.tasks}\n            shared[\"tasks\"].update(task_dict)\n\n            # Plan-Metadaten setzen\n            shared[\"plan_created_at\"] = datetime.now().isoformat()\n            shared[\"plan_strategy\"] = exec_res.execution_strategy\n            shared[\"total_tasks_planned\"] = len(exec_res.tasks)\n\n            logger.info(f\"Plan created successfully: {exec_res.name} with {len(exec_res.tasks)} tasks\")\n            return \"planned\"\n\n        else:\n            # Plan creation failed\n            shared[\"planning_error\"] = \"Invalid plan format returned\"\n            shared[\"current_plan\"] = None\n            logger.error(\"Plan creation failed - invalid format\")\n            return \"planning_failed\"\n\n    async def _create_simple_plan(self, prep_res) -&gt; TaskPlan:\n        \"\"\"Fast lightweight planning for direct or simple multi-step queries.\"\"\"\n        taw = self._build_tool_intelligence(prep_res) # TODO contet and var injection\n        logger.info(\"You are a FAST \"+ taw)\n        prompt = f\"\"\"\nYou are a FAST abstract pattern recognizer and task planner.\nIdentify if the query needs a **single-step LLM answer** or a **simple 2\u20133 task plan** using available tools.\nOutput ONLY YAML.\n\n## User Query\n{prep_res['query']}\n\n## Available Tools\n{taw}\n\n## Pattern Recognition (Internal Only)\n- Detect if query is informational, action-based, or tool-eligible.\n- Map to minimal plan type: \"direct_llm\" or \"simple_tool_plus_llm\".\n\n## YAML Schema\n```yaml\nplan_name: string\ndescription: string\nexecution_strategy: \"sequential\" | \"parallel\"\ntasks:\n  - id: string\n    type: \"LLMTask\" | \"ToolTask\"\n    description: string\n    priority: int\n    dependencies: [list]\nExample 1 \u2014 Direct LLM\n```yaml\nplan_name: \"Direct Response\"\ndescription: \"Quick answer from LLM\"\nexecution_strategy: \"sequential\"\ntasks:\n  - id: \"answer\"\n    type: \"LLMTask\"\n    description: \"Respond to query\"\n    priority: 1\n    dependencies: []\n    prompt_template: \"Respond concisely to: {prep_res['query']}\"\n    llm_config:\n      model_preference: \"fast\"\n      temperature: 0.3\n```\nExample 2 \u2014 Tool + LLM\n```yaml\nplan_name: \"Fetch and Answer\"\ndescription: \"Get info from tool and summarize\"\nexecution_strategy: \"sequential\"\ntasks:\n  - id: \"fetch_info\"\n    type: \"ToolTask\"\n    description: \"Get required data\"\n    priority: 1\n    dependencies: []\n    tool_name: \"info_api\"\n    arguments:\n      query: \"{{ prep_res['query'] }}\"\n  - id: \"summarize\"\n    type: \"LLMTask\"\n    description: \"Summarize fetched data\"\n    priority: 2\n    dependencies: [\"fetch_info\"]\n    prompt_template: \"Summarize: {{ results.fetch_info.data }}\"\n    llm_config:\n      model_preference: \"fast\"\n      temperature: 0.3\n```\nOutput Requirements\nUse ONLY YAML for the final output\nPick minimal plan type for fastest completion!\n    \"\"\"\n\n        try:\n            agent_instance = prep_res[\"agent_instance\"]\n            content = await agent_instance.a_run_llm_completion(\n                model=prep_res.get(\"complex_llm_model\", \"openrouter/anthropic/claude-3-haiku\"),\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.3,\n                max_tokens=512,\n                node_name=\"TaskPlannerNode\", task_id=\"fast_simple_planning\"\n            )\n\n            yaml_content = content.split(\"```yaml\")[1].split(\"```\")[0].strip() if \"```yaml\" in content else content\n            plan_data = yaml.safe_load(yaml_content)\n            # print(\"Simple\", json.dumps(plan_data, indent=2))\n            return TaskPlan(\n                id=str(uuid.uuid4()),\n                name=plan_data.get(\"plan_name\", \"Generated Plan\"),\n                description=plan_data.get(\"description\", f\"Plan for: {prep_res['query']}\"),\n                tasks=[\n                    [LLMTask, ToolTask, DecisionTask, CompoundTask, Task][[\"LLMTask\", \"ToolTask\", \"DecisionTask\", \"CompoundTask\", \"Task\"].index(t.get(\"type\"))](**t)\n                    for t in plan_data.get(\"tasks\", [])\n                ],\n                execution_strategy=plan_data.get(\"execution_strategy\", \"sequential\")\n            )\n\n        except Exception as e:\n            logger.error(f\"Simple plan creation failed: {e}\")\n            import traceback\n            print(traceback.format_exc())\n            return TaskPlan(\n                id=str(uuid.uuid4()),\n                name=\"Fallback Plan\",\n                description=\"Direct response only\",\n                tasks=[\n                    LLMTask(\n                        id=\"fast_simple_planning\",\n                        type=\"LLMTask\",\n                        description=\"Generate direct response\",\n                        priority=1,\n                        dependencies=[],\n                        prompt_template=f\"Respond to the query: {prep_res['query']}\",\n                        llm_config={\"model_preference\": \"fast\"}\n                    )\n                ]\n            )\n\n    async def _advanced_llm_decomposition(self, prep_res) -&gt; TaskPlan:\n        \"\"\"Erweiterte LLM-basierte Dekomposition mit dynamischer Plan-Anpassung\"\"\"\n        variable_manager = prep_res.get(\"variable_manager\")\n        tool_intelligence = self._build_tool_intelligence(prep_res)\n\n        # Check for replan context from failed DecisionTask\n        replan_context = prep_res.get(\"replan_context\", {})\n        is_replanning = bool(replan_context)\n\n        # Get variable context for planning\n        var_suggestions = \"\"\n        if variable_manager:\n            suggestions = variable_manager.get_variable_suggestions(prep_res['query'])\n            if suggestions:\n                var_suggestions = f\"\\n## Suggested Variables\\n{', '.join(suggestions)}\"\n\n        # Build context-aware prompt\n        base_query = prep_res['query']\n        if is_replanning:\n            context_info = f\"\"\"\n    ## REPLANNING CONTEXT\n    Original Query: {base_query}\n    Previous Failure: {replan_context.get('failure_reason', 'Unknown')}\n    New Goal: {replan_context.get('new_goal', 'Continue with refined approach')}\n    Failed Approach: {replan_context.get('avoid_approaches', [])}\n\n    Focus on the NEW GOAL and avoid the failed approaches.\"\"\"\n            effective_query = replan_context.get('new_goal', base_query)\n        else:\n            context_info = \"\"\n            effective_query = base_query\n\n        prompt = f\"\"\"\nYou are an expert **task planner** with **dynamic adaptation capabilities**.\nCreate intelligent, adaptive execution plans that can modify themselves during execution.\n\n## User Query\n{effective_query}\n\n{context_info}\n\n## Your Available Tools &amp; Intelligence\n{tool_intelligence}\n\n{variable_manager.get_llm_variable_context() if variable_manager else \"\"}\n{var_suggestions}\n\n\n## TASK TYPES (Dataclass-Aligned)\n- **LLMTask**: Step that uses a language model.\n- **ToolTask**: Step that calls an available tool.\n- **DecisionTask**: Step that decides routing between tasks. or triggers plan modifications.\n- **CompoundTask**: Step grouping sub-tasks.\n\n## YAML SCHEMA\n```yaml\nplan_name: string\ndescription: string\nexecution_strategy: \"sequential\" | \"parallel\" | \"mixed\"\ntasks:\n  - id: string\n    type: \"LLMTask\" | \"ToolTask\" | \"DecisionTask\" | \"CompoundTask\"\n    description: string\n    priority: int\n    dependencies: [list of task ids]\n    # Additional fields depending on type:\n    # LLMTask: prompt_template, llm_config, context_keys\n    # ToolTask: tool_name, arguments, hypothesis, validation_criteria, expectation\n    # DecisionTask: decision_prompt, routing_map, decision_model\n    # CompoundTask: sub_task_ids, execution_strategy, success_criteria\n```\n\nDecisionTasks can now trigger **plan modifications** during execution:\n\n### Routing Map Options:\n```yaml\nrouting_map:\n  # Classical routing to existing task\n  \"success\":\n    action: \"route_to_task\"\n    task_id: \"next_task_id\"\n\n  # Complete replan from this point\n  \"failure\":\n    action: \"replan_from_here\"\n    new_goal: \"Specific new objective based on failure\"\n    context: \"What went wrong and why\"\n\n  # Append new tasks to existing plan\n  \"partial_success\":\n    action: \"append_plan\"\n    new_goal: \"Extended objective requiring additional steps\"\n```\nPLANNING STRATEGY\n\nFor uncertain outcomes (web searches, API calls, data analysis):\nUse DecisionTask with \"replan_from_here\" for failure cases\nInclude specific new_goal based on potential failure modes\n\nFor multi-stage workflows:\nUse \"append_plan\" when intermediate results may require different next steps\n\nFor critical path dependencies:\nPlan alternative routes using DecisionTask routing\n\nENHANCED EXAMPLES\nAdaptive Web Research Pattern:\n&lt;reasoning&gt;\nThe logic ensures that subsequent steps are chosen depending on the sufficiency of the retrieved information.\nIt\u2019s a decision-making scaffold for research workflows.\n&lt;/reasoning&gt;\n```yaml\ntasks:\n  - id: \"web_search\"\n    type: \"ToolTask\"\n    tool_name: \"search_web\"\n    description: \"Get required data\"\n    priority: 1\n    dependencies: []\n    arguments:\n      query: \"{{ user.query }}\"\n\n  - id: \"evaluate_results\"\n    type: \"DecisionTask\"\n    priority: 2\n    dependencies: [\"web_search\"]\n    description: \"Evaluate search results and decide on next steps\"\n    decision_prompt: \"Are the search results sufficient?\\nQuery: {{ user.query }}\\nResults:\\n{{ results.web_search.data }}\\nEnd of Results\\n Answer 'good', 'poor', or 'empty'\"\n    routing_map:\n      \"good\":\n        action: \"route_to_task\"\n        task_id: \"analyze_findings\"\n      \"poor\":\n        action: \"replan_from_here\"\n        new_goal: \"Search results were poor quality. Analyze the query '{{ user.query }}' and create a refined, more specific search strategy.\"\n      \"empty\":\n        action: \"replan_from_here\"\n        new_goal: \"No search results found for '{{ user.query }}'. Try alternative search terms or different information sources.\"\n```\nDynamic Tool Selection Pattern:\n&lt;reasoning&gt;\nBy offering multiple branching paths\n(direct search, analysis-first, or multi-source), it maximizes adaptability for\ndiverse data retrieval contexts. The routing logic ensures that each branch aligns\nwith the complexity and reliability requirements of the query.\n&lt;/reasoning&gt;\n```yaml\ntasks:\n...\n  - id: \"choose_approach\"\n    type: \"DecisionTask\"\n    priority: 2\n    dependencies: [\"xyz\"]\n    description: \"Evaluate results and decide on next steps\"\n    decision_prompt: \"For query '{{ results.xyz.data }}', should we use 'direct_search', 'analysis_first', or 'multi_source'?\"\n    routing_map:\n      \"direct_search\":\n        action: \"append_plan\"\n        new_goal: \"Execute direct search approach for: {{ results.xyz.data }}\"\n      \"analysis_first\":\n        action: \"append_plan\"\n        new_goal: \"First analyze the query structure, then plan appropriate search strategy\"\n      \"multi_source\":\n        action: \"append_plan\"\n        new_goal: \"Use multiple information sources and cross-reference results\"\n```\nExample 2 \u2014 Tool + LLM\n&lt;reasoning&gt;\nThe separation of concerns\u2014data acquisition\nfirst, followed by language model summarization\u2014ensures.\nThis is effective for scenarios where raw data must be condensed.\n&lt;/reasoning&gt;\n```yaml\nplan_name: \"Fetch and Answer\"\ndescription: \"Get info from tool and summarize\"\nexecution_strategy: \"sequential\"\ntasks:\n  - id: \"fetch_info\"\n    type: \"ToolTask\"\n    description: \"Get required data\"\n    priority: 1\n    dependencies: []\n    tool_name: \"info_api\"\n    arguments:\n      query: \"{{ user.query }}\"\n  - id: \"summarize\"\n    type: \"LLMTask\"\n    description: \"Summarize fetched data\"\n    priority: 2\n    dependencies: [\"fetch_info\"]\n    prompt_template: \"Summarize: {{ results.fetch_info.data }}\"\n    llm_config:\n      model_preference: \"fast\"\n      temperature: 0.3\n```\nExample: Complex Research with CompoundTask\n&lt;reasoning&gt;\nCompoundTask groups related sub-tasks that should be executed together as a logical unit.\nIn this research case, we need to gather information from multiple sources simultaneously,\nthen analyze all results together.\nThe CompoundTask ensures that all data collection tasks are completed before moving to analysis,\nwhile allowing parallel execution within the compound task.\n&lt;/reasoning&gt;\n```yaml\nplan_name: \"Multi-Source Research Analysis\"\ndescription: \"Research topic from multiple sources and provide comprehensive analysis\"\nexecution_strategy: \"sequential\"\ntasks:\n  - id: \"research_compound\"\n    type: \"CompoundTask\"\n    description: \"Gather information from multiple sources\"\n    priority: 1\n    dependencies: []\n    sub_task_ids: [\"web_search\", \"academic_search\", \"news_search\"]\n    execution_strategy: \"parallel\"\n    success_criteria: \"At least 2 out of 3 searches return useful data\"\n\n  - id: \"web_search\"\n    type: \"ToolTask\"\n    description: \"Search web for general information\"\n    priority: 1\n    dependencies: []\n    tool_name: \"search_web\"\n    arguments:\n      query: \"{{ user.query }}\"\n      max_results: 5\n    hypothesis: \"Web search will provide current general information\"\n    validation_criteria: \"Results should contain relevant recent information\"\n\n  - id: \"academic_search\"\n    type: \"ToolTask\"\n    description: \"Search academic sources\"\n    priority: 1\n    dependencies: []\n    tool_name: \"search_academic\"\n    arguments:\n      query: \"{{ user.query }}\"\n      max_results: 3\n    hypothesis: \"Academic search will provide authoritative sources\"\n    validation_criteria: \"Results should contain peer-reviewed content\"\n\n  - id: \"news_search\"\n    type: \"ToolTask\"\n    description: \"Search recent news\"\n    priority: 1\n    dependencies: []\n    tool_name: \"search_news\"\n    arguments:\n      query: \"{{ user.query }}\"\n      timeframe: \"last_month\"\n    hypothesis: \"News search will provide latest developments\"\n    validation_criteria: \"Results should contain recent updates\"\n\n  - id: \"analyze_all_sources\"\n    type: \"LLMTask\"\n    description: \"Analyze and synthesize information from all sources\"\n    priority: 2\n    dependencies: [\"research_compound\"]\n    prompt_template: |\n      Analyze and synthesize the following research results for: {{ user.query }}\n\n      Web Results: {{ results.web_search.data }}\n      Academic Results: {{ results.academic_search.data }}\n      News Results: {{ results.news_search.data }}\n\n      Provide a comprehensive analysis covering:\n      1. Key findings from each source type\n      2. Consensus and contradictions\n      3. Most reliable information\n      4. Recent developments\n    llm_config:\n      model_preference: \"complex\"\n      temperature: 0.3\n\n  - id: \"final_report\"\n    type: \"LLMTask\"\n    description: \"Create final user-friendly report\"\n    priority: 3\n    dependencies: [\"analyze_all_sources\"]\n    prompt_template: |\n      Create a user-friendly report based on this analysis: {{ results.analyze_all_sources.data }}\n\n      Structure the response with:\n      - Executive summary\n      - Key findings\n      - Sources used\n      - Confidence levels\n    llm_config:\n      model_preference: \"fast\"\n      temperature: 0.4\n```\nOUTPUT REQUIREMENTS\n\nFor complex/uncertain queries: Include at least one DecisionTask with replan_from_here\nUse specific, actionable new_goal descriptions\nInclude failure context in routing decisions\nMaintain logical task dependencies\nOutput ONLY YAML and wrap it in ```yaml``` tags.\nProvide high-level reasoning before in &lt;reasoning&gt; tags, independent from the YAML code.\n\nGenerate the adaptive execution plan:\n\"\"\"\n        try:\n            model_to_use = prep_res.get(\"complex_llm_model\", \"openrouter/openai/gpt-4o\")\n            agent_instance = prep_res[\"agent_instance\"]\n            content = await agent_instance.a_run_llm_completion(\n                model=model_to_use,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.3,\n                max_tokens=2048, node_name=\"TaskExecutorNode\", task_id=\"adaptive_planning\"\n            )\n\n            if \"```yaml\" in content:\n                yaml_content = content.split(\"```yaml\")[1].split(\"```\")[0].strip()\n            else:\n                yaml_content = content\n\n            plan_data = yaml.safe_load(yaml_content)\n            # print(\"Advanced\", json.dumps(plan_data, indent=2))\n\n            # Create specialized tasks with enhanced DecisionTask support\n            tasks = []\n            for task_data in plan_data.get(\"tasks\", []):\n                task_type = task_data.pop(\"type\", \"generic\")\n\n                if task_type == \"DecisionTask\" or task_type == \"decision\":\n                    # Enhanced DecisionTask with dynamic capabilities\n                    task = DecisionTask(\n                        id=task_data.get(\"id\", str(uuid.uuid4())),\n                        type=\"decision\",\n                        description=task_data.get(\"description\", \"\"),\n                        priority=task_data.get(\"priority\", 1),\n                        dependencies=task_data.get(\"dependencies\", []),\n                        decision_prompt=task_data.get(\"decision_prompt\", \"\"),\n                        routing_map=task_data.get(\"routing_map\", {}),\n                        decision_model=task_data.get(\"decision_model\", \"fast\"),\n                        critical=True\n                    )\n                else:\n                    task = create_task(task_type, **task_data)\n\n                tasks.append(task)\n\n            plan = TaskPlan(\n                id=str(uuid.uuid4()),\n                name=plan_data.get(\"plan_name\", \"Adaptive Plan\"),\n                description=plan_data.get(\"description\", f\"Adaptive plan for: {effective_query}\"),\n                tasks=tasks,\n                execution_strategy=plan_data.get(\"execution_strategy\", \"sequential\"),\n                metadata={\"is_replan\": is_replanning, \"replan_context\": replan_context}\n            )\n\n            logger.info(f\"Created adaptive plan with {len(tasks)} tasks (replanning: {is_replanning})\")\n            return plan\n\n        except Exception as e:\n            logger.error(f\"Advanced adaptive planning failed: {e}\")\n            import traceback\n            print(traceback.format_exc())\n            return await self._create_simple_plan(prep_res)\n\n    def _build_tool_intelligence(self, prep_res: Dict) -&gt; str:\n        \"\"\"Build detailed tool intelligence for planning\"\"\"\n\n        agent_instance = prep_res.get(\"agent_instance\")\n        if not agent_instance or not hasattr(agent_instance, '_tool_capabilities'):\n            return \"No tool intelligence available.\"\n\n        capabilities = agent_instance._tool_capabilities\n        query = prep_res.get('query', '').lower()\n\n        context_parts = []\n        context_parts.append(\"### Intelligent Tool Analysis:\")\n\n        for tool_name, cap in capabilities.items():\n            context_parts.append(f\"\\n**{tool_name}:**\")\n            context_parts.append(f\"- Function: {cap.get('primary_function', 'Unknown')}\")\n            context_parts.append(f\"- Arguments: {yaml.dump(cap.get('args_schema', 'takes no arguments!'), default_flow_style=False)}\")\n\n            # Check relevance to current query\n            relevance_score = self._calculate_tool_relevance(query, cap)\n            context_parts.append(f\"- Query relevance: {relevance_score:.2f}\")\n\n            if relevance_score &gt; 0.4:\n                context_parts.append(\"- \u2b50 HIGHLY RELEVANT - SHOULD USE THIS TOOL!\")\n\n            # Show trigger analysis\n            triggers = cap.get('trigger_phrases', [])\n            matched_triggers = [t for t in triggers if t.lower() in query]\n            if matched_triggers:\n                context_parts.append(f\"- Matched triggers: {matched_triggers}\")\n\n            # Show use cases\n            use_cases = cap.get('use_cases', [])[:3]\n            context_parts.append(f\"- Use cases: {', '.join(use_cases)}\")\n\n        return \"\\n\".join(context_parts)\n\n    def _calculate_tool_relevance(self, query: str, capabilities: Dict) -&gt; float:\n        \"\"\"Calculate how relevant a tool is to the current query\"\"\"\n\n        query_words = set(query.lower().split())\n\n        # Check trigger phrases\n        trigger_score = 0.0\n        triggers = capabilities.get('trigger_phrases', [])\n        for trigger in triggers:\n            trigger_words = set(trigger.lower().split())\n            if trigger_words.intersection(query_words):\n                trigger_score += 0.04\n        # Check confidence triggers if available\n        conf_triggers = capabilities.get('confidence_triggers', {})\n        for phrase, confidence in conf_triggers.items():\n            if phrase.lower() in query:\n                trigger_score += confidence/10\n        # Check indirect connections\n        indirect = capabilities.get('indirect_connections', [])\n        for connection in indirect:\n            connection_words = set(connection.lower().split())\n            if connection_words.intersection(query_words):\n                trigger_score += 0.02\n        return min(1.0, trigger_score)\n</code></pre> <code>post_async(shared, prep_res, exec_res)</code> <code>async</code> \u00b6 <p>Post-processing nach Plan-Erstellung</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>async def post_async(self, shared, prep_res, exec_res):\n    \"\"\"Post-processing nach Plan-Erstellung\"\"\"\n\n    if exec_res is None:\n        shared[\"planning_error\"] = \"Plan creation returned None\"\n        return \"planning_failed\"\n\n    if isinstance(exec_res, TaskPlan):\n\n        progress_tracker = shared.get(\"progress_tracker\")\n        if progress_tracker:\n            e = ProgressEvent(\n                event_type=\"plan_created\",\n                timestamp=time.time(),\n                status=NodeStatus.COMPLETING,\n                node_name=\"TaskPlannerNode\",\n                node_phase=\"plan_created\",\n                session_id=shared.get(\"session_id\"),\n                metadata={\"plan_name\": exec_res.name, \"task_count\": len(exec_res.tasks),\n                          \"strategy\": exec_res.execution_strategy, \"full_plan\": exec_res}\n            )\n            await progress_tracker.emit_event(e)\n            await asyncio.sleep(0.1)\n\n        # Erfolgreicher Plan\n        shared[\"current_plan\"] = exec_res\n\n        # Tasks in shared state f\u00fcr Executor verf\u00fcgbar machen\n        task_dict = {task.id: task for task in exec_res.tasks}\n        shared[\"tasks\"].update(task_dict)\n\n        # Plan-Metadaten setzen\n        shared[\"plan_created_at\"] = datetime.now().isoformat()\n        shared[\"plan_strategy\"] = exec_res.execution_strategy\n        shared[\"total_tasks_planned\"] = len(exec_res.tasks)\n\n        logger.info(f\"Plan created successfully: {exec_res.name} with {len(exec_res.tasks)} tasks\")\n        return \"planned\"\n\n    else:\n        # Plan creation failed\n        shared[\"planning_error\"] = \"Invalid plan format returned\"\n        shared[\"current_plan\"] = None\n        logger.error(\"Plan creation failed - invalid format\")\n        return \"planning_failed\"\n</code></pre> <code>ToolAnalysis</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Defines the structure for a valid tool analysis.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>class ToolAnalysis(BaseModel):\n    \"\"\"Defines the structure for a valid tool analysis.\"\"\"\n    primary_function: str = Field(..., description=\"The main purpose of the tool.\")\n    use_cases: List[str] = Field(..., description=\"Specific use cases for the tool.\")\n    trigger_phrases: List[str] = Field(..., description=\"Phrases that should trigger the tool.\")\n    indirect_connections: List[str] = Field(..., description=\"Non-obvious connections or applications.\")\n    complexity_scenarios: List[str] = Field(..., description=\"Complex scenarios where the tool can be applied.\")\n    user_intent_categories: List[str] = Field(..., description=\"Categories of user intent the tool addresses.\")\n    confidence_triggers: Dict[str, float] = Field(..., description=\"Phrases mapped to confidence scores.\")\n    tool_complexity: str = Field(..., description=\"The complexity of the tool, rated as low, medium, or high.\")\n    args_schema: Optional[Dict[str, Any]] = Field(..., description=\"The schema for the tool's arguments.\")\n</code></pre> <code>ToolTask</code> <code>dataclass</code> \u00b6 <p>               Bases: <code>Task</code></p> <p>Spezialisierter Task f\u00fcr Tool-Aufrufe</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass ToolTask(Task):\n    \"\"\"Spezialisierter Task f\u00fcr Tool-Aufrufe\"\"\"\n    tool_name: str = \"\"\n    arguments: Dict[str, Any] = field(default_factory=dict)  # Kann {{ }} Referenzen enthalten\n    hypothesis: str = \"\"  # Was erwarten wir von diesem Tool?\n    validation_criteria: str = \"\"  # Wie validieren wir das Ergebnis?\n    expectation: str = \"\"  # Wie sollte das Ergebnis aussehen?\n</code></pre> <code>VariableManager</code> \u00b6 <p>Unified variable management system with advanced features</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>class VariableManager:\n    \"\"\"Unified variable management system with advanced features\"\"\"\n\n    def __init__(self, world_model: Dict, shared_state: Dict = None):\n        self.world_model = world_model\n        self.shared_state = shared_state or {}\n        self.scopes = {\n            'world': world_model,\n            'shared': self.shared_state,\n            'results': {},\n            'tasks': {},\n            'user': {},\n            'system': {}\n        }\n        self._cache = {}\n\n    def register_scope(self, name: str, data: Dict):\n        \"\"\"Register a new variable scope\"\"\"\n        self.scopes[name] = data\n        self._cache.clear()\n\n    def set_results_store(self, results_store: Dict):\n        \"\"\"Set the results store for task result references\"\"\"\n        self.scopes['results'] = results_store\n        self._cache.clear()\n\n    def set_tasks_store(self, tasks_store: Dict):\n        \"\"\"Set tasks store for task metadata access\"\"\"\n        self.scopes['tasks'] = tasks_store\n        self._cache.clear()\n\n    def get(self, path: str, default=None, use_cache: bool = True):\n        \"\"\"Get variable with dot notation path support\"\"\"\n        if use_cache and path in self._cache:\n            return self._cache[path]\n\n        try:\n            value = self._resolve_path(path)\n            if use_cache:\n                self._cache[path] = value\n            return value if value is not None else default\n        except:\n            return default\n\n    def set(self, path: str, value, create_scope: bool = True):\n        \"\"\"Set variable with dot notation path support\"\"\"\n        parts = path.split('.')\n\n        if len(parts) == 1:\n            # Simple key in world_model\n            self.world_model[path] = value\n        else:\n            scope_name = parts[0]\n\n            if scope_name not in self.scopes:\n                if create_scope:\n                    self.scopes[scope_name] = {}\n                else:\n                    raise KeyError(f\"Scope '{scope_name}' not found\")\n\n            # Navigate to nested location\n            current = self.scopes[scope_name]\n            for part in parts[1:-1]:\n                if part not in current:\n                    current[part] = {}\n                current = current[part]\n            current[parts[-1]] = value\n\n        self._cache.clear()\n\n    def _resolve_path(self, path: str):\n        \"\"\"Resolve dot notation path to actual value\"\"\"\n        parts = path.split('.')\n\n        if len(parts) == 1:\n            # Try world_model first, then other scopes\n            if path in self.world_model:\n                return self.world_model[path]\n\n            for scope in self.scopes.values():\n                if isinstance(scope, dict) and path in scope:\n                    return scope[path]\n            return None\n\n        # Multi-part path\n        scope_name = parts[0]\n        if scope_name not in self.scopes:\n            return None\n\n        current = self.scopes[scope_name]\n\n        for part in parts[1:]:\n            if isinstance(current, dict) and part in current:\n                current = current[part]\n            else:\n                return None\n\n        return current\n\n    def format_text(self, text: str, context: Dict = None) -&gt; str:\n        \"\"\"Enhanced text formatting with multiple syntaxes\"\"\"\n        if not text or not isinstance(text, str):\n            return str(text) if text is not None else \"\"\n\n        # Temporary context overlay\n        if context:\n            original_scopes = self.scopes.copy()\n            self.scopes['context'] = context\n\n        try:\n            # Handle {{ variable }} syntax\n            formatted = self._format_double_braces(text)\n\n            # Handle {variable} syntax\n            formatted = self._format_single_braces(formatted)\n\n            # Handle $variable syntax\n            formatted = self._format_dollar_syntax(formatted)\n\n            return formatted\n\n        finally:\n            if context:\n                self.scopes = original_scopes\n\n    def _format_double_braces(self, text: str) -&gt; str:\n        \"\"\"Handle {{ variable.path }} syntax with improved debugging\"\"\"\n        import re\n\n        def replace_var(match):\n            var_path = match.group(1).strip()\n            value = self.get(var_path)\n\n            if value is None:\n                # IMPROVED: Log missing variables for debugging\n                available_vars = list(self.get_available_variables().keys())\n                logger.warning(f\"Variable '{var_path}' not found. Available: {available_vars[:10]}\")\n                return match.group(0)  # Keep original if not found\n\n            return self._value_to_string(value)\n\n        return re.sub(r'\\{\\{\\s*([^}]+)\\s*\\}\\}', replace_var, text)\n\n    def _format_single_braces(self, text: str) -&gt; str:\n        \"\"\"Handle {variable.path} syntax, including with spaces like { variable.path }.\"\"\"\n        import re\n\n        def replace_var(match):\n            # Extrahiert den Variablennamen und entfernt f\u00fchrende/nachfolgende Leerzeichen\n            var_path = match.group(1).strip()\n\n            # Ruft den Wert \u00fcber die get-Methode ab, die die Punktnotation bereits verarbeitet\n            value = self.get(var_path)\n\n            # Gibt den konvertierten Wert oder das Original-Tag zur\u00fcck, wenn der Wert nicht gefunden wurde\n            return self._value_to_string(value) if value is not None else match.group(0)\n\n        # Dieser Regex findet {beliebiger.inhalt} und erlaubt Leerzeichen um den Inhalt\n        # Er schlie\u00dft verschachtelte oder leere Klammern wie {} oder { {var} } aus.\n        return re.sub(r'\\{([^{}]+)\\}', replace_var, text)\n\n    def _format_dollar_syntax(self, text: str) -&gt; str:\n        \"\"\"Handle $variable syntax\"\"\"\n        import re\n\n        def replace_var(match):\n            var_name = match.group(1)\n            value = self.get(var_name)\n            return self._value_to_string(value) if value is not None else match.group(0)\n\n        return re.sub(r'\\$([a-zA-Z_][a-zA-Z0-9_]*)', replace_var, text)\n\n    def _value_to_string(self, value) -&gt; str:\n        \"\"\"Convert value to string representation\"\"\"\n        if isinstance(value, str):\n            return value\n        elif isinstance(value, (dict, list)):\n            return json.dumps(value, default=str)\n        else:\n            return str(value)\n\n    def get_available_variables(self) -&gt; Dict[str, Dict]:\n        \"\"\"Get comprehensive variable documentation\"\"\"\n        variables = {}\n\n        for scope_name, scope_data in self.scopes.items():\n            if isinstance(scope_data, dict):\n                variables[scope_name] = {}\n                for key, value in scope_data.items():\n                    if isinstance(value, str):\n                        preview = value[:50] + \"...\" if len(value) &gt; 50 else value\n                    elif isinstance(value, dict):\n                        preview = f\"Object with {len(value)} keys\"\n                    elif isinstance(value, list):\n                        preview = f\"Array with {len(value)} items\"\n                    else:\n                        preview = str(type(value).__name__)\n\n                    variables[scope_name][key] = {\n                        'preview': preview,\n                        'type': type(value).__name__,\n                        'path': f\"{scope_name}.{key}\"\n                    }\n\n        return variables\n\n    def validate_references(self, text: str) -&gt; Dict[str, bool]:\n        \"\"\"Validate all variable references in text\"\"\"\n        import re\n\n        references = {}\n\n        # Find all {{ }} references\n        double_brace_refs = re.findall(r'\\{\\{\\s*([^}]+)\\s*\\}\\}', text)\n        for ref in double_brace_refs:\n            references[\"{{\"+ref+\"}}\"] = self.get(ref.strip()) is not None\n\n        # Find all {} references\n        single_brace_refs = re.findall(r'\\{([^{}\\s]+)\\}', text)\n        for ref in single_brace_refs:\n            if '.' not in ref:  # Only simple vars\n                references[\"{\"+ref+\"}\"] = self.get(ref.strip()) is not None\n\n        # Find all $ references\n        dollar_refs = re.findall(r'\\$([a-zA-Z_][a-zA-Z0-9_]*)', text)\n        for ref in dollar_refs:\n            references[f\"${ref}\"] = self.get(ref) is not None\n\n        return references\n\n    def get_scope_info(self) -&gt; Dict[str, Any]:\n        \"\"\"Get information about all available scopes\"\"\"\n        info = {}\n        for scope_name, scope_data in self.scopes.items():\n            if isinstance(scope_data, dict):\n                info[scope_name] = {\n                    'type': 'dict',\n                    'keys': len(scope_data),\n                    'sample_keys': list(scope_data.keys())[:5]\n                }\n            else:\n                info[scope_name] = {\n                    'type': type(scope_data).__name__,\n                    'value': str(scope_data)[:100]\n                }\n        return info\n\n    def _validate_task_references(self, task: Task) -&gt; Dict[str, Any]:\n        \"\"\"Validate all variable references in a task\"\"\"\n        validation_results = {\n            'valid': True,\n            'errors': [],\n            'warnings': []\n        }\n\n        # Check different task types\n        if isinstance(task, LLMTask):\n            if task.prompt_template:\n                refs = self.validate_references(task.prompt_template)\n                for ref, is_valid in refs.items():\n                    if not is_valid:\n                        validation_results['errors'].append(f\"Invalid reference in prompt: {ref}\")\n                        validation_results['valid'] = False\n\n        elif isinstance(task, ToolTask):\n            for key, value in task.arguments.items():\n                if isinstance(value, str):\n                    refs = self.validate_references(value)\n                    for ref, is_valid in refs.items():\n                        if not is_valid:\n                            validation_results['warnings'].append(f\"Invalid reference in {key}: {ref}\")\n\n        return validation_results\n\n    def get_llm_variable_context(self) -&gt; str:\n        \"\"\"Get variable context formatted for LLM consumption\"\"\"\n        context_parts = []\n        context_parts.append(\"## Variable System\")\n        context_parts.append(\"You have access to a powerful variable system:\")\n\n        # Show available scopes with examples\n        for scope_name, scope_data in self.scopes.items():\n            if isinstance(scope_data, dict) and scope_data:\n                sample_keys = list(scope_data.keys())[:3]\n                context_parts.append(f\"- {scope_name}: {', '.join(sample_keys)}\")\n\n        context_parts.append(\"\\nSyntax: {{ variable.path }} or {variable} or $variable\")\n        context_parts.append(\"Examples: {{ user.name }}, {{ results.search.data }}, {{ system_context.timestamp }}\")\n\n        return \"\\n\".join(context_parts)\n\n    def get_variable_suggestions(self, query: str) -&gt; List[str]:\n        \"\"\"Get variable suggestions based on query content\"\"\"\n\n        query_lower = query.lower()\n        suggestions = []\n\n        # Check all variables for relevance\n        for scope in self.scopes.values():\n            for name, var_def in scope.items():\n                # Name similarity\n                if any(word in name.lower() for word in query_lower.split()):\n                    suggestions.append(name)\n                    continue\n\n                # Description similarity\n                if var_def and any(word in str(var_def).lower() for word in query_lower.split()):\n                    suggestions.append(name)\n                    continue\n\n\n        return list(set(suggestions))[:10]\n</code></pre> <code>format_text(text, context=None)</code> \u00b6 <p>Enhanced text formatting with multiple syntaxes</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def format_text(self, text: str, context: Dict = None) -&gt; str:\n    \"\"\"Enhanced text formatting with multiple syntaxes\"\"\"\n    if not text or not isinstance(text, str):\n        return str(text) if text is not None else \"\"\n\n    # Temporary context overlay\n    if context:\n        original_scopes = self.scopes.copy()\n        self.scopes['context'] = context\n\n    try:\n        # Handle {{ variable }} syntax\n        formatted = self._format_double_braces(text)\n\n        # Handle {variable} syntax\n        formatted = self._format_single_braces(formatted)\n\n        # Handle $variable syntax\n        formatted = self._format_dollar_syntax(formatted)\n\n        return formatted\n\n    finally:\n        if context:\n            self.scopes = original_scopes\n</code></pre> <code>get(path, default=None, use_cache=True)</code> \u00b6 <p>Get variable with dot notation path support</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def get(self, path: str, default=None, use_cache: bool = True):\n    \"\"\"Get variable with dot notation path support\"\"\"\n    if use_cache and path in self._cache:\n        return self._cache[path]\n\n    try:\n        value = self._resolve_path(path)\n        if use_cache:\n            self._cache[path] = value\n        return value if value is not None else default\n    except:\n        return default\n</code></pre> <code>get_available_variables()</code> \u00b6 <p>Get comprehensive variable documentation</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def get_available_variables(self) -&gt; Dict[str, Dict]:\n    \"\"\"Get comprehensive variable documentation\"\"\"\n    variables = {}\n\n    for scope_name, scope_data in self.scopes.items():\n        if isinstance(scope_data, dict):\n            variables[scope_name] = {}\n            for key, value in scope_data.items():\n                if isinstance(value, str):\n                    preview = value[:50] + \"...\" if len(value) &gt; 50 else value\n                elif isinstance(value, dict):\n                    preview = f\"Object with {len(value)} keys\"\n                elif isinstance(value, list):\n                    preview = f\"Array with {len(value)} items\"\n                else:\n                    preview = str(type(value).__name__)\n\n                variables[scope_name][key] = {\n                    'preview': preview,\n                    'type': type(value).__name__,\n                    'path': f\"{scope_name}.{key}\"\n                }\n\n    return variables\n</code></pre> <code>get_llm_variable_context()</code> \u00b6 <p>Get variable context formatted for LLM consumption</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def get_llm_variable_context(self) -&gt; str:\n    \"\"\"Get variable context formatted for LLM consumption\"\"\"\n    context_parts = []\n    context_parts.append(\"## Variable System\")\n    context_parts.append(\"You have access to a powerful variable system:\")\n\n    # Show available scopes with examples\n    for scope_name, scope_data in self.scopes.items():\n        if isinstance(scope_data, dict) and scope_data:\n            sample_keys = list(scope_data.keys())[:3]\n            context_parts.append(f\"- {scope_name}: {', '.join(sample_keys)}\")\n\n    context_parts.append(\"\\nSyntax: {{ variable.path }} or {variable} or $variable\")\n    context_parts.append(\"Examples: {{ user.name }}, {{ results.search.data }}, {{ system_context.timestamp }}\")\n\n    return \"\\n\".join(context_parts)\n</code></pre> <code>get_scope_info()</code> \u00b6 <p>Get information about all available scopes</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def get_scope_info(self) -&gt; Dict[str, Any]:\n    \"\"\"Get information about all available scopes\"\"\"\n    info = {}\n    for scope_name, scope_data in self.scopes.items():\n        if isinstance(scope_data, dict):\n            info[scope_name] = {\n                'type': 'dict',\n                'keys': len(scope_data),\n                'sample_keys': list(scope_data.keys())[:5]\n            }\n        else:\n            info[scope_name] = {\n                'type': type(scope_data).__name__,\n                'value': str(scope_data)[:100]\n            }\n    return info\n</code></pre> <code>get_variable_suggestions(query)</code> \u00b6 <p>Get variable suggestions based on query content</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def get_variable_suggestions(self, query: str) -&gt; List[str]:\n    \"\"\"Get variable suggestions based on query content\"\"\"\n\n    query_lower = query.lower()\n    suggestions = []\n\n    # Check all variables for relevance\n    for scope in self.scopes.values():\n        for name, var_def in scope.items():\n            # Name similarity\n            if any(word in name.lower() for word in query_lower.split()):\n                suggestions.append(name)\n                continue\n\n            # Description similarity\n            if var_def and any(word in str(var_def).lower() for word in query_lower.split()):\n                suggestions.append(name)\n                continue\n\n\n    return list(set(suggestions))[:10]\n</code></pre> <code>register_scope(name, data)</code> \u00b6 <p>Register a new variable scope</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def register_scope(self, name: str, data: Dict):\n    \"\"\"Register a new variable scope\"\"\"\n    self.scopes[name] = data\n    self._cache.clear()\n</code></pre> <code>set(path, value, create_scope=True)</code> \u00b6 <p>Set variable with dot notation path support</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def set(self, path: str, value, create_scope: bool = True):\n    \"\"\"Set variable with dot notation path support\"\"\"\n    parts = path.split('.')\n\n    if len(parts) == 1:\n        # Simple key in world_model\n        self.world_model[path] = value\n    else:\n        scope_name = parts[0]\n\n        if scope_name not in self.scopes:\n            if create_scope:\n                self.scopes[scope_name] = {}\n            else:\n                raise KeyError(f\"Scope '{scope_name}' not found\")\n\n        # Navigate to nested location\n        current = self.scopes[scope_name]\n        for part in parts[1:-1]:\n            if part not in current:\n                current[part] = {}\n            current = current[part]\n        current[parts[-1]] = value\n\n    self._cache.clear()\n</code></pre> <code>set_results_store(results_store)</code> \u00b6 <p>Set the results store for task result references</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def set_results_store(self, results_store: Dict):\n    \"\"\"Set the results store for task result references\"\"\"\n    self.scopes['results'] = results_store\n    self._cache.clear()\n</code></pre> <code>set_tasks_store(tasks_store)</code> \u00b6 <p>Set tasks store for task metadata access</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def set_tasks_store(self, tasks_store: Dict):\n    \"\"\"Set tasks store for task metadata access\"\"\"\n    self.scopes['tasks'] = tasks_store\n    self._cache.clear()\n</code></pre> <code>validate_references(text)</code> \u00b6 <p>Validate all variable references in text</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def validate_references(self, text: str) -&gt; Dict[str, bool]:\n    \"\"\"Validate all variable references in text\"\"\"\n    import re\n\n    references = {}\n\n    # Find all {{ }} references\n    double_brace_refs = re.findall(r'\\{\\{\\s*([^}]+)\\s*\\}\\}', text)\n    for ref in double_brace_refs:\n        references[\"{{\"+ref+\"}}\"] = self.get(ref.strip()) is not None\n\n    # Find all {} references\n    single_brace_refs = re.findall(r'\\{([^{}\\s]+)\\}', text)\n    for ref in single_brace_refs:\n        if '.' not in ref:  # Only simple vars\n            references[\"{\"+ref+\"}\"] = self.get(ref.strip()) is not None\n\n    # Find all $ references\n    dollar_refs = re.findall(r'\\$([a-zA-Z_][a-zA-Z0-9_]*)', text)\n    for ref in dollar_refs:\n        references[f\"${ref}\"] = self.get(ref) is not None\n\n    return references\n</code></pre> <code>YAMLFormatterNode</code> \u00b6 <p>               Bases: <code>AsyncNode</code></p> <p>Enhanced YAML formatter with schema-based generation</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>@with_progress_tracking\nclass YAMLFormatterNode(AsyncNode):\n    \"\"\"Enhanced YAML formatter with schema-based generation\"\"\"\n\n    def __init__(self, schema_class: Optional[Type[BaseModel]] = None, **kwargs):\n        super().__init__(**kwargs)\n        self.schema_class = schema_class\n\n    async def prep_async(self, shared):\n        task_description = shared.get(\"current_task_description\", \"\")\n        schema_mode = shared.get(\"yaml_format_mode\", \"general\")\n        custom_schema = shared.get(\"custom_schema\", {})\n        raw_input = shared.get(\"raw_llm_output\", \"\")\n\n        return {\n            \"task_description\": task_description,\n            \"schema_mode\": schema_mode,\n            \"custom_schema\": custom_schema,\n            \"raw_input\": raw_input,\n            \"context\": shared.get(\"formatted_context\", {}),\n            \"fast_llm_model\": shared.get(\"fast_llm_model\"),\n            \"complex_llm_model\": shared.get(\"complex_llm_model\"),\n            \"agent_instance\": shared.get(\"agent_instance\")\n        }\n\n    async def exec_async(self, prep_res):\n        if prep_res[\"raw_input\"]:\n            # Parse existing LLM output into YAML\n            return await self._parse_to_yaml(prep_res)\n        else:\n            # Generate new YAML based on schema\n            return await self._generate_yaml_from_schema(prep_res)\n\n    async def _parse_to_yaml(self, prep_res):\n        raw_input = prep_res[\"raw_input\"]\n\n        try:\n            # Try to extract YAML from markdown code blocks\n            if \"```yaml\" in raw_input:\n                yaml_content = raw_input.split(\"```yaml\")[1].split(\"```\")[0].strip()\n            elif \"```\" in raw_input:\n                yaml_content = raw_input.split(\"```\")[1].split(\"```\")[0].strip()\n            else:\n                yaml_content = raw_input\n\n            parsed = yaml.safe_load(yaml_content)\n            return {\n                \"success\": True,\n                \"data\": parsed,\n                \"raw_yaml\": yaml_content\n            }\n        except Exception as e:\n            logger.error(f\"Failed to parse YAML: {e}\")\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"fallback\": {\"raw_content\": raw_input}\n            }\n\n    async def _generate_yaml_from_schema(self, prep_res):\n        schema_mode = prep_res[\"schema_mode\"]\n\n        if self.schema_class:\n            schema = self.schema_class.model_json_schema()\n        else:\n            schema = self._get_default_schema(schema_mode)\n\n        # Generate LLM prompt to create YAML based on schema\n        prompt = self._build_schema_prompt(schema, prep_res)\n\n        if LITELLM_AVAILABLE:\n            try:\n                # Use fast model from shared context\n                model_to_use = prep_res.get(\"fast_llm_model\", \"openrouter/anthropic/claude-3-haiku\")\n                logger.info(f\"Using model {model_to_use} for YAML generation\")\n                agent_instance = prep_res[\"agent_instance\"]\n                yaml_content = await agent_instance.a_run_llm_completion(\n                    model=model_to_use,\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                    temperature=0.1, node_name=\"YAMLFormatterNode\", task_id=\"yaml_format\"\n                )\n\n                # Extract and validate YAML\n                if \"```yaml\" in yaml_content:\n                    yaml_str = yaml_content.split(\"```yaml\")[1].split(\"```\")[0].strip()\n                else:\n                    yaml_str = yaml_content.strip()\n\n                parsed = yaml.safe_load(yaml_str)\n                return {\n                    \"success\": True,\n                    \"data\": parsed,\n                    \"raw_yaml\": yaml_str\n                }\n            except Exception as e:\n                logger.error(f\"LLM YAML generation failed: {e}\")\n                return self._generate_fallback_yaml(prep_res)\n        else:\n            return self._generate_fallback_yaml(prep_res)\n\n    def _get_default_schema(self, mode: str) -&gt; Dict:\n        schemas = {\n            \"task_plan\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"plan_name\": {\"type\": \"string\"},\n                    \"description\": {\"type\": \"string\"},\n                    \"tasks\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"id\": {\"type\": \"string\"},\n                                \"description\": {\"type\": \"string\"},\n                                \"priority\": {\"type\": \"integer\"},\n                                \"dependencies\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                            }\n                        }\n                    }\n                }\n            },\n            \"action\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"action_type\": {\"type\": \"string\"},\n                    \"parameters\": {\"type\": \"object\"},\n                    \"reasoning\": {\"type\": \"string\"}\n                }\n            },\n            \"analysis\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"summary\": {\"type\": \"string\"},\n                    \"key_points\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                    \"recommendations\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                }\n            }\n        }\n        return schemas.get(mode, schemas[\"analysis\"])\n\n    def _build_schema_prompt(self, schema: Dict, prep_res: Dict) -&gt; str:\n        return f\"\"\"\nGenerate a YAML structure based on the following schema and context:\n\n## Task Description\n{prep_res['task_description']}\n\n## Required Schema\n```yaml\n{yaml.safe_dump(schema, indent=2)}\n```\nContext\n{prep_res.get('context', {})}\nGenerate valid YAML that conforms to this schema.\nWrap your response in one\n```yaml\n```\ncode block!.\n\"\"\"\n    def _generate_fallback_yaml(self, prep_res):\n        fallback = {\n            \"task_description\": prep_res[\"task_description\"],\n            \"schema_mode\": prep_res[\"schema_mode\"],\n            \"timestamp\": datetime.now().isoformat()\n        }\n        return {\n            \"success\": True,\n            \"data\": fallback,\n            \"raw_yaml\": yaml.dump(fallback)\n        }\n\n    async def post_async(self, shared, prep_res, exec_res):\n        shared[\"formatted_yaml\"] = exec_res\n        if exec_res[\"success\"]:\n            shared[\"structured_data\"] = exec_res[\"data\"]\n        return \"formatted\" if exec_res[\"success\"] else \"format_failed\"\n</code></pre> <code>create_task(task_type, **kwargs)</code> \u00b6 <p>Factory f\u00fcr Task-Erstellung mit korrektem Typ</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def create_task(task_type: str, **kwargs) -&gt; Task:\n    \"\"\"Factory f\u00fcr Task-Erstellung mit korrektem Typ\"\"\"\n    task_classes = {\n        \"llm_call\": LLMTask,\n        \"tool_call\": ToolTask,\n        \"decision\": DecisionTask,\n        \"compound\": CompoundTask,\n        \"generic\": Task,\n        \"LLMTask\": LLMTask,\n        \"ToolTask\": ToolTask,\n        \"DecisionTask\": DecisionTask,\n        \"CompoundTask\": CompoundTask,\n        \"Task\": Task,\n    }\n\n    task_class = task_classes.get(task_type, Task)\n\n    # Standard-Felder setzen\n    if \"id\" not in kwargs:\n        kwargs[\"id\"] = str(uuid.uuid4())\n    if \"type\" not in kwargs:\n        kwargs[\"type\"] = task_type\n    if \"critical\" not in kwargs:\n        kwargs[\"critical\"] = task_type in [\"llm_call\", \"decision\"]\n\n    # Ensure metadata is initialized\n    if \"metadata\" not in kwargs:\n        kwargs[\"metadata\"] = {}\n\n    # Create task and ensure post_init is called\n    task = task_class(**kwargs)\n\n    # Double-check metadata initialization\n    if not hasattr(task, 'metadata') or task.metadata is None:\n        task.metadata = {}\n\n    return task\n</code></pre> <code>get_args_schema(func)</code> \u00b6 <p>Generate a string representation of a function's arguments and annotations. Keeps args and *kwargs indicators and handles modern Python type hints.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def get_args_schema(func: Callable) -&gt; str:\n    \"\"\"\n    Generate a string representation of a function's arguments and annotations.\n    Keeps *args and **kwargs indicators and handles modern Python type hints.\n    \"\"\"\n    sig = inspect.signature(func)\n    parts = []\n\n    for name, param in sig.parameters.items():\n        ann = \"\"\n        if param.annotation is not inspect._empty:\n            ann = f\": {_annotation_to_str(param.annotation)}\"\n\n        default = \"\"\n        if param.default is not inspect._empty:\n            default = f\" = {repr(param.default)}\"\n\n        prefix = \"\"\n        if param.kind == inspect.Parameter.VAR_POSITIONAL:\n            prefix = \"*\"\n        elif param.kind == inspect.Parameter.VAR_KEYWORD:\n            prefix = \"**\"\n\n        parts.append(f\"{prefix}{name}{ann}{default}\")\n\n    return f\"({', '.join(parts)})\"\n</code></pre> <code>get_progress_summary(self)</code> \u00b6 <p>Get comprehensive progress summary from the agent</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def get_progress_summary(self) -&gt; Dict[str, Any]:\n    \"\"\"Get comprehensive progress summary from the agent\"\"\"\n    if hasattr(self, 'progress_tracker'):\n        return self.progress_tracker.get_summary()\n    return {\"error\": \"No progress tracker available\"}\n</code></pre> <code>with_progress_tracking(cls)</code> \u00b6 <p>Ein Klassendekorator, der die Methoden run_async, prep_async, exec_async, und exec_fallback_async automatisch mit umfassendem Progress-Tracking umwickelt.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/agent.py</code> <pre><code>def with_progress_tracking(cls):\n    \"\"\"\n    Ein Klassendekorator, der die Methoden run_async, prep_async, exec_async,\n    und exec_fallback_async automatisch mit umfassendem Progress-Tracking umwickelt.\n    \"\"\"\n\n    # --- Wrapper f\u00fcr run_async ---\n    original_run = getattr(cls, 'run_async', None)\n    if original_run:\n        @functools.wraps(original_run)\n        async def wrapped_run_async(self, shared):\n            progress_tracker = shared.get(\"progress_tracker\")\n            node_name = self.__class__.__name__\n\n            if not progress_tracker:\n                return await original_run(self, shared)\n\n            timer_key = f\"{node_name}_total\"\n            progress_tracker.start_timer(timer_key)\n            await progress_tracker.emit_event(ProgressEvent(\n                event_type=\"node_enter\",\n                timestamp=time.time(),\n                node_name=node_name,\n                session_id=shared.get(\"session_id\"),\n                task_id=shared.get(\"current_task_id\"),\n                plan_id=shared.get(\"current_plan\", TaskPlan(id=\"none\", name=\"none\", description=\"none\")).id if shared.get(\"current_plan\") else None,\n                status=NodeStatus.RUNNING,\n                success=None\n            ))\n\n            try:\n                # Hier wird die urspr\u00fcngliche Methode aufgerufen\n                result = await original_run(self, shared)\n\n                total_duration = progress_tracker.end_timer(timer_key)\n                await progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"node_exit\",\n                    timestamp=time.time(),\n                    node_name=node_name,\n                    status=NodeStatus.COMPLETED,\n                    success=True,\n                    node_duration=total_duration,\n                    routing_decision=result,\n                    session_id=shared.get(\"session_id\"),\n                    task_id=shared.get(\"current_task_id\"),\n                    metadata={\"success\": True}\n                ))\n\n                return result\n            except Exception as e:\n                total_duration = progress_tracker.end_timer(timer_key)\n                await progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"error\",\n                    timestamp=time.time(),\n                    node_name=node_name,\n                    status=NodeStatus.FAILED,\n                    success=False,\n                    node_duration=total_duration,\n                    session_id=shared.get(\"session_id\"),\n                    metadata={\"error\": str(e), \"error_type\": type(e).__name__}\n                ))\n                raise\n\n        setattr(cls, 'run_async', wrapped_run_async)\n\n    # --- Wrapper f\u00fcr prep_async ---\n    original_prep = getattr(cls, 'prep_async', None)\n    if original_prep:\n        @functools.wraps(original_prep)\n        async def wrapped_prep_async(self, shared):\n            progress_tracker = shared.get(\"progress_tracker\")\n            node_name = self.__class__.__name__\n\n            if not progress_tracker:\n                return await original_prep(self, shared)\n            timer_key = f\"{node_name}_total_p\"\n            progress_tracker.start_timer(timer_key)\n            timer_key = f\"{node_name}_prep\"\n            progress_tracker.start_timer(timer_key)\n            await progress_tracker.emit_event(ProgressEvent(\n                event_type=\"node_phase\",\n                timestamp=time.time(),\n                node_name=node_name,\n                status=NodeStatus.STARTING,\n                node_phase=\"prep\",\n                session_id=shared.get(\"session_id\")\n            ))\n\n            try:\n                result = await original_prep(self, shared)\n\n                prep_duration = progress_tracker.end_timer(timer_key)\n                await progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"node_phase\",\n                    timestamp=time.time(),\n                    status=NodeStatus.RUNNING,\n                    success=True,\n                    node_name=node_name,\n                    node_phase=\"prep_complete\",\n                    node_duration=prep_duration,\n                    session_id=shared.get(\"session_id\")\n                ))\n                return result\n            except Exception as e:\n                progress_tracker.end_timer(timer_key)\n                await progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"error\",\n                    timestamp=time.time(),\n                    node_name=node_name,\n                    status=NodeStatus.FAILED,\n                    success=False,\n                    metadata={\"error\": str(e), \"error_type\": type(e).__name__},\n                    node_phase=\"prep_failed\"\n                ))\n                raise\n\n\n        setattr(cls, 'prep_async', wrapped_prep_async)\n\n    # --- Wrapper f\u00fcr exec_async ---\n    original_exec = getattr(cls, 'exec_async', None)\n    if original_exec:\n        @functools.wraps(original_exec)\n        async def wrapped_exec_async(self, prep_res):\n            progress_tracker = prep_res.get(\"progress_tracker\") if isinstance(prep_res, dict) else None\n            node_name = self.__class__.__name__\n\n            if not progress_tracker:\n                return await original_exec(self, prep_res)\n\n            timer_key = f\"{node_name}_exec\"\n            progress_tracker.start_timer(timer_key)\n            await progress_tracker.emit_event(ProgressEvent(\n                event_type=\"node_phase\",\n                timestamp=time.time(),\n                node_name=node_name,\n                status=NodeStatus.RUNNING,\n                node_phase=\"exec\",\n                session_id=prep_res.get(\"session_id\") if isinstance(prep_res, dict) else None\n            ))\n\n            # In exec gibt es normalerweise keine Fehlerbehandlung, da diese von run_async \u00fcbernommen wird\n            result = await original_exec(self, prep_res)\n\n            exec_duration = progress_tracker.end_timer(timer_key)\n            await progress_tracker.emit_event(ProgressEvent(\n                event_type=\"node_phase\",\n                timestamp=time.time(),\n                node_name=node_name,\n                status=NodeStatus.RUNNING,\n                success=True,\n                node_phase=\"exec_complete\",\n                node_duration=exec_duration,\n                session_id=prep_res.get(\"session_id\") if isinstance(prep_res, dict) else None\n            ))\n            return result\n\n        setattr(cls, 'exec_async', wrapped_exec_async)\n\n    # --- Wrapper f\u00fcr post_async ---\n    original_post = getattr(cls, 'post_async', None)\n    if original_post:\n        @functools.wraps(original_post)\n        async def wrapped_post_async(self, shared, prep_res, exec_res):\n            progress_tracker = shared.get(\"progress_tracker\")\n            node_name = self.__class__.__name__\n\n            if not progress_tracker:\n                return await original_post(self, shared, prep_res, exec_res)\n\n            timer_key_post = f\"{node_name}_post\"\n            progress_tracker.start_timer(timer_key_post)\n            await progress_tracker.emit_event(ProgressEvent(\n                event_type=\"node_phase\",\n                timestamp=time.time(),\n                node_name=node_name,\n                status=NodeStatus.COMPLETING,  # Neue Phase \"completing\"\n                node_phase=\"post\",\n                session_id=shared.get(\"session_id\")\n            ))\n\n            try:\n                # Die eigentliche post_async Methode aufrufen\n                result = await original_post(self, shared, prep_res, exec_res)\n\n                post_duration = progress_tracker.end_timer(timer_key_post)\n                total_duration = progress_tracker.end_timer(f\"{node_name}_total_p\")  # Gesamtdauer stoppen\n\n                # Sende das entscheidende \"node_exit\" Event nach erfolgreicher post-Phase\n                await progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"node_exit\",\n                    timestamp=time.time(),\n                    node_name=node_name,\n                    status=NodeStatus.COMPLETED,\n                    success=True,\n                    node_duration=total_duration,\n                    routing_decision=result,\n                    session_id=shared.get(\"session_id\"),\n                    task_id=shared.get(\"current_task_id\"),\n                    metadata={\n                        \"success\": True,\n                        \"post_duration\": post_duration\n                    }\n                ))\n\n                return result\n            except Exception as e:\n                # Fehler in der post-Phase\n                post_duration = progress_tracker.end_timer(timer_key_post)\n                total_duration = progress_tracker.end_timer(f\"{node_name}_total\")\n                await progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"error\",\n                    timestamp=time.time(),\n                    node_name=node_name,\n                    status=NodeStatus.FAILED,\n                    success=False,\n                    node_duration=total_duration,\n                    metadata={\"error\": str(e), \"error_type\": type(e).__name__, \"phase\": \"post\"},\n                    node_phase=\"post_failed\"\n                ))\n                raise\n\n        setattr(cls, 'post_async', wrapped_post_async)\n\n    # --- Wrapper f\u00fcr exec_fallback_async ---\n    original_fallback = getattr(cls, 'exec_fallback_async', None)\n    if original_fallback:\n        @functools.wraps(original_fallback)\n        async def wrapped_fallback_async(self, prep_res, exc):\n            progress_tracker = prep_res.get(\"progress_tracker\") if isinstance(prep_res, dict) else None\n            node_name = self.__class__.__name__\n\n            if progress_tracker:\n                timer_key = f\"{node_name}_exec\"\n                exec_duration = progress_tracker.end_timer(timer_key)\n                await progress_tracker.emit_event(ProgressEvent(\n                    event_type=\"node_phase\",\n                    timestamp=time.time(),\n                    node_name=node_name,\n                    node_phase=\"exec_fallback\",\n                    node_duration=exec_duration,\n                    status=NodeStatus.FAILED,\n                    success=False,\n                    session_id=prep_res.get(\"session_id\") if isinstance(prep_res, dict) else None,\n                    metadata={\"error\": str(exc), \"error_type\": type(exc).__name__},\n                ))\n\n            return await original_fallback(self, prep_res, exc)\n\n        setattr(cls, 'exec_fallback_async', wrapped_fallback_async)\n\n    return cls\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent.builder","title":"<code>builder</code>","text":"<code>A2AConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>A2A server configuration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>class A2AConfig(BaseModel):\n    \"\"\"A2A server configuration\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    enabled: bool = False\n    host: str = \"0.0.0.0\"\n    port: int = 5000\n    agent_name: Optional[str] = None\n    agent_description: Optional[str] = None\n    agent_version: str = \"1.0.0\"\n    expose_tools_as_skills: bool = True\n</code></pre> <code>AgentConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Complete agent configuration for loading/saving</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>class AgentConfig(BaseModel):\n    \"\"\"Complete agent configuration for loading/saving\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    # Basic settings\n    name: str = \"ProductionAgent\"\n    description: str = \"Production-ready PocketFlow agent\"\n    version: str = \"2.0.0\"\n\n    # LLM settings\n    fast_llm_model: str = \"openrouter/anthropic/claude-3-haiku\"\n    complex_llm_model: str = \"openrouter/openai/gpt-4o\"\n    system_message: str = \"\"\"You are a production-ready autonomous agent with advanced capabilities including:\n- Native MCP tool integration for extensible functionality\n- A2A compatibility for agent-to-agent communication\n- Dynamic task planning and execution with adaptive reflection\n- Advanced context management with session awareness\n- Variable system for dynamic content generation\n- Checkpoint/resume capabilities for reliability\n\nAlways utilize available tools when they can help solve the user's request efficiently.\"\"\"\n\n    temperature: float = 0.7\n    max_tokens_output: int = 2048\n    max_tokens_input: int = 32768\n    api_key_env_var: Optional[str] = \"OPENROUTER_API_KEY\"\n    use_fast_response: bool = True\n\n    # Features\n    mcp: MCPConfig = Field(default_factory=MCPConfig)\n    a2a: A2AConfig = Field(default_factory=A2AConfig)\n    telemetry: TelemetryConfig = Field(default_factory=TelemetryConfig)\n    checkpoint: CheckpointConfig = Field(default_factory=CheckpointConfig)\n\n    # Agent behavior\n    max_parallel_tasks: int = 3\n    verbose_logging: bool = False\n\n    # Persona and formatting\n    active_persona: Optional[str] = None\n    persona_profiles: Dict[str, Dict[str, Any]] = Field(default_factory=dict)\n    default_format_config: Optional[Dict[str, Any]] = None\n\n    # Custom variables and world model\n    custom_variables: Dict[str, Any] = Field(default_factory=dict)\n    initial_world_model: Dict[str, Any] = Field(default_factory=dict)\n</code></pre> <code>CheckpointConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Checkpoint configuration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>class CheckpointConfig(BaseModel):\n    \"\"\"Checkpoint configuration\"\"\"\n    enabled: bool = True\n    interval_seconds: int = 300  # 5 minutes\n    max_checkpoints: int = 10\n    checkpoint_dir: str = \"./checkpoints\"\n    auto_save_on_exit: bool = True\n</code></pre> <code>FlowAgentBuilder</code> \u00b6 <p>Production-ready FlowAgent builder focused on MCP, A2A, and robust deployment</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>class FlowAgentBuilder:\n    \"\"\"Production-ready FlowAgent builder focused on MCP, A2A, and robust deployment\"\"\"\n\n    def __init__(self, config: AgentConfig = None, config_path: str = None):\n        \"\"\"Initialize builder with configuration\"\"\"\n\n        if config and config_path:\n            raise ValueError(\"Provide either config object or config_path, not both\")\n\n        if config_path:\n            self.config = self.load_config(config_path)\n        elif config:\n            self.config = config\n        else:\n            self.config = AgentConfig()\n\n        # Runtime components\n        self._custom_tools: Dict[str, tuple[Callable, str]] = {}\n        self._mcp_tools: Dict[str, Dict] = {}\n        self._budget_manager: Optional[BudgetManager] = None\n        self._tracer_provider: Optional[TracerProvider] = None\n        self._mcp_server: Optional[FastMCP] = None\n        self._a2a_server: Optional[Any] = None\n\n        # Set logging level\n        if self.config.verbose_logging:\n            logging.getLogger().setLevel(logging.DEBUG)\n\n        logger.info(f\"FlowAgent Builder initialized: {self.config.name}\")\n\n    # ===== CONFIGURATION MANAGEMENT =====\n\n    def load_config(self, config_path: str) -&gt; AgentConfig:\n        \"\"\"Load agent configuration from file\"\"\"\n        path = Path(config_path)\n        if not path.exists():\n            raise FileNotFoundError(f\"Config file not found: {config_path}\")\n\n        try:\n            with open(path, 'r', encoding='utf-8') as f:\n                if path.suffix.lower() in ['.yaml', '.yml']:\n                    data = yaml.safe_load(f)\n                else:\n                    data = json.load(f)\n\n            return AgentConfig(**data)\n\n        except Exception as e:\n            logger.error(f\"Failed to load config from {config_path}: {e}\")\n            raise\n\n    def save_config(self, config_path: str, format: str = 'yaml'):\n        \"\"\"Save current configuration to file\"\"\"\n        path = Path(config_path)\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        try:\n            data = self.config.model_dump()\n\n            with open(path, 'w', encoding='utf-8') as f:\n                if format.lower() == 'yaml':\n                    yaml.dump(data, f, default_flow_style=False, indent=2)\n                else:\n                    json.dump(data, f, indent=2)\n\n            logger.info(f\"Configuration saved to {config_path}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to save config to {config_path}: {e}\")\n            raise\n\n    @classmethod\n    def from_config_file(cls, config_path: str) -&gt; 'FlowAgentBuilder':\n        \"\"\"Create builder from configuration file\"\"\"\n        return cls(config_path=config_path)\n\n    # ===== FLUENT BUILDER API =====\n\n    def with_name(self, name: str) -&gt; 'FlowAgentBuilder':\n        \"\"\"Set agent name\"\"\"\n        self.config.name = name\n        return self\n\n    def with_models(self, fast_model: str, complex_model: str = None) -&gt; 'FlowAgentBuilder':\n        \"\"\"Set LLM models\"\"\"\n        self.config.fast_llm_model = fast_model\n        if complex_model:\n            self.config.complex_llm_model = complex_model\n        return self\n\n    def with_system_message(self, message: str) -&gt; 'FlowAgentBuilder':\n        \"\"\"Set system message\"\"\"\n        self.config.system_message = message\n        return self\n\n    def with_temperature(self, temp: float) -&gt; 'FlowAgentBuilder':\n        \"\"\"Set temperature\"\"\"\n        self.config.temperature = temp\n        return self\n\n    def with_budget_manager(self, max_cost: float = 10.0) -&gt; 'FlowAgentBuilder':\n        \"\"\"Enable budget management\"\"\"\n        if LITELLM_AVAILABLE:\n            self._budget_manager = BudgetManager(\"agent\")\n            logger.info(f\"Budget manager enabled: ${max_cost}\")\n        else:\n            logger.warning(\"LiteLLM not available, budget manager disabled\")\n        return self\n\n    def verbose(self, enable: bool = True) -&gt; 'FlowAgentBuilder':\n        \"\"\"Enable verbose logging\"\"\"\n        self.config.verbose_logging = enable\n        if enable:\n            logging.getLogger().setLevel(logging.DEBUG)\n        return self\n\n    # ===== MCP INTEGRATION =====\n\n    def enable_mcp_server(self, host: str = \"0.0.0.0\", port: int = 8000,\n                          server_name: str = None) -&gt; 'FlowAgentBuilder':\n        \"\"\"Enable MCP server\"\"\"\n        if not MCP_AVAILABLE:\n            logger.warning(\"MCP not available, cannot enable server\")\n            return self\n\n        self.config.mcp.enabled = True\n        self.config.mcp.host = host\n        self.config.mcp.port = port\n        self.config.mcp.server_name = server_name or f\"{self.config.name}_MCP\"\n\n        logger.info(f\"MCP server enabled: {host}:{port}\")\n        return self\n\n    def _load_mcp_server_tools(self, server_name: str, server_config: Dict[str, Any]):\n        \"\"\"Load tools from MCP server configuration with actual command execution\"\"\"\n        command = server_config.get('command')\n        args = server_config.get('args', [])\n        env = server_config.get('env', {})\n\n        if not command:\n            logger.warning(f\"No command specified for MCP server {server_name}\")\n            return\n\n        # Create a tool that can execute the MCP server command\n        async def mcp_server_tool(query: str = \"\", **kwargs) -&gt; str:\n            \"\"\"Tool backed by actual MCP server execution\"\"\"\n            try:\n                return await self._execute_mcp_server(command, args, env, query, **kwargs)\n            except Exception as e:\n                logger.error(f\"MCP server tool {server_name} failed: {e}\")\n                return f\"Error executing {server_name}: {str(e)}\"\n\n        self._mcp_tools[server_name] = {\n            'function': mcp_server_tool,\n            'command': command,\n            'args': args,\n            'env': env,\n            'description': f\"MCP server tool: {server_name}\",\n            'server_type': 'command_execution'\n        }\n\n        logger.info(f\"Registered MCP server tool: {server_name} ({command})\")\n\n    async def _execute_mcp_server(self, command: str, args: List[str], env: Dict[str, str],\n                                  query: str, **kwargs) -&gt; str:\n        \"\"\"Execute MCP server command and handle communication\"\"\"\n        try:\n            # Prepare environment\n            process_env = os.environ.copy()\n            process_env.update(env)\n\n            # Build full command\n            full_command = [command] + args\n\n            # Create the subprocess\n            process = await asyncio.create_subprocess_exec(\n                *full_command,\n                stdin=asyncio.subprocess.PIPE,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE,\n                env=process_env,\n                cwd=os.getcwd()\n            )\n\n            # For MCP, we need to send JSON-RPC messages\n            # This is a simplified implementation - in production you'd want full MCP protocol\n            mcp_request = {\n                \"jsonrpc\": \"2.0\",\n                \"id\": str(uuid.uuid4()),\n                \"method\": \"tools/call\",\n                \"params\": {\n                    \"name\": \"process_query\",\n                    \"arguments\": {\"query\": query, **kwargs}\n                }\n            }\n\n            request_json = json.dumps(mcp_request) + \"\\n\"\n\n            # Send request and get response\n            stdout, stderr = await asyncio.wait_for(\n                process.communicate(input=request_json.encode()),\n                timeout=30.0  # 30 second timeout\n            )\n\n            # Parse response\n            if process.returncode == 0:\n                response_text = stdout.decode().strip()\n                if response_text:\n                    try:\n                        # Try to parse as JSON-RPC response\n                        response_data = json.loads(response_text)\n                        if \"result\" in response_data:\n                            return str(response_data[\"result\"])\n                        elif \"error\" in response_data:\n                            return f\"MCP Error: {response_data['error']}\"\n                        else:\n                            return response_text\n                    except json.JSONDecodeError:\n                        # Return raw output if not valid JSON\n                        return response_text\n                else:\n                    return \"MCP server returned empty response\"\n            else:\n                error_msg = stderr.decode().strip() if stderr else \"Unknown error\"\n                return f\"MCP server failed (code {process.returncode}): {error_msg}\"\n\n        except asyncio.TimeoutError:\n            logger.error(f\"MCP server timeout for command: {command}\")\n            return \"MCP server request timed out\"\n        except Exception as e:\n            logger.error(f\"MCP server execution error: {e}\")\n            return f\"MCP server execution failed: {str(e)}\"\n\n    async def _execute_mcp_stdio_server(self, command: str, args: List[str], env: Dict[str, str]) -&gt; Optional[Any]:\n        \"\"\"Execute MCP server using stdio transport (more robust MCP communication)\"\"\"\n        if not MCP_AVAILABLE:\n            logger.warning(\"MCP not available for stdio server execution\")\n            return None\n\n        try:\n            # This would use the actual MCP client to communicate with the server\n            # For now, this is a placeholder for full MCP protocol implementation\n            from mcp.client.stdio import stdio_client\n            from mcp import ClientSession, StdioServerParameters\n\n            # Prepare environment\n            process_env = os.environ.copy()\n            process_env.update(env)\n\n            # Create server parameters\n            server_params = StdioServerParameters(\n                command=command,\n                args=args,\n                env=process_env\n            )\n\n            # This would establish proper MCP communication\n            # Implementation would depend on full MCP client integration\n            logger.info(f\"Would establish MCP stdio connection to: {command} {' '.join(args)}\")\n\n            return None  # Placeholder - full implementation would return MCP client session\n\n        except Exception as e:\n            logger.error(f\"Failed to establish MCP stdio connection: {e}\")\n            return None\n\n    def load_mcp_tools_from_config(self, config_path: str) -&gt; 'FlowAgentBuilder':\n        \"\"\"Enhanced MCP config loading with better error handling and validation\"\"\"\n        if not MCP_AVAILABLE:\n            logger.warning(\"MCP not available, skipping tool loading\")\n            return self\n\n        config_path = Path(config_path)\n        if not config_path.exists():\n            raise FileNotFoundError(f\"MCP config not found: {config_path}\")\n\n        try:\n            with open(config_path, 'r', encoding='utf-8') as f:\n                if config_path.suffix.lower() in ['.yaml', '.yml']:\n                    mcp_config = yaml.safe_load(f)\n                else:\n                    mcp_config = json.load(f)\n\n            # Parse MCP tools from official config format\n            tools_loaded = 0\n\n            # Handle standard MCP server configuration\n            if 'mcpServers' in mcp_config:\n                for server_name, server_config in mcp_config['mcpServers'].items():\n                    # Validate server config\n                    if not self._validate_mcp_server_config(server_name, server_config):\n                        continue\n\n                    self._load_mcp_server_tools(server_name, server_config)\n                    tools_loaded += 1\n\n                    logger.info(\n                        f\"Loaded MCP server: {server_name} - Command: {server_config.get('command')} {' '.join(server_config.get('args', []))}\")\n\n            # Handle direct tools configuration\n            elif 'tools' in mcp_config:\n                for tool_config in mcp_config['tools']:\n                    self._load_direct_mcp_tool(tool_config)\n                    tools_loaded += 1\n\n            # Store config path for later use\n            self.config.mcp.config_path = str(config_path)\n            self.config.mcp.tools_from_config = mcp_config.get('tools', [])\n\n            logger.info(f\"Successfully loaded {tools_loaded} MCP configurations from {config_path}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to load MCP config from {config_path}: {e}\")\n            raise\n\n        return self\n\n    @staticmethod\n    def _validate_mcp_server_config(server_name: str, server_config: Dict[str, Any]) -&gt; bool:\n        \"\"\"Validate MCP server configuration\"\"\"\n        command = server_config.get('command')\n        if not command:\n            logger.error(f\"MCP server {server_name} missing 'command' field\")\n            return False\n\n        # Check if command exists and is executable\n        if command in ['npx', 'node', 'python', 'python3', 'docker']:\n            # These are common commands, assume they exist\n            return True\n\n        # For other commands, check if they exist\n        import shutil\n        if not shutil.which(command):\n            logger.warning(f\"MCP server {server_name}: command '{command}' not found in PATH\")\n            # Don't fail completely, just warn - the command might be available at runtime\n\n        args = server_config.get('args', [])\n        if not isinstance(args, list):\n            logger.error(f\"MCP server {server_name}: 'args' must be a list\")\n            return False\n\n        env = server_config.get('env', {})\n        if not isinstance(env, dict):\n            logger.error(f\"MCP server {server_name}: 'env' must be a dictionary\")\n            return False\n\n        logger.debug(f\"Validated MCP server config: {server_name}\")\n        return True\n\n    def _load_direct_mcp_tool(self, tool_config: Dict[str, Any]):\n        \"\"\"Load tool from direct configuration\"\"\"\n        name = tool_config.get('name')\n        description = tool_config.get('description', '')\n        function_code = tool_config.get('function_code')\n\n        if not name or not function_code:\n            logger.warning(f\"Incomplete tool config: {tool_config}\")\n            return\n\n        # Create function from code\n        try:\n            namespace = {\"__builtins__\": __builtins__}\n            exec(function_code, namespace)\n\n            # Find the function\n            func = None\n            for obj in namespace.values():\n                if callable(obj) and not getattr(obj, '__name__', '').startswith('_'):\n                    func = obj\n                    break\n\n            if func:\n                self._mcp_tools[name] = {\n                    'function': func,\n                    'description': description,\n                    'source': 'code'\n                }\n                logger.debug(f\"Loaded MCP tool from code: {name}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to load MCP tool {name}: {e}\")\n\n    def add_mcp_tool_from_code(self, name: str, code: str, description: str = \"\") -&gt; 'FlowAgentBuilder':\n        \"\"\"Add MCP tool from code string\"\"\"\n        tool_config = {\n            'name': name,\n            'description': description,\n            'function_code': code\n        }\n        self._load_direct_mcp_tool(tool_config)\n        return self\n\n    # ===== A2A INTEGRATION =====\n\n    def enable_a2a_server(self, host: str = \"0.0.0.0\", port: int = 5000,\n                          agent_name: str = None, agent_description: str = None) -&gt; 'FlowAgentBuilder':\n        \"\"\"Enable A2A server for agent-to-agent communication\"\"\"\n        if not A2A_AVAILABLE:\n            logger.warning(\"A2A not available, cannot enable server\")\n            return self\n\n        self.config.a2a.enabled = True\n        self.config.a2a.host = host\n        self.config.a2a.port = port\n        self.config.a2a.agent_name = agent_name or self.config.name\n        self.config.a2a.agent_description = agent_description or self.config.description\n\n        logger.info(f\"A2A server enabled: {host}:{port}\")\n        return self\n\n    # ===== TELEMETRY INTEGRATION =====\n\n    def enable_telemetry(self, service_name: str = None, endpoint: str = None,\n                         console_export: bool = True) -&gt; 'FlowAgentBuilder':\n        \"\"\"Enable OpenTelemetry tracing\"\"\"\n        if not OTEL_AVAILABLE:\n            logger.warning(\"OpenTelemetry not available, cannot enable telemetry\")\n            return self\n\n        self.config.telemetry.enabled = True\n        self.config.telemetry.service_name = service_name or self.config.name\n        self.config.telemetry.endpoint = endpoint\n        self.config.telemetry.console_export = console_export\n\n        # Initialize tracer provider\n        self._tracer_provider = TracerProvider()\n        trace.set_tracer_provider(self._tracer_provider)\n\n        # Add exporters\n        if console_export:\n            console_exporter = ConsoleSpanExporter()\n            span_processor = BatchSpanProcessor(console_exporter)\n            self._tracer_provider.add_span_processor(span_processor)\n\n        if endpoint:\n            try:\n                otlp_exporter = OTLPSpanExporter(endpoint=endpoint)\n                otlp_processor = BatchSpanProcessor(otlp_exporter)\n                self._tracer_provider.add_span_processor(otlp_processor)\n            except Exception as e:\n                logger.warning(f\"Failed to setup OTLP exporter: {e}\")\n\n        logger.info(f\"Telemetry enabled for service: {service_name}\")\n        return self\n\n    # ===== CHECKPOINT CONFIGURATION =====\n\n    def with_checkpointing(self, enabled: bool = True, interval_seconds: int = 300,\n                           checkpoint_dir: str = \"./checkpoints\", max_checkpoints: int = 10) -&gt; 'FlowAgentBuilder':\n        \"\"\"Configure checkpointing\"\"\"\n        self.config.checkpoint.enabled = enabled\n        self.config.checkpoint.interval_seconds = interval_seconds\n        self.config.checkpoint.checkpoint_dir = checkpoint_dir\n        self.config.checkpoint.max_checkpoints = max_checkpoints\n\n        if enabled:\n            # Ensure checkpoint directory exists\n            Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n            logger.info(f\"Checkpointing enabled: {checkpoint_dir} (every {interval_seconds}s)\")\n\n        return self\n\n    # ===== TOOL MANAGEMENT =====\n\n    def add_tool(self, func: Callable, name: str = None, description: str = None) -&gt; 'FlowAgentBuilder':\n        \"\"\"Add custom tool function\"\"\"\n        tool_name = name or func.__name__\n        self._custom_tools[tool_name] = (func, description or func.__doc__)\n\n        logger.info(f\"Tool added: {tool_name}\")\n        return self\n\n    def add_tools_from_module(self, module, prefix: str = \"\", exclude: List[str] = None) -&gt; 'FlowAgentBuilder':\n        \"\"\"Add all functions from a module as tools\"\"\"\n        exclude = exclude or []\n\n        for name, obj in inspect.getmembers(module, inspect.isfunction):\n            if name in exclude or name.startswith('_'):\n                continue\n\n            tool_name = f\"{prefix}{name}\" if prefix else name\n            self.add_tool(obj, name=tool_name)\n\n        logger.info(f\"Added tools from module {module.__name__}\")\n        return self\n\n    # ===== PERSONA MANAGEMENT =====\n\n    def add_persona_profile(self, profile_name: str, name: str, style: str = \"professional\",\n                            tone: str = \"friendly\", personality_traits: List[str] = None,\n                            custom_instructions: str = \"\", response_format: str = None,\n                            text_length: str = None) -&gt; 'FlowAgentBuilder':\n        \"\"\"Add a persona profile with optional format configuration\"\"\"\n\n        if personality_traits is None:\n            personality_traits = [\"helpful\", \"concise\"]\n\n        # Create persona config\n        persona_data = {\n            \"name\": name,\n            \"style\": style,\n            \"tone\": tone,\n            \"personality_traits\": personality_traits,\n            \"custom_instructions\": custom_instructions,\n            \"apply_method\": \"system_prompt\",\n            \"integration_level\": \"light\"\n        }\n\n        # Add format config if specified\n        if response_format or text_length:\n            format_config = {\n                \"response_format\": response_format or \"frei-text\",\n                \"text_length\": text_length or \"chat-conversation\",\n                \"custom_instructions\": \"\",\n                \"strict_format_adherence\": True,\n                \"quality_threshold\": 0.7\n            }\n            persona_data[\"format_config\"] = format_config\n\n        self.config.persona_profiles[profile_name] = persona_data\n        logger.info(f\"Persona profile added: {profile_name}\")\n        return self\n\n    def set_active_persona(self, profile_name: str) -&gt; 'FlowAgentBuilder':\n        \"\"\"Set active persona profile\"\"\"\n        if profile_name in self.config.persona_profiles:\n            self.config.active_persona = profile_name\n            logger.info(f\"Active persona set: {profile_name}\")\n        else:\n            logger.warning(f\"Persona profile not found: {profile_name}\")\n        return self\n\n    def with_developer_persona(self, name: str = \"Senior Developer\") -&gt; 'FlowAgentBuilder':\n        \"\"\"Add and set a pre-built developer persona\"\"\"\n        return (self\n                .add_persona_profile(\n            \"developer\",\n            name=name,\n            style=\"technical\",\n            tone=\"professional\",\n            personality_traits=[\"precise\", \"thorough\", \"security_conscious\", \"best_practices\"],\n            custom_instructions=\"Focus on code quality, maintainability, and security. Always consider edge cases.\",\n            response_format=\"code-structure\",\n            text_length=\"detailed-indepth\"\n        )\n                .set_active_persona(\"developer\"))\n\n    def with_analyst_persona(self, name: str = \"Data Analyst\") -&gt; 'FlowAgentBuilder':\n        \"\"\"Add and set a pre-built analyst persona\"\"\"\n        return (self\n                .add_persona_profile(\n            \"analyst\",\n            name=name,\n            style=\"analytical\",\n            tone=\"objective\",\n            personality_traits=[\"methodical\", \"insight_driven\", \"evidence_based\"],\n            custom_instructions=\"Focus on statistical rigor and actionable recommendations.\",\n            response_format=\"with-tables\",\n            text_length=\"detailed-indepth\"\n        )\n                .set_active_persona(\"analyst\"))\n\n    def with_assistant_persona(self, name: str = \"AI Assistant\") -&gt; 'FlowAgentBuilder':\n        \"\"\"Add and set a pre-built general assistant persona\"\"\"\n        return (self\n                .add_persona_profile(\n            \"assistant\",\n            name=name,\n            style=\"friendly\",\n            tone=\"helpful\",\n            personality_traits=[\"helpful\", \"patient\", \"clear\", \"adaptive\"],\n            custom_instructions=\"Be helpful and adapt communication to user expertise level.\",\n            response_format=\"with-bullet-points\",\n            text_length=\"chat-conversation\"\n        )\n                .set_active_persona(\"assistant\"))\n\n    def with_creative_persona(self, name: str = \"Creative Assistant\") -&gt; 'FlowAgentBuilder':\n        \"\"\"Add and set a pre-built creative persona\"\"\"\n        return (self\n                .add_persona_profile(\n            \"creative\",\n            name=name,\n            style=\"creative\",\n            tone=\"inspiring\",\n            personality_traits=[\"imaginative\", \"expressive\", \"innovative\", \"engaging\"],\n            custom_instructions=\"Think outside the box and provide creative, inspiring solutions.\",\n            response_format=\"md-text\",\n            text_length=\"detailed-indepth\"\n        )\n                .set_active_persona(\"creative\"))\n\n    def with_executive_persona(self, name: str = \"Executive Assistant\") -&gt; 'FlowAgentBuilder':\n        \"\"\"Add and set a pre-built executive persona\"\"\"\n        return (self\n                .add_persona_profile(\n            \"executive\",\n            name=name,\n            style=\"professional\",\n            tone=\"authoritative\",\n            personality_traits=[\"strategic\", \"decisive\", \"results_oriented\", \"efficient\"],\n            custom_instructions=\"Provide strategic insights with executive-level clarity and focus on outcomes.\",\n            response_format=\"with-bullet-points\",\n            text_length=\"table-conversation\"\n        )\n                .set_active_persona(\"executive\"))\n\n    # ===== VARIABLE MANAGEMENT =====\n\n    def with_custom_variables(self, variables: Dict[str, Any]) -&gt; 'FlowAgentBuilder':\n        \"\"\"Add custom variables\"\"\"\n        self.config.custom_variables.update(variables)\n        return self\n\n    def with_world_model(self, world_model: Dict[str, Any]) -&gt; 'FlowAgentBuilder':\n        \"\"\"Set initial world model\"\"\"\n        self.config.initial_world_model.update(world_model)\n        return self\n\n    # ===== VALIDATION =====\n\n    def validate_config(self) -&gt; Dict[str, List[str]]:\n        \"\"\"Validate the current configuration\"\"\"\n        issues = {\"errors\": [], \"warnings\": []}\n\n        # Validate required settings\n        if not self.config.fast_llm_model:\n            issues[\"errors\"].append(\"Fast LLM model not specified\")\n        if not self.config.complex_llm_model:\n            issues[\"errors\"].append(\"Complex LLM model not specified\")\n\n        # Validate MCP configuration\n        if self.config.mcp.enabled and not MCP_AVAILABLE:\n            issues[\"errors\"].append(\"MCP enabled but MCP not available\")\n\n        # Validate A2A configuration\n        if self.config.a2a.enabled and not A2A_AVAILABLE:\n            issues[\"errors\"].append(\"A2A enabled but A2A not available\")\n\n        # Validate telemetry\n        if self.config.telemetry.enabled and not OTEL_AVAILABLE:\n            issues[\"errors\"].append(\"Telemetry enabled but OpenTelemetry not available\")\n\n        # Validate personas\n        if self.config.active_persona and self.config.active_persona not in self.config.persona_profiles:\n            issues[\"errors\"].append(f\"Active persona '{self.config.active_persona}' not found in profiles\")\n\n        # Validate checkpoint directory\n        if self.config.checkpoint.enabled:\n            try:\n                Path(self.config.checkpoint.checkpoint_dir).mkdir(parents=True, exist_ok=True)\n            except Exception as e:\n                issues[\"warnings\"].append(f\"Cannot create checkpoint directory: {e}\")\n\n        return issues\n\n    # ===== MAIN BUILD METHOD =====\n\n    async def build(self) -&gt; FlowAgent:\n        \"\"\"Build the production-ready FlowAgent\"\"\"\n\n        logger.info(f\"Building production FlowAgent: {self.config.name}\")\n\n        # Validate configuration\n        validation_issues = self.validate_config()\n        if validation_issues[\"errors\"]:\n            error_msg = f\"Configuration validation failed: {', '.join(validation_issues['errors'])}\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        # Log warnings\n        for warning in validation_issues[\"warnings\"]:\n            logger.warning(f\"Configuration warning: {warning}\")\n\n        try:\n            # 1. Setup API configuration\n            api_key = None\n            if self.config.api_key_env_var:\n                api_key = os.getenv(self.config.api_key_env_var)\n                if not api_key:\n                    logger.warning(f\"API key env var {self.config.api_key_env_var} not set\")\n\n            # 2. Create persona if configured\n            active_persona = None\n            if self.config.active_persona and self.config.active_persona in self.config.persona_profiles:\n                persona_data = self.config.persona_profiles[self.config.active_persona]\n\n                # Create FormatConfig if present\n                format_config = None\n                if \"format_config\" in persona_data:\n                    fc_data = persona_data.pop(\"format_config\")\n                    format_config = FormatConfig(\n                        response_format=ResponseFormat(fc_data.get(\"response_format\", \"frei-text\")),\n                        text_length=TextLength(fc_data.get(\"text_length\", \"chat-conversation\")),\n                        custom_instructions=fc_data.get(\"custom_instructions\", \"\"),\n                        strict_format_adherence=fc_data.get(\"strict_format_adherence\", True),\n                        quality_threshold=fc_data.get(\"quality_threshold\", 0.7)\n                    )\n\n                active_persona = PersonaConfig(**persona_data)\n                active_persona.format_config = format_config\n\n                logger.info(f\"Using persona: {active_persona.name}\")\n\n            # 3. Create AgentModelData\n            amd = AgentModelData(\n                name=self.config.name,\n                fast_llm_model=self.config.fast_llm_model,\n                complex_llm_model=self.config.complex_llm_model,\n                system_message=self.config.system_message,\n                temperature=self.config.temperature,\n                max_tokens=self.config.max_tokens_output,\n                max_input_tokens=self.config.max_tokens_input,\n                api_key=api_key,\n                budget_manager=self._budget_manager,\n                persona=active_persona,\n                use_fast_response=self.config.use_fast_response\n            )\n\n            # 4. Create FlowAgent\n            agent = FlowAgent(\n                amd=amd,\n                world_model=self.config.initial_world_model.copy(),\n                verbose=self.config.verbose_logging,\n                enable_pause_resume=self.config.checkpoint.enabled,\n                checkpoint_interval=self.config.checkpoint.interval_seconds,\n                max_parallel_tasks=self.config.max_parallel_tasks\n            )\n\n            # 5. Add custom variables\n            for key, value in self.config.custom_variables.items():\n                agent.set_variable(key, value)\n\n            # 6. Add custom tools\n            tools_added = 0\n            for tool_name, (tool_func, tool_description) in self._custom_tools.items():\n                try:\n                    await agent.add_tool(tool_func, tool_name, tool_description)\n                    tools_added += 1\n                except Exception as e:\n                    logger.error(f\"Failed to add tool {tool_name}: {e}\")\n\n            # 7. Add MCP tools\n            for tool_name, tool_info in self._mcp_tools.items():\n                try:\n                    await agent.add_tool(\n                        tool_info['function'],\n                        tool_name,\n                        tool_info['description']\n                    )\n                    tools_added += 1\n                except Exception as e:\n                    logger.error(f\"Failed to add MCP tool {tool_name}: {e}\")\n\n            # 8. Setup MCP server\n            if self.config.mcp.enabled and MCP_AVAILABLE:\n                try:\n                    agent.setup_mcp_server(\n                        host=self.config.mcp.host,\n                        port=self.config.mcp.port,\n                        name=self.config.mcp.server_name\n                    )\n                    logger.info(\"MCP server configured\")\n                except Exception as e:\n                    logger.error(f\"Failed to setup MCP server: {e}\")\n\n            # 9. Setup A2A server\n            if self.config.a2a.enabled and A2A_AVAILABLE:\n                try:\n                    agent.setup_a2a_server(\n                        host=self.config.a2a.host,\n                        port=self.config.a2a.port\n                    )\n                    logger.info(\"A2A server configured\")\n                except Exception as e:\n                    logger.error(f\"Failed to setup A2A server: {e}\")\n\n            # 10. Initialize enhanced session context\n            try:\n                await agent.initialize_session_context(max_history=200)\n                logger.info(\"Enhanced session context initialized\")\n            except Exception as e:\n                logger.warning(f\"Session context initialization failed: {e}\")\n\n            # Final summary\n            logger.info(f\"ok FlowAgent built successfully!\")\n            logger.info(f\"   Agent: {agent.amd.name}\")\n            logger.info(f\"   Tools: {tools_added}\")\n            logger.info(f\"   MCP: {'ok' if self.config.mcp.enabled else 'F'}\")\n            logger.info(f\"   A2A: {'ok' if self.config.a2a.enabled else 'F'}\")\n            logger.info(f\"   Telemetry: {'ok' if self.config.telemetry.enabled else 'F'}\")\n            logger.info(f\"   Checkpoints: {'ok' if self.config.checkpoint.enabled else 'F'}\")\n            logger.info(f\"   Persona: {active_persona.name if active_persona else 'Default'}\")\n\n            return agent\n\n        except Exception as e:\n            logger.error(f\"Failed to build FlowAgent: {e}\")\n            raise\n\n    # ===== FACTORY METHODS =====\n\n    @classmethod\n    def create_developer_agent(cls, name: str = \"DeveloperAgent\",\n                               with_mcp: bool = True, with_a2a: bool = False) -&gt; 'FlowAgentBuilder':\n        \"\"\"Create a pre-configured developer agent\"\"\"\n        builder = (cls()\n                   .with_name(name)\n                   .with_developer_persona()\n                   .with_checkpointing(enabled=True, interval_seconds=300)\n                   .verbose(True))\n\n        if with_mcp:\n            builder.enable_mcp_server(port=8001)\n        if with_a2a:\n            builder.enable_a2a_server(port=5001)\n\n        return builder\n\n    @classmethod\n    def create_analyst_agent(cls, name: str = \"AnalystAgent\",\n                             with_telemetry: bool = True) -&gt; 'FlowAgentBuilder':\n        \"\"\"Create a pre-configured data analyst agent\"\"\"\n        builder = (cls()\n                   .with_name(name)\n                   .with_analyst_persona()\n                   .with_checkpointing(enabled=True)\n                   .verbose(False))\n\n        if with_telemetry:\n            builder.enable_telemetry(console_export=True)\n\n        return builder\n\n    @classmethod\n    def create_general_assistant(cls, name: str = \"AssistantAgent\",\n                                 full_integration: bool = True) -&gt; 'FlowAgentBuilder':\n        \"\"\"Create a general-purpose assistant with full integration\"\"\"\n        builder = (cls()\n                   .with_name(name)\n                   .with_assistant_persona()\n                   .with_checkpointing(enabled=True))\n\n        if full_integration:\n            builder.enable_mcp_server()\n            builder.enable_a2a_server()\n            builder.enable_telemetry()\n\n        return builder\n\n    @classmethod\n    def create_creative_agent(cls, name: str = \"CreativeAgent\") -&gt; 'FlowAgentBuilder':\n        \"\"\"Create a creative assistant agent\"\"\"\n        return (cls()\n                .with_name(name)\n                .with_creative_persona()\n                .with_temperature(0.8)  # More creative\n                .with_checkpointing(enabled=True))\n\n    @classmethod\n    def create_executive_agent(cls, name: str = \"ExecutiveAgent\",\n                               with_integrations: bool = True) -&gt; 'FlowAgentBuilder':\n        \"\"\"Create an executive assistant agent\"\"\"\n        builder = (cls()\n                   .with_name(name)\n                   .with_executive_persona()\n                   .with_checkpointing(enabled=True))\n\n        if with_integrations:\n            builder.enable_a2a_server()  # Executives need A2A for delegation\n            builder.enable_telemetry()  # Need metrics\n\n        return builder\n</code></pre> <code>__init__(config=None, config_path=None)</code> \u00b6 <p>Initialize builder with configuration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def __init__(self, config: AgentConfig = None, config_path: str = None):\n    \"\"\"Initialize builder with configuration\"\"\"\n\n    if config and config_path:\n        raise ValueError(\"Provide either config object or config_path, not both\")\n\n    if config_path:\n        self.config = self.load_config(config_path)\n    elif config:\n        self.config = config\n    else:\n        self.config = AgentConfig()\n\n    # Runtime components\n    self._custom_tools: Dict[str, tuple[Callable, str]] = {}\n    self._mcp_tools: Dict[str, Dict] = {}\n    self._budget_manager: Optional[BudgetManager] = None\n    self._tracer_provider: Optional[TracerProvider] = None\n    self._mcp_server: Optional[FastMCP] = None\n    self._a2a_server: Optional[Any] = None\n\n    # Set logging level\n    if self.config.verbose_logging:\n        logging.getLogger().setLevel(logging.DEBUG)\n\n    logger.info(f\"FlowAgent Builder initialized: {self.config.name}\")\n</code></pre> <code>add_mcp_tool_from_code(name, code, description='')</code> \u00b6 <p>Add MCP tool from code string</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def add_mcp_tool_from_code(self, name: str, code: str, description: str = \"\") -&gt; 'FlowAgentBuilder':\n    \"\"\"Add MCP tool from code string\"\"\"\n    tool_config = {\n        'name': name,\n        'description': description,\n        'function_code': code\n    }\n    self._load_direct_mcp_tool(tool_config)\n    return self\n</code></pre> <code>add_persona_profile(profile_name, name, style='professional', tone='friendly', personality_traits=None, custom_instructions='', response_format=None, text_length=None)</code> \u00b6 <p>Add a persona profile with optional format configuration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def add_persona_profile(self, profile_name: str, name: str, style: str = \"professional\",\n                        tone: str = \"friendly\", personality_traits: List[str] = None,\n                        custom_instructions: str = \"\", response_format: str = None,\n                        text_length: str = None) -&gt; 'FlowAgentBuilder':\n    \"\"\"Add a persona profile with optional format configuration\"\"\"\n\n    if personality_traits is None:\n        personality_traits = [\"helpful\", \"concise\"]\n\n    # Create persona config\n    persona_data = {\n        \"name\": name,\n        \"style\": style,\n        \"tone\": tone,\n        \"personality_traits\": personality_traits,\n        \"custom_instructions\": custom_instructions,\n        \"apply_method\": \"system_prompt\",\n        \"integration_level\": \"light\"\n    }\n\n    # Add format config if specified\n    if response_format or text_length:\n        format_config = {\n            \"response_format\": response_format or \"frei-text\",\n            \"text_length\": text_length or \"chat-conversation\",\n            \"custom_instructions\": \"\",\n            \"strict_format_adherence\": True,\n            \"quality_threshold\": 0.7\n        }\n        persona_data[\"format_config\"] = format_config\n\n    self.config.persona_profiles[profile_name] = persona_data\n    logger.info(f\"Persona profile added: {profile_name}\")\n    return self\n</code></pre> <code>add_tool(func, name=None, description=None)</code> \u00b6 <p>Add custom tool function</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def add_tool(self, func: Callable, name: str = None, description: str = None) -&gt; 'FlowAgentBuilder':\n    \"\"\"Add custom tool function\"\"\"\n    tool_name = name or func.__name__\n    self._custom_tools[tool_name] = (func, description or func.__doc__)\n\n    logger.info(f\"Tool added: {tool_name}\")\n    return self\n</code></pre> <code>add_tools_from_module(module, prefix='', exclude=None)</code> \u00b6 <p>Add all functions from a module as tools</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def add_tools_from_module(self, module, prefix: str = \"\", exclude: List[str] = None) -&gt; 'FlowAgentBuilder':\n    \"\"\"Add all functions from a module as tools\"\"\"\n    exclude = exclude or []\n\n    for name, obj in inspect.getmembers(module, inspect.isfunction):\n        if name in exclude or name.startswith('_'):\n            continue\n\n        tool_name = f\"{prefix}{name}\" if prefix else name\n        self.add_tool(obj, name=tool_name)\n\n    logger.info(f\"Added tools from module {module.__name__}\")\n    return self\n</code></pre> <code>build()</code> <code>async</code> \u00b6 <p>Build the production-ready FlowAgent</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>async def build(self) -&gt; FlowAgent:\n    \"\"\"Build the production-ready FlowAgent\"\"\"\n\n    logger.info(f\"Building production FlowAgent: {self.config.name}\")\n\n    # Validate configuration\n    validation_issues = self.validate_config()\n    if validation_issues[\"errors\"]:\n        error_msg = f\"Configuration validation failed: {', '.join(validation_issues['errors'])}\"\n        logger.error(error_msg)\n        raise ValueError(error_msg)\n\n    # Log warnings\n    for warning in validation_issues[\"warnings\"]:\n        logger.warning(f\"Configuration warning: {warning}\")\n\n    try:\n        # 1. Setup API configuration\n        api_key = None\n        if self.config.api_key_env_var:\n            api_key = os.getenv(self.config.api_key_env_var)\n            if not api_key:\n                logger.warning(f\"API key env var {self.config.api_key_env_var} not set\")\n\n        # 2. Create persona if configured\n        active_persona = None\n        if self.config.active_persona and self.config.active_persona in self.config.persona_profiles:\n            persona_data = self.config.persona_profiles[self.config.active_persona]\n\n            # Create FormatConfig if present\n            format_config = None\n            if \"format_config\" in persona_data:\n                fc_data = persona_data.pop(\"format_config\")\n                format_config = FormatConfig(\n                    response_format=ResponseFormat(fc_data.get(\"response_format\", \"frei-text\")),\n                    text_length=TextLength(fc_data.get(\"text_length\", \"chat-conversation\")),\n                    custom_instructions=fc_data.get(\"custom_instructions\", \"\"),\n                    strict_format_adherence=fc_data.get(\"strict_format_adherence\", True),\n                    quality_threshold=fc_data.get(\"quality_threshold\", 0.7)\n                )\n\n            active_persona = PersonaConfig(**persona_data)\n            active_persona.format_config = format_config\n\n            logger.info(f\"Using persona: {active_persona.name}\")\n\n        # 3. Create AgentModelData\n        amd = AgentModelData(\n            name=self.config.name,\n            fast_llm_model=self.config.fast_llm_model,\n            complex_llm_model=self.config.complex_llm_model,\n            system_message=self.config.system_message,\n            temperature=self.config.temperature,\n            max_tokens=self.config.max_tokens_output,\n            max_input_tokens=self.config.max_tokens_input,\n            api_key=api_key,\n            budget_manager=self._budget_manager,\n            persona=active_persona,\n            use_fast_response=self.config.use_fast_response\n        )\n\n        # 4. Create FlowAgent\n        agent = FlowAgent(\n            amd=amd,\n            world_model=self.config.initial_world_model.copy(),\n            verbose=self.config.verbose_logging,\n            enable_pause_resume=self.config.checkpoint.enabled,\n            checkpoint_interval=self.config.checkpoint.interval_seconds,\n            max_parallel_tasks=self.config.max_parallel_tasks\n        )\n\n        # 5. Add custom variables\n        for key, value in self.config.custom_variables.items():\n            agent.set_variable(key, value)\n\n        # 6. Add custom tools\n        tools_added = 0\n        for tool_name, (tool_func, tool_description) in self._custom_tools.items():\n            try:\n                await agent.add_tool(tool_func, tool_name, tool_description)\n                tools_added += 1\n            except Exception as e:\n                logger.error(f\"Failed to add tool {tool_name}: {e}\")\n\n        # 7. Add MCP tools\n        for tool_name, tool_info in self._mcp_tools.items():\n            try:\n                await agent.add_tool(\n                    tool_info['function'],\n                    tool_name,\n                    tool_info['description']\n                )\n                tools_added += 1\n            except Exception as e:\n                logger.error(f\"Failed to add MCP tool {tool_name}: {e}\")\n\n        # 8. Setup MCP server\n        if self.config.mcp.enabled and MCP_AVAILABLE:\n            try:\n                agent.setup_mcp_server(\n                    host=self.config.mcp.host,\n                    port=self.config.mcp.port,\n                    name=self.config.mcp.server_name\n                )\n                logger.info(\"MCP server configured\")\n            except Exception as e:\n                logger.error(f\"Failed to setup MCP server: {e}\")\n\n        # 9. Setup A2A server\n        if self.config.a2a.enabled and A2A_AVAILABLE:\n            try:\n                agent.setup_a2a_server(\n                    host=self.config.a2a.host,\n                    port=self.config.a2a.port\n                )\n                logger.info(\"A2A server configured\")\n            except Exception as e:\n                logger.error(f\"Failed to setup A2A server: {e}\")\n\n        # 10. Initialize enhanced session context\n        try:\n            await agent.initialize_session_context(max_history=200)\n            logger.info(\"Enhanced session context initialized\")\n        except Exception as e:\n            logger.warning(f\"Session context initialization failed: {e}\")\n\n        # Final summary\n        logger.info(f\"ok FlowAgent built successfully!\")\n        logger.info(f\"   Agent: {agent.amd.name}\")\n        logger.info(f\"   Tools: {tools_added}\")\n        logger.info(f\"   MCP: {'ok' if self.config.mcp.enabled else 'F'}\")\n        logger.info(f\"   A2A: {'ok' if self.config.a2a.enabled else 'F'}\")\n        logger.info(f\"   Telemetry: {'ok' if self.config.telemetry.enabled else 'F'}\")\n        logger.info(f\"   Checkpoints: {'ok' if self.config.checkpoint.enabled else 'F'}\")\n        logger.info(f\"   Persona: {active_persona.name if active_persona else 'Default'}\")\n\n        return agent\n\n    except Exception as e:\n        logger.error(f\"Failed to build FlowAgent: {e}\")\n        raise\n</code></pre> <code>create_analyst_agent(name='AnalystAgent', with_telemetry=True)</code> <code>classmethod</code> \u00b6 <p>Create a pre-configured data analyst agent</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>@classmethod\ndef create_analyst_agent(cls, name: str = \"AnalystAgent\",\n                         with_telemetry: bool = True) -&gt; 'FlowAgentBuilder':\n    \"\"\"Create a pre-configured data analyst agent\"\"\"\n    builder = (cls()\n               .with_name(name)\n               .with_analyst_persona()\n               .with_checkpointing(enabled=True)\n               .verbose(False))\n\n    if with_telemetry:\n        builder.enable_telemetry(console_export=True)\n\n    return builder\n</code></pre> <code>create_creative_agent(name='CreativeAgent')</code> <code>classmethod</code> \u00b6 <p>Create a creative assistant agent</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>@classmethod\ndef create_creative_agent(cls, name: str = \"CreativeAgent\") -&gt; 'FlowAgentBuilder':\n    \"\"\"Create a creative assistant agent\"\"\"\n    return (cls()\n            .with_name(name)\n            .with_creative_persona()\n            .with_temperature(0.8)  # More creative\n            .with_checkpointing(enabled=True))\n</code></pre> <code>create_developer_agent(name='DeveloperAgent', with_mcp=True, with_a2a=False)</code> <code>classmethod</code> \u00b6 <p>Create a pre-configured developer agent</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>@classmethod\ndef create_developer_agent(cls, name: str = \"DeveloperAgent\",\n                           with_mcp: bool = True, with_a2a: bool = False) -&gt; 'FlowAgentBuilder':\n    \"\"\"Create a pre-configured developer agent\"\"\"\n    builder = (cls()\n               .with_name(name)\n               .with_developer_persona()\n               .with_checkpointing(enabled=True, interval_seconds=300)\n               .verbose(True))\n\n    if with_mcp:\n        builder.enable_mcp_server(port=8001)\n    if with_a2a:\n        builder.enable_a2a_server(port=5001)\n\n    return builder\n</code></pre> <code>create_executive_agent(name='ExecutiveAgent', with_integrations=True)</code> <code>classmethod</code> \u00b6 <p>Create an executive assistant agent</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>@classmethod\ndef create_executive_agent(cls, name: str = \"ExecutiveAgent\",\n                           with_integrations: bool = True) -&gt; 'FlowAgentBuilder':\n    \"\"\"Create an executive assistant agent\"\"\"\n    builder = (cls()\n               .with_name(name)\n               .with_executive_persona()\n               .with_checkpointing(enabled=True))\n\n    if with_integrations:\n        builder.enable_a2a_server()  # Executives need A2A for delegation\n        builder.enable_telemetry()  # Need metrics\n\n    return builder\n</code></pre> <code>create_general_assistant(name='AssistantAgent', full_integration=True)</code> <code>classmethod</code> \u00b6 <p>Create a general-purpose assistant with full integration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>@classmethod\ndef create_general_assistant(cls, name: str = \"AssistantAgent\",\n                             full_integration: bool = True) -&gt; 'FlowAgentBuilder':\n    \"\"\"Create a general-purpose assistant with full integration\"\"\"\n    builder = (cls()\n               .with_name(name)\n               .with_assistant_persona()\n               .with_checkpointing(enabled=True))\n\n    if full_integration:\n        builder.enable_mcp_server()\n        builder.enable_a2a_server()\n        builder.enable_telemetry()\n\n    return builder\n</code></pre> <code>enable_a2a_server(host='0.0.0.0', port=5000, agent_name=None, agent_description=None)</code> \u00b6 <p>Enable A2A server for agent-to-agent communication</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def enable_a2a_server(self, host: str = \"0.0.0.0\", port: int = 5000,\n                      agent_name: str = None, agent_description: str = None) -&gt; 'FlowAgentBuilder':\n    \"\"\"Enable A2A server for agent-to-agent communication\"\"\"\n    if not A2A_AVAILABLE:\n        logger.warning(\"A2A not available, cannot enable server\")\n        return self\n\n    self.config.a2a.enabled = True\n    self.config.a2a.host = host\n    self.config.a2a.port = port\n    self.config.a2a.agent_name = agent_name or self.config.name\n    self.config.a2a.agent_description = agent_description or self.config.description\n\n    logger.info(f\"A2A server enabled: {host}:{port}\")\n    return self\n</code></pre> <code>enable_mcp_server(host='0.0.0.0', port=8000, server_name=None)</code> \u00b6 <p>Enable MCP server</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def enable_mcp_server(self, host: str = \"0.0.0.0\", port: int = 8000,\n                      server_name: str = None) -&gt; 'FlowAgentBuilder':\n    \"\"\"Enable MCP server\"\"\"\n    if not MCP_AVAILABLE:\n        logger.warning(\"MCP not available, cannot enable server\")\n        return self\n\n    self.config.mcp.enabled = True\n    self.config.mcp.host = host\n    self.config.mcp.port = port\n    self.config.mcp.server_name = server_name or f\"{self.config.name}_MCP\"\n\n    logger.info(f\"MCP server enabled: {host}:{port}\")\n    return self\n</code></pre> <code>enable_telemetry(service_name=None, endpoint=None, console_export=True)</code> \u00b6 <p>Enable OpenTelemetry tracing</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def enable_telemetry(self, service_name: str = None, endpoint: str = None,\n                     console_export: bool = True) -&gt; 'FlowAgentBuilder':\n    \"\"\"Enable OpenTelemetry tracing\"\"\"\n    if not OTEL_AVAILABLE:\n        logger.warning(\"OpenTelemetry not available, cannot enable telemetry\")\n        return self\n\n    self.config.telemetry.enabled = True\n    self.config.telemetry.service_name = service_name or self.config.name\n    self.config.telemetry.endpoint = endpoint\n    self.config.telemetry.console_export = console_export\n\n    # Initialize tracer provider\n    self._tracer_provider = TracerProvider()\n    trace.set_tracer_provider(self._tracer_provider)\n\n    # Add exporters\n    if console_export:\n        console_exporter = ConsoleSpanExporter()\n        span_processor = BatchSpanProcessor(console_exporter)\n        self._tracer_provider.add_span_processor(span_processor)\n\n    if endpoint:\n        try:\n            otlp_exporter = OTLPSpanExporter(endpoint=endpoint)\n            otlp_processor = BatchSpanProcessor(otlp_exporter)\n            self._tracer_provider.add_span_processor(otlp_processor)\n        except Exception as e:\n            logger.warning(f\"Failed to setup OTLP exporter: {e}\")\n\n    logger.info(f\"Telemetry enabled for service: {service_name}\")\n    return self\n</code></pre> <code>from_config_file(config_path)</code> <code>classmethod</code> \u00b6 <p>Create builder from configuration file</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>@classmethod\ndef from_config_file(cls, config_path: str) -&gt; 'FlowAgentBuilder':\n    \"\"\"Create builder from configuration file\"\"\"\n    return cls(config_path=config_path)\n</code></pre> <code>load_config(config_path)</code> \u00b6 <p>Load agent configuration from file</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def load_config(self, config_path: str) -&gt; AgentConfig:\n    \"\"\"Load agent configuration from file\"\"\"\n    path = Path(config_path)\n    if not path.exists():\n        raise FileNotFoundError(f\"Config file not found: {config_path}\")\n\n    try:\n        with open(path, 'r', encoding='utf-8') as f:\n            if path.suffix.lower() in ['.yaml', '.yml']:\n                data = yaml.safe_load(f)\n            else:\n                data = json.load(f)\n\n        return AgentConfig(**data)\n\n    except Exception as e:\n        logger.error(f\"Failed to load config from {config_path}: {e}\")\n        raise\n</code></pre> <code>load_mcp_tools_from_config(config_path)</code> \u00b6 <p>Enhanced MCP config loading with better error handling and validation</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def load_mcp_tools_from_config(self, config_path: str) -&gt; 'FlowAgentBuilder':\n    \"\"\"Enhanced MCP config loading with better error handling and validation\"\"\"\n    if not MCP_AVAILABLE:\n        logger.warning(\"MCP not available, skipping tool loading\")\n        return self\n\n    config_path = Path(config_path)\n    if not config_path.exists():\n        raise FileNotFoundError(f\"MCP config not found: {config_path}\")\n\n    try:\n        with open(config_path, 'r', encoding='utf-8') as f:\n            if config_path.suffix.lower() in ['.yaml', '.yml']:\n                mcp_config = yaml.safe_load(f)\n            else:\n                mcp_config = json.load(f)\n\n        # Parse MCP tools from official config format\n        tools_loaded = 0\n\n        # Handle standard MCP server configuration\n        if 'mcpServers' in mcp_config:\n            for server_name, server_config in mcp_config['mcpServers'].items():\n                # Validate server config\n                if not self._validate_mcp_server_config(server_name, server_config):\n                    continue\n\n                self._load_mcp_server_tools(server_name, server_config)\n                tools_loaded += 1\n\n                logger.info(\n                    f\"Loaded MCP server: {server_name} - Command: {server_config.get('command')} {' '.join(server_config.get('args', []))}\")\n\n        # Handle direct tools configuration\n        elif 'tools' in mcp_config:\n            for tool_config in mcp_config['tools']:\n                self._load_direct_mcp_tool(tool_config)\n                tools_loaded += 1\n\n        # Store config path for later use\n        self.config.mcp.config_path = str(config_path)\n        self.config.mcp.tools_from_config = mcp_config.get('tools', [])\n\n        logger.info(f\"Successfully loaded {tools_loaded} MCP configurations from {config_path}\")\n\n    except Exception as e:\n        logger.error(f\"Failed to load MCP config from {config_path}: {e}\")\n        raise\n\n    return self\n</code></pre> <code>save_config(config_path, format='yaml')</code> \u00b6 <p>Save current configuration to file</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def save_config(self, config_path: str, format: str = 'yaml'):\n    \"\"\"Save current configuration to file\"\"\"\n    path = Path(config_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    try:\n        data = self.config.model_dump()\n\n        with open(path, 'w', encoding='utf-8') as f:\n            if format.lower() == 'yaml':\n                yaml.dump(data, f, default_flow_style=False, indent=2)\n            else:\n                json.dump(data, f, indent=2)\n\n        logger.info(f\"Configuration saved to {config_path}\")\n\n    except Exception as e:\n        logger.error(f\"Failed to save config to {config_path}: {e}\")\n        raise\n</code></pre> <code>set_active_persona(profile_name)</code> \u00b6 <p>Set active persona profile</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def set_active_persona(self, profile_name: str) -&gt; 'FlowAgentBuilder':\n    \"\"\"Set active persona profile\"\"\"\n    if profile_name in self.config.persona_profiles:\n        self.config.active_persona = profile_name\n        logger.info(f\"Active persona set: {profile_name}\")\n    else:\n        logger.warning(f\"Persona profile not found: {profile_name}\")\n    return self\n</code></pre> <code>validate_config()</code> \u00b6 <p>Validate the current configuration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def validate_config(self) -&gt; Dict[str, List[str]]:\n    \"\"\"Validate the current configuration\"\"\"\n    issues = {\"errors\": [], \"warnings\": []}\n\n    # Validate required settings\n    if not self.config.fast_llm_model:\n        issues[\"errors\"].append(\"Fast LLM model not specified\")\n    if not self.config.complex_llm_model:\n        issues[\"errors\"].append(\"Complex LLM model not specified\")\n\n    # Validate MCP configuration\n    if self.config.mcp.enabled and not MCP_AVAILABLE:\n        issues[\"errors\"].append(\"MCP enabled but MCP not available\")\n\n    # Validate A2A configuration\n    if self.config.a2a.enabled and not A2A_AVAILABLE:\n        issues[\"errors\"].append(\"A2A enabled but A2A not available\")\n\n    # Validate telemetry\n    if self.config.telemetry.enabled and not OTEL_AVAILABLE:\n        issues[\"errors\"].append(\"Telemetry enabled but OpenTelemetry not available\")\n\n    # Validate personas\n    if self.config.active_persona and self.config.active_persona not in self.config.persona_profiles:\n        issues[\"errors\"].append(f\"Active persona '{self.config.active_persona}' not found in profiles\")\n\n    # Validate checkpoint directory\n    if self.config.checkpoint.enabled:\n        try:\n            Path(self.config.checkpoint.checkpoint_dir).mkdir(parents=True, exist_ok=True)\n        except Exception as e:\n            issues[\"warnings\"].append(f\"Cannot create checkpoint directory: {e}\")\n\n    return issues\n</code></pre> <code>verbose(enable=True)</code> \u00b6 <p>Enable verbose logging</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def verbose(self, enable: bool = True) -&gt; 'FlowAgentBuilder':\n    \"\"\"Enable verbose logging\"\"\"\n    self.config.verbose_logging = enable\n    if enable:\n        logging.getLogger().setLevel(logging.DEBUG)\n    return self\n</code></pre> <code>with_analyst_persona(name='Data Analyst')</code> \u00b6 <p>Add and set a pre-built analyst persona</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_analyst_persona(self, name: str = \"Data Analyst\") -&gt; 'FlowAgentBuilder':\n    \"\"\"Add and set a pre-built analyst persona\"\"\"\n    return (self\n            .add_persona_profile(\n        \"analyst\",\n        name=name,\n        style=\"analytical\",\n        tone=\"objective\",\n        personality_traits=[\"methodical\", \"insight_driven\", \"evidence_based\"],\n        custom_instructions=\"Focus on statistical rigor and actionable recommendations.\",\n        response_format=\"with-tables\",\n        text_length=\"detailed-indepth\"\n    )\n            .set_active_persona(\"analyst\"))\n</code></pre> <code>with_assistant_persona(name='AI Assistant')</code> \u00b6 <p>Add and set a pre-built general assistant persona</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_assistant_persona(self, name: str = \"AI Assistant\") -&gt; 'FlowAgentBuilder':\n    \"\"\"Add and set a pre-built general assistant persona\"\"\"\n    return (self\n            .add_persona_profile(\n        \"assistant\",\n        name=name,\n        style=\"friendly\",\n        tone=\"helpful\",\n        personality_traits=[\"helpful\", \"patient\", \"clear\", \"adaptive\"],\n        custom_instructions=\"Be helpful and adapt communication to user expertise level.\",\n        response_format=\"with-bullet-points\",\n        text_length=\"chat-conversation\"\n    )\n            .set_active_persona(\"assistant\"))\n</code></pre> <code>with_budget_manager(max_cost=10.0)</code> \u00b6 <p>Enable budget management</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_budget_manager(self, max_cost: float = 10.0) -&gt; 'FlowAgentBuilder':\n    \"\"\"Enable budget management\"\"\"\n    if LITELLM_AVAILABLE:\n        self._budget_manager = BudgetManager(\"agent\")\n        logger.info(f\"Budget manager enabled: ${max_cost}\")\n    else:\n        logger.warning(\"LiteLLM not available, budget manager disabled\")\n    return self\n</code></pre> <code>with_checkpointing(enabled=True, interval_seconds=300, checkpoint_dir='./checkpoints', max_checkpoints=10)</code> \u00b6 <p>Configure checkpointing</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_checkpointing(self, enabled: bool = True, interval_seconds: int = 300,\n                       checkpoint_dir: str = \"./checkpoints\", max_checkpoints: int = 10) -&gt; 'FlowAgentBuilder':\n    \"\"\"Configure checkpointing\"\"\"\n    self.config.checkpoint.enabled = enabled\n    self.config.checkpoint.interval_seconds = interval_seconds\n    self.config.checkpoint.checkpoint_dir = checkpoint_dir\n    self.config.checkpoint.max_checkpoints = max_checkpoints\n\n    if enabled:\n        # Ensure checkpoint directory exists\n        Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n        logger.info(f\"Checkpointing enabled: {checkpoint_dir} (every {interval_seconds}s)\")\n\n    return self\n</code></pre> <code>with_creative_persona(name='Creative Assistant')</code> \u00b6 <p>Add and set a pre-built creative persona</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_creative_persona(self, name: str = \"Creative Assistant\") -&gt; 'FlowAgentBuilder':\n    \"\"\"Add and set a pre-built creative persona\"\"\"\n    return (self\n            .add_persona_profile(\n        \"creative\",\n        name=name,\n        style=\"creative\",\n        tone=\"inspiring\",\n        personality_traits=[\"imaginative\", \"expressive\", \"innovative\", \"engaging\"],\n        custom_instructions=\"Think outside the box and provide creative, inspiring solutions.\",\n        response_format=\"md-text\",\n        text_length=\"detailed-indepth\"\n    )\n            .set_active_persona(\"creative\"))\n</code></pre> <code>with_custom_variables(variables)</code> \u00b6 <p>Add custom variables</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_custom_variables(self, variables: Dict[str, Any]) -&gt; 'FlowAgentBuilder':\n    \"\"\"Add custom variables\"\"\"\n    self.config.custom_variables.update(variables)\n    return self\n</code></pre> <code>with_developer_persona(name='Senior Developer')</code> \u00b6 <p>Add and set a pre-built developer persona</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_developer_persona(self, name: str = \"Senior Developer\") -&gt; 'FlowAgentBuilder':\n    \"\"\"Add and set a pre-built developer persona\"\"\"\n    return (self\n            .add_persona_profile(\n        \"developer\",\n        name=name,\n        style=\"technical\",\n        tone=\"professional\",\n        personality_traits=[\"precise\", \"thorough\", \"security_conscious\", \"best_practices\"],\n        custom_instructions=\"Focus on code quality, maintainability, and security. Always consider edge cases.\",\n        response_format=\"code-structure\",\n        text_length=\"detailed-indepth\"\n    )\n            .set_active_persona(\"developer\"))\n</code></pre> <code>with_executive_persona(name='Executive Assistant')</code> \u00b6 <p>Add and set a pre-built executive persona</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_executive_persona(self, name: str = \"Executive Assistant\") -&gt; 'FlowAgentBuilder':\n    \"\"\"Add and set a pre-built executive persona\"\"\"\n    return (self\n            .add_persona_profile(\n        \"executive\",\n        name=name,\n        style=\"professional\",\n        tone=\"authoritative\",\n        personality_traits=[\"strategic\", \"decisive\", \"results_oriented\", \"efficient\"],\n        custom_instructions=\"Provide strategic insights with executive-level clarity and focus on outcomes.\",\n        response_format=\"with-bullet-points\",\n        text_length=\"table-conversation\"\n    )\n            .set_active_persona(\"executive\"))\n</code></pre> <code>with_models(fast_model, complex_model=None)</code> \u00b6 <p>Set LLM models</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_models(self, fast_model: str, complex_model: str = None) -&gt; 'FlowAgentBuilder':\n    \"\"\"Set LLM models\"\"\"\n    self.config.fast_llm_model = fast_model\n    if complex_model:\n        self.config.complex_llm_model = complex_model\n    return self\n</code></pre> <code>with_name(name)</code> \u00b6 <p>Set agent name</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_name(self, name: str) -&gt; 'FlowAgentBuilder':\n    \"\"\"Set agent name\"\"\"\n    self.config.name = name\n    return self\n</code></pre> <code>with_system_message(message)</code> \u00b6 <p>Set system message</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_system_message(self, message: str) -&gt; 'FlowAgentBuilder':\n    \"\"\"Set system message\"\"\"\n    self.config.system_message = message\n    return self\n</code></pre> <code>with_temperature(temp)</code> \u00b6 <p>Set temperature</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_temperature(self, temp: float) -&gt; 'FlowAgentBuilder':\n    \"\"\"Set temperature\"\"\"\n    self.config.temperature = temp\n    return self\n</code></pre> <code>with_world_model(world_model)</code> \u00b6 <p>Set initial world model</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>def with_world_model(self, world_model: Dict[str, Any]) -&gt; 'FlowAgentBuilder':\n    \"\"\"Set initial world model\"\"\"\n    self.config.initial_world_model.update(world_model)\n    return self\n</code></pre> <code>MCPConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>MCP server and tools configuration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>class MCPConfig(BaseModel):\n    \"\"\"MCP server and tools configuration\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    enabled: bool = False\n    config_path: Optional[str] = None  # Path to MCP tools config file\n    server_name: Optional[str] = None\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    auto_expose_tools: bool = True\n    tools_from_config: List[Dict[str, Any]] = Field(default_factory=list)\n</code></pre> <code>TelemetryConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>OpenTelemetry configuration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>class TelemetryConfig(BaseModel):\n    \"\"\"OpenTelemetry configuration\"\"\"\n    enabled: bool = False\n    service_name: Optional[str] = None\n    endpoint: Optional[str] = None  # OTLP endpoint\n    console_export: bool = True\n    batch_export: bool = True\n    sample_rate: float = 1.0\n</code></pre> <code>example_production_usage()</code> <code>async</code> \u00b6 <p>Production usage example with full features</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>async def example_production_usage():\n    \"\"\"Production usage example with full features\"\"\"\n\n    logger.info(\"=== Production FlowAgent Builder Example ===\")\n\n    # Example 1: Developer agent with full MCP integration\n    logger.info(\"Creating developer agent with MCP integration...\")\n\n    # Add a custom tool\n    def get_system_info():\n        \"\"\"Get basic system information\"\"\"\n        import platform\n        return {\n            \"platform\": platform.platform(),\n            \"python_version\": platform.python_version(),\n            \"architecture\": platform.architecture()\n        }\n\n    developer_agent = await (FlowAgentBuilder\n                             .create_developer_agent(\"ProductionDev\", with_mcp=True, with_a2a=True)\n                             .add_tool(get_system_info, \"get_system_info\", \"Get system information\")\n                             .enable_telemetry(console_export=True)\n                             .with_custom_variables({\n        \"project_name\": \"FlowAgent Production\",\n        \"environment\": \"production\"\n    })\n                             .build())\n\n    # Test the developer agent\n    dev_response = await developer_agent.a_run(\n        \"Hello! I'm working on {{ project_name }}. Can you tell me about the system and create a simple Python function?\"\n    )\n    logger.info(f\"Developer agent response: {dev_response[:200]}...\")\n\n    # Example 2: Load from configuration file\n    logger.info(\"\\nTesting configuration save/load...\")\n\n    # Save current config\n    config_path = \"/tmp/production_agent_config.yaml\"\n    builder = FlowAgentBuilder.create_analyst_agent(\"ConfigTestAgent\")\n    builder.save_config(config_path)\n\n    # Load from config\n    loaded_builder = FlowAgentBuilder.from_config_file(config_path)\n    config_agent = await loaded_builder.build()\n\n    config_response = await config_agent.a_run(\"Analyze this data: [1, 2, 3, 4, 5]\")\n    logger.info(f\"Config-loaded agent response: {config_response[:150]}...\")\n\n    # Example 3: Agent with MCP tools from config\n    logger.info(\"\\nTesting MCP tools integration...\")\n\n    # Create a sample MCP config\n    mcp_config = {\n        \"tools\": [\n            {\n                \"name\": \"weather_checker\",\n                \"description\": \"Check weather for a location\",\n                \"function_code\": '''\nasync def weather_checker(location: str) -&gt; str:\n    \"\"\"Mock weather checker\"\"\"\n    import random\n    conditions = [\"sunny\", \"cloudy\", \"rainy\", \"snowy\"]\n    temp = random.randint(-10, 35)\n    condition = random.choice(conditions)\n    return f\"Weather in {location}: {condition}, {temp}\u00b0C\"\n'''\n            }\n        ]\n    }\n\n    mcp_config_path = \"/tmp/mcp_tools_config.json\"\n    with open(mcp_config_path, 'w') as f:\n        json.dump(mcp_config, f, indent=2)\n\n    mcp_agent = await (FlowAgentBuilder()\n                       .with_name(\"MCPTestAgent\")\n                       .with_assistant_persona()\n                       .enable_mcp_server(port=8002)\n                       .load_mcp_tools_from_config(mcp_config_path)\n                       .build())\n\n    mcp_response = await mcp_agent.a_run(\"What's the weather like in Berlin?\")\n    logger.info(f\"MCP agent response: {mcp_response[:150]}...\")\n\n    # Show agent status\n    logger.info(\"\\n=== Agent Status ===\")\n    status = developer_agent.status(pretty_print=False)\n    logger.info(f\"Developer agent tools: {len(status['capabilities']['tool_names'])}\")\n    logger.info(f\"MCP agent tools: {len(mcp_agent.shared.get('available_tools', []))}\")\n\n    # Cleanup\n    await developer_agent.close()\n    await config_agent.close()\n    await mcp_agent.close()\n\n    logger.info(\"Production example completed successfully!\")\n</code></pre> <code>example_quick_start()</code> <code>async</code> \u00b6 <p>Quick start examples for common scenarios</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/builder.py</code> <pre><code>async def example_quick_start():\n    \"\"\"Quick start examples for common scenarios\"\"\"\n\n    logger.info(\"=== Quick Start Examples ===\")\n\n    # 1. Simple developer agent\n    dev_agent = await FlowAgentBuilder.create_developer_agent(\"QuickDev\").build()\n    response1 = await dev_agent.a_run(\"Create a Python function to validate email addresses\")\n    logger.info(f\"Quick dev response: {response1[:100]}...\")\n    await dev_agent.close()\n\n    # 2. Analyst with custom data\n    analyst_agent = await (FlowAgentBuilder\n                           .create_analyst_agent(\"QuickAnalyst\")\n                           .with_custom_variables({\"dataset\": \"sales_data_2024\"})\n                           .build())\n    response2 = await analyst_agent.a_run(\"Analyze the trends in {{ dataset }}\")\n    logger.info(f\"Quick analyst response: {response2[:100]}...\")\n    await analyst_agent.close()\n\n    # 3. Creative assistant\n    creative_agent = await FlowAgentBuilder.create_creative_agent(\"QuickCreative\").build()\n    response3 = await creative_agent.a_run(\"Write a creative story about AI agents collaborating\")\n    logger.info(f\"Quick creative response: {response3[:100]}...\")\n    await creative_agent.close()\n\n    logger.info(\"Quick start examples completed!\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent.config","title":"<code>config</code>","text":"<code>A2AConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Configuration for A2A integration.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>class A2AConfig(BaseModel):\n    \"\"\"Configuration for A2A integration.\"\"\"\n    server: dict[str, Any] | None = Field(default=None, description=\"Configuration to run an A2A server (host, port, etc.).\")\n    known_agents: dict[str, str] = Field(default_factory=dict, description=\"Named A2A agent URLs to interact with (e.g., {'weather_agent': 'http://weather:5000'}).\")\n    default_task_timeout: int = Field(default=120, description=\"Default timeout in seconds for waiting on A2A task results.\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre> <code>ADKConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Configuration for ADK integration.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>class ADKConfig(BaseModel):\n    \"\"\"Configuration for ADK integration.\"\"\"\n    enabled: bool = Field(default=True, description=\"Enable ADK features if ADK is installed.\")\n    description: str | None = Field(default=None, description=\"ADK LlmAgent description.\")\n    instruction_override: str | None = Field(default=None, description=\"Override agent's system message for ADK.\")\n    # Tools added via builder or auto-discovery\n    code_executor: str | BaseCodeExecutor | None = Field(default=None, description=\"Reference name or instance of ADK code executor.\")\n    planner: str | BasePlanner | None = Field(default=None, description=\"Reference name or instance of ADK planner.\")\n    examples: list[Example] | None = Field(default=None, description=\"Few-shot examples for ADK.\")\n    output_schema: type[BaseModel] | None = Field(default=None, description=\"Pydantic model for structured output.\")\n    # MCP Toolset config handled separately if ADK is enabled\n    use_mcp_toolset: bool = Field(default=True, description=\"Use ADK's MCPToolset for MCP client connections if ADK is enabled.\")\n    # Runner config handled separately\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre> <code>AgentConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Main configuration schema for an EnhancedAgent.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>class AgentConfig(BaseModel):\n    \"\"\"Main configuration schema for an EnhancedAgent.\"\"\"\n    agent_name: str = Field(..., description=\"Unique name for this agent instance.\")\n    version: str = Field(default=\"0.1.0\")\n\n    agent_instruction: str = Field(default=\"You are a helpful AI assistant. Answer user questions to the best of your knowledge. Respond concisely. use tools when needed\")\n    agent_description: str = Field(default=\"An configurable, production-ready agent with integrated capabilities.\")\n\n    # Model Selection\n    models: list[ModelConfig] = Field(..., description=\"List of available LLM configurations.\")\n    default_llm_model: str = Field(..., description=\"Name of the ModelConfig to use for general LLM calls.\")\n    formatter_llm_model: str | None = Field(default=None, description=\"Optional: Name of a faster/cheaper ModelConfig for a_format_class calls.\")\n\n    # Core Agent Settings\n    world_model_initial_data: dict[str, Any] | None = Field(default=None)\n    enable_streaming: bool = Field(default=False)\n    verbose: bool = Field(default=False)\n    log_level: str = Field(default=\"INFO\", description=\"Logging level (DEBUG, INFO, WARNING, ERROR).\")\n    max_history_length: int = Field(default=20, description=\"Max conversation turns for LiteLLM history.\")\n    trim_strategy: Literal[\"litellm\", \"basic\"] = Field(default=\"litellm\")\n    persist_history: bool = Field(default=True, description=\"Persist conversation history (requires persistent ChatSession).\")\n    user_id_default: str | None = Field(default=None, description=\"Default user ID for interactions.\")\n\n    # Secure Code Execution\n    code_executor_type: Literal[\"restricted\", \"docker\", \"none\"] = Field(default=\"restricted\", description=\"Type of code executor to use.\")\n    code_executor_config: dict[str, Any] = Field(default_factory=dict, description=\"Configuration specific to the chosen code executor.\")\n    enable_adk_code_execution_tool: bool = Field(default=True, description=\"Expose code execution as an ADK tool if ADK is enabled.\")\n\n    # Framework Integrations\n    adk: ADKConfig | None = Field(default_factory=ADKConfig if ADK_AVAILABLE_CONF else lambda: None)\n    mcp: MCPConfig | None = Field(default_factory=MCPConfig if MCP_AVAILABLE_CONF else lambda: None)\n    a2a: A2AConfig | None = Field(default_factory=A2AConfig if A2A_AVAILABLE_CONF else lambda: None)\n\n    # Observability &amp; Cost\n    observability: ObservabilityConfig | None = Field(default_factory=ObservabilityConfig)\n    budget_manager: BudgetManager | None = Field(default=None, description=\"Global LiteLLM budget manager instance.\") # Needs to be passed in\n\n    # Human-in-the-Loop\n    enable_hitl: bool = Field(default=False, description=\"Enable basic Human-in-the-Loop hooks.\")\n\n    # Add other global settings as needed\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode='after')\n    def validate_model_references(self) -&gt; 'AgentConfig':\n        model_names = {m.name for m in self.models}\n        if self.default_llm_model not in model_names:\n            raise ValueError(f\"default_llm_model '{self.default_llm_model}' not found in defined models.\")\n        if self.formatter_llm_model and self.formatter_llm_model not in model_names:\n            raise ValueError(f\"formatter_llm_model '{self.formatter_llm_model}' not found in defined models.\")\n        return self\n\n    @model_validator(mode='after')\n    def validate_framework_availability(self) -&gt; 'AgentConfig':\n        if self.adk and self.adk.enabled and not ADK_AVAILABLE_CONF:\n            logger.warning(\"ADK configuration provided but ADK library not installed. Disabling ADK features.\")\n            self.adk.enabled = False\n        if self.mcp and (self.mcp.server or self.mcp.client_connections) and not MCP_AVAILABLE_CONF:\n             logger.warning(\"MCP configuration provided but MCP library not installed. Disabling MCP features.\")\n             self.mcp = None # Or disable specific parts\n        if self.a2a and (self.a2a.server or self.a2a.known_agents) and not A2A_AVAILABLE_CONF:\n             logger.warning(\"A2A configuration provided but A2A library not installed. Disabling A2A features.\")\n             self.a2a = None # Or disable specific parts\n        return self\n\n    @classmethod\n    def load_from_yaml(cls, path: str | Path) -&gt; 'AgentConfig':\n        \"\"\"Loads configuration from a YAML file.\"\"\"\n        file_path = Path(path)\n        if not file_path.is_file():\n            raise FileNotFoundError(f\"Configuration file not found: {path}\")\n        with open(file_path) as f:\n            config_data = yaml.safe_load(f)\n        logger.info(f\"Loaded agent configuration from {path}\")\n        return cls(**config_data)\n\n    def save_to_yaml(self, path: str | Path):\n        \"\"\"Saves the current configuration to a YAML file.\"\"\"\n        file_path = Path(path)\n        file_path.parent.mkdir(parents=True, exist_ok=True)\n        with open(file_path, 'w') as f:\n            # Use Pydantic's model_dump for clean serialization\n            yaml.dump(self.model_dump(mode='python'), f, sort_keys=False)\n        logger.info(f\"Saved agent configuration to {path}\")\n</code></pre> <code>load_from_yaml(path)</code> <code>classmethod</code> \u00b6 <p>Loads configuration from a YAML file.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>@classmethod\ndef load_from_yaml(cls, path: str | Path) -&gt; 'AgentConfig':\n    \"\"\"Loads configuration from a YAML file.\"\"\"\n    file_path = Path(path)\n    if not file_path.is_file():\n        raise FileNotFoundError(f\"Configuration file not found: {path}\")\n    with open(file_path) as f:\n        config_data = yaml.safe_load(f)\n    logger.info(f\"Loaded agent configuration from {path}\")\n    return cls(**config_data)\n</code></pre> <code>save_to_yaml(path)</code> \u00b6 <p>Saves the current configuration to a YAML file.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>def save_to_yaml(self, path: str | Path):\n    \"\"\"Saves the current configuration to a YAML file.\"\"\"\n    file_path = Path(path)\n    file_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(file_path, 'w') as f:\n        # Use Pydantic's model_dump for clean serialization\n        yaml.dump(self.model_dump(mode='python'), f, sort_keys=False)\n    logger.info(f\"Saved agent configuration to {path}\")\n</code></pre> <code>MCPConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Configuration for MCP integration.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>class MCPConfig(BaseModel):\n    \"\"\"Configuration for MCP integration.\"\"\"\n    server: dict[str, Any] | None = Field(default=None, description=\"Configuration to run an MCP server (host, port, etc.).\")\n    client_connections: dict[str, str] = Field(default_factory=dict, description=\"Named MCP server URLs to connect to as a client (e.g., {'files': 'stdio:npx @mcp/server-filesystem /data'}).\")\n    # ADK's MCPToolset handles client connections if ADKConfig.use_mcp_toolset is True\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre> <code>ModelConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Configuration specific to an LLM model via LiteLLM.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>class ModelConfig(BaseModel):\n    \"\"\"Configuration specific to an LLM model via LiteLLM.\"\"\"\n    # Used as key for model selection\n    name: str = Field(..., description=\"Unique identifier/alias for this model configuration (e.g., 'fast_formatter', 'main_reasoner').\")\n    model: str = Field(..., description=\"LiteLLM model string (e.g., 'gemini/gemini-1.5-pro-latest', 'ollama/mistral').\")\n    provider: str | None = Field(default=None, description=\"LiteLLM provider override if needed.\")\n    api_key: str | None = Field(default=None, description=\"API Key (consider using environment variables).\")\n    api_base: str | None = Field(default=None, description=\"API Base URL (for local models, proxies).\")\n    api_version: str | None = Field(default=None, description=\"API Version (e.g., for Azure).\")\n\n    # Common LLM Parameters\n    temperature: float | None = Field(default=0.7)\n    top_p: float | None = Field(default=None)\n    top_k: int | None = Field(default=None)\n    max_tokens: int | None = Field(default=2048, description=\"Max tokens for generation.\")\n    max_input_tokens: int | None = Field(default=None, description=\"Max input context window (autodetected if None).\")\n    stop_sequence: list[str] | None = Field(default=None)\n    presence_penalty: float | None = Field(default=None)\n    frequency_penalty: float | None = Field(default=None)\n    system_message: str | None = Field(default=None, description=\"Default system message for this model.\")\n\n    # LiteLLM Specific\n    caching: bool = Field(default=True, description=\"Enable LiteLLM caching for this model.\")\n    # budget_manager: Optional[BudgetManager] = Field(default=None) # Budget manager applied globally or per-agent\n\n    model_config = ConfigDict(arbitrary_types_allowed=True, extra='allow') # Allow extra LiteLLM params\n</code></pre> <code>ObservabilityConfig</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Configuration for observability (OpenTelemetry).</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/config.py</code> <pre><code>class ObservabilityConfig(BaseModel):\n    \"\"\"Configuration for observability (OpenTelemetry).\"\"\"\n    enabled: bool = Field(default=True)\n    endpoint: str | None = Field(default=None, description=\"OTLP endpoint URL (e.g., http://jaeger:4317).\")\n    service_name: str | None = Field(default=None, description=\"Service name for traces/metrics (defaults to agent name).\")\n    # Add more OTel config options as needed (headers, certs, resource attributes)\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent.executors","title":"<code>executors</code>","text":"<code>DockerCodeExecutor</code> \u00b6 <p>               Bases: <code>_BaseExecutorClass</code></p> <p>Executes Python code in a sandboxed Docker container.</p> <p>Requires Docker to be installed and running, and the 'docker' Python SDK.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/executors.py</code> <pre><code>class DockerCodeExecutor(_BaseExecutorClass):\n    \"\"\"\n    Executes Python code in a sandboxed Docker container.\n\n    Requires Docker to be installed and running, and the 'docker' Python SDK.\n    \"\"\"\n    DEFAULT_DOCKER_IMAGE = \"python:3.10-slim\" # Use a minimal image\n    DEFAULT_TIMEOUT = 10 # Seconds\n    DEFAULT_MEM_LIMIT = \"128m\"\n    DEFAULT_CPUS = 0.5\n\n    def __init__(self,\n                 docker_image: str = DEFAULT_DOCKER_IMAGE,\n                 timeout: int = DEFAULT_TIMEOUT,\n                 mem_limit: str = DEFAULT_MEM_LIMIT,\n                 cpus: float = DEFAULT_CPUS,\n                 network_mode: str = \"none\", # Disable networking by default for security\n                 docker_client_config: dict | None = None):\n        if not DOCKER_AVAILABLE:\n            raise ImportError(\"Docker SDK not installed ('pip install docker'). Cannot use DockerCodeExecutor.\")\n\n        self.docker_image = docker_image\n        self.timeout = timeout\n        self.mem_limit = mem_limit\n        self.cpus = cpus\n        self.network_mode = network_mode\n        try:\n            self.client = docker.from_env(**(docker_client_config or {}))\n            self.client.ping() # Check connection\n            # Ensure image exists locally or pull it\n            try:\n                self.client.images.get(self.docker_image)\n                logger.info(f\"Docker image '{self.docker_image}' found locally.\")\n            except ImageNotFound:\n                logger.warning(f\"Docker image '{self.docker_image}' not found locally. Attempting to pull...\")\n                try:\n                    self.client.images.pull(self.docker_image)\n                    logger.info(f\"Successfully pulled Docker image '{self.docker_image}'.\")\n                except APIError as pull_err:\n                    raise RuntimeError(f\"Failed to pull Docker image '{self.docker_image}': {pull_err}\") from pull_err\n        except Exception as e:\n            raise RuntimeError(f\"Failed to connect to Docker daemon: {e}. Is Docker running?\") from e\n        logger.info(f\"DockerCodeExecutor initialized (Image: {docker_image}, Timeout: {timeout}s, Network: {network_mode})\")\n\n    def _execute(self, code: str) -&gt; dict[str, Any]:\n        \"\"\"Internal execution logic.\"\"\"\n        result = {\"stdout\": \"\", \"stderr\": \"\", \"error\": None, \"exit_code\": None}\n        container = None\n\n        try:\n            logger.debug(f\"Creating Docker container from image '{self.docker_image}'...\")\n            container = self.client.containers.run(\n                image=self.docker_image,\n                command=[\"python\", \"-c\", code],\n                detach=True,\n                mem_limit=self.mem_limit,\n                nano_cpus=int(self.cpus * 1e9),\n                network_mode=self.network_mode,\n                # Security considerations: Consider read-only filesystem, dropping capabilities\n                read_only=True,\n                # working_dir=\"/app\", # Define a working dir if needed\n                # volumes={...} # Mount volumes carefully if required\n            )\n            logger.debug(f\"Container '{container.short_id}' started.\")\n\n            # Wait for container completion with timeout\n            container_result = container.wait(timeout=self.timeout)\n            result[\"exit_code\"] = container_result.get(\"StatusCode\", None)\n\n            # Retrieve logs\n            result[\"stdout\"] = container.logs(stdout=True, stderr=False).decode('utf-8', errors='replace').strip()\n            result[\"stderr\"] = container.logs(stdout=False, stderr=True).decode('utf-8', errors='replace').strip()\n\n            logger.debug(f\"Container '{container.short_id}' finished with exit code {result['exit_code']}.\")\n            if result[\"exit_code\"] != 0:\n                 logger.warning(f\"Container stderr: {result['stderr'][:500]}...\") # Log stderr on failure\n\n        except ContainerError as e:\n            result[\"error\"] = f\"ContainerError: {e}\"\n            result[\"stderr\"] = e.stderr.decode('utf-8', errors='replace').strip() if e.stderr else str(e)\n            result[\"exit_code\"] = e.exit_status\n            logger.error(f\"Container '{container.short_id if container else 'N/A'}' failed: {result['error']}\\nStderr: {result['stderr']}\")\n        except APIError as e:\n            result[\"error\"] = f\"Docker APIError: {e}\"\n            result[\"exit_code\"] = -1\n            logger.error(f\"Docker API error during execution: {e}\")\n        except Exception as e:\n            # Catch potential timeout errors from container.wait or other unexpected issues\n            result[\"error\"] = f\"Unexpected execution error: {type(e).__name__}: {e}\"\n            result[\"exit_code\"] = -1\n            # Check if it looks like a timeout\n            if isinstance(e, TimeoutError) or \"Timeout\" in str(e): # docker SDK might raise requests.exceptions.ReadTimeout\n                result[\"stderr\"] = f\"Execution timed out after {self.timeout} seconds.\"\n                logger.warning(f\"Container execution timed out ({self.timeout}s).\")\n            else:\n                logger.error(f\"Unexpected error during Docker execution: {e}\", exc_info=True)\n        finally:\n            if container:\n                try:\n                    logger.debug(f\"Removing container '{container.short_id}'...\")\n                    container.remove(force=True)\n                except APIError as rm_err:\n                    logger.warning(f\"Failed to remove container {container.short_id}: {rm_err}\")\n\n        return result\n\n     # --- ADK Compatibility Method ---\n    if ADK_EXEC_AVAILABLE:\n        def execute_code(self, invocation_context: InvocationContext, code_input: CodeExecutionInput) -&gt; CodeExecutionResult:\n            logger.debug(f\"DockerCodeExecutor executing ADK request (lang: {code_input.language}). Code: {code_input.code[:100]}...\")\n            if code_input.language.lower() != 'python':\n                 return CodeExecutionResult(output=f\"Error: Unsupported language '{code_input.language}'. Only Python is supported.\", outcome=\"OUTCOME_FAILURE\")\n\n            exec_result = self._execute(code_input.code)\n\n            output_str = \"\"\n            if exec_result[\"stdout\"]:\n                output_str += f\"Stdout:\\n{exec_result['stdout']}\\n\"\n            if exec_result[\"stderr\"]:\n                 output_str += f\"Stderr:\\n{exec_result['stderr']}\\n\"\n            if not output_str and exec_result[\"exit_code\"] == 0:\n                 output_str = \"Execution successful with no output.\"\n            elif not output_str and exec_result[\"exit_code\"] != 0:\n                 output_str = f\"Execution failed with no output (Exit code: {exec_result['exit_code']}). Error: {exec_result['error']}\"\n\n            outcome = \"OUTCOME_SUCCESS\" if exec_result[\"exit_code\"] == 0 else \"OUTCOME_FAILURE\"\n\n            return CodeExecutionResult(output=output_str.strip(), outcome=outcome)\n    # --- End ADK Compatibility ---\n\n    # --- Direct Call Method ---\n    def execute(self, code: str) -&gt; dict[str, Any]:\n        \"\"\"Directly execute code, returning detailed dictionary.\"\"\"\n        logger.debug(f\"DockerCodeExecutor executing direct call. Code: {code[:100]}...\")\n        return self._execute(code)\n</code></pre> <code>execute(code)</code> \u00b6 <p>Directly execute code, returning detailed dictionary.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/executors.py</code> <pre><code>def execute(self, code: str) -&gt; dict[str, Any]:\n    \"\"\"Directly execute code, returning detailed dictionary.\"\"\"\n    logger.debug(f\"DockerCodeExecutor executing direct call. Code: {code[:100]}...\")\n    return self._execute(code)\n</code></pre> <code>RestrictedPythonExecutor</code> \u00b6 <p>               Bases: <code>_BaseExecutorClass</code></p> <p>Executes Python code using restrictedpython.</p> <p>Safer than exec() but NOT a full sandbox. Known vulnerabilities exist. Use with extreme caution and only with trusted code sources or for low-risk operations. Docker is strongly recommended for untrusted code.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/executors.py</code> <pre><code>class RestrictedPythonExecutor(_BaseExecutorClass):\n    \"\"\"\n    Executes Python code using restrictedpython.\n\n    Safer than exec() but NOT a full sandbox. Known vulnerabilities exist.\n    Use with extreme caution and only with trusted code sources or for\n    low-risk operations. Docker is strongly recommended for untrusted code.\n    \"\"\"\n    DEFAULT_ALLOWED_GLOBALS = {\n        **safe_globals,\n        '_print_': restrictedpython.PrintCollector,\n        '_getattr_': restrictedpython.safe_getattr,\n        '_getitem_': restrictedpython.safe_getitem,\n        '_write_': restrictedpython.guarded_setattr, # Allows modifying specific safe objects if needed\n        # Add other safe builtins or modules carefully\n        'math': __import__('math'),\n        'random': __import__('random'),\n        'datetime': __import__('datetime'),\n        'time': __import__('time'),\n        # 'requests': None, # Example: Explicitly disallow\n    }\n\n    def __init__(self, allowed_globals: dict | None = None, max_execution_time: int = 5):\n        if not RESTRICTEDPYTHON_AVAILABLE:\n            raise ImportError(\"restrictedpython is not installed. Cannot use RestrictedPythonExecutor.\")\n        self.allowed_globals = allowed_globals or self.DEFAULT_ALLOWED_GLOBALS\n        self.max_execution_time = max_execution_time # Basic timeout (not perfectly enforced by restrictedpython)\n        logger.warning(\"Initialized RestrictedPythonExecutor. This provides LIMITED sandboxing. Use Docker for untrusted code.\")\n\n    def _execute(self, code: str) -&gt; dict[str, Any]:\n        \"\"\"Internal execution logic.\"\"\"\n        start_time = time.monotonic()\n        result = {\"stdout\": \"\", \"stderr\": \"\", \"error\": None, \"exit_code\": None}\n        local_vars = {}\n        stdout_capture = io.StringIO()\n        stderr_capture = io.StringIO()\n\n        try:\n            # Basic timeout check (not preemptive)\n            if time.monotonic() - start_time &gt; self.max_execution_time:\n                 raise TimeoutError(f\"Execution exceeded max time of {self.max_execution_time}s (pre-check).\")\n\n            # Compile the code in restricted mode\n            byte_code = compile_restricted(code, filename='&lt;inline code&gt;', mode='exec')\n\n            # Add a print collector to capture output\n            self.allowed_globals['_print_'] = restrictedpython.PrintCollector\n            print_collector = self.allowed_globals['_print_']()\n            exec_globals = {**self.allowed_globals, '_print': print_collector}\n\n            # Execute the compiled code\n            # Note: restrictedpython does not inherently support robust timeouts during exec\n            exec(byte_code, exec_globals, local_vars)\n\n            # Check execution time again\n            duration = time.monotonic() - start_time\n            if duration &gt; self.max_execution_time:\n                logger.warning(f\"Execution finished but exceeded max time ({duration:.2f}s &gt; {self.max_execution_time}s).\")\n                # Potentially treat as an error or partial success\n\n            result[\"stdout\"] = print_collector.printed_text # Access collected prints\n            result[\"exit_code\"] = 0 # Assume success if no exception\n\n        except TimeoutError as e:\n            result[\"stderr\"] = f\"TimeoutError: {e}\"\n            result[\"error\"] = str(e)\n            result[\"exit_code\"] = -1 # Indicate timeout\n        except SyntaxError as e:\n            result[\"stderr\"] = f\"SyntaxError: {e}\"\n            result[\"error\"] = str(e)\n            result[\"exit_code\"] = 1\n        except Exception as e:\n            # Capture other potential execution errors allowed by restrictedpython\n            error_type = type(e).__name__\n            error_msg = f\"{error_type}: {e}\"\n            result[\"stderr\"] = error_msg\n            result[\"error\"] = str(e)\n            result[\"exit_code\"] = 1\n            logger.warning(f\"RestrictedPython execution caught exception: {error_msg}\", exc_info=False) # Avoid logging potentially sensitive details from code\n        finally:\n            stdout_capture.close() # Not used directly with PrintCollector\n            stderr_capture.close()\n\n        return result\n\n    # --- ADK Compatibility Method ---\n    if ADK_EXEC_AVAILABLE:\n        def execute_code(self, invocation_context: InvocationContext, code_input: CodeExecutionInput) -&gt; CodeExecutionResult:\n            logger.debug(f\"RestrictedPythonExecutor executing ADK request (lang: {code_input.language}). Code: {code_input.code[:100]}...\")\n            if code_input.language.lower() != 'python':\n                 return CodeExecutionResult(output=f\"Error: Unsupported language '{code_input.language}'. Only Python is supported.\", outcome=\"OUTCOME_FAILURE\")\n\n            exec_result = self._execute(code_input.code)\n\n            output_str = \"\"\n            if exec_result[\"stdout\"]:\n                output_str += f\"Stdout:\\n{exec_result['stdout']}\\n\"\n            if exec_result[\"stderr\"]:\n                 output_str += f\"Stderr:\\n{exec_result['stderr']}\\n\"\n            if not output_str and exec_result[\"exit_code\"] == 0:\n                 output_str = \"Execution successful with no output.\"\n            elif not output_str and exec_result[\"exit_code\"] != 0:\n                 output_str = f\"Execution failed with no output (Exit code: {exec_result['exit_code']}). Error: {exec_result['error']}\"\n\n\n            outcome = \"OUTCOME_SUCCESS\" if exec_result[\"exit_code\"] == 0 else \"OUTCOME_FAILURE\"\n\n            return CodeExecutionResult(output=output_str.strip(), outcome=outcome)\n    # --- End ADK Compatibility ---\n\n    # --- Direct Call Method ---\n    def execute(self, code: str) -&gt; dict[str, Any]:\n        \"\"\"Directly execute code, returning detailed dictionary.\"\"\"\n        logger.debug(f\"RestrictedPythonExecutor executing direct call. Code: {code[:100]}...\")\n        return self._execute(code)\n</code></pre> <code>execute(code)</code> \u00b6 <p>Directly execute code, returning detailed dictionary.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/executors.py</code> <pre><code>def execute(self, code: str) -&gt; dict[str, Any]:\n    \"\"\"Directly execute code, returning detailed dictionary.\"\"\"\n    logger.debug(f\"RestrictedPythonExecutor executing direct call. Code: {code[:100]}...\")\n    return self._execute(code)\n</code></pre> <code>get_code_executor(config)</code> \u00b6 <p>Creates a code executor instance based on configuration.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/executors.py</code> <pre><code>def get_code_executor(config: 'AgentConfig') -&gt; RestrictedPythonExecutor | DockerCodeExecutor | BaseCodeExecutor | None:\n    \"\"\"Creates a code executor instance based on configuration.\"\"\"\n    executor_type = config.code_executor_type\n    executor_config = config.code_executor_config or {}\n\n    if executor_type == \"restricted\":\n        if not RESTRICTEDPYTHON_AVAILABLE:\n            logger.error(\"RestrictedPython executor configured but library not installed. Code execution disabled.\")\n            return None\n        return RestrictedPythonExecutor(**executor_config)\n    elif executor_type == \"docker\":\n        if not DOCKER_AVAILABLE:\n            logger.error(\"Docker executor configured but library not installed or Docker not running. Code execution disabled.\")\n            return None\n        try:\n            return DockerCodeExecutor(**executor_config)\n        except Exception as e:\n            logger.error(f\"Failed to initialize DockerCodeExecutor: {e}. Code execution disabled.\")\n            return None\n    elif executor_type == \"none\":\n        logger.info(\"Code execution explicitly disabled in configuration.\")\n        return None\n    elif executor_type and ADK_EXEC_AVAILABLE and isinstance(executor_type, BaseCodeExecutor):\n        # Allow passing a pre-configured ADK executor instance\n        logger.info(f\"Using pre-configured ADK code executor: {type(executor_type).__name__}\")\n        return executor_type\n    else:\n        logger.warning(f\"Unknown or unsupported code_executor_type: '{executor_type}'. Code execution disabled.\")\n        return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent.types","title":"<code>types</code>","text":"<code>AgentModelData</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>class AgentModelData(BaseModel):\n    name: str = \"FlowAgent\"\n    fast_llm_model: str = \"openrouter/anthropic/claude-3-haiku\"\n    complex_llm_model: str = \"openrouter/openai/gpt-4o\"\n    system_message: str = \"You are a production-ready autonomous agent.\"\n    temperature: float = 0.7\n    max_tokens: int = 2048\n    max_input_tokens: int = 32768\n    api_key: Optional[str] = None\n    api_base: Optional[str] = None\n    budget_manager: Optional[Any] = None\n    caching: bool = True\n    persona: Optional[PersonaConfig] = None\n    use_fast_response: bool = True\n\n    def get_system_message_with_persona(self) -&gt; str:\n        \"\"\"Get system message with persona integration\"\"\"\n        base_message = self.system_message\n\n        if self.persona and self.persona.apply_method in [\"system_prompt\", \"both\"]:\n            persona_addition = self.persona.to_system_prompt_addition()\n            if persona_addition:\n                base_message += f\"\\n\\n## Persona Instructions\\n{persona_addition}\"\n\n        return base_message\n</code></pre> <code>get_system_message_with_persona()</code> \u00b6 <p>Get system message with persona integration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def get_system_message_with_persona(self) -&gt; str:\n    \"\"\"Get system message with persona integration\"\"\"\n    base_message = self.system_message\n\n    if self.persona and self.persona.apply_method in [\"system_prompt\", \"both\"]:\n        persona_addition = self.persona.to_system_prompt_addition()\n        if persona_addition:\n            base_message += f\"\\n\\n## Persona Instructions\\n{persona_addition}\"\n\n    return base_message\n</code></pre> <code>CompoundTask</code> <code>dataclass</code> \u00b6 <p>               Bases: <code>Task</code></p> <p>Task der Sub-Tasks gruppiert</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass CompoundTask(Task):\n    \"\"\"Task der Sub-Tasks gruppiert\"\"\"\n    sub_task_ids: List[str] = field(default_factory=list)\n    execution_strategy: str = \"sequential\"  # \"sequential\" | \"parallel\"\n    success_criteria: str = \"\"  # Wann ist der Compound-Task erfolgreich?\n</code></pre> <code>DecisionTask</code> <code>dataclass</code> \u00b6 <p>               Bases: <code>Task</code></p> <p>Task f\u00fcr dynamisches Routing</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass DecisionTask(Task):\n    \"\"\"Task f\u00fcr dynamisches Routing\"\"\"\n    decision_prompt: str = \"\"  # Kurze Frage an LLM\n    routing_map: Dict[str, str] = field(default_factory=dict)  # Ergebnis -&gt; n\u00e4chster Task\n    decision_model: str = \"fast\"  # Welches LLM f\u00fcr Entscheidung\n</code></pre> <code>FormatConfig</code> <code>dataclass</code> \u00b6 <p>Konfiguration f\u00fcr Response-Format und -L\u00e4nge</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass FormatConfig:\n    \"\"\"Konfiguration f\u00fcr Response-Format und -L\u00e4nge\"\"\"\n    response_format: ResponseFormat = ResponseFormat.FREI_TEXT\n    text_length: TextLength = TextLength.CHAT_CONVERSATION\n    custom_instructions: str = \"\"\n    strict_format_adherence: bool = True\n    quality_threshold: float = 0.7\n\n    def get_format_instructions(self) -&gt; str:\n        \"\"\"Generiere Format-spezifische Anweisungen\"\"\"\n        format_instructions = {\n            ResponseFormat.FREI_TEXT: \"Verwende nat\u00fcrlichen Flie\u00dftext ohne spezielle Formatierung.\",\n            ResponseFormat.WITH_TABLES: \"Integriere Tabellen zur strukturierten Darstellung von Daten. Verwende Markdown-Tabellen.\",\n            ResponseFormat.WITH_BULLET_POINTS: \"Strukturiere Informationen mit Bullet Points (\u2022, -, *) f\u00fcr bessere Lesbarkeit.\",\n            ResponseFormat.WITH_LISTS: \"Verwende nummerierte und unnummerierte Listen zur Organisation von Inhalten.\",\n            ResponseFormat.TEXT_ONLY: \"Nur reiner Text ohne Formatierung, Symbole oder Strukturelemente.\",\n            ResponseFormat.MD_TEXT: \"Vollst\u00e4ndige Markdown-Formatierung mit Headings, Code-Blocks, Links etc.\",\n            ResponseFormat.YAML_TEXT: \"Strukturiere Antworten als YAML-Format f\u00fcr maschinenlesbare Ausgabe.\",\n            ResponseFormat.JSON_TEXT: \"Formatiere Antworten als JSON-Struktur f\u00fcr API-Integration.\",\n            ResponseFormat.PSEUDO_CODE: \"Verwende Pseudocode-Struktur f\u00fcr algorithmische oder logische Erkl\u00e4rungen.\",\n            ResponseFormat.CODE_STRUCTURE: \"Strukturiere wie Code mit Einr\u00fcckungen, Kommentaren und logischen Bl\u00f6cken.\"\n        }\n        return format_instructions.get(self.response_format, \"Standard-Formatierung.\")\n\n    def get_length_instructions(self) -&gt; str:\n        \"\"\"Generiere L\u00e4ngen-spezifische Anweisungen\"\"\"\n        length_instructions = {\n            TextLength.MINI_CHAT: \"Sehr kurze, pr\u00e4gnante Antworten (1-2 S\u00e4tze, max 50 W\u00f6rter). Chat-Style.\",\n            TextLength.CHAT_CONVERSATION: \"Moderate Gespr\u00e4chsl\u00e4nge (2-4 S\u00e4tze, 50-150 W\u00f6rter). Nat\u00fcrlicher Unterhaltungsstil.\",\n            TextLength.TABLE_CONVERSATION: \"Strukturierte, tabellarische Darstellung mit kompakten Erkl\u00e4rungen (100-250 W\u00f6rter).\",\n            TextLength.DETAILED_INDEPTH: \"Ausf\u00fchrliche, detaillierte Erkl\u00e4rungen (300-800 W\u00f6rter) mit Tiefe und Kontext.\",\n            TextLength.PHD_LEVEL: \"Akademische Tiefe mit umfassenden Erkl\u00e4rungen (800+ W\u00f6rter), Quellenangaben und Fachterminologie.\"\n        }\n        return length_instructions.get(self.text_length, \"Standard-L\u00e4nge.\")\n\n    def get_combined_instructions(self) -&gt; str:\n        \"\"\"Kombiniere Format- und L\u00e4ngen-Anweisungen\"\"\"\n        instructions = []\n        instructions.append(\"## Format-Anforderungen:\")\n        instructions.append(self.get_format_instructions())\n        instructions.append(\"\\n## L\u00e4ngen-Anforderungen:\")\n        instructions.append(self.get_length_instructions())\n\n        if self.custom_instructions:\n            instructions.append(f\"\\n## Zus\u00e4tzliche Anweisungen:\")\n            instructions.append(self.custom_instructions)\n\n        if self.strict_format_adherence:\n            instructions.append(\"\\n## WICHTIG: Halte dich strikt an diese Format- und L\u00e4ngen-Vorgaben!\")\n\n        return \"\\n\".join(instructions)\n\n    def get_expected_word_range(self) -&gt; tuple[int, int]:\n        \"\"\"Erwartete Wortanzahl f\u00fcr Qualit\u00e4tsbewertung\"\"\"\n        ranges = {\n            TextLength.MINI_CHAT: (10, 50),\n            TextLength.CHAT_CONVERSATION: (50, 150),\n            TextLength.TABLE_CONVERSATION: (100, 250),\n            TextLength.DETAILED_INDEPTH: (300, 800),\n            TextLength.PHD_LEVEL: (800, 2000)\n        }\n        return ranges.get(self.text_length, (50, 200))\n</code></pre> <code>get_combined_instructions()</code> \u00b6 <p>Kombiniere Format- und L\u00e4ngen-Anweisungen</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def get_combined_instructions(self) -&gt; str:\n    \"\"\"Kombiniere Format- und L\u00e4ngen-Anweisungen\"\"\"\n    instructions = []\n    instructions.append(\"## Format-Anforderungen:\")\n    instructions.append(self.get_format_instructions())\n    instructions.append(\"\\n## L\u00e4ngen-Anforderungen:\")\n    instructions.append(self.get_length_instructions())\n\n    if self.custom_instructions:\n        instructions.append(f\"\\n## Zus\u00e4tzliche Anweisungen:\")\n        instructions.append(self.custom_instructions)\n\n    if self.strict_format_adherence:\n        instructions.append(\"\\n## WICHTIG: Halte dich strikt an diese Format- und L\u00e4ngen-Vorgaben!\")\n\n    return \"\\n\".join(instructions)\n</code></pre> <code>get_expected_word_range()</code> \u00b6 <p>Erwartete Wortanzahl f\u00fcr Qualit\u00e4tsbewertung</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def get_expected_word_range(self) -&gt; tuple[int, int]:\n    \"\"\"Erwartete Wortanzahl f\u00fcr Qualit\u00e4tsbewertung\"\"\"\n    ranges = {\n        TextLength.MINI_CHAT: (10, 50),\n        TextLength.CHAT_CONVERSATION: (50, 150),\n        TextLength.TABLE_CONVERSATION: (100, 250),\n        TextLength.DETAILED_INDEPTH: (300, 800),\n        TextLength.PHD_LEVEL: (800, 2000)\n    }\n    return ranges.get(self.text_length, (50, 200))\n</code></pre> <code>get_format_instructions()</code> \u00b6 <p>Generiere Format-spezifische Anweisungen</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def get_format_instructions(self) -&gt; str:\n    \"\"\"Generiere Format-spezifische Anweisungen\"\"\"\n    format_instructions = {\n        ResponseFormat.FREI_TEXT: \"Verwende nat\u00fcrlichen Flie\u00dftext ohne spezielle Formatierung.\",\n        ResponseFormat.WITH_TABLES: \"Integriere Tabellen zur strukturierten Darstellung von Daten. Verwende Markdown-Tabellen.\",\n        ResponseFormat.WITH_BULLET_POINTS: \"Strukturiere Informationen mit Bullet Points (\u2022, -, *) f\u00fcr bessere Lesbarkeit.\",\n        ResponseFormat.WITH_LISTS: \"Verwende nummerierte und unnummerierte Listen zur Organisation von Inhalten.\",\n        ResponseFormat.TEXT_ONLY: \"Nur reiner Text ohne Formatierung, Symbole oder Strukturelemente.\",\n        ResponseFormat.MD_TEXT: \"Vollst\u00e4ndige Markdown-Formatierung mit Headings, Code-Blocks, Links etc.\",\n        ResponseFormat.YAML_TEXT: \"Strukturiere Antworten als YAML-Format f\u00fcr maschinenlesbare Ausgabe.\",\n        ResponseFormat.JSON_TEXT: \"Formatiere Antworten als JSON-Struktur f\u00fcr API-Integration.\",\n        ResponseFormat.PSEUDO_CODE: \"Verwende Pseudocode-Struktur f\u00fcr algorithmische oder logische Erkl\u00e4rungen.\",\n        ResponseFormat.CODE_STRUCTURE: \"Strukturiere wie Code mit Einr\u00fcckungen, Kommentaren und logischen Bl\u00f6cken.\"\n    }\n    return format_instructions.get(self.response_format, \"Standard-Formatierung.\")\n</code></pre> <code>get_length_instructions()</code> \u00b6 <p>Generiere L\u00e4ngen-spezifische Anweisungen</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def get_length_instructions(self) -&gt; str:\n    \"\"\"Generiere L\u00e4ngen-spezifische Anweisungen\"\"\"\n    length_instructions = {\n        TextLength.MINI_CHAT: \"Sehr kurze, pr\u00e4gnante Antworten (1-2 S\u00e4tze, max 50 W\u00f6rter). Chat-Style.\",\n        TextLength.CHAT_CONVERSATION: \"Moderate Gespr\u00e4chsl\u00e4nge (2-4 S\u00e4tze, 50-150 W\u00f6rter). Nat\u00fcrlicher Unterhaltungsstil.\",\n        TextLength.TABLE_CONVERSATION: \"Strukturierte, tabellarische Darstellung mit kompakten Erkl\u00e4rungen (100-250 W\u00f6rter).\",\n        TextLength.DETAILED_INDEPTH: \"Ausf\u00fchrliche, detaillierte Erkl\u00e4rungen (300-800 W\u00f6rter) mit Tiefe und Kontext.\",\n        TextLength.PHD_LEVEL: \"Akademische Tiefe mit umfassenden Erkl\u00e4rungen (800+ W\u00f6rter), Quellenangaben und Fachterminologie.\"\n    }\n    return length_instructions.get(self.text_length, \"Standard-L\u00e4nge.\")\n</code></pre> <code>LLMTask</code> <code>dataclass</code> \u00b6 <p>               Bases: <code>Task</code></p> <p>Spezialisierter Task f\u00fcr LLM-Aufrufe</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass LLMTask(Task):\n    \"\"\"Spezialisierter Task f\u00fcr LLM-Aufrufe\"\"\"\n    llm_config: Dict[str, Any] = field(default_factory=lambda: {\n        \"model_preference\": \"fast\",  # \"fast\" | \"complex\"\n        \"temperature\": 0.7,\n        \"max_tokens\": 1024\n    })\n    prompt_template: str = \"\"\n    context_keys: List[str] = field(default_factory=list)  # Keys aus shared state\n    output_schema: Optional[Dict] = None  # JSON Schema f\u00fcr Validierung\n</code></pre> <code>PersonaConfig</code> <code>dataclass</code> \u00b6 Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass PersonaConfig:\n    name: str\n    style: str = \"professional\"\n    personality_traits: List[str] = field(default_factory=lambda: [\"helpful\", \"concise\"])\n    tone: str = \"friendly\"\n    response_format: str = \"direct\"\n    custom_instructions: str = \"\"\n\n    format_config: Optional[FormatConfig] = None\n\n    apply_method: str = \"system_prompt\"  # \"system_prompt\" | \"post_process\" | \"both\"\n    integration_level: str = \"light\"  # \"light\" | \"medium\" | \"heavy\"\n\n    def to_system_prompt_addition(self) -&gt; str:\n        \"\"\"Convert persona to system prompt addition with format integration\"\"\"\n        if self.apply_method in [\"system_prompt\", \"both\"]:\n            additions = []\n            additions.append(f\"You are {self.name}.\")\n            additions.append(f\"Your communication style is {self.style} with a {self.tone} tone.\")\n\n            if self.personality_traits:\n                traits_str = \", \".join(self.personality_traits)\n                additions.append(f\"Your key traits are: {traits_str}.\")\n\n            if self.custom_instructions:\n                additions.append(self.custom_instructions)\n\n            # Format-spezifische Anweisungen hinzuf\u00fcgen\n            if self.format_config:\n                additions.append(\"\\n\" + self.format_config.get_combined_instructions())\n\n            return \" \".join(additions)\n        return \"\"\n\n    def update_format(self, response_format: ResponseFormat|str, text_length: TextLength|str, custom_instructions: str = \"\"):\n        \"\"\"Dynamische Format-Aktualisierung\"\"\"\n        try:\n            format_enum = ResponseFormat(response_format) if isinstance(response_format, str) else response_format\n            length_enum = TextLength(text_length) if isinstance(text_length, str) else text_length\n\n            if not self.format_config:\n                self.format_config = FormatConfig()\n\n            self.format_config.response_format = format_enum\n            self.format_config.text_length = length_enum\n\n            if custom_instructions:\n                self.format_config.custom_instructions = custom_instructions\n\n\n        except ValueError as e:\n            raise ValueError(f\"Invalid format '{response_format}' or length '{text_length}'\")\n\n    def should_post_process(self) -&gt; bool:\n        \"\"\"Check if post-processing should be applied\"\"\"\n        return self.apply_method in [\"post_process\", \"both\"]\n</code></pre> <code>should_post_process()</code> \u00b6 <p>Check if post-processing should be applied</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def should_post_process(self) -&gt; bool:\n    \"\"\"Check if post-processing should be applied\"\"\"\n    return self.apply_method in [\"post_process\", \"both\"]\n</code></pre> <code>to_system_prompt_addition()</code> \u00b6 <p>Convert persona to system prompt addition with format integration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def to_system_prompt_addition(self) -&gt; str:\n    \"\"\"Convert persona to system prompt addition with format integration\"\"\"\n    if self.apply_method in [\"system_prompt\", \"both\"]:\n        additions = []\n        additions.append(f\"You are {self.name}.\")\n        additions.append(f\"Your communication style is {self.style} with a {self.tone} tone.\")\n\n        if self.personality_traits:\n            traits_str = \", \".join(self.personality_traits)\n            additions.append(f\"Your key traits are: {traits_str}.\")\n\n        if self.custom_instructions:\n            additions.append(self.custom_instructions)\n\n        # Format-spezifische Anweisungen hinzuf\u00fcgen\n        if self.format_config:\n            additions.append(\"\\n\" + self.format_config.get_combined_instructions())\n\n        return \" \".join(additions)\n    return \"\"\n</code></pre> <code>update_format(response_format, text_length, custom_instructions='')</code> \u00b6 <p>Dynamische Format-Aktualisierung</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def update_format(self, response_format: ResponseFormat|str, text_length: TextLength|str, custom_instructions: str = \"\"):\n    \"\"\"Dynamische Format-Aktualisierung\"\"\"\n    try:\n        format_enum = ResponseFormat(response_format) if isinstance(response_format, str) else response_format\n        length_enum = TextLength(text_length) if isinstance(text_length, str) else text_length\n\n        if not self.format_config:\n            self.format_config = FormatConfig()\n\n        self.format_config.response_format = format_enum\n        self.format_config.text_length = length_enum\n\n        if custom_instructions:\n            self.format_config.custom_instructions = custom_instructions\n\n\n    except ValueError as e:\n        raise ValueError(f\"Invalid format '{response_format}' or length '{text_length}'\")\n</code></pre> <code>ProgressEvent</code> <code>dataclass</code> \u00b6 <p>Enhanced progress event with better error handling</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass ProgressEvent:\n    \"\"\"Enhanced progress event with better error handling\"\"\"\n    event_type: str\n    timestamp: float\n    node_name: str\n    event_id: str = \"\"\n\n    # Status information\n    status: Optional[NodeStatus] = None\n    success: Optional[bool] = None\n    error_details: Optional[Dict[str, Any]] = None\n\n    # LLM-specific data\n    llm_model: Optional[str] = None\n    llm_prompt_tokens: Optional[int] = None\n    llm_completion_tokens: Optional[int] = None\n    llm_total_tokens: Optional[int] = None\n    llm_cost: Optional[float] = None\n    llm_duration: Optional[float] = None\n    llm_temperature: Optional[float] = None\n\n    # Tool-specific data\n    tool_name: Optional[str] = None\n    tool_args: Optional[Dict[str, Any]] = None\n    tool_result: Optional[Any] = None\n    tool_duration: Optional[float] = None\n    tool_success: Optional[bool] = None\n    tool_error: Optional[str] = None\n\n    # Node/Routing data\n    routing_decision: Optional[str] = None\n    routing_from: Optional[str] = None\n    routing_to: Optional[str] = None\n    node_phase: Optional[str] = None\n    node_duration: Optional[float] = None\n\n    # Context data\n    task_id: Optional[str] = None\n    session_id: Optional[str] = None\n    plan_id: Optional[str] = None\n\n    # Additional metadata\n    metadata: Dict[str, Any] = None\n\n    def __post_init__(self):\n        if self.metadata is None:\n            self.metadata = {}\n        if not self.event_id:\n            self.event_id = f\"{self.node_name}_{self.event_type}_{int(self.timestamp * 1000000)}\"\n        if 'error' in self.metadata or 'error_type' in self.metadata:\n            if self.error_details is None:\n                self.error_details = {}\n            self.error_details['error'] = self.metadata.get('error')\n            self.error_details['error_type'] = self.metadata.get('error_type')\n            self.status = NodeStatus.FAILED\n        if self.status == NodeStatus.FAILED:\n            self.success = False\n        if self.status == NodeStatus.COMPLETED:\n            self.success = True\n</code></pre> <code>ProgressTracker</code> \u00b6 <p>Advanced progress tracking with cost calculation</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>class ProgressTracker:\n    \"\"\"Advanced progress tracking with cost calculation\"\"\"\n\n    def __init__(self, progress_callback: Optional[callable] = None, agent_name=\"unknown\"):\n        self.progress_callback = progress_callback\n        self.events: List[ProgressEvent] = []\n        self.active_timers: Dict[str, float] = {}\n\n        # Cost tracking (simplified - would need actual provider pricing)\n        self.token_costs = {\n            \"input\": 0.00001,  # $0.01/1K tokens input\n            \"output\": 0.00003,  # $0.03/1K tokens output\n        }\n        self.agent_name = agent_name\n\n    async def emit_event(self, event: ProgressEvent):\n        \"\"\"Emit progress event with callback and storage\"\"\"\n        self.events.append(event)\n        event.agent_name = self.agent_name\n\n        if self.progress_callback:\n            try:\n                if asyncio.iscoroutinefunction(self.progress_callback):\n                    await self.progress_callback(event)\n                else:\n                    self.progress_callback(event)\n            except Exception as e:\n                import traceback\n                print(traceback.format_exc())\n\n\n    def start_timer(self, key: str) -&gt; float:\n        \"\"\"Start timing operation\"\"\"\n        start_time = time.perf_counter()\n        self.active_timers[key] = start_time\n        return start_time\n\n    def end_timer(self, key: str) -&gt; float:\n        \"\"\"End timing operation and return duration\"\"\"\n        if key not in self.active_timers:\n            return 0.0\n        duration = time.perf_counter() - self.active_timers[key]\n        del self.active_timers[key]\n        return duration\n\n    def calculate_llm_cost(self, model: str, input_tokens: int, output_tokens: int) -&gt; float:\n        \"\"\"Calculate approximate LLM cost\"\"\"\n        # Simplified cost calculation - would need actual provider pricing\n        input_cost = (input_tokens / 1000) * self.token_costs[\"input\"]\n        output_cost = (output_tokens / 1000) * self.token_costs[\"output\"]\n        return input_cost + output_cost\n\n    def get_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Get comprehensive progress summary\"\"\"\n        summary = {\n            \"total_events\": len(self.events),\n            \"llm_calls\": len([e for e in self.events if e.event_type == \"llm_call\"]),\n            \"tool_calls\": len([e for e in self.events if e.event_type == \"tool_call\"]),\n            \"total_cost\": sum(e.llm_cost for e in self.events if e.llm_cost),\n            \"total_tokens\": sum(e.llm_total_tokens for e in self.events if e.llm_total_tokens),\n            \"total_duration\": sum(e.node_duration for e in self.events if e.node_duration),\n            \"nodes_visited\": list(set(e.node_name for e in self.events)),\n            \"tools_used\": list(set(e.tool_name for e in self.events if e.tool_name)),\n            \"models_used\": list(set(e.llm_model for e in self.events if e.llm_model))\n        }\n        return summary\n</code></pre> <code>calculate_llm_cost(model, input_tokens, output_tokens)</code> \u00b6 <p>Calculate approximate LLM cost</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def calculate_llm_cost(self, model: str, input_tokens: int, output_tokens: int) -&gt; float:\n    \"\"\"Calculate approximate LLM cost\"\"\"\n    # Simplified cost calculation - would need actual provider pricing\n    input_cost = (input_tokens / 1000) * self.token_costs[\"input\"]\n    output_cost = (output_tokens / 1000) * self.token_costs[\"output\"]\n    return input_cost + output_cost\n</code></pre> <code>emit_event(event)</code> <code>async</code> \u00b6 <p>Emit progress event with callback and storage</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>async def emit_event(self, event: ProgressEvent):\n    \"\"\"Emit progress event with callback and storage\"\"\"\n    self.events.append(event)\n    event.agent_name = self.agent_name\n\n    if self.progress_callback:\n        try:\n            if asyncio.iscoroutinefunction(self.progress_callback):\n                await self.progress_callback(event)\n            else:\n                self.progress_callback(event)\n        except Exception as e:\n            import traceback\n            print(traceback.format_exc())\n</code></pre> <code>end_timer(key)</code> \u00b6 <p>End timing operation and return duration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def end_timer(self, key: str) -&gt; float:\n    \"\"\"End timing operation and return duration\"\"\"\n    if key not in self.active_timers:\n        return 0.0\n    duration = time.perf_counter() - self.active_timers[key]\n    del self.active_timers[key]\n    return duration\n</code></pre> <code>get_summary()</code> \u00b6 <p>Get comprehensive progress summary</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def get_summary(self) -&gt; Dict[str, Any]:\n    \"\"\"Get comprehensive progress summary\"\"\"\n    summary = {\n        \"total_events\": len(self.events),\n        \"llm_calls\": len([e for e in self.events if e.event_type == \"llm_call\"]),\n        \"tool_calls\": len([e for e in self.events if e.event_type == \"tool_call\"]),\n        \"total_cost\": sum(e.llm_cost for e in self.events if e.llm_cost),\n        \"total_tokens\": sum(e.llm_total_tokens for e in self.events if e.llm_total_tokens),\n        \"total_duration\": sum(e.node_duration for e in self.events if e.node_duration),\n        \"nodes_visited\": list(set(e.node_name for e in self.events)),\n        \"tools_used\": list(set(e.tool_name for e in self.events if e.tool_name)),\n        \"models_used\": list(set(e.llm_model for e in self.events if e.llm_model))\n    }\n    return summary\n</code></pre> <code>start_timer(key)</code> \u00b6 <p>Start timing operation</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def start_timer(self, key: str) -&gt; float:\n    \"\"\"Start timing operation\"\"\"\n    start_time = time.perf_counter()\n    self.active_timers[key] = start_time\n    return start_time\n</code></pre> <code>Task</code> <code>dataclass</code> \u00b6 Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass Task:\n    id: str\n    type: str\n    description: str\n    status: str = \"pending\"  # pending, running, completed, failed, paused\n    priority: int = 1\n    dependencies: List[str] = field(default_factory=list)\n    subtasks: List[str] = field(default_factory=list)\n    result: Any = None\n    error: str = None\n    created_at: datetime = field(default_factory=datetime.now)\n    started_at: Optional[datetime] = None\n    completed_at: Optional[datetime] = None\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    retry_count: int = 0\n    max_retries: int = 3\n    critical: bool = False\n\n\n    def __post_init__(self):\n        \"\"\"Ensure all mutable defaults are properly initialized\"\"\"\n        if self.metadata is None:\n            self.metadata = {}\n        if self.dependencies is None:\n            self.dependencies = []\n        if self.subtasks is None:\n            self.subtasks = []\n\n    def __getitem__(self, key):\n        return getattr(self, key)\n\n    def __setitem__(self, key, value):\n        setattr(self, key, value)\n</code></pre> <code>__post_init__()</code> \u00b6 <p>Ensure all mutable defaults are properly initialized</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Ensure all mutable defaults are properly initialized\"\"\"\n    if self.metadata is None:\n        self.metadata = {}\n    if self.dependencies is None:\n        self.dependencies = []\n    if self.subtasks is None:\n        self.subtasks = []\n</code></pre> <code>ToolAnalysis</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Defines the structure for a valid tool analysis.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>class ToolAnalysis(BaseModel):\n    \"\"\"Defines the structure for a valid tool analysis.\"\"\"\n    primary_function: str = Field(..., description=\"The main purpose of the tool.\")\n    use_cases: List[str] = Field(..., description=\"Specific use cases for the tool.\")\n    trigger_phrases: List[str] = Field(..., description=\"Phrases that should trigger the tool.\")\n    indirect_connections: List[str] = Field(..., description=\"Non-obvious connections or applications.\")\n    complexity_scenarios: List[str] = Field(..., description=\"Complex scenarios where the tool can be applied.\")\n    user_intent_categories: List[str] = Field(..., description=\"Categories of user intent the tool addresses.\")\n    confidence_triggers: Dict[str, float] = Field(..., description=\"Phrases mapped to confidence scores.\")\n    tool_complexity: str = Field(..., description=\"The complexity of the tool, rated as low, medium, or high.\")\n    args_schema: Optional[Dict[str, Any]] = Field(..., description=\"The schema for the tool's arguments.\")\n</code></pre> <code>ToolTask</code> <code>dataclass</code> \u00b6 <p>               Bases: <code>Task</code></p> <p>Spezialisierter Task f\u00fcr Tool-Aufrufe</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass ToolTask(Task):\n    \"\"\"Spezialisierter Task f\u00fcr Tool-Aufrufe\"\"\"\n    tool_name: str = \"\"\n    arguments: Dict[str, Any] = field(default_factory=dict)  # Kann {{ }} Referenzen enthalten\n    hypothesis: str = \"\"  # Was erwarten wir von diesem Tool?\n    validation_criteria: str = \"\"  # Wie validieren wir das Ergebnis?\n    expectation: str = \"\"  # Wie sollte das Ergebnis aussehen?\n</code></pre> <code>create_task(task_type, **kwargs)</code> \u00b6 <p>Factory f\u00fcr Task-Erstellung mit korrektem Typ</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def create_task(task_type: str, **kwargs) -&gt; Task:\n    \"\"\"Factory f\u00fcr Task-Erstellung mit korrektem Typ\"\"\"\n    task_classes = {\n        \"llm_call\": LLMTask,\n        \"tool_call\": ToolTask,\n        \"decision\": DecisionTask,\n        \"compound\": CompoundTask,\n        \"generic\": Task,\n        \"LLMTask\": LLMTask,\n        \"ToolTask\": ToolTask,\n        \"DecisionTask\": DecisionTask,\n        \"CompoundTask\": CompoundTask,\n        \"Task\": Task,\n    }\n\n    task_class = task_classes.get(task_type, Task)\n\n    # Standard-Felder setzen\n    if \"id\" not in kwargs:\n        kwargs[\"id\"] = str(uuid.uuid4())\n    if \"type\" not in kwargs:\n        kwargs[\"type\"] = task_type\n    if \"critical\" not in kwargs:\n        kwargs[\"critical\"] = task_type in [\"llm_call\", \"decision\"]\n\n    # Ensure metadata is initialized\n    if \"metadata\" not in kwargs:\n        kwargs[\"metadata\"] = {}\n\n    # Create task and ensure post_init is called\n    task = task_class(**kwargs)\n\n    # Double-check metadata initialization\n    if not hasattr(task, 'metadata') or task.metadata is None:\n        task.metadata = {}\n\n    return task\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.Agent.utils","title":"<code>utils</code>","text":"<code>LLMMessage</code> <code>dataclass</code> \u00b6 <p>Represents a message in a conversation with the LLM.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/utils.py</code> <pre><code>@dataclass\nclass LLMMessage:\n    \"\"\"Represents a message in a conversation with the LLM.\"\"\"\n    role: str  # \"user\", \"assistant\", \"system\", \"tool\"\n    # Content can be string or list (e.g., multimodal with text/image dicts)\n    # Conforms to LiteLLM/OpenAI structure\n    content: str | list[dict[str, Any]]\n    tool_call_id: str | None = None  # For tool responses\n    name: str | None = None  # For tool calls/responses (function name)\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert to dictionary, handling potential dataclass nuances.\"\"\"\n        d = {\"role\": self.role, \"content\": self.content}\n        if self.tool_call_id:\n            d[\"tool_call_id\"] = self.tool_call_id\n        if self.name:\n            d[\"name\"] = self.name\n        return d\n</code></pre> <code>to_dict()</code> \u00b6 <p>Convert to dictionary, handling potential dataclass nuances.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/utils.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert to dictionary, handling potential dataclass nuances.\"\"\"\n    d = {\"role\": self.role, \"content\": self.content}\n    if self.tool_call_id:\n        d[\"tool_call_id\"] = self.tool_call_id\n    if self.name:\n        d[\"name\"] = self.name\n    return d\n</code></pre> <code>WorldModel</code> <code>dataclass</code> \u00b6 <p>Thread-safe representation of the agent's persistent understanding of the world.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/utils.py</code> <pre><code>@dataclass\nclass WorldModel:\n    \"\"\"Thread-safe representation of the agent's persistent understanding of the world.\"\"\"\n    data: dict[str, Any] = dataclass_field(default_factory=dict)\n    _lock: threading.Lock = dataclass_field(default_factory=threading.Lock)\n\n    def get(self, key: str, default: Any = None) -&gt; Any:\n        with self._lock:\n            return self.data.get(key, default)\n\n    def set(self, key: str, value: Any):\n        with self._lock:\n            logger_wm.debug(f\"WorldModel SET: {key} = {value}\")\n            self.data[key] = value\n\n    def remove(self, key: str):\n        with self._lock:\n            if key in self.data:\n                logger_wm.debug(f\"WorldModel REMOVE: {key}\")\n                del self.data[key]\n\n    def show(self) -&gt; str:\n        with self._lock:\n            if not self.data:\n                return \"[empty]\"\n            try:\n                items = [f\"- {k}: {json.dumps(v, indent=None, ensure_ascii=False, default=str)}\"\n                         for k, v in self.data.items()]\n                return \"\\n\".join(items)\n            except Exception:\n                items = [f\"- {k}: {str(v)}\" for k, v in self.data.items()]\n                return \"\\n\".join(items)\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        with self._lock:\n            # Deep copy might be needed if values are mutable and modified externally\n            # For simplicity, shallow copy is used here.\n            return self.data.copy()\n\n    def update_from_dict(self, data_dict: dict[str, Any]):\n        with self._lock:\n            self.data.update(data_dict)\n            logger_wm.debug(f\"WorldModel updated from dict: {list(data_dict.keys())}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils","title":"<code>AgentUtils</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.AISemanticMemory","title":"<code>AISemanticMemory</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>class AISemanticMemory(metaclass=Singleton):\n    def __init__(self,\n                 base_path: str = \"/semantic_memory\",\n                 default_model: str = os.getenv(\"DEFAULTMODELSUMMERY\"),\n                 default_embedding_model: str = os.getenv(\"DEFAULTMODELEMBEDDING\"),\n                 default_similarity_threshold: float = 0.61,\n                 default_batch_size: int = 64,\n                 default_n_clusters: int = 2,\n                 default_deduplication_threshold: float = 0.85):\n        \"\"\"\n        Initialize AISemanticMemory with KnowledgeBase integration\n\n        Args:\n            base_path: Root directory for memory storage\n            default_model: Default model for text generation\n            default_embedding_model: Default embedding model\n            default_similarity_threshold: Default similarity threshold for retrieval\n            default_batch_size: Default batch size for processing\n            default_n_clusters: Default number of clusters for FAISS\n            default_deduplication_threshold: Default threshold for deduplication\n        \"\"\"\n        self.base_path = os.path.join(os.getcwd(), \".data\", base_path)\n        self.memories: dict[str, KnowledgeBase] = {}\n\n        # Map of embedding models to their dimensions\n        self.embedding_dims = {\n            \"text-embedding-3-small\": 1536,\n            \"text-embedding-3-large\": 3072,\n            \"nomic-embed-text\": 768,\n            \"default\": 768\n        }\n\n        self.default_config = {\n            \"embedding_model\": default_embedding_model,\n            \"embedding_dim\": self._get_embedding_dim(default_embedding_model),\n            \"similarity_threshold\": default_similarity_threshold,\n            \"batch_size\": default_batch_size,\n            \"n_clusters\": default_n_clusters,\n            \"deduplication_threshold\": default_deduplication_threshold,\n            \"model_name\": default_model\n        }\n\n    def _get_embedding_dim(self, model_name: str) -&gt; int:\n        \"\"\"Get embedding dimension for a model\"\"\"\n        return self.embedding_dims.get(model_name, 768)\n\n    @staticmethod\n    def _sanitize_name(name: str) -&gt; str:\n        \"\"\"Sanitize memory name for filesystem safety\"\"\"\n        name = re.sub(r'[^a-zA-Z0-9_-]', '-', name)[:63].strip('-')\n        if not name:\n            raise ValueError(\"Invalid memory name\")\n        if len(name) &lt; 3:\n            name += \"Z\" * (3 - len(name))\n        return name\n\n    def create_memory(self,\n                      name: str,\n                      model_config: dict | None = None,\n                      storage_config: dict | None = None) -&gt; KnowledgeBase:\n        \"\"\"\n        Create new memory store with KnowledgeBase\n\n        Args:\n            name: Unique name for the memory store\n            model_config: Configuration for embedding model\n            storage_config: Configuration for KnowledgeBase parameters\n        \"\"\"\n        sanitized = self._sanitize_name(name)\n        if sanitized in self.memories:\n            raise ValueError(f\"Memory '{name}' already exists\")\n\n        # Determine embedding model and dimension\n        embedding_model = self.default_config[\"embedding_model\"]\n        model_name = self.default_config[\"model_name\"]\n        if model_config:\n            embedding_model = model_config.get(\"embedding_model\", embedding_model)\n            model_name = model_config.get(\"model_name\", model_name)\n        embedding_dim = self._get_embedding_dim(embedding_model)\n\n        # Get KnowledgeBase parameters\n        kb_params = {\n            \"embedding_dim\": embedding_dim,\n            \"embedding_model\": embedding_model,\n            \"similarity_threshold\": self.default_config[\"similarity_threshold\"],\n            \"batch_size\": self.default_config[\"batch_size\"],\n            \"n_clusters\": self.default_config[\"n_clusters\"],\n            \"deduplication_threshold\": self.default_config[\"deduplication_threshold\"],\n            \"model_name\": model_name,\n        }\n\n        if storage_config:\n            kb_params.update({\n                \"similarity_threshold\": storage_config.get(\"similarity_threshold\", kb_params[\"similarity_threshold\"]),\n                \"batch_size\": storage_config.get(\"batch_size\", kb_params[\"batch_size\"]),\n                \"n_clusters\": storage_config.get(\"n_clusters\", kb_params[\"n_clusters\"]),\n                \"model_name\": storage_config.get(\"model_name\", kb_params[\"model_name\"]),\n                \"embedding_model\": storage_config.get(\"embedding_model\", kb_params[\"embedding_model\"]),\n                \"deduplication_threshold\": storage_config.get(\"deduplication_threshold\",\n                                                              kb_params[\"deduplication_threshold\"]),\n            })\n\n        # Create KnowledgeBase instance\n        self.memories[sanitized] = KnowledgeBase(**kb_params)\n        return self.memories[sanitized]\n\n    async def add_data(self,\n                       memory_name: str,\n                       data: str | list[str] | bytes | dict,\n                       metadata: dict | None = None, direct=False) -&gt; bool:\n        \"\"\"\n        Add data to memory store\n\n        Args:\n            memory_name: Target memory store\n            data: Text, list of texts, binary file, or structured data\n            metadata: Optional metadata\n        \"\"\"\n        name = self._sanitize_name(memory_name)\n        kb = self.memories.get(name)\n        if not kb:\n            kb = self.create_memory(name)\n\n        # Process input data\n        texts = []\n        if isinstance(data, bytes):\n            try:\n                import textract\n                text = textract.process(data).decode('utf-8')\n                texts = [text.replace('\\\\t', '').replace('\\t', '')]\n            except Exception as e:\n                raise ValueError(f\"File processing failed: {str(e)}\")\n        elif isinstance(data, str):\n            texts = [data.replace('\\\\t', '').replace('\\t', '')]\n        elif isinstance(data, list):\n            texts = [d.replace('\\\\t', '').replace('\\t', '') for d in data]\n        elif isinstance(data, dict):\n            # Custom KG not supported in current KnowledgeBase\n            raise NotImplementedError(\"Custom knowledge graph insertion not supported\")\n        else:\n            raise ValueError(\"Unsupported data type\")\n\n        # Add data to KnowledgeBase\n        try:\n            added, duplicates = await kb.add_data(texts, metadata, direct=direct)\n            return added &gt; 0\n        except Exception as e:\n            import traceback\n            print(traceback.format_exc())\n            raise RuntimeError(f\"Data addition failed: {str(e)}\")\n\n    def get(self, names):\n        return [m for n,m in self._get_target_memories(names)]\n\n    async def query(self,\n                    query: str,\n                    memory_names: str | list[str] | None = None,\n                    query_params: dict | None = None,\n                    to_str: bool = False,\n                    unified_retrieve: bool =False) -&gt; str | list[dict]:\n        \"\"\"\n        Query memories using KnowledgeBase retrieval\n\n        Args:\n            query: Search query\n            memory_names: Target memory names\n            query_params: Query parameters\n            to_str: Return string format\n            unified_retrieve: Unified retrieve\n        \"\"\"\n        targets = self._get_target_memories(memory_names)\n        if not targets:\n            return []\n\n        results = []\n        for name, kb in targets:\n            #try:\n                # Use KnowledgeBase's retrieve_with_overview for comprehensive results\n                result = await kb.retrieve_with_overview(\n                    query=query,\n                    k=query_params.get(\"k\", 3) if query_params else 3,\n                    min_similarity=query_params.get(\"min_similarity\", 0.2) if query_params else 0.2,\n                    cross_ref_depth=query_params.get(\"cross_ref_depth\", 2) if query_params else 2,\n                    max_cross_refs=query_params.get(\"max_cross_refs\", 2) if query_params else 2,\n                    max_sentences=query_params.get(\"max_sentences\", 5) if query_params else 5\n                ) if not unified_retrieve else await kb.unified_retrieve(\n                    query=query,\n                    k=query_params.get(\"k\", 2) if query_params else 2,\n                    min_similarity=query_params.get(\"min_similarity\", 0.2) if query_params else 0.2,\n                    cross_ref_depth=query_params.get(\"cross_ref_depth\", 2) if query_params else 2,\n                    max_cross_refs=query_params.get(\"max_cross_refs\", 6) if query_params else 6,\n                    max_sentences=query_params.get(\"max_sentences\", 12) if query_params else 12\n                )\n                results.append({\n                    \"memory\": name,\n                    \"result\": result\n                })\n            #except Exception as e:\n            #    print(f\"Query failed on {name}: {str(e)}\")\n        if to_str:\n            if not unified_retrieve:\n                str_res = [\n                    f\"{x['memory']} - {json.dumps(x['result'].overview)}\\n - {[c.text for c in x['result'].details]}\\n - {[(k, [c.text for c in v]) for k, v in x['result'].cross_references.items()]}\"\n                    for x in results]\n                # str_res =\n            else:\n                str_res = json.dumps(results)\n            return str_res\n        return results\n\n    def _get_target_memories(self, memory_names: str | list[str] | None) -&gt; list[tuple[str, KnowledgeBase]]:\n        \"\"\"Get target memories for query\"\"\"\n        if not memory_names:\n            return list(self.memories.items())\n\n        names = [memory_names] if isinstance(memory_names, str) else memory_names\n\n        targets = []\n        for name in names:\n            sanitized = self._sanitize_name(name)\n            if kb := self.memories.get(sanitized):\n                targets.append((sanitized, kb))\n        return targets\n\n    def list_memories(self) -&gt; list[str]:\n        \"\"\"List all available memories\"\"\"\n        return list(self.memories.keys())\n\n    async def delete_memory(self, name: str) -&gt; bool:\n        \"\"\"Delete a memory store\"\"\"\n        sanitized = self._sanitize_name(name)\n        if sanitized in self.memories:\n            del self.memories[sanitized]\n            return True\n        return False\n\n    def save_memory(self, name: str, path: str) -&gt; bool | bytes:\n        \"\"\"Save a memory store to disk\"\"\"\n        sanitized = self._sanitize_name(name)\n        if kb := self.memories.get(sanitized):\n            try:\n                return kb.save(path)\n            except Exception as e:\n                print(f\"Error saving memory: {str(e)}\")\n                return False\n        return False\n\n    def save_all_memories(self, path: str) -&gt; bool:\n        \"\"\"Save all memory stores to disk\"\"\"\n        for name, kb in self.memories.items():\n            try:\n                kb.save(os.path.join(path, f\"{name}.pkl\"))\n            except Exception as e:\n                print(f\"Error saving memory: {str(e)}\")\n                return False\n        return True\n\n    def load_all_memories(self, path: str) -&gt; bool:\n        \"\"\"Load all memory stores from disk\"\"\"\n        for file in os.listdir(path):\n            if file.endswith(\".pkl\"):\n                try:\n                    self.memories[file[:-4]] = KnowledgeBase.load(os.path.join(path, file))\n                except Exception as e:\n                    print(f\"Error loading memory: {str(e)}\")\n                    return False\n        return True\n\n    def load_memory(self, name: str, path: str | bytes) -&gt; bool:\n        \"\"\"Load a memory store from disk\"\"\"\n        sanitized = self._sanitize_name(name)\n        if sanitized in self.memories:\n            return False\n        try:\n            self.memories[sanitized] = KnowledgeBase.load(path)\n            return True\n        except Exception:\n            # print(f\"Error loading memory: {str(e)}\")\n            return False\n</code></pre> <code>__init__(base_path='/semantic_memory', default_model=os.getenv('DEFAULTMODELSUMMERY'), default_embedding_model=os.getenv('DEFAULTMODELEMBEDDING'), default_similarity_threshold=0.61, default_batch_size=64, default_n_clusters=2, default_deduplication_threshold=0.85)</code> \u00b6 <p>Initialize AISemanticMemory with KnowledgeBase integration</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str</code> <p>Root directory for memory storage</p> <code>'/semantic_memory'</code> <code>default_model</code> <code>str</code> <p>Default model for text generation</p> <code>getenv('DEFAULTMODELSUMMERY')</code> <code>default_embedding_model</code> <code>str</code> <p>Default embedding model</p> <code>getenv('DEFAULTMODELEMBEDDING')</code> <code>default_similarity_threshold</code> <code>float</code> <p>Default similarity threshold for retrieval</p> <code>0.61</code> <code>default_batch_size</code> <code>int</code> <p>Default batch size for processing</p> <code>64</code> <code>default_n_clusters</code> <code>int</code> <p>Default number of clusters for FAISS</p> <code>2</code> <code>default_deduplication_threshold</code> <code>float</code> <p>Default threshold for deduplication</p> <code>0.85</code> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def __init__(self,\n             base_path: str = \"/semantic_memory\",\n             default_model: str = os.getenv(\"DEFAULTMODELSUMMERY\"),\n             default_embedding_model: str = os.getenv(\"DEFAULTMODELEMBEDDING\"),\n             default_similarity_threshold: float = 0.61,\n             default_batch_size: int = 64,\n             default_n_clusters: int = 2,\n             default_deduplication_threshold: float = 0.85):\n    \"\"\"\n    Initialize AISemanticMemory with KnowledgeBase integration\n\n    Args:\n        base_path: Root directory for memory storage\n        default_model: Default model for text generation\n        default_embedding_model: Default embedding model\n        default_similarity_threshold: Default similarity threshold for retrieval\n        default_batch_size: Default batch size for processing\n        default_n_clusters: Default number of clusters for FAISS\n        default_deduplication_threshold: Default threshold for deduplication\n    \"\"\"\n    self.base_path = os.path.join(os.getcwd(), \".data\", base_path)\n    self.memories: dict[str, KnowledgeBase] = {}\n\n    # Map of embedding models to their dimensions\n    self.embedding_dims = {\n        \"text-embedding-3-small\": 1536,\n        \"text-embedding-3-large\": 3072,\n        \"nomic-embed-text\": 768,\n        \"default\": 768\n    }\n\n    self.default_config = {\n        \"embedding_model\": default_embedding_model,\n        \"embedding_dim\": self._get_embedding_dim(default_embedding_model),\n        \"similarity_threshold\": default_similarity_threshold,\n        \"batch_size\": default_batch_size,\n        \"n_clusters\": default_n_clusters,\n        \"deduplication_threshold\": default_deduplication_threshold,\n        \"model_name\": default_model\n    }\n</code></pre> <code>add_data(memory_name, data, metadata=None, direct=False)</code> <code>async</code> \u00b6 <p>Add data to memory store</p> <p>Parameters:</p> Name Type Description Default <code>memory_name</code> <code>str</code> <p>Target memory store</p> required <code>data</code> <code>str | list[str] | bytes | dict</code> <p>Text, list of texts, binary file, or structured data</p> required <code>metadata</code> <code>dict | None</code> <p>Optional metadata</p> <code>None</code> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>async def add_data(self,\n                   memory_name: str,\n                   data: str | list[str] | bytes | dict,\n                   metadata: dict | None = None, direct=False) -&gt; bool:\n    \"\"\"\n    Add data to memory store\n\n    Args:\n        memory_name: Target memory store\n        data: Text, list of texts, binary file, or structured data\n        metadata: Optional metadata\n    \"\"\"\n    name = self._sanitize_name(memory_name)\n    kb = self.memories.get(name)\n    if not kb:\n        kb = self.create_memory(name)\n\n    # Process input data\n    texts = []\n    if isinstance(data, bytes):\n        try:\n            import textract\n            text = textract.process(data).decode('utf-8')\n            texts = [text.replace('\\\\t', '').replace('\\t', '')]\n        except Exception as e:\n            raise ValueError(f\"File processing failed: {str(e)}\")\n    elif isinstance(data, str):\n        texts = [data.replace('\\\\t', '').replace('\\t', '')]\n    elif isinstance(data, list):\n        texts = [d.replace('\\\\t', '').replace('\\t', '') for d in data]\n    elif isinstance(data, dict):\n        # Custom KG not supported in current KnowledgeBase\n        raise NotImplementedError(\"Custom knowledge graph insertion not supported\")\n    else:\n        raise ValueError(\"Unsupported data type\")\n\n    # Add data to KnowledgeBase\n    try:\n        added, duplicates = await kb.add_data(texts, metadata, direct=direct)\n        return added &gt; 0\n    except Exception as e:\n        import traceback\n        print(traceback.format_exc())\n        raise RuntimeError(f\"Data addition failed: {str(e)}\")\n</code></pre> <code>create_memory(name, model_config=None, storage_config=None)</code> \u00b6 <p>Create new memory store with KnowledgeBase</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique name for the memory store</p> required <code>model_config</code> <code>dict | None</code> <p>Configuration for embedding model</p> <code>None</code> <code>storage_config</code> <code>dict | None</code> <p>Configuration for KnowledgeBase parameters</p> <code>None</code> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def create_memory(self,\n                  name: str,\n                  model_config: dict | None = None,\n                  storage_config: dict | None = None) -&gt; KnowledgeBase:\n    \"\"\"\n    Create new memory store with KnowledgeBase\n\n    Args:\n        name: Unique name for the memory store\n        model_config: Configuration for embedding model\n        storage_config: Configuration for KnowledgeBase parameters\n    \"\"\"\n    sanitized = self._sanitize_name(name)\n    if sanitized in self.memories:\n        raise ValueError(f\"Memory '{name}' already exists\")\n\n    # Determine embedding model and dimension\n    embedding_model = self.default_config[\"embedding_model\"]\n    model_name = self.default_config[\"model_name\"]\n    if model_config:\n        embedding_model = model_config.get(\"embedding_model\", embedding_model)\n        model_name = model_config.get(\"model_name\", model_name)\n    embedding_dim = self._get_embedding_dim(embedding_model)\n\n    # Get KnowledgeBase parameters\n    kb_params = {\n        \"embedding_dim\": embedding_dim,\n        \"embedding_model\": embedding_model,\n        \"similarity_threshold\": self.default_config[\"similarity_threshold\"],\n        \"batch_size\": self.default_config[\"batch_size\"],\n        \"n_clusters\": self.default_config[\"n_clusters\"],\n        \"deduplication_threshold\": self.default_config[\"deduplication_threshold\"],\n        \"model_name\": model_name,\n    }\n\n    if storage_config:\n        kb_params.update({\n            \"similarity_threshold\": storage_config.get(\"similarity_threshold\", kb_params[\"similarity_threshold\"]),\n            \"batch_size\": storage_config.get(\"batch_size\", kb_params[\"batch_size\"]),\n            \"n_clusters\": storage_config.get(\"n_clusters\", kb_params[\"n_clusters\"]),\n            \"model_name\": storage_config.get(\"model_name\", kb_params[\"model_name\"]),\n            \"embedding_model\": storage_config.get(\"embedding_model\", kb_params[\"embedding_model\"]),\n            \"deduplication_threshold\": storage_config.get(\"deduplication_threshold\",\n                                                          kb_params[\"deduplication_threshold\"]),\n        })\n\n    # Create KnowledgeBase instance\n    self.memories[sanitized] = KnowledgeBase(**kb_params)\n    return self.memories[sanitized]\n</code></pre> <code>delete_memory(name)</code> <code>async</code> \u00b6 <p>Delete a memory store</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>async def delete_memory(self, name: str) -&gt; bool:\n    \"\"\"Delete a memory store\"\"\"\n    sanitized = self._sanitize_name(name)\n    if sanitized in self.memories:\n        del self.memories[sanitized]\n        return True\n    return False\n</code></pre> <code>list_memories()</code> \u00b6 <p>List all available memories</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def list_memories(self) -&gt; list[str]:\n    \"\"\"List all available memories\"\"\"\n    return list(self.memories.keys())\n</code></pre> <code>load_all_memories(path)</code> \u00b6 <p>Load all memory stores from disk</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def load_all_memories(self, path: str) -&gt; bool:\n    \"\"\"Load all memory stores from disk\"\"\"\n    for file in os.listdir(path):\n        if file.endswith(\".pkl\"):\n            try:\n                self.memories[file[:-4]] = KnowledgeBase.load(os.path.join(path, file))\n            except Exception as e:\n                print(f\"Error loading memory: {str(e)}\")\n                return False\n    return True\n</code></pre> <code>load_memory(name, path)</code> \u00b6 <p>Load a memory store from disk</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def load_memory(self, name: str, path: str | bytes) -&gt; bool:\n    \"\"\"Load a memory store from disk\"\"\"\n    sanitized = self._sanitize_name(name)\n    if sanitized in self.memories:\n        return False\n    try:\n        self.memories[sanitized] = KnowledgeBase.load(path)\n        return True\n    except Exception:\n        # print(f\"Error loading memory: {str(e)}\")\n        return False\n</code></pre> <code>query(query, memory_names=None, query_params=None, to_str=False, unified_retrieve=False)</code> <code>async</code> \u00b6 <p>Query memories using KnowledgeBase retrieval</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query</p> required <code>memory_names</code> <code>str | list[str] | None</code> <p>Target memory names</p> <code>None</code> <code>query_params</code> <code>dict | None</code> <p>Query parameters</p> <code>None</code> <code>to_str</code> <code>bool</code> <p>Return string format</p> <code>False</code> <code>unified_retrieve</code> <code>bool</code> <p>Unified retrieve</p> <code>False</code> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>async def query(self,\n                query: str,\n                memory_names: str | list[str] | None = None,\n                query_params: dict | None = None,\n                to_str: bool = False,\n                unified_retrieve: bool =False) -&gt; str | list[dict]:\n    \"\"\"\n    Query memories using KnowledgeBase retrieval\n\n    Args:\n        query: Search query\n        memory_names: Target memory names\n        query_params: Query parameters\n        to_str: Return string format\n        unified_retrieve: Unified retrieve\n    \"\"\"\n    targets = self._get_target_memories(memory_names)\n    if not targets:\n        return []\n\n    results = []\n    for name, kb in targets:\n        #try:\n            # Use KnowledgeBase's retrieve_with_overview for comprehensive results\n            result = await kb.retrieve_with_overview(\n                query=query,\n                k=query_params.get(\"k\", 3) if query_params else 3,\n                min_similarity=query_params.get(\"min_similarity\", 0.2) if query_params else 0.2,\n                cross_ref_depth=query_params.get(\"cross_ref_depth\", 2) if query_params else 2,\n                max_cross_refs=query_params.get(\"max_cross_refs\", 2) if query_params else 2,\n                max_sentences=query_params.get(\"max_sentences\", 5) if query_params else 5\n            ) if not unified_retrieve else await kb.unified_retrieve(\n                query=query,\n                k=query_params.get(\"k\", 2) if query_params else 2,\n                min_similarity=query_params.get(\"min_similarity\", 0.2) if query_params else 0.2,\n                cross_ref_depth=query_params.get(\"cross_ref_depth\", 2) if query_params else 2,\n                max_cross_refs=query_params.get(\"max_cross_refs\", 6) if query_params else 6,\n                max_sentences=query_params.get(\"max_sentences\", 12) if query_params else 12\n            )\n            results.append({\n                \"memory\": name,\n                \"result\": result\n            })\n        #except Exception as e:\n        #    print(f\"Query failed on {name}: {str(e)}\")\n    if to_str:\n        if not unified_retrieve:\n            str_res = [\n                f\"{x['memory']} - {json.dumps(x['result'].overview)}\\n - {[c.text for c in x['result'].details]}\\n - {[(k, [c.text for c in v]) for k, v in x['result'].cross_references.items()]}\"\n                for x in results]\n            # str_res =\n        else:\n            str_res = json.dumps(results)\n        return str_res\n    return results\n</code></pre> <code>save_all_memories(path)</code> \u00b6 <p>Save all memory stores to disk</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def save_all_memories(self, path: str) -&gt; bool:\n    \"\"\"Save all memory stores to disk\"\"\"\n    for name, kb in self.memories.items():\n        try:\n            kb.save(os.path.join(path, f\"{name}.pkl\"))\n        except Exception as e:\n            print(f\"Error saving memory: {str(e)}\")\n            return False\n    return True\n</code></pre> <code>save_memory(name, path)</code> \u00b6 <p>Save a memory store to disk</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def save_memory(self, name: str, path: str) -&gt; bool | bytes:\n    \"\"\"Save a memory store to disk\"\"\"\n    sanitized = self._sanitize_name(name)\n    if kb := self.memories.get(sanitized):\n        try:\n            return kb.save(path)\n        except Exception as e:\n            print(f\"Error saving memory: {str(e)}\")\n            return False\n    return False\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.PyEnvEval","title":"<code>PyEnvEval</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>class PyEnvEval:\n    def __init__(self):\n        self.local_env = locals().copy()\n        self.global_env = {'local_env': self.local_env}  # globals().copy()\n\n    def eval_code(self, code):\n        try:\n            exec(code, self.global_env, self.local_env)\n            result = eval(code, self.global_env, self.local_env)\n            return self.format_output(result)\n        except Exception as e:\n            return self.format_output(str(e))\n\n    def get_env(self):\n        local_env_str = self.format_env(self.local_env)\n        return f'Locals:\\n{local_env_str}'\n\n    @staticmethod\n    def format_output(output):\n        return f'Ergebnis: {output}'\n\n    @staticmethod\n    def format_env(env):\n        return '\\n'.join(f'{key}: {value}' for key, value in env.items())\n\n    def run_and_display(self, python_code):\n        \"\"\"function to eval python code\"\"\"\n        start = f'Start-state:\\n{self.get_env()}'\n        result = self.eval_code(python_code)\n        end = f'End-state:\\n{self.get_env()}'\n        return f'{start}\\nResult:\\n{result}\\n{end}'\n\n    def tool(self):\n        return {\"PythonEval\": {\"func\": self.run_and_display, \"description\": \"Use Python Code to Get to an Persis Answer! input must be valid python code all non code parts must be comments!\"}}\n</code></pre> <code>run_and_display(python_code)</code> \u00b6 <p>function to eval python code</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def run_and_display(self, python_code):\n    \"\"\"function to eval python code\"\"\"\n    start = f'Start-state:\\n{self.get_env()}'\n    result = self.eval_code(python_code)\n    end = f'End-state:\\n{self.get_env()}'\n    return f'{start}\\nResult:\\n{result}\\n{end}'\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.anything_from_str_to_dict","title":"<code>anything_from_str_to_dict(data, expected_keys=None, mini_task=lambda x: '')</code>","text":"<p>Versucht, einen String in ein oder mehrere Dictionaries umzuwandeln. Ber\u00fccksichtigt dabei die erwarteten Schl\u00fcssel und ihre Standardwerte.</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def anything_from_str_to_dict(data: str, expected_keys: dict = None, mini_task=lambda x: ''):\n    \"\"\"\n    Versucht, einen String in ein oder mehrere Dictionaries umzuwandeln.\n    Ber\u00fccksichtigt dabei die erwarteten Schl\u00fcssel und ihre Standardwerte.\n    \"\"\"\n    if len(data) &lt; 4:\n        return []\n\n    if expected_keys is None:\n        expected_keys = {}\n\n    result = []\n    json_objects = find_json_objects_in_str(data)\n    if not json_objects and data.startswith('[') and data.endswith(']'):\n        json_objects = eval(data)\n    if json_objects and len(json_objects) &gt; 0 and isinstance(json_objects[0], dict):\n        result.extend([{**expected_keys, **ob} for ob in json_objects])\n    if not result:\n        completed_object = complete_json_object(data, mini_task)\n        if completed_object is not None:\n            result.append(completed_object)\n    if len(result) == 0 and expected_keys:\n        result = [{list(expected_keys.keys())[0]: data}]\n    for res in result:\n        if isinstance(res, list) and len(res) &gt; 0:\n            res = res[0]\n        for key, value in expected_keys.items():\n            if key not in res:\n                res[key] = value\n\n    if len(result) == 0:\n        fixed = fix_json(data)\n        if fixed:\n            result.append(fixed)\n\n    return result\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.complete_json_object","title":"<code>complete_json_object(data, mini_task)</code>","text":"<p>Ruft eine Funktion auf, um einen String in das richtige Format zu bringen. Gibt das resultierende JSON-Objekt zur\u00fcck, wenn die Funktion erfolgreich ist, sonst None.</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def complete_json_object(data: str, mini_task):\n    \"\"\"\n    Ruft eine Funktion auf, um einen String in das richtige Format zu bringen.\n    Gibt das resultierende JSON-Objekt zur\u00fcck, wenn die Funktion erfolgreich ist, sonst None.\n    \"\"\"\n    ret = mini_task(\n        f\"Vervollst\u00e4ndige das Json Object. Und bringe den string in das Richtige format. data={data}\\nJson=\")\n    if ret:\n        return anything_from_str_to_dict(ret)\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.find_json_objects_in_str","title":"<code>find_json_objects_in_str(data)</code>","text":"<p>Sucht nach JSON-Objekten innerhalb eines Strings. Gibt eine Liste von JSON-Objekten zur\u00fcck, die im String gefunden wurden.</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def find_json_objects_in_str(data: str):\n    \"\"\"\n    Sucht nach JSON-Objekten innerhalb eines Strings.\n    Gibt eine Liste von JSON-Objekten zur\u00fcck, die im String gefunden wurden.\n    \"\"\"\n    json_objects = extract_json_objects(data)\n    if not isinstance(json_objects, list):\n        json_objects = [json_objects]\n    return [get_json_from_json_str(ob, 10) for ob in json_objects if get_json_from_json_str(ob, 10) is not None]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.get_json_from_json_str","title":"<code>get_json_from_json_str(json_str, repeat=1)</code>","text":"<p>Versucht, einen JSON-String in ein Python-Objekt umzuwandeln.</p> <p>Wenn beim Parsen ein Fehler auftritt, versucht die Funktion, das Problem zu beheben, indem sie das Zeichen an der Position des Fehlers durch ein Escape-Zeichen ersetzt. Dieser Vorgang wird bis zu <code>repeat</code>-mal wiederholt.</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str or list or dict</code> <p>Der JSON-String, der geparst werden soll.</p> required <code>repeat</code> <code>int</code> <p>Die Anzahl der Versuche, das Parsen durchzuf\u00fchren.</p> <code>1</code> <p>Returns:</p> Type Description <code>dict or None</code> <p>Das resultierende Python-Objekt.</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def get_json_from_json_str(json_str: str or list or dict, repeat: int = 1) -&gt; dict or None:\n    \"\"\"Versucht, einen JSON-String in ein Python-Objekt umzuwandeln.\n\n    Wenn beim Parsen ein Fehler auftritt, versucht die Funktion, das Problem zu beheben,\n    indem sie das Zeichen an der Position des Fehlers durch ein Escape-Zeichen ersetzt.\n    Dieser Vorgang wird bis zu `repeat`-mal wiederholt.\n\n    Args:\n        json_str: Der JSON-String, der geparst werden soll.\n        repeat: Die Anzahl der Versuche, das Parsen durchzuf\u00fchren.\n\n    Returns:\n        Das resultierende Python-Objekt.\n    \"\"\"\n    for _ in range(repeat):\n        try:\n            return parse_json_with_auto_detection(json_str)\n        except json.JSONDecodeError as e:\n            unexp = int(re.findall(r'\\(char (\\d+)\\)', str(e))[0])\n            unesc = json_str.rfind(r'\"', 0, unexp)\n            json_str = json_str[:unesc] + r'\\\"' + json_str[unesc + 1:]\n            closg = json_str.find(r'\"', unesc + 2)\n            json_str = json_str[:closg] + r'\\\"' + json_str[closg + 1:]\n        new = fix_json_object(json_str)\n        if new is not None:\n            json_str = new\n    get_logger().info(f\"Unable to parse JSON string after {json_str}\")\n    return None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.AgentUtils.parse_json_with_auto_detection","title":"<code>parse_json_with_auto_detection(json_data)</code>","text":"<p>Parses JSON data, automatically detecting if a value is a JSON string and parsing it accordingly. If a value cannot be parsed as JSON, it is returned as is.</p> Source code in <code>toolboxv2/mods/isaa/base/AgentUtils.py</code> <pre><code>def parse_json_with_auto_detection(json_data):\n    \"\"\"\n    Parses JSON data, automatically detecting if a value is a JSON string and parsing it accordingly.\n    If a value cannot be parsed as JSON, it is returned as is.\n    \"\"\"\n\n    def try_parse_json(value):\n        \"\"\"\n        Tries to parse a value as JSON. If the parsing fails, the original value is returned.\n        \"\"\"\n        try:\n            # print(\"parse_json_with_auto_detection:\", type(value), value)\n            parsed_value = json.loads(value)\n            # print(\"parsed_value:\", type(parsed_value), parsed_value)\n            # If the parsed value is a string, it might be a JSON string, so we try to parse it again\n            if isinstance(parsed_value, str):\n                return eval(parsed_value)\n            else:\n                return parsed_value\n        except Exception:\n            # logging.warning(f\"Failed to parse value as JSON: {value}. Exception: {e}\")\n            return value\n\n    get_logger()\n\n    if isinstance(json_data, dict):\n        return {key: parse_json_with_auto_detection(value) for key, value in json_data.items()}\n    elif isinstance(json_data, list):\n        return [parse_json_with_auto_detection(item) for item in json_data]\n    else:\n        return try_parse_json(json_data)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase","title":"<code>KnowledgeBase</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.Chunk","title":"<code>Chunk</code>  <code>dataclass</code>","text":"<p>Represents a chunk of text with its embedding and metadata</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>@dataclass(slots=True)\nclass Chunk:\n    \"\"\"Represents a chunk of text with its embedding and metadata\"\"\"\n    text: str\n    embedding: np.ndarray\n    metadata: dict[str, Any]\n    content_hash: str\n    cluster_id: int | None = None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.ConceptAnalysis","title":"<code>ConceptAnalysis</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the analysis of key concepts.</p> <p>Attributes:</p> Name Type Description <code>key_concepts</code> <code>list[str]</code> <p>A list of primary key concepts identified.</p> <code>relationships</code> <code>list[str]</code> <p>A list of relationships between the identified key concepts.</p> <code>importance_hierarchy</code> <code>list[str]</code> <p>A list that represents the hierarchical importance of the key concepts.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class ConceptAnalysis(BaseModel):\n    \"\"\"\n    Represents the analysis of key concepts.\n\n    Attributes:\n        key_concepts (list[str]): A list of primary key concepts identified.\n        relationships (list[str]): A list of relationships between the identified key concepts.\n        importance_hierarchy (list[str]): A list that represents the hierarchical importance of the key concepts.\n    \"\"\"\n    key_concepts: list[str]\n    relationships: list[str]\n    importance_hierarchy: list[str]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.ConceptExtractor","title":"<code>ConceptExtractor</code>","text":"<p>Handles extraction of concepts and relationships from text</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class ConceptExtractor:\n    \"\"\"Handles extraction of concepts and relationships from text\"\"\"\n\n    def __init__(self, knowledge_base, requests_per_second = 85.):\n        self.kb = knowledge_base\n        self.concept_graph = ConceptGraph()\n        self.requests_per_second = requests_per_second\n\n    async def extract_concepts(self, texts: list[str], metadatas: list[dict[str, Any]]) -&gt; list[list[Concept]]:\n        \"\"\"\n        Extract concepts from texts using concurrent processing with rate limiting.\n        Requests are made at the specified rate while responses are processed asynchronously.\n        \"\"\"\n        # Ensure metadatas list matches texts length\n        metadatas = metadatas + [{}] * (len(texts) - len(metadatas))\n\n        # Initialize rate limiter\n        rate_limiter = DynamicRateLimiter()\n\n        system_prompt = (\n            \"Analyze the given text and extract key concepts and their relationships. For each concept:\\n\"\n            \"1. Identify the concept name and category (technical, domain, method, property, ...)\\n\"\n            \"2. Determine relationships with other concepts (uses, part_of, similar_to, depends_on, ...)\\n\"\n            \"3. Assess importance (0-1 score) based on centrality to the text\\n\"\n            \"4. Extract relevant context snippets\\n\"\n            \"5. Max 5 Concepts!\\n\"\n            \"only return in json format!\\n\"\n            \"\"\"{\"concepts\": [{\n                \"name\": \"concept_name\",\n                \"category\": \"category_name\",\n                \"relationships\": {\n                    \"relationship_type\": [\"related_concept1\", \"related_concept2\"]\n                },\n                \"importance_score\": 0.0,\n                \"context_snippets\": [\"relevant text snippet\"]\n            }]}\\n\"\"\"\n        )\n\n        # Prepare all requests\n        requests = [\n            (idx, f\"Text to Convert in to JSON structure:\\n{text}\", system_prompt, metadata)\n            for idx, (text, metadata) in enumerate(zip(texts, metadatas, strict=False))\n        ]\n\n        async def process_single_request(idx: int, prompt: str, system_prompt: str, metadata: dict[str, Any]):\n            \"\"\"Process a single request with rate limiting\"\"\"\n            try:\n                # Wait for rate limit\n                await rate_limiter.acquire()\n                i__[1] += 1\n                # Make API call without awaiting the response\n                response_future = litellm_complete(\n                    prompt=prompt,\n                    system_prompt=system_prompt,\n                    response_format=Concepts,\n                    model_name=self.kb.model_name,\n                    fallbacks=[\"groq/gemma2-9b-it\"] +\n                              [m for m in os.getenv(\"FALLBACKS_MODELS_PREM\", '').split(',') if m]\n                )\n\n                return idx, response_future\n\n            except Exception as e:\n                print(f\"Error initiating request {idx}: {str(e)}\")\n                return idx, None\n\n        async def process_response(idx: int, response_future) -&gt; list[Concept]:\n            \"\"\"Process the response once it's ready\"\"\"\n            try:\n                if response_future is None:\n                    return []\n\n                response = await response_future\n                return await self._process_response(response, metadatas[idx])\n\n            except Exception as e:\n                print(f\"Error processing response {idx}: {str(e)}\")\n                return []\n\n        # Create tasks for all requests\n        request_tasks = []\n        batch_size = self.kb.batch_size\n\n        rate_limiter.update_rate(self.requests_per_second)\n\n        for batch_start in range(0, len(requests), batch_size):\n            batch = requests[batch_start:batch_start + batch_size]\n\n            # Create tasks for the batch\n            batch_tasks = [\n                process_single_request(idx, prompt, sys_prompt, meta)\n                for idx, prompt, sys_prompt, meta in batch\n            ]\n            request_tasks.extend(batch_tasks)\n\n        # Execute all requests with rate limiting\n        request_results = await asyncio.gather(*request_tasks)\n\n        # Process responses as they complete\n        response_tasks = [\n            process_response(idx, response_future)\n            for idx, response_future in request_results\n        ]\n\n        # Gather all results\n        all_results = await asyncio.gather(*response_tasks)\n\n        # Sort results by original index\n        sorted_results = [[] for _ in texts]\n        for idx, concepts in enumerate(all_results):\n            sorted_results[idx] = concepts\n\n        return sorted_results\n\n    async def _process_response(self, response: Any, metadata: dict[str, Any]) -&gt; list[Concept]:\n        \"\"\"Helper method to process a single response and convert it to Concepts\"\"\"\n        try:\n            # Extract content from response\n            if hasattr(response, 'choices'):\n                content = response.choices[0].message.content\n                if content is None:\n                    content = response.choices[0].message.tool_calls[0].function.arguments\n                if content is None:\n                    return []\n            elif isinstance(response, str):\n                content = response\n            else:\n                print(f\"Unexpected response type: {type(response)}\")\n                return []\n\n            # Parse JSON and create concepts\n            concept_data = after_format(content)\n            concepts = []\n\n            for concept_info in concept_data.get(\"concepts\", []):\n                concept = Concept(\n                    name=concept_info[\"name\"],\n                    category=concept_info.get(\"category\", \"N/A\"),\n                    relationships={k: set(v) for k, v in concept_info.get(\"relationships\", {}).items()},\n                    importance_score=concept_info.get(\"importance_score\", 0.1),\n                    context_snippets=concept_info.get(\"context_snippets\", \"N/A\"),\n                    metadata=metadata\n                )\n                concepts.append(concept)\n                self.concept_graph.add_concept(concept)\n\n            return concepts\n\n        except Exception:\n            i__[2] +=1\n            return []\n\n    async def process_chunks(self, chunks: list[Chunk]) -&gt; None:\n        \"\"\"\n        Process all chunks in batch to extract and store concepts.\n        Each chunk's metadata will be updated with the concept names and relationships.\n        \"\"\"\n        # Gather all texts from the chunks.\n        texts = [chunk.text for chunk in chunks]\n        # Call extract_concepts once with all texts.\n        all_concepts = await self.extract_concepts(texts, [chunk.metadata for chunk in chunks])\n\n        # Update each chunk's metadata with its corresponding concepts.\n        for chunk, concepts in zip(chunks, all_concepts, strict=False):\n            chunk.metadata[\"concepts\"] = [c.name for c in concepts]\n            chunk.metadata[\"concept_relationships\"] = {\n                c.name: {k: list(v) for k, v in c.relationships.items()}\n                for c in concepts\n            }\n\n    async def query_concepts(self, query: str) -&gt; dict[str, any]:\n        \"\"\"Query the concept graph based on natural language query\"\"\"\n\n        system_prompt = \"\"\"\n        Convert the natural language query about concepts into a structured format that specifies:\n        1. Main concepts of interest\n        2. Desired relationship types\n        3. Any category filters\n        4. Importance threshold\n\n        Format as JSON.\n        \"\"\"\n\n        prompt = f\"\"\"\n        Query: {query}\n\n        Convert to this JSON structure:\n        {{\n            \"target_concepts\": [\"concept1\", \"concept2\"],\n            \"relationship_types\": [\"type1\", \"type2\"],\n            \"categories\": [\"category1\", \"category2\"],\n            \"min_importance\": 0.0\n        }}\n        \"\"\"\n\n        try:\n            response = await litellm_complete(\n                model_name=self.kb.model_name,\n                prompt=prompt,\n                system_prompt=system_prompt,\n                response_format=TConcept\n            )\n\n            query_params = json.loads(response)\n\n            results = {\n                \"concepts\": {},\n                \"relationships\": [],\n                \"groups\": []\n            }\n\n            # Find matching concepts\n            for concept_name in query_params[\"target_concepts\"]:\n                if concept_name in self.concept_graph.concepts:\n                    concept = self.concept_graph.concepts[concept_name]\n                    if concept.importance_score &gt;= query_params[\"min_importance\"]:\n                        results[\"concepts\"][concept_name] = {\n                            \"category\": concept.category,\n                            \"importance\": concept.importance_score,\n                            \"context\": concept.context_snippets\n                        }\n\n                        # Get relationships\n                        for rel_type in query_params[\"relationship_types\"]:\n                            related = self.concept_graph.get_related_concepts(\n                                concept_name, rel_type\n                            )\n                            for related_concept in related:\n                                results[\"relationships\"].append({\n                                    \"from\": concept_name,\n                                    \"to\": related_concept,\n                                    \"type\": rel_type\n                                })\n\n            # Group concepts by category\n            category_groups = defaultdict(list)\n            for concept_name, concept_info in results[\"concepts\"].items():\n                category_groups[concept_info[\"category\"]].append(concept_name)\n            results[\"groups\"] = [\n                {\"category\": cat, \"concepts\": concepts}\n                for cat, concepts in category_groups.items()\n            ]\n\n            return results\n\n        except Exception as e:\n            print(f\"Error querying concepts: {str(e)}\")\n            return {\"concepts\": {}, \"relationships\": [], \"groups\": []}\n</code></pre> <code>extract_concepts(texts, metadatas)</code> <code>async</code> \u00b6 <p>Extract concepts from texts using concurrent processing with rate limiting. Requests are made at the specified rate while responses are processed asynchronously.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def extract_concepts(self, texts: list[str], metadatas: list[dict[str, Any]]) -&gt; list[list[Concept]]:\n    \"\"\"\n    Extract concepts from texts using concurrent processing with rate limiting.\n    Requests are made at the specified rate while responses are processed asynchronously.\n    \"\"\"\n    # Ensure metadatas list matches texts length\n    metadatas = metadatas + [{}] * (len(texts) - len(metadatas))\n\n    # Initialize rate limiter\n    rate_limiter = DynamicRateLimiter()\n\n    system_prompt = (\n        \"Analyze the given text and extract key concepts and their relationships. For each concept:\\n\"\n        \"1. Identify the concept name and category (technical, domain, method, property, ...)\\n\"\n        \"2. Determine relationships with other concepts (uses, part_of, similar_to, depends_on, ...)\\n\"\n        \"3. Assess importance (0-1 score) based on centrality to the text\\n\"\n        \"4. Extract relevant context snippets\\n\"\n        \"5. Max 5 Concepts!\\n\"\n        \"only return in json format!\\n\"\n        \"\"\"{\"concepts\": [{\n            \"name\": \"concept_name\",\n            \"category\": \"category_name\",\n            \"relationships\": {\n                \"relationship_type\": [\"related_concept1\", \"related_concept2\"]\n            },\n            \"importance_score\": 0.0,\n            \"context_snippets\": [\"relevant text snippet\"]\n        }]}\\n\"\"\"\n    )\n\n    # Prepare all requests\n    requests = [\n        (idx, f\"Text to Convert in to JSON structure:\\n{text}\", system_prompt, metadata)\n        for idx, (text, metadata) in enumerate(zip(texts, metadatas, strict=False))\n    ]\n\n    async def process_single_request(idx: int, prompt: str, system_prompt: str, metadata: dict[str, Any]):\n        \"\"\"Process a single request with rate limiting\"\"\"\n        try:\n            # Wait for rate limit\n            await rate_limiter.acquire()\n            i__[1] += 1\n            # Make API call without awaiting the response\n            response_future = litellm_complete(\n                prompt=prompt,\n                system_prompt=system_prompt,\n                response_format=Concepts,\n                model_name=self.kb.model_name,\n                fallbacks=[\"groq/gemma2-9b-it\"] +\n                          [m for m in os.getenv(\"FALLBACKS_MODELS_PREM\", '').split(',') if m]\n            )\n\n            return idx, response_future\n\n        except Exception as e:\n            print(f\"Error initiating request {idx}: {str(e)}\")\n            return idx, None\n\n    async def process_response(idx: int, response_future) -&gt; list[Concept]:\n        \"\"\"Process the response once it's ready\"\"\"\n        try:\n            if response_future is None:\n                return []\n\n            response = await response_future\n            return await self._process_response(response, metadatas[idx])\n\n        except Exception as e:\n            print(f\"Error processing response {idx}: {str(e)}\")\n            return []\n\n    # Create tasks for all requests\n    request_tasks = []\n    batch_size = self.kb.batch_size\n\n    rate_limiter.update_rate(self.requests_per_second)\n\n    for batch_start in range(0, len(requests), batch_size):\n        batch = requests[batch_start:batch_start + batch_size]\n\n        # Create tasks for the batch\n        batch_tasks = [\n            process_single_request(idx, prompt, sys_prompt, meta)\n            for idx, prompt, sys_prompt, meta in batch\n        ]\n        request_tasks.extend(batch_tasks)\n\n    # Execute all requests with rate limiting\n    request_results = await asyncio.gather(*request_tasks)\n\n    # Process responses as they complete\n    response_tasks = [\n        process_response(idx, response_future)\n        for idx, response_future in request_results\n    ]\n\n    # Gather all results\n    all_results = await asyncio.gather(*response_tasks)\n\n    # Sort results by original index\n    sorted_results = [[] for _ in texts]\n    for idx, concepts in enumerate(all_results):\n        sorted_results[idx] = concepts\n\n    return sorted_results\n</code></pre> <code>process_chunks(chunks)</code> <code>async</code> \u00b6 <p>Process all chunks in batch to extract and store concepts. Each chunk's metadata will be updated with the concept names and relationships.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def process_chunks(self, chunks: list[Chunk]) -&gt; None:\n    \"\"\"\n    Process all chunks in batch to extract and store concepts.\n    Each chunk's metadata will be updated with the concept names and relationships.\n    \"\"\"\n    # Gather all texts from the chunks.\n    texts = [chunk.text for chunk in chunks]\n    # Call extract_concepts once with all texts.\n    all_concepts = await self.extract_concepts(texts, [chunk.metadata for chunk in chunks])\n\n    # Update each chunk's metadata with its corresponding concepts.\n    for chunk, concepts in zip(chunks, all_concepts, strict=False):\n        chunk.metadata[\"concepts\"] = [c.name for c in concepts]\n        chunk.metadata[\"concept_relationships\"] = {\n            c.name: {k: list(v) for k, v in c.relationships.items()}\n            for c in concepts\n        }\n</code></pre> <code>query_concepts(query)</code> <code>async</code> \u00b6 <p>Query the concept graph based on natural language query</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def query_concepts(self, query: str) -&gt; dict[str, any]:\n    \"\"\"Query the concept graph based on natural language query\"\"\"\n\n    system_prompt = \"\"\"\n    Convert the natural language query about concepts into a structured format that specifies:\n    1. Main concepts of interest\n    2. Desired relationship types\n    3. Any category filters\n    4. Importance threshold\n\n    Format as JSON.\n    \"\"\"\n\n    prompt = f\"\"\"\n    Query: {query}\n\n    Convert to this JSON structure:\n    {{\n        \"target_concepts\": [\"concept1\", \"concept2\"],\n        \"relationship_types\": [\"type1\", \"type2\"],\n        \"categories\": [\"category1\", \"category2\"],\n        \"min_importance\": 0.0\n    }}\n    \"\"\"\n\n    try:\n        response = await litellm_complete(\n            model_name=self.kb.model_name,\n            prompt=prompt,\n            system_prompt=system_prompt,\n            response_format=TConcept\n        )\n\n        query_params = json.loads(response)\n\n        results = {\n            \"concepts\": {},\n            \"relationships\": [],\n            \"groups\": []\n        }\n\n        # Find matching concepts\n        for concept_name in query_params[\"target_concepts\"]:\n            if concept_name in self.concept_graph.concepts:\n                concept = self.concept_graph.concepts[concept_name]\n                if concept.importance_score &gt;= query_params[\"min_importance\"]:\n                    results[\"concepts\"][concept_name] = {\n                        \"category\": concept.category,\n                        \"importance\": concept.importance_score,\n                        \"context\": concept.context_snippets\n                    }\n\n                    # Get relationships\n                    for rel_type in query_params[\"relationship_types\"]:\n                        related = self.concept_graph.get_related_concepts(\n                            concept_name, rel_type\n                        )\n                        for related_concept in related:\n                            results[\"relationships\"].append({\n                                \"from\": concept_name,\n                                \"to\": related_concept,\n                                \"type\": rel_type\n                            })\n\n        # Group concepts by category\n        category_groups = defaultdict(list)\n        for concept_name, concept_info in results[\"concepts\"].items():\n            category_groups[concept_info[\"category\"]].append(concept_name)\n        results[\"groups\"] = [\n            {\"category\": cat, \"concepts\": concepts}\n            for cat, concepts in category_groups.items()\n        ]\n\n        return results\n\n    except Exception as e:\n        print(f\"Error querying concepts: {str(e)}\")\n        return {\"concepts\": {}, \"relationships\": [], \"groups\": []}\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.ConceptGraph","title":"<code>ConceptGraph</code>","text":"<p>Manages concept relationships and hierarchies</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class ConceptGraph:\n    \"\"\"Manages concept relationships and hierarchies\"\"\"\n\n    def __init__(self):\n        self.concepts: dict[str, Concept] = {}\n\n    def add_concept(self, concept: Concept):\n        \"\"\"Add or update a concept in the graph\"\"\"\n        if concept.name.lower() in self.concepts:\n            # Merge relationships and context\n            existing = self.concepts[concept.name.lower()]\n            for rel_type, related in concept.relationships.items():\n                if rel_type not in existing.relationships:\n                    existing.relationships[rel_type] = set()\n                existing.relationships[rel_type].update(related)\n            existing.context_snippets.extend(concept.context_snippets)\n            # Update importance score with rolling average\n            existing.importance_score = (existing.importance_score + concept.importance_score) / 2\n        else:\n            self.concepts[concept.name.lower()] = concept\n\n    def get_related_concepts(self, concept_name: str, relationship_type: str | None = None) -&gt; set[str]:\n        \"\"\"Get related concepts, optionally filtered by relationship type\"\"\"\n        if concept_name not in self.concepts:\n            return set()\n\n        concept = self.concepts[concept_name.lower()]\n        if relationship_type:\n            return concept.relationships.get(relationship_type, set())\n\n        related = set()\n        for relations in concept.relationships.values():\n            related.update(relations)\n        return related\n\n\n    def convert_to_networkx(self) -&gt; nx.DiGraph:\n        \"\"\"Convert ConceptGraph to NetworkX graph with layout\"\"\"\n        print(f\"Converting to NetworkX graph with {len(self.concepts.values())} concepts\")\n\n        G = nx.DiGraph()\n\n        if len(self.concepts.values()) == 0:\n            return G\n\n        for concept in self.concepts.values():\n            cks = '\\n - '.join(concept.context_snippets[:4])\n            G.add_node(\n                concept.name,\n                size=concept.importance_score * 10,\n                group=concept.category,\n                title=f\"\"\"\n                    {concept.name}\n                    Category: {concept.category}\n                    Importance: {concept.importance_score:.2f}\n                    Context: \\n - {cks}\n                    \"\"\"\n            )\n\n            for rel_type, targets in concept.relationships.items():\n                for target in targets:\n                    G.add_edge(concept.name, target, label=rel_type, title=rel_type)\n\n        return G\n</code></pre> <code>add_concept(concept)</code> \u00b6 <p>Add or update a concept in the graph</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def add_concept(self, concept: Concept):\n    \"\"\"Add or update a concept in the graph\"\"\"\n    if concept.name.lower() in self.concepts:\n        # Merge relationships and context\n        existing = self.concepts[concept.name.lower()]\n        for rel_type, related in concept.relationships.items():\n            if rel_type not in existing.relationships:\n                existing.relationships[rel_type] = set()\n            existing.relationships[rel_type].update(related)\n        existing.context_snippets.extend(concept.context_snippets)\n        # Update importance score with rolling average\n        existing.importance_score = (existing.importance_score + concept.importance_score) / 2\n    else:\n        self.concepts[concept.name.lower()] = concept\n</code></pre> <code>convert_to_networkx()</code> \u00b6 <p>Convert ConceptGraph to NetworkX graph with layout</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def convert_to_networkx(self) -&gt; nx.DiGraph:\n    \"\"\"Convert ConceptGraph to NetworkX graph with layout\"\"\"\n    print(f\"Converting to NetworkX graph with {len(self.concepts.values())} concepts\")\n\n    G = nx.DiGraph()\n\n    if len(self.concepts.values()) == 0:\n        return G\n\n    for concept in self.concepts.values():\n        cks = '\\n - '.join(concept.context_snippets[:4])\n        G.add_node(\n            concept.name,\n            size=concept.importance_score * 10,\n            group=concept.category,\n            title=f\"\"\"\n                {concept.name}\n                Category: {concept.category}\n                Importance: {concept.importance_score:.2f}\n                Context: \\n - {cks}\n                \"\"\"\n        )\n\n        for rel_type, targets in concept.relationships.items():\n            for target in targets:\n                G.add_edge(concept.name, target, label=rel_type, title=rel_type)\n\n    return G\n</code></pre> <code>get_related_concepts(concept_name, relationship_type=None)</code> \u00b6 <p>Get related concepts, optionally filtered by relationship type</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def get_related_concepts(self, concept_name: str, relationship_type: str | None = None) -&gt; set[str]:\n    \"\"\"Get related concepts, optionally filtered by relationship type\"\"\"\n    if concept_name not in self.concepts:\n        return set()\n\n    concept = self.concepts[concept_name.lower()]\n    if relationship_type:\n        return concept.relationships.get(relationship_type, set())\n\n    related = set()\n    for relations in concept.relationships.values():\n        related.update(relations)\n    return related\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.Concepts","title":"<code>Concepts</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a collection of key concepts.</p> <p>Attributes:</p> Name Type Description <code>concepts</code> <code>List[rConcept]</code> <p>A list of Concept instances, each representing an individual key concept.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class Concepts(BaseModel):\n    \"\"\"\n    Represents a collection of key concepts.\n\n    Attributes:\n        concepts (List[rConcept]): A list of Concept instances, each representing an individual key concept.\n    \"\"\"\n    concepts: list[rConcept]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.DataModel","title":"<code>DataModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The main data model that encapsulates the overall analysis.</p> <p>Attributes:</p> Name Type Description <code>main_summary</code> <code>str</code> <p>A Detailed overview summarizing the key findings and relations format MD string.</p> <code>concept_analysis</code> <code>ConceptAnalysis</code> <p>An instance containing the analysis of key concepts.</p> <code>topic_insights</code> <code>TopicInsights</code> <p>An instance containing insights regarding the topics.</p> <code>relevance_assessment</code> <code>RelevanceAssessment</code> <p>An instance assessing the relevance and alignment of the query.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class DataModel(BaseModel):\n    \"\"\"\n    The main data model that encapsulates the overall analysis.\n\n    Attributes:\n        main_summary (str): A Detailed overview summarizing the key findings and relations format MD string.\n        concept_analysis (ConceptAnalysis): An instance containing the analysis of key concepts.\n        topic_insights (TopicInsights): An instance containing insights regarding the topics.\n        relevance_assessment (RelevanceAssessment): An instance assessing the relevance and alignment of the query.\n    \"\"\"\n    main_summary: str\n    concept_analysis: ConceptAnalysis\n    topic_insights: TopicInsights\n    relevance_assessment: RelevanceAssessment\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.DynamicRateLimiter","title":"<code>DynamicRateLimiter</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class DynamicRateLimiter:\n    def __init__(self):\n        self.last_request_time = 0.0\n        self._lock = asyncio.Lock()\n\n    def update_rate(self, requests_per_second: float):\n        \"\"\"Update rate limit dynamically\"\"\"\n        self.min_interval = 1.0 / requests_per_second if requests_per_second &gt; 0 else float('inf')\n\n    async def acquire(self):\n        \"\"\"Acquire permission to make a request\"\"\"\n        async with self._lock:\n            current_time = time.time()\n            time_since_last = current_time - self.last_request_time\n            if time_since_last &lt; self.min_interval:\n                wait_time = self.min_interval - time_since_last\n                await asyncio.sleep(wait_time)\n            self.last_request_time = time.time()\n</code></pre> <code>acquire()</code> <code>async</code> \u00b6 <p>Acquire permission to make a request</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def acquire(self):\n    \"\"\"Acquire permission to make a request\"\"\"\n    async with self._lock:\n        current_time = time.time()\n        time_since_last = current_time - self.last_request_time\n        if time_since_last &lt; self.min_interval:\n            wait_time = self.min_interval - time_since_last\n            await asyncio.sleep(wait_time)\n        self.last_request_time = time.time()\n</code></pre> <code>update_rate(requests_per_second)</code> \u00b6 <p>Update rate limit dynamically</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def update_rate(self, requests_per_second: float):\n    \"\"\"Update rate limit dynamically\"\"\"\n    self.min_interval = 1.0 / requests_per_second if requests_per_second &gt; 0 else float('inf')\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.GraphVisualizer","title":"<code>GraphVisualizer</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class GraphVisualizer:\n    @staticmethod\n    def visualize(nx_graph: nx.DiGraph, output_file: str = \"concept_graph.html\", get_output=False):\n        \"\"\"Create interactive visualization using PyVis\"\"\"\n        from pyvis.network import Network\n        net = Network(\n            height=\"800px\",\n            width=\"100%\",\n            notebook=False,\n            directed=True,\n            bgcolor=\"#1a1a1a\",\n            font_color=\"white\"\n        )\n\n        net.from_nx(nx_graph)\n\n        net.save_graph(output_file)\n        print(f\"Graph saved to {output_file} Open in browser to view.\", len(nx_graph))\n        if get_output:\n            c = open(output_file, encoding=\"utf-8\").read()\n            os.remove(output_file)\n            return c\n</code></pre> <code>visualize(nx_graph, output_file='concept_graph.html', get_output=False)</code> <code>staticmethod</code> \u00b6 <p>Create interactive visualization using PyVis</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>@staticmethod\ndef visualize(nx_graph: nx.DiGraph, output_file: str = \"concept_graph.html\", get_output=False):\n    \"\"\"Create interactive visualization using PyVis\"\"\"\n    from pyvis.network import Network\n    net = Network(\n        height=\"800px\",\n        width=\"100%\",\n        notebook=False,\n        directed=True,\n        bgcolor=\"#1a1a1a\",\n        font_color=\"white\"\n    )\n\n    net.from_nx(nx_graph)\n\n    net.save_graph(output_file)\n    print(f\"Graph saved to {output_file} Open in browser to view.\", len(nx_graph))\n    if get_output:\n        c = open(output_file, encoding=\"utf-8\").read()\n        os.remove(output_file)\n        return c\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.KnowledgeBase","title":"<code>KnowledgeBase</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class KnowledgeBase:\n    def __init__(self, embedding_dim: int = 768, similarity_threshold: float = 0.61, batch_size: int = 64,\n                 n_clusters: int = 4, deduplication_threshold: float = 0.85, model_name=os.getenv(\"DEFAULTMODELSUMMERY\"),\n                 embedding_model=os.getenv(\"DEFAULTMODELEMBEDDING\"),\n                 vis_class:str | None = \"FastVectorStoreO\",\n                 vis_kwargs:dict[str, Any] | None=None,\n                 requests_per_second=85.,\n                 chunk_size: int = 3600,\n                 chunk_overlap: int = 130,\n                 separator: str = \"\\n\"\n                 ):\n        \"\"\"Initialize the knowledge base with given parameters\"\"\"\n\n        self.existing_hashes: set[str] = set()\n        self.embedding_model = embedding_model\n        self.embedding_dim = embedding_dim\n        self.similarity_threshold = similarity_threshold\n        self.deduplication_threshold = deduplication_threshold\n        if model_name == \"openrouter/mistralai/mistral-nemo\":\n            batch_size = 9\n            requests_per_second = 1.5\n        self.batch_size = batch_size\n        self.n_clusters = n_clusters\n        self.model_name = model_name\n        self.sto: list = []\n\n        self.text_splitter = TextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap, separator=separator)\n        self.similarity_graph = {}\n        self.concept_extractor = ConceptExtractor(self, requests_per_second)\n\n        self.vis_class = None\n        self.vis_kwargs = None\n        self.vdb = None\n        self.init_vis(vis_class, vis_kwargs)\n\n    def init_vis(self, vis_class, vis_kwargs):\n        if vis_class is None:\n            vis_class = \"FaissVectorStore\"\n        if vis_class == \"FaissVectorStore\":\n            if vis_kwargs is None:\n                vis_kwargs = {\n                    \"dimension\": self.embedding_dim\n                }\n            self.vdb = FaissVectorStore(**vis_kwargs)\n        else:\n            from toolboxv2.mods.isaa.base.VectorStores.taichiNumpyNumbaVectorStores import (\n                EnhancedVectorStore,\n                FastVectorStore1,\n                FastVectorStoreO,\n                NumpyVectorStore,\n                VectorStoreConfig,\n            )\n        if vis_class == \"FastVectorStoreO\":\n            if vis_kwargs is None:\n                vis_kwargs = {\n                    \"embedding_size\": self.embedding_dim\n                }\n            self.vdb = FastVectorStoreO(**vis_kwargs)\n        if vis_class == \"EnhancedVectorStore\":\n            if vis_kwargs is None:\n                vis_kwargs = {\n                    \"dimension\": self.embedding_dim\n                }\n            vis_kwargs = VectorStoreConfig(**vis_kwargs)\n            self.vdb = EnhancedVectorStore(vis_kwargs)\n        if vis_class == \"FastVectorStore1\":\n            self.vdb = FastVectorStore1()\n        if vis_class == \"NumpyVectorStore\":\n            self.vdb = NumpyVectorStore()\n\n        self.vis_class = vis_class\n        self.vis_kwargs = vis_kwargs\n\n\n    @staticmethod\n    def compute_hash(text: str) -&gt; str:\n        \"\"\"Compute SHA-256 hash of text\"\"\"\n        return hashlib.sha256(text.encode('utf-8', errors='ignore')).hexdigest()\n\n    async def _get_embeddings(self, texts: list[str]) -&gt; np.ndarray:\n        \"\"\"Get normalized embeddings in batches\"\"\"\n        try:\n            async def process_batch(batch: list[str]) -&gt; np.ndarray:\n                from toolboxv2.mods.isaa.extras.adapter import litellm_embed\n                # print(\"Processing\", batch)\n                embeddings = await litellm_embed(texts=batch, model=self.embedding_model)\n                return normalize_vectors(embeddings)\n\n            tasks = []\n            for i in range(0, len(texts), self.batch_size):\n                batch = texts[i:i + self.batch_size]\n                tasks.append(process_batch(batch))\n\n            embeddings = await asyncio.gather(*tasks)\n            i__[0] += len(texts)\n            return np.vstack(embeddings)\n        except Exception as e:\n            get_logger().error(f\"Error generating embeddings: {str(e)}\")\n            raise\n\n\n\n    def _remove_similar_chunks(self, threshold: float = None) -&gt; int:\n        \"\"\"Remove chunks that are too similar to each other\"\"\"\n        if len(self.vdb.chunks) &lt; 2:\n            return 0\n\n        if threshold is None:\n            threshold = self.deduplication_threshold\n\n        try:\n            # Get all embeddings\n            embeddings = np.vstack([c.embedding for c in self.vdb.chunks])\n            n = len(embeddings)\n\n            # Compute similarity matrix\n            similarities = np.dot(embeddings, embeddings.T)\n\n            # Create mask for chunks to keep\n            keep_mask = np.ones(n, dtype=bool)\n\n            # Iterate through chunks\n            for i in range(n):\n                if not keep_mask[i]:\n                    continue\n\n                # Find chunks that are too similar to current chunk\n                similar_indices = similarities[i] &gt;= threshold\n                similar_indices[i] = False  # Don't count self-similarity\n\n                # Mark similar chunks for removal\n                keep_mask[similar_indices] = False\n\n            # Keep only unique chunks\n            unique_chunks = [chunk for chunk, keep in zip(self.vdb.chunks, keep_mask, strict=False) if keep]\n            removed_count = len(self.vdb.chunks) - len(unique_chunks)\n\n            # Update chunks and hashes\n            self.vdb.chunks = unique_chunks\n            self.existing_hashes = {chunk.content_hash for chunk in self.vdb.chunks}\n\n            # Rebuild index if chunks were removed\n            if removed_count &gt; 0:\n                self.vdb.rebuild_index()\n\n\n            return removed_count\n\n        except Exception as e:\n            get_logger().error(f\"Error removing similar chunks: {str(e)}\")\n            raise\n\n    async def _add_data(\n        self,\n        texts: list[str],\n        metadata: list[dict[str, Any]] | None= None,\n    ) -&gt; tuple[int, int]:\n        \"\"\"\n        Process and add new data to the knowledge base\n        Returns: Tuple of (added_count, duplicate_count)\n        \"\"\"\n        if len(texts) == 0:\n            return -1, -1\n        try:\n            # Compute hashes and filter exact duplicates\n            hashes = [self.compute_hash(text) for text in texts]\n            unique_data = []\n            for t, m, h in zip(texts, metadata, hashes, strict=False):\n                if h in self.existing_hashes:\n                    continue\n                # Update existing hashes\n                self.existing_hashes.add(h)\n                unique_data.append((t, m, h))\n\n            if not unique_data:\n                return 0, len(texts)\n\n            # Get embeddings\n            embeddings = await self._get_embeddings(texts)\n\n            texts = []\n            metadata = []\n            hashes = []\n            embeddings_final = []\n            if len(self.vdb.chunks):\n                for i, d in enumerate(unique_data):\n                    c = self.vdb.search(embeddings[i], 5, self.deduplication_threshold)\n                    if len(c) &gt; 2:\n                        continue\n                    t, m, h = d\n                    texts.append(t)\n                    metadata.append(m)\n                    hashes.append(h)\n                    embeddings_final.append(embeddings[i])\n\n            else:\n                texts , metadata, hashes = zip(*unique_data, strict=False)\n                embeddings_final = embeddings\n\n            if not texts:  # All were similar to existing chunks\n                return 0, len(unique_data)\n\n            # Create and add new chunks\n            new_chunks = [\n                Chunk(text=t, embedding=e, metadata=m, content_hash=h)\n                for t, e, m, h in zip(texts, embeddings_final, metadata, hashes, strict=False)\n            ]\n\n            # Add new chunks\n            # Update index\n            if new_chunks:\n                all_embeddings = np.vstack([c.embedding for c in new_chunks])\n                self.vdb.add_embeddings(all_embeddings, new_chunks)\n\n            # Remove similar chunks from the entire collection\n            removed = self._remove_similar_chunks()\n            get_logger().info(f\"Removed {removed} similar chunks during deduplication\")\n            # Invalidate visualization cache\n\n            if len(new_chunks) - removed &gt; 0:\n                # Process new chunks for concepts\n                await self.concept_extractor.process_chunks(new_chunks)\n            print(\"[total, calls, errors]\", i__)\n\n            return len(new_chunks) - removed, len(texts) - len(new_chunks) + removed\n\n        except Exception as e:\n            get_logger().error(f\"Error adding data: {str(e)}\")\n            raise\n\n\n    async def add_data(\n        self,\n        texts: list[str],\n        metadata: list[dict[str, Any]] | None = None, direct:bool = False\n    ) -&gt; tuple[int, int]:\n        \"\"\"Enhanced version with smart splitting and clustering\"\"\"\n        if isinstance(texts, str):\n            texts = [texts]\n        if metadata is None:\n            metadata = [{}] * len(texts)\n        if isinstance(metadata, dict):\n            metadata = [metadata]\n        if len(texts) != len(metadata):\n            raise ValueError(\"Length of texts and metadata must match\")\n\n        if not direct:\n            if len(texts) == 1 and len(texts[0]) &lt; 10_000:\n                if len(self.sto) &lt; self.batch_size and len(texts) == 1:\n                    self.sto.append((texts[0], metadata[0]))\n                    return -1, -1\n                if len(self.sto) &gt;= self.batch_size:\n                    _ = [texts.append(t) or metadata.append([m]) for (t, m) in self.sto]\n                    self.sto = []\n\n        # Split large texts\n        split_texts = []\n        split_metadata = []\n\n        while Spinner(\"Saving Data to Memory\", symbols='t'):\n\n            for idx, text in enumerate(texts):\n                chunks = self.text_splitter.split_text(text)\n                split_texts.extend(chunks)\n\n                # Adjust metadata for splits\n                meta = metadata[idx] if metadata else {}\n                if isinstance(meta, list):\n                    meta = meta[0]\n                for i, _chunk in enumerate(chunks):\n                    chunk_meta = meta.copy()\n                    chunk_meta.update({\n                        'chunk_index': i,\n                        'total_chunks': len(chunks),\n                        'original_text_id': idx\n                    })\n                    split_metadata.append(chunk_meta)\n\n            return await self._add_data(split_texts, split_metadata)\n\n    def _update_similarity_graph(self, embeddings: np.ndarray, chunk_ids: list[int]):\n        \"\"\"Update similarity graph for connected information detection\"\"\"\n        similarities = np.dot(embeddings, embeddings.T)\n\n        for i in range(len(chunk_ids)):\n            for j in range(i + 1, len(chunk_ids)):\n                if similarities[i, j] &gt;= self.similarity_threshold:\n                    id1, id2 = chunk_ids[i], chunk_ids[j]\n                    if id1 not in self.similarity_graph:\n                        self.similarity_graph[id1] = set()\n                    if id2 not in self.similarity_graph:\n                        self.similarity_graph[id2] = set()\n                    self.similarity_graph[id1].add(id2)\n                    self.similarity_graph[id2].add(id1)\n\n    async def retrieve(\n        self,\n        query: str=\"\",\n        query_embedding: np.ndarray | None = None,\n        k: int = 5,\n        min_similarity: float = 0.2,\n        include_connected: bool = True\n    ) -&gt; list[Chunk]:\n        \"\"\"Enhanced retrieval with connected information\"\"\"\n        if query_embedding is None:\n            query_embedding = (await self._get_embeddings([query]))[0]\n        k = min(k, len(self.vdb.chunks))\n        if k &lt;= 0:\n            return []\n        initial_results = self.vdb.search(query_embedding, k, min_similarity)\n\n        if not include_connected or not initial_results:\n            return initial_results\n\n        # Find connected chunks\n        connected_chunks = set()\n        for chunk in initial_results:\n            chunk_id = self.vdb.chunks.index(chunk)\n            if chunk_id in self.similarity_graph:\n                connected_chunks.update(self.similarity_graph[chunk_id])\n\n        # Add connected chunks to results\n        all_chunks = self.vdb.chunks\n        additional_results = [all_chunks[i] for i in connected_chunks\n                              if all_chunks[i] not in initial_results]\n\n        # Sort by similarity to query\n        all_results = initial_results + additional_results\n\n        return sorted(\n            all_results,\n            key=lambda x: np.dot(x.embedding, query_embedding),\n            reverse=True\n        )[:k * 2]  # Return more results when including connected information\n\n    async def forget_irrelevant(self, irrelevant_concepts: list[str], similarity_threshold: float | None=None) -&gt; int:\n        \"\"\"\n        Remove chunks similar to irrelevant concepts\n        Returns: Number of chunks removed\n        \"\"\"\n        if not irrelevant_concepts:\n            return 0\n\n        if similarity_threshold is None:\n            similarity_threshold = self.similarity_threshold\n\n        try:\n            irrelevant_embeddings = await self._get_embeddings(irrelevant_concepts)\n            initial_count = len(self.vdb.chunks)\n\n            def is_relevant(chunk: Chunk) -&gt; bool:\n                similarities = np.dot(chunk.embedding, irrelevant_embeddings.T)\n                do_keep = np.max(similarities) &lt; similarity_threshold\n                if do_keep:\n                    return True\n                for c in chunk.metadata.get(\"concepts\", []):\n                    if c in self.concept_extractor.concept_graph.concepts:\n                        del self.concept_extractor.concept_graph.concepts[c]\n                return False\n\n            relevant_chunks = [chunk for chunk in self.vdb.chunks if is_relevant(chunk)]\n            self.vdb.chunks = relevant_chunks\n            self.existing_hashes = {chunk.content_hash for chunk in self.vdb.chunks}\n            self.vdb.rebuild_index()\n\n            return initial_count - len(self.vdb.chunks)\n\n        except Exception as e:\n            get_logger().error(f\"Error forgetting irrelevant concepts: {str(e)}\")\n            raise\n\n    ## ----------------------------------------------------------------\n\n    def _cluster_chunks(\n        self,\n        chunks: list[Chunk],\n        query_embedding: np.ndarray | None = None,\n        min_cluster_size: int = 2,\n        min_samples: int = 1,\n        max_clusters: int = 10\n    ) -&gt; dict[int, list[Chunk]]:\n        \"\"\"\n        Enhanced clustering of chunks into topics with query awareness\n        and dynamic parameter adjustment\n        \"\"\"\n        if len(chunks) &lt; 2:\n            return {0: chunks}\n\n        embeddings = np.vstack([chunk.embedding for chunk in chunks])\n\n        # Normalize embeddings for cosine similarity\n        embeddings = normalize_vectors(embeddings)\n\n        # If query is provided, weight embeddings by query relevance\n        if query_embedding is not None:\n            query_similarities = np.dot(embeddings, query_embedding)\n            # Apply soft weighting to maintain structure while considering query relevance\n            embeddings = embeddings * query_similarities[:, np.newaxis]\n            embeddings = normalize_vectors(embeddings)\n\n        # Dynamic parameter adjustment based on dataset size\n        adjusted_min_cluster_size = max(\n            min_cluster_size,\n            min(len(chunks) // 10, 5)  # Scale with data size, max 5\n        )\n\n        adjusted_min_samples = max(\n            min_samples,\n            adjusted_min_cluster_size // 2\n        )\n\n        # Try different parameter combinations for optimal clustering\n        best_clusters = None\n        best_score = float('-inf')\n\n        epsilon_range = [0.2, 0.3, 0.4]\n\n        for epsilon in epsilon_range:\n            clusterer = HDBSCAN(\n                min_cluster_size=adjusted_min_cluster_size,\n                min_samples=adjusted_min_samples,\n                metric='cosine',\n                cluster_selection_epsilon=epsilon\n            )\n\n            cluster_labels = clusterer.fit_predict(embeddings)\n\n            # Skip if all points are noise\n            if len(set(cluster_labels)) &lt;= 1:\n                continue\n\n            # Calculate clustering quality metrics\n            score = self._evaluate_clustering(\n                embeddings,\n                cluster_labels,\n                query_embedding\n            )\n\n            if score &gt; best_score:\n                best_score = score\n                best_clusters = cluster_labels\n\n        # If no good clustering found, fall back to simpler approach\n        if best_clusters is None:\n            return self._fallback_clustering(chunks, query_embedding)\n\n        # Organize chunks by cluster\n        clusters: dict[int, list[Chunk]] = {}\n\n        # Sort clusters by size and relevance\n        cluster_scores = []\n\n        for label in set(best_clusters):\n            if label == -1:  # Handle noise points separately\n                continue\n\n            # Fixed: Use boolean mask to select chunks for current cluster\n            cluster_mask = best_clusters == label\n            cluster_chunks = [chunk for chunk, is_in_cluster in zip(chunks, cluster_mask, strict=False) if is_in_cluster]\n\n            # Skip empty clusters\n            if not cluster_chunks:\n                continue\n\n            # Calculate cluster score based on size and query relevance\n            score = len(cluster_chunks)\n            if query_embedding is not None:\n                cluster_embeddings = np.vstack([c.embedding for c in cluster_chunks])\n                query_relevance = np.mean(np.dot(cluster_embeddings, query_embedding))\n                score = score * (1 + query_relevance)  # Boost by relevance\n\n            cluster_scores.append((label, score, cluster_chunks))\n\n        # Sort clusters by score and limit to max_clusters\n        cluster_scores.sort(key=lambda x: x[1], reverse=True)\n\n        # Assign cleaned clusters\n        for i, (_, _, cluster_chunks) in enumerate(cluster_scores[:max_clusters]):\n            clusters[i] = cluster_chunks\n\n        # Handle noise points by assigning to nearest cluster\n        noise_chunks = [chunk for chunk, label in zip(chunks, best_clusters, strict=False) if label == -1]\n        if noise_chunks:\n            self._assign_noise_points(noise_chunks, clusters, query_embedding)\n\n        return clusters\n\n    @staticmethod\n    def _evaluate_clustering(\n        embeddings: np.ndarray,\n        labels: np.ndarray,\n        query_embedding: np.ndarray | None = None\n    ) -&gt; float:\n        \"\"\"\n        Evaluate clustering quality using multiple metrics\n        \"\"\"\n        if len(set(labels)) &lt;= 1:\n            return float('-inf')\n\n        # Calculate silhouette score for cluster cohesion\n        from sklearn.metrics import silhouette_score\n        try:\n            sil_score = silhouette_score(embeddings, labels, metric='cosine')\n        except:\n            sil_score = -1\n\n        # Calculate Davies-Bouldin score for cluster separation\n        from sklearn.metrics import davies_bouldin_score\n        try:\n            db_score = -davies_bouldin_score(embeddings, labels)  # Negated as lower is better\n        except:\n            db_score = -1\n\n        # Calculate query relevance if provided\n        query_score = 0\n        if query_embedding is not None:\n            unique_labels = set(labels) - {-1}\n            if unique_labels:\n                query_sims = []\n                for label in unique_labels:\n                    cluster_mask = labels == label\n                    cluster_embeddings = embeddings[cluster_mask]\n                    cluster_centroid = np.mean(cluster_embeddings, axis=0)\n                    query_sims.append(np.dot(cluster_centroid, query_embedding))\n                query_score = np.mean(query_sims)\n\n        # Combine scores with weights\n        combined_score = (\n            0.4 * sil_score +\n            0.3 * db_score +\n            0.3 * query_score\n        )\n\n        return combined_score\n\n    @staticmethod\n    def _fallback_clustering(\n        chunks: list[Chunk],\n        query_embedding: np.ndarray | None = None\n    ) -&gt; dict[int, list[Chunk]]:\n        \"\"\"\n        Simple fallback clustering when HDBSCAN fails\n        \"\"\"\n        if query_embedding is not None:\n            # Sort by query relevance\n            chunks_with_scores = [\n                (chunk, np.dot(chunk.embedding, query_embedding))\n                for chunk in chunks\n            ]\n            chunks_with_scores.sort(key=lambda x: x[1], reverse=True)\n            chunks = [c for c, _ in chunks_with_scores]\n\n        # Create fixed-size clusters\n        clusters = {}\n        cluster_size = max(2, len(chunks) // 5)\n\n        for i in range(0, len(chunks), cluster_size):\n            clusters[len(clusters)] = chunks[i:i + cluster_size]\n\n        return clusters\n\n    @staticmethod\n    def _assign_noise_points(\n        noise_chunks: list[Chunk],\n        clusters: dict[int, list[Chunk]],\n        query_embedding: np.ndarray | None = None\n    ) -&gt; None:\n        \"\"\"\n        Assign noise points to nearest clusters\n        \"\"\"\n        if not clusters:\n            clusters[0] = noise_chunks\n            return\n\n        for chunk in noise_chunks:\n            best_cluster = None\n            best_similarity = float('-inf')\n\n            for cluster_id, cluster_chunks in clusters.items():\n                cluster_embeddings = np.vstack([c.embedding for c in cluster_chunks])\n                cluster_centroid = np.mean(cluster_embeddings, axis=0)\n\n                similarity = np.dot(chunk.embedding, cluster_centroid)\n\n                # Consider query relevance in assignment if available\n                if query_embedding is not None:\n                    query_sim = np.dot(chunk.embedding, query_embedding)\n                    similarity = 0.7 * similarity + 0.3 * query_sim\n\n                if similarity &gt; best_similarity:\n                    best_similarity = similarity\n                    best_cluster = cluster_id\n\n            if best_cluster is not None:\n                clusters[best_cluster].append(chunk)\n\n    @staticmethod\n    def _generate_topic_summary(\n        chunks: list[Chunk],\n        query_embedding: np.ndarray,\n        max_sentences=3\n    ) -&gt; str:\n        \"\"\"Generate a summary for a topic using most representative chunks\"\"\"\n        if not chunks:\n            return \"\"\n\n        # Find chunks most similar to cluster centroid\n        embeddings = np.vstack([chunk.embedding for chunk in chunks])\n        centroid = embeddings.mean(axis=0)\n\n        # Calculate similarities to both centroid and query\n        centroid_sims = np.dot(embeddings, centroid)\n        query_sims = np.dot(embeddings, query_embedding)\n\n        # Combine both similarities\n        combined_sims = 0.7 * centroid_sims + 0.3 * query_sims\n\n        # Select top sentences from most representative chunks\n        top_indices = np.argsort(combined_sims)[-max_sentences:]\n        summary_chunks = [chunks[i] for i in top_indices]\n\n        # Extract key sentences\n        sentences = []\n        for chunk in summary_chunks:\n            sentences.extend(sent.strip() for sent in chunk.text.split('.') if sent.strip())\n\n        return '. '.join(sentences[:max_sentences]) + '.'\n\n    async def retrieve_with_overview(\n        self,\n        query: str,\n        query_embedding=None,\n        k: int = 5,\n        min_similarity: float = 0.2,\n        max_sentences: int = 5,\n        cross_ref_depth: int = 2,\n        max_cross_refs: int = 10  # New parameter to control cross-reference count\n    ) -&gt; RetrievalResult:\n        \"\"\"Enhanced retrieval with better cross-reference handling\"\"\"\n        # Get initial results with query embedding\n        if query_embedding is None:\n            query_embedding = (await self._get_embeddings([query]))[0]\n        initial_results = await self.retrieve(query_embedding=query_embedding, k=k, min_similarity=min_similarity)\n\n        if not initial_results:\n            return RetrievalResult([], [], {})\n\n        # Find cross-references with similarity scoring\n        initial_ids = {self.vdb.chunks.index(chunk) for chunk in initial_results}\n        related_ids = self._find_cross_references(\n            initial_ids,\n            depth=cross_ref_depth,\n            query_embedding=query_embedding  # Pass query embedding for relevance scoring\n        )\n\n        # Get all relevant chunks with smarter filtering\n        all_chunks = self.vdb.chunks\n        all_relevant_chunks = initial_results + [\n            chunk for i, chunk in enumerate(all_chunks)\n            if i in related_ids and self._is_relevant_cross_ref(\n                chunk,\n                query_embedding,\n                initial_results\n            )\n        ]\n\n        # Enhanced clustering with dynamic cluster size\n        clusters = self._cluster_chunks(\n            all_relevant_chunks,\n            query_embedding=query_embedding\n        )\n\n        # Fallback: If no clusters are found, treat all relevant chunks as a single cluster.\n        if not clusters:\n            print(\"No clusters found. Falling back to using all relevant chunks as a single cluster.\")\n            clusters = {0: all_relevant_chunks}\n\n        # Generate summaries and organize results\n        overview = []\n        cross_references = {}\n\n        for cluster_id, cluster_chunks in clusters.items():\n            summary = self._generate_topic_summary(\n                cluster_chunks,\n                query_embedding,\n                max_sentences=max_sentences  # Increased for more context\n            )\n\n            # Enhanced chunk sorting with combined scoring\n            sorted_chunks = self._sort_chunks_by_relevance(\n                cluster_chunks,\n                query_embedding,\n                initial_results\n            )\n\n            # Separate direct matches and cross-references\n            direct_matches_ = [{'text':c.text, 'metadata':c.metadata} for c in sorted_chunks if c in initial_results]\n            direct_matches = []\n            for match in direct_matches_:\n                if match in direct_matches:\n                    continue\n                direct_matches.append(match)\n            cross_refs_ = [c for c in sorted_chunks if c not in initial_results]\n            cross_refs = []\n            for match in cross_refs_:\n                if match in cross_refs:\n                    continue\n                cross_refs.append(match)\n            # Limit cross-references while maintaining diversity\n            selected_cross_refs = self._select_diverse_cross_refs(\n                cross_refs,\n                max_cross_refs,\n                query_embedding\n            )\n\n            topic_info = {\n                'topic_id': cluster_id,\n                'summary': summary,\n                'main_chunks': [x for x in direct_matches[:3]],\n                'chunk_count': len(cluster_chunks),\n                'relevance_score': self._calculate_topic_relevance(\n                    cluster_chunks,\n                    query_embedding\n                )\n            }\n            overview.append(topic_info)\n\n            if selected_cross_refs:\n                cross_references[f\"topic_{cluster_id}\"] = selected_cross_refs\n\n        # Sort overview by relevance score\n        overview.sort(key=lambda x: x['relevance_score'], reverse=True)\n\n        return RetrievalResult(\n            overview=overview,\n            details=initial_results,\n            cross_references=cross_references\n        )\n\n    def _find_cross_references(\n        self,\n        chunk_ids: set[int],\n        depth: int,\n        query_embedding: np.ndarray\n    ) -&gt; set[int]:\n        \"\"\"Enhanced cross-reference finding with relevance scoring\"\"\"\n        related_ids = set(chunk_ids)\n        current_depth = 0\n        frontier = set(chunk_ids)\n\n        while current_depth &lt; depth and frontier:\n            new_frontier = set()\n            for chunk_id in frontier:\n                if chunk_id in self.similarity_graph:\n                    # Score potential cross-references by relevance\n                    candidates = self.similarity_graph[chunk_id] - related_ids\n                    scored_candidates = [\n                        (cid, self._calculate_topic_relevance(\n                            [self.vdb.chunks[cid]],\n                            query_embedding\n                        ))\n                        for cid in candidates\n                    ]\n\n                    # Filter by relevance threshold\n                    relevant_candidates = {\n                        cid for cid, score in scored_candidates\n                        if score &gt; 0.5  # Adjustable threshold\n                    }\n                    new_frontier.update(relevant_candidates)\n\n            related_ids.update(new_frontier)\n            frontier = new_frontier\n            current_depth += 1\n\n        return related_ids\n\n    @staticmethod\n    def _is_relevant_cross_ref(\n        chunk: Chunk,\n        query_embedding: np.ndarray,\n        initial_results: list[Chunk]\n    ) -&gt; bool:\n        \"\"\"Determine if a cross-reference is relevant enough to include\"\"\"\n        # Calculate similarity to query\n        query_similarity = np.dot(chunk.embedding, query_embedding)\n\n        # Calculate similarity to initial results\n        initial_similarities = [\n            np.dot(chunk.embedding, r.embedding) for r in initial_results\n        ]\n        max_initial_similarity = max(initial_similarities)\n\n        # Combined relevance score\n        relevance_score = 0.7 * query_similarity + 0.3 * max_initial_similarity\n\n        return relevance_score &gt; 0.6  # Adjustable threshold\n\n    @staticmethod\n    def _select_diverse_cross_refs(\n        cross_refs: list[Chunk],\n        max_count: int,\n        query_embedding: np.ndarray\n    ) -&gt; list[Chunk]:\n        \"\"\"Select diverse and relevant cross-references\"\"\"\n        if not cross_refs or len(cross_refs) &lt;= max_count:\n            return cross_refs\n\n        # Calculate diversity scores\n        embeddings = np.vstack([c.embedding for c in cross_refs])\n        similarities = np.dot(embeddings, embeddings.T)\n\n        selected = []\n        remaining = list(enumerate(cross_refs))\n\n        while len(selected) &lt; max_count and remaining:\n            # Score remaining chunks by relevance and diversity\n            scores = []\n            for idx, chunk in remaining:\n                relevance = np.dot(chunk.embedding, query_embedding)\n                diversity = 1.0\n                if selected:\n                    # Calculate diversity penalty based on similarity to selected chunks\n                    selected_similarities = [\n                        similarities[idx][list(cross_refs).index(s)]\n                        for s in selected\n                    ]\n                    diversity = 1.0 - max(selected_similarities)\n\n                combined_score = 0.7 * relevance + 0.3 * diversity\n                scores.append((combined_score, idx, chunk))\n\n            # Select the highest scoring chunk\n            scores.sort(reverse=True)\n            _, idx, chunk = scores[0]\n            selected.append(chunk)\n            remaining = [(i, c) for i, c in remaining if i != idx]\n\n        return selected\n\n    @staticmethod\n    def _calculate_topic_relevance(\n        chunks: list[Chunk],\n        query_embedding: np.ndarray,\n    ) -&gt; float:\n        \"\"\"Calculate overall topic relevance score\"\"\"\n        if not chunks:\n            return 0.0\n\n        similarities = [\n            np.dot(chunk.embedding, query_embedding) for chunk in chunks\n        ]\n        return np.mean(similarities)\n\n    @staticmethod\n    def _sort_chunks_by_relevance(\n        chunks: list[Chunk],\n        query_embedding: np.ndarray,\n        initial_results: list[Chunk]\n    ) -&gt; list[Chunk]:\n        \"\"\"Sort chunks by combined relevance score\"\"\"\n        scored_chunks = []\n        for chunk in chunks:\n            query_similarity = np.dot(chunk.embedding, query_embedding)\n            initial_similarities = [\n                np.dot(chunk.embedding, r.embedding)\n                for r in initial_results\n            ]\n            max_initial_similarity = max(initial_similarities) if initial_similarities else 0\n\n            # Combined score favoring query relevance\n            combined_score = 0.7 * query_similarity + 0.3 * max_initial_similarity\n            scored_chunks.append((combined_score, chunk))\n\n        scored_chunks.sort(reverse=True)\n        return [chunk for _, chunk in scored_chunks]\n\n    async def query_concepts(self, query: str) -&gt; dict[str, any]:\n        \"\"\"Query concepts extracted from the knowledge base\"\"\"\n        return await self.concept_extractor.query_concepts(query)\n\n    async def unified_retrieve(\n        self,\n        query: str,\n        k: int = 5,\n        min_similarity: float = 0.2,\n        cross_ref_depth: int = 2,\n        max_cross_refs: int = 10,\n        max_sentences: int = 10\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Unified retrieval function that combines concept querying, retrieval with overview,\n        and basic retrieval, then generates a comprehensive summary using LLM.\n\n        Args:\n            query: Search query string\n            k: Number of primary results to retrieve\n            min_similarity: Minimum similarity threshold for retrieval\n            cross_ref_depth: Depth for cross-reference search\n            max_cross_refs: Maximum number of cross-references per topic\n            max_sentences: Maximum number Sentences in the main summary text\n\n        Returns:\n            Dictionary containing comprehensive results including summary and details\n        \"\"\"\n        # Get concept information\n        concept_results = await self.concept_extractor.query_concepts(query)\n\n        # Get retrieval overview\n\n        query_embedding = (await self._get_embeddings([query]))[0]\n        overview_results = await self.retrieve_with_overview(\n            query=query,\n            query_embedding=query_embedding,\n            k=k,\n            min_similarity=min_similarity,\n            cross_ref_depth=cross_ref_depth,\n            max_cross_refs=max_cross_refs,\n            max_sentences=max_sentences\n        )\n\n        # Get basic retrieval results\n        basic_results = await self.retrieve(\n            query_embedding=query_embedding,\n            k=k,\n            min_similarity=min_similarity\n        )\n        if len(basic_results) == 0:\n            return {}\n        if len(basic_results) == 1 and isinstance(basic_results[0], str) and basic_results[0].endswith('[]\\n - []\\n - []'):\n            return {}\n\n        # Prepare context for LLM summary\n        context = {\n            \"concepts\": {\n                \"main_concepts\": concept_results.get(\"concepts\", {}),\n                \"relationships\": concept_results.get(\"relationships\", []),\n                \"concept_groups\": concept_results.get(\"groups\", [])\n            },\n            \"topics\": [\n                {\n                    \"id\": topic[\"topic_id\"],\n                    \"summary\": topic[\"summary\"],\n                    \"relevance\": topic[\"relevance_score\"],\n                    \"chunk_count\": topic[\"chunk_count\"]\n                }\n                for topic in overview_results.overview\n            ],\n            \"key_chunks\": [\n                {\n                    \"text\": chunk.text,\n                    \"metadata\": chunk.metadata\n                }\n                for chunk in basic_results\n            ]\n        }\n\n        # Generate comprehensive summary using LLM\n        system_prompt = \"\"\"\n        Analyze the provided search results and generate a comprehensive summary\n        that includes:\n        1. Main concepts and their relationships\n        2. Key topics and their relevance\n        3. Most important findings and insights\n        4. Cross-references and connections between topics\n        5. Potential gaps or areas for further investigation\n\n        Format the response as a JSON object with these sections.\n        \"\"\"\n\n        prompt = f\"\"\"\n        Query: {query}\n\n        Context:\n        {json.dumps(context, indent=2)}\n\n        Generate a comprehensive analysis and summary following the structure:\n        \"\"\"\n\n        try:\n            await asyncio.sleep(0.25)\n            llm_response = await litellm_complete(\n                model_name=self.model_name,\n                prompt=prompt,\n                system_prompt=system_prompt,\n                response_format=DataModel,\n            )\n            summary_analysis = json.loads(llm_response)\n        except Exception as e:\n            get_logger().error(f\"Error generating summary: {str(e)}\")\n            summary_analysis = {\n                \"main_summary\": \"Error generating summary\",\n                \"error\": str(e)\n            }\n\n        # Compile final results\n        return {\n            \"summary\": summary_analysis,\n            \"raw_results\": {\n                \"concepts\": concept_results,\n                \"overview\": {\n                    \"topics\": overview_results.overview,\n                    \"cross_references\": overview_results.cross_references\n                },\n                \"relevant_chunks\": [\n                    {\n                        \"text\": chunk.text,\n                        \"metadata\": chunk.metadata,\n                        \"cluster_id\": chunk.cluster_id\n                    }\n                    for chunk in basic_results\n                ]\n            },\n            \"metadata\": {\n                \"query\": query,\n                \"timestamp\": time.time(),\n                \"retrieval_params\": {\n                    \"k\": k,\n                    \"min_similarity\": min_similarity,\n                    \"cross_ref_depth\": cross_ref_depth,\n                    \"max_cross_refs\": max_cross_refs\n                }\n            }\n        }\n\n    def save(self, path: str) -&gt; bytes | None:\n        \"\"\"\n        Save the complete knowledge base to disk, including all sub-components\n\n        Args:\n            path (str): Path where the knowledge base will be saved\n        \"\"\"\n        try:\n            data = {\n                # Core components\n                'vdb': self.vdb.save(),\n                'vis_kwargs': self.vis_kwargs,\n                'vis_class': self.vis_class,\n                'existing_hashes': self.existing_hashes,\n\n                # Configuration parameters\n                'embedding_dim': self.embedding_dim,\n                'similarity_threshold': self.similarity_threshold,\n                'batch_size': self.batch_size,\n                'n_clusters': self.n_clusters,\n                'deduplication_threshold': self.deduplication_threshold,\n                'model_name': self.model_name,\n                'embedding_model': self.embedding_model,\n\n                # Cache and graph data\n                'similarity_graph': self.similarity_graph,\n                'sto': self.sto,\n\n                # Text splitter configuration\n                'text_splitter_config': {\n                    'chunk_size': self.text_splitter.chunk_size,\n                    'chunk_overlap': self.text_splitter.chunk_overlap,\n                    'separator': self.text_splitter.separator\n                },\n\n                # Concept extractor data\n                'concept_graph': {\n                    'concepts': {\n                        name: {\n                            'name': concept.name,\n                            'category': concept.category,\n                            'relationships': {k: list(v) for k, v in concept.relationships.items()},\n                            'importance_score': concept.importance_score,\n                            'context_snippets': concept.context_snippets,\n                            'metadata': concept.metadata\n                        }\n                        for name, concept in self.concept_extractor.concept_graph.concepts.items()\n                    }\n                }\n            }\n            b = pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL)\n\n            if path is None:\n                return b\n\n            path = Path(path)\n            tmp = path.with_suffix(path.suffix + \".tmp\") if path.suffix else path.with_name(path.name + \".tmp\")\n\n            try:\n                # Schreibe zuerst in eine tempor\u00e4re Datei\n                with open(tmp, \"wb\") as f:\n                    f.write(b)\n                    f.flush()\n                    os.fsync(f.fileno())  # sicherstellen, dass die Daten auf Platte sind\n                # Atomischer Austausch\n                os.replace(tmp, path)\n            finally:\n                # Aufr\u00e4umen falls tmp noch existiert (bei Fehlern)\n                if tmp.exists():\n                    try:\n                        tmp.unlink()\n                    except Exception:\n                        pass\n            return None\n            # print(f\"Knowledge base successfully saved to {path} with {len(self.concept_extractor.concept_graph.concepts.items())} concepts\")\n\n        except Exception as e:\n            print(f\"Error saving knowledge base: {str(e)}\")\n            raise\n    def init_vdb(self, db:AbstractVectorStore=AbstractVectorStore):\n        pass\n    @classmethod\n    def load(cls, path: str | bytes) -&gt; 'KnowledgeBase':\n        \"\"\"\n        Load a complete knowledge base from disk, including all sub-components\n\n        Args:\n            path (str): Path from where to load the knowledge base\n\n        Returns:\n            KnowledgeBase: A fully restored knowledge base instance\n        \"\"\"\n        try:\n            if isinstance(path, (bytes, bytearray, memoryview)):\n                data_bytes = bytes(path)\n                try:\n                    data = pickle.loads(data_bytes)\n                except Exception as e:\n                    raise EOFError(f\"Fehler beim pickle.loads von bytes: {e}\") from e\n            else:\n                p = Path(path)\n                if not p.exists():\n                    raise FileNotFoundError(f\"{p} existiert nicht\")\n                size = p.stat().st_size\n                if size == 0:\n                    raise EOFError(f\"{p} ist leer (0 bytes)\")\n                try:\n                    with open(p, \"rb\") as f:\n                        try:\n                            data = pickle.load(f)\n                        except EOFError as e:\n                            # Debug info: erste bytes ausgeben\n                            f.seek(0)\n                            snippet = f.read(128)\n                            raise EOFError(\n                                f\"EOFError beim Laden {p} (Gr\u00f6\u00dfe {size} bytes). Erste 128 bytes: {snippet!r}\") from e\n\n                except Exception as e:\n                    raise\n                raise ValueError(\"Invalid path type\")\n\n            # Create new knowledge base instance with saved configuration\n            kb = cls(\n                embedding_dim=data['embedding_dim'],\n                similarity_threshold=data['similarity_threshold'],\n                batch_size=data['batch_size'],\n                n_clusters=data['n_clusters'],\n                deduplication_threshold=data['deduplication_threshold'],\n                model_name=data['model_name'],\n                embedding_model=data['embedding_model']\n            )\n\n            # Restore core components\n            kb.init_vis(data.get('vis_class'), data.get('vis_kwargs'))\n            kb.existing_hashes = data['existing_hashes']\n\n            # Restore cache and graph data\n            kb.similarity_graph = data.get('similarity_graph', {})\n            kb.sto = data.get('sto', [])\n\n            # Restore text splitter configuration\n            splitter_config = data.get('text_splitter_config', {})\n            kb.text_splitter = TextSplitter(\n                chunk_size=splitter_config.get('chunk_size', 12_000),\n                chunk_overlap=splitter_config.get('chunk_overlap', 200),\n                separator=splitter_config.get('separator', '\\n')\n            )\n\n            # Restore concept graph\n            concept_data = data.get('concept_graph', {}).get('concepts', {})\n            for concept_info in concept_data.values():\n                concept = Concept(\n                    name=concept_info['name'],\n                    category=concept_info['category'],\n                    relationships={k: set(v) for k, v in concept_info['relationships'].items()},\n                    importance_score=concept_info['importance_score'],\n                    context_snippets=concept_info['context_snippets'],\n                    metadata=concept_info['metadata']\n                )\n                kb.concept_extractor.concept_graph.add_concept(concept)\n\n            # print(f\"Knowledge base successfully loaded from {path} with {len(concept_data)} concepts\")\n            return kb\n\n        except Exception as e:\n            print(f\"Error loading knowledge base: {str(e)}\")\n            raise\n\n    def vis(self,output_file: str = \"concept_graph.html\", get_output_html=False, get_output_net=False):\n        if not self.concept_extractor.concept_graph.concepts:\n            print(\"NO Concepts defined\")\n            return None\n        net = self.concept_extractor.concept_graph.convert_to_networkx()\n        if get_output_net:\n            return net\n        return GraphVisualizer.visualize(net, output_file=output_file, get_output=get_output_html)\n</code></pre> <code>__init__(embedding_dim=768, similarity_threshold=0.61, batch_size=64, n_clusters=4, deduplication_threshold=0.85, model_name=os.getenv('DEFAULTMODELSUMMERY'), embedding_model=os.getenv('DEFAULTMODELEMBEDDING'), vis_class='FastVectorStoreO', vis_kwargs=None, requests_per_second=85.0, chunk_size=3600, chunk_overlap=130, separator='\\n')</code> \u00b6 <p>Initialize the knowledge base with given parameters</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def __init__(self, embedding_dim: int = 768, similarity_threshold: float = 0.61, batch_size: int = 64,\n             n_clusters: int = 4, deduplication_threshold: float = 0.85, model_name=os.getenv(\"DEFAULTMODELSUMMERY\"),\n             embedding_model=os.getenv(\"DEFAULTMODELEMBEDDING\"),\n             vis_class:str | None = \"FastVectorStoreO\",\n             vis_kwargs:dict[str, Any] | None=None,\n             requests_per_second=85.,\n             chunk_size: int = 3600,\n             chunk_overlap: int = 130,\n             separator: str = \"\\n\"\n             ):\n    \"\"\"Initialize the knowledge base with given parameters\"\"\"\n\n    self.existing_hashes: set[str] = set()\n    self.embedding_model = embedding_model\n    self.embedding_dim = embedding_dim\n    self.similarity_threshold = similarity_threshold\n    self.deduplication_threshold = deduplication_threshold\n    if model_name == \"openrouter/mistralai/mistral-nemo\":\n        batch_size = 9\n        requests_per_second = 1.5\n    self.batch_size = batch_size\n    self.n_clusters = n_clusters\n    self.model_name = model_name\n    self.sto: list = []\n\n    self.text_splitter = TextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap, separator=separator)\n    self.similarity_graph = {}\n    self.concept_extractor = ConceptExtractor(self, requests_per_second)\n\n    self.vis_class = None\n    self.vis_kwargs = None\n    self.vdb = None\n    self.init_vis(vis_class, vis_kwargs)\n</code></pre> <code>add_data(texts, metadata=None, direct=False)</code> <code>async</code> \u00b6 <p>Enhanced version with smart splitting and clustering</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def add_data(\n    self,\n    texts: list[str],\n    metadata: list[dict[str, Any]] | None = None, direct:bool = False\n) -&gt; tuple[int, int]:\n    \"\"\"Enhanced version with smart splitting and clustering\"\"\"\n    if isinstance(texts, str):\n        texts = [texts]\n    if metadata is None:\n        metadata = [{}] * len(texts)\n    if isinstance(metadata, dict):\n        metadata = [metadata]\n    if len(texts) != len(metadata):\n        raise ValueError(\"Length of texts and metadata must match\")\n\n    if not direct:\n        if len(texts) == 1 and len(texts[0]) &lt; 10_000:\n            if len(self.sto) &lt; self.batch_size and len(texts) == 1:\n                self.sto.append((texts[0], metadata[0]))\n                return -1, -1\n            if len(self.sto) &gt;= self.batch_size:\n                _ = [texts.append(t) or metadata.append([m]) for (t, m) in self.sto]\n                self.sto = []\n\n    # Split large texts\n    split_texts = []\n    split_metadata = []\n\n    while Spinner(\"Saving Data to Memory\", symbols='t'):\n\n        for idx, text in enumerate(texts):\n            chunks = self.text_splitter.split_text(text)\n            split_texts.extend(chunks)\n\n            # Adjust metadata for splits\n            meta = metadata[idx] if metadata else {}\n            if isinstance(meta, list):\n                meta = meta[0]\n            for i, _chunk in enumerate(chunks):\n                chunk_meta = meta.copy()\n                chunk_meta.update({\n                    'chunk_index': i,\n                    'total_chunks': len(chunks),\n                    'original_text_id': idx\n                })\n                split_metadata.append(chunk_meta)\n\n        return await self._add_data(split_texts, split_metadata)\n</code></pre> <code>compute_hash(text)</code> <code>staticmethod</code> \u00b6 <p>Compute SHA-256 hash of text</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>@staticmethod\ndef compute_hash(text: str) -&gt; str:\n    \"\"\"Compute SHA-256 hash of text\"\"\"\n    return hashlib.sha256(text.encode('utf-8', errors='ignore')).hexdigest()\n</code></pre> <code>forget_irrelevant(irrelevant_concepts, similarity_threshold=None)</code> <code>async</code> \u00b6 <p>Remove chunks similar to irrelevant concepts Returns: Number of chunks removed</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def forget_irrelevant(self, irrelevant_concepts: list[str], similarity_threshold: float | None=None) -&gt; int:\n    \"\"\"\n    Remove chunks similar to irrelevant concepts\n    Returns: Number of chunks removed\n    \"\"\"\n    if not irrelevant_concepts:\n        return 0\n\n    if similarity_threshold is None:\n        similarity_threshold = self.similarity_threshold\n\n    try:\n        irrelevant_embeddings = await self._get_embeddings(irrelevant_concepts)\n        initial_count = len(self.vdb.chunks)\n\n        def is_relevant(chunk: Chunk) -&gt; bool:\n            similarities = np.dot(chunk.embedding, irrelevant_embeddings.T)\n            do_keep = np.max(similarities) &lt; similarity_threshold\n            if do_keep:\n                return True\n            for c in chunk.metadata.get(\"concepts\", []):\n                if c in self.concept_extractor.concept_graph.concepts:\n                    del self.concept_extractor.concept_graph.concepts[c]\n            return False\n\n        relevant_chunks = [chunk for chunk in self.vdb.chunks if is_relevant(chunk)]\n        self.vdb.chunks = relevant_chunks\n        self.existing_hashes = {chunk.content_hash for chunk in self.vdb.chunks}\n        self.vdb.rebuild_index()\n\n        return initial_count - len(self.vdb.chunks)\n\n    except Exception as e:\n        get_logger().error(f\"Error forgetting irrelevant concepts: {str(e)}\")\n        raise\n</code></pre> <code>load(path)</code> <code>classmethod</code> \u00b6 <p>Load a complete knowledge base from disk, including all sub-components</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path from where to load the knowledge base</p> required <p>Returns:</p> Name Type Description <code>KnowledgeBase</code> <code>KnowledgeBase</code> <p>A fully restored knowledge base instance</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>@classmethod\ndef load(cls, path: str | bytes) -&gt; 'KnowledgeBase':\n    \"\"\"\n    Load a complete knowledge base from disk, including all sub-components\n\n    Args:\n        path (str): Path from where to load the knowledge base\n\n    Returns:\n        KnowledgeBase: A fully restored knowledge base instance\n    \"\"\"\n    try:\n        if isinstance(path, (bytes, bytearray, memoryview)):\n            data_bytes = bytes(path)\n            try:\n                data = pickle.loads(data_bytes)\n            except Exception as e:\n                raise EOFError(f\"Fehler beim pickle.loads von bytes: {e}\") from e\n        else:\n            p = Path(path)\n            if not p.exists():\n                raise FileNotFoundError(f\"{p} existiert nicht\")\n            size = p.stat().st_size\n            if size == 0:\n                raise EOFError(f\"{p} ist leer (0 bytes)\")\n            try:\n                with open(p, \"rb\") as f:\n                    try:\n                        data = pickle.load(f)\n                    except EOFError as e:\n                        # Debug info: erste bytes ausgeben\n                        f.seek(0)\n                        snippet = f.read(128)\n                        raise EOFError(\n                            f\"EOFError beim Laden {p} (Gr\u00f6\u00dfe {size} bytes). Erste 128 bytes: {snippet!r}\") from e\n\n            except Exception as e:\n                raise\n            raise ValueError(\"Invalid path type\")\n\n        # Create new knowledge base instance with saved configuration\n        kb = cls(\n            embedding_dim=data['embedding_dim'],\n            similarity_threshold=data['similarity_threshold'],\n            batch_size=data['batch_size'],\n            n_clusters=data['n_clusters'],\n            deduplication_threshold=data['deduplication_threshold'],\n            model_name=data['model_name'],\n            embedding_model=data['embedding_model']\n        )\n\n        # Restore core components\n        kb.init_vis(data.get('vis_class'), data.get('vis_kwargs'))\n        kb.existing_hashes = data['existing_hashes']\n\n        # Restore cache and graph data\n        kb.similarity_graph = data.get('similarity_graph', {})\n        kb.sto = data.get('sto', [])\n\n        # Restore text splitter configuration\n        splitter_config = data.get('text_splitter_config', {})\n        kb.text_splitter = TextSplitter(\n            chunk_size=splitter_config.get('chunk_size', 12_000),\n            chunk_overlap=splitter_config.get('chunk_overlap', 200),\n            separator=splitter_config.get('separator', '\\n')\n        )\n\n        # Restore concept graph\n        concept_data = data.get('concept_graph', {}).get('concepts', {})\n        for concept_info in concept_data.values():\n            concept = Concept(\n                name=concept_info['name'],\n                category=concept_info['category'],\n                relationships={k: set(v) for k, v in concept_info['relationships'].items()},\n                importance_score=concept_info['importance_score'],\n                context_snippets=concept_info['context_snippets'],\n                metadata=concept_info['metadata']\n            )\n            kb.concept_extractor.concept_graph.add_concept(concept)\n\n        # print(f\"Knowledge base successfully loaded from {path} with {len(concept_data)} concepts\")\n        return kb\n\n    except Exception as e:\n        print(f\"Error loading knowledge base: {str(e)}\")\n        raise\n</code></pre> <code>query_concepts(query)</code> <code>async</code> \u00b6 <p>Query concepts extracted from the knowledge base</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def query_concepts(self, query: str) -&gt; dict[str, any]:\n    \"\"\"Query concepts extracted from the knowledge base\"\"\"\n    return await self.concept_extractor.query_concepts(query)\n</code></pre> <code>retrieve(query='', query_embedding=None, k=5, min_similarity=0.2, include_connected=True)</code> <code>async</code> \u00b6 <p>Enhanced retrieval with connected information</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def retrieve(\n    self,\n    query: str=\"\",\n    query_embedding: np.ndarray | None = None,\n    k: int = 5,\n    min_similarity: float = 0.2,\n    include_connected: bool = True\n) -&gt; list[Chunk]:\n    \"\"\"Enhanced retrieval with connected information\"\"\"\n    if query_embedding is None:\n        query_embedding = (await self._get_embeddings([query]))[0]\n    k = min(k, len(self.vdb.chunks))\n    if k &lt;= 0:\n        return []\n    initial_results = self.vdb.search(query_embedding, k, min_similarity)\n\n    if not include_connected or not initial_results:\n        return initial_results\n\n    # Find connected chunks\n    connected_chunks = set()\n    for chunk in initial_results:\n        chunk_id = self.vdb.chunks.index(chunk)\n        if chunk_id in self.similarity_graph:\n            connected_chunks.update(self.similarity_graph[chunk_id])\n\n    # Add connected chunks to results\n    all_chunks = self.vdb.chunks\n    additional_results = [all_chunks[i] for i in connected_chunks\n                          if all_chunks[i] not in initial_results]\n\n    # Sort by similarity to query\n    all_results = initial_results + additional_results\n\n    return sorted(\n        all_results,\n        key=lambda x: np.dot(x.embedding, query_embedding),\n        reverse=True\n    )[:k * 2]  # Return more results when including connected information\n</code></pre> <code>retrieve_with_overview(query, query_embedding=None, k=5, min_similarity=0.2, max_sentences=5, cross_ref_depth=2, max_cross_refs=10)</code> <code>async</code> \u00b6 <p>Enhanced retrieval with better cross-reference handling</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def retrieve_with_overview(\n    self,\n    query: str,\n    query_embedding=None,\n    k: int = 5,\n    min_similarity: float = 0.2,\n    max_sentences: int = 5,\n    cross_ref_depth: int = 2,\n    max_cross_refs: int = 10  # New parameter to control cross-reference count\n) -&gt; RetrievalResult:\n    \"\"\"Enhanced retrieval with better cross-reference handling\"\"\"\n    # Get initial results with query embedding\n    if query_embedding is None:\n        query_embedding = (await self._get_embeddings([query]))[0]\n    initial_results = await self.retrieve(query_embedding=query_embedding, k=k, min_similarity=min_similarity)\n\n    if not initial_results:\n        return RetrievalResult([], [], {})\n\n    # Find cross-references with similarity scoring\n    initial_ids = {self.vdb.chunks.index(chunk) for chunk in initial_results}\n    related_ids = self._find_cross_references(\n        initial_ids,\n        depth=cross_ref_depth,\n        query_embedding=query_embedding  # Pass query embedding for relevance scoring\n    )\n\n    # Get all relevant chunks with smarter filtering\n    all_chunks = self.vdb.chunks\n    all_relevant_chunks = initial_results + [\n        chunk for i, chunk in enumerate(all_chunks)\n        if i in related_ids and self._is_relevant_cross_ref(\n            chunk,\n            query_embedding,\n            initial_results\n        )\n    ]\n\n    # Enhanced clustering with dynamic cluster size\n    clusters = self._cluster_chunks(\n        all_relevant_chunks,\n        query_embedding=query_embedding\n    )\n\n    # Fallback: If no clusters are found, treat all relevant chunks as a single cluster.\n    if not clusters:\n        print(\"No clusters found. Falling back to using all relevant chunks as a single cluster.\")\n        clusters = {0: all_relevant_chunks}\n\n    # Generate summaries and organize results\n    overview = []\n    cross_references = {}\n\n    for cluster_id, cluster_chunks in clusters.items():\n        summary = self._generate_topic_summary(\n            cluster_chunks,\n            query_embedding,\n            max_sentences=max_sentences  # Increased for more context\n        )\n\n        # Enhanced chunk sorting with combined scoring\n        sorted_chunks = self._sort_chunks_by_relevance(\n            cluster_chunks,\n            query_embedding,\n            initial_results\n        )\n\n        # Separate direct matches and cross-references\n        direct_matches_ = [{'text':c.text, 'metadata':c.metadata} for c in sorted_chunks if c in initial_results]\n        direct_matches = []\n        for match in direct_matches_:\n            if match in direct_matches:\n                continue\n            direct_matches.append(match)\n        cross_refs_ = [c for c in sorted_chunks if c not in initial_results]\n        cross_refs = []\n        for match in cross_refs_:\n            if match in cross_refs:\n                continue\n            cross_refs.append(match)\n        # Limit cross-references while maintaining diversity\n        selected_cross_refs = self._select_diverse_cross_refs(\n            cross_refs,\n            max_cross_refs,\n            query_embedding\n        )\n\n        topic_info = {\n            'topic_id': cluster_id,\n            'summary': summary,\n            'main_chunks': [x for x in direct_matches[:3]],\n            'chunk_count': len(cluster_chunks),\n            'relevance_score': self._calculate_topic_relevance(\n                cluster_chunks,\n                query_embedding\n            )\n        }\n        overview.append(topic_info)\n\n        if selected_cross_refs:\n            cross_references[f\"topic_{cluster_id}\"] = selected_cross_refs\n\n    # Sort overview by relevance score\n    overview.sort(key=lambda x: x['relevance_score'], reverse=True)\n\n    return RetrievalResult(\n        overview=overview,\n        details=initial_results,\n        cross_references=cross_references\n    )\n</code></pre> <code>save(path)</code> \u00b6 <p>Save the complete knowledge base to disk, including all sub-components</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path where the knowledge base will be saved</p> required Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def save(self, path: str) -&gt; bytes | None:\n    \"\"\"\n    Save the complete knowledge base to disk, including all sub-components\n\n    Args:\n        path (str): Path where the knowledge base will be saved\n    \"\"\"\n    try:\n        data = {\n            # Core components\n            'vdb': self.vdb.save(),\n            'vis_kwargs': self.vis_kwargs,\n            'vis_class': self.vis_class,\n            'existing_hashes': self.existing_hashes,\n\n            # Configuration parameters\n            'embedding_dim': self.embedding_dim,\n            'similarity_threshold': self.similarity_threshold,\n            'batch_size': self.batch_size,\n            'n_clusters': self.n_clusters,\n            'deduplication_threshold': self.deduplication_threshold,\n            'model_name': self.model_name,\n            'embedding_model': self.embedding_model,\n\n            # Cache and graph data\n            'similarity_graph': self.similarity_graph,\n            'sto': self.sto,\n\n            # Text splitter configuration\n            'text_splitter_config': {\n                'chunk_size': self.text_splitter.chunk_size,\n                'chunk_overlap': self.text_splitter.chunk_overlap,\n                'separator': self.text_splitter.separator\n            },\n\n            # Concept extractor data\n            'concept_graph': {\n                'concepts': {\n                    name: {\n                        'name': concept.name,\n                        'category': concept.category,\n                        'relationships': {k: list(v) for k, v in concept.relationships.items()},\n                        'importance_score': concept.importance_score,\n                        'context_snippets': concept.context_snippets,\n                        'metadata': concept.metadata\n                    }\n                    for name, concept in self.concept_extractor.concept_graph.concepts.items()\n                }\n            }\n        }\n        b = pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL)\n\n        if path is None:\n            return b\n\n        path = Path(path)\n        tmp = path.with_suffix(path.suffix + \".tmp\") if path.suffix else path.with_name(path.name + \".tmp\")\n\n        try:\n            # Schreibe zuerst in eine tempor\u00e4re Datei\n            with open(tmp, \"wb\") as f:\n                f.write(b)\n                f.flush()\n                os.fsync(f.fileno())  # sicherstellen, dass die Daten auf Platte sind\n            # Atomischer Austausch\n            os.replace(tmp, path)\n        finally:\n            # Aufr\u00e4umen falls tmp noch existiert (bei Fehlern)\n            if tmp.exists():\n                try:\n                    tmp.unlink()\n                except Exception:\n                    pass\n        return None\n        # print(f\"Knowledge base successfully saved to {path} with {len(self.concept_extractor.concept_graph.concepts.items())} concepts\")\n\n    except Exception as e:\n        print(f\"Error saving knowledge base: {str(e)}\")\n        raise\n</code></pre> <code>unified_retrieve(query, k=5, min_similarity=0.2, cross_ref_depth=2, max_cross_refs=10, max_sentences=10)</code> <code>async</code> \u00b6 <p>Unified retrieval function that combines concept querying, retrieval with overview, and basic retrieval, then generates a comprehensive summary using LLM.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query string</p> required <code>k</code> <code>int</code> <p>Number of primary results to retrieve</p> <code>5</code> <code>min_similarity</code> <code>float</code> <p>Minimum similarity threshold for retrieval</p> <code>0.2</code> <code>cross_ref_depth</code> <code>int</code> <p>Depth for cross-reference search</p> <code>2</code> <code>max_cross_refs</code> <code>int</code> <p>Maximum number of cross-references per topic</p> <code>10</code> <code>max_sentences</code> <code>int</code> <p>Maximum number Sentences in the main summary text</p> <code>10</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing comprehensive results including summary and details</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>async def unified_retrieve(\n    self,\n    query: str,\n    k: int = 5,\n    min_similarity: float = 0.2,\n    cross_ref_depth: int = 2,\n    max_cross_refs: int = 10,\n    max_sentences: int = 10\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Unified retrieval function that combines concept querying, retrieval with overview,\n    and basic retrieval, then generates a comprehensive summary using LLM.\n\n    Args:\n        query: Search query string\n        k: Number of primary results to retrieve\n        min_similarity: Minimum similarity threshold for retrieval\n        cross_ref_depth: Depth for cross-reference search\n        max_cross_refs: Maximum number of cross-references per topic\n        max_sentences: Maximum number Sentences in the main summary text\n\n    Returns:\n        Dictionary containing comprehensive results including summary and details\n    \"\"\"\n    # Get concept information\n    concept_results = await self.concept_extractor.query_concepts(query)\n\n    # Get retrieval overview\n\n    query_embedding = (await self._get_embeddings([query]))[0]\n    overview_results = await self.retrieve_with_overview(\n        query=query,\n        query_embedding=query_embedding,\n        k=k,\n        min_similarity=min_similarity,\n        cross_ref_depth=cross_ref_depth,\n        max_cross_refs=max_cross_refs,\n        max_sentences=max_sentences\n    )\n\n    # Get basic retrieval results\n    basic_results = await self.retrieve(\n        query_embedding=query_embedding,\n        k=k,\n        min_similarity=min_similarity\n    )\n    if len(basic_results) == 0:\n        return {}\n    if len(basic_results) == 1 and isinstance(basic_results[0], str) and basic_results[0].endswith('[]\\n - []\\n - []'):\n        return {}\n\n    # Prepare context for LLM summary\n    context = {\n        \"concepts\": {\n            \"main_concepts\": concept_results.get(\"concepts\", {}),\n            \"relationships\": concept_results.get(\"relationships\", []),\n            \"concept_groups\": concept_results.get(\"groups\", [])\n        },\n        \"topics\": [\n            {\n                \"id\": topic[\"topic_id\"],\n                \"summary\": topic[\"summary\"],\n                \"relevance\": topic[\"relevance_score\"],\n                \"chunk_count\": topic[\"chunk_count\"]\n            }\n            for topic in overview_results.overview\n        ],\n        \"key_chunks\": [\n            {\n                \"text\": chunk.text,\n                \"metadata\": chunk.metadata\n            }\n            for chunk in basic_results\n        ]\n    }\n\n    # Generate comprehensive summary using LLM\n    system_prompt = \"\"\"\n    Analyze the provided search results and generate a comprehensive summary\n    that includes:\n    1. Main concepts and their relationships\n    2. Key topics and their relevance\n    3. Most important findings and insights\n    4. Cross-references and connections between topics\n    5. Potential gaps or areas for further investigation\n\n    Format the response as a JSON object with these sections.\n    \"\"\"\n\n    prompt = f\"\"\"\n    Query: {query}\n\n    Context:\n    {json.dumps(context, indent=2)}\n\n    Generate a comprehensive analysis and summary following the structure:\n    \"\"\"\n\n    try:\n        await asyncio.sleep(0.25)\n        llm_response = await litellm_complete(\n            model_name=self.model_name,\n            prompt=prompt,\n            system_prompt=system_prompt,\n            response_format=DataModel,\n        )\n        summary_analysis = json.loads(llm_response)\n    except Exception as e:\n        get_logger().error(f\"Error generating summary: {str(e)}\")\n        summary_analysis = {\n            \"main_summary\": \"Error generating summary\",\n            \"error\": str(e)\n        }\n\n    # Compile final results\n    return {\n        \"summary\": summary_analysis,\n        \"raw_results\": {\n            \"concepts\": concept_results,\n            \"overview\": {\n                \"topics\": overview_results.overview,\n                \"cross_references\": overview_results.cross_references\n            },\n            \"relevant_chunks\": [\n                {\n                    \"text\": chunk.text,\n                    \"metadata\": chunk.metadata,\n                    \"cluster_id\": chunk.cluster_id\n                }\n                for chunk in basic_results\n            ]\n        },\n        \"metadata\": {\n            \"query\": query,\n            \"timestamp\": time.time(),\n            \"retrieval_params\": {\n                \"k\": k,\n                \"min_similarity\": min_similarity,\n                \"cross_ref_depth\": cross_ref_depth,\n                \"max_cross_refs\": max_cross_refs\n            }\n        }\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.RelevanceAssessment","title":"<code>RelevanceAssessment</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an assessment of the relevance of the data in relation to a specific query.</p> <p>Attributes:</p> Name Type Description <code>query_alignment</code> <code>float</code> <p>A float representing the alignment between the query and the data.</p> <code>confidence_score</code> <code>float</code> <p>A float indicating the confidence level in the alignment.</p> <code>coverage_analysis</code> <code>str</code> <p>A textual description analyzing the data coverage.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class RelevanceAssessment(BaseModel):\n    \"\"\"\n    Represents an assessment of the relevance of the data in relation to a specific query.\n\n    Attributes:\n        query_alignment (float): A float representing the alignment between the query and the data.\n        confidence_score (float): A float indicating the confidence level in the alignment.\n        coverage_analysis (str): A textual description analyzing the data coverage.\n    \"\"\"\n    query_alignment: float\n    confidence_score: float\n    coverage_analysis: str\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.RetrievalResult","title":"<code>RetrievalResult</code>  <code>dataclass</code>","text":"<p>Structure for organizing retrieval results</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>@dataclass\nclass RetrievalResult:\n    \"\"\"Structure for organizing retrieval results\"\"\"\n    overview: list[dict[str, any]]  # List of topic summaries\n    details: list[Chunk]  # Detailed chunks\n    cross_references: dict[str, list[Chunk]]  # Related chunks by topic\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.TConcept","title":"<code>TConcept</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the criteria or target parameters for concept selection and filtering.</p> <p>Attributes:</p> Name Type Description <code>min_importance</code> <code>float</code> <p>The minimum importance score a concept must have to be considered.</p> <code>target_concepts</code> <code>List[str]</code> <p>A list of names of target concepts to focus on.</p> <code>relationship_types</code> <code>List[str]</code> <p>A list of relationship types to be considered in the analysis.</p> <code>categories</code> <code>List[str]</code> <p>A list of concept categories to filter or group the concepts.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class TConcept(BaseModel):\n    \"\"\"\n    Represents the criteria or target parameters for concept selection and filtering.\n\n    Attributes:\n        min_importance (float): The minimum importance score a concept must have to be considered.\n        target_concepts (List[str]): A list of names of target concepts to focus on.\n        relationship_types (List[str]): A list of relationship types to be considered in the analysis.\n        categories (List[str]): A list of concept categories to filter or group the concepts.\n    \"\"\"\n    min_importance: float\n    target_concepts: list[str]\n    relationship_types: list[str]\n    categories: list[str]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.TextSplitter","title":"<code>TextSplitter</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class TextSplitter:\n    def __init__(\n        self,\n        chunk_size: int = 3600,\n        chunk_overlap: int = 130,\n        separator: str = \"\\n\"\n    ):\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.separator = separator\n\n    def approximate(self, text_len: int) -&gt; float:\n        \"\"\"\n        Approximate the number of chunks and average chunk size for a given text length\n\n        Args:\n            text_len (int): Length of the text to be split\n\n        Returns:\n            Tuple[int, int]: (number_of_chunks, approximate_chunk_size)\n        \"\"\"\n        if text_len &lt;= self.chunk_size:\n            return 1, text_len\n\n        # Handle extreme overlap cases\n        if self.chunk_overlap &gt;= self.chunk_size:\n            estimated_chunks = text_len\n            return estimated_chunks, 1\n\n        # Calculate based on overlap ratio\n        overlap_ratio = self.chunk_overlap / self.chunk_size\n        base_chunks = text_len / self.chunk_size\n        estimated_chunks = base_chunks * 2 / (overlap_ratio if overlap_ratio &gt; 0 else 1)\n\n        # print('#',estimated_chunks, base_chunks, overlap_ratio)\n        # Calculate average chunk size\n        avg_chunk_size = max(1, text_len / estimated_chunks)\n\n        return estimated_chunks * avg_chunk_size\n\n    def split_text(self, text: str) -&gt; list[str]:\n        \"\"\"Split text into chunks with overlap\"\"\"\n        # Clean and normalize text\n        text = re.sub(r'\\s+', ' ', text).strip()\n\n        # If text is shorter than chunk_size, return as is\n        if len(text) &lt;= self.chunk_size:\n            return [text]\n\n        chunks = []\n        start = 0\n\n        while start &lt; len(text):\n            # Find end of chunk\n            end = start + self.chunk_size\n\n            if end &gt;= len(text):\n                chunks.append(text[start:])\n                break\n\n            # Try to find a natural break point\n            last_separator = text.rfind(self.separator, start, end)\n            if last_separator != -1:\n                end = last_separator\n\n            # Add chunk\n            chunks.append(text[start:end])\n\n            # Calculate allowed overlap for this chunk\n            chunk_length = end - start\n            allowed_overlap = min(self.chunk_overlap, chunk_length - 1)\n\n            # Move start position considering adjusted overlap\n            start = end - allowed_overlap\n\n        return chunks\n</code></pre> <code>approximate(text_len)</code> \u00b6 <p>Approximate the number of chunks and average chunk size for a given text length</p> <p>Parameters:</p> Name Type Description Default <code>text_len</code> <code>int</code> <p>Length of the text to be split</p> required <p>Returns:</p> Type Description <code>float</code> <p>Tuple[int, int]: (number_of_chunks, approximate_chunk_size)</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def approximate(self, text_len: int) -&gt; float:\n    \"\"\"\n    Approximate the number of chunks and average chunk size for a given text length\n\n    Args:\n        text_len (int): Length of the text to be split\n\n    Returns:\n        Tuple[int, int]: (number_of_chunks, approximate_chunk_size)\n    \"\"\"\n    if text_len &lt;= self.chunk_size:\n        return 1, text_len\n\n    # Handle extreme overlap cases\n    if self.chunk_overlap &gt;= self.chunk_size:\n        estimated_chunks = text_len\n        return estimated_chunks, 1\n\n    # Calculate based on overlap ratio\n    overlap_ratio = self.chunk_overlap / self.chunk_size\n    base_chunks = text_len / self.chunk_size\n    estimated_chunks = base_chunks * 2 / (overlap_ratio if overlap_ratio &gt; 0 else 1)\n\n    # print('#',estimated_chunks, base_chunks, overlap_ratio)\n    # Calculate average chunk size\n    avg_chunk_size = max(1, text_len / estimated_chunks)\n\n    return estimated_chunks * avg_chunk_size\n</code></pre> <code>split_text(text)</code> \u00b6 <p>Split text into chunks with overlap</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def split_text(self, text: str) -&gt; list[str]:\n    \"\"\"Split text into chunks with overlap\"\"\"\n    # Clean and normalize text\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    # If text is shorter than chunk_size, return as is\n    if len(text) &lt;= self.chunk_size:\n        return [text]\n\n    chunks = []\n    start = 0\n\n    while start &lt; len(text):\n        # Find end of chunk\n        end = start + self.chunk_size\n\n        if end &gt;= len(text):\n            chunks.append(text[start:])\n            break\n\n        # Try to find a natural break point\n        last_separator = text.rfind(self.separator, start, end)\n        if last_separator != -1:\n            end = last_separator\n\n        # Add chunk\n        chunks.append(text[start:end])\n\n        # Calculate allowed overlap for this chunk\n        chunk_length = end - start\n        allowed_overlap = min(self.chunk_overlap, chunk_length - 1)\n\n        # Move start position considering adjusted overlap\n        start = end - allowed_overlap\n\n    return chunks\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.TopicInsights","title":"<code>TopicInsights</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents insights related to various topics.</p> <p>Attributes:</p> Name Type Description <code>primary_topics</code> <code>list[str]</code> <p>A list of main topics addressed.</p> <code>cross_references</code> <code>list[str]</code> <p>A list of cross-references that connect different topics.</p> <code>knowledge_gaps</code> <code>list[str]</code> <p>A list of identified gaps in the current knowledge.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class TopicInsights(BaseModel):\n    \"\"\"\n    Represents insights related to various topics.\n\n    Attributes:\n        primary_topics (list[str]): A list of main topics addressed.\n        cross_references (list[str]): A list of cross-references that connect different topics.\n        knowledge_gaps (list[str]): A list of identified gaps in the current knowledge.\n    \"\"\"\n    primary_topics: list[str]\n    cross_references: list[str]\n    knowledge_gaps: list[str]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.rConcept","title":"<code>rConcept</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a key concept with its relationships and associated metadata.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the concept.</p> <code>category</code> <code>str</code> <p>The category of the concept (e.g., 'technical', 'domain', 'method', etc.).</p> <code>relationships</code> <code>Dict[str, List[str]]</code> <p>A mapping where each key is a type of relationship and the value is a list of related concept names.</p> <code>importance_score</code> <code>float</code> <p>A numerical score representing the importance or relevance of the concept.</p> <code>context_snippets</code> <code>List[str]</code> <p>A list of text snippets providing context where the concept appears.</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>class rConcept(BaseModel):\n    \"\"\"\n    Represents a key concept with its relationships and associated metadata.\n\n    Attributes:\n        name (str): The name of the concept.\n        category (str): The category of the concept (e.g., 'technical', 'domain', 'method', etc.).\n        relationships (Dict[str, List[str]]): A mapping where each key is a type of relationship and the\n            value is a list of related concept names.\n        importance_score (float): A numerical score representing the importance or relevance of the concept.\n        context_snippets (List[str]): A list of text snippets providing context where the concept appears.\n    \"\"\"\n    name: str\n    category: str\n    relationships: dict[str, list[str]]\n    importance_score: float\n    context_snippets: list[str]\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.KnowledgeBase.normalize_vectors","title":"<code>normalize_vectors(vectors)</code>","text":"<p>Normalize vectors to unit length</p> Source code in <code>toolboxv2/mods/isaa/base/KnowledgeBase.py</code> <pre><code>def normalize_vectors(vectors: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Normalize vectors to unit length\"\"\"\n    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n    return np.divide(vectors, norms, where=norms != 0)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.VectorStores","title":"<code>VectorStores</code>","text":"<p>Vector store implementations for the toolboxv2 system.</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.VectorStores.taichiNumpyNumbaVectorStores","title":"<code>taichiNumpyNumbaVectorStores</code>","text":"<code>NumpyVectorStore</code> \u00b6 <p>               Bases: <code>AbstractVectorStore</code></p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/taichiNumpyNumbaVectorStores.py</code> <pre><code>class NumpyVectorStore(AbstractVectorStore):\n    def __init__(self, use_gpu=False):\n        self.embeddings = np.empty((0, 0))\n        self.chunks = []\n        # Initialize Taich\n\n\n        self.normalized_embeddings = None\n\n    def add_embeddings(self, embeddings: np.ndarray, chunks: list[Chunk]) -&gt; None:\n        if len(embeddings.shape) != 2:\n            raise ValueError(\"Embeddings must be 2D array\")\n        if len(chunks) != embeddings.shape[0]:\n            raise ValueError(\"Mismatch between embeddings and chunks count\")\n\n        if self.embeddings.size == 0:\n            self.embeddings = embeddings\n        else:\n            if embeddings.shape[1] != self.embeddings.shape[1]:\n                raise ValueError(\"Embedding dimensions must match\")\n            self.embeddings = np.vstack([self.embeddings, embeddings])\n        self.chunks.extend(chunks)\n        # Reset normalized embeddings cache\n        self.normalized_embeddings = None\n\n    def search(self, query_embedding: np.ndarray, k: int = 5, min_similarity: float = 0.7) -&gt; list[Chunk]:\n        if self.embeddings.size == 0:\n            return []\n\n        # Pre-compute normalized embeddings if not cached\n        if self.normalized_embeddings is None:\n            self._precompute_normalized_embeddings()\n\n        # Normalize query\n        query_norm = self._normalize_vector(query_embedding)\n\n        # Enhanced Taichi kernel for similarity computation\n        n = len(self.chunks)\n        similarities = np.zeros(n, dtype=np.float32)\n\n        @ti.kernel\n        def compute_similarities_optimized(\n            query: ti.types.ndarray(dtype=ti.f32),\n            embeddings: ti.types.ndarray(dtype=ti.f32),\n            similarities: ti.types.ndarray(dtype=ti.f32),\n            n: ti.i32,\n            dim: ti.i32\n        ):\n            ti.loop_config(block_dim=256)\n            for i in range(n):\n                dot_product = 0.0\n                # Vectorized dot product computation\n                for j in range(dim):\n                    dot_product += embeddings[i, j] * query[j]\n                similarities[i] = dot_product\n\n        # Alternative optimized kernel using tile-based computation\n        @ti.kernel\n        def compute_similarities_tiled(\n            query: ti.types.ndarray(dtype=ti.f32),\n            embeddings: ti.types.ndarray(dtype=ti.f32),\n            similarities: ti.types.ndarray(dtype=ti.f32),\n            n: ti.i32,\n            dim: ti.i32\n        ):\n            tile_size = 16  # Adjust based on hardware\n            for i in range(n):\n                dot_product = 0.0\n                # Process in tiles for better cache utilization\n                for jt in range(0, dim):\n                    if jt % tile_size != 0:\n                        continue\n                    tile_sum = 0.0\n                    for j in range(jt, ti.min(jt + tile_size, dim)):\n                        tile_sum += embeddings[i, j] * query[j]\n                    dot_product += tile_sum\n                similarities[i] = dot_product\n\n        # Choose the appropriate kernel based on dimension size\n        if query_embedding.shape[0] &gt;= 256:\n            compute_similarities_tiled(\n                query_norm.astype(np.float32),\n                self.normalized_embeddings,\n                similarities,\n                n,\n                query_embedding.shape[0]\n            )\n        else:\n            compute_similarities_optimized(\n                query_norm.astype(np.float32),\n                self.normalized_embeddings,\n                similarities,\n                n,\n                query_embedding.shape[0]\n            )\n\n        # Optimize top-k selection\n        if k &gt;= n:\n            indices = np.argsort(-similarities)\n        else:\n            # Use partial sort for better performance when k &lt; n\n            indices = np.argpartition(-similarities, k)[:k]\n            indices = indices[np.argsort(-similarities[indices])]\n\n        # Filter results efficiently using vectorized operations\n        mask = similarities[indices] &gt;= min_similarity\n        filtered_indices = indices[mask]\n        return [self.chunks[idx] for idx in filtered_indices[:k]]\n\n    def save(self) -&gt; bytes:\n        return pickle.dumps({\n            'embeddings': self.embeddings,\n            'chunks': self.chunks\n        })\n\n    def load(self, data: bytes) -&gt; 'NumpyVectorStore':\n        loaded = pickle.loads(data)\n        self.embeddings = loaded['embeddings']\n        self.chunks = loaded['chunks']\n        return self\n\n    def clear(self) -&gt; None:\n        self.embeddings = np.empty((0, 0))\n        self.chunks = []\n        self.normalized_embeddings = None\n\n    def rebuild_index(self) -&gt; None:\n        pass  # No index to rebuild for numpy implementation\n\n    def _normalize_vector(self, vector: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Normalize a single vector efficiently.\"\"\"\n        return vector / (np.linalg.norm(vector) + 1e-8)\n\n    def _precompute_normalized_embeddings(self) -&gt; None:\n        \"\"\"Pre-compute and cache normalized embeddings.\"\"\"\n        # Allocate output array\n        self.normalized_embeddings = np.empty_like(self.embeddings, dtype=np.float32)\n\n        # Normalize embeddings using Taichi\n        batch_normalize(\n            self.embeddings.astype(np.float32),\n            self.normalized_embeddings,\n            self.embeddings.shape[0],\n            self.embeddings.shape[1]\n        )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.base.VectorStores.types","title":"<code>types</code>","text":"<code>AbstractVectorStore</code> \u00b6 <p>               Bases: <code>ABC</code></p> <p>Abstract base class for vector stores</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/types.py</code> <pre><code>class AbstractVectorStore(ABC):\n    \"\"\"Abstract base class for vector stores\"\"\"\n\n    @abstractmethod\n    def add_embeddings(self, embeddings: np.ndarray, chunks: list[Chunk]) -&gt; None:\n        \"\"\"Add embeddings and their corresponding chunks to the store\"\"\"\n        pass\n\n    @abstractmethod\n    def search(self, query_embedding: np.ndarray, k: int = 5, min_similarity: float = 0.7) -&gt; list[Chunk]:\n        \"\"\"Search for similar vectors\"\"\"\n        pass\n\n    @abstractmethod\n    def save(self) -&gt; bytes:\n        \"\"\"Save the vector store to disk\"\"\"\n        pass\n\n    @abstractmethod\n    def load(self, data: bytes) -&gt; 'AbstractVectorStore':\n        \"\"\"Load the vector store from disk\"\"\"\n        pass\n\n    @abstractmethod\n    def clear(self) -&gt; None:\n        \"\"\"Clear all data from the store\"\"\"\n        pass\n\n    @abstractmethod\n    def rebuild_index(self) -&gt; None:\n        \"\"\"Optional for faster searches\"\"\"\n        pass\n</code></pre> <code>add_embeddings(embeddings, chunks)</code> <code>abstractmethod</code> \u00b6 <p>Add embeddings and their corresponding chunks to the store</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/types.py</code> <pre><code>@abstractmethod\ndef add_embeddings(self, embeddings: np.ndarray, chunks: list[Chunk]) -&gt; None:\n    \"\"\"Add embeddings and their corresponding chunks to the store\"\"\"\n    pass\n</code></pre> <code>clear()</code> <code>abstractmethod</code> \u00b6 <p>Clear all data from the store</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/types.py</code> <pre><code>@abstractmethod\ndef clear(self) -&gt; None:\n    \"\"\"Clear all data from the store\"\"\"\n    pass\n</code></pre> <code>load(data)</code> <code>abstractmethod</code> \u00b6 <p>Load the vector store from disk</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/types.py</code> <pre><code>@abstractmethod\ndef load(self, data: bytes) -&gt; 'AbstractVectorStore':\n    \"\"\"Load the vector store from disk\"\"\"\n    pass\n</code></pre> <code>rebuild_index()</code> <code>abstractmethod</code> \u00b6 <p>Optional for faster searches</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/types.py</code> <pre><code>@abstractmethod\ndef rebuild_index(self) -&gt; None:\n    \"\"\"Optional for faster searches\"\"\"\n    pass\n</code></pre> <code>save()</code> <code>abstractmethod</code> \u00b6 <p>Save the vector store to disk</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/types.py</code> <pre><code>@abstractmethod\ndef save(self) -&gt; bytes:\n    \"\"\"Save the vector store to disk\"\"\"\n    pass\n</code></pre> <code>search(query_embedding, k=5, min_similarity=0.7)</code> <code>abstractmethod</code> \u00b6 <p>Search for similar vectors</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/types.py</code> <pre><code>@abstractmethod\ndef search(self, query_embedding: np.ndarray, k: int = 5, min_similarity: float = 0.7) -&gt; list[Chunk]:\n    \"\"\"Search for similar vectors\"\"\"\n    pass\n</code></pre> <code>Chunk</code> <code>dataclass</code> \u00b6 <p>Represents a chunk of text with its embedding and metadata</p> Source code in <code>toolboxv2/mods/isaa/base/VectorStores/types.py</code> <pre><code>@dataclass(slots=True)\nclass Chunk:\n    \"\"\"Represents a chunk of text with its embedding and metadata\"\"\"\n    text: str\n    embedding: np.ndarray\n    metadata: dict[str, Any]\n    content_hash: str\n    cluster_id: int | None = None\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.chainUi","title":"<code>chainUi</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.chainUi.delete_task_chain","title":"<code>delete_task_chain(app, request=None)</code>  <code>async</code>","text":"<p>Deletes a task chain.</p> Source code in <code>toolboxv2/mods/isaa/chainUi.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, request_as_kwarg=True, api_methods=['DELETE'])\nasync def delete_task_chain(app: App, request: Optional[RequestData] = None):\n    \"\"\"Deletes a task chain.\"\"\"\n    chain_name = request.query_params.get(\"chain_name\") if request and request.query_params else None\n    if not chain_name:\n        return Result.default_user_error(info=\"Chain name is required for deletion.\", exec_code=400)\n\n    isaa = get_isaa_instance(app)\n    try:\n        isaa.remove_task(chain_name)  # Removes from memory\n        isaa.save_task()  # Saves all chains, effectively removing the deleted one from file\n        # This also deletes the .chain.json file\n\n        # Delete associated Drawflow file if it exists\n        drawflow_file_path = isaa.agent_chain.directory / f\"{chain_name}.drawflow.json\"\n        if drawflow_file_path.exists():\n            try:\n                drawflow_file_path.unlink()\n                app.logger.info(f\"Deleted Drawflow data for chain '{chain_name}'.\")\n            except Exception as e:\n                app.logger.warning(f\"Could not delete Drawflow data file for chain '{chain_name}': {e}\")\n\n        return Result.ok(info=f\"Task chain '{chain_name}' deleted successfully.\")\n    except KeyError:  # If isaa.remove_task raises KeyError for non-existent chain\n        return Result.default_user_error(info=f\"Task chain '{chain_name}' not found.\", exec_code=404)\n    except Exception as e:\n        app.logger.error(f\"Error deleting task chain '{chain_name}': {e}\", exc_info=True)\n        return Result.custom_error(info=f\"Failed to delete task chain: {str(e)}\", exec_code=500)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.chainUi.get_task_chain_definition","title":"<code>get_task_chain_definition(app, request=None)</code>  <code>async</code>","text":"<p>Gets the definition of a specific task chain, including its Drawflow export if available.</p> Source code in <code>toolboxv2/mods/isaa/chainUi.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, request_as_kwarg=True, api_methods=['GET'])\nasync def get_task_chain_definition(app: App, request: Optional[RequestData] = None):\n    \"\"\"Gets the definition of a specific task chain, including its Drawflow export if available.\"\"\"\n    chain_name = request.query_params.get(\"chain_name\") if request and request.query_params else None\n    if not chain_name:\n        return Result.default_user_error(info=\"Chain name is required.\", exec_code=400)\n\n    isaa = get_isaa_instance(app)\n    try:\n        # Get logical task definition\n        tasks_list_dicts = isaa.get_task(chain_name)\n        if tasks_list_dicts is None:  # Check if chain exists\n            return Result.default_user_error(info=f\"Task chain '{chain_name}' not found.\", exec_code=404)\n\n        description = isaa.agent_chain.get_discr(chain_name) or \"\"\n\n        # Attempt to load Drawflow specific data if it exists\n        # This assumes Drawflow data is saved in a parallel file or embedded\n        drawflow_data = None\n        drawflow_file_path = isaa.agent_chain.directory / f\"{chain_name}.drawflow.json\"\n        if drawflow_file_path.exists():\n            try:\n                with open(drawflow_file_path, 'r') as f:\n                    drawflow_data = json.load(f)\n            except Exception as e:\n                app.logger.warning(f\"Could not load Drawflow data for chain '{chain_name}': {e}\")\n\n        chain_pydantic_tasks = [ISAAPydanticTask(**task_dict) for task_dict in tasks_list_dicts]\n\n        response_data = ISAAPydanticTaskChain(\n            name=chain_name,\n            description=description,\n            tasks=chain_pydantic_tasks\n        ).model_dump()\n\n        if drawflow_data:\n            response_data[\"drawflow_export\"] = drawflow_data  # Embed Drawflow data\n\n        return Result.json(data=response_data)\n\n    except Exception as e:\n        app.logger.error(f\"Error getting task chain definition for '{chain_name}': {e}\", exc_info=True)\n        return Result.custom_error(info=f\"Failed to get task chain definition: {str(e)}\", exec_code=500)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.chainUi.get_task_chain_editor_page_drawflow","title":"<code>get_task_chain_editor_page_drawflow(app, request=None)</code>  <code>async</code>","text":"<p>Serves the HTML page for the Drawflow-based Task Chain Editor.</p> Source code in <code>toolboxv2/mods/isaa/chainUi.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, name=\"task_chain_editor_drawflow\", api_methods=['GET'])\nasync def get_task_chain_editor_page_drawflow(app: App, request: Optional[RequestData] = None):\n    \"\"\"Serves the HTML page for the Drawflow-based Task Chain Editor.\"\"\"\n    if app is None:  # Should not happen if called via export\n        app = get_app()\n\n    # The Drawflow HTML and JS will be substantial.\n    # It's better to load it from a separate .html file for maintainability.\n    # For this example, I'll provide a condensed version here.\n    # In a real setup, use:\n    #   ui_file_path = Path(__file__).parent / \"task_chain_editor_drawflow.html\"\n    #   with open(ui_file_path, \"r\") as f:\n    #       html_content = f.read()\n    # And then inject app.web_context() if needed, or ensure tb.js handles it.\n\n    html_content = DRAWFLOW_TASK_CHAIN_EDITOR_HTML_TEMPLATE  # Defined below\n    return Result.html(data=app.web_context() + html_content)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.chainUi.get_task_chain_list","title":"<code>get_task_chain_list(app, request=None)</code>  <code>async</code>","text":"<p>Lists all available global task chains.</p> Source code in <code>toolboxv2/mods/isaa/chainUi.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, request_as_kwarg=True, api_methods=['GET'])\nasync def get_task_chain_list(app: App, request: Optional[RequestData] = None):\n    \"\"\"Lists all available global task chains.\"\"\"\n    isaa = get_isaa_instance(app)\n    try:\n        chain_names = list(isaa.agent_chain.chains.keys() ) # This should return List[str]\n        return Result.json(data=chain_names)\n    except Exception as e:\n        app.logger.error(f\"Error listing task chains: {e}\", exc_info=True)\n        return Result.custom_error(info=f\"Failed to list task chains: {str(e)}\", exec_code=500)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.chainUi.initialize_module","title":"<code>initialize_module(app)</code>","text":"<p>Initializes the ISAA ChainUI module and registers its UI with CloudM.</p> Source code in <code>toolboxv2/mods/isaa/chainUi.py</code> <pre><code>@export(mod_name=MOD_NAME, version=VERSION)\ndef initialize_module(app: App):\n    \"\"\"Initializes the ISAA ChainUI module and registers its UI with CloudM.\"\"\"\n    print(f\"ISAA Drawflow ChainUI Modul ({MOD_NAME} v{VERSION}) initialisiert.\")\n    if app is None:\n        app = get_app()\n\n    # Register the new Drawflow-based Task Chain Editor UI\n    app.run_any((\"CloudM\", \"add_ui\"),\n                name=f\"{Name}_TaskChainEditorDrawflow\",  # Unique name\n                title=\"Task Chain Editor (Drawflow)\",\n                path=f\"/api/{Name}/task_chain_editor_drawflow\",  # Unique path\n                description=\"Visual editor for ISAA Task Chains using Drawflow.\",\n                auth=True\n                )\n    return Result.ok(info=\"ISAA Drawflow ChainUI Modul und Editor UI bereit.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.chainUi.run_chain_visualized","title":"<code>run_chain_visualized(app, request=None, data=None)</code>  <code>async</code>","text":"<p>Executes a specified task chain with the given input.</p> Source code in <code>toolboxv2/mods/isaa/chainUi.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, request_as_kwarg=True, api_methods=['POST'])\nasync def run_chain_visualized(app: App, request: Optional[RequestData] = None, data: RunChainRequest = None):\n    \"\"\"Executes a specified task chain with the given input.\"\"\"\n    if not data:  # Compatibility\n        if request and request.body and isinstance(request.body, dict):\n            try:\n                data = RunChainRequest(**request.body)\n            except Exception as e:\n                return Result.default_user_error(info=f\"Invalid run chain data: {e}\", exec_code=400)\n        else:\n            return Result.default_user_error(info=\"No run chain data provided.\", exec_code=400)\n    elif isinstance(data, dict):\n        data = RunChainRequest(**data)\n\n    if not data.chain_name:\n        return Result.default_user_error(info=\"Chain name is required for execution.\", exec_code=400)\n    if data.task_input is None:  # Allow empty string as input\n        return Result.default_user_error(info=\"Task input is required.\", exec_code=400)\n\n    isaa = get_isaa_instance(app)\n\n    # TODO: Add SSE streaming for execution progress if desired in the future.\n    # For now, simple blocking execution.\n\n    try:\n        # If chain_definition is provided, use it directly (for running unsaved chains)\n        if data.chain_definition:\n            app.logger.info(\n                f\"Executing unsaved chain definition for '{data.chain_name}' with input: {data.task_input[:50]}...\")\n            # Temporarily add this chain definition to isaa.agent_chain without saving to file\n            # This requires isaa.agent_chain to support in-memory, non-persistent additions or direct execution\n            # For simplicity, let's assume isaa.run_task can accept a task list directly if AgentChain is adapted.\n            # If not, we'd save it temporarily or find another way.\n            # For now, assuming run_task primarily uses named, saved chains.\n            # A more robust solution would be to modify `isaa.run_task` or `ChainTreeExecutor`\n            # to accept a raw list of task dictionaries.\n            # This example will proceed assuming the chain must be saved first if not already.\n            # Let's add a note that this feature (running unsaved chains from UI) needs more work on ISAA core.\n            app.logger.warning(\n                \"Running unsaved chain definitions directly is not fully supported by this endpoint version. The chain should be saved first.\")\n            # Fallback to trying to run by name, assuming it was saved.\n\n        app.logger.info(f\"Executing chain '{data.chain_name}' with input: {data.task_input[:50]}...\")\n        # `isaa.run_task` is already async\n        execution_result = await isaa.run_task(task_input=data.task_input, chain_name=data.chain_name)\n\n        # `execution_result` structure depends on `ChainTreeExecutor.execute`\n        # It's usually a dictionary of results.\n        return Result.json(data={\"output\": execution_result, \"final_message\": \"Chain execution completed.\"})\n\n    except Exception as e:\n        app.logger.error(f\"Error executing task chain '{data.chain_name}': {e}\", exc_info=True)\n        return Result.custom_error(info=f\"Chain execution failed: {str(e)}\", exec_code=500)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.chainUi.save_task_chain_definition","title":"<code>save_task_chain_definition(app, request=None, data=None)</code>  <code>async</code>","text":"<p>Saves a task chain definition. Expects logical tasks and optional Drawflow export.</p> Source code in <code>toolboxv2/mods/isaa/chainUi.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, version=VERSION, request_as_kwarg=True, api_methods=['POST'])\nasync def save_task_chain_definition(app: App, request: Optional[RequestData] = None,\n                                     data: SaveTaskChainRequest = None):\n    \"\"\"Saves a task chain definition. Expects logical tasks and optional Drawflow export.\"\"\"\n    if not data:  # Compatibility for direct data passthrough if decorator doesn't parse body for Pydantic model\n        if request and request.body and isinstance(request.body, dict):\n            try:\n                data = SaveTaskChainRequest(**request.body)\n            except Exception as e:\n                return Result.default_user_error(info=f\"Invalid chain data provided: {e}\", exec_code=400)\n        else:\n            return Result.default_user_error(info=\"No chain data provided.\", exec_code=400)\n    elif isinstance(data, dict):\n        data = SaveTaskChainRequest(**data)\n\n    if not data.name:\n        return Result.default_user_error(info=\"Chain name cannot be empty.\", exec_code=400)\n\n    isaa = get_isaa_instance(app)\n    try:\n        # Save logical tasks to ISAA's AgentChain\n        task_dicts = [task.model_dump() for task in data.tasks]\n        isaa.add_task(data.name, task_dicts)  # add_task replaces if exists\n        if data.description is not None:  # Allow empty description\n            isaa.agent_chain.add_discr(data.name, data.description)\n        isaa.save_task(data.name)  # Persists the .chain.json file\n\n        # Save Drawflow specific data if provided\n        if data.drawflow_export:\n            drawflow_file_path = Path(isaa.agent_chain.directory) / f\"{data.name}.drawflow.json\"\n            try:\n                with open(drawflow_file_path, 'w') as f:\n                    json.dump(data.drawflow_export, f, indent=2)\n                app.logger.info(f\"Saved Drawflow data for chain '{data.name}' to {drawflow_file_path}\")\n            except Exception as e:\n                app.logger.error(f\"Failed to save Drawflow data for chain '{data.name}': {e}\")\n                # Optionally, inform client that logical save succeeded but visual save failed\n                return Result.ok(\n                    info=f\"Task chain '{data.name}' saved (logical part), but Drawflow visual data failed to save.\")\n\n        return Result.ok(info=f\"Task chain '{data.name}' saved successfully.\")\n\n    except Exception as e:\n        app.logger.error(f\"Error saving task chain '{data.name}': {e}\", exc_info=True)\n        return Result.custom_error(info=f\"Failed to save task chain: {str(e)}\", exec_code=500)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras","title":"<code>extras</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.adapter","title":"<code>adapter</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.adapter--litellm-llm-interface-module","title":"LiteLLM LLM Interface Module","text":"<p>This module provides interfaces for interacting with LiteLLM's language models, including text generation and embedding capabilities.</p> <p>Author: Lightrag Team Created: 2025-02-04 License: MIT License Version: 1.0.0</p> <p>Change Log: - 1.0.0 (2025-02-04): Initial LiteLLM release     * Ported OpenAI logic to use litellm async client     * Updated error types and environment variable names     * Preserved streaming and embedding support</p> Dependencies <ul> <li>litellm</li> <li>numpy</li> <li>pipmaster</li> <li>Python &gt;= 3.10</li> </ul> Usage <p>from llm_interfaces.litellm import logging</p> <p>if not hasattr(logging, 'NONE'):     logging.NONE = 100</p> <p>import litellm_complete, litellm_embed</p>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.adapter.litellm_complete","title":"<code>litellm_complete(prompt, system_prompt=None, history_messages=None, keyword_extraction=False, model_name='groq/gemma2-9b-it', **kwargs)</code>  <code>async</code>","text":"<p>Public completion interface using the model name specified in the global configuration. Optionally extracts keywords if requested.</p> Source code in <code>toolboxv2/mods/isaa/extras/adapter.py</code> <pre><code>async def litellm_complete(\n    prompt, system_prompt=None, history_messages=None, keyword_extraction=False, model_name = \"groq/gemma2-9b-it\", **kwargs\n) -&gt; str | AsyncIterator[str]:\n    \"\"\"\n    Public completion interface using the model name specified in the global configuration.\n    Optionally extracts keywords if requested.\n    \"\"\"\n    if history_messages is None:\n        history_messages = []\n    # Check and set response format for keyword extraction if needed\n    keyword_extraction_flag = kwargs.pop(\"keyword_extraction\", None)\n    if keyword_extraction_flag:\n        kwargs[\"response_format\"] = \"json\"\n\n    if \"response_format\" in kwargs:\n        if isinstance(kwargs[\"response_format\"], dict):\n            kwargs[\"response_format\"] = enforce_no_additional_properties(kwargs[\"response_format\"])\n        elif isinstance(kwargs[\"response_format\"], str):\n            pass\n        else:\n            kwargs[\"response_format\"] = enforce_no_additional_properties(kwargs[\"response_format\"].model_json_schema())  # oder .schema() in v1\n     # kwargs[\"hashing_kv\"].global_config[\"llm_model_name\"]\n\n    if any(x in model_name for x in [\"mistral\", \"mixtral\"]):\n        kwargs.pop(\"response_format\", None)\n\n    return await litellm_complete_if_cache(\n        model_name,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        **kwargs,\n    )\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.adapter.litellm_complete_if_cache","title":"<code>litellm_complete_if_cache(model, prompt, system_prompt=None, history_messages=None, base_url=None, api_key=None, **kwargs)</code>  <code>async</code>","text":"<p>Core function to query the LiteLLM model. It builds the message context, invokes the completion API, and returns either a complete result string or an async iterator for streaming responses.</p> Source code in <code>toolboxv2/mods/isaa/extras/adapter.py</code> <pre><code>@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n    retry=retry_if_exception_type((RateLimitError, Timeout, APIConnectionError)),\n)\nasync def litellm_complete_if_cache(\n    model,\n    prompt,\n    system_prompt=None,\n    history_messages=None,\n    base_url=None,\n    api_key=None,\n    **kwargs,\n) -&gt; str | AsyncIterator[str]:\n    \"\"\"\n    Core function to query the LiteLLM model. It builds the message context,\n    invokes the completion API, and returns either a complete result string or\n    an async iterator for streaming responses.\n    \"\"\"\n    # Set the API key if provided\n    if api_key:\n        os.environ[\"LITELLM_API_KEY\"] = api_key\n\n    # Remove internal keys not needed for the client call\n    kwargs.pop(\"hashing_kv\", None)\n    kwargs.pop(\"keyword_extraction\", None)\n\n    fallbacks_ = kwargs.pop(\"fallbacks\", [])\n    # Build the messages list from system prompt, conversation history, and the new prompt\n    messages = []\n    if system_prompt:\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\n    if history_messages is not None:\n        messages.extend(history_messages)\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    # Log query details for debugging purposes\n    try:\n        # Depending on the response format, choose the appropriate API call\n        if \"response_format\" in kwargs:\n            response = await acompletion(\n                model=model, messages=messages,\n                fallbacks=fallbacks_+os.getenv(\"FALLBACKS_MODELS\", '').split(','),\n                **kwargs\n            )\n        else:\n            response = await acompletion(\n                model=model, messages=messages,\n                fallbacks=os.getenv(\"FALLBACKS_MODELS\", '').split(','),\n                **kwargs\n            )\n    except Exception as e:\n        print(f\"\\n{model=}\\n{prompt=}\\n{system_prompt=}\\n{history_messages=}\\n{base_url=}\\n{api_key=}\\n{kwargs=}\")\n        get_logger().error(f\"Failed to litellm memory work {e}\")\n        return \"\"\n\n    # Check if the response is a streaming response (i.e. an async iterator)\n    if hasattr(response, \"__aiter__\"):\n\n        async def inner():\n            async for chunk in response:\n                # Assume LiteLLM response structure is similar to OpenAI's\n                content = chunk.choices[0].delta.content\n                if content is None:\n                    continue\n                yield content\n\n        return inner()\n    else:\n        # Non-streaming: extract and return the full content string\n\n        content = response.choices[0].message.content\n        if content is None:\n            content = response.choices[0].message.tool_calls[0].function.arguments\n        return content\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.adapter.litellm_embed","title":"<code>litellm_embed(texts, model='gemini/text-embedding-004', base_url=None, api_key=None)</code>  <code>async</code>","text":"<p>Generates embeddings for the given list of texts using LiteLLM.</p> Source code in <code>toolboxv2/mods/isaa/extras/adapter.py</code> <pre><code>@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=60),\n    retry=retry_if_exception_type((RateLimitError, Timeout, APIConnectionError)),\n)\nasync def litellm_embed(\n    texts: list[str],\n    model: str = \"gemini/text-embedding-004\",\n    base_url: str = None,\n    api_key: str = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Generates embeddings for the given list of texts using LiteLLM.\n    \"\"\"\n    response = await litellm.aembedding(\n        model=model, input=texts,\n        # encoding_format=\"float\"\n    )\n    return np.array([dp.embedding for dp in response.data])\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.filter","title":"<code>filter</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.filter.filter_relevant_texts","title":"<code>filter_relevant_texts(query, texts, fuzzy_threshold=70, semantic_threshold=0.75, model=None)</code>","text":"<p>Filters a list of texts based on their relevance to the query. It first uses a fuzzy matching score and, if that score is below the threshold, it then checks the semantic similarity.</p> <p>:param query: The query string. :param texts: List of page texts. :param fuzzy_threshold: Fuzzy matching score threshold (0-100). :param semantic_threshold: Semantic similarity threshold (0.0-1.0). :param model: A preloaded SentenceTransformer model (if None, one will be loaded). :return: Filtered list of texts deemed relevant.</p> Source code in <code>toolboxv2/mods/isaa/extras/filter.py</code> <pre><code>def filter_relevant_texts(query: str,\n                          texts: list[str],\n                          fuzzy_threshold: int = 70,\n                          semantic_threshold: float = 0.75,\n                          model = None) -&gt; list[str]:\n    \"\"\"\n    Filters a list of texts based on their relevance to the query.\n    It first uses a fuzzy matching score and, if that score is below the threshold,\n    it then checks the semantic similarity.\n\n    :param query: The query string.\n    :param texts: List of page texts.\n    :param fuzzy_threshold: Fuzzy matching score threshold (0-100).\n    :param semantic_threshold: Semantic similarity threshold (0.0-1.0).\n    :param model: A preloaded SentenceTransformer model (if None, one will be loaded).\n    :return: Filtered list of texts deemed relevant.\n    \"\"\"\n    try:\n        from rapidfuzz import fuzz\n    except Exception:\n        os.system([sys.executable, '-m', 'pip', 'install', 'RapidFuzz'])\n        from rapidfuzz import fuzz\n    try:\n        from sentence_transformers import SentenceTransformer, util\n    except Exception:\n        os.system([sys.executable, '-m', 'pip', 'install', 'sentence-transformers'])\n        from sentence_transformers import SentenceTransformer, util\n\n    if model is None:\n        # For efficiency, consider pre-loading this model outside the function.\n        model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\n    # Pre-compute query embedding for the semantic check:\n    query_embedding = model.encode(query, convert_to_tensor=True)\n\n    relevant_texts = []\n    for text in texts:\n        # --- Fuzzy Keyword Filtering ---\n        fuzzy_score = fuzz.partial_ratio(query.lower(), text.lower())\n        if fuzzy_score &gt;= fuzzy_threshold:\n            relevant_texts.append(text)\n        else:\n            # --- Semantic Similarity Filtering ---\n            text_embedding = model.encode(text, convert_to_tensor=True)\n            similarity = util.pytorch_cos_sim(query_embedding, text_embedding).item()\n            if similarity &gt;= semantic_threshold:\n                relevant_texts.append(text)\n    return relevant_texts\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.modes","title":"<code>modes</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.modes.generate_prompt","title":"<code>generate_prompt(subject, context='', additional_requirements=None)</code>","text":"<p>Generates a prompt based on the given subject, with optional context and additional requirements.</p> <p>Parameters: - subject (str): The main subject for the prompt. - context (str): Optional additional context to tailor the prompt. - additional_requirements (Dict[str, Any]): Optional additional parameters or requirements for the prompt.</p> <p>Returns: - str: A crafted prompt.</p> Source code in <code>toolboxv2/mods/isaa/extras/modes.py</code> <pre><code>def generate_prompt(subject: str, context: str = \"\", additional_requirements: dict[str, Any] = None) -&gt; str:\n    \"\"\"\n    Generates a prompt based on the given subject, with optional context and additional requirements.\n\n    Parameters:\n    - subject (str): The main subject for the prompt.\n    - context (str): Optional additional context to tailor the prompt.\n    - additional_requirements (Dict[str, Any]): Optional additional parameters or requirements for the prompt.\n\n    Returns:\n    - str: A crafted prompt.\n    \"\"\"\n    prompt = f\"Based on the subject '{subject}', with the context '{context}', generate a clear and precise instruction.\"\n    if additional_requirements:\n        prompt += f\" Consider the following requirements: {additional_requirements}.\"\n    return prompt\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress","title":"<code>terminal_progress</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.AgentModelData","title":"<code>AgentModelData</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>class AgentModelData(BaseModel):\n    name: str = \"FlowAgent\"\n    fast_llm_model: str = \"openrouter/anthropic/claude-3-haiku\"\n    complex_llm_model: str = \"openrouter/openai/gpt-4o\"\n    system_message: str = \"You are a production-ready autonomous agent.\"\n    temperature: float = 0.7\n    max_tokens: int = 2048\n    max_input_tokens: int = 32768\n    api_key: Optional[str] = None\n    api_base: Optional[str] = None\n    budget_manager: Optional[Any] = None\n    caching: bool = True\n    persona: Optional[PersonaConfig] = None\n    use_fast_response: bool = True\n\n    def get_system_message_with_persona(self) -&gt; str:\n        \"\"\"Get system message with persona integration\"\"\"\n        base_message = self.system_message\n\n        if self.persona and self.persona.apply_method in [\"system_prompt\", \"both\"]:\n            persona_addition = self.persona.to_system_prompt_addition()\n            if persona_addition:\n                base_message += f\"\\n\\n## Persona Instructions\\n{persona_addition}\"\n\n        return base_message\n</code></pre> <code>get_system_message_with_persona()</code> \u00b6 <p>Get system message with persona integration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def get_system_message_with_persona(self) -&gt; str:\n    \"\"\"Get system message with persona integration\"\"\"\n    base_message = self.system_message\n\n    if self.persona and self.persona.apply_method in [\"system_prompt\", \"both\"]:\n        persona_addition = self.persona.to_system_prompt_addition()\n        if persona_addition:\n            base_message += f\"\\n\\n## Persona Instructions\\n{persona_addition}\"\n\n    return base_message\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.CompoundTask","title":"<code>CompoundTask</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Task</code></p> <p>Task der Sub-Tasks gruppiert</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass CompoundTask(Task):\n    \"\"\"Task der Sub-Tasks gruppiert\"\"\"\n    sub_task_ids: List[str] = field(default_factory=list)\n    execution_strategy: str = \"sequential\"  # \"sequential\" | \"parallel\"\n    success_criteria: str = \"\"  # Wann ist der Compound-Task erfolgreich?\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.DecisionTask","title":"<code>DecisionTask</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Task</code></p> <p>Task f\u00fcr dynamisches Routing</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass DecisionTask(Task):\n    \"\"\"Task f\u00fcr dynamisches Routing\"\"\"\n    decision_prompt: str = \"\"  # Kurze Frage an LLM\n    routing_map: Dict[str, str] = field(default_factory=dict)  # Ergebnis -&gt; n\u00e4chster Task\n    decision_model: str = \"fast\"  # Welches LLM f\u00fcr Entscheidung\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.ExecutionNode","title":"<code>ExecutionNode</code>","text":"<p>Enhanced execution node with better status management</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>class ExecutionNode:\n    \"\"\"Enhanced execution node with better status management\"\"\"\n\n    def __init__(self, name: str, node_type: str = \"unknown\"):\n        self.name = name\n        self.node_type = node_type\n        self.start_time: Optional[float] = None\n        self.end_time: Optional[float] = None\n        self.duration: Optional[float] = None\n        self.phase: str = \"unknown\"\n\n        # Enhanced status management\n        self.status: NodeStatus = NodeStatus.PENDING\n        self.previous_status: Optional[NodeStatus] = None\n        self.status_history: List[Dict[str, Any]] = []\n\n        # Error handling\n        self.error: Optional[str] = None\n        self.error_details: Optional[Dict[str, Any]] = None\n        self.retry_count: int = 0\n\n        # Child operations\n        self.llm_calls: List[ProgressEvent] = []\n        self.tool_calls: List[ProgressEvent] = []\n        self.sub_events: List[ProgressEvent] = []\n\n        # Enhanced metadata\n        self.reasoning: Optional[str] = None\n        self.strategy: Optional[str] = None\n        self.routing_from: Optional[str] = None\n        self.routing_to: Optional[str] = None\n        self.completion_criteria: Optional[Dict[str, Any]] = None\n\n        # Stats\n        self.total_cost: float = 0.0\n        self.total_tokens: int = 0\n        self.performance_metrics: Dict[str, Any] = {}\n\n    def update_status(self, new_status: NodeStatus, reason: str = \"\", error_details: Dict = None):\n        \"\"\"Update node status with history tracking\"\"\"\n        if new_status != self.status:\n            self.previous_status = self.status\n            self.status_history.append({\n                \"from\": self.status.value if self.status else None,\n                \"to\": new_status.value,\n                \"timestamp\": time.time(),\n                \"reason\": reason,\n                \"error_details\": error_details\n            })\n            self.status = new_status\n\n            if error_details:\n                self.error_details = error_details\n                self.error = error_details.get(\"error\", reason)\n\n    def add_event(self, event: ProgressEvent):\n        \"\"\"Enhanced event processing with auto-completion detection\"\"\"\n        # Categorize event\n        if event.event_type == \"llm_call\":\n            self.llm_calls.append(event)\n            if event.llm_cost:\n                self.total_cost += event.llm_cost\n            if event.llm_total_tokens:\n                self.total_tokens += event.llm_total_tokens\n\n        elif event.event_type == \"tool_call\":\n            self.tool_calls.append(event)\n\n        else:\n            self.sub_events.append(event)\n\n        # Update node info from metadata\n        if event.metadata:\n            if \"strategy\" in event.metadata:\n                self.strategy = event.metadata[\"strategy\"]\n            if \"reasoning\" in event.metadata:\n                self.reasoning = event.metadata[\"reasoning\"]\n\n        # Update routing info\n        if event.routing_from:\n            self.routing_from = event.routing_from\n        if event.routing_to:\n            self.routing_to = event.routing_to\n\n        # Auto-completion detection\n        self._detect_completion(event)\n\n        # Update timing\n        if not self.start_time:\n            self.start_time = event.timestamp\n\n        # Status updates based on event\n        self._update_status_from_event(event)\n\n    def _detect_completion(self, event: ProgressEvent):\n        \"\"\"Detect node completion based on various criteria\"\"\"\n\n        # Check for explicit completion signals from flows or the entire execution\n        if event.event_type in [\"node_exit\", \"execution_complete\", \"task_complete\"] or event.success:\n            # This logic correctly handles the completion of Flows (like TaskManagementFlow)\n            if event.node_duration:\n                self.duration = event.node_duration\n                self.end_time = event.timestamp\n                self.update_status(NodeStatus.COMPLETED, \"Explicit completion signal\")\n                return\n\n        # --- KORRIGIERTER ABSCHNITT START ---\n        # General auto-completion for simple Nodes (not Flows) after their main action.\n        # This replaces the hardcoded rule for just \"StrategyOrchestratorNode\".\n        is_simple_node = \"Flow\" not in self.name\n        is_finalizing_event = event.event_type in [\"llm_call\", \"tool_call\", \"node_phase\"] and event.success\n\n        if is_simple_node and is_finalizing_event:\n            # A simple node is often considered \"done\" after its last successful major operation.\n            self.end_time = event.timestamp\n            # If the event provides a duration for the whole node, use it. Otherwise, calculate from start.\n            if event.node_duration:\n                self.duration = event.node_duration\n            elif self.start_time:\n                self.duration = self.end_time - self.start_time\n\n            self.update_status(NodeStatus.COMPLETED, f\"Auto-detected completion after successful '{event.event_type}'\")\n            return\n\n        # Error-based completion detection\n        if event.event_type == \"error\" or event.success is False:\n            print(event.metadata, event.event_type, event.event_id)\n            print(\"=\" * 200)\n            self.update_status(NodeStatus.FAILED, \"Error detected\", {\n                \"error\": event.metadata.get(\"error\", (\n                    event.tool_error if hasattr(event, 'tool_error') else \"Unknown error\") or \"Unknown error\"),\n                \"error_type\": event.metadata.get(\"error_type\", \"UnknownError\")\n            })\n            if event.node_duration:\n                self.duration = event.node_duration\n                self.end_time = event.timestamp\n\n    def _update_status_from_event(self, event: ProgressEvent):\n        \"\"\"Update status based on incoming events\"\"\"\n\n        if event.event_type == \"node_enter\":\n            self.update_status(NodeStatus.STARTING, \"Node entered\")\n\n        elif event.event_type in [\"llm_call\", \"tool_call\"] and self.status == NodeStatus.STARTING:\n            self.update_status(NodeStatus.RUNNING, f\"Started {event.event_type}\")\n\n        elif event.event_type == \"error\":\n            self.update_status(NodeStatus.FAILED, \"Error occurred\", {\n                \"error\": event.metadata.get(\"error\"),\n                \"error_type\": event.metadata.get(\"error_type\")\n            })\n\n    def is_completed(self) -&gt; bool:\n        \"\"\"Check if node is truly completed\"\"\"\n        return self.status in [NodeStatus.COMPLETED, NodeStatus.FAILED, NodeStatus.SKIPPED]\n\n    def is_active(self) -&gt; bool:\n        \"\"\"Check if node is currently active\"\"\"\n        return self.status in [NodeStatus.STARTING, NodeStatus.RUNNING, NodeStatus.WAITING]\n\n    def get_status_icon(self) -&gt; str:\n        \"\"\"Get appropriate status icon\"\"\"\n        icons = {\n            NodeStatus.PENDING: \"\u23f8\ufe0f\",\n            NodeStatus.STARTING: \"\ud83d\udd04\",\n            NodeStatus.RUNNING: \"\ud83d\udd04\",\n            NodeStatus.WAITING: \"\u23f8\ufe0f\",\n            NodeStatus.COMPLETING: \"\ud83d\udd04\",\n            NodeStatus.COMPLETED: \"\u2705\",\n            NodeStatus.FAILED: \"\u274c\",\n            NodeStatus.SKIPPED: \"\u23ed\ufe0f\"\n        }\n        return icons.get(self.status, \"\u2753\")\n\n    def get_status_color(self) -&gt; str:\n        \"\"\"Get appropriate color for rich console\"\"\"\n        colors = {\n            NodeStatus.PENDING: \"yellow dim\",\n            NodeStatus.STARTING: \"yellow\",\n            NodeStatus.RUNNING: \"blue bold\",\n            NodeStatus.WAITING: \"yellow dim\",\n            NodeStatus.COMPLETING: \"green\",\n            NodeStatus.COMPLETED: \"green bold\",\n            NodeStatus.FAILED: \"red bold\",\n            NodeStatus.SKIPPED: \"cyan dim\"\n        }\n        return colors.get(self.status, \"white\")\n\n    def get_duration_str(self) -&gt; str:\n        \"\"\"Enhanced duration string with better formatting\"\"\"\n        if self.duration:\n            if self.duration &lt; 1:\n                return f\"{self.duration * 1000:.0f}ms\"\n            elif self.duration &lt; 60:\n                return f\"{self.duration:.1f}s\"\n            elif self.duration &lt; 3600:\n                minutes = int(self.duration // 60)\n                seconds = self.duration % 60\n                return f\"{minutes}m{seconds:.1f}s\"\n            else:\n                hours = int(self.duration // 3600)\n                minutes = int((self.duration % 3600) // 60)\n                return f\"{hours}h{minutes}m\"\n        elif self.start_time and self.status in [NodeStatus.RUNNING, NodeStatus.STARTING]:\n            elapsed = time.time() - self.start_time\n            return f\"~{elapsed:.1f}s\"\n        return \"...\"\n\n    def get_performance_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Get comprehensive performance metrics\"\"\"\n        return {\n            \"duration\": self.duration,\n            \"total_cost\": self.total_cost,\n            \"total_tokens\": self.total_tokens,\n            \"llm_calls\": len(self.llm_calls),\n            \"tool_calls\": len(self.tool_calls),\n            \"retry_count\": self.retry_count,\n            \"status_changes\": len(self.status_history),\n            \"efficiency_score\": self._calculate_efficiency_score()\n        }\n\n    def _calculate_efficiency_score(self) -&gt; float:\n        \"\"\"Calculate efficiency score based on various metrics\"\"\"\n        if not self.duration:\n            return 0.0\n\n        # Base score\n        score = 1.0\n\n        # Penalize long durations (relative to complexity)\n        complexity = len(self.llm_calls) + len(self.tool_calls)\n        if complexity &gt; 0:\n            expected_duration = complexity * 2  # 2 seconds per operation\n            if self.duration &gt; expected_duration:\n                score *= 0.8\n\n        # Penalize retries\n        if self.retry_count &gt; 0:\n            score *= (0.9 ** self.retry_count)\n\n        # Bonus for successful completion\n        if self.status == NodeStatus.COMPLETED:\n            score *= 1.1\n\n        return max(0.0, min(1.0, score))\n</code></pre> <code>add_event(event)</code> \u00b6 <p>Enhanced event processing with auto-completion detection</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def add_event(self, event: ProgressEvent):\n    \"\"\"Enhanced event processing with auto-completion detection\"\"\"\n    # Categorize event\n    if event.event_type == \"llm_call\":\n        self.llm_calls.append(event)\n        if event.llm_cost:\n            self.total_cost += event.llm_cost\n        if event.llm_total_tokens:\n            self.total_tokens += event.llm_total_tokens\n\n    elif event.event_type == \"tool_call\":\n        self.tool_calls.append(event)\n\n    else:\n        self.sub_events.append(event)\n\n    # Update node info from metadata\n    if event.metadata:\n        if \"strategy\" in event.metadata:\n            self.strategy = event.metadata[\"strategy\"]\n        if \"reasoning\" in event.metadata:\n            self.reasoning = event.metadata[\"reasoning\"]\n\n    # Update routing info\n    if event.routing_from:\n        self.routing_from = event.routing_from\n    if event.routing_to:\n        self.routing_to = event.routing_to\n\n    # Auto-completion detection\n    self._detect_completion(event)\n\n    # Update timing\n    if not self.start_time:\n        self.start_time = event.timestamp\n\n    # Status updates based on event\n    self._update_status_from_event(event)\n</code></pre> <code>get_duration_str()</code> \u00b6 <p>Enhanced duration string with better formatting</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def get_duration_str(self) -&gt; str:\n    \"\"\"Enhanced duration string with better formatting\"\"\"\n    if self.duration:\n        if self.duration &lt; 1:\n            return f\"{self.duration * 1000:.0f}ms\"\n        elif self.duration &lt; 60:\n            return f\"{self.duration:.1f}s\"\n        elif self.duration &lt; 3600:\n            minutes = int(self.duration // 60)\n            seconds = self.duration % 60\n            return f\"{minutes}m{seconds:.1f}s\"\n        else:\n            hours = int(self.duration // 3600)\n            minutes = int((self.duration % 3600) // 60)\n            return f\"{hours}h{minutes}m\"\n    elif self.start_time and self.status in [NodeStatus.RUNNING, NodeStatus.STARTING]:\n        elapsed = time.time() - self.start_time\n        return f\"~{elapsed:.1f}s\"\n    return \"...\"\n</code></pre> <code>get_performance_summary()</code> \u00b6 <p>Get comprehensive performance metrics</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def get_performance_summary(self) -&gt; Dict[str, Any]:\n    \"\"\"Get comprehensive performance metrics\"\"\"\n    return {\n        \"duration\": self.duration,\n        \"total_cost\": self.total_cost,\n        \"total_tokens\": self.total_tokens,\n        \"llm_calls\": len(self.llm_calls),\n        \"tool_calls\": len(self.tool_calls),\n        \"retry_count\": self.retry_count,\n        \"status_changes\": len(self.status_history),\n        \"efficiency_score\": self._calculate_efficiency_score()\n    }\n</code></pre> <code>get_status_color()</code> \u00b6 <p>Get appropriate color for rich console</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def get_status_color(self) -&gt; str:\n    \"\"\"Get appropriate color for rich console\"\"\"\n    colors = {\n        NodeStatus.PENDING: \"yellow dim\",\n        NodeStatus.STARTING: \"yellow\",\n        NodeStatus.RUNNING: \"blue bold\",\n        NodeStatus.WAITING: \"yellow dim\",\n        NodeStatus.COMPLETING: \"green\",\n        NodeStatus.COMPLETED: \"green bold\",\n        NodeStatus.FAILED: \"red bold\",\n        NodeStatus.SKIPPED: \"cyan dim\"\n    }\n    return colors.get(self.status, \"white\")\n</code></pre> <code>get_status_icon()</code> \u00b6 <p>Get appropriate status icon</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def get_status_icon(self) -&gt; str:\n    \"\"\"Get appropriate status icon\"\"\"\n    icons = {\n        NodeStatus.PENDING: \"\u23f8\ufe0f\",\n        NodeStatus.STARTING: \"\ud83d\udd04\",\n        NodeStatus.RUNNING: \"\ud83d\udd04\",\n        NodeStatus.WAITING: \"\u23f8\ufe0f\",\n        NodeStatus.COMPLETING: \"\ud83d\udd04\",\n        NodeStatus.COMPLETED: \"\u2705\",\n        NodeStatus.FAILED: \"\u274c\",\n        NodeStatus.SKIPPED: \"\u23ed\ufe0f\"\n    }\n    return icons.get(self.status, \"\u2753\")\n</code></pre> <code>is_active()</code> \u00b6 <p>Check if node is currently active</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def is_active(self) -&gt; bool:\n    \"\"\"Check if node is currently active\"\"\"\n    return self.status in [NodeStatus.STARTING, NodeStatus.RUNNING, NodeStatus.WAITING]\n</code></pre> <code>is_completed()</code> \u00b6 <p>Check if node is truly completed</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def is_completed(self) -&gt; bool:\n    \"\"\"Check if node is truly completed\"\"\"\n    return self.status in [NodeStatus.COMPLETED, NodeStatus.FAILED, NodeStatus.SKIPPED]\n</code></pre> <code>update_status(new_status, reason='', error_details=None)</code> \u00b6 <p>Update node status with history tracking</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def update_status(self, new_status: NodeStatus, reason: str = \"\", error_details: Dict = None):\n    \"\"\"Update node status with history tracking\"\"\"\n    if new_status != self.status:\n        self.previous_status = self.status\n        self.status_history.append({\n            \"from\": self.status.value if self.status else None,\n            \"to\": new_status.value,\n            \"timestamp\": time.time(),\n            \"reason\": reason,\n            \"error_details\": error_details\n        })\n        self.status = new_status\n\n        if error_details:\n            self.error_details = error_details\n            self.error = error_details.get(\"error\", reason)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.ExecutionTreeBuilder","title":"<code>ExecutionTreeBuilder</code>","text":"<p>Enhanced execution tree builder with better error handling and metrics</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>class ExecutionTreeBuilder:\n    \"\"\"Enhanced execution tree builder with better error handling and metrics\"\"\"\n\n    def __init__(self):\n        self.nodes: Dict[str, ExecutionNode] = {}\n        self.execution_flow: List[str] = []\n        self.current_node: Optional[str] = None\n        self.root_node: Optional[str] = None\n        self.routing_history: List[Dict[str, str]] = []\n\n        # Enhanced tracking\n        self.error_log: List[Dict[str, Any]] = []\n        self.completion_order: List[str] = []\n        self.active_nodes: Set[str] = set()\n\n        # Stats\n        self.start_time: Optional[float] = None\n        self.end_time: Optional[float] = None\n        self.total_cost: float = 0.0\n        self.total_tokens: int = 0\n        self.total_events: int = 0\n        self.session_id: Optional[str] = None\n\n    def add_event(self, event: ProgressEvent):\n        \"\"\"Enhanced event processing with better error handling\"\"\"\n        try:\n            self.total_events += 1\n\n            if not self.start_time:\n                self.start_time = event.timestamp\n                self.session_id = event.session_id\n\n            # Create or update node\n            node_name = event.node_name\n            if node_name not in self.nodes:\n                self.nodes[node_name] = ExecutionNode(node_name, event.event_type)\n                if not self.root_node:\n                    self.root_node = node_name\n\n            node = self.nodes[node_name]\n            previous_status = node.status\n\n            # Add event to node\n            node.add_event(event)\n\n            # Track status changes\n            if previous_status != node.status:\n                if node.status in [NodeStatus.RUNNING, NodeStatus.STARTING]:\n                    self.active_nodes.add(node_name)\n                elif node.is_completed():\n                    self.active_nodes.discard(node_name)\n                    if node_name not in self.completion_order:\n                        self.completion_order.append(node_name)\n\n            # Update current node tracking\n            if event.event_type in [\"node_enter\", \"llm_call\", \"tool_call\"]:\n                if self.current_node != node_name:\n                    self.current_node = node_name\n                    if node_name not in self.execution_flow:\n                        self.execution_flow.append(node_name)\n\n            # Track routing decisions\n            if event.routing_from and event.routing_to:\n                self.routing_history.append({\n                    \"from\": event.routing_from,\n                    \"to\": event.routing_to,\n                    \"timestamp\": event.timestamp,\n                    \"decision\": event.routing_decision or \"unknown\"\n                })\n\n            # Track errors\n            if event.event_type == \"error\" or event.success is False:\n                error_entry = {\n                    \"timestamp\": event.timestamp,\n                    \"node\": node_name,\n                    \"event_type\": event.event_type,\n                    \"error\": event.metadata.get(\"error\", \"Unknown error\"),\n                    \"error_type\": event.metadata.get(\"error_type\", \"UnknownError\"),\n                    \"retry_count\": node.retry_count\n                }\n                self.error_log.append(error_entry)\n\n            # Update global stats\n            if event.llm_cost:\n                self.total_cost += event.llm_cost\n            if event.llm_total_tokens:\n                self.total_tokens += event.llm_total_tokens\n\n        except Exception as e:\n            # Fallback error handling\n            self.error_log.append({\n                \"timestamp\": time.time(),\n                \"node\": \"SYSTEM\",\n                \"event_type\": \"processing_error\",\n                \"error\": f\"Failed to process event: {str(e)}\",\n                \"error_type\": \"EventProcessingError\",\n                \"original_event\": event.event_id if hasattr(event, 'event_id') else \"unknown\"\n            })\n\n    def get_execution_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Enhanced execution summary with detailed metrics\"\"\"\n        current_time = time.time()\n\n        return {\n            \"session_info\": {\n                \"session_id\": self.session_id,\n                \"total_nodes\": len(self.nodes),\n                \"completed_nodes\": len(self.completion_order),\n                \"active_nodes\": len(self.active_nodes),\n                \"failed_nodes\": len([n for n in self.nodes.values() if n.status == NodeStatus.FAILED])\n            },\n            \"execution_flow\": {\n                \"flow\": self.execution_flow,\n                \"completion_order\": self.completion_order,\n                \"current_node\": self.current_node,\n                \"active_nodes\": list(self.active_nodes)\n            },\n            \"performance_metrics\": {\n                \"total_cost\": self.total_cost,\n                \"total_tokens\": self.total_tokens,\n                \"total_events\": self.total_events,\n                \"error_count\": len(self.error_log),\n                \"routing_steps\": len(self.routing_history)\n            },\n            \"timing\": {\n                \"start_time\": self.start_time,\n                \"current_time\": current_time,\n                \"elapsed\": current_time - self.start_time if self.start_time else 0,\n                \"estimated_completion\": self._estimate_completion_time()\n            },\n            \"health_indicators\": {\n                \"overall_health\": self._calculate_health_score(),\n                \"error_rate\": len(self.error_log) / max(self.total_events, 1),\n                \"completion_rate\": len(self.completion_order) / max(len(self.nodes), 1),\n                \"average_node_efficiency\": self._calculate_average_efficiency()\n            }\n        }\n\n    def _estimate_completion_time(self) -&gt; Optional[float]:\n        \"\"\"Estimate when execution might complete\"\"\"\n        if not self.active_nodes or not self.start_time:\n            return None\n\n        # Simple heuristic based on current progress\n        completed_ratio = len(self.completion_order) / max(len(self.nodes), 1)\n        if completed_ratio &gt; 0:\n            elapsed = time.time() - self.start_time\n            estimated_total = elapsed / completed_ratio\n            return self.start_time + estimated_total\n\n        return None\n\n    def _calculate_health_score(self) -&gt; float:\n        \"\"\"Calculate overall execution health score\"\"\"\n        if not self.nodes:\n            return 1.0\n\n        scores = []\n        for node in self.nodes.values():\n            if node.status == NodeStatus.COMPLETED:\n                scores.append(1.0)\n            elif node.status == NodeStatus.FAILED:\n                scores.append(0.0)\n            elif node.status in [NodeStatus.RUNNING, NodeStatus.STARTING]:\n                scores.append(0.7)  # In progress\n            else:\n                scores.append(0.5)  # Pending/waiting\n\n        return sum(scores) / len(scores)\n\n    def _calculate_average_efficiency(self) -&gt; float:\n        \"\"\"Calculate average node efficiency\"\"\"\n        efficiencies = [node._calculate_efficiency_score() for node in self.nodes.values()\n                        if node.duration is not None]\n        return sum(efficiencies) / max(len(efficiencies), 1)\n</code></pre> <code>add_event(event)</code> \u00b6 <p>Enhanced event processing with better error handling</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def add_event(self, event: ProgressEvent):\n    \"\"\"Enhanced event processing with better error handling\"\"\"\n    try:\n        self.total_events += 1\n\n        if not self.start_time:\n            self.start_time = event.timestamp\n            self.session_id = event.session_id\n\n        # Create or update node\n        node_name = event.node_name\n        if node_name not in self.nodes:\n            self.nodes[node_name] = ExecutionNode(node_name, event.event_type)\n            if not self.root_node:\n                self.root_node = node_name\n\n        node = self.nodes[node_name]\n        previous_status = node.status\n\n        # Add event to node\n        node.add_event(event)\n\n        # Track status changes\n        if previous_status != node.status:\n            if node.status in [NodeStatus.RUNNING, NodeStatus.STARTING]:\n                self.active_nodes.add(node_name)\n            elif node.is_completed():\n                self.active_nodes.discard(node_name)\n                if node_name not in self.completion_order:\n                    self.completion_order.append(node_name)\n\n        # Update current node tracking\n        if event.event_type in [\"node_enter\", \"llm_call\", \"tool_call\"]:\n            if self.current_node != node_name:\n                self.current_node = node_name\n                if node_name not in self.execution_flow:\n                    self.execution_flow.append(node_name)\n\n        # Track routing decisions\n        if event.routing_from and event.routing_to:\n            self.routing_history.append({\n                \"from\": event.routing_from,\n                \"to\": event.routing_to,\n                \"timestamp\": event.timestamp,\n                \"decision\": event.routing_decision or \"unknown\"\n            })\n\n        # Track errors\n        if event.event_type == \"error\" or event.success is False:\n            error_entry = {\n                \"timestamp\": event.timestamp,\n                \"node\": node_name,\n                \"event_type\": event.event_type,\n                \"error\": event.metadata.get(\"error\", \"Unknown error\"),\n                \"error_type\": event.metadata.get(\"error_type\", \"UnknownError\"),\n                \"retry_count\": node.retry_count\n            }\n            self.error_log.append(error_entry)\n\n        # Update global stats\n        if event.llm_cost:\n            self.total_cost += event.llm_cost\n        if event.llm_total_tokens:\n            self.total_tokens += event.llm_total_tokens\n\n    except Exception as e:\n        # Fallback error handling\n        self.error_log.append({\n            \"timestamp\": time.time(),\n            \"node\": \"SYSTEM\",\n            \"event_type\": \"processing_error\",\n            \"error\": f\"Failed to process event: {str(e)}\",\n            \"error_type\": \"EventProcessingError\",\n            \"original_event\": event.event_id if hasattr(event, 'event_id') else \"unknown\"\n        })\n</code></pre> <code>get_execution_summary()</code> \u00b6 <p>Enhanced execution summary with detailed metrics</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def get_execution_summary(self) -&gt; Dict[str, Any]:\n    \"\"\"Enhanced execution summary with detailed metrics\"\"\"\n    current_time = time.time()\n\n    return {\n        \"session_info\": {\n            \"session_id\": self.session_id,\n            \"total_nodes\": len(self.nodes),\n            \"completed_nodes\": len(self.completion_order),\n            \"active_nodes\": len(self.active_nodes),\n            \"failed_nodes\": len([n for n in self.nodes.values() if n.status == NodeStatus.FAILED])\n        },\n        \"execution_flow\": {\n            \"flow\": self.execution_flow,\n            \"completion_order\": self.completion_order,\n            \"current_node\": self.current_node,\n            \"active_nodes\": list(self.active_nodes)\n        },\n        \"performance_metrics\": {\n            \"total_cost\": self.total_cost,\n            \"total_tokens\": self.total_tokens,\n            \"total_events\": self.total_events,\n            \"error_count\": len(self.error_log),\n            \"routing_steps\": len(self.routing_history)\n        },\n        \"timing\": {\n            \"start_time\": self.start_time,\n            \"current_time\": current_time,\n            \"elapsed\": current_time - self.start_time if self.start_time else 0,\n            \"estimated_completion\": self._estimate_completion_time()\n        },\n        \"health_indicators\": {\n            \"overall_health\": self._calculate_health_score(),\n            \"error_rate\": len(self.error_log) / max(self.total_events, 1),\n            \"completion_rate\": len(self.completion_order) / max(len(self.nodes), 1),\n            \"average_node_efficiency\": self._calculate_average_efficiency()\n        }\n    }\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.FormatConfig","title":"<code>FormatConfig</code>  <code>dataclass</code>","text":"<p>Konfiguration f\u00fcr Response-Format und -L\u00e4nge</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass FormatConfig:\n    \"\"\"Konfiguration f\u00fcr Response-Format und -L\u00e4nge\"\"\"\n    response_format: ResponseFormat = ResponseFormat.FREI_TEXT\n    text_length: TextLength = TextLength.CHAT_CONVERSATION\n    custom_instructions: str = \"\"\n    strict_format_adherence: bool = True\n    quality_threshold: float = 0.7\n\n    def get_format_instructions(self) -&gt; str:\n        \"\"\"Generiere Format-spezifische Anweisungen\"\"\"\n        format_instructions = {\n            ResponseFormat.FREI_TEXT: \"Verwende nat\u00fcrlichen Flie\u00dftext ohne spezielle Formatierung.\",\n            ResponseFormat.WITH_TABLES: \"Integriere Tabellen zur strukturierten Darstellung von Daten. Verwende Markdown-Tabellen.\",\n            ResponseFormat.WITH_BULLET_POINTS: \"Strukturiere Informationen mit Bullet Points (\u2022, -, *) f\u00fcr bessere Lesbarkeit.\",\n            ResponseFormat.WITH_LISTS: \"Verwende nummerierte und unnummerierte Listen zur Organisation von Inhalten.\",\n            ResponseFormat.TEXT_ONLY: \"Nur reiner Text ohne Formatierung, Symbole oder Strukturelemente.\",\n            ResponseFormat.MD_TEXT: \"Vollst\u00e4ndige Markdown-Formatierung mit Headings, Code-Blocks, Links etc.\",\n            ResponseFormat.YAML_TEXT: \"Strukturiere Antworten als YAML-Format f\u00fcr maschinenlesbare Ausgabe.\",\n            ResponseFormat.JSON_TEXT: \"Formatiere Antworten als JSON-Struktur f\u00fcr API-Integration.\",\n            ResponseFormat.PSEUDO_CODE: \"Verwende Pseudocode-Struktur f\u00fcr algorithmische oder logische Erkl\u00e4rungen.\",\n            ResponseFormat.CODE_STRUCTURE: \"Strukturiere wie Code mit Einr\u00fcckungen, Kommentaren und logischen Bl\u00f6cken.\"\n        }\n        return format_instructions.get(self.response_format, \"Standard-Formatierung.\")\n\n    def get_length_instructions(self) -&gt; str:\n        \"\"\"Generiere L\u00e4ngen-spezifische Anweisungen\"\"\"\n        length_instructions = {\n            TextLength.MINI_CHAT: \"Sehr kurze, pr\u00e4gnante Antworten (1-2 S\u00e4tze, max 50 W\u00f6rter). Chat-Style.\",\n            TextLength.CHAT_CONVERSATION: \"Moderate Gespr\u00e4chsl\u00e4nge (2-4 S\u00e4tze, 50-150 W\u00f6rter). Nat\u00fcrlicher Unterhaltungsstil.\",\n            TextLength.TABLE_CONVERSATION: \"Strukturierte, tabellarische Darstellung mit kompakten Erkl\u00e4rungen (100-250 W\u00f6rter).\",\n            TextLength.DETAILED_INDEPTH: \"Ausf\u00fchrliche, detaillierte Erkl\u00e4rungen (300-800 W\u00f6rter) mit Tiefe und Kontext.\",\n            TextLength.PHD_LEVEL: \"Akademische Tiefe mit umfassenden Erkl\u00e4rungen (800+ W\u00f6rter), Quellenangaben und Fachterminologie.\"\n        }\n        return length_instructions.get(self.text_length, \"Standard-L\u00e4nge.\")\n\n    def get_combined_instructions(self) -&gt; str:\n        \"\"\"Kombiniere Format- und L\u00e4ngen-Anweisungen\"\"\"\n        instructions = []\n        instructions.append(\"## Format-Anforderungen:\")\n        instructions.append(self.get_format_instructions())\n        instructions.append(\"\\n## L\u00e4ngen-Anforderungen:\")\n        instructions.append(self.get_length_instructions())\n\n        if self.custom_instructions:\n            instructions.append(f\"\\n## Zus\u00e4tzliche Anweisungen:\")\n            instructions.append(self.custom_instructions)\n\n        if self.strict_format_adherence:\n            instructions.append(\"\\n## WICHTIG: Halte dich strikt an diese Format- und L\u00e4ngen-Vorgaben!\")\n\n        return \"\\n\".join(instructions)\n\n    def get_expected_word_range(self) -&gt; tuple[int, int]:\n        \"\"\"Erwartete Wortanzahl f\u00fcr Qualit\u00e4tsbewertung\"\"\"\n        ranges = {\n            TextLength.MINI_CHAT: (10, 50),\n            TextLength.CHAT_CONVERSATION: (50, 150),\n            TextLength.TABLE_CONVERSATION: (100, 250),\n            TextLength.DETAILED_INDEPTH: (300, 800),\n            TextLength.PHD_LEVEL: (800, 2000)\n        }\n        return ranges.get(self.text_length, (50, 200))\n</code></pre> <code>get_combined_instructions()</code> \u00b6 <p>Kombiniere Format- und L\u00e4ngen-Anweisungen</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def get_combined_instructions(self) -&gt; str:\n    \"\"\"Kombiniere Format- und L\u00e4ngen-Anweisungen\"\"\"\n    instructions = []\n    instructions.append(\"## Format-Anforderungen:\")\n    instructions.append(self.get_format_instructions())\n    instructions.append(\"\\n## L\u00e4ngen-Anforderungen:\")\n    instructions.append(self.get_length_instructions())\n\n    if self.custom_instructions:\n        instructions.append(f\"\\n## Zus\u00e4tzliche Anweisungen:\")\n        instructions.append(self.custom_instructions)\n\n    if self.strict_format_adherence:\n        instructions.append(\"\\n## WICHTIG: Halte dich strikt an diese Format- und L\u00e4ngen-Vorgaben!\")\n\n    return \"\\n\".join(instructions)\n</code></pre> <code>get_expected_word_range()</code> \u00b6 <p>Erwartete Wortanzahl f\u00fcr Qualit\u00e4tsbewertung</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def get_expected_word_range(self) -&gt; tuple[int, int]:\n    \"\"\"Erwartete Wortanzahl f\u00fcr Qualit\u00e4tsbewertung\"\"\"\n    ranges = {\n        TextLength.MINI_CHAT: (10, 50),\n        TextLength.CHAT_CONVERSATION: (50, 150),\n        TextLength.TABLE_CONVERSATION: (100, 250),\n        TextLength.DETAILED_INDEPTH: (300, 800),\n        TextLength.PHD_LEVEL: (800, 2000)\n    }\n    return ranges.get(self.text_length, (50, 200))\n</code></pre> <code>get_format_instructions()</code> \u00b6 <p>Generiere Format-spezifische Anweisungen</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def get_format_instructions(self) -&gt; str:\n    \"\"\"Generiere Format-spezifische Anweisungen\"\"\"\n    format_instructions = {\n        ResponseFormat.FREI_TEXT: \"Verwende nat\u00fcrlichen Flie\u00dftext ohne spezielle Formatierung.\",\n        ResponseFormat.WITH_TABLES: \"Integriere Tabellen zur strukturierten Darstellung von Daten. Verwende Markdown-Tabellen.\",\n        ResponseFormat.WITH_BULLET_POINTS: \"Strukturiere Informationen mit Bullet Points (\u2022, -, *) f\u00fcr bessere Lesbarkeit.\",\n        ResponseFormat.WITH_LISTS: \"Verwende nummerierte und unnummerierte Listen zur Organisation von Inhalten.\",\n        ResponseFormat.TEXT_ONLY: \"Nur reiner Text ohne Formatierung, Symbole oder Strukturelemente.\",\n        ResponseFormat.MD_TEXT: \"Vollst\u00e4ndige Markdown-Formatierung mit Headings, Code-Blocks, Links etc.\",\n        ResponseFormat.YAML_TEXT: \"Strukturiere Antworten als YAML-Format f\u00fcr maschinenlesbare Ausgabe.\",\n        ResponseFormat.JSON_TEXT: \"Formatiere Antworten als JSON-Struktur f\u00fcr API-Integration.\",\n        ResponseFormat.PSEUDO_CODE: \"Verwende Pseudocode-Struktur f\u00fcr algorithmische oder logische Erkl\u00e4rungen.\",\n        ResponseFormat.CODE_STRUCTURE: \"Strukturiere wie Code mit Einr\u00fcckungen, Kommentaren und logischen Bl\u00f6cken.\"\n    }\n    return format_instructions.get(self.response_format, \"Standard-Formatierung.\")\n</code></pre> <code>get_length_instructions()</code> \u00b6 <p>Generiere L\u00e4ngen-spezifische Anweisungen</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def get_length_instructions(self) -&gt; str:\n    \"\"\"Generiere L\u00e4ngen-spezifische Anweisungen\"\"\"\n    length_instructions = {\n        TextLength.MINI_CHAT: \"Sehr kurze, pr\u00e4gnante Antworten (1-2 S\u00e4tze, max 50 W\u00f6rter). Chat-Style.\",\n        TextLength.CHAT_CONVERSATION: \"Moderate Gespr\u00e4chsl\u00e4nge (2-4 S\u00e4tze, 50-150 W\u00f6rter). Nat\u00fcrlicher Unterhaltungsstil.\",\n        TextLength.TABLE_CONVERSATION: \"Strukturierte, tabellarische Darstellung mit kompakten Erkl\u00e4rungen (100-250 W\u00f6rter).\",\n        TextLength.DETAILED_INDEPTH: \"Ausf\u00fchrliche, detaillierte Erkl\u00e4rungen (300-800 W\u00f6rter) mit Tiefe und Kontext.\",\n        TextLength.PHD_LEVEL: \"Akademische Tiefe mit umfassenden Erkl\u00e4rungen (800+ W\u00f6rter), Quellenangaben und Fachterminologie.\"\n    }\n    return length_instructions.get(self.text_length, \"Standard-L\u00e4nge.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.LLMTask","title":"<code>LLMTask</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Task</code></p> <p>Spezialisierter Task f\u00fcr LLM-Aufrufe</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass LLMTask(Task):\n    \"\"\"Spezialisierter Task f\u00fcr LLM-Aufrufe\"\"\"\n    llm_config: Dict[str, Any] = field(default_factory=lambda: {\n        \"model_preference\": \"fast\",  # \"fast\" | \"complex\"\n        \"temperature\": 0.7,\n        \"max_tokens\": 1024\n    })\n    prompt_template: str = \"\"\n    context_keys: List[str] = field(default_factory=list)  # Keys aus shared state\n    output_schema: Optional[Dict] = None  # JSON Schema f\u00fcr Validierung\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.PersonaConfig","title":"<code>PersonaConfig</code>  <code>dataclass</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass PersonaConfig:\n    name: str\n    style: str = \"professional\"\n    personality_traits: List[str] = field(default_factory=lambda: [\"helpful\", \"concise\"])\n    tone: str = \"friendly\"\n    response_format: str = \"direct\"\n    custom_instructions: str = \"\"\n\n    format_config: Optional[FormatConfig] = None\n\n    apply_method: str = \"system_prompt\"  # \"system_prompt\" | \"post_process\" | \"both\"\n    integration_level: str = \"light\"  # \"light\" | \"medium\" | \"heavy\"\n\n    def to_system_prompt_addition(self) -&gt; str:\n        \"\"\"Convert persona to system prompt addition with format integration\"\"\"\n        if self.apply_method in [\"system_prompt\", \"both\"]:\n            additions = []\n            additions.append(f\"You are {self.name}.\")\n            additions.append(f\"Your communication style is {self.style} with a {self.tone} tone.\")\n\n            if self.personality_traits:\n                traits_str = \", \".join(self.personality_traits)\n                additions.append(f\"Your key traits are: {traits_str}.\")\n\n            if self.custom_instructions:\n                additions.append(self.custom_instructions)\n\n            # Format-spezifische Anweisungen hinzuf\u00fcgen\n            if self.format_config:\n                additions.append(\"\\n\" + self.format_config.get_combined_instructions())\n\n            return \" \".join(additions)\n        return \"\"\n\n    def update_format(self, response_format: ResponseFormat|str, text_length: TextLength|str, custom_instructions: str = \"\"):\n        \"\"\"Dynamische Format-Aktualisierung\"\"\"\n        try:\n            format_enum = ResponseFormat(response_format) if isinstance(response_format, str) else response_format\n            length_enum = TextLength(text_length) if isinstance(text_length, str) else text_length\n\n            if not self.format_config:\n                self.format_config = FormatConfig()\n\n            self.format_config.response_format = format_enum\n            self.format_config.text_length = length_enum\n\n            if custom_instructions:\n                self.format_config.custom_instructions = custom_instructions\n\n\n        except ValueError as e:\n            raise ValueError(f\"Invalid format '{response_format}' or length '{text_length}'\")\n\n    def should_post_process(self) -&gt; bool:\n        \"\"\"Check if post-processing should be applied\"\"\"\n        return self.apply_method in [\"post_process\", \"both\"]\n</code></pre> <code>should_post_process()</code> \u00b6 <p>Check if post-processing should be applied</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def should_post_process(self) -&gt; bool:\n    \"\"\"Check if post-processing should be applied\"\"\"\n    return self.apply_method in [\"post_process\", \"both\"]\n</code></pre> <code>to_system_prompt_addition()</code> \u00b6 <p>Convert persona to system prompt addition with format integration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def to_system_prompt_addition(self) -&gt; str:\n    \"\"\"Convert persona to system prompt addition with format integration\"\"\"\n    if self.apply_method in [\"system_prompt\", \"both\"]:\n        additions = []\n        additions.append(f\"You are {self.name}.\")\n        additions.append(f\"Your communication style is {self.style} with a {self.tone} tone.\")\n\n        if self.personality_traits:\n            traits_str = \", \".join(self.personality_traits)\n            additions.append(f\"Your key traits are: {traits_str}.\")\n\n        if self.custom_instructions:\n            additions.append(self.custom_instructions)\n\n        # Format-spezifische Anweisungen hinzuf\u00fcgen\n        if self.format_config:\n            additions.append(\"\\n\" + self.format_config.get_combined_instructions())\n\n        return \" \".join(additions)\n    return \"\"\n</code></pre> <code>update_format(response_format, text_length, custom_instructions='')</code> \u00b6 <p>Dynamische Format-Aktualisierung</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def update_format(self, response_format: ResponseFormat|str, text_length: TextLength|str, custom_instructions: str = \"\"):\n    \"\"\"Dynamische Format-Aktualisierung\"\"\"\n    try:\n        format_enum = ResponseFormat(response_format) if isinstance(response_format, str) else response_format\n        length_enum = TextLength(text_length) if isinstance(text_length, str) else text_length\n\n        if not self.format_config:\n            self.format_config = FormatConfig()\n\n        self.format_config.response_format = format_enum\n        self.format_config.text_length = length_enum\n\n        if custom_instructions:\n            self.format_config.custom_instructions = custom_instructions\n\n\n    except ValueError as e:\n        raise ValueError(f\"Invalid format '{response_format}' or length '{text_length}'\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.ProgressEvent","title":"<code>ProgressEvent</code>  <code>dataclass</code>","text":"<p>Enhanced progress event with better error handling</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>@dataclass\nclass ProgressEvent:\n    \"\"\"Enhanced progress event with better error handling\"\"\"\n    event_type: str\n    timestamp: float\n    node_name: str\n    event_id: str = \"\"\n\n    #\n    agent_name: Optional[str] = None\n\n    # Status information\n    status: Optional[NodeStatus] = None\n    success: Optional[bool] = None\n    error_details: Optional[Dict[str, Any]] = None\n\n    # LLM-specific data\n    llm_model: Optional[str] = None\n    llm_prompt_tokens: Optional[int] = None\n    llm_completion_tokens: Optional[int] = None\n    llm_total_tokens: Optional[int] = None\n    llm_cost: Optional[float] = None\n    llm_duration: Optional[float] = None\n    llm_temperature: Optional[float] = None\n\n    # Tool-specific data\n    tool_name: Optional[str] = None\n    tool_args: Optional[Dict[str, Any]] = None\n    tool_result: Optional[Any] = None\n    tool_duration: Optional[float] = None\n    tool_success: Optional[bool] = None\n    tool_error: Optional[str] = None\n\n    # Node/Routing data\n    routing_decision: Optional[str] = None\n    routing_from: Optional[str] = None\n    routing_to: Optional[str] = None\n    node_phase: Optional[str] = None\n    node_duration: Optional[float] = None\n\n    # Context data\n    task_id: Optional[str] = None\n    session_id: Optional[str] = None\n    plan_id: Optional[str] = None\n\n    # Additional metadata\n    metadata: Dict[str, Any] = None\n\n    def __post_init__(self):\n        if self.metadata is None:\n            self.metadata = {}\n        if not self.event_id:\n            self.event_id = f\"{self.node_name}_{self.event_type}_{int(self.timestamp * 1000000)}\"\n        if 'error' in self.metadata or 'error_type' in self.metadata:\n            if self.error_details is None:\n                self.error_details = {}\n            self.error_details['error'] = self.metadata.get('error')\n            self.error_details['error_type'] = self.metadata.get('error_type')\n            self.status = NodeStatus.FAILED\n        if self.status == NodeStatus.FAILED:\n            self.success = False\n        if self.status == NodeStatus.COMPLETED:\n            self.success = True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.ProgressTracker","title":"<code>ProgressTracker</code>","text":"<p>Advanced progress tracking with cost calculation</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>class ProgressTracker:\n    \"\"\"Advanced progress tracking with cost calculation\"\"\"\n\n    def __init__(self, progress_callback: Optional[callable] = None, agent_name=\"unknown\"):\n        self.progress_callback = progress_callback\n        self.events: List[ProgressEvent] = []\n        self.active_timers: Dict[str, float] = {}\n\n        # Cost tracking (simplified - would need actual provider pricing)\n        self.token_costs = {\n            \"input\": 0.00001,  # $0.01/1K tokens input\n            \"output\": 0.00003,  # $0.03/1K tokens output\n        }\n        self.agent_name = agent_name\n\n    async def emit_event(self, event: ProgressEvent):\n        \"\"\"Emit progress event with callback and storage\"\"\"\n        self.events.append(event)\n        event.agent_name = self.agent_name\n\n        if self.progress_callback:\n            try:\n                if asyncio.iscoroutinefunction(self.progress_callback):\n                    await self.progress_callback(event)\n                else:\n                    self.progress_callback(event)\n            except Exception as e:\n                import traceback\n                print(traceback.format_exc())\n\n\n    def start_timer(self, key: str) -&gt; float:\n        \"\"\"Start timing operation\"\"\"\n        start_time = time.perf_counter()\n        self.active_timers[key] = start_time\n        return start_time\n\n    def end_timer(self, key: str) -&gt; float:\n        \"\"\"End timing operation and return duration\"\"\"\n        if key not in self.active_timers:\n            return 0.0\n        duration = time.perf_counter() - self.active_timers[key]\n        del self.active_timers[key]\n        return duration\n\n    def calculate_llm_cost(self, model: str, input_tokens: int, output_tokens: int) -&gt; float:\n        \"\"\"Calculate approximate LLM cost\"\"\"\n        # Simplified cost calculation - would need actual provider pricing\n        input_cost = (input_tokens / 1000) * self.token_costs[\"input\"]\n        output_cost = (output_tokens / 1000) * self.token_costs[\"output\"]\n        return input_cost + output_cost\n\n    def get_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Get comprehensive progress summary\"\"\"\n        summary = {\n            \"total_events\": len(self.events),\n            \"llm_calls\": len([e for e in self.events if e.event_type == \"llm_call\"]),\n            \"tool_calls\": len([e for e in self.events if e.event_type == \"tool_call\"]),\n            \"total_cost\": sum(e.llm_cost for e in self.events if e.llm_cost),\n            \"total_tokens\": sum(e.llm_total_tokens for e in self.events if e.llm_total_tokens),\n            \"total_duration\": sum(e.node_duration for e in self.events if e.node_duration),\n            \"nodes_visited\": list(set(e.node_name for e in self.events)),\n            \"tools_used\": list(set(e.tool_name for e in self.events if e.tool_name)),\n            \"models_used\": list(set(e.llm_model for e in self.events if e.llm_model))\n        }\n        return summary\n</code></pre> <code>calculate_llm_cost(model, input_tokens, output_tokens)</code> \u00b6 <p>Calculate approximate LLM cost</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def calculate_llm_cost(self, model: str, input_tokens: int, output_tokens: int) -&gt; float:\n    \"\"\"Calculate approximate LLM cost\"\"\"\n    # Simplified cost calculation - would need actual provider pricing\n    input_cost = (input_tokens / 1000) * self.token_costs[\"input\"]\n    output_cost = (output_tokens / 1000) * self.token_costs[\"output\"]\n    return input_cost + output_cost\n</code></pre> <code>emit_event(event)</code> <code>async</code> \u00b6 <p>Emit progress event with callback and storage</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>async def emit_event(self, event: ProgressEvent):\n    \"\"\"Emit progress event with callback and storage\"\"\"\n    self.events.append(event)\n    event.agent_name = self.agent_name\n\n    if self.progress_callback:\n        try:\n            if asyncio.iscoroutinefunction(self.progress_callback):\n                await self.progress_callback(event)\n            else:\n                self.progress_callback(event)\n        except Exception as e:\n            import traceback\n            print(traceback.format_exc())\n</code></pre> <code>end_timer(key)</code> \u00b6 <p>End timing operation and return duration</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def end_timer(self, key: str) -&gt; float:\n    \"\"\"End timing operation and return duration\"\"\"\n    if key not in self.active_timers:\n        return 0.0\n    duration = time.perf_counter() - self.active_timers[key]\n    del self.active_timers[key]\n    return duration\n</code></pre> <code>get_summary()</code> \u00b6 <p>Get comprehensive progress summary</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def get_summary(self) -&gt; Dict[str, Any]:\n    \"\"\"Get comprehensive progress summary\"\"\"\n    summary = {\n        \"total_events\": len(self.events),\n        \"llm_calls\": len([e for e in self.events if e.event_type == \"llm_call\"]),\n        \"tool_calls\": len([e for e in self.events if e.event_type == \"tool_call\"]),\n        \"total_cost\": sum(e.llm_cost for e in self.events if e.llm_cost),\n        \"total_tokens\": sum(e.llm_total_tokens for e in self.events if e.llm_total_tokens),\n        \"total_duration\": sum(e.node_duration for e in self.events if e.node_duration),\n        \"nodes_visited\": list(set(e.node_name for e in self.events)),\n        \"tools_used\": list(set(e.tool_name for e in self.events if e.tool_name)),\n        \"models_used\": list(set(e.llm_model for e in self.events if e.llm_model))\n    }\n    return summary\n</code></pre> <code>start_timer(key)</code> \u00b6 <p>Start timing operation</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def start_timer(self, key: str) -&gt; float:\n    \"\"\"Start timing operation\"\"\"\n    start_time = time.perf_counter()\n    self.active_timers[key] = start_time\n    return start_time\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.ProgressiveTreePrinter","title":"<code>ProgressiveTreePrinter</code>","text":"<p>Production-ready progressive tree printer with enhanced features</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>class ProgressiveTreePrinter:\n    \"\"\"Production-ready progressive tree printer with enhanced features\"\"\"\n\n    def __init__(self, mode: VerbosityMode = VerbosityMode.STANDARD, use_rich: bool = True,\n                 auto_refresh: bool = True, max_history: int = 1000,\n                 realtime_minimal: bool = None):\n        self.mode = mode\n        self.agent_name = \"self\"\n        self.use_rich = use_rich and RICH_AVAILABLE\n        self.auto_refresh = auto_refresh\n        self.max_history = max_history\n\n        self.tree_builder = ExecutionTreeBuilder()\n        self.print_history: List[Dict[str, Any]] = []\n\n        # Optimized realtime option\n        self.realtime_minimal = realtime_minimal if realtime_minimal is not None else (mode == VerbosityMode.REALTIME)\n        self._last_summary = \"\"\n        self._needs_full_tree = False\n        self._spinner_chars = \"\u280b\u2819\u2839\u2838\u283c\u2834\u2826\u2827\u2807\u280f\"\n        self._spinner_index = 0\n\n        # External accumulation storage\n        self._accumulated_runs: List[Dict[str, Any]] = []\n        self._current_run_id = 0\n        self._global_start_time = time.time()\n\n        # Rich console setup\n        if self.use_rich:\n            self.console = Console(record=True)\n            if mode == VerbosityMode.REALTIME:\n                self.progress = Progress(\n                    SpinnerColumn(),\n                    TextColumn(\"[progress.description]{task.description}\"),\n                    console=self.console,\n                    transient=True\n                )\n                self.progress_task = None\n\n        # State tracking\n        self._last_print_hash = None\n        self._print_counter = 0\n        self._last_update_time = 0\n        self._consecutive_errors = 0\n\n        # Error handling\n        self._error_threshold = 5\n        self._fallback_mode = False\n\n    def reset_global_start_time(self):\n        \"\"\"Reset global start time for new session\"\"\"\n        self._global_start_time = time.time()\n\n    def print_strategy_selection(self, strategy: str, event: ProgressEvent = None, context: Dict[str, Any] = None):\n        \"\"\"Print strategy selection information with descriptions based on verbosity mode\"\"\"\n\n        # Strategy descriptions mapping\n        strategy_descriptions = {\n            \"direct_response\": \"Simple LLM flow with optional tool calls\",\n            \"fast_simple_planning\": \"Simple multi-step plan with tool orchestration\",\n            \"slow_complex_planning\": \"Complex task breakdown with tool orchestration, use for tasks with more than 2 'and' words\",\n            \"research_and_analyze\": \"Information gathering with variable integration\",\n            \"creative_generation\": \"Content creation with personalization\",\n            \"problem_solving\": \"Analysis with tool validation\"\n        }\n\n        strategy_icons = {\n            \"direct_response\": \"\ud83d\udcac\",\n            \"fast_simple_planning\": \"\u26a1\",\n            \"slow_complex_planning\": \"\ud83d\udd04\",\n            \"research_and_analyze\": \"\ud83d\udd0d\",\n            \"creative_generation\": \"\ud83c\udfa8\",\n            \"problem_solving\": \"\ud83e\udde9\"\n        }\n\n        try:\n            if self._fallback_mode or not self.use_rich:\n                self._print_strategy_fallback(strategy, strategy_descriptions, strategy_icons)\n                return\n\n            # Get strategy info\n            icon = strategy_icons.get(strategy, \"\ud83c\udfaf\")+\" \"+self.agent_name\n            description = strategy_descriptions.get(strategy, \"Unknown strategy\")\n\n            # Format based on verbosity mode\n            if self.mode == VerbosityMode.MINIMAL:\n                # Just show strategy name\n                strategy_text = f\"{icon} Strategy: {strategy}\"\n                self.console.print(strategy_text, style=\"cyan\")\n\n            elif self.mode == VerbosityMode.STANDARD:\n                # Show strategy with description\n                strategy_text = f\"{icon} Strategy selected: [bold]{strategy}[/bold]\\n\ud83d\udcdd {description}\"\n                strategy_panel = Panel(\n                    strategy_text,\n                    title=\"\ud83c\udfaf Execution Strategy\",\n                    style=\"cyan\",\n                    box=box.ROUNDED\n                )\n                self.console.print(strategy_panel)\n\n            elif self.mode in [VerbosityMode.VERBOSE, VerbosityMode.DEBUG]:\n                # Full details with context\n                strategy_content = [\n                    f\"{icon} Strategy: [bold cyan]{strategy}[/bold cyan]\",\n                    f\"\ud83d\udcdd Description: {description}\"\n                ]\n\n                # Add context information if available\n                if context:\n                    if context.get(\"reasoning\"):\n                        strategy_content.append(f\"\ud83e\udde0 Reasoning: {context['reasoning']}\")\n                    if context.get(\"complexity_score\"):\n                        strategy_content.append(f\"\ud83d\udcca Complexity: {context['complexity_score']}\")\n                    if context.get(\"estimated_steps\"):\n                        strategy_content.append(f\"\ud83d\udccb Est. Steps: {context['estimated_steps']}\")\n\n                # Add event context in debug mode\n                if self.mode == VerbosityMode.DEBUG and event:\n                    strategy_content.append(\n                        f\"\u23f1\ufe0f Selected at: {datetime.fromtimestamp(event.timestamp).strftime('%H:%M:%S')}\")\n                    if event.node_name:\n                        strategy_content.append(f\"\ud83d\udccd Node: {event.node_name}\")\n\n                strategy_panel = Panel(\n                    \"\\n\".join(strategy_content),\n                    title=\"\ud83c\udfaf Strategy Selection Details\",\n                    style=\"cyan bold\",\n                    box=box.DOUBLE\n                )\n                self.console.print()\n                self.console.print(strategy_panel)\n\n            elif self.mode == VerbosityMode.REALTIME:\n                # Minimal output for realtime mode\n                if not self.realtime_minimal:\n                    strategy_text = f\"\\n{icon} Strategy: {strategy} - {description}\"\n                    self.console.print(strategy_text, style=\"cyan dim\")\n\n        except Exception as e:\n            # Fallback on error\n            self._consecutive_errors += 1\n            if self._consecutive_errors &lt;= self._error_threshold:\n                print(f\"\u26a0\ufe0f Strategy print error: {e}\")\n            self._print_strategy_fallback(strategy, strategy_descriptions, strategy_icons)\n\n    def _print_strategy_fallback(self, strategy: str, descriptions: Dict[str, str], icons: Dict[str, str]):\n        \"\"\"Fallback strategy printing without Rich\"\"\"\n        try:\n            icon = icons.get(strategy, \"\ud83c\udfaf\")\n            description = descriptions.get(strategy, \"Unknown strategy\")\n\n            if self.mode == VerbosityMode.MINIMAL:\n                print(f\"{icon} Strategy: {strategy}\")\n\n            elif self.mode == VerbosityMode.STANDARD:\n                print(f\"\\n{'-' * 50}\")\n                print(f\"{icon} Strategy selected: {strategy}\")\n                print(f\"\ud83d\udcdd {description}\")\n                print(f\"{'-' * 50}\")\n\n            elif self.mode in [VerbosityMode.VERBOSE, VerbosityMode.DEBUG]:\n                print(f\"\\n{'=' * 60}\")\n                print(f\"\ud83c\udfaf STRATEGY SELECTION\")\n                print(f\"{'=' * 60}\")\n                print(f\"{icon} Strategy: {strategy}\")\n                print(f\"\ud83d\udcdd Description: {description}\")\n                print(f\"{'=' * 60}\")\n\n            elif self.mode == VerbosityMode.REALTIME and not self.realtime_minimal:\n                print(f\"{icon} Strategy: {strategy} - {description}\")\n\n        except Exception as e:\n            # Ultimate fallback\n            print(f\"Strategy selected: {strategy}\")\n\n    def print_strategy_from_event(self, event: ProgressEvent):\n        \"\"\"Convenience method to print strategy from event metadata\"\"\"\n        try:\n            if not event.metadata or 'strategy' not in event.metadata:\n                return\n\n            strategy = event.metadata['strategy']\n            context = {\n                'reasoning': event.metadata.get('reasoning'),\n                'complexity_score': event.metadata.get('complexity_score'),\n                'estimated_steps': event.metadata.get('estimated_steps')\n            }\n\n            self.print_strategy_selection(strategy, event, context)\n\n        except Exception as e:\n            if self.mode == VerbosityMode.DEBUG:\n                print(f\"\u26a0\ufe0f Error printing strategy from event: {e}\")\n\n    def print_plan_from_event(self, event: ProgressEvent):\n        \"\"\"Convenience method to print plan from event metadata\"\"\"\n        try:\n            if not event.metadata or 'full_plan' not in event.metadata:\n                return\n\n            plan = event.metadata['full_plan']\n            self.pretty_print_task_plan(plan)\n\n        except Exception as e:\n            if self.mode == VerbosityMode.DEBUG:\n                print(f\"\u26a0\ufe0f Error printing plan from event: {e}\")\n\n    def _should_print_update(self) -&gt; bool:\n        \"\"\"Enhanced decision logic for when to print updates\"\"\"\n        current_time = time.time()\n\n        # Force full tree on errors or completion\n        if self._needs_full_tree:\n            self._last_update_time = current_time\n            return True\n\n        # In minimal realtime mode, only show one-line updates frequently\n        if self.realtime_minimal and self.mode == VerbosityMode.REALTIME:\n            # Update one-line summary more frequently (every 0.5s)\n            return current_time - self._last_update_time &gt; 0.5\n\n        # Rate limiting for other modes - don't print too frequently\n        if current_time - self._last_update_time &lt; 1.5:\n            return False\n\n        try:\n            # Create state hash for change detection\n            summary = self.tree_builder.get_execution_summary()\n            current_state = {\n                \"total_nodes\": summary[\"session_info\"][\"total_nodes\"],\n                \"completed_nodes\": summary[\"session_info\"][\"completed_nodes\"],\n                \"active_nodes\": summary[\"session_info\"][\"active_nodes\"],\n                \"failed_nodes\": summary[\"session_info\"][\"failed_nodes\"],\n                \"current_node\": summary[\"execution_flow\"][\"current_node\"],\n                \"total_events\": summary[\"performance_metrics\"][\"total_events\"],\n                \"error_count\": summary[\"performance_metrics\"][\"error_count\"]\n            }\n\n            current_hash = hash(str(sorted(current_state.items())))\n\n            # Mode-specific update logic\n            if self.mode == VerbosityMode.MINIMAL:\n                should_update = (current_hash != self._last_print_hash and\n                                 (current_state[\"completed_nodes\"] !=\n                                  getattr(self, '_last_completed_count', 0) or\n                                  current_state[\"failed_nodes\"] !=\n                                  getattr(self, '_last_failed_count', 0)))\n\n                self._last_completed_count = current_state[\"completed_nodes\"]\n                self._last_failed_count = current_state[\"failed_nodes\"]\n\n            elif self.mode in [VerbosityMode.STANDARD, VerbosityMode.VERBOSE]:\n                should_update = current_hash != self._last_print_hash\n\n            else:  # DEBUG mode\n                should_update = True\n\n            if should_update:\n                self._last_print_hash = current_hash\n                self._last_update_time = current_time\n                return True\n\n            return False\n\n        except Exception as e:\n            self._consecutive_errors += 1\n            if self._consecutive_errors &gt; self._error_threshold:\n                self._fallback_mode = True\n                print(f\"\u26a0\ufe0f  Printer error threshold exceeded, switching to fallback mode: {e}\")\n            return False\n\n    def flush(self, run_name: str = None) -&gt; Dict[str, Any]:\n        \"\"\"\n        Flush current execution data and store externally for accumulation.\n        Resets internal state for fresh execution timing.\n\n        Args:\n            run_name: Optional name for this run\n\n        Returns:\n            Dict containing the flushed execution data\n        \"\"\"\n        try:\n            # Generate run info\n            current_time = time.time()\n            if run_name is None:\n                run_name = f\"run_{self._current_run_id + 1}\"\n\n            # Collect current execution data\n            summary = self.tree_builder.get_execution_summary()\n\n            # Create comprehensive run data\n            run_data = {\n                \"run_id\": self._current_run_id + 1,\n                \"run_name\": run_name,\n                \"flush_timestamp\": current_time,\n                \"execution_summary\": summary,\n                \"detailed_nodes\": {},\n                \"execution_history\": self.print_history.copy(),\n                \"error_log\": self.tree_builder.error_log.copy(),\n                \"routing_history\": self.tree_builder.routing_history.copy(),\n                \"print_counter\": self._print_counter,\n                \"consecutive_errors\": self._consecutive_errors,\n                \"fallback_mode\": self._fallback_mode\n            }\n\n            # Add detailed node information\n            for node_name, node in self.tree_builder.nodes.items():\n                run_data[\"detailed_nodes\"][node_name] = {\n                    \"status\": node.status.value,\n                    \"duration\": node.duration,\n                    \"start_time\": node.start_time,\n                    \"end_time\": node.end_time,\n                    \"total_cost\": node.total_cost,\n                    \"total_tokens\": node.total_tokens,\n                    \"llm_calls\": len(node.llm_calls),\n                    \"tool_calls\": len(node.tool_calls),\n                    \"error\": node.error,\n                    \"retry_count\": node.retry_count,\n                    \"performance_metrics\": node.get_performance_summary(),\n                    \"strategy\": node.strategy,\n                    \"reasoning\": node.reasoning,\n                    \"routing_from\": node.routing_from,\n                    \"routing_to\": node.routing_to\n                }\n\n            # Store in accumulated runs\n            self._accumulated_runs.append(run_data)\n\n            # Reset internal state for fresh execution\n            self._reset_for_fresh_execution()\n\n            if self.use_rich:\n                self.console.print(f\"\u2705 Run '{run_name}' flushed and stored\", style=\"green bold\")\n                self.console.print(f\"\ud83d\udcca Total accumulated runs: {len(self._accumulated_runs)}\", style=\"blue\")\n            else:\n                print(f\"\u2705 Run '{run_name}' flushed and stored\")\n                print(f\"\ud83d\udcca Total accumulated runs: {len(self._accumulated_runs)}\")\n\n            return run_data\n\n        except Exception as e:\n            error_msg = f\"\u274c Error during flush: {e}\"\n            if self.use_rich:\n                self.console.print(error_msg, style=\"red bold\")\n            else:\n                print(error_msg)\n\n            # Still try to reset for fresh execution\n            self._reset_for_fresh_execution()\n\n            return {\"error\": str(e), \"timestamp\": current_time}\n\n    def pretty_print_task_plan(self, task_plan: Any):\n        \"\"\"Pretty print a Any with full details and structure\"\"\"\n        try:\n            if self._fallback_mode or not self.use_rich:\n                self._print_task_plan_fallback(task_plan)\n                return\n\n            # Create main header\n            self.console.print()\n            header_text = f\"\ud83d\udccb Task Plan: {task_plan.name}\\n\"\n            header_text += f\"Status: {task_plan.status.upper()} | Strategy: {task_plan.execution_strategy}\\n\"\n            header_text += f\"Created: {task_plan.created_at.strftime('%Y-%m-%d %H:%M:%S')} | Tasks: {len(task_plan.tasks)}\"\n\n            header = Panel(\n                header_text,\n                title=\"\ud83d\ude80 Task Plan Overview\",\n                style=\"cyan bold\",\n                box=box.ROUNDED\n            )\n            self.console.print(header)\n\n            # Description panel\n            if task_plan.description:\n                desc_panel = Panel(\n                    task_plan.description,\n                    title=\"\ud83d\udcdd Description\",\n                    style=\"blue\",\n                    box=box.ROUNDED\n                )\n                self.console.print(desc_panel)\n\n            # Create task tree\n            tree = Tree(f\"\ud83d\udd17 Task Execution Flow ({len(task_plan.tasks)} tasks)\", style=\"bold green\")\n\n            # Group tasks by type for better organization\n            task_groups = {}\n            for task in task_plan.tasks:\n                task_type = task.type if hasattr(task, 'type') else type(task).__name__\n                if task_type not in task_groups:\n                    task_groups[task_type] = []\n                task_groups[task_type].append(task)\n\n            # Add tasks organized by dependencies and priority\n            sorted_tasks = sorted(task_plan.tasks, key=lambda t: (t.priority, t.id))\n\n            for i, task in enumerate(sorted_tasks):\n                # Task status icon\n                status_icon = self._get_task_status_icon(task)\n                task_type = task.type if hasattr(task, 'type') else type(task).__name__\n\n                # Main task info\n                task_text = f\"{status_icon} [{i + 1}] {task.id}\"\n                if task.priority != 1:\n                    task_text += f\" (Priority: {task.priority})\"\n\n                task_style = self._get_task_status_color(task)\n                task_branch = tree.add(task_text, style=task_style)\n\n                # Add task details based on verbosity mode\n                if self.mode == VerbosityMode.MINIMAL:\n                    # Only show basic info\n                    task_branch.add(f\"\ud83d\udcc4 {task.description[:80]}...\", style=\"dim\")\n                else:\n                    # Show full details\n                    self._add_task_details(task_branch, task)\n\n            self.console.print(tree)\n\n            # Add metadata if available\n            if task_plan.metadata and self.mode in [VerbosityMode.VERBOSE, VerbosityMode.DEBUG]:\n                self._print_task_plan_metadata(task_plan)\n\n            # Add dependency analysis\n            if self.mode in [VerbosityMode.VERBOSE, VerbosityMode.DEBUG]:\n                self._print_dependency_analysis(task_plan)\n\n        except Exception as e:\n            self.console.print(f\"\u274c Error printing task plan: {e}\", style=\"red bold\")\n            self._print_task_plan_fallback(task_plan)\n\n    def _get_task_status_icon(self, task: Any) -&gt; str:\n        \"\"\"Get appropriate status icon for task\"\"\"\n        status_icons = {\n            \"pending\": \"\u23f3\",\n            \"running\": \"\ud83d\udd04\",\n            \"completed\": \"\u2705\",\n            \"failed\": \"\u274c\",\n            \"paused\": \"\u23f8\ufe0f\"\n        }\n        return status_icons.get(task.status, \"\u2753\")\n\n    def _get_task_status_color(self, task: Any) -&gt; str:\n        \"\"\"Get appropriate color styling for task status\"\"\"\n        status_colors = {\n            \"pending\": \"yellow\",\n            \"running\": \"blue bold\",\n            \"completed\": \"green bold\",\n            \"failed\": \"red bold\",\n            \"paused\": \"orange3\"\n        }\n        return status_colors.get(task.status, \"white\")\n\n    def _add_task_details(self, parent_branch: Tree, task: Any):\n        \"\"\"Add detailed task information based on task type\"\"\"\n        # Description\n        parent_branch.add(f\"\ud83d\udcc4 {task.description}\", style=\"blue dim\")\n\n        # Dependencies\n        if task.dependencies:\n            deps_text = f\"\ud83d\udd17 Dependencies: {', '.join(task.dependencies)}\"\n            parent_branch.add(deps_text, style=\"yellow dim\")\n\n        # Task type specific details\n\n        self._add_llm_task_details(parent_branch, task)\n        self._add_tool_task_details(parent_branch, task)\n        self._add_decision_task_details(parent_branch, task)\n        self._add_compound_task_details(parent_branch, task)\n\n        # Timing info\n        if hasattr(task, 'created_at') and task.created_at:\n            timing_info = f\"\ud83d\udcc5 Created: {task.created_at.strftime('%H:%M:%S')}\"\n            if hasattr(task, 'started_at') and task.started_at:\n                timing_info += f\" | Started: {task.started_at.strftime('%H:%M:%S')}\"\n            if hasattr(task, 'completed_at') and task.completed_at:\n                timing_info += f\" | Completed: {task.completed_at.strftime('%H:%M:%S')}\"\n            parent_branch.add(timing_info, style=\"cyan dim\")\n\n        # Error info\n        if hasattr(task, 'error') and task.error:\n            error_text = f\"\u274c Error: {task.error}\"\n            if hasattr(task, 'retry_count') and task.retry_count &gt; 0:\n                error_text += f\" (Retries: {task.retry_count}/{task.max_retries})\"\n            parent_branch.add(error_text, style=\"red dim\")\n\n        # Critical flag\n        if hasattr(task, 'critical') and task.critical:\n            parent_branch.add(\"\ud83d\udea8 CRITICAL TASK\", style=\"red bold\")\n\n    def _add_llm_task_details(self, parent_branch: Tree, task: Any):\n        \"\"\"Add LLM-specific task details\"\"\"\n        if hasattr(task, 'llm_config') and task.llm_config:\n            config_text = f\"\ud83e\udde0 Model: {task.llm_config.get('model_preference', 'default')}\"\n            config_text += f\" | Temp: {task.llm_config.get('temperature', 0.7)}\"\n            parent_branch.add(config_text, style=\"purple dim\")\n\n        if hasattr(task, 'context_keys') and task.context_keys:\n            context_text = f\"\ud83d\udd11 Context: {', '.join(task.context_keys)}\"\n            parent_branch.add(context_text, style=\"blue dim\")\n\n        if hasattr(task, 'prompt_template') and task.prompt_template and self.mode == VerbosityMode.DEBUG:\n            prompt_preview = task.prompt_template[:100] + \"...\" if len(\n                task.prompt_template) &gt; 100 else task.prompt_template\n            parent_branch.add(f\"\ud83d\udcac Prompt: {prompt_preview}\", style=\"green dim\")\n\n    def _add_tool_task_details(self, parent_branch: Tree, task: Any):\n        \"\"\"Add Tool-specific task details\"\"\"\n        if hasattr(task, 'tool_name') and task.tool_name:\n            parent_branch.add(f\"\ud83d\udd27 Tool: {task.tool_name}\", style=\"green dim\")\n\n        if hasattr(task, 'arguments') and task.arguments and self.mode in [VerbosityMode.VERBOSE, VerbosityMode.DEBUG]:\n            args_text = f\"\u2699\ufe0f Args: {str(task.arguments)[:80]}...\"\n            parent_branch.add(args_text, style=\"yellow dim\")\n\n        if hasattr(task, 'hypothesis') and task.hypothesis:\n            parent_branch.add(f\"\ud83d\udd2c Hypothesis: {task.hypothesis}\", style=\"blue dim\")\n\n        if hasattr(task, 'expectation') and task.expectation:\n            parent_branch.add(f\"\ud83c\udfaf Expected: {task.expectation}\", style=\"cyan dim\")\n\n    def _add_decision_task_details(self, parent_branch: Tree, task: Any):\n        \"\"\"Add Decision-specific task details\"\"\"\n        if hasattr(task, 'decision_model') and task.decision_model:\n            parent_branch.add(f\"\ud83e\udde0 Decision Model: {task.decision_model}\", style=\"purple dim\")\n\n        if hasattr(task, 'routing_map') and task.routing_map and self.mode == VerbosityMode.DEBUG:\n            routes_text = f\"\ud83d\uddfa\ufe0f Routes: {list(task.routing_map.keys())}\"\n            parent_branch.add(routes_text, style=\"orange dim\")\n\n    def _add_compound_task_details(self, parent_branch: Tree, task: Any):\n        \"\"\"Add Compound-specific task details\"\"\"\n        if hasattr(task, 'sub_task_ids') and task.sub_task_ids:\n            subtasks_text = f\"\ud83d\udccb Subtasks: {', '.join(task.sub_task_ids)}\"\n            parent_branch.add(subtasks_text, style=\"magenta dim\")\n\n        if hasattr(task, 'execution_strategy') and task.execution_strategy:\n            parent_branch.add(f\"\u26a1 Strategy: {task.execution_strategy}\", style=\"blue dim\")\n\n    def _print_task_plan_metadata(self, task_plan: Any):\n        \"\"\"Print task plan metadata in verbose modes\"\"\"\n        if not task_plan.metadata:\n            return\n\n        metadata_table = Table(title=\"\ud83d\udcca Task Plan Metadata\", box=box.ROUNDED)\n        metadata_table.add_column(\"Key\", style=\"cyan\", min_width=15)\n        metadata_table.add_column(\"Value\", style=\"green\", min_width=20)\n\n        for key, value in task_plan.metadata.items():\n            metadata_table.add_row(key, str(value))\n\n        self.console.print()\n        self.console.print(metadata_table)\n\n    def _print_dependency_analysis(self, task_plan: Any):\n        \"\"\"Print dependency analysis\"\"\"\n        try:\n            # Build dependency graph\n            dependency_info = self._analyze_dependencies(task_plan)\n\n            if dependency_info[\"cycles\"] or dependency_info[\"orphans\"] or dependency_info[\"leaves\"]:\n                analysis_text = []\n\n                if dependency_info[\"cycles\"]:\n                    analysis_text.append(f\"\ud83d\udd04 Circular dependencies detected: {dependency_info['cycles']}\")\n\n                if dependency_info[\"orphans\"]:\n                    analysis_text.append(f\"\ud83c\udfdd\ufe0f Tasks without dependencies: {dependency_info['orphans']}\")\n\n                if dependency_info[\"leaves\"]:\n                    analysis_text.append(f\"\ud83c\udf43 Final tasks: {dependency_info['leaves']}\")\n\n                analysis_text.append(f\"\ud83d\udcca Max depth: {dependency_info['max_depth']} levels\")\n\n                analysis_panel = Panel(\n                    \"\\n\".join(analysis_text),\n                    title=\"\ud83d\udd0d Dependency Analysis\",\n                    style=\"yellow\"\n                )\n                self.console.print()\n                self.console.print(analysis_panel)\n\n        except Exception as e:\n            if self.mode == VerbosityMode.DEBUG:\n                self.console.print(f\"\u26a0\ufe0f Dependency analysis error: {e}\", style=\"red dim\")\n\n    def _analyze_dependencies(self, task_plan: Any) -&gt; Dict[str, Any]:\n        \"\"\"Analyze task dependencies for insights\"\"\"\n        task_map = {task.id: task for task in task_plan.tasks}\n\n        cycles = []\n        orphans = []\n        leaves = []\n        max_depth = 0\n\n        # Find orphans (no dependencies)\n        for task in task_plan.tasks:\n            if not task.dependencies:\n                orphans.append(task.id)\n\n        # Find leaves (no one depends on them)\n        all_deps = set()\n        for task in task_plan.tasks:\n            all_deps.update(task.dependencies)\n\n        for task in task_plan.tasks:\n            if task.id not in all_deps:\n                leaves.append(task.id)\n\n        # Calculate max depth (simplified)\n        def get_depth(task_id, visited=None):\n            if visited is None:\n                visited = set()\n            if task_id in visited:\n                return 0  # Cycle detected\n            if task_id not in task_map:\n                return 0\n\n            visited.add(task_id)\n            task = task_map[task_id]\n            if not task.dependencies:\n                return 1\n\n            return 1 + max((get_depth(dep, visited.copy()) for dep in task.dependencies), default=0)\n\n        for task in task_plan.tasks:\n            depth = get_depth(task.id)\n            max_depth = max(max_depth, depth)\n\n        return {\n            \"cycles\": cycles,\n            \"orphans\": orphans,\n            \"leaves\": leaves,\n            \"max_depth\": max_depth\n        }\n\n    def _print_task_plan_fallback(self, task_plan: Any):\n        \"\"\"Fallback task plan printing without Rich\"\"\"\n        print(f\"\\n{'=' * 80}\")\n        print(f\"\ud83d\udccb TASK PLAN: {task_plan.name}\")\n        print(f\"{'=' * 80}\")\n        print(f\"Description: {task_plan.description}\")\n        print(f\"Status: {task_plan.status} | Strategy: {task_plan.execution_strategy}\")\n        print(f\"Created: {task_plan.created_at.strftime('%Y-%m-%d %H:%M:%S')}\")\n        print(f\"Tasks: {len(task_plan.tasks)}\")\n        print(f\"{'=' * 80}\")\n\n        print(\"\\n\ud83d\udccb TASKS:\")\n        print(f\"{'-' * 40}\")\n\n        sorted_tasks = sorted(task_plan.tasks, key=lambda t: (t.priority, t.id))\n        for i, task in enumerate(sorted_tasks):\n            status_icon = self._get_task_status_icon(task)\n            task_type = task.type if hasattr(task, 'type') else type(task).__name__\n\n            print(f\"{status_icon} [{i + 1}] {task.id} ({task_type})\")\n            print(f\"    \ud83d\udcc4 {task.description}\")\n\n            if task.dependencies:\n                print(f\"    \ud83d\udd17 Dependencies: {', '.join(task.dependencies)}\")\n\n            if hasattr(task, 'error') and task.error:\n                print(f\"    \u274c Error: {task.error}\")\n\n            if i &lt; len(sorted_tasks) - 1:\n                print()\n\n        print(f\"{'=' * 80}\")\n\n    def _reset_for_fresh_execution(self):\n        \"\"\"Reset internal state for a completely fresh execution\"\"\"\n        try:\n            # Increment run counter\n            self._current_run_id += 1\n\n            # Reset tree builder with completely fresh state\n            self.tree_builder = ExecutionTreeBuilder()\n\n            # Reset print history\n            self.print_history = []\n\n            # Reset timing and state tracking\n            self._last_print_hash = None\n            self._print_counter = 0\n            self._last_update_time = 0\n\n            # Reset realtime state\n            self._last_summary = \"\"\n            self._needs_full_tree = False\n            self._spinner_index = 0\n\n            # Reset error handling but don't reset fallback mode completely\n            # (if we're in fallback mode due to Rich issues, stay there)\n            self._consecutive_errors = 0\n\n            # Reset Rich progress if exists\n            if hasattr(self, 'progress') and self.progress:\n                self.progress_task = None\n\n            # Clear any cached state\n            if hasattr(self, '_last_completed_count'):\n                delattr(self, '_last_completed_count')\n            if hasattr(self, '_last_failed_count'):\n                delattr(self, '_last_failed_count')\n\n        except Exception as e:\n            print(f\"\u26a0\ufe0f Error during reset: {e}\")\n\n    def get_accumulated_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Get comprehensive summary of all accumulated runs\"\"\"\n        try:\n            if not self._accumulated_runs:\n                return {\n                    \"total_runs\": 0,\n                    \"message\": \"No runs have been flushed yet\"\n                }\n\n            # Calculate aggregate metrics\n            total_cost = 0.0\n            total_tokens = 0\n            total_events = 0\n            total_errors = 0\n            total_nodes = 0\n            total_duration = 0.0\n\n            run_summaries = []\n\n            for run in self._accumulated_runs:\n                summary = run[\"execution_summary\"]\n                perf = summary[\"performance_metrics\"]\n                timing = summary[\"timing\"]\n                session_info = summary[\"session_info\"]\n\n                total_cost += perf[\"total_cost\"]\n                total_tokens += perf[\"total_tokens\"]\n                total_events += perf[\"total_events\"]\n                total_errors += perf[\"error_count\"]\n                total_nodes += session_info[\"total_nodes\"]\n                total_duration += timing[\"elapsed\"]\n\n                run_summaries.append({\n                    \"run_id\": run[\"run_id\"],\n                    \"run_name\": run[\"run_name\"],\n                    \"nodes\": session_info[\"total_nodes\"],\n                    \"completed\": session_info[\"completed_nodes\"],\n                    \"failed\": session_info[\"failed_nodes\"],\n                    \"duration\": timing[\"elapsed\"],\n                    \"cost\": perf[\"total_cost\"],\n                    \"tokens\": perf[\"total_tokens\"],\n                    \"errors\": perf[\"error_count\"],\n                    \"health_score\": summary[\"health_indicators\"][\"overall_health\"]\n                })\n\n            # Calculate averages\n            num_runs = len(self._accumulated_runs)\n            avg_duration = total_duration / num_runs\n            avg_cost = total_cost / num_runs\n            avg_tokens = total_tokens / num_runs\n            avg_nodes = total_nodes / num_runs\n\n            return {\n                \"total_runs\": num_runs,\n                \"current_run_id\": self._current_run_id,\n                \"global_start_time\": self._global_start_time,\n                \"total_accumulated_time\": time.time() - self._global_start_time,\n\n                \"aggregate_metrics\": {\n                    \"total_cost\": total_cost,\n                    \"total_tokens\": total_tokens,\n                    \"total_events\": total_events,\n                    \"total_errors\": total_errors,\n                    \"total_nodes\": total_nodes,\n                    \"total_duration\": total_duration,\n                },\n\n                \"average_metrics\": {\n                    \"avg_duration\": avg_duration,\n                    \"avg_cost\": avg_cost,\n                    \"avg_tokens\": avg_tokens,\n                    \"avg_nodes\": avg_nodes,\n                    \"avg_error_rate\": total_errors / max(total_events, 1),\n                    \"avg_health_score\": sum(r[\"health_score\"] for r in run_summaries) / num_runs\n                },\n\n                \"run_summaries\": run_summaries,\n\n                \"performance_insights\": self._generate_accumulated_insights(run_summaries)\n            }\n\n        except Exception as e:\n            return {\"error\": f\"Error generating accumulated summary: {e}\"}\n\n    def _generate_accumulated_insights(self, run_summaries: List[Dict[str, Any]]) -&gt; List[str]:\n        \"\"\"Generate insights from accumulated run data\"\"\"\n        insights = []\n\n        if not run_summaries:\n            return insights\n\n        try:\n            num_runs = len(run_summaries)\n\n            # Performance trends\n            if num_runs &gt; 1:\n                recent_runs = run_summaries[-3:]  # Last 3 runs\n                older_runs = run_summaries[:-3] if len(run_summaries) &gt; 3 else []\n\n                if older_runs:\n                    recent_avg_duration = sum(r[\"duration\"] for r in recent_runs) / len(recent_runs)\n                    older_avg_duration = sum(r[\"duration\"] for r in older_runs) / len(older_runs)\n\n                    if recent_avg_duration &lt; older_avg_duration * 0.8:\n                        insights.append(\"\ud83d\ude80 Performance improving: Recent runs 20% faster\")\n                    elif recent_avg_duration &gt; older_avg_duration * 1.2:\n                        insights.append(\"\u26a0\ufe0f Performance degrading: Recent runs 20% slower\")\n\n            # Error patterns\n            error_rates = [r[\"errors\"] / max(r[\"nodes\"], 1) for r in run_summaries]\n            avg_error_rate = sum(error_rates) / len(error_rates)\n\n            if avg_error_rate == 0:\n                insights.append(\"\u2728 Perfect reliability: Zero errors across all runs\")\n            elif avg_error_rate &lt; 0.1:\n                insights.append(f\"\u2705 High reliability: {avg_error_rate:.1%} average error rate\")\n            elif avg_error_rate &gt; 0.3:\n                insights.append(f\"\ud83d\udd27 Reliability concerns: {avg_error_rate:.1%} average error rate\")\n\n            # Cost efficiency\n            costs = [r[\"cost\"] for r in run_summaries if r[\"cost\"] &gt; 0]\n            if costs:\n                avg_cost = sum(costs) / len(costs)\n                if avg_cost &lt; 0.01:\n                    insights.append(f\"\ud83d\udc9a Very cost efficient: ${avg_cost:.4f} average per run\")\n                elif avg_cost &gt; 0.1:\n                    insights.append(f\"\ud83d\udcb8 High cost per run: ${avg_cost:.4f} average\")\n\n            # Consistency\n            durations = [r[\"duration\"] for r in run_summaries]\n            if len(durations) &gt; 1:\n                import statistics\n                duration_std = statistics.stdev(durations)\n                duration_mean = statistics.mean(durations)\n                cv = duration_std / duration_mean if duration_mean &gt; 0 else 0\n\n                if cv &lt; 0.2:\n                    insights.append(\"\ud83c\udfaf Highly consistent execution times\")\n                elif cv &gt; 0.5:\n                    insights.append(\"\ud83d\udcca Variable execution times - investigate bottlenecks\")\n\n            # Success patterns\n            completion_rates = [r[\"completed\"] / max(r[\"nodes\"], 1) for r in run_summaries]\n            avg_completion = sum(completion_rates) / len(completion_rates)\n\n            if avg_completion &gt; 0.95:\n                insights.append(f\"\ud83c\udf89 Excellent completion rate: {avg_completion:.1%}\")\n            elif avg_completion &lt; 0.8:\n                insights.append(f\"\u26a0\ufe0f Low completion rate: {avg_completion:.1%}\")\n\n        except Exception as e:\n            insights.append(f\"\u26a0\ufe0f Error generating insights: {e}\")\n\n        return insights\n\n    def print_accumulated_summary(self):\n        \"\"\"Print comprehensive summary of all accumulated runs\"\"\"\n        try:\n            summary = self.get_accumulated_summary()\n\n            if summary.get(\"total_runs\", 0) == 0:\n                if self.use_rich:\n                    self.console.print(\"\ud83d\udcca No accumulated runs to display\", style=\"yellow\")\n                else:\n                    print(\"\ud83d\udcca No accumulated runs to display\")\n                return\n\n            if not self.use_rich:\n                self._print_accumulated_summary_fallback(summary)\n                return\n\n            # Rich formatted output\n            self.console.print()\n            self.console.print(\"\ud83d\uddc2\ufe0f [bold cyan]ACCUMULATED EXECUTION SUMMARY[/bold cyan] \ud83d\uddc2\ufe0f\")\n\n            # Overview table\n            overview_table = Table(title=\"\ud83d\udcca Aggregate Overview\", box=box.ROUNDED)\n            overview_table.add_column(\"Metric\", style=\"cyan\", min_width=20)\n            overview_table.add_column(\"Value\", style=\"green\", min_width=15)\n            overview_table.add_column(\"Average\", style=\"blue\", min_width=15)\n\n            agg = summary[\"aggregate_metrics\"]\n            avg = summary[\"average_metrics\"]\n\n            overview_table.add_row(\"Total Runs\", str(summary[\"total_runs\"]), \"\")\n            overview_table.add_row(\"Total Duration\", f\"{agg['total_duration']:.1f}s\", f\"{avg['avg_duration']:.1f}s\")\n            overview_table.add_row(\"Total Nodes\", str(agg[\"total_nodes\"]), f\"{avg['avg_nodes']:.1f}\")\n            overview_table.add_row(\"Total Events\", str(agg[\"total_events\"]), \"\")\n\n            if agg[\"total_cost\"] &gt; 0:\n                overview_table.add_row(\"Total Cost\", self._format_cost(agg[\"total_cost\"]),\n                                       self._format_cost(avg[\"avg_cost\"]))\n\n            if agg[\"total_tokens\"] &gt; 0:\n                overview_table.add_row(\"Total Tokens\", f\"{agg['total_tokens']:,}\",\n                                       f\"{avg['avg_tokens']:,.0f}\")\n\n            overview_table.add_row(\"Error Rate\", f\"{avg['avg_error_rate']:.1%}\", \"\")\n            overview_table.add_row(\"Health Score\", f\"{avg['avg_health_score']:.1%}\", \"\")\n\n            self.console.print(overview_table)\n\n            # Individual runs table\n            runs_table = Table(title=\"\ud83c\udfc3 Individual Runs\", box=box.ROUNDED)\n            runs_table.add_column(\"Run\", style=\"cyan\")\n            runs_table.add_column(\"Duration\", style=\"blue\")\n            runs_table.add_column(\"Nodes\", style=\"green\")\n            runs_table.add_column(\"Success\", style=\"green\")\n            runs_table.add_column(\"Cost\", style=\"yellow\")\n            runs_table.add_column(\"Health\", style=\"magenta\")\n\n            for run in summary[\"run_summaries\"]:\n                success_rate = run[\"completed\"] / max(run[\"nodes\"], 1)\n                cost_str = self._format_cost(run[\"cost\"]) if run[\"cost\"] &gt; 0 else \"-\"\n\n                runs_table.add_row(\n                    run[\"run_name\"],\n                    f\"{run['duration']:.1f}s\",\n                    f\"{run['completed']}/{run['nodes']}\",\n                    f\"{success_rate:.1%}\",\n                    cost_str,\n                    f\"{run['health_score']:.1%}\"\n                )\n\n            self.console.print(runs_table)\n\n            # Insights\n            if summary.get(\"performance_insights\"):\n                insights_panel = Panel(\n                    \"\\n\".join(f\"\u2022 {insight}\" for insight in summary[\"performance_insights\"]),\n                    title=\"\ud83d\udd0d Performance Insights\",\n                    style=\"yellow\"\n                )\n                self.console.print(insights_panel)\n\n        except Exception as e:\n            error_msg = f\"\u274c Error printing accumulated summary: {e}\"\n            if self.use_rich:\n                self.console.print(error_msg, style=\"red bold\")\n            else:\n                print(error_msg)\n\n    def export_accumulated_data(self, filepath: str = None, extra_data: Dict[str, Any] = None) -&gt; str:\n        \"\"\"Export all accumulated run data to file\"\"\"\n        try:\n            if filepath is None:\n                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n                filepath = f\"accumulated_execution_data_{timestamp}.json\"\n\n            export_data = {\n                \"export_timestamp\": time.time(),\n                \"export_version\": \"1.0\",\n                \"printer_config\": {\n                    \"mode\": self.mode.value,\n                    \"use_rich\": self.use_rich,\n                    \"realtime_minimal\": self.realtime_minimal\n                },\n                \"accumulated_summary\": self.get_accumulated_summary(),\n                \"all_runs\": self._accumulated_runs,\n\n            }\n\n            export_data.update(extra_data or {})\n\n            import json\n            with open(filepath, 'w') as f:\n                json.dump(export_data, f, indent=2, default=str)\n\n            if self.use_rich:\n                self.console.print(f\"\ud83d\udcc1 Accumulated data exported to: {filepath}\", style=\"green bold\")\n                self.console.print(f\"\ud83d\udcca Total runs exported: {len(self._accumulated_runs)}\", style=\"blue\")\n            else:\n                print(f\"\ud83d\udcc1 Accumulated data exported to: {filepath}\")\n                print(f\"\ud83d\udcca Total runs exported: {len(self._accumulated_runs)}\")\n\n            return filepath\n\n        except Exception as e:\n            error_msg = f\"\u274c Error exporting accumulated data: {e}\"\n            if self.use_rich:\n                self.console.print(error_msg, style=\"red bold\")\n            else:\n                print(error_msg)\n            return \"\"\n\n    def _print_accumulated_summary_fallback(self, summary: Dict[str, Any]):\n        \"\"\"Fallback accumulated summary without Rich\"\"\"\n        try:\n            print(f\"\\n{'=' * 80}\")\n            print(\"\ud83d\uddc2\ufe0f ACCUMULATED EXECUTION SUMMARY \ud83d\uddc2\ufe0f\")\n            print(f\"{'=' * 80}\")\n\n            agg = summary[\"aggregate_metrics\"]\n            avg = summary[\"average_metrics\"]\n\n            print(f\"Total Runs: {summary['total_runs']}\")\n            print(f\"Total Duration: {agg['total_duration']:.1f}s (avg: {avg['avg_duration']:.1f}s)\")\n            print(f\"Total Nodes: {agg['total_nodes']} (avg: {avg['avg_nodes']:.1f})\")\n            print(f\"Total Events: {agg['total_events']}\")\n\n            if agg[\"total_cost\"] &gt; 0:\n                print(f\"Total Cost: {self._format_cost(agg['total_cost'])} (avg: {self._format_cost(avg['avg_cost'])})\")\n\n            if agg[\"total_tokens\"] &gt; 0:\n                print(f\"Total Tokens: {agg['total_tokens']:,} (avg: {avg['avg_tokens']:,.0f})\")\n\n            print(f\"Average Error Rate: {avg['avg_error_rate']:.1%}\")\n            print(f\"Average Health Score: {avg['avg_health_score']:.1%}\")\n\n            print(f\"\\n{'=' * 80}\")\n            print(\"\ud83c\udfc3 INDIVIDUAL RUNS:\")\n            print(f\"{'=' * 80}\")\n\n            for run in summary[\"run_summaries\"]:\n                success_rate = run[\"completed\"] / max(run[\"nodes\"], 1)\n                cost_str = self._format_cost(run[\"cost\"]) if run[\"cost\"] &gt; 0 else \"N/A\"\n\n                print(f\"\u2022 {run['run_name']}: {run['duration']:.1f}s | \"\n                      f\"{run['completed']}/{run['nodes']} nodes ({success_rate:.1%}) | \"\n                      f\"Cost: {cost_str} | Health: {run['health_score']:.1%}\")\n\n            # Insights\n            if summary.get(\"performance_insights\"):\n                print(f\"\\n\ud83d\udd0d PERFORMANCE INSIGHTS:\")\n                print(f\"{'-' * 40}\")\n                for insight in summary[\"performance_insights\"]:\n                    print(f\"\u2022 {insight}\")\n\n            print(f\"{'=' * 80}\")\n\n        except Exception as e:\n            print(f\"\u274c Error printing fallback summary: {e}\")\n\n    def _create_one_line_summary(self) -&gt; str:\n        \"\"\"Create a concise one-line summary of current execution state\"\"\"\n        try:\n            summary = self.tree_builder.get_execution_summary()\n            current_node = summary[\"execution_flow\"][\"current_node\"]\n            active_nodes = summary[\"execution_flow\"][\"active_nodes\"]\n            timing = summary[\"timing\"]\n\n            # Get spinner\n            spinner = f\"@{self.agent_name} \"\n\n            # Format elapsed time\n            elapsed = timing[\"elapsed\"]\n            if elapsed &lt; 60:\n                time_str = f\"{elapsed:.1f}s\"\n            elif elapsed &lt; 3600:\n                minutes = int(elapsed // 60)\n                seconds = elapsed % 60\n                time_str = f\"{minutes}m{seconds:.1f}s\"\n            else:\n                hours = int(elapsed // 3600)\n                minutes = int((elapsed % 3600) // 60)\n                time_str = f\"{hours}h{minutes}m\"\n\n            # Get current event info\n            if current_node and current_node in self.tree_builder.nodes:\n                node = self.tree_builder.nodes[current_node]\n\n                # Get the most relevant info\n                info_parts = []\n                if node.strategy:\n                    info_parts.append(f\"strategy: {node.strategy}\")\n                if node.reasoning:\n                    reasoning_short = node.reasoning[:50] + \"...\" if len(node.reasoning) &gt; 50 else node.reasoning\n                    info_parts.append(f\"reasoning: {reasoning_short}\")\n\n                # Recent activity\n                recent_activity = \"processing\"\n                if node.llm_calls and node.llm_calls[-1].timestamp &gt; time.time() - 5:\n                    recent_activity = \"llm_call\"\n                elif node.tool_calls and node.tool_calls[-1].timestamp &gt; time.time() - 5:\n                    recent_activity = f\"tool: {node.tool_calls[-1].tool_name}\"\n\n                info_str = \" | \".join(info_parts) if info_parts else recent_activity\n                if len(info_str) &gt; 80:\n                    info_str = info_str[15:92] + \"...\"\n\n                return f\"{spinner} {current_node} \u2192 {recent_activity} | {info_str} | {time_str}\" if recent_activity != info_str else f\"{spinner} {current_node}  \u2192 | {info_str} | {time_str}\"\n\n            # Fallback summary\n            session_info = summary[\"session_info\"]\n            progress_text = f\"{session_info['completed_nodes']}/{session_info['total_nodes']} nodes\"\n            return f\"{spinner} Processing {progress_text} | {time_str}\"\n\n        except Exception as e:\n            return f\"\u26a0\ufe0f Processing... | {time.time():.1f}s\"\n\n    def _print_one_line_summary(self):\n        \"\"\"Print or update the one-line summary\"\"\"\n        try:\n            summary_line = self._create_one_line_summary()\n\n            if summary_line != self._last_summary:\n                # Clear the previous line and print new summary\n                if self._last_summary:\n                    print(f\"\\r{' ' * len(self._last_summary)}\", end=\"\", flush=True)\n                print(f\"\\r{summary_line}\", end=\"\", flush=True)\n                self._last_summary = summary_line\n\n        except Exception as e:\n            print(f\"\\r\u26a0\ufe0f Error updating summary: {e}\", end=\"\", flush=True)\n\n    def _create_execution_tree(self) -&gt; Tree:\n        \"\"\"Create comprehensive execution tree with enhanced features\"\"\"\n        try:\n            summary = self.tree_builder.get_execution_summary()\n            session_info = summary[\"session_info\"]\n            timing = summary[\"timing\"]\n            health = summary[\"health_indicators\"]\n\n            # Root tree with health indicator\n            health_emoji = \"\ud83d\udfe2\" if health[\"overall_health\"] &gt; 0.8 else \"\ud83d\udfe1\" if health[\"overall_health\"] &gt; 0.5 else \"\ud83d\udd34\"\n            root_title = f\"{health_emoji} Agent Execution Flow\"\n\n            if timing[\"elapsed\"] &gt; 0:\n                root_title += f\" ({timing['elapsed']:.1f}s elapsed)\"\n\n            tree = Tree(root_title, style=\"bold cyan\")\n\n            # Execution status overview\n            self._add_execution_overview(tree, summary)\n\n            # Main execution flow\n            self._add_execution_flow_branch(tree, summary)\n\n            # Error log (if any errors)\n            if self.tree_builder.error_log and self.mode in [VerbosityMode.VERBOSE, VerbosityMode.DEBUG]:\n                self._add_error_log_branch(tree)\n\n            # Performance metrics\n            if self.mode in [VerbosityMode.VERBOSE, VerbosityMode.DEBUG]:\n                self._add_performance_branch(tree, summary)\n\n            # Routing history\n            if (self.tree_builder.routing_history and\n                self.mode in [VerbosityMode.VERBOSE, VerbosityMode.DEBUG]):\n                self._add_routing_branch(tree)\n\n            return tree\n\n        except Exception as e:\n            # Fallback tree on error\n            error_tree = Tree(\"\u274c Error creating execution tree\", style=\"red\")\n            error_tree.add(f\"Error: {str(e)}\", style=\"red dim\")\n            return error_tree\n\n    def _add_execution_overview(self, tree: Tree, summary: Dict[str, Any]):\n        \"\"\"Add execution overview section\"\"\"\n        session_info = summary[\"session_info\"]\n        health = summary[\"health_indicators\"]\n\n        overview_text = (f\"\ud83d\udcca Status: {session_info['completed_nodes']}/{session_info['total_nodes']} completed \"\n                         f\"({health['completion_rate']:.1%})\")\n\n        if session_info[\"active_nodes\"] &gt; 0:\n            overview_text += f\" | {session_info['active_nodes']} active\"\n        if session_info[\"failed_nodes\"] &gt; 0:\n            overview_text += f\" | {session_info['failed_nodes']} failed\"\n\n        overview_branch = tree.add(overview_text, style=\"bold yellow\")\n\n        # Health indicators\n        if self.mode in [VerbosityMode.VERBOSE, VerbosityMode.DEBUG]:\n            health_text = f\"Health: {health['overall_health']:.1%} | Error Rate: {health['error_rate']:.1%}\"\n            overview_branch.add(health_text, style=\"blue dim\")\n\n    def _add_execution_flow_branch(self, tree: Tree, summary: Dict[str, Any]):\n        \"\"\"Add detailed execution flow branch\"\"\"\n        flow_branch = tree.add(\"\ud83d\udd04 Execution Flow\", style=\"bold blue\")\n\n        execution_flow = summary[\"execution_flow\"][\"flow\"]\n        active_nodes = set(summary[\"execution_flow\"][\"active_nodes\"])\n        completion_order = summary[\"execution_flow\"][\"completion_order\"]\n\n        for i, node_name in enumerate(execution_flow):\n            if node_name not in self.tree_builder.nodes:\n                continue\n\n            node = self.tree_builder.nodes[node_name]\n\n            # Status icon and styling\n            status_icon = node.get_status_icon()\n            status_style = node.get_status_color()\n\n            # Node info with enhanced details\n            node_text = f\"{status_icon} [{i + 1}] {node_name}\"\n\n            # Add timing info\n            duration_str = node.get_duration_str()\n            if duration_str != \"...\":\n                node_text += f\" ({duration_str})\"\n\n            # Add performance indicator\n            if node.is_completed() and node.duration:\n                efficiency = node._calculate_efficiency_score()\n                if efficiency &gt; 0.8:\n                    node_text += \"\"\n                elif efficiency &lt; 0.5:\n                    node_text += \" \ud83d\udc0c\"\n\n            node_branch = flow_branch.add(node_text, style=status_style)\n\n            # Add detailed information based on verbosity\n            if self.mode in [VerbosityMode.VERBOSE, VerbosityMode.DEBUG]:\n                self._add_node_details(node_branch, node)\n            elif self.mode == VerbosityMode.STANDARD and node.error:\n                # Show errors even in standard mode\n                node_branch.add(f\"\u274c {node.error}\", style=\"red dim\")\n\n    def _add_node_details(self, parent_branch: Tree, node: ExecutionNode):\n        \"\"\"Add comprehensive node details\"\"\"\n\n        # Strategy and reasoning\n        if node.strategy:\n            parent_branch.add(f\"\ud83c\udfaf Strategy: {node.strategy}\", style=\"cyan dim\")\n        if node.reasoning:\n            parent_branch.add(f\"\ud83e\udde0 Reasoning: {node.reasoning[:100]}...\", style=\"blue dim\")\n\n        # Error details\n        if node.error and node.error_details:\n            error_branch = parent_branch.add(f\"\u274c Error: {node.error}\", style=\"red\")\n            if self.mode == VerbosityMode.DEBUG:\n                for key, value in node.error_details.items():\n                    error_branch.add(f\"{key}: {value}\", style=\"red dim\")\n\n        # LLM calls summary\n        if node.llm_calls:\n            llm_summary = f\"\ud83e\udde0 LLM: {len(node.llm_calls)} calls\"\n            if node.total_cost &gt; 0:\n                llm_summary += f\", ${node.total_cost:.4f}\"\n            if node.total_tokens &gt; 0:\n                llm_summary += f\", {node.total_tokens:,} tokens\"\n\n            llm_branch = parent_branch.add(llm_summary, style=\"blue dim\")\n\n            # Show individual calls in debug mode\n            if self.mode == VerbosityMode.DEBUG:\n                for call in node.llm_calls[-3:]:  # Last 3 calls\n                    call_info = f\"{call.llm_model or 'Unknown'}\"\n                    if call.llm_duration:\n                        call_info += f\" ({call.llm_duration:.1f}s)\"\n                    llm_branch.add(call_info, style=\"blue dim\")\n\n        # Tool calls summary\n        if node.tool_calls:\n            tool_summary = f\"\ud83d\udd27 Tools: {len(node.tool_calls)} calls\"\n            successful_tools = sum(1 for call in node.tool_calls if call.tool_success)\n            if successful_tools &lt; len(node.tool_calls):\n                tool_summary += f\" ({successful_tools}/{len(node.tool_calls)} successful)\"\n\n            tool_branch = parent_branch.add(tool_summary, style=\"green dim\")\n\n            # Show individual tool calls\n            if self.mode == VerbosityMode.DEBUG:\n                for call in node.tool_calls[-3:]:  # Last 3 calls\n                    success_icon = \"\u2713\" if call.tool_success else \"\u2717\"\n                    call_info = f\"{success_icon} {call.tool_name}\"\n                    if call.tool_duration:\n                        call_info += f\" ({call.tool_duration:.1f}s)\"\n                    style = \"green dim\" if call.tool_success else \"red dim\"\n                    tool_branch.add(call_info, style=style)\n\n        # Performance metrics\n        if node.is_completed() and self.mode == VerbosityMode.DEBUG:\n            perf = node.get_performance_summary()\n            perf_text = f\"\ud83d\udcc8 Efficiency: {perf['efficiency_score']:.1%}\"\n            if perf['retry_count'] &gt; 0:\n                perf_text += f\" (Retries: {perf['retry_count']})\"\n            parent_branch.add(perf_text, style=\"yellow dim\")\n\n    def _add_error_log_branch(self, tree: Tree):\n        \"\"\"Add error log branch\"\"\"\n        if not self.tree_builder.error_log:\n            return\n\n        error_branch = tree.add(f\"\u274c Error Log ({len(self.tree_builder.error_log)})\", style=\"red bold\")\n\n        # Show recent errors\n        recent_errors = self.tree_builder.error_log[-5:]  # Last 5 errors\n        for error in recent_errors:\n            timestamp = datetime.fromtimestamp(error[\"timestamp\"]).strftime(\"%H:%M:%S\")\n            error_text = f\"[{timestamp}] {error['node']}: {error['error']}\"\n            if error.get('retry_count', 0) &gt; 0:\n                error_text += f\" (Retry #{error['retry_count']})\"\n            error_branch.add(error_text, style=\"red dim\")\n\n    def _add_performance_branch(self, tree: Tree, summary: Dict[str, Any]):\n        \"\"\"Add performance metrics branch\"\"\"\n        perf = summary[\"performance_metrics\"]\n        health = summary[\"health_indicators\"]\n        timing = summary[\"timing\"]\n\n        perf_branch = tree.add(\"\ud83d\udcca Performance Metrics\", style=\"bold green\")\n\n        # Cost and token metrics\n        if perf[\"total_cost\"] &gt; 0:\n            cost_text = f\"\ud83d\udcb0 Cost: {self._format_cost(perf['total_cost'])}\"\n            perf_branch.add(cost_text, style=\"green dim\")\n\n        if perf[\"total_tokens\"] &gt; 0:\n            tokens_text = f\"\ud83c\udfaf Tokens: {perf['total_tokens']:,}\"\n            if timing[\"elapsed\"] &gt; 0:\n                tokens_per_sec = perf[\"total_tokens\"] / timing[\"elapsed\"]\n                tokens_text += f\" ({tokens_per_sec:.0f}/sec)\"\n            perf_branch.add(tokens_text, style=\"green dim\")\n\n        # Efficiency metrics\n        if health[\"average_node_efficiency\"] &gt; 0:\n            efficiency_text = f\"\u26a1 Avg Efficiency: {health['average_node_efficiency']:.1%}\"\n            perf_branch.add(efficiency_text, style=\"green dim\")\n\n        # Event processing rate\n        if timing[\"elapsed\"] &gt; 0:\n            events_per_sec = perf[\"total_events\"] / timing[\"elapsed\"]\n            processing_text = f\"\ud83d\udcdd Events: {perf['total_events']} ({events_per_sec:.1f}/sec)\"\n            perf_branch.add(processing_text, style=\"green dim\")\n\n    def _add_routing_branch(self, tree: Tree):\n        \"\"\"Add routing decisions branch\"\"\"\n        if not self.tree_builder.routing_history:\n            return\n\n        routing_branch = tree.add(f\"\ud83e\udded Routing History ({len(self.tree_builder.routing_history)})\",\n                                  style=\"bold purple\")\n\n        # Show recent routing decisions\n        recent_routes = self.tree_builder.routing_history[-5:]  # Last 5\n        for i, route in enumerate(recent_routes):\n            timestamp = datetime.fromtimestamp(route[\"timestamp\"]).strftime(\"%H:%M:%S\")\n            route_text = f\"[{timestamp}] {route['from']} \u2192 {route['to']}\"\n            if route[\"decision\"] != \"unknown\":\n                route_text += f\" ({route['decision']})\"\n            routing_branch.add(route_text, style=\"purple dim\")\n\n    def _format_cost(self, cost: float) -&gt; str:\n        \"\"\"Enhanced cost formatting\"\"\"\n        if cost &lt; 0.0001:\n            return f\"${cost * 1000000:.1f}\u03bc\"\n        elif cost &lt; 0.001:\n            return f\"${cost * 1000:.1f}m\"\n        elif cost &lt; 1:\n            return f\"${cost * 1000:.1f}m\"\n        else:\n            return f\"${cost:.4f}\"\n\n    def _print_tree_update(self):\n        \"\"\"Print tree update with minimal realtime support\"\"\"\n        try:\n            if self._fallback_mode:\n                self._print_fallback()\n                return\n\n            if not self.use_rich:\n                self._print_fallback()\n                return\n\n            # In minimal realtime mode, only print one-line summary unless full tree is needed\n            if self.realtime_minimal and self.mode == VerbosityMode.REALTIME and not self._needs_full_tree:\n                self._print_one_line_summary()\n                return\n\n            # Full tree printing (existing logic)\n            self._print_counter += 1\n            summary = self.tree_builder.get_execution_summary()\n\n            # If we printed a one-line summary before, clear it and add newline\n            if self._last_summary and self.realtime_minimal:\n                print()  # Move to next line\n                self._last_summary = \"\"\n\n            # Clear screen in realtime mode only for full tree updates\n            if self.mode == VerbosityMode.REALTIME and self._print_counter &gt; 1 and not self.realtime_minimal:\n                self.console.clear()\n\n            # Create and print header\n            header = self._create_header(summary)\n            tree = self._create_execution_tree()\n\n            # Print everything\n            self.console.print()\n            self.console.print(header)\n            self.console.print(tree)\n\n            # Update progress in realtime mode\n            if self.mode == VerbosityMode.REALTIME:\n                self._update_progress_display(summary)\n\n            # Reset full tree flag\n            self._needs_full_tree = False\n\n            # Reset error counter on successful print\n            self._consecutive_errors = 0\n\n        except Exception as e:\n            self._consecutive_errors += 1\n            if self._consecutive_errors &lt;= self._error_threshold:\n                print(f\"\u26a0\ufe0f  Print error #{self._consecutive_errors}: {e}\")\n                if self._consecutive_errors == self._error_threshold:\n                    print(\"\ud83d\udd04 Switching to fallback mode...\")\n                    self._fallback_mode = True\n\n            # Always try fallback\n            self._print_fallback()\n\n    def _create_header(self, summary: Dict[str, Any]) -&gt; Panel:\n        \"\"\"Create informative header panel\"\"\"\n        session_info = summary[\"session_info\"]\n        timing = summary[\"timing\"]\n        health = summary[\"health_indicators\"]\n\n        # Status indicators\n        status_parts = []\n        if session_info[\"active_nodes\"] &gt; 0:\n            status_parts.append(f\"\ud83d\udd04 Running\")\n        elif session_info[\"failed_nodes\"] &gt; 0:\n            status_parts.append(f\"\u274c Errors\")\n        elif session_info[\"completed_nodes\"] == session_info[\"total_nodes\"]:\n            status_parts.append(f\"\u2705 Complete\")\n        else:\n            status_parts.append(f\"\u23f8\ufe0f Waiting\")\n\n        status_parts[-1] += f\" ({self.agent_name})\"\n\n        status_text = \" | \".join(status_parts)\n\n        # Progress info\n        progress_text = (f\"Progress: {session_info['completed_nodes']}/{session_info['total_nodes']} \"\n                         f\"({health['completion_rate']:.1%})\")\n\n        # Timing info\n        timing_text = f\"Runtime: {human_readable_time(timing['elapsed'])}\"\n        if timing[\"estimated_completion\"]:\n            eta = timing[\"estimated_completion\"] - time.time()\n            if eta &gt; 0:\n                timing_text += f\" | ETA: {eta:.0f}s\"\n\n        # Performance info\n        perf_metrics = summary[\"performance_metrics\"]\n        perf_text = f\"Events: {perf_metrics['total_events']}\"\n        if perf_metrics[\"total_cost\"] &gt; 0:\n            perf_text += f\" | Cost: {self._format_cost(perf_metrics['total_cost'])}\"\n\n        header_content = f\"{status_text}\\n{progress_text} | {timing_text}\\n{perf_text}\"\n\n        return Panel(\n            header_content,\n            title=f\"\ud83d\udcca Update #{self._print_counter}\",\n            style=\"cyan\",\n            box=box.ROUNDED\n        )\n\n    def _update_progress_display(self, summary: Dict[str, Any]):\n        \"\"\"Update progress display for realtime mode\"\"\"\n        if not hasattr(self, 'progress'):\n            return\n\n        session_info = summary[\"session_info\"]\n\n        if not self.progress_task:\n            description = f\"Processing {session_info['total_nodes']} nodes...\"\n            self.progress_task = self.progress.add_task(description, total=session_info['total_nodes'])\n\n        # Update progress\n        completed = session_info[\"completed_nodes\"]\n        self.progress.update(self.progress_task, completed=completed)\n\n        # Update description\n        if session_info[\"active_nodes\"] &gt; 0:\n            current_node = summary[\"execution_flow\"][\"current_node\"]\n            description = f\"Processing: {current_node}...\"\n        else:\n            description = \"Processing complete\"\n\n        self.progress.update(self.progress_task, description=description)\n\n    def _print_fallback(self):\n        \"\"\"Enhanced fallback printing without Rich\"\"\"\n        try:\n            summary = self.tree_builder.get_execution_summary()\n            session_info = summary[\"session_info\"]\n            timing = summary[\"timing\"]\n            perf = summary[\"performance_metrics\"]\n\n            print(f\"\\n{'=' * 80}\")\n            print(f\"\ud83d\ude80 AGENT EXECUTION UPDATE #{self._print_counter}\")\n            print(f\"Session: {summary.get('session_id', 'unknown')} | Runtime: {timing['elapsed']:.1f}s\")\n            print(f\"Progress: {session_info['completed_nodes']}/{session_info['total_nodes']} nodes\")\n\n            if session_info[\"failed_nodes\"] &gt; 0:\n                print(f\"\u274c Failures: {session_info['failed_nodes']}\")\n            if perf[\"total_cost\"] &gt; 0:\n                print(f\"\ud83d\udcb0 Cost: {self._format_cost(perf['total_cost'])}\")\n\n            print(f\"{'=' * 80}\")\n\n            # Show execution flow\n            print(\"\\n\ud83d\udd04 Execution Flow:\")\n            for i, node_name in enumerate(summary[\"execution_flow\"][\"flow\"]):\n                if node_name not in self.tree_builder.nodes:\n                    continue\n\n                node = self.tree_builder.nodes[node_name]\n                status_icon = node.get_status_icon()\n                duration = node.get_duration_str()\n\n                print(f\"  {status_icon} [{i + 1}] {node_name} ({duration})\")\n\n                if node.error and self.mode in [VerbosityMode.STANDARD, VerbosityMode.VERBOSE]:\n                    print(f\"    \u274c {node.error}\")\n\n            # Show errors in verbose modes\n            if (self.tree_builder.error_log and\n                self.mode in [VerbosityMode.VERBOSE, VerbosityMode.DEBUG]):\n                print(f\"\\n\u274c Recent Errors:\")\n                for error in self.tree_builder.error_log[-3:]:\n                    timestamp = datetime.fromtimestamp(error[\"timestamp\"]).strftime(\"%H:%M:%S\")\n                    print(f\"  [{timestamp}] {error['node']}: {error['error']}\")\n\n            print(f\"{'=' * 80}\")\n\n        except Exception as e:\n            # Ultimate fallback\n            print(f\"\\n\u26a0\ufe0f  EXECUTION UPDATE #{self._print_counter} - Basic fallback\")\n            print(f\"Agent Name: {self.agent_name}\")\n            print(f\"Total events processed: {self.tree_builder.total_events}\")\n            print(f\"Nodes: {len(self.tree_builder.nodes)}\")\n            print(f\"Errors encountered: {len(self.tree_builder.error_log)}\")\n            if e:\n                print(f\"Print error: {e}\")\n\n    async def progress_callback(self, event: ProgressEvent):\n        \"\"\"Main progress callback with minimal realtime support\"\"\"\n        try:\n            # Add event to tree builder\n            self.tree_builder.add_event(event)\n\n            # Store in history with size limit\n            self.print_history.append({\n                \"timestamp\": event.timestamp,\n                \"event_type\": event.event_type,\n                \"node_name\": event.node_name,\n                \"event_id\": event.event_id\n            })\n\n            # Maintain history size limit\n            if len(self.print_history) &gt; self.max_history:\n                self.print_history = self.print_history[-self.max_history:]\n\n            # Check if we need to show full tree (errors or completion)\n            if self.realtime_minimal:\n                # Check for errors\n                if (event.event_type == \"error\" or\n                    event.success is False or\n                    (event.metadata and event.metadata.get(\"error\"))):\n                    self._needs_full_tree = True\n\n                # Check for completion\n                if (event.event_type in [\"execution_complete\", \"task_complete\", \"node_exit\"] or\n                    (event.node_name in self.tree_builder.nodes and\n                     self.tree_builder.nodes[event.node_name].is_completed())):\n                    # Check if this is final completion\n                    summary = self.tree_builder.get_execution_summary()\n                    if (summary[\"session_info\"][\"completed_nodes\"] + summary[\"session_info\"][\"failed_nodes\"] ==\n                        summary[\"session_info\"][\"total_nodes\"]):\n                        self._needs_full_tree = True\n\n            # Print debug info in debug mode\n            if self.mode == VerbosityMode.DEBUG:\n                self._print_debug_event(event)\n\n            # Decide whether to print update\n            if event.node_name == \"FlowAgent\" or self._should_print_update():\n                self.print_strategy_from_event(event)\n                self.print_plan_from_event(event)\n                self.agent_name = event.agent_name if event.agent_name else event.metadata.get(\"agent_name\", self.agent_name)\n                self._print_tree_update()\n\n        except Exception as e:\n            # Emergency error handling\n            self._consecutive_errors += 1\n            print(f\"\u26a0\ufe0f  Progress callback error #{self._consecutive_errors}: {e}\")\n\n            if self._consecutive_errors &gt; self._error_threshold:\n                print(\"\ud83d\udea8 Progress printing disabled due to excessive errors\")\n                # Disable further callbacks\n                self.progress_callback = self._noop_callback\n\n    def _print_debug_event(self, event: ProgressEvent):\n        \"\"\"Print individual event details in debug mode\"\"\"\n        timestamp = datetime.fromtimestamp(event.timestamp).strftime(\"%H:%M:%S.%f\")[:-3]\n\n        if self.use_rich:\n            debug_text = f\"[{timestamp}] {event.event_type.upper()} - {event.node_name}\"\n            if event.success is not None:\n                success_icon = \"\u2705\" if event.success else \"\u274c\"\n                debug_text += f\" {success_icon}\"\n            self.console.print(debug_text, style=\"dim\")\n        else:\n            print(f\"[{timestamp}] {event.event_type.upper()} - {event.node_name}\")\n\n    async def _noop_callback(self, event: ProgressEvent):\n        \"\"\"No-op callback when printing is disabled\"\"\"\n        pass\n\n    def print_final_summary(self):\n        \"\"\"Print comprehensive final summary\"\"\"\n        try:\n            if self._fallback_mode:\n                self._print_final_summary_fallback()\n                return\n\n            if not self.use_rich:\n                self._print_final_summary_fallback()\n                return\n\n            summary = self.tree_builder.get_execution_summary()\n\n            # Final completion message\n            self.console.print()\n            self.console.print(\"\ud83c\udf89 [bold green]EXECUTION COMPLETED[/bold green] \ud83c\udf89\")\n\n            # Final execution tree\n            final_tree = self._create_execution_tree()\n            self.console.print(final_tree)\n\n            # Comprehensive summary table\n            self._print_final_summary_table(summary)\n\n            # Performance analysis\n            if self.mode in [VerbosityMode.VERBOSE, VerbosityMode.DEBUG]:\n                self._print_performance_analysis(summary)\n\n        except Exception as e:\n            print(f\"\u26a0\ufe0f  Error printing final summary: {e}\")\n            self._print_final_summary_fallback()\n\n    def _print_final_summary_table(self, summary: Dict[str, Any]):\n        \"\"\"Print detailed final summary table\"\"\"\n        session_info = summary[\"session_info\"]\n        timing = summary[\"timing\"]\n        perf = summary[\"performance_metrics\"]\n        health = summary[\"health_indicators\"]\n\n        table = Table(title=\"\ud83d\udcca Final Execution Summary\", box=box.ROUNDED)\n        table.add_column(\"Metric\", style=\"cyan\", min_width=20)\n        table.add_column(\"Value\", style=\"green\", min_width=15)\n        table.add_column(\"Details\", style=\"dim\", min_width=25)\n\n        # Session metrics\n        table.add_row(\"Session ID\", str(summary.get(\"session_id\", \"N/A\")), \"\")\n        table.add_row(\"Total Runtime\", f\"{timing['elapsed']:.2f}s\", \"\")\n        table.add_row(\"Nodes Processed\", str(session_info[\"total_nodes\"]),\n                      f\"{session_info['completed_nodes']} completed, {session_info['failed_nodes']} failed\")\n\n        # Performance metrics\n        table.add_row(\"Total Events\", str(perf[\"total_events\"]),\n                      f\"{perf['total_events'] / max(timing['elapsed'], 1):.1f} events/sec\")\n        table.add_row(\"Routing Steps\", str(perf[\"routing_steps\"]), \"\")\n\n        if perf[\"total_cost\"] &gt; 0:\n            table.add_row(\"Total Cost\", self._format_cost(perf[\"total_cost\"]), \"\")\n        if perf[\"total_tokens\"] &gt; 0:\n            tokens_per_sec = perf[\"total_tokens\"] / max(timing[\"elapsed\"], 1)\n            table.add_row(\"Total Tokens\", f\"{perf['total_tokens']:,}\", f\"{tokens_per_sec:.0f} tokens/sec\")\n\n        # Health metrics\n        table.add_row(\"Overall Health\", f\"{health['overall_health']:.1%}\", \"\")\n        table.add_row(\"Error Rate\", f\"{health['error_rate']:.1%}\", f\"{perf['error_count']} total errors\")\n        table.add_row(\"Completion Rate\", f\"{health['completion_rate']:.1%}\", \"\")\n        table.add_row(\"Avg Efficiency\", f\"{health['average_node_efficiency']:.1%}\", \"\")\n\n        self.console.print()\n        self.console.print(table)\n\n    def _print_performance_analysis(self, summary: Dict[str, Any]):\n        \"\"\"Print detailed performance analysis\"\"\"\n        analysis_panel = Panel(\n            self._generate_performance_insights(summary),\n            title=\"\ud83d\udd0d Performance Analysis\",\n            style=\"yellow\"\n        )\n        self.console.print()\n        self.console.print(analysis_panel)\n\n    def _generate_performance_insights(self, summary: Dict[str, Any]) -&gt; str:\n        \"\"\"Generate performance insights\"\"\"\n        insights = []\n\n        health = summary[\"health_indicators\"]\n        timing = summary[\"timing\"]\n        perf = summary[\"performance_metrics\"]\n        session_info = summary[\"session_info\"]\n\n        # Health insights\n        if health[\"overall_health\"] &gt; 0.9:\n            insights.append(\"\u2728 Excellent execution with minimal issues\")\n        elif health[\"overall_health\"] &gt; 0.7:\n            insights.append(\"\u2705 Good execution with minor issues\")\n        elif health[\"overall_health\"] &gt; 0.5:\n            insights.append(\"\u26a0\ufe0f Moderate execution with some failures\")\n        else:\n            insights.append(\"\u274c Poor execution with significant issues\")\n\n        # Performance insights\n        if timing[\"elapsed\"] &gt; 0:\n            events_per_sec = perf[\"total_events\"] / timing[\"elapsed\"]\n            if events_per_sec &gt; 10:\n                insights.append(f\"\u26a1 High event processing rate: {events_per_sec:.1f}/sec\")\n            elif events_per_sec &lt; 2:\n                insights.append(f\"\ud83d\udc0c Low event processing rate: {events_per_sec:.1f}/sec\")\n\n        # Error insights\n        if perf[\"error_count\"] == 0:\n            insights.append(\"\ud83c\udfaf Zero errors - perfect execution\")\n        elif health[\"error_rate\"] &lt; 0.1:\n            insights.append(f\"\u2705 Low error rate: {health['error_rate']:.1%}\")\n        else:\n            insights.append(f\"\u26a0\ufe0f High error rate: {health['error_rate']:.1%} - review failed operations\")\n\n        # Cost insights\n        if perf[\"total_cost\"] &gt; 0:\n            cost_per_node = perf[\"total_cost\"] / max(session_info[\"total_nodes\"], 1)\n            if cost_per_node &lt; 0.001:\n                insights.append(f\"\ud83d\udc9a Very cost-efficient: {self._format_cost(cost_per_node)}/node\")\n            elif cost_per_node &gt; 0.01:\n                insights.append(f\"\ud83d\udcb8 High cost per node: {self._format_cost(cost_per_node)}/node\")\n\n        # Node efficiency insights\n        if health[\"average_node_efficiency\"] &gt; 0.8:\n            insights.append(\"\ud83d\ude80 High node efficiency - well-optimized execution\")\n        elif health[\"average_node_efficiency\"] &lt; 0.5:\n            insights.append(\"\ud83d\udd27 Low node efficiency - consider optimization\")\n\n        return \"\\n\".join(f\"\u2022 {insight}\" for insight in insights)\n\n    def _print_final_summary_fallback(self):\n        \"\"\"Fallback final summary without Rich\"\"\"\n        summary = self.tree_builder.get_execution_summary()\n        session_info = summary[\"session_info\"]\n        timing = summary[\"timing\"]\n        perf = summary[\"performance_metrics\"]\n        health = summary[\"health_indicators\"]\n\n        print(f\"\\n{'=' * 80}\")\n        print(\"\ud83c\udf89 EXECUTION COMPLETED \ud83c\udf89\")\n        print(f\"{'=' * 80}\")\n\n        print(f\"Session ID: {summary.get('session_id', 'N/A')}\")\n        print(f\"Total Runtime: {timing['elapsed']:.2f}s\")\n        print(f\"Nodes: {session_info['completed_nodes']}/{session_info['total_nodes']} completed\")\n        print(f\"Events: {perf['total_events']}\")\n        print(f\"Errors: {perf['error_count']}\")\n        print(f\"Overall Health: {health['overall_health']:.1%}\")\n\n        if perf[\"total_cost\"] &gt; 0:\n            print(f\"Total Cost: {self._format_cost(perf['total_cost'])}\")\n        if perf[\"total_tokens\"] &gt; 0:\n            print(f\"Total Tokens: {perf['total_tokens']:,}\")\n\n        print(f\"{'=' * 80}\")\n\n    def get_execution_log(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get complete execution log for analysis\"\"\"\n        return self.print_history.copy()\n\n    def export_summary(self, filepath: str = None) -&gt; Dict[str, Any]:\n        \"\"\"Export comprehensive execution summary\"\"\"\n        summary = self.tree_builder.get_execution_summary()\n\n        # Add detailed node information\n        summary[\"detailed_nodes\"] = {}\n        for node_name, node in self.tree_builder.nodes.items():\n            summary[\"detailed_nodes\"][node_name] = {\n                \"status\": node.status.value,\n                \"duration\": node.duration,\n                \"start_time\": node.start_time,\n                \"end_time\": node.end_time,\n                \"total_cost\": node.total_cost,\n                \"total_tokens\": node.total_tokens,\n                \"llm_calls\": len(node.llm_calls),\n                \"tool_calls\": len(node.tool_calls),\n                \"error\": node.error,\n                \"retry_count\": node.retry_count,\n                \"performance_metrics\": node.get_performance_summary()\n            }\n\n        # Add execution history\n        summary[\"execution_history\"] = self.print_history.copy()\n        summary[\"error_log\"] = self.tree_builder.error_log.copy()\n        summary[\"routing_history\"] = self.tree_builder.routing_history.copy()\n\n        # Export to file if specified\n        if filepath:\n            import json\n            with open(filepath, 'w') as f:\n                json.dump(summary, f, indent=2, default=str)\n\n        return summary\n</code></pre> <code>export_accumulated_data(filepath=None, extra_data=None)</code> \u00b6 <p>Export all accumulated run data to file</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def export_accumulated_data(self, filepath: str = None, extra_data: Dict[str, Any] = None) -&gt; str:\n    \"\"\"Export all accumulated run data to file\"\"\"\n    try:\n        if filepath is None:\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            filepath = f\"accumulated_execution_data_{timestamp}.json\"\n\n        export_data = {\n            \"export_timestamp\": time.time(),\n            \"export_version\": \"1.0\",\n            \"printer_config\": {\n                \"mode\": self.mode.value,\n                \"use_rich\": self.use_rich,\n                \"realtime_minimal\": self.realtime_minimal\n            },\n            \"accumulated_summary\": self.get_accumulated_summary(),\n            \"all_runs\": self._accumulated_runs,\n\n        }\n\n        export_data.update(extra_data or {})\n\n        import json\n        with open(filepath, 'w') as f:\n            json.dump(export_data, f, indent=2, default=str)\n\n        if self.use_rich:\n            self.console.print(f\"\ud83d\udcc1 Accumulated data exported to: {filepath}\", style=\"green bold\")\n            self.console.print(f\"\ud83d\udcca Total runs exported: {len(self._accumulated_runs)}\", style=\"blue\")\n        else:\n            print(f\"\ud83d\udcc1 Accumulated data exported to: {filepath}\")\n            print(f\"\ud83d\udcca Total runs exported: {len(self._accumulated_runs)}\")\n\n        return filepath\n\n    except Exception as e:\n        error_msg = f\"\u274c Error exporting accumulated data: {e}\"\n        if self.use_rich:\n            self.console.print(error_msg, style=\"red bold\")\n        else:\n            print(error_msg)\n        return \"\"\n</code></pre> <code>export_summary(filepath=None)</code> \u00b6 <p>Export comprehensive execution summary</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def export_summary(self, filepath: str = None) -&gt; Dict[str, Any]:\n    \"\"\"Export comprehensive execution summary\"\"\"\n    summary = self.tree_builder.get_execution_summary()\n\n    # Add detailed node information\n    summary[\"detailed_nodes\"] = {}\n    for node_name, node in self.tree_builder.nodes.items():\n        summary[\"detailed_nodes\"][node_name] = {\n            \"status\": node.status.value,\n            \"duration\": node.duration,\n            \"start_time\": node.start_time,\n            \"end_time\": node.end_time,\n            \"total_cost\": node.total_cost,\n            \"total_tokens\": node.total_tokens,\n            \"llm_calls\": len(node.llm_calls),\n            \"tool_calls\": len(node.tool_calls),\n            \"error\": node.error,\n            \"retry_count\": node.retry_count,\n            \"performance_metrics\": node.get_performance_summary()\n        }\n\n    # Add execution history\n    summary[\"execution_history\"] = self.print_history.copy()\n    summary[\"error_log\"] = self.tree_builder.error_log.copy()\n    summary[\"routing_history\"] = self.tree_builder.routing_history.copy()\n\n    # Export to file if specified\n    if filepath:\n        import json\n        with open(filepath, 'w') as f:\n            json.dump(summary, f, indent=2, default=str)\n\n    return summary\n</code></pre> <code>flush(run_name=None)</code> \u00b6 <p>Flush current execution data and store externally for accumulation. Resets internal state for fresh execution timing.</p> <p>Parameters:</p> Name Type Description Default <code>run_name</code> <code>str</code> <p>Optional name for this run</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict containing the flushed execution data</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def flush(self, run_name: str = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Flush current execution data and store externally for accumulation.\n    Resets internal state for fresh execution timing.\n\n    Args:\n        run_name: Optional name for this run\n\n    Returns:\n        Dict containing the flushed execution data\n    \"\"\"\n    try:\n        # Generate run info\n        current_time = time.time()\n        if run_name is None:\n            run_name = f\"run_{self._current_run_id + 1}\"\n\n        # Collect current execution data\n        summary = self.tree_builder.get_execution_summary()\n\n        # Create comprehensive run data\n        run_data = {\n            \"run_id\": self._current_run_id + 1,\n            \"run_name\": run_name,\n            \"flush_timestamp\": current_time,\n            \"execution_summary\": summary,\n            \"detailed_nodes\": {},\n            \"execution_history\": self.print_history.copy(),\n            \"error_log\": self.tree_builder.error_log.copy(),\n            \"routing_history\": self.tree_builder.routing_history.copy(),\n            \"print_counter\": self._print_counter,\n            \"consecutive_errors\": self._consecutive_errors,\n            \"fallback_mode\": self._fallback_mode\n        }\n\n        # Add detailed node information\n        for node_name, node in self.tree_builder.nodes.items():\n            run_data[\"detailed_nodes\"][node_name] = {\n                \"status\": node.status.value,\n                \"duration\": node.duration,\n                \"start_time\": node.start_time,\n                \"end_time\": node.end_time,\n                \"total_cost\": node.total_cost,\n                \"total_tokens\": node.total_tokens,\n                \"llm_calls\": len(node.llm_calls),\n                \"tool_calls\": len(node.tool_calls),\n                \"error\": node.error,\n                \"retry_count\": node.retry_count,\n                \"performance_metrics\": node.get_performance_summary(),\n                \"strategy\": node.strategy,\n                \"reasoning\": node.reasoning,\n                \"routing_from\": node.routing_from,\n                \"routing_to\": node.routing_to\n            }\n\n        # Store in accumulated runs\n        self._accumulated_runs.append(run_data)\n\n        # Reset internal state for fresh execution\n        self._reset_for_fresh_execution()\n\n        if self.use_rich:\n            self.console.print(f\"\u2705 Run '{run_name}' flushed and stored\", style=\"green bold\")\n            self.console.print(f\"\ud83d\udcca Total accumulated runs: {len(self._accumulated_runs)}\", style=\"blue\")\n        else:\n            print(f\"\u2705 Run '{run_name}' flushed and stored\")\n            print(f\"\ud83d\udcca Total accumulated runs: {len(self._accumulated_runs)}\")\n\n        return run_data\n\n    except Exception as e:\n        error_msg = f\"\u274c Error during flush: {e}\"\n        if self.use_rich:\n            self.console.print(error_msg, style=\"red bold\")\n        else:\n            print(error_msg)\n\n        # Still try to reset for fresh execution\n        self._reset_for_fresh_execution()\n\n        return {\"error\": str(e), \"timestamp\": current_time}\n</code></pre> <code>get_accumulated_summary()</code> \u00b6 <p>Get comprehensive summary of all accumulated runs</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def get_accumulated_summary(self) -&gt; Dict[str, Any]:\n    \"\"\"Get comprehensive summary of all accumulated runs\"\"\"\n    try:\n        if not self._accumulated_runs:\n            return {\n                \"total_runs\": 0,\n                \"message\": \"No runs have been flushed yet\"\n            }\n\n        # Calculate aggregate metrics\n        total_cost = 0.0\n        total_tokens = 0\n        total_events = 0\n        total_errors = 0\n        total_nodes = 0\n        total_duration = 0.0\n\n        run_summaries = []\n\n        for run in self._accumulated_runs:\n            summary = run[\"execution_summary\"]\n            perf = summary[\"performance_metrics\"]\n            timing = summary[\"timing\"]\n            session_info = summary[\"session_info\"]\n\n            total_cost += perf[\"total_cost\"]\n            total_tokens += perf[\"total_tokens\"]\n            total_events += perf[\"total_events\"]\n            total_errors += perf[\"error_count\"]\n            total_nodes += session_info[\"total_nodes\"]\n            total_duration += timing[\"elapsed\"]\n\n            run_summaries.append({\n                \"run_id\": run[\"run_id\"],\n                \"run_name\": run[\"run_name\"],\n                \"nodes\": session_info[\"total_nodes\"],\n                \"completed\": session_info[\"completed_nodes\"],\n                \"failed\": session_info[\"failed_nodes\"],\n                \"duration\": timing[\"elapsed\"],\n                \"cost\": perf[\"total_cost\"],\n                \"tokens\": perf[\"total_tokens\"],\n                \"errors\": perf[\"error_count\"],\n                \"health_score\": summary[\"health_indicators\"][\"overall_health\"]\n            })\n\n        # Calculate averages\n        num_runs = len(self._accumulated_runs)\n        avg_duration = total_duration / num_runs\n        avg_cost = total_cost / num_runs\n        avg_tokens = total_tokens / num_runs\n        avg_nodes = total_nodes / num_runs\n\n        return {\n            \"total_runs\": num_runs,\n            \"current_run_id\": self._current_run_id,\n            \"global_start_time\": self._global_start_time,\n            \"total_accumulated_time\": time.time() - self._global_start_time,\n\n            \"aggregate_metrics\": {\n                \"total_cost\": total_cost,\n                \"total_tokens\": total_tokens,\n                \"total_events\": total_events,\n                \"total_errors\": total_errors,\n                \"total_nodes\": total_nodes,\n                \"total_duration\": total_duration,\n            },\n\n            \"average_metrics\": {\n                \"avg_duration\": avg_duration,\n                \"avg_cost\": avg_cost,\n                \"avg_tokens\": avg_tokens,\n                \"avg_nodes\": avg_nodes,\n                \"avg_error_rate\": total_errors / max(total_events, 1),\n                \"avg_health_score\": sum(r[\"health_score\"] for r in run_summaries) / num_runs\n            },\n\n            \"run_summaries\": run_summaries,\n\n            \"performance_insights\": self._generate_accumulated_insights(run_summaries)\n        }\n\n    except Exception as e:\n        return {\"error\": f\"Error generating accumulated summary: {e}\"}\n</code></pre> <code>get_execution_log()</code> \u00b6 <p>Get complete execution log for analysis</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def get_execution_log(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get complete execution log for analysis\"\"\"\n    return self.print_history.copy()\n</code></pre> <code>pretty_print_task_plan(task_plan)</code> \u00b6 <p>Pretty print a Any with full details and structure</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def pretty_print_task_plan(self, task_plan: Any):\n    \"\"\"Pretty print a Any with full details and structure\"\"\"\n    try:\n        if self._fallback_mode or not self.use_rich:\n            self._print_task_plan_fallback(task_plan)\n            return\n\n        # Create main header\n        self.console.print()\n        header_text = f\"\ud83d\udccb Task Plan: {task_plan.name}\\n\"\n        header_text += f\"Status: {task_plan.status.upper()} | Strategy: {task_plan.execution_strategy}\\n\"\n        header_text += f\"Created: {task_plan.created_at.strftime('%Y-%m-%d %H:%M:%S')} | Tasks: {len(task_plan.tasks)}\"\n\n        header = Panel(\n            header_text,\n            title=\"\ud83d\ude80 Task Plan Overview\",\n            style=\"cyan bold\",\n            box=box.ROUNDED\n        )\n        self.console.print(header)\n\n        # Description panel\n        if task_plan.description:\n            desc_panel = Panel(\n                task_plan.description,\n                title=\"\ud83d\udcdd Description\",\n                style=\"blue\",\n                box=box.ROUNDED\n            )\n            self.console.print(desc_panel)\n\n        # Create task tree\n        tree = Tree(f\"\ud83d\udd17 Task Execution Flow ({len(task_plan.tasks)} tasks)\", style=\"bold green\")\n\n        # Group tasks by type for better organization\n        task_groups = {}\n        for task in task_plan.tasks:\n            task_type = task.type if hasattr(task, 'type') else type(task).__name__\n            if task_type not in task_groups:\n                task_groups[task_type] = []\n            task_groups[task_type].append(task)\n\n        # Add tasks organized by dependencies and priority\n        sorted_tasks = sorted(task_plan.tasks, key=lambda t: (t.priority, t.id))\n\n        for i, task in enumerate(sorted_tasks):\n            # Task status icon\n            status_icon = self._get_task_status_icon(task)\n            task_type = task.type if hasattr(task, 'type') else type(task).__name__\n\n            # Main task info\n            task_text = f\"{status_icon} [{i + 1}] {task.id}\"\n            if task.priority != 1:\n                task_text += f\" (Priority: {task.priority})\"\n\n            task_style = self._get_task_status_color(task)\n            task_branch = tree.add(task_text, style=task_style)\n\n            # Add task details based on verbosity mode\n            if self.mode == VerbosityMode.MINIMAL:\n                # Only show basic info\n                task_branch.add(f\"\ud83d\udcc4 {task.description[:80]}...\", style=\"dim\")\n            else:\n                # Show full details\n                self._add_task_details(task_branch, task)\n\n        self.console.print(tree)\n\n        # Add metadata if available\n        if task_plan.metadata and self.mode in [VerbosityMode.VERBOSE, VerbosityMode.DEBUG]:\n            self._print_task_plan_metadata(task_plan)\n\n        # Add dependency analysis\n        if self.mode in [VerbosityMode.VERBOSE, VerbosityMode.DEBUG]:\n            self._print_dependency_analysis(task_plan)\n\n    except Exception as e:\n        self.console.print(f\"\u274c Error printing task plan: {e}\", style=\"red bold\")\n        self._print_task_plan_fallback(task_plan)\n</code></pre> <code>print_accumulated_summary()</code> \u00b6 <p>Print comprehensive summary of all accumulated runs</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def print_accumulated_summary(self):\n    \"\"\"Print comprehensive summary of all accumulated runs\"\"\"\n    try:\n        summary = self.get_accumulated_summary()\n\n        if summary.get(\"total_runs\", 0) == 0:\n            if self.use_rich:\n                self.console.print(\"\ud83d\udcca No accumulated runs to display\", style=\"yellow\")\n            else:\n                print(\"\ud83d\udcca No accumulated runs to display\")\n            return\n\n        if not self.use_rich:\n            self._print_accumulated_summary_fallback(summary)\n            return\n\n        # Rich formatted output\n        self.console.print()\n        self.console.print(\"\ud83d\uddc2\ufe0f [bold cyan]ACCUMULATED EXECUTION SUMMARY[/bold cyan] \ud83d\uddc2\ufe0f\")\n\n        # Overview table\n        overview_table = Table(title=\"\ud83d\udcca Aggregate Overview\", box=box.ROUNDED)\n        overview_table.add_column(\"Metric\", style=\"cyan\", min_width=20)\n        overview_table.add_column(\"Value\", style=\"green\", min_width=15)\n        overview_table.add_column(\"Average\", style=\"blue\", min_width=15)\n\n        agg = summary[\"aggregate_metrics\"]\n        avg = summary[\"average_metrics\"]\n\n        overview_table.add_row(\"Total Runs\", str(summary[\"total_runs\"]), \"\")\n        overview_table.add_row(\"Total Duration\", f\"{agg['total_duration']:.1f}s\", f\"{avg['avg_duration']:.1f}s\")\n        overview_table.add_row(\"Total Nodes\", str(agg[\"total_nodes\"]), f\"{avg['avg_nodes']:.1f}\")\n        overview_table.add_row(\"Total Events\", str(agg[\"total_events\"]), \"\")\n\n        if agg[\"total_cost\"] &gt; 0:\n            overview_table.add_row(\"Total Cost\", self._format_cost(agg[\"total_cost\"]),\n                                   self._format_cost(avg[\"avg_cost\"]))\n\n        if agg[\"total_tokens\"] &gt; 0:\n            overview_table.add_row(\"Total Tokens\", f\"{agg['total_tokens']:,}\",\n                                   f\"{avg['avg_tokens']:,.0f}\")\n\n        overview_table.add_row(\"Error Rate\", f\"{avg['avg_error_rate']:.1%}\", \"\")\n        overview_table.add_row(\"Health Score\", f\"{avg['avg_health_score']:.1%}\", \"\")\n\n        self.console.print(overview_table)\n\n        # Individual runs table\n        runs_table = Table(title=\"\ud83c\udfc3 Individual Runs\", box=box.ROUNDED)\n        runs_table.add_column(\"Run\", style=\"cyan\")\n        runs_table.add_column(\"Duration\", style=\"blue\")\n        runs_table.add_column(\"Nodes\", style=\"green\")\n        runs_table.add_column(\"Success\", style=\"green\")\n        runs_table.add_column(\"Cost\", style=\"yellow\")\n        runs_table.add_column(\"Health\", style=\"magenta\")\n\n        for run in summary[\"run_summaries\"]:\n            success_rate = run[\"completed\"] / max(run[\"nodes\"], 1)\n            cost_str = self._format_cost(run[\"cost\"]) if run[\"cost\"] &gt; 0 else \"-\"\n\n            runs_table.add_row(\n                run[\"run_name\"],\n                f\"{run['duration']:.1f}s\",\n                f\"{run['completed']}/{run['nodes']}\",\n                f\"{success_rate:.1%}\",\n                cost_str,\n                f\"{run['health_score']:.1%}\"\n            )\n\n        self.console.print(runs_table)\n\n        # Insights\n        if summary.get(\"performance_insights\"):\n            insights_panel = Panel(\n                \"\\n\".join(f\"\u2022 {insight}\" for insight in summary[\"performance_insights\"]),\n                title=\"\ud83d\udd0d Performance Insights\",\n                style=\"yellow\"\n            )\n            self.console.print(insights_panel)\n\n    except Exception as e:\n        error_msg = f\"\u274c Error printing accumulated summary: {e}\"\n        if self.use_rich:\n            self.console.print(error_msg, style=\"red bold\")\n        else:\n            print(error_msg)\n</code></pre> <code>print_final_summary()</code> \u00b6 <p>Print comprehensive final summary</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def print_final_summary(self):\n    \"\"\"Print comprehensive final summary\"\"\"\n    try:\n        if self._fallback_mode:\n            self._print_final_summary_fallback()\n            return\n\n        if not self.use_rich:\n            self._print_final_summary_fallback()\n            return\n\n        summary = self.tree_builder.get_execution_summary()\n\n        # Final completion message\n        self.console.print()\n        self.console.print(\"\ud83c\udf89 [bold green]EXECUTION COMPLETED[/bold green] \ud83c\udf89\")\n\n        # Final execution tree\n        final_tree = self._create_execution_tree()\n        self.console.print(final_tree)\n\n        # Comprehensive summary table\n        self._print_final_summary_table(summary)\n\n        # Performance analysis\n        if self.mode in [VerbosityMode.VERBOSE, VerbosityMode.DEBUG]:\n            self._print_performance_analysis(summary)\n\n    except Exception as e:\n        print(f\"\u26a0\ufe0f  Error printing final summary: {e}\")\n        self._print_final_summary_fallback()\n</code></pre> <code>print_plan_from_event(event)</code> \u00b6 <p>Convenience method to print plan from event metadata</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def print_plan_from_event(self, event: ProgressEvent):\n    \"\"\"Convenience method to print plan from event metadata\"\"\"\n    try:\n        if not event.metadata or 'full_plan' not in event.metadata:\n            return\n\n        plan = event.metadata['full_plan']\n        self.pretty_print_task_plan(plan)\n\n    except Exception as e:\n        if self.mode == VerbosityMode.DEBUG:\n            print(f\"\u26a0\ufe0f Error printing plan from event: {e}\")\n</code></pre> <code>print_strategy_from_event(event)</code> \u00b6 <p>Convenience method to print strategy from event metadata</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def print_strategy_from_event(self, event: ProgressEvent):\n    \"\"\"Convenience method to print strategy from event metadata\"\"\"\n    try:\n        if not event.metadata or 'strategy' not in event.metadata:\n            return\n\n        strategy = event.metadata['strategy']\n        context = {\n            'reasoning': event.metadata.get('reasoning'),\n            'complexity_score': event.metadata.get('complexity_score'),\n            'estimated_steps': event.metadata.get('estimated_steps')\n        }\n\n        self.print_strategy_selection(strategy, event, context)\n\n    except Exception as e:\n        if self.mode == VerbosityMode.DEBUG:\n            print(f\"\u26a0\ufe0f Error printing strategy from event: {e}\")\n</code></pre> <code>print_strategy_selection(strategy, event=None, context=None)</code> \u00b6 <p>Print strategy selection information with descriptions based on verbosity mode</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def print_strategy_selection(self, strategy: str, event: ProgressEvent = None, context: Dict[str, Any] = None):\n    \"\"\"Print strategy selection information with descriptions based on verbosity mode\"\"\"\n\n    # Strategy descriptions mapping\n    strategy_descriptions = {\n        \"direct_response\": \"Simple LLM flow with optional tool calls\",\n        \"fast_simple_planning\": \"Simple multi-step plan with tool orchestration\",\n        \"slow_complex_planning\": \"Complex task breakdown with tool orchestration, use for tasks with more than 2 'and' words\",\n        \"research_and_analyze\": \"Information gathering with variable integration\",\n        \"creative_generation\": \"Content creation with personalization\",\n        \"problem_solving\": \"Analysis with tool validation\"\n    }\n\n    strategy_icons = {\n        \"direct_response\": \"\ud83d\udcac\",\n        \"fast_simple_planning\": \"\u26a1\",\n        \"slow_complex_planning\": \"\ud83d\udd04\",\n        \"research_and_analyze\": \"\ud83d\udd0d\",\n        \"creative_generation\": \"\ud83c\udfa8\",\n        \"problem_solving\": \"\ud83e\udde9\"\n    }\n\n    try:\n        if self._fallback_mode or not self.use_rich:\n            self._print_strategy_fallback(strategy, strategy_descriptions, strategy_icons)\n            return\n\n        # Get strategy info\n        icon = strategy_icons.get(strategy, \"\ud83c\udfaf\")+\" \"+self.agent_name\n        description = strategy_descriptions.get(strategy, \"Unknown strategy\")\n\n        # Format based on verbosity mode\n        if self.mode == VerbosityMode.MINIMAL:\n            # Just show strategy name\n            strategy_text = f\"{icon} Strategy: {strategy}\"\n            self.console.print(strategy_text, style=\"cyan\")\n\n        elif self.mode == VerbosityMode.STANDARD:\n            # Show strategy with description\n            strategy_text = f\"{icon} Strategy selected: [bold]{strategy}[/bold]\\n\ud83d\udcdd {description}\"\n            strategy_panel = Panel(\n                strategy_text,\n                title=\"\ud83c\udfaf Execution Strategy\",\n                style=\"cyan\",\n                box=box.ROUNDED\n            )\n            self.console.print(strategy_panel)\n\n        elif self.mode in [VerbosityMode.VERBOSE, VerbosityMode.DEBUG]:\n            # Full details with context\n            strategy_content = [\n                f\"{icon} Strategy: [bold cyan]{strategy}[/bold cyan]\",\n                f\"\ud83d\udcdd Description: {description}\"\n            ]\n\n            # Add context information if available\n            if context:\n                if context.get(\"reasoning\"):\n                    strategy_content.append(f\"\ud83e\udde0 Reasoning: {context['reasoning']}\")\n                if context.get(\"complexity_score\"):\n                    strategy_content.append(f\"\ud83d\udcca Complexity: {context['complexity_score']}\")\n                if context.get(\"estimated_steps\"):\n                    strategy_content.append(f\"\ud83d\udccb Est. Steps: {context['estimated_steps']}\")\n\n            # Add event context in debug mode\n            if self.mode == VerbosityMode.DEBUG and event:\n                strategy_content.append(\n                    f\"\u23f1\ufe0f Selected at: {datetime.fromtimestamp(event.timestamp).strftime('%H:%M:%S')}\")\n                if event.node_name:\n                    strategy_content.append(f\"\ud83d\udccd Node: {event.node_name}\")\n\n            strategy_panel = Panel(\n                \"\\n\".join(strategy_content),\n                title=\"\ud83c\udfaf Strategy Selection Details\",\n                style=\"cyan bold\",\n                box=box.DOUBLE\n            )\n            self.console.print()\n            self.console.print(strategy_panel)\n\n        elif self.mode == VerbosityMode.REALTIME:\n            # Minimal output for realtime mode\n            if not self.realtime_minimal:\n                strategy_text = f\"\\n{icon} Strategy: {strategy} - {description}\"\n                self.console.print(strategy_text, style=\"cyan dim\")\n\n    except Exception as e:\n        # Fallback on error\n        self._consecutive_errors += 1\n        if self._consecutive_errors &lt;= self._error_threshold:\n            print(f\"\u26a0\ufe0f Strategy print error: {e}\")\n        self._print_strategy_fallback(strategy, strategy_descriptions, strategy_icons)\n</code></pre> <code>progress_callback(event)</code> <code>async</code> \u00b6 <p>Main progress callback with minimal realtime support</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>async def progress_callback(self, event: ProgressEvent):\n    \"\"\"Main progress callback with minimal realtime support\"\"\"\n    try:\n        # Add event to tree builder\n        self.tree_builder.add_event(event)\n\n        # Store in history with size limit\n        self.print_history.append({\n            \"timestamp\": event.timestamp,\n            \"event_type\": event.event_type,\n            \"node_name\": event.node_name,\n            \"event_id\": event.event_id\n        })\n\n        # Maintain history size limit\n        if len(self.print_history) &gt; self.max_history:\n            self.print_history = self.print_history[-self.max_history:]\n\n        # Check if we need to show full tree (errors or completion)\n        if self.realtime_minimal:\n            # Check for errors\n            if (event.event_type == \"error\" or\n                event.success is False or\n                (event.metadata and event.metadata.get(\"error\"))):\n                self._needs_full_tree = True\n\n            # Check for completion\n            if (event.event_type in [\"execution_complete\", \"task_complete\", \"node_exit\"] or\n                (event.node_name in self.tree_builder.nodes and\n                 self.tree_builder.nodes[event.node_name].is_completed())):\n                # Check if this is final completion\n                summary = self.tree_builder.get_execution_summary()\n                if (summary[\"session_info\"][\"completed_nodes\"] + summary[\"session_info\"][\"failed_nodes\"] ==\n                    summary[\"session_info\"][\"total_nodes\"]):\n                    self._needs_full_tree = True\n\n        # Print debug info in debug mode\n        if self.mode == VerbosityMode.DEBUG:\n            self._print_debug_event(event)\n\n        # Decide whether to print update\n        if event.node_name == \"FlowAgent\" or self._should_print_update():\n            self.print_strategy_from_event(event)\n            self.print_plan_from_event(event)\n            self.agent_name = event.agent_name if event.agent_name else event.metadata.get(\"agent_name\", self.agent_name)\n            self._print_tree_update()\n\n    except Exception as e:\n        # Emergency error handling\n        self._consecutive_errors += 1\n        print(f\"\u26a0\ufe0f  Progress callback error #{self._consecutive_errors}: {e}\")\n\n        if self._consecutive_errors &gt; self._error_threshold:\n            print(\"\ud83d\udea8 Progress printing disabled due to excessive errors\")\n            # Disable further callbacks\n            self.progress_callback = self._noop_callback\n</code></pre> <code>reset_global_start_time()</code> \u00b6 <p>Reset global start time for new session</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>def reset_global_start_time(self):\n    \"\"\"Reset global start time for new session\"\"\"\n    self._global_start_time = time.time()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.Task","title":"<code>Task</code>  <code>dataclass</code>","text":"Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass Task:\n    id: str\n    type: str\n    description: str\n    status: str = \"pending\"  # pending, running, completed, failed, paused\n    priority: int = 1\n    dependencies: List[str] = field(default_factory=list)\n    subtasks: List[str] = field(default_factory=list)\n    result: Any = None\n    error: str = None\n    created_at: datetime = field(default_factory=datetime.now)\n    started_at: Optional[datetime] = None\n    completed_at: Optional[datetime] = None\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    retry_count: int = 0\n    max_retries: int = 3\n    critical: bool = False\n\n\n    def __post_init__(self):\n        \"\"\"Ensure all mutable defaults are properly initialized\"\"\"\n        if self.metadata is None:\n            self.metadata = {}\n        if self.dependencies is None:\n            self.dependencies = []\n        if self.subtasks is None:\n            self.subtasks = []\n\n    def __getitem__(self, key):\n        return getattr(self, key)\n\n    def __setitem__(self, key, value):\n        setattr(self, key, value)\n</code></pre> <code>__post_init__()</code> \u00b6 <p>Ensure all mutable defaults are properly initialized</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Ensure all mutable defaults are properly initialized\"\"\"\n    if self.metadata is None:\n        self.metadata = {}\n    if self.dependencies is None:\n        self.dependencies = []\n    if self.subtasks is None:\n        self.subtasks = []\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.ToolAnalysis","title":"<code>ToolAnalysis</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines the structure for a valid tool analysis.</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>class ToolAnalysis(BaseModel):\n    \"\"\"Defines the structure for a valid tool analysis.\"\"\"\n    primary_function: str = Field(..., description=\"The main purpose of the tool.\")\n    use_cases: List[str] = Field(..., description=\"Specific use cases for the tool.\")\n    trigger_phrases: List[str] = Field(..., description=\"Phrases that should trigger the tool.\")\n    indirect_connections: List[str] = Field(..., description=\"Non-obvious connections or applications.\")\n    complexity_scenarios: List[str] = Field(..., description=\"Complex scenarios where the tool can be applied.\")\n    user_intent_categories: List[str] = Field(..., description=\"Categories of user intent the tool addresses.\")\n    confidence_triggers: Dict[str, float] = Field(..., description=\"Phrases mapped to confidence scores.\")\n    tool_complexity: str = Field(..., description=\"The complexity of the tool, rated as low, medium, or high.\")\n    args_schema: Optional[Dict[str, Any]] = Field(..., description=\"The schema for the tool's arguments.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.ToolTask","title":"<code>ToolTask</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Task</code></p> <p>Spezialisierter Task f\u00fcr Tool-Aufrufe</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>@dataclass\nclass ToolTask(Task):\n    \"\"\"Spezialisierter Task f\u00fcr Tool-Aufrufe\"\"\"\n    tool_name: str = \"\"\n    arguments: Dict[str, Any] = field(default_factory=dict)  # Kann {{ }} Referenzen enthalten\n    hypothesis: str = \"\"  # Was erwarten wir von diesem Tool?\n    validation_criteria: str = \"\"  # Wie validieren wir das Ergebnis?\n    expectation: str = \"\"  # Wie sollte das Ergebnis aussehen?\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.create_complex_scenario","title":"<code>create_complex_scenario()</code>  <code>async</code>","text":"<p>Create a complex scenario with multiple nodes and error recovery</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>async def create_complex_scenario():\n    \"\"\"Create a complex scenario with multiple nodes and error recovery\"\"\"\n    base_time = time.time()\n    events = []\n\n    nodes = [\n        \"FlowAgent\",\n        \"StrategyOrchestratorNode\",\n        \"TaskPlannerFlow\",\n        \"ResearchNode\",\n        \"AnalysisNode\",\n        \"ValidationNode\",\n        \"ResponseGeneratorNode\"\n    ]\n\n    # Start execution\n    events.append(ProgressEvent(\n        event_type=\"execution_start\",\n        timestamp=base_time,\n        node_name=\"FlowAgent\",\n        session_id=f\"complex_session_{int(base_time)}\",\n        metadata={\"complexity\": \"high\", \"estimated_duration\": 25}\n    ))\n\n    current_time = base_time\n\n    for i, node in enumerate(nodes[1:], 1):\n        # Node entry\n        current_time += 0.5\n        events.append(ProgressEvent(\n            event_type=\"node_enter\",\n            timestamp=current_time,\n            node_name=node\n        ))\n\n        # Main operation (LLM or tool call)\n        current_time += 1.2\n        if i % 3 == 0:  # Tool call\n            success = i != 5  # Fail on ValidationNode\n            events.append(ProgressEvent(\n                event_type=\"tool_call\",\n                timestamp=current_time,\n                node_name=node,\n                tool_name=f\"tool_{i}\",\n                tool_duration=1.8,\n                tool_success=success,\n                tool_result=f\"Tool result {i}\" if success else None,\n                tool_error=f\"Tool error {i}\" if not success else None,\n                success=success,\n                metadata={\"error\": \"Validation failed\", \"error_type\": \"ValidationError\"} if not success else {}\n            ))\n\n            # Recovery if failed\n            if not success:\n                current_time += 2.0\n                events.append(ProgressEvent(\n                    event_type=\"tool_call\",\n                    timestamp=current_time,\n                    node_name=node,\n                    tool_name=\"recovery_tool\",\n                    tool_duration=1.5,\n                    tool_success=True,\n                    tool_result=\"Recovery successful\"\n                ))\n        else:  # LLM call\n            events.append(ProgressEvent(\n                event_type=\"llm_call\",\n                timestamp=current_time,\n                node_name=node,\n                llm_model=\"gpt-4\" if i % 2 == 0 else \"gpt-3.5-turbo\",\n                llm_total_tokens=1200 + i * 200,\n                llm_cost=0.024 + i * 0.005,\n                llm_duration=1.5 + i * 0.3,\n                success=True\n            ))\n\n        # Node completion\n        current_time += 0.8\n        if node.endswith(\"Node\"):  # Simple nodes auto-complete\n            events.append(ProgressEvent(\n                event_type=\"node_phase\",\n                timestamp=current_time,\n                node_name=node,\n                success=True,\n                node_duration=current_time - (base_time + i * 2.5)\n            ))\n\n    # Final completion\n    events.append(ProgressEvent(\n        event_type=\"execution_complete\",\n        timestamp=current_time + 1.0,\n        node_name=\"FlowAgent\",\n        node_duration=current_time + 1.0 - base_time,\n        status=NodeStatus.COMPLETED,\n        success=True,\n        metadata={\"total_cost\": 0.156, \"total_tokens\": 12500}\n    ))\n\n    return events\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.create_demo_scenario","title":"<code>create_demo_scenario(run_name='Demo Run', duration=10.0, cost=0.025, should_fail=False)</code>  <code>async</code>","text":"<p>Create a demo scenario with configurable parameters</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>async def create_demo_scenario(run_name=\"Demo Run\", duration=10.0, cost=0.025, should_fail=False):\n    \"\"\"Create a demo scenario with configurable parameters\"\"\"\n    base_time = time.time()\n    events = []\n\n    # Execution start\n    events.append(ProgressEvent(\n        event_type=\"execution_start\",\n        timestamp=base_time,\n        node_name=\"FlowAgent\",\n        session_id=f\"demo_session_{int(base_time)}\",\n        metadata={\"query\": f\"Execute {run_name}\", \"user_id\": \"demo_user\"}\n    ))\n\n    # Strategy orchestrator\n    events.append(ProgressEvent(\n        event_type=\"node_enter\",\n        timestamp=base_time + 0.1,\n        node_name=\"StrategyOrchestratorNode\"\n    ))\n\n    events.append(ProgressEvent(\n        event_type=\"llm_call\",\n        timestamp=base_time + 1.2,\n        node_name=\"StrategyOrchestratorNode\",\n        llm_model=\"gpt-4\",\n        llm_total_tokens=1200,\n        llm_cost=cost * 0.4,\n        llm_duration=1.1,\n        success=True,\n        metadata={\"strategy\": \"research_and_analyze\"}\n    ))\n\n    # Planning\n    events.append(ProgressEvent(\n        event_type=\"node_enter\",\n        timestamp=base_time + 2.5,\n        node_name=\"PlannerNode\"\n    ))\n\n    events.append(ProgressEvent(\n        event_type=\"llm_call\",\n        timestamp=base_time + 3.8,\n        node_name=\"PlannerNode\",\n        llm_model=\"gpt-3.5-turbo\",\n        llm_total_tokens=800,\n        llm_cost=cost * 0.2,\n        llm_duration=1.3,\n        success=True\n    ))\n    # TaskPlan\n    events.append(ProgressEvent(\n        event_type=\"plan_created\",\n        timestamp=base_time + 4.0,\n        node_name=\"PlannerNode\",\n        status=NodeStatus.COMPLETED,\n        success=True,\n        metadata={\"plan_name\": \"Demo Plan\", \"task_count\": 3, \"full_plan\": TaskPlan(id='bf5053ad-1eae-4dd2-9c08-0c7fab49f80d', name='File Cleanup Task', description='Remove turtle_on_bike.py and execution_summary.json if they exist', tasks=[LLMTask(id='analyze_files', type='LLMTask', description='Analyze the current directory for turtle_on_bike.py and execution_summary.json', status='pending', priority=1, dependencies=[], subtasks=[], result=None, error=None, created_at=datetime(2025, 8, 13, 23, 51, 38, 726320), started_at=None, completed_at=None, metadata={}),ToolTask(id='remove_files', type='ToolTask', description='Delete turtle_on_bike.py and execution_summary.json using shell command', status='pending', priority=1, dependencies=[], subtasks=[], result=None, error=None, created_at=datetime(2025, 8, 13, 23, 51, 38, 726320), started_at=None, completed_at=None, metadata={}, retry_count=0, max_retries=3, critical=False, tool_name='shell', arguments={'command': \"Remove-Item -Path 'turtle_on_bike.py', 'execution_summary.json' -ErrorAction SilentlyContinue\"}, hypothesis='', validation_criteria='', expectation='')], status='created', created_at=datetime(2025, 8, 13, 23, 51, 38, 726320), metadata={}, execution_strategy='sequential')}\n    ))\n\n    # Execution with tools\n    events.append(ProgressEvent(\n        event_type=\"node_enter\",\n        timestamp=base_time + 5.0,\n        node_name=\"ExecutorNode\"\n    ))\n\n    events.append(ProgressEvent(\n        event_type=\"tool_call\",\n        timestamp=base_time + 6.2,\n        node_name=\"ExecutorNode\",\n        tool_name=\"web_search\",\n        tool_duration=2.1,\n        tool_success=not should_fail,\n        tool_result=\"Search completed\" if not should_fail else None,\n        tool_error=\"Search failed\" if should_fail else None,\n        success=not should_fail,\n        metadata={\"error\": \"Search API timeout\"} if should_fail else {}\n    ))\n\n    if not should_fail:\n        # Analysis\n        events.append(ProgressEvent(\n            event_type=\"llm_call\",\n            timestamp=base_time + 8.5,\n            node_name=\"AnalysisNode\",\n            llm_model=\"gpt-4\",\n            llm_total_tokens=1500,\n            llm_cost=cost * 0.4,\n            llm_duration=2.3,\n            success=True\n        ))\n\n        # Completion\n        events.append(ProgressEvent(\n            event_type=\"execution_complete\",\n            timestamp=base_time + duration,\n            node_name=\"FlowAgent\",\n            node_duration=duration,\n            status=NodeStatus.COMPLETED,\n            success=True,\n            metadata={\"result\": \"Successfully completed\"}\n        ))\n    else:\n        # Failed completion\n        events.append(ProgressEvent(\n            event_type=\"error\",\n            timestamp=base_time + duration * 0.7,\n            node_name=\"ExecutorNode\",\n            status=NodeStatus.FAILED,\n            success=False,\n            metadata={\n                \"error\": \"Execution failed due to tool error\",\n                \"error_type\": \"ToolError\"\n            }\n        ))\n\n    return events\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.create_task","title":"<code>create_task(task_type, **kwargs)</code>","text":"<p>Factory f\u00fcr Task-Erstellung mit korrektem Typ</p> Source code in <code>toolboxv2/mods/isaa/base/Agent/types.py</code> <pre><code>def create_task(task_type: str, **kwargs) -&gt; Task:\n    \"\"\"Factory f\u00fcr Task-Erstellung mit korrektem Typ\"\"\"\n    task_classes = {\n        \"llm_call\": LLMTask,\n        \"tool_call\": ToolTask,\n        \"decision\": DecisionTask,\n        \"compound\": CompoundTask,\n        \"generic\": Task,\n        \"LLMTask\": LLMTask,\n        \"ToolTask\": ToolTask,\n        \"DecisionTask\": DecisionTask,\n        \"CompoundTask\": CompoundTask,\n        \"Task\": Task,\n    }\n\n    task_class = task_classes.get(task_type, Task)\n\n    # Standard-Felder setzen\n    if \"id\" not in kwargs:\n        kwargs[\"id\"] = str(uuid.uuid4())\n    if \"type\" not in kwargs:\n        kwargs[\"type\"] = task_type\n    if \"critical\" not in kwargs:\n        kwargs[\"critical\"] = task_type in [\"llm_call\", \"decision\"]\n\n    # Ensure metadata is initialized\n    if \"metadata\" not in kwargs:\n        kwargs[\"metadata\"] = {}\n\n    # Create task and ensure post_init is called\n    task = task_class(**kwargs)\n\n    # Double-check metadata initialization\n    if not hasattr(task, 'metadata') or task.metadata is None:\n        task.metadata = {}\n\n    return task\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.demo_accumulated_runs","title":"<code>demo_accumulated_runs()</code>  <code>async</code>","text":"<p>Demo accumulated runs functionality</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>async def demo_accumulated_runs():\n    \"\"\"Demo accumulated runs functionality\"\"\"\n    print(\"\\n\ud83d\udcca ACCUMULATED RUNS DEMONSTRATION\")\n    print(\"=\" * 50)\n    print(\"This demo shows how multiple execution runs are accumulated and analyzed\")\n\n    printer = ProgressiveTreePrinter(mode=VerbosityMode.STANDARD)\n\n    # Simulate 3 different runs\n    runs = [\n        (\"Market Analysis\", \"research_and_analyze\", True, 12.5, 0.045),\n        (\"Content Creation\", \"creative_generation\", True, 8.2, 0.032),\n        (\"Problem Solving\", \"problem_solving\", False, 15.8, 0.067)  # This one fails\n    ]\n\n    for i, (run_name, strategy, success, duration, cost) in enumerate(runs):\n        print(f\"\\n\ud83c\udfc3 Running execution {i + 1}/3: {run_name}\")\n\n        # Strategy selection\n        printer.print_strategy_selection(strategy)\n        await asyncio.sleep(1)\n\n        # Quick execution simulation\n        events = await create_demo_scenario(\n            run_name=run_name,\n            duration=duration,\n            cost=cost,\n            should_fail=not success\n        )\n\n        for event in events:\n            await printer.progress_callback(event)\n            await asyncio.sleep(0.2)  # Fast execution\n\n        # Flush the run\n        printer.flush(run_name)\n        await asyncio.sleep(2)\n\n    # Show accumulated summary\n    print(\"\\n\ud83d\udcc8 ACCUMULATED SUMMARY:\")\n    printer.print_accumulated_summary()\n\n    # Export data\n    if input(\"\\n\ud83d\udcbe Export accumulated data? (y/n): \").lower().startswith('y'):\n        filepath = printer.export_accumulated_data()\n        print(f\"\u2705 Data exported to: {filepath}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.demo_all_modes","title":"<code>demo_all_modes()</code>  <code>async</code>","text":"<p>Demo all verbosity modes with the same scenario</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>async def demo_all_modes():\n    \"\"\"Demo all verbosity modes with the same scenario\"\"\"\n    print(\"\\n\ud83c\udfad ALL MODES DEMONSTRATION\")\n    print(\"=\" * 50)\n    print(\"This demo will run the same scenario in all verbosity modes\")\n    print(\"to show the differences in output detail.\")\n\n    modes = [\n        (VerbosityMode.MINIMAL, \"MINIMAL - Only major updates\"),\n        (VerbosityMode.STANDARD, \"STANDARD - Regular updates with panels\"),\n        (VerbosityMode.VERBOSE, \"VERBOSE - Detailed information with metrics\"),\n        (VerbosityMode.DEBUG, \"DEBUG - Full debugging info with all details\"),\n        (VerbosityMode.REALTIME, \"REALTIME - Live updates (will show final tree)\")\n    ]\n\n    for mode, description in modes:\n        print(f\"\\n{'=' * 60}\")\n        print(f\"\ud83c\udfaf NOW DEMONSTRATING: {description}\")\n        print(f\"{'=' * 60}\")\n\n        await asyncio.sleep(2)\n\n        printer = ProgressiveTreePrinter(mode=mode, realtime_minimal=False)\n\n        # Strategy selection demo\n        printer.print_strategy_selection(\n            \"research_and_analyze\",\n            context={\n                \"reasoning\": \"Complex query requires multi-source research and analysis\",\n                \"complexity_score\": 0.8,\n                \"estimated_steps\": 5\n            }\n        )\n\n        await asyncio.sleep(1)\n\n        # Run scenario\n        events = await create_demo_scenario()\n\n        for event in events:\n            await printer.progress_callback(event)\n            if mode == VerbosityMode.REALTIME:\n                await asyncio.sleep(0.5)\n            else:\n                await asyncio.sleep(0.3)\n\n        # Final summary\n        printer.print_final_summary()\n\n        if mode != modes[-1][0]:  # Not the last mode\n            input(f\"\\n\u23f8\ufe0f  Press Enter to continue to next mode...\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.demo_complete_features","title":"<code>demo_complete_features()</code>  <code>async</code>","text":"<p>Complete feature demonstration</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>async def demo_complete_features():\n    \"\"\"Complete feature demonstration\"\"\"\n    print(\"\\n\ud83d\ude80 COMPLETE FEATURE DEMONSTRATION\")\n    print(\"=\" * 50)\n    print(\"This demo showcases all features in a comprehensive scenario\")\n\n    # Start with verbose mode\n    printer = ProgressiveTreePrinter(mode=VerbosityMode.VERBOSE)\n\n    print(\"\\n1\ufe0f\u20e3 STRATEGY SELECTION SHOWCASE:\")\n    strategies = [\"direct_response\", \"research_and_analyze\", \"problem_solving\"]\n    for strategy in strategies:\n        printer.print_strategy_selection(strategy, context={\n            \"reasoning\": f\"Demonstrating {strategy} strategy selection\",\n            \"complexity_score\": 0.6,\n            \"estimated_steps\": 4\n        })\n        await asyncio.sleep(1)\n\n    print(\"\\n2\ufe0f\u20e3 COMPLEX EXECUTION WITH ERRORS:\")\n    # Complex scenario with multiple nodes, errors, and recovery\n    complex_events = await create_complex_scenario()\n\n    for event in complex_events:\n        await printer.progress_callback(event)\n        await asyncio.sleep(0.4)\n\n    printer.print_final_summary()\n\n    print(\"\\n3\ufe0f\u20e3 MODE COMPARISON:\")\n    print(\"Switching to REALTIME mode for live demo...\")\n    await asyncio.sleep(2)\n\n    # Switch to realtime mode\n    realtime_printer = ProgressiveTreePrinter(\n        mode=VerbosityMode.REALTIME,\n        realtime_minimal=True\n    )\n\n    print(\"Running same scenario in REALTIME minimal mode:\")\n    simple_events = await create_demo_scenario()\n\n    for event in simple_events:\n        await realtime_printer.progress_callback(event)\n        await asyncio.sleep(0.3)\n\n    print(\"\\n\\n4\ufe0f\u20e3 ACCUMULATED ANALYTICS:\")\n    # Flush both runs\n    printer.flush(\"Complex Execution\")\n    realtime_printer.flush(\"Realtime Execution\")\n\n    # Transfer accumulated data to one printer for summary\n    printer._accumulated_runs.extend(realtime_printer._accumulated_runs)\n    printer.print_accumulated_summary()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.demo_enhanced_printer","title":"<code>demo_enhanced_printer()</code>  <code>async</code>","text":"<p>Comprehensive demo of the enhanced progress printer showcasing all modes</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>async def demo_enhanced_printer():\n    \"\"\"Comprehensive demo of the enhanced progress printer showcasing all modes\"\"\"\n    import asyncio\n\n    print(\"\ud83d\ude80 Starting Enhanced Progress Printer Demo...\")\n    print(\"Choose demo type:\")\n    print(\"1. All Modes Demo - Show all verbosity modes with same scenario\")\n    print(\"2. Interactive Mode Selection - Choose specific mode\")\n    print(\"3. Strategy Selection Demo - Show strategy printing\")\n    print(\"4. Accumulated Runs Demo - Show multi-run accumulation\")\n    print(\"5. Complete Feature Demo - All features in sequence\")\n    print(\"6. Exit\")\n\n    try:\n        choice = input(\"Enter choice (1-6) [default: 1]: \").strip() or \"1\"\n    except:\n        choice = \"1\"\n\n    if choice == \"6\":\n        return\n    elif choice == \"1\":\n        await demo_all_modes()\n    elif choice == \"2\":\n        await demo_interactive_mode()\n    elif choice == \"3\":\n        await demo_strategy_selection()\n    elif choice == \"4\":\n        await demo_accumulated_runs()\n    elif choice == \"5\":\n        await demo_complete_features()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.demo_interactive_mode","title":"<code>demo_interactive_mode()</code>  <code>async</code>","text":"<p>Interactive mode selection demo</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>async def demo_interactive_mode():\n    \"\"\"Interactive mode selection demo\"\"\"\n    print(\"\\n\ud83c\udfae INTERACTIVE MODE SELECTION\")\n    print(\"Choose your preferred verbosity mode:\")\n    print(\"1. MINIMAL - Only major updates\")\n    print(\"2. STANDARD - Regular updates\")\n    print(\"3. VERBOSE - Detailed information\")\n    print(\"4. DEBUG - Full debugging info\")\n    print(\"5. REALTIME - Live updates\")\n\n    try:\n        choice = input(\"Enter choice (1-5) [default: 2]: \").strip() or \"2\"\n        modes = {\n            \"1\": VerbosityMode.MINIMAL,\n            \"2\": VerbosityMode.STANDARD,\n            \"3\": VerbosityMode.VERBOSE,\n            \"4\": VerbosityMode.DEBUG,\n            \"5\": VerbosityMode.REALTIME\n        }\n        mode = modes.get(choice, VerbosityMode.STANDARD)\n    except:\n        mode = VerbosityMode.STANDARD\n\n    printer = ProgressiveTreePrinter(mode=mode)\n    print(f\"\\n\ud83c\udfaf Running demo in {mode.value.upper()} mode...\")\n\n    # Strategy selection\n    printer.print_strategy_selection(\"slow_complex_planning\", context={\n        \"reasoning\": \"Task has multiple 'and' conditions requiring complex breakdown\",\n        \"complexity_score\": 0.9,\n        \"estimated_steps\": 8\n    })\n\n    await asyncio.sleep(1)\n\n    events = await create_demo_scenario()\n    for event in events:\n        await printer.progress_callback(event)\n        await asyncio.sleep(0.5 if mode == VerbosityMode.REALTIME else 0.8)\n\n    printer.print_final_summary()\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.extras.terminal_progress.demo_strategy_selection","title":"<code>demo_strategy_selection()</code>  <code>async</code>","text":"<p>Demo all strategy selection options</p> Source code in <code>toolboxv2/mods/isaa/extras/terminal_progress.py</code> <pre><code>async def demo_strategy_selection():\n    \"\"\"Demo all strategy selection options\"\"\"\n    print(\"\\n\ud83c\udfaf STRATEGY SELECTION DEMONSTRATION\")\n    print(\"=\" * 50)\n\n    strategies = [\n        (\"direct_response\", \"Simple question that needs direct answer\"),\n        (\"fast_simple_planning\", \"Task needs quick multi-step approach\"),\n        (\"slow_complex_planning\", \"Complex task with multiple 'and' conditions\"),\n        (\"research_and_analyze\", \"Needs information gathering and analysis\"),\n        (\"creative_generation\", \"Content creation with personalization\"),\n        (\"problem_solving\", \"Analysis with validation required\")\n    ]\n\n    for mode in [VerbosityMode.MINIMAL, VerbosityMode.STANDARD, VerbosityMode.VERBOSE]:\n        print(f\"\\n\ud83d\udd0d Strategy demo in {mode.value.upper()} mode:\")\n        print(\"-\" * 40)\n\n        printer = ProgressiveTreePrinter(mode=mode)\n\n        for strategy, reasoning in strategies:\n            complexity = 0.3 if \"simple\" in strategy else 0.7 if \"complex\" in strategy else 0.5\n\n            printer.print_strategy_selection(\n                strategy,\n                context={\n                    \"reasoning\": reasoning,\n                    \"complexity_score\": complexity,\n                    \"estimated_steps\": 1 if \"direct\" in strategy else 3 if \"fast\" in strategy else 6\n                }\n            )\n            await asyncio.sleep(0.8)\n\n        if mode != VerbosityMode.VERBOSE:\n            input(\"\\n\u23f8\ufe0f  Press Enter for next mode...\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.module","title":"<code>module</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.isaa.module.Tools","title":"<code>Tools</code>","text":"<p>               Bases: <code>MainTool</code>, <code>FileHandler</code></p> Source code in <code>toolboxv2/mods/isaa/module.py</code> <pre><code>class Tools(MainTool, FileHandler):\n\n    def __init__(self, app=None):\n\n        self.run_callback = None\n        # self.coding_projects: dict[str, ProjectManager] = {} # Assuming ProjectManager is defined elsewhere or removed\n        self.pipes: dict[str, Pipeline] = {}\n        if app is None:\n            app = get_app(\"isaa-mod\")\n        self.version = version\n        self.name = \"isaa\"\n        self.Name = \"isaa\"\n        self.color = \"VIOLET2\"\n        self.config = {'controller-init': False,\n                       'agents-name-list': [],\n                       \"DEFAULTMODEL0\": os.getenv(\"DEFAULTMODEL0\", \"ollama/llama3.1\"),\n                       \"DEFAULT_AUDIO_MODEL\": os.getenv(\"DEFAULT_AUDIO_MODEL\", \"groq/whisper-large-v3-turbo\"),\n                       \"DEFAULTMODEL1\": os.getenv(\"DEFAULTMODEL1\", \"ollama/llama3.1\"),\n                       \"DEFAULTMODELST\": os.getenv(\"DEFAULTMODELST\", \"ollama/llama3.1\"),\n                       \"DEFAULTMODEL2\": os.getenv(\"DEFAULTMODEL2\", \"ollama/llama3.1\"),\n                       \"DEFAULTMODELCODE\": os.getenv(\"DEFAULTMODELCODE\", \"ollama/llama3.1\"),\n                       \"DEFAULTMODELSUMMERY\": os.getenv(\"DEFAULTMODELSUMMERY\", \"ollama/llama3.1\"),\n                       \"DEFAULTMODEL_LF_TOOLS\": os.getenv(\"DEFAULTMODEL_LF_TOOLS\", \"ollama/llama3.1\"),\n                       }\n        self.per_data = {}\n        self.agent_data: dict[str, dict] = {}  # Will store AgentConfig dicts\n        self.keys = {\n            \"KEY\": \"key~~~~~~~\",\n            \"Config\": \"config~~~~\"\n        }\n        self.initstate = {}\n\n        extra_path = \"\"\n        if self.toolID:  # MainTool attribute\n            extra_path = f\"/{self.toolID}\"\n        self.observation_term_mem_file = f\"{app.data_dir}/Memory{extra_path}/observationMemory/\"\n        self.config['controller_file'] = f\"{app.data_dir}{extra_path}/controller.json\"\n        self.mas_text_summaries_dict = FileCache(folder=f\"{app.data_dir}/Memory{extra_path}/summaries/\")\n        self.tools = {\n            \"name\": \"isaa\",\n            \"Version\": self.show_version,\n            \"add_task\": self.add_task,\n            \"save_task\": self.save_task,\n            \"load_task\": self.load_task,\n            \"get_task\": self.get_task,\n            \"list_task\": self.list_task,\n            \"mini_task_completion\": self.mini_task_completion,\n            \"run_agent\": self.run_agent,\n            \"save_to_mem\": self.save_to_mem_sync,\n            \"get_agent\": self.get_agent,  # Now async\n            \"run_task\": self.run_task,  # Now async\n            \"create_task_chain\": self.create_task_chain,  # Now async\n            \"format_class\": self.format_class,  # Now async\n            \"get_memory\": self.get_memory,\n            \"get_pipe\": self.get_pipe,  # Now async\n            \"run_pipe\": self.run_pipe,  # Now async\n            \"rget_mode\": lambda mode: self.controller.rget(mode),\n            \"set_local_files_tools\": self.set_local_files_tools,\n        }\n        self.working_directory = os.getenv('ISAA_WORKING_PATH', os.getcwd())\n        self.print_stream = stram_print\n        self.agent_collective_senses = False  # This might be obsolete with FlowAgent's design\n        self.global_stream_override = False  # Handled by FlowAgentBuilder\n        self.pipes_device = 1  # For HuggingFace pipelines\n        self.lang_chain_tools_dict: dict[str, Any] = {}  # Store actual tool objects for wrapping\n        self.agent_chain = AgentChain(directory=f\"{app.data_dir}{extra_path}/chains\")\n        self.agent_chain_executor = ChainTreeExecutor()\n        # These runners will become async due to get_agent being async\n        self.agent_chain_executor.function_runner = self._async_function_runner\n        self.agent_chain_executor.agent_runner = self._async_agent_runner\n\n        self.agent_memory: AISemanticMemory = f\"{app.id}{extra_path}/Memory\"  # Path for AISemanticMemory\n        self.controller = ControllerManager({})\n        self.summarization_mode = 1\n        self.summarization_limiter = 102000\n        self.speak = lambda x, *args, **kwargs: x  # Placeholder\n        self.scripts = Scripts(f\"{app.data_dir}{extra_path}/ScriptFile\")\n        self.ac_task = None  # Unused?\n        self.default_setter = None  # For agent builder customization\n        self.local_files_tools = True  # Related to old FileManagementToolkit\n        self.initialized = False\n\n        self.personality_code = ISAA0CODE  # Constant string\n\n        FileHandler.__init__(self, f\"isaa{extra_path.replace('/', '-')}.config\", app.id if app else __name__)\n        MainTool.__init__(self, load=self.on_start, v=self.version, tool=self.tools,\n                          name=self.name, logs=None, color=self.color, on_exit=self.on_exit)\n\n        self.fc_generators = {}  # Unused?\n        self.toolID = \"\"  # MainTool attribute\n        MainTool.toolID = \"\"  # Static attribute?\n        self.web_search = web_search\n        self.shell_tool_function = shell_tool_function\n        self.tools[\"shell\"] = shell_tool_function\n\n        self.print(f\"Start {self.spec}.isaa\")\n        with Spinner(message=\"Starting module\", symbols='c'):\n            self.load_file_handler()\n            config_fh = self.get_file_handler(self.keys[\"Config\"])\n            if config_fh is not None:\n                if isinstance(config_fh, str):\n                    try:\n                        config_fh = json.loads(config_fh)\n                    except json.JSONDecodeError:\n                        self.print(f\"Warning: Could not parse config from file handler: {config_fh[:100]}...\")\n                        config_fh = {}\n\n                if isinstance(config_fh, dict):\n                    # Merge, prioritizing existing self.config for defaults not in file\n                    loaded_config = config_fh\n                    for key, value in self.config.items():\n                        if key not in loaded_config:\n                            loaded_config[key] = value\n                    self.config = loaded_config\n\n            if self.spec == 'app':  # MainTool attribute\n                self.load_keys_from_env()\n\n            # Ensure directories exist\n            Path(f\"{get_app('isaa-initIsaa').data_dir}/Agents/\").mkdir(parents=True, exist_ok=True)\n            Path(f\"{get_app('isaa-initIsaa').data_dir}/Memory/\").mkdir(parents=True, exist_ok=True)\n\n        #initialize_isaa_chains(self.app)\n        #initialize_isaa_webui_module(self.app, self)\n        #self.print(\"ISAA module started. fallback\")\n\n    async def _async_function_runner(self, name, **kwargs):\n        agent = await self.get_agent(\"self\")  # Get self agent for its tools\n        # FlowAgent doesn't have function_invoke. Need to find tool and run.\n        # This is a simplified version. Real ADK tool execution is more complex.\n        for tool in agent.tools:  # Assuming agent.tools is populated for FlowAgent\n            if tool.name == name:\n                # This is a placeholder. ADK tools expect ToolContext.\n                # For simple FunctionTool, calling tool.func might work.\n                if hasattr(tool, 'func') and callable(tool.func):\n                    if asyncio.iscoroutinefunction(tool.func):\n                        return await tool.func(**kwargs)\n                    else:\n                        return tool.func(**kwargs)\n                else:  # Fallback to trying to run the tool directly if it's a BaseTool instance\n                    # This is highly dependent on the tool's implementation\n                    try:\n                        if hasattr(tool, 'run_async'):\n                            return await tool.run_async(args=kwargs, tool_context=None)\n                        elif hasattr(tool, 'run'):\n                            return tool.run(args=kwargs, tool_context=None)\n                    except Exception as e:\n                        self.print(f\"Error running tool {name} via fallback: {e}\")\n                        return f\"Error running tool {name}\"\n        return f\"Tool {name} not found on self agent.\"\n\n    async def _async_agent_runner(self, name, task, **kwargs):\n        return await self.run_agent(name, task, **kwargs)\n\n    def add_task(self, name, task):\n        return self.agent_chain.add_task(name, task)\n\n    def list_task(self):\n        return str(self.agent_chain)\n\n    def remove_task(self, name:str):\n        return self.agent_chain.remove(name)\n\n    def save_task(self, name:Optional[str]=None):\n        self.agent_chain.save_to_file(name)\n\n    def load_task(self, name:Optional[str]=None):\n        self.agent_chain.load_from_file(name)\n\n    def get_task(self, name:Optional[str]=None):\n        return self.agent_chain.get(name)\n\n    async def run_task(self, task_input: str, chain_name: str, sum_up:bool=True, agent_name:Optional[str]=None):\n        self.agent_chain_executor.reset()\n        if agent_name is None:\n            agent_name = \"self\"\n\n        agent_instance = await self.get_agent(agent_name)\n        self.agent_chain_executor.set_runner( agent_instance.arun_function, agent_instance.a_run)\n\n        return await self.agent_chain_executor.a_execute(task_input,\n                                          self.agent_chain.get(chain_name), sum_up)\n\n    async def create_task_chain(self, prompt: str, agent_name:Optional[str]=None):\n        if prompt is None:\n            return None\n        if prompt == \"\": return None\n        agents_list = self.config.get('agents-name-list', ['self', 'isaa'])\n        # Tools list needs to be adapted for FlowAgent/ADK\n        self_agent = await self.get_agent(\"self\")\n        tools_list_str = \", \".join(\n            [tool.name for tool in self_agent.tools]) if self_agent.tools else \"No tools available\"\n\n        prompt += f\"\\n\\nAvailable Agents: {agents_list}\"\n        prompt += f\"\\n\\nAvailable Tools on 'self' agent: {tools_list_str}\"\n        prompt += f\"\\n\\nAvailable Chains: {self.list_task()}\"\n        agent_name = agent_name or \"TaskChainAgent\"\n        if agent_name not in self.config['agents-name-list']:\n            task_chain_builder = self.get_agent_builder(\"code\")\n            task_chain_builder.with_agent_name(agent_name)\n            tcm = self.controller.rget(TaskChainMode)\n            task_chain_builder.with_system_message(tcm.system_msg)  # Use system_msg from LLMMode\n            await self.register_agent(task_chain_builder)  # await here\n\n        task_chain_dict = await self.format_class(TaskChain, prompt, agent_name=\"TaskChainAgent\")\n        task_chain = TaskChain(**task_chain_dict)\n\n        self.print(f\"New TaskChain {task_chain.name} len:{len(task_chain.tasks)}\")\n\n        if task_chain and len(task_chain.tasks):\n            self.print(f\"adding : {task_chain.name}\")\n            self.agent_chain.add(task_chain.name, task_chain.model_dump().get(\"tasks\"))\n            self.agent_chain.add_discr(task_chain.name, task_chain.description)\n        return task_chain.name\n\n    def get_augment(self, task_name=None, exclude=None):\n        # This needs to be adapted. Serialization of FlowAgent is through AgentConfig.\n        return {\n            \"tools\": {},  # Tool configurations might be part of AgentConfig now\n            \"Agents\": self.serialize_all(exclude=exclude),  # Returns dict of AgentConfig dicts\n            \"customFunctions\": json.dumps(self.scripts.scripts),  # Remains same\n            \"tasks\": self.agent_chain.save_to_dict(task_name)  # Remains same\n        }\n\n    async def init_from_augment(self, augment, agent_name: str = 'self', exclude=None):\n        \"\"\"Initialize from augmented data using new builder system\"\"\"\n\n        # Handle agent_name parameter\n        if isinstance(agent_name, str):\n            pass  # Use string name\n        elif hasattr(agent_name, 'config'):  # FlowAgentBuilder\n            agent_name = agent_name.config.name\n        else:\n            raise ValueError(f\"Invalid agent_name type: {type(agent_name)}\")\n\n        a_keys = augment.keys()\n\n        # Load agent configurations\n        if \"Agents\" in a_keys:\n            agents_configs_dict = augment['Agents']\n            self.deserialize_all(agents_configs_dict)\n            self.print(\"Agent configurations loaded.\")\n\n        # Load custom functions (scripts)\n        if \"customFunctions\" in a_keys:\n            custom_functions = augment['customFunctions']\n            if isinstance(custom_functions, str):\n                custom_functions = json.loads(custom_functions)\n            if custom_functions:\n                self.scripts.scripts = custom_functions\n                self.print(\"Custom functions loaded\")\n\n        # Load task chains\n        if \"tasks\" in a_keys:\n            tasks = augment['tasks']\n            if isinstance(tasks, str):\n                tasks = json.loads(tasks)\n            if tasks:\n                self.agent_chain.load_from_dict(tasks)\n                self.print(\"Task chains restored\")\n\n        # Tools are now handled by the builder system during agent creation\n        if \"tools\" in a_keys:\n            self.print(\"Tool configurations noted - will be applied during agent building\")\n    async def init_tools(self, tools_config: dict, agent_builder: FlowAgentBuilder):\n        # This function needs to be adapted to add tools to the FlowAgentBuilder\n        # For LangChain tools, they need to be wrapped as callables or ADK BaseTool instances.\n        lc_tools_names = tools_config.get('lagChinTools', [])\n        # hf_tools_names = tools_config.get('huggingTools', []) # HuggingFace tools are also LangChain tools\n        # plugin_urls = tools_config.get('Plugins', [])\n\n        all_lc_tool_names = list(set(lc_tools_names))  # + hf_tools_names\n\n        for tool_name in all_lc_tool_names:\n            try:\n                # Load tool instance (LangChain's load_tools might return a list)\n                loaded_tools = load_tools([tool_name], llm=None)  # LLM not always needed for tool definition\n                for lc_tool_instance in loaded_tools:\n                    # Wrap and add to builder\n                    # Simple case: wrap lc_tool_instance.run or lc_tool_instance._run\n                    if hasattr(lc_tool_instance, 'run') and callable(lc_tool_instance.run):\n                        # ADK FunctionTool needs a schema, or infers it.\n                        # We might need to manually create Pydantic models for args.\n                        # For simplicity, assume ADK can infer or the tool takes simple args.\n                        agent_builder.with_adk_tool_function(lc_tool_instance.run, name=lc_tool_instance.name,\n                                                             description=lc_tool_instance.description)\n                        self.print(f\"Added LangChain tool '{lc_tool_instance.name}' to builder.\")\n                        self.lang_chain_tools_dict[lc_tool_instance.name] = lc_tool_instance  # Store for reference\n            except Exception as e:\n                self.print(f\"Failed to load/add LangChain tool '{tool_name}': {e}\")\n\n        # AIPluginTool needs more complex handling as it's a class\n        # for url in plugin_urls:\n        #     try:\n        #         plugin = AIPluginTool.from_plugin_url(url)\n        #         # Exposing AIPluginTool methods might require creating individual FunctionTools\n        #         # Or creating a custom ADK BaseTool wrapper for AIPluginTool\n        #         self.print(f\"AIPluginTool {plugin.name} loaded. Manual ADK wrapping needed.\")\n        #     except Exception as e:\n        #         self.print(f\"Failed to load AIPlugin from {url}: {e}\")\n\n    def serialize_all(self, exclude=None):\n        # Returns a copy of agent_data, which contains AgentConfig dicts\n        # The exclude logic might be different if it was excluding fields from old AgentBuilder\n        # For AgentConfig, exclusion happens during model_dump if needed.\n        return copy.deepcopy(self.agent_data)\n\n    def deserialize_all(self, data: dict[str, dict]):\n        # Data is a dict of {agent_name: builder_config_dict}\n        self.agent_data.update(data)\n        # Clear instances from self.config so they are rebuilt with new configs\n        for agent_name in data.keys():\n            self.config.pop(f'agent-instance-{agent_name}', None)\n\n    async def init_isaa(self, name='self', build=False, only_v=False, **kwargs):  # build/only_v seem unused\n        if self.initialized:\n            self.print(f\"Already initialized. Getting agent/builder: {name}\")\n            # build=True implies getting the builder, build=False (default) implies getting agent instance\n            return self.get_agent_builder(name) if build else await self.get_agent(name)\n\n        self.initialized = True\n        sys.setrecursionlimit(1500)\n        self.load_keys_from_env()\n\n        # Background loading\n        loop = asyncio.get_event_loop()\n        loop.run_in_executor(None, self.agent_chain.load_from_file)\n        loop.run_in_executor(None, self.scripts.load_scripts)\n\n        with Spinner(message=\"Building Controller\", symbols='c'):\n            self.controller.init(self.config['controller_file'])\n        self.config[\"controller-init\"] = True\n\n\n        return self.get_agent_builder(name) if build else await self.get_agent(name)\n\n    def show_version(self):\n        self.print(\"Version: \", self.version)\n        return self.version\n\n    def on_start(self):\n\n        initialize_isaa_chains(self.app)\n        initialize_isaa_webui_module(self.app, self)\n\n        threading.Thread(target=self.load_to_mem_sync, daemon=True).start()\n        self.print(\"ISAA module started.\")\n\n    def load_secrit_keys_from_env(self):\n        # These are often used by LiteLLM if not passed directly or set as env vars for LiteLLM\n        pass  # Keeping this empty as API keys are better handled by direct env vars or builder config\n\n    def load_keys_from_env(self):\n        # Update default model names from environment variables\n        for key in self.config:\n            if key.startswith(\"DEFAULTMODEL\"):\n                self.config[key] = os.getenv(key, self.config[key])\n        self.config['VAULTS'] = os.getenv(\"VAULTS\")\n\n    def on_exit(self):\n        # Save agent configurations\n        for agent_name, agent_instance in self.config.items():\n            if agent_name.startswith('agent-instance-') and agent_instance and isinstance(agent_instance, list) and isinstance(agent_instance[0], FlowAgent):\n                self.app.run_bg_task_advanced(asyncio.gather(*[agent_instance.close() for agent_instance in agent_instance]))\n                # If agent instance has its own save logic (e.g. cost tracker)\n                # asyncio.run(agent_instance.close()) # This might block, consider task group\n                # The AgentConfig is already in self.agent_data, which should be saved.\n                pass  # Agent instances are not directly saved, their configs are.\n\n        threading.Thread(target=self.save_to_mem_sync, daemon=True).start()  # Sync wrapper for save_to_mem\n\n        # Save controller if initialized\n        if self.config.get(\"controller-init\"):\n            self.controller.save(self.config['controller_file'])\n\n        # Clean up self.config for saving\n        clean_config = {}\n        for key, value in self.config.items():\n            if key.startswith('agent-instance-'): continue  # Don't save instances\n            if key.startswith('LLM-model-'): continue  # Don't save langchain models\n            clean_config[key] = value\n        self.add_to_save_file_handler(self.keys[\"Config\"], json.dumps(clean_config))\n\n        # Save other persistent data\n        self.save_file_handler()\n        self.agent_chain.save_to_file()\n        self.scripts.save_scripts()\n\n    def save_to_mem_sync(self):\n        # This used to call agent.save_memory(). FlowAgent does not have this.\n        # If AISemanticMemory needs global saving, it should be handled by AISemanticMemory itself.\n        # For now, this can be a no-op or save AISemanticMemory instances if managed by Tools.\n        memory_instance = self.get_memory()  # Assuming this returns AISemanticMemory\n        if hasattr(memory_instance, 'save_all_memories'):  # Hypothetical method\n            memory_instance.save_all_memories(f\"{get_app().data_dir}/Memory/\")\n        self.print(\"Memory saving process initiated\")\n    def load_to_mem_sync(self):\n        # This used to call agent.save_memory(). FlowAgent does not have this.\n        # If AISemanticMemory needs global saving, it should be handled by AISemanticMemory itself.\n        # For now, this can be a no-op or save AISemanticMemory instances if managed by Tools.\n        memory_instance = self.get_memory()  # Assuming this returns AISemanticMemory\n        if hasattr(memory_instance, 'load_all_memories'):  # Hypothetical method\n            memory_instance.load_all_memories(f\"{get_app().data_dir}/Memory/\")\n        self.print(\"Memory loading process initiated\")\n\n    def get_agent_builder(self, name=\"self\") -&gt; FlowAgentBuilder:\n        if name == 'None':\n            name = \"self\"\n\n        self.print(f\"Creating FlowAgentBuilder: {name}\")\n\n        # Create builder with agent-specific configuration\n        config = AgentConfig(\n            name=name,\n            fast_llm_model=self.config.get(f'DEFAULTMODEL{name.upper()}', self.config['DEFAULTMODEL0']),\n            complex_llm_model=self.config.get(f'DEFAULTMODEL{name.upper()}', self.config['DEFAULTMODEL1']),\n            system_message=\"You are a production-ready autonomous agent.\",\n            temperature=0.7,\n            max_tokens_output=2048,\n            max_tokens_input=32768,\n            use_fast_response=True,\n            max_parallel_tasks=3,\n            verbose_logging=False\n        )\n\n        builder = FlowAgentBuilder(config=config)\n        builder._isaa_ref = self  # Store ISAA reference\n\n        # Load existing configuration if available\n        agent_config_path = Path(f\"{get_app().data_dir}/Agents/{name}.agent.json\")\n        if agent_config_path.exists():\n            try:\n                builder = FlowAgentBuilder.from_config_file(str(agent_config_path))\n                builder._isaa_ref = self\n                self.print(f\"Loaded existing configuration for builder {name}\")\n            except Exception as e:\n                self.print(f\"Failed to load config for {name}: {e}. Using defaults.\")\n                # Keep the default builder\n\n        # Apply global settings\n        if self.global_stream_override:\n            builder.verbose(True)\n\n        # Apply custom setter if available\n        if self.default_setter:\n            builder = self.default_setter(builder, name)\n\n        # Add common ISAA tools\n        async def run_isaa_agent_tool(target_agent_name: str, instructions: str, **kwargs_):\n            return await self.run_agent(target_agent_name, instructions, **kwargs_)\n\n        async def memory_search_tool(\n            query: str,\n            search_mode: Optional[str] = \"balanced\",\n            context_name: Optional[str] = None\n        ) -&gt; str:\n            \"\"\"Memory search with configurable precision\"\"\"\n            mem_instance = self.get_memory()\n            memory_names_list = [name.strip() for name in context_name.split(',')] if context_name else None\n\n            search_params = {\n                \"wide\": {\"k\": 7, \"min_similarity\": 0.1, \"cross_ref_depth\": 3, \"max_cross_refs\": 4, \"max_sentences\": 8},\n                \"narrow\": {\"k\": 2, \"min_similarity\": 0.75, \"cross_ref_depth\": 1, \"max_cross_refs\": 1,\n                           \"max_sentences\": 3},\n                \"balanced\": {\"k\": 3, \"min_similarity\": 0.2, \"cross_ref_depth\": 2, \"max_cross_refs\": 2,\n                             \"max_sentences\": 5}\n            }.get(search_mode,\n                  {\"k\": 3, \"min_similarity\": 0.2, \"cross_ref_depth\": 2, \"max_cross_refs\": 2, \"max_sentences\": 5})\n\n            return await mem_instance.query(\n                query=query, memory_names=memory_names_list,\n                query_params=search_params, to_str=True\n            )\n\n        async def save_to_memory_tool(data_to_save: str, context_name: str = name):\n            mem_instance = self.get_memory()\n            result = await mem_instance.add_data(context_name, str(data_to_save), direct=True)\n            return 'Data added to memory.' if result else 'Error adding data to memory.'\n\n        # Add tools to builder\n        builder.add_tool(memory_search_tool, \"memorySearch\", \"Search ISAA's semantic memory\")\n        builder.add_tool(save_to_memory_tool, \"saveDataToMemory\", \"Save data to ISAA's semantic memory\")\n        builder.add_tool(self.web_search, \"searchWeb\", \"Search the web for information\")\n        builder.add_tool(self.shell_tool_function, \"shell\", f\"Run shell command in {detect_shell()}\")\n\n        # Add specialized tools based on agent type\n        if name == \"self\" or \"code\" in name.lower() or \"pipe\" in name.lower():\n            async def code_pipeline_tool(task: str, do_continue: bool = False):\n                return await self.run_pipe(name, task, do_continue=do_continue)\n\n            builder.add_tool(code_pipeline_tool, \"runCodePipeline\",\n                             \"Run multi-step code generation/execution in isolated Python environment\")\n\n        if name == \"self\" or any(keyword in name.lower() for keyword in [\"task\", \"chain\", \"supervisor\"]):\n            # Add task management tools\n            task_tools = [\n                (self.create_task_chain, \"createTaskChain\", \"Create a task chain from sequence of steps\"),\n                (self.run_task, \"runTaskChain\", \"Run a specific task chain\"),\n                (self.get_task, \"getTaskChain\", \"Get information about a task chain\"),\n                (self.load_task, \"loadTaskChain\", \"Load a task chain from storage\"),\n                (self.save_task, \"saveTaskChain\", \"Save a task chain to persistent storage\"),\n                (self.remove_task, \"removeTaskChain\", \"Remove a task chain from system\"),\n                (self.list_task, \"listTasksChains\", \"List all available task chains\"),\n            ]\n\n            builder.add_tool(run_isaa_agent_tool, \"askAgent\",\n                             f\"Ask any agent: {self.config.get('agents-name-list', [])} for results/status\")\n\n            for func, tool_name, desc in task_tools:\n                builder.add_tool(func, tool_name, desc)\n\n        # Configure cost tracking\n        cost_file = Path(f\"{get_app().data_dir}/Agents/{name}.costs.json\")\n        builder.with_budget_manager(max_cost=100.0)  # Set reasonable default\n\n        return builder\n\n    async def register_agent(self, agent_builder: FlowAgentBuilder):\n        agent_name = agent_builder.config.name\n\n        if f'agent-instance-{agent_name}' in self.config:\n            self.print(f\"Agent '{agent_name}' instance already exists. Overwriting config and rebuilding on next get.\")\n            self.config.pop(f'agent-instance-{agent_name}', None)\n\n        # Save the builder's configuration\n        config_path = Path(f\"{get_app().data_dir}/Agents/{agent_name}.agent.json\")\n        agent_builder.save_config(str(config_path), format='json')\n        self.print(f\"Saved FlowAgentBuilder config for '{agent_name}' to {config_path}\")\n\n        # Store serializable config in agent_data\n        self.agent_data[agent_name] = agent_builder.config.model_dump()\n\n        if agent_name not in self.config.get(\"agents-name-list\", []):\n            if \"agents-name-list\" not in self.config:\n                self.config[\"agents-name-list\"] = []\n            self.config[\"agents-name-list\"].append(agent_name)\n\n        self.print(f\"FlowAgent '{agent_name}' configuration registered. Will be built on first use.\")\n        row_agent_builder_sto[agent_name] = agent_builder  # Cache builder\n\n    async def get_agent(self, agent_name=\"Normal\", model_override: str | None = None) -&gt; FlowAgent:\n        if \"agents-name-list\" not in self.config:\n            self.config[\"agents-name-list\"] = []\n\n        instance_key = f'agent-instance-{agent_name}'\n        if instance_key in self.config:\n            agent_instance = self.config[instance_key]\n            if model_override and agent_instance.amd.fast_llm_model != model_override:\n                self.print(f\"Model override for {agent_name}: {model_override}. Rebuilding.\")\n                self.config.pop(instance_key, None)\n            else:\n                self.print(f\"Returning existing FlowAgent instance: {agent_name}\")\n                return agent_instance\n\n        builder_to_use = None\n\n        # Try to get cached builder first\n        if agent_name in row_agent_builder_sto:\n            builder_to_use = row_agent_builder_sto[agent_name]\n            self.print(f\"Using cached builder for {agent_name}\")\n\n        # Try to load from stored config\n        elif agent_name in self.agent_data:\n            self.print(f\"Loading configuration for FlowAgent: {agent_name}\")\n            try:\n                config = AgentConfig(**self.agent_data[agent_name])\n                builder_to_use = FlowAgentBuilder(config=config)\n            except Exception as e:\n                self.print(f\"Error loading config for {agent_name}: {e}. Falling back to default.\")\n\n        # Create default builder if none found\n        if builder_to_use is None:\n            self.print(f\"No existing config for {agent_name}. Creating default builder.\")\n            builder_to_use = self.get_agent_builder(agent_name)\n\n        # Apply overrides and ensure correct name\n        builder_to_use._isaa_ref = self\n        if model_override:\n            builder_to_use.with_models(model_override, model_override)\n\n        if builder_to_use.config.name != agent_name:\n            builder_to_use.with_name(agent_name)\n\n        self.print(\n            f\"Building FlowAgent: {agent_name} with models {builder_to_use.config.fast_llm_model}/{builder_to_use.config.complex_llm_model}\")\n\n        # Build the agent\n        agent_instance: FlowAgent = await builder_to_use.build()\n\n        # colletive cabability cahring for reduched reduanda analysis _tool_capabilities\n        agent_tool_nams = set(agent_instance.tool_registry.keys())\n\n        tools_data = {}\n        for _agent_name in self.config[\"agents-name-list\"]:\n            _instance_key = f'agent-instance-{_agent_name}'\n            if _instance_key not in self.config:\n                if agent_name != \"self\" and _agent_name == \"self\":\n                    await self.get_agent(\"self\")\n\n            if _instance_key not in self.config:\n                continue\n            _agent_instance = self.config[_instance_key]\n            _agent_tool_nams = set(_agent_instance._tool_capabilities.keys())\n            # extract the tool names that are in both agents_registry\n            overlap_tool_nams = agent_tool_nams.intersection(_agent_tool_nams)\n            _tc = _agent_instance._tool_capabilities\n            for tool_name in overlap_tool_nams:\n                if tool_name not in _tc:\n                    continue\n                tools_data[tool_name] = _tc[tool_name]\n\n        agent_instance._tool_capabilities.update(tools_data)\n        # Cache the instance and update tracking\n        self.config[instance_key] = agent_instance\n        if agent_name not in self.agent_data:\n            self.agent_data[agent_name] = builder_to_use.config.model_dump()\n        if agent_name not in self.config[\"agents-name-list\"]:\n            self.config[\"agents-name-list\"].append(agent_name)\n\n        self.print(f\"Built and cached FlowAgent instance: {agent_name}\")\n        return agent_instance\n\n    async def mini_task_completion(self, mini_task: str, user_task: str | None = None, mode: Any = None,  # LLMMode\n                                   max_tokens_override: int | None = None, task_from=\"system\",\n                                   stream_function: Callable | None = None, message_history: list | None = None, agent_name=\"TaskCompletion\"):\n        if mini_task is None: return None\n        if agent_name is None: return None\n        if mini_task == \"test\": return \"test\"\n        self.print(f\"Running mini task, volume {len(mini_task)}\")\n\n        agent = await self.get_agent(agent_name)  # Ensure agent is retrieved (and built if needed)\n\n        effective_system_message = agent.amd.system_message\n        if mode and hasattr(mode, 'system_msg') and mode.system_msg:\n            effective_system_message = mode.system_msg\n\n        messages = []\n        if effective_system_message:\n            messages.append({\"role\": \"system\", \"content\": effective_system_message})\n        if message_history:\n            messages.extend(message_history)\n\n        current_prompt = mini_task\n        if user_task:  # If user_task is provided, it becomes the main prompt, mini_task is context\n            messages.append({\"role\": task_from, \"content\": mini_task})  # mini_task as prior context\n            current_prompt = user_task  # user_task as the current prompt\n\n        messages.append({\"role\": \"user\", \"content\": current_prompt})\n\n        # Prepare params for a_run_llm_completion\n        llm_params = {\"model\": agent.amd.fast_llm_model if agent.amd.use_fast_response else agent.amd.complex_llm_model, \"messages\": messages}\n        if max_tokens_override:\n            llm_params['max_tokens'] = max_tokens_override\n        else:\n            llm_params['max_tokens'] = agent.amd.max_tokens\n\n        if stream_function:\n            llm_params['stream'] = True\n            # FlowAgent a_run_llm_completion handles stream_callback via agent.stream_callback\n            # For a one-off, we might need a temporary override or pass it if supported.\n            # For now, assume stream_callback is set on agent instance if needed globally.\n            # If stream_function is for this call only, agent.a_run_llm_completion needs modification\n            # or we use a temporary agent instance. This part is tricky.\n            # Let's assume for now that if stream_function is passed, it's a global override for this agent type.\n            original_stream_cb = agent.stream_callback\n            original_stream_val = agent.stream\n            agent.stream_callback = stream_function\n            agent.stream = True\n            try:\n                response_content = await agent.a_run_llm_completion(**llm_params)\n            finally:\n                agent.stream_callback = original_stream_cb\n                agent.stream = original_stream_val  # Reset to builder's config\n            return response_content  # Streaming output handled by callback\n\n        llm_params['stream'] = False\n        response_content = await agent.a_run_llm_completion(**llm_params)\n        return response_content\n\n    async def mini_task_completion_format(self, mini_task, format_schema: type[BaseModel],\n                                          max_tokens_override: int | None = None, agent_name=\"TaskCompletion\",\n                                          task_from=\"system\", mode_overload: Any = None, user_task: str | None = None):\n        if mini_task is None: return None\n        self.print(f\"Running formatted mini task, volume {len(mini_task)}\")\n\n        agent = await self.get_agent(agent_name)\n\n        effective_system_message = None\n        if mode_overload and hasattr(mode_overload, 'system_msg') and mode_overload.system_msg:\n            effective_system_message = mode_overload.system_msg\n\n        message_context = []\n        if effective_system_message:\n            message_context.append({\"role\": \"system\", \"content\": effective_system_message})\n\n        current_prompt = mini_task\n        if user_task:\n            message_context.append({\"role\": task_from, \"content\": mini_task})\n            current_prompt = user_task\n\n        # Use agent.a_format_class\n        try:\n            result_dict = await agent.a_format_class(\n                pydantic_model=format_schema,\n                prompt=current_prompt,\n                message_context=message_context,\n                # max_tokens can be part of agent's model config or passed if a_format_class supports it\n            )\n            if format_schema == bool:  # Special handling for boolean schema\n                # a_format_class returns a dict, e.g. {\"value\": True}. Extract the bool.\n                # This depends on how bool schema is defined. A common way: class BoolResponse(BaseModel): value: bool\n                return result_dict.get(\"value\", False) if isinstance(result_dict, dict) else False\n            return result_dict\n        except Exception as e:\n            self.print(f\"Error in mini_task_completion_format: {e}\")\n            return None  # Or raise\n\n    async def format_class(self, format_schema: type[BaseModel], task: str, agent_name=\"TaskCompletion\"):\n        if format_schema is None or not task: return None\n\n        agent = None\n        if isinstance(agent_name, str):\n            agent = await self.get_agent(agent_name)\n        elif isinstance(agent_name, FlowAgent):\n            agent = agent_name\n        else:\n            raise TypeError(\"agent_name must be str or FlowAgent instance\")\n\n        return await agent.a_format_class(format_schema, task)\n\n    async def get_pipe(self, agent_name_or_instance: str | FlowAgent, *args, **kwargs) -&gt; Pipeline:\n        agent_instance = None\n        agent_name_str = \"\"\n\n        if isinstance(agent_name_or_instance, str):\n            agent_name_str = agent_name_or_instance\n            agent_instance = await self.get_agent(agent_name_str)\n        elif isinstance(agent_name_or_instance, FlowAgent):\n            agent_instance = agent_name_or_instance\n            agent_name_str = agent_instance.amd.name  # amd is AgentModelData\n        else:\n            return self.return_result().default_internal_error(f\"agent_name_or_instance must be str or FlowAgent is {type(agent_name_or_instance)}\")\n\n        if agent_name_str in self.pipes:\n            # Optionally reconfigure if args/kwargs are different\n            # For simplicity, returning existing pipe.\n            return self.pipes[agent_name_str]\n        else:\n            # Pass the already fetched/validated agent_instance to Pipeline\n            variables = []\n            if 'variables' in kwargs:\n                variables = kwargs.pop('variables')\n            if isinstance(variables, list):\n                variables += [self.app, agent_instance]\n            if isinstance(variables, dict):\n                variables.update({'app': self.app, 'agent': agent_instance})\n\n            self.pipes[agent_name_str] = Pipeline(agent_instance, *args, **kwargs, variables=variables)\n            return self.pipes[agent_name_str]\n\n    async def run_pipe(self, agent_name_or_instance: str | FlowAgent, task: str, do_continue=False):\n        if task is None: return \"\"\n        if task == \"test\": return \"test\"\n        if agent_name_or_instance is None: return \"\"\n\n        pipe = await self.get_pipe(agent_name_or_instance)\n        if not hasattr(pipe, 'run'):\n            return pipe\n        return await pipe.run(task, do_continue=do_continue)  # pipeline.run is async\n\n    async def run_agent(self, name: str | FlowAgent,\n                        text: str,\n                        verbose: bool = False,  # Handled by agent's own config mostly\n                        session_id: Optional[str] = None,\n                        progress_callback: Callable[[Any], None | Awaitable[None]] | None = None,\n                        **kwargs):  # Other kwargs for a_run\n        if text is None: return \"\"\n        if name is None: return \"\"\n        if text == \"test\": return \"\"\n\n        agent_instance = None\n        if isinstance(name, str):\n            agent_instance = await self.get_agent(name)\n        elif isinstance(name, FlowAgent):\n            agent_instance = name\n        else:\n            return self.return_result().default_internal_error(\n                f\"Invalid agent identifier type: {type(name)}\")\n\n        self.print(f\"Running agent {agent_instance.amd.name} for task: {text[:100]}...\")\n        save_p = None\n        if progress_callback:\n            save_p = agent_instance.progress_callback\n            agent_instance.progress_callback = progress_callback\n\n        if verbose:\n            agent_instance.verbose = True\n\n        # Call FlowAgent's a_run method\n        response = await agent_instance.a_run(\n            query=text,\n            session_id=session_id,\n            user_id=None,\n            stream_callback=None\n\n        )\n        if save_p:\n            agent_instance.progress_callback = save_p\n\n        return response\n\n    # mass_text_summaries and related methods remain complex and depend on AISemanticMemory\n    # and specific summarization strategies. For now, keeping their structure,\n    # but calls to self.format_class or self.mini_task_completion will become async.\n\n    async def mas_text_summaries(self, text, min_length=3600, ref=None):\n        len_text = len(text)\n        if len_text &lt; min_length: return text\n        key = self.one_way_hash(text, 'summaries', 'isaa')\n        value = self.mas_text_summaries_dict.get(key)\n        if value is not None: return value\n\n        # This part needs to become async due to format_class\n        # Simplified version:\n        summary = await self.mini_task_completion(\n            mini_task=f\"Summarize this text, focusing on aspects related to '{ref if ref else 'key details'}'. The text is: {text}\",\n            mode=self.controller.rget(SummarizationMode))\n\n        if summary is None or not isinstance(summary, str):\n            # Fallback or error handling\n            summary = text[:min_length] + \"... (summarization failed)\"\n\n        self.mas_text_summaries_dict.set(key, summary)\n        return summary\n\n    def get_memory(self, name: str | None = None) -&gt; AISemanticMemory:\n        # This method's logic seems okay, AISemanticMemory is a separate system.\n        logger_ = get_logger()  # Renamed to avoid conflict with self.logger\n        if isinstance(self.agent_memory, str):  # Path string\n            logger_.info(Style.GREYBG(\"AISemanticMemory Initialized from path\"))\n            self.agent_memory = AISemanticMemory(base_path=self.agent_memory)\n\n        cm = self.agent_memory\n        if name is not None:\n            # Assuming AISemanticMemory.get is synchronous or you handle async appropriately\n            # If AISemanticMemory methods become async, this needs adjustment\n            mem_kb = cm.get(name)  # This might return a list of KnowledgeBase or single one\n            return mem_kb\n        return cm\n\n    # set_local_files_tools seems related to old FileManagementToolkit, may need removal or ADK equivalent\n    def set_local_files_tools(self, local_files_tools_enabled: bool):\n        self.local_files_tools = local_files_tools_enabled\n        self.print(f\"Local file tools (old system) set to: {self.local_files_tools}\")\n        # If using ADK, file tools would be added to agents via builder.\n        # This flag might control whether default builders get default file tools.\n        return f\"Local file tools (old system) set to: {self.local_files_tools}\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.module.Tools.init_from_augment","title":"<code>init_from_augment(augment, agent_name='self', exclude=None)</code>  <code>async</code>","text":"<p>Initialize from augmented data using new builder system</p> Source code in <code>toolboxv2/mods/isaa/module.py</code> <pre><code>async def init_from_augment(self, augment, agent_name: str = 'self', exclude=None):\n    \"\"\"Initialize from augmented data using new builder system\"\"\"\n\n    # Handle agent_name parameter\n    if isinstance(agent_name, str):\n        pass  # Use string name\n    elif hasattr(agent_name, 'config'):  # FlowAgentBuilder\n        agent_name = agent_name.config.name\n    else:\n        raise ValueError(f\"Invalid agent_name type: {type(agent_name)}\")\n\n    a_keys = augment.keys()\n\n    # Load agent configurations\n    if \"Agents\" in a_keys:\n        agents_configs_dict = augment['Agents']\n        self.deserialize_all(agents_configs_dict)\n        self.print(\"Agent configurations loaded.\")\n\n    # Load custom functions (scripts)\n    if \"customFunctions\" in a_keys:\n        custom_functions = augment['customFunctions']\n        if isinstance(custom_functions, str):\n            custom_functions = json.loads(custom_functions)\n        if custom_functions:\n            self.scripts.scripts = custom_functions\n            self.print(\"Custom functions loaded\")\n\n    # Load task chains\n    if \"tasks\" in a_keys:\n        tasks = augment['tasks']\n        if isinstance(tasks, str):\n            tasks = json.loads(tasks)\n        if tasks:\n            self.agent_chain.load_from_dict(tasks)\n            self.print(\"Task chains restored\")\n\n    # Tools are now handled by the builder system during agent creation\n    if \"tools\" in a_keys:\n        self.print(\"Tool configurations noted - will be applied during agent building\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.isaa.module.detect_shell","title":"<code>detect_shell()</code>","text":"<p>Detects the best available shell and the argument to execute a command. Returns:     A tuple of (shell_executable, command_argument).     e.g., ('/bin/bash', '-c') or ('powershell.exe', '-Command')</p> Source code in <code>toolboxv2/mods/isaa/module.py</code> <pre><code>def detect_shell() -&gt; tuple[str, str]:\n    \"\"\"\n    Detects the best available shell and the argument to execute a command.\n    Returns:\n        A tuple of (shell_executable, command_argument).\n        e.g., ('/bin/bash', '-c') or ('powershell.exe', '-Command')\n    \"\"\"\n    if platform.system() == \"Windows\":\n        if shell_path := shutil.which(\"pwsh\"):\n            return shell_path, \"-Command\"\n        if shell_path := shutil.which(\"powershell\"):\n            return shell_path, \"-Command\"\n        return \"cmd.exe\", \"/c\"\n\n    shell_env = os.environ.get(\"SHELL\")\n    if shell_env and shutil.which(shell_env):\n        return shell_env, \"-c\"\n\n    for shell in [\"bash\", \"zsh\", \"sh\"]:\n        if shell_path := shutil.which(shell):\n            return shell_path, \"-c\"\n\n    return \"/bin/sh\", \"-c\"\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.talk","title":"<code>talk</code>","text":""},{"location":"toolboxv2/#toolboxv2.mods.talk.TalkSession","title":"<code>TalkSession</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the state of a single voice conversation session.</p> Source code in <code>toolboxv2/mods/talk.py</code> <pre><code>class TalkSession(BaseModel):\n    \"\"\"Represents the state of a single voice conversation session.\"\"\"\n    session_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    user_id: str\n    chat_session: ChatSession\n    event_queue: asyncio.Queue = Field(default_factory=asyncio.Queue, exclude=True)\n    # Task to track the running agent process, preventing concurrent requests\n    agent_task: Optional[asyncio.Task] = Field(default=None, exclude=True)\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.talk.Tools","title":"<code>Tools</code>","text":"<p>               Bases: <code>MainTool</code></p> <p>The main class for the Talk module, handling initialization, session management, and dependency loading.</p> Source code in <code>toolboxv2/mods/talk.py</code> <pre><code>class Tools(MainTool):\n    \"\"\"\n    The main class for the Talk module, handling initialization,\n    session management, and dependency loading.\n    \"\"\"\n\n    def __init__(self, app: App):\n        # Initialize the MainTool with module-specific information\n        self.version = VERSION\n        self.name = MOD_NAME\n        self.color = \"CYAN\"\n        self.sessions: Dict[str, TalkSession] = {}\n        self.stt_func = None\n        self.tts_func = None\n        self.isaa_mod = None\n        super().__init__(load=self.on_start, v=VERSION, name=MOD_NAME, tool={}, on_exit=self.on_exit)\n\n    def on_start(self):\n        \"\"\"Initializes the Talk module, its dependencies (ISAA, AUDIO), and UI registration.\"\"\"\n        self.app.logger.info(f\"Starting {self.name} v{self.version}...\")\n\n        # Get the ISAA module instance, which is a critical dependency\n        self.isaa_mod = self.app.get_mod(\"isaa\")\n        if not self.isaa_mod:\n            self.app.logger.error(\n                f\"{self.name}: ISAA module not found or failed to load. Voice assistant will not be functional.\")\n            return\n\n        # Initialize STT and TTS services from the AUDIO module\n        if hasattr(TBEF, \"AUDIO\") and self.app.get_mod(\"AUDIO\"):\n            self.stt_func = self.app.run_any(TBEF.AUDIO.STT_GENERATE, model=\"openai/whisper-small\", row=True, device=0)\n            self.tts_func = self.app.get_function(TBEF.AUDIO.SPEECH, state=False)[0]\n\n            if self.stt_func and self.stt_func != \"404\":\n                self.app.logger.info(\"Talk STT (whisper-small) is Online.\")\n            else:\n                self.app.logger.warning(\"Talk STT function not available.\")\n                self.stt_func = None\n\n            if self.tts_func and self.tts_func != \"404\":\n                self.app.logger.info(\"Talk TTS function is Online.\")\n            else:\n                self.app.logger.warning(\"Talk TTS function not available.\")\n                self.tts_func = None\n        else:\n            self.app.logger.warning(\"Talk module: AUDIO module features are not available or the module is not loaded.\")\n\n        if not all([self.stt_func, self.tts_func]):\n            self.app.logger.error(\"Talk module cannot function without both STT and TTS services.\")\n\n        # Register the UI component with CloudM\n        self.app.run_any((\"CloudM\", \"add_ui\"),\n                         name=MOD_NAME, title=\"Voice Assistant\", path=f\"/api/{MOD_NAME}/ui\",\n                         description=\"Natural conversation with an AI assistant.\", auth=True)\n        self.app.logger.info(f\"{self.name} UI registered with CloudM.\")\n\n    def on_exit(self):\n        \"\"\"Clean up resources, especially cancelling any active agent tasks.\"\"\"\n        for session in self.sessions.values():\n            if session.agent_task and not session.agent_task.done():\n                session.agent_task.cancel()\n        self.app.logger.info(f\"Closing {self.name} and cleaning up sessions.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.talk.Tools.on_exit","title":"<code>on_exit()</code>","text":"<p>Clean up resources, especially cancelling any active agent tasks.</p> Source code in <code>toolboxv2/mods/talk.py</code> <pre><code>def on_exit(self):\n    \"\"\"Clean up resources, especially cancelling any active agent tasks.\"\"\"\n    for session in self.sessions.values():\n        if session.agent_task and not session.agent_task.done():\n            session.agent_task.cancel()\n    self.app.logger.info(f\"Closing {self.name} and cleaning up sessions.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.talk.Tools.on_start","title":"<code>on_start()</code>","text":"<p>Initializes the Talk module, its dependencies (ISAA, AUDIO), and UI registration.</p> Source code in <code>toolboxv2/mods/talk.py</code> <pre><code>def on_start(self):\n    \"\"\"Initializes the Talk module, its dependencies (ISAA, AUDIO), and UI registration.\"\"\"\n    self.app.logger.info(f\"Starting {self.name} v{self.version}...\")\n\n    # Get the ISAA module instance, which is a critical dependency\n    self.isaa_mod = self.app.get_mod(\"isaa\")\n    if not self.isaa_mod:\n        self.app.logger.error(\n            f\"{self.name}: ISAA module not found or failed to load. Voice assistant will not be functional.\")\n        return\n\n    # Initialize STT and TTS services from the AUDIO module\n    if hasattr(TBEF, \"AUDIO\") and self.app.get_mod(\"AUDIO\"):\n        self.stt_func = self.app.run_any(TBEF.AUDIO.STT_GENERATE, model=\"openai/whisper-small\", row=True, device=0)\n        self.tts_func = self.app.get_function(TBEF.AUDIO.SPEECH, state=False)[0]\n\n        if self.stt_func and self.stt_func != \"404\":\n            self.app.logger.info(\"Talk STT (whisper-small) is Online.\")\n        else:\n            self.app.logger.warning(\"Talk STT function not available.\")\n            self.stt_func = None\n\n        if self.tts_func and self.tts_func != \"404\":\n            self.app.logger.info(\"Talk TTS function is Online.\")\n        else:\n            self.app.logger.warning(\"Talk TTS function not available.\")\n            self.tts_func = None\n    else:\n        self.app.logger.warning(\"Talk module: AUDIO module features are not available or the module is not loaded.\")\n\n    if not all([self.stt_func, self.tts_func]):\n        self.app.logger.error(\"Talk module cannot function without both STT and TTS services.\")\n\n    # Register the UI component with CloudM\n    self.app.run_any((\"CloudM\", \"add_ui\"),\n                     name=MOD_NAME, title=\"Voice Assistant\", path=f\"/api/{MOD_NAME}/ui\",\n                     description=\"Natural conversation with an AI assistant.\", auth=True)\n    self.app.logger.info(f\"{self.name} UI registered with CloudM.\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.talk.api_open_stream","title":"<code>api_open_stream(self, request, session_id)</code>  <code>async</code>","text":"<p>Opens a Server-Sent Events (SSE) stream for a given session ID.</p> Source code in <code>toolboxv2/mods/talk.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, name=\"stream\", api_methods=['GET'], request_as_kwarg=True)\nasync def api_open_stream(self: Tools, request: RequestData, session_id: str) -&gt; Result:\n    \"\"\"Opens a Server-Sent Events (SSE) stream for a given session ID.\"\"\"\n    if not session_id or session_id not in self.sessions:\n        return Result.default_user_error(info=\"Invalid or expired session ID.\", exec_code=404)\n\n    session = self.sessions[session_id]\n    queue = session.event_queue\n\n    async def event_generator() -&gt; AsyncGenerator[Dict[str, Any], None]:\n        self.app.logger.info(f\"SSE stream opened for session {session_id}\")\n        await queue.put({\"event\": \"connection_ready\", \"data\": \"Stream connected successfully.\"})\n        try:\n            while True:\n                event_data = await queue.get()\n                yield event_data\n                queue.task_done()\n        except asyncio.CancelledError:\n            self.app.logger.info(f\"SSE stream for session {session_id} cancelled by client.\")\n        finally:\n            if session_id in self.sessions:\n                if self.sessions[session_id].agent_task and not self.sessions[session_id].agent_task.done():\n                    self.sessions[session_id].agent_task.cancel()\n                del self.sessions[session_id]\n                self.app.logger.info(f\"Cleaned up and closed session {session_id}.\")\n\n    return Result.sse(stream_generator=event_generator())\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.talk.api_process_audio","title":"<code>api_process_audio(self, request, form_data)</code>  <code>async</code>","text":"<p>Receives audio, transcribes it, and starts the agent processing task.</p> Source code in <code>toolboxv2/mods/talk.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, name=\"process_audio\", api_methods=['POST'], request_as_kwarg=True)\nasync def api_process_audio(self: Tools, request: RequestData, form_data: Dict) -&gt; Result:\n    \"\"\"Receives audio, transcribes it, and starts the agent processing task.\"\"\"\n    if not self.stt_func:\n        return Result.default_internal_error(info=\"Speech-to-text service is not available.\")\n\n    session_id = form_data.get('session_id')\n    audio_file_data = form_data.get('audio_blob')\n\n    if not session_id or session_id not in self.sessions:\n        return Result.default_user_error(info=\"Invalid or missing session_id.\", exec_code=400)\n\n    session = self.sessions[session_id]\n\n    if session.agent_task and not session.agent_task.done():\n        return Result.default_user_error(info=\"Already processing a previous request.\", exec_code=429)\n\n    if not audio_file_data or 'content_base64' not in audio_file_data:\n        return Result.default_user_error(info=\"Audio data is missing or in the wrong format.\", exec_code=400)\n\n    try:\n        audio_bytes = base64.b64decode(audio_file_data['content_base64'])\n        transcription_result = self.stt_func(audio_bytes)\n        transcribed_text = transcription_result.get('text', '').strip()\n\n        if not transcribed_text:\n            await session.event_queue.put({\"event\": \"error\", \"data\": \"Could not understand audio. Please try again.\"})\n            return Result.ok(data={\"message\": \"Transcription was empty.\"})\n\n        await session.event_queue.put({\"event\": \"transcription_update\", \"data\": transcribed_text})\n\n        voice_params = {\n            \"voice_index\": int(form_data.get('voice_index', '0')),\n            \"provider\": form_data.get('provider', 'piper'),\n            \"model_name\": form_data.get('model_name', 'ryan')\n        }\n\n        # Start the background task; the request returns immediately.\n        session.agent_task = asyncio.create_task(\n            _run_agent_and_respond(self, session, transcribed_text, voice_params)\n        )\n        return Result.ok(data={\"message\": \"Audio received and processing started.\"})\n\n    except Exception as e:\n        self.app.logger.error(f\"Error processing audio for session {session_id}: {e}\", exc_info=True)\n        return Result.default_internal_error(info=f\"Failed to process audio: {str(e)}\")\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.talk.api_start_session","title":"<code>api_start_session(self, request)</code>  <code>async</code>","text":"<p>Creates a new talk session for an authenticated user.</p> Source code in <code>toolboxv2/mods/talk.py</code> <pre><code>@export(mod_name=MOD_NAME, api=True, name=\"start_session\", api_methods=['POST'], request_as_kwarg=True)\nasync def api_start_session(self: Tools, request: RequestData) -&gt; Result:\n    \"\"\"Creates a new talk session for an authenticated user.\"\"\"\n    user_id = await _get_user_uid(self.app, request)\n    if not user_id:\n        return Result.default_user_error(info=\"User authentication required.\", exec_code=401)\n\n    if not self.isaa_mod:\n        return Result.default_internal_error(info=\"ISAA module is not available.\")\n\n    # Create a new ISAA ChatSession for conversation history\n    chat_session = ChatSession(mem=self.isaa_mod.get_memory())\n    session = TalkSession(user_id=user_id, chat_session=chat_session)\n    self.sessions[session.session_id] = session\n\n    self.app.logger.info(f\"Started new talk session {session.session_id} for user {user_id}\")\n    return Result.json(data={\"session_id\": session.session_id})\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.mods.talk.get_main_ui","title":"<code>get_main_ui(self, request)</code>","text":"<p>Serves the main HTML and JavaScript UI for the Talk widget.</p> Source code in <code>toolboxv2/mods/talk.py</code> <pre><code>@export(mod_name=MOD_NAME, name=\"ui\", api=True, api_methods=['GET'], request_as_kwarg=True)\ndef get_main_ui(self: Tools, request: RequestData) -&gt; Result:\n    \"\"\"Serves the main HTML and JavaScript UI for the Talk widget.\"\"\"\n    html_content = \"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\" data-theme=\"light\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;ToolBoxV2 - Voice Assistant&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200\" /&gt;\n    &lt;style&gt;\n        body { font-family: sans-serif; background-color: var(--theme-bg); color: var(--theme-text); display: flex; justify-content: center; align-items: center; min-height: 100vh; margin: 0; }\n        .container { display: flex; flex-direction: column; align-items: center; justify-content: center; width: 100%; max-width: 600px; padding: 20px; text-align: center; }\n        .visualizer { width: 250px; height: 250px; background-color: var(--glass-bg); border-radius: 50%; position: relative; overflow: hidden; border: 3px solid var(--theme-border); box-shadow: inset 0 0 15px rgba(0,0,0,0.2); transition: border-color 0.3s, box-shadow 0.3s; }\n        .visualizer.recording { border-color: #ef4444; }\n        .visualizer.thinking { border-color: #3b82f6; animation: pulse 2s infinite; }\n        .visualizer.speaking { border-color: #22c55e; }\n        .particle { position: absolute; width: 8px; height: 8px; background-color: var(--theme-primary); border-radius: 50%; pointer-events: none; transition: all 0.1s; }\n        #micButton { margin-top: 30px; width: 80px; height: 80px; border-radius: 50%; border: none; background-color: var(--theme-primary); color: white; cursor: pointer; display: flex; justify-content: center; align-items: center; box-shadow: 0 4px 10px rgba(0,0,0,0.2); transition: background-color 0.2s, transform 0.1s; }\n        #micButton:active { transform: scale(0.95); }\n        #micButton:disabled { background-color: #9ca3af; cursor: not-allowed; }\n        #micButton .material-symbols-outlined { font-size: 40px; }\n        #statusText { margin-top: 20px; min-height: 50px; font-size: 1.2em; color: var(--theme-text-muted); line-height: 1.5; }\n        @keyframes pulse { 0% { box-shadow: inset 0 0 15px rgba(0,0,0,0.2), 0 0 0 0 rgba(59, 130, 246, 0.7); } 70% { box-shadow: inset 0 0 15px rgba(0,0,0,0.2), 0 0 0 15px rgba(59, 130, 246, 0); } 100% { box-shadow: inset 0 0 15px rgba(0,0,0,0.2), 0 0 0 0 rgba(59, 130, 246, 0); } }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"container\"&gt;\n        &lt;div class=\"visualizer\" id=\"visualizer\"&gt;&lt;/div&gt;\n        &lt;p id=\"statusText\"&gt;Press the microphone to start&lt;/p&gt;\n        &lt;button id=\"micButton\"&gt;&lt;span class=\"material-symbols-outlined\"&gt;hourglass_empty&lt;/span&gt;&lt;/button&gt;\n        &lt;div class=\"options\" style=\"margin-top: 20px;\"&gt;\n            &lt;label for=\"voiceSelect\"&gt;Voice:&lt;/label&gt;\n            &lt;select id=\"voiceSelect\"&gt;\n                &lt;option value='{\"provider\": \"piper\", \"model_name\": \"ryan\", \"voice_index\": 0}'&gt;Ryan (EN)&lt;/option&gt;\n                &lt;option value='{\"provider\": \"piper\", \"model_name\": \"kathleen\", \"voice_index\": 0}'&gt;Kathleen (EN)&lt;/option&gt;\n                &lt;option value='{\"provider\": \"piper\", \"model_name\": \"karlsson\", \"voice_index\": 0}'&gt;Karlsson (DE)&lt;/option&gt;\n            &lt;/select&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;script unSave=\"true\"&gt;\n    function initTalk() {\n        const visualizer = document.getElementById('visualizer');\n        const micButton = document.getElementById('micButton');\n        const statusText = document.getElementById('statusText');\n        const voiceSelect = document.getElementById('voiceSelect');\n\n        const state = { sessionId: null, sseConnection: null, mediaRecorder: null, audioChunks: [], isRecording: false, isProcessing: false, currentAudio: null };\n        let audioContext, analyser, particles = [];\n\n        function setStatus(text, mode = 'idle') {\n            statusText.textContent = text;\n            visualizer.className = 'visualizer ' + mode;\n        }\n\n        function createParticles(num = 50) {\n            visualizer.innerHTML = ''; particles = [];\n            for (let i = 0; i &lt; num; i++) {\n                const p = document.createElement('div'); p.classList.add('particle');\n                visualizer.appendChild(p);\n                particles.push({ element: p, angle: Math.random() * Math.PI * 2, radius: 50 + Math.random() * 50, speed: 0.01 + Math.random() * 0.02 });\n            }\n        }\n\n        function animateVisualizer() {\n            if (analyser) {\n                const dataArray = new Uint8Array(analyser.frequencyBinCount);\n                analyser.getByteFrequencyData(dataArray);\n                let average = dataArray.reduce((a, b) =&gt; a + b, 0) / dataArray.length;\n                particles.forEach(p =&gt; {\n                    p.angle += p.speed;\n                    const scale = 1 + (average / 128);\n                    p.element.style.transform = `translate(${Math.cos(p.angle) * p.radius * scale}px, ${Math.sin(p.angle) * p.radius * scale}px)`;\n                });\n            }\n            requestAnimationFrame(animateVisualizer);\n        }\n\n        async function startSession() {\n            if (state.sessionId) return;\n            setStatus(\"Connecting...\", 'thinking');\n            micButton.disabled = true;\n            try {\n                const response = await TB.api.request('talk', 'start_session', {}, 'POST');\n                if (response.error === 'none' &amp;&amp; response.get()?.session_id) {\n                    state.sessionId = response.get().session_id;\n                    connectSse();\n                } else {\n                    setStatus(response.info?.help_text || \"Failed to start session.\", 'error');\n                }\n            } catch (e) {\n                setStatus(\"Connection error.\", 'error');\n            }\n        }\n\n        function connectSse() {\n            if (!state.sessionId) return;\n            state.sseConnection = TB.sse.connect(`/sse/talk/stream?session_id=${state.sessionId}`, {\n                onOpen: () =&gt; console.log(\"SSE Stream Open\"),\n                onError: () =&gt; setStatus(\"Connection lost.\", 'error'),\n                listeners: {\n                    'connection_ready': (data) =&gt; { setStatus(\"Press the microphone to start\"); micButton.disabled = false; micButton.innerHTML = '&lt;span class=\"material-symbols-outlined\"&gt;mic&lt;/span&gt;'; },\n                    'transcription_update': (data) =&gt; { setStatus(`\u201c${data}\u201d`, 'thinking'); state.isProcessing = true; },\n                    'agent_thought': (data) =&gt; setStatus(data, 'thinking'),\n                    'agent_response_chunk': (data) =&gt; { if (statusText.textContent.startsWith('\u201c')) statusText.textContent = \"\"; statusText.textContent += data; },\n                    'audio_playback': (data) =&gt; playAudio(data.content, data.format),\n                    'processing_complete': (data) =&gt; { state.isProcessing = false; setStatus(data); micButton.disabled = false; micButton.innerHTML = '&lt;span class=\"material-symbols-outlined\"&gt;mic&lt;/span&gt;'; },\n                    'error': (data) =&gt; { state.isProcessing = false; setStatus(data, 'error'); micButton.disabled = false; micButton.innerHTML = '&lt;span class=\"material-symbols-outlined\"&gt;mic&lt;/span&gt;'; }\n                }\n            });\n        }\n\n        async function playAudio(base64, format) {\n            setStatus(\"...\", 'speaking');\n            const blob = await (await fetch(`data:${format};base64,${base64}`)).blob();\n            const url = URL.createObjectURL(blob);\n            if (state.currentAudio) state.currentAudio.pause();\n            state.currentAudio = new Audio(url);\n\n            if (!audioContext) audioContext = new AudioContext();\n            const source = audioContext.createMediaElementSource(state.currentAudio);\n            if (!analyser) { analyser = audioContext.createAnalyser(); analyser.fftSize = 64; }\n            source.connect(analyser);\n            analyser.connect(audioContext.destination);\n\n            state.currentAudio.play();\n            state.currentAudio.onended = () =&gt; { setStatus(\"Finished speaking.\"); URL.revokeObjectURL(url); };\n        }\n\n        async function toggleRecording() {\n            if (state.isProcessing) return;\n            if (!state.sessionId) { await startSession(); return; }\n\n            if (state.isRecording) {\n                state.mediaRecorder.stop();\n                micButton.disabled = true;\n                micButton.innerHTML = '&lt;span class=\"material-symbols-outlined\"&gt;hourglass_top&lt;/span&gt;';\n                setStatus(\"Processing...\", 'thinking');\n            } else {\n                if (!state.mediaRecorder) {\n                    try {\n                        const stream = await navigator.mediaDevices.getUserMedia({ audio: { sampleRate: 16000, channelCount: 1 } });\n                        if (!audioContext) audioContext = new AudioContext();\n                        const source = audioContext.createMediaStreamSource(stream);\n                        if (!analyser) { analyser = audioContext.createAnalyser(); analyser.fftSize = 64; }\n                        source.connect(analyser);\n\n                        state.mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm;codecs=opus' });\n                        state.mediaRecorder.ondataavailable = e =&gt; state.audioChunks.push(e.data);\n                        state.mediaRecorder.onstop = uploadAudio;\n                    } catch (e) { setStatus(\"Could not access microphone.\", 'error'); return; }\n                }\n                state.audioChunks = []; state.mediaRecorder.start(); state.isRecording = true;\n                setStatus(\"Listening...\", 'recording');\n                micButton.innerHTML = '&lt;span class=\"material-symbols-outlined\"&gt;stop_circle&lt;/span&gt;';\n            }\n        }\n\n        async function uploadAudio() {\n            state.isRecording = false; state.isProcessing = true;\n            if (state.audioChunks.length === 0) { setStatus(\"No audio recorded.\"); state.isProcessing = false; micButton.disabled = false; micButton.innerHTML = '&lt;span class=\"material-symbols-outlined\"&gt;mic&lt;/span&gt;'; return; }\n            const audioBlob = new Blob(state.audioChunks, { type: 'audio/webm;codecs=opus' });\n\n            const formData = new FormData();\n            formData.append('session_id', state.sessionId);\n            formData.append('audio_blob', audioBlob, 'recording.webm');\n\n            const voiceParams = JSON.parse(voiceSelect.value);\n            for (const key in voiceParams) formData.append(key, voiceParams[key]);\n\n            try {\n                const response = await TB.api.request('talk', 'process_audio', formData, 'POST');\n                if (response.error !== 'none') {\n                    setStatus(response.info?.help_text || \"Failed to process audio.\", 'error');\n                    state.isProcessing = false; micButton.disabled = false; micButton.innerHTML = '&lt;span class=\"material-symbols-outlined\"&gt;mic&lt;/span&gt;';\n                }\n            } catch(e) {\n                 setStatus(\"Error sending audio.\", 'error'); state.isProcessing = false; micButton.disabled = false; micButton.innerHTML = '&lt;span class=\"material-symbols-outlined\"&gt;mic&lt;/span&gt;';\n            }\n        }\n\n        micButton.addEventListener('click', toggleRecording);\n        createParticles(); animateVisualizer();\n        if (window.TB.isInitialized) startSession(); else window.TB.events.on('tbjs:initialized', startSession, { once: true });\n    }\nif (window.TB?.events) {\n    if (window.TB.config?.get('appRootId')) { // A sign that TB.init might have run\n         initTalk();\n    } else {\n        window.TB.events.on('tbjs:initialized', initTalk, { once: true });\n    }\n} else {\n    // Fallback if TB is not even an object yet, very early load\n    document.addEventListener('tbjs:initialized', initTalk, { once: true }); // Custom event dispatch from TB.init\n}\n\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\"\"\"\n    return Result.html(data=html_content)\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.flows_dict","title":"<code>toolboxv2.flows_dict(s='.py', remote=False, dir_path=None, flows_dict_=None)</code>","text":"Source code in <code>toolboxv2/flows/__init__.py</code> <pre><code>def flows_dict(s='.py', remote=False, dir_path=None, flows_dict_=None):\n\n    if flows_dict_ is None:\n        flows_dict_ = {}\n    with Spinner(\"Loading flows\"):\n        # Erhalte den Pfad zum aktuellen Verzeichnis\n        if dir_path is None:\n            for ex_path in os.getenv(\"EXTERNAL_PATH_RUNNABELS\", '').split(','):\n                if not ex_path or len(ex_path) == 0:\n                    continue\n                flows_dict(s,remote,ex_path,flows_dict_)\n            dir_path = os.path.dirname(os.path.realpath(__file__))\n        to = time.perf_counter()\n        # Iteriere \u00fcber alle Dateien im Verzeichnis\n        files = os.listdir(dir_path)\n        l_files = len(files)\n        for i, file_name in enumerate(files):\n            with Spinner(f\"{file_name} {i}/{l_files}\"):\n                # \u00dcberpr\u00fcfe, ob die Datei eine Python-Datei ist\n                if file_name == \"__init__.py\":\n                    pass\n\n                elif remote and s in file_name and file_name.endswith('.gist'):\n                    # print(\"Loading from Gist :\", file_name)\n                    name_f = os.path.splitext(file_name)[0]\n                    name = name_f.split('.')[0]\n                    # publisher = name_f.split('.')[1]\n                    url = name_f.split('.')[-1]\n                    # print(\"Ent\", name)\n                    # Lade das Modul\n                    print(f\"Gist Name: {name}, URL: {url}\")\n                    try:\n                        module = GistLoader(f\"{name}/{url}\").load_module(name)\n                    #try:\n                    #    module = GistLoader(f\"{name}/{url}\")\n                    except Exception as e:\n                        print(f\"Error loading module {name} from github {url}\")\n                        print(e)\n                        continue\n\n                    # F\u00fcge das Modul der Dictionary hinzu\n                    print(f\"{hasattr(module, 'run')} and {callable(module.run)} and {hasattr(module, 'NAME')}\")\n                    if hasattr(module, 'run') and callable(module.run) and hasattr(module, 'NAME'):\n                        # print(\"Collecing :\", module.NAME)\n                        flows_dict_[module.NAME] = module.run\n                elif file_name.endswith('.py') and s in file_name:\n                    name = os.path.splitext(file_name)[0]\n                    # print(\"Loading :\", name)\n                    # Lade das Modul\n                    spec = importlib.util.spec_from_file_location(name, os.path.join(dir_path, file_name))\n                    module = importlib.util.module_from_spec(spec)\n                    try:\n                        spec.loader.exec_module(module)\n                    except Exception as e:\n                        print(\"Error loading module \", name)\n                        import traceback\n                        traceback.print_exc()\n                        continue\n\n                    # F\u00fcge das Modul der Dictionary hinzu\n                    if hasattr(module, 'run') and callable(module.run) and hasattr(module, 'NAME'):\n                        # print(\"Collecing :\", module.NAME)\n                        flows_dict_[module.NAME] = module.run\n\n        print(f\"Getting all flows took {time.perf_counter() - to:.2f} for {len(flows_dict_.keys())} elements\")\n        return flows_dict_\n</code></pre>"},{"location":"toolboxv2/#toolboxv2.TBEF","title":"<code>toolboxv2.TBEF</code>","text":"<p>Automatic generated by ToolBox v = 0.1.21</p>"},{"location":"toolboxv2/#other-exposed-items","title":"Other Exposed Items","text":""},{"location":"toolboxv2/#toolboxv2.ToolBox_over","title":"<code>toolboxv2.ToolBox_over = 'root'</code>  <code>module-attribute</code>","text":""},{"location":"usage/","title":"ToolBoxV2 Developer Guide","text":"<p>Based on the provided documentation, here's a comprehensive guide on how to use the ToolBoxV2 framework for building applications.</p>"},{"location":"usage/#introduction","title":"Introduction","text":"<p>ToolBoxV2 is a Python framework that provides a structured approach to building applications with standardized request handling and response formatting. It consists of two main components:</p> <ol> <li>RequestData Classes - For handling HTTP requests with strong typing</li> <li>Result Class - For standardized response handling and error management</li> </ol>"},{"location":"usage/#setting-up-your-application","title":"Setting Up Your Application","text":""},{"location":"usage/#creating-a-module","title":"Creating a Module","text":"<p>Start by initializing your application module:</p> <pre><code>from toolboxv2 import get_app, App, RequestData, Result\nfrom typing import Dict, Optional\n\n# Define your module\nMOD_NAME = \"YOUR_MODULE_NAME\"\nversion = \"1.0\"\nexport = get_app(\"{MODULE-NAME.SUB-MODULE}\").tb\n</code></pre>"},{"location":"usage/#registering-functions","title":"Registering Functions","text":"<p>Use the <code>@export</code> decorator to register functions within your module:</p> <pre><code>@export(mod_name=MOD_NAME, version=version)\ndef your_function():\n    # Function logic here\n    return Result.ok(data=\"Success\")\n</code></pre>"},{"location":"usage/#function-types","title":"Function Types","text":""},{"location":"usage/#standard-system-functions","title":"Standard System Functions","text":"<pre><code># Basic function with App parameter\n@export(mod_name=MOD_NAME, version=version, row=True)\ndef system_function(app: App):\n    # Implementation\n    return \"Raw return value\"  # Will be returned as-is because row=True\n\n# Function without App parameter\n@export(mod_name=MOD_NAME, version=version)\ndef function_without_app():\n    # Implementation\n    return Result.ok(data=\"Success\")\n\n# Function with arguments\n@export(mod_name=MOD_NAME, version=version)\ndef function_with_args(name: str) -&gt; Result:\n    # Implementation\n    return Result.ok(data=name)\n\n# Function returning raw data\n@export(mod_name=MOD_NAME, version=version, row=True)\ndef function_with_args_kwargs(name: str, nickname: Optional[str]=None) -&gt; str:\n    if nickname is None:\n        nickname = \"\"\n    return name + nickname  # Returned as raw string\n</code></pre>"},{"location":"usage/#async-functions","title":"Async Functions","text":"<pre><code>@export(mod_name=MOD_NAME, version=version, row=True)\nasync def async_function(app: App):\n    # Async implementation\n    result = await some_async_operation()\n    return result\n</code></pre>"},{"location":"usage/#api-endpoints","title":"API Endpoints","text":"<pre><code># API endpoint with request parameter\n@export(mod_name=MOD_NAME, api=True, version=\"1.0\", request_as_kwarg=True)\nasync def get_data(request: Optional[RequestData]=None):\n    if request:\n        query_params = request.query_params\n        # Process query parameters\n    return Result.json(data={\"status\": \"success\"})\n\n# API endpoint with App and Request parameters\n@export(mod_name=MOD_NAME, api=True, version=\"1.0\", request_as_kwarg=True)\nasync def get_user_data(app, request: Optional[RequestData]=None):\n    # Implementation using app and request\n    return Result.ok(data={\"user\": \"data\"})\n\n# API endpoint with specific HTTP methods\n@export(mod_name=MOD_NAME, api=True, version=\"1.0\", api_methods=['PUT', 'POST'])\nasync def update_data(app, data: Dict):\n    # Process the JSON data received in the request body\n    return Result.ok(data=data)\n\n# API endpoint handling form data\n@export(mod_name=MOD_NAME, api=True, version=\"1.0\", api_methods=['PUT', 'POST'])\nasync def submit_form(app, form_data: Dict):\n    # Process form data\n    return Result.ok(data=form_data)\n</code></pre>"},{"location":"usage/#working-with-request-data","title":"Working with Request Data","text":""},{"location":"usage/#accessing-request-information","title":"Accessing Request Information","text":"<pre><code>@export(mod_name=MOD_NAME, api=True, version=\"1.0\", request_as_kwarg=True)\nasync def process_request(request: Optional[RequestData]=None):\n    if request:\n        # Access method and path\n        method = request.method\n        path = request.path\n\n        # Access headers\n        user_agent = request.headers.user_agent\n        content_type = request.headers.content_type\n        custom_header = request.headers.extra_headers.get('x-custom-header')\n\n        # Access query parameters\n        query_params = request.query_params\n        search_term = query_params.get('search')\n\n        # Access form data or JSON body\n        if request.form_data:\n            form_values = request.form_data\n\n        if request.body and request.content_type == 'application/json':\n            json_data = request.body\n\n    return Result.ok(data=\"Request processed\")\n</code></pre>"},{"location":"usage/#accessing-session-information","title":"Accessing Session Information","text":"<pre><code>@export(mod_name=MOD_NAME, api=True, version=\"1.0\", request_as_kwarg=True)\nasync def get_user_session(request: Optional[RequestData]=None):\n    if request and hasattr(request, 'session'):\n        # Access session data\n        session_id = request.session.SiID\n        user_name = request.session.user_name\n        session_level = request.session.level\n\n        # Access custom session data\n        custom_data = request.session.extra_data.get('custom_key')\n\n    return Result.ok(data={\"user\": user_name})\n</code></pre>"},{"location":"usage/#working-with-results","title":"Working with Results","text":""},{"location":"usage/#creating-different-types-of-responses","title":"Creating Different Types of Responses","text":"<pre><code>@export(mod_name=MOD_NAME, api=True, version=\"1.0\")\nasync def response_examples(app):\n    # Choose the appropriate response type based on your needs\n\n    # 1. Standard success response\n    return Result.ok(\n        data={\"key\": \"value\"},\n        info=\"Operation completed successfully\"\n    )\n\n    # 2. JSON response\n    return Result.json(\n        data={\"status\": \"online\", \"version\": \"1.0\"},\n        info=\"API status retrieved\"\n    )\n\n    # 3. HTML response\n    return Result.html(\n        data=\"&lt;html&gt;&lt;body&gt;&lt;h1&gt;Welcome&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\",\n        info=\"Page rendered\"\n    )\n\n    # 4. Text response\n    return Result.text(\n        text_data=\"Plain text content\",\n        content_type=\"text/plain\"\n    )\n\n    # 5. Binary file response\n    return Result.binary(\n        data=binary_data,\n        content_type=\"application/pdf\",\n        download_name=\"report.pdf\"\n    )\n\n    # 6. Redirect response\n    return Result.redirect(\n        url=\"/dashboard\",\n        status_code=302\n    )\n</code></pre>"},{"location":"usage/#error-handling","title":"Error Handling","text":"<pre><code>@export(mod_name=MOD_NAME, version=version)\ndef process_with_validation(user_input):\n    # Validate input\n    if not user_input:\n        return Result.default_user_error(\n            info=\"Empty input is not allowed\",\n            exec_code=400\n        )\n\n    # Process valid input\n    try:\n        processed_data = process_data(user_input)\n        return Result.ok(data=processed_data)\n    except Exception as e:\n        return Result.default_internal_error(\n            info=f\"Processing error: {str(e)}\",\n            exec_code=500\n        )\n</code></pre>"},{"location":"usage/#using-lazy_return-for-simplified-error-handling","title":"Using lazy_return for Simplified Error Handling","text":"<pre><code>@export(mod_name=MOD_NAME, version=version)\ndef validate_and_process(data):\n    # Validate data\n    validation_result = validate_data(data)\n\n    # If validation fails, return the error\n    # If validation succeeds, return the processed data\n    return validation_result.lazy_return(\n        'user',  # Use user error if validation fails\n        data={\"processed\": True, \"original\": data}  # Return this if successful\n    )\n</code></pre>"},{"location":"usage/#streaming-responses","title":"Streaming Responses","text":"<pre><code>@export(mod_name=MOD_NAME, api=True, version=\"1.0\")\nasync def stream_data():\n    async def generator():\n        for i in range(10):\n            yield {\"chunk\": i}\n            await asyncio.sleep(0.5)\n\n    async def cleanup():\n        # Cleanup resources when the stream closes\n        print(\"Stream closed, performing cleanup\")\n\n    return Result.stream(\n        stream_generator=generator(),\n        info=\"Streaming data chunks\",\n        cleanup_func=cleanup\n    )\n</code></pre>"},{"location":"usage/#advanced-features","title":"Advanced Features","text":""},{"location":"usage/#caching","title":"Caching","text":"<pre><code># Memory caching\n@export(mod_name=MOD_NAME, version=version, memory_cache=True,\n        memory_cache_max_size=100, memory_cache_ttl=300)\ndef cached_function(key):\n    # Expensive operation here\n    return Result.ok(data=compute_expensive_data(key))\n\n# File caching\n@export(mod_name=MOD_NAME, version=version, file_cache=True)\ndef file_cached_function(key):\n    # Expensive operation here\n    return Result.ok(data=compute_expensive_data(key))\n</code></pre>"},{"location":"usage/#background-functions","title":"Background Functions","text":"<pre><code># Memory caching\n@export(mod_name=MOD_NAME, version=version)\ndef function_with_log_running_bg_call():\n    # Expensive operation here\n    def sync_bg_function():\n        print(\"running in gb\")\n        compute_expensive_function()\n\n    return Result.ok(data=\"Starting processing\").task(sync_bg_function)\n\n# File caching\n@export(mod_name=MOD_NAME, version=version)\nasync def function_with_log_running_bg_call():\n    # Expensive operation here\n    async def bg_function():\n        print(\"running in gb\")\n        await compute_expensive_function()\n    return Result.ok(data=\"Starting processing\").task(bg_function())\n</code></pre>"},{"location":"usage/#lifecycle-management","title":"Lifecycle Management","text":"<pre><code># Initialization function\n@export(mod_name=MOD_NAME, version=version, initial=True)\ndef initialize_module(app: App):\n    # Called when the module is loaded\n    print(f\"Initializing {MOD_NAME} module\")\n    # Set up resources, connections, etc.\n    return Result.ok(info=\"Module initialized\")\n\n# Exit function\n@export(mod_name=MOD_NAME, version=version, exit_f=True)\ndef cleanup_module(app: App):\n    # Called when the application is shutting down\n    print(f\"Cleaning up {MOD_NAME} module\")\n    # Release resources, close connections, etc.\n    return Result.ok(info=\"Module cleaned up\")\n</code></pre>"},{"location":"usage/#prepost-compute-functions","title":"Pre/Post Compute Functions","text":"<pre><code>def log_before_execution(func, *args, **kwargs):\n    print(f\"Executing {func.__name__} with args: {args}, kwargs: {kwargs}\")\n    return args, kwargs\n\ndef log_after_execution(result, func, *args, **kwargs):\n    print(f\"Function {func.__name__} returned: {result}\")\n    return result\n\n@export(mod_name=MOD_NAME, version=version,\n        pre_compute=log_before_execution,\n        post_compute=log_after_execution)\ndef monitored_function(name):\n    # Function logic\n    return Result.ok(data=f\"Hello, {name}!\")\n</code></pre>"},{"location":"usage/#url-patterns-for-api-endpoints","title":"URL Patterns for API Endpoints","text":"<p>API endpoints are accessible using the following URL patterns:</p> <ul> <li>Regular API: <code>/api/MOD_NAME/{function_name}?param1=value1&amp;param2=value2</code></li> <li>Server-Sent Events (streaming): <code>/sse/MOD_NAME/{function_name}?param1=value1&amp;param2=value2</code></li> </ul>"},{"location":"utils/","title":"ToolBoxV2: The <code>App</code> Class","text":"<p>The <code>App</code> class is the central singleton instance in ToolBoxV2, responsible for managing the application's lifecycle, configuration, module loading, and core functionalities. It's typically accessed via the <code>get_app()</code> utility function.</p>"},{"location":"utils/#initialization","title":"Initialization","text":"<p>The <code>App</code> instance is initialized with a <code>prefix</code> and <code>AppArgs</code> (command-line arguments).</p> <pre><code>from toolboxv2 import App, AppArgs, get_app\n\n# Example: Initialize or get the App instance\n# The prefix helps differentiate multiple App instances if needed,\n# and is often used in directory naming.\nargs = AppArgs().default() # Or parsed from sys.argv in __main__.py\napp_instance = get_app(prefix=\"my_app_instance\", args=args)\n\n# Accessing key attributes:\nprint(f\"App ID: {app_instance.id}\")\nprint(f\"Version: {app_instance.version}\")\nprint(f\"Start Directory: {app_instance.start_dir}\")\nprint(f\"Data Directory: {app_instance.data_dir}\")\nprint(f\"Config Directory: {app_instance.config_dir}\")\nprint(f\"Debug Mode: {app_instance.debug}\")\n</code></pre>"},{"location":"utils/#key-initialization-steps","title":"Key Initialization Steps:","text":"<ol> <li> <p>System &amp; Paths:</p> <ul> <li>Determines the operating system (<code>system_flag</code>).</li> <li>Sets the <code>start_dir</code> to the application's root directory.</li> <li>Resolves the <code>prefix</code>:<ul> <li>If no prefix is provided, it attempts to load the last used prefix from <code>.data/last-app-prefix.txt</code>.</li> <li>If a prefix is provided, it's saved to this file for future use.</li> </ul> </li> <li>Constructs the <code>app_id</code> (e.g., <code>prefix-hostname</code>).</li> <li>Sets up <code>data_dir</code>, <code>config_dir</code>, and <code>info_dir</code> based on the <code>app_id</code> (e.g., <code>./.data/prefix-hostname/</code>).</li> <li>Sets up <code>appdata</code> directory (OS-specific application data folder).</li> </ul> </li> <li> <p>Logging:</p> <ul> <li>Initializes a logger (<code>app.logger</code>). The logging level and output (terminal/file) can vary based on the <code>prefix</code> (e.g., \"test\", \"live\", \"debug\") and the <code>--debug</code> CLI argument.</li> </ul> </li> <li> <p>Configuration:</p> <ul> <li>Loads application configuration using <code>FileHandler</code> from a file typically named <code>app_id.config</code> in the <code>config_dir</code>.</li> <li>Defines default configuration <code>keys</code> and <code>defaults</code> (e.g., for macros, helpers, debug status).</li> </ul> </li> <li> <p>Core Attributes:</p> <ul> <li><code>version</code>: ToolBoxV2 version, read from <code>pyproject.toml</code>.</li> <li><code>debug</code>: Boolean, controlled by CLI args and config.</li> <li><code>dev_modi</code>: Boolean, development mode status from config.</li> <li><code>functions</code>: A dictionary to store registered functions from modules.</li> <li><code>modules</code>: A dictionary to store loaded module objects.</li> <li><code>interface_type</code>: Default <code>ToolBoxInterfaces.native</code>.</li> <li><code>alive</code>: Boolean, controls the main application loop.</li> <li><code>args_sto</code>: Stores the parsed <code>AppArgs</code>.</li> <li><code>loop</code>: The asyncio event loop (initialized later or if already running).</li> <li><code>session</code>: A <code>Session</code> object for managing user/remote session state.</li> </ul> </li> <li> <p>Conditional Actions (based on <code>AppArgs</code>):</p> <ul> <li><code>args.init</code>: If true, adds <code>start_dir</code> to <code>sys.path</code>.</li> <li>The <code>__main__.py</code> script handles other arguments like <code>--update</code>, <code>--get-version</code>, etc., by calling <code>App</code> methods or other utilities.</li> </ul> </li> </ol>"},{"location":"utils/#core-functionalities","title":"Core Functionalities","text":""},{"location":"utils/#module-management","title":"Module Management","text":"<ul> <li> <p><code>load_mod(mod_name: str, spec='app', mlm='I', **kwargs)</code> / <code>save_load(modname, spec='app')</code>:</p> <ul> <li>Loads a module into the application.</li> <li><code>spec</code> (specification): Used to namespace or categorize the module instance (e.g., 'app' for general, or a specific session ID).</li> <li>Supports different loading mechanisms (<code>mlm</code>):<ul> <li><code>'I'</code>: In-place load (imports the Python module directly). This is the default.</li> <li><code>'C'</code>: Copies the module file to a runtime directory before loading (less common).</li> </ul> </li> <li>Handles <code>ModuleNotFoundError</code> by attempting to guide the user (e.g., install via <code>CloudM</code> or <code>pip</code>).</li> <li>Registers the module's exported functions and/or its <code>Tools</code> class instance.</li> <li>Can reload modules if they are already loaded. <pre><code># Load the 'MyModule'\nmy_module_instance = app_instance.load_mod(\"MyModule\")\n\n# Or if it's a Tool-based module:\n# my_tool_instance = app_instance.load_mod(\"MyToolModule\")\n</code></pre></li> </ul> </li> <li> <p><code>get_mod(name: str, spec='app') -&gt; ModuleType | MainToolType</code>:</p> <ul> <li>Retrieves a loaded module instance. If the module isn't loaded, it attempts to load it. <pre><code>db_mod = app_instance.get_mod(\"DB\")\nif db_mod:\n    db_mod.some_db_function()\n</code></pre></li> </ul> </li> <li> <p><code>remove_mod(mod_name: str, spec='app', delete=True)</code> / <code>a_remove_mod(...)</code> (async):</p> <ul> <li>Unloads a module, calling its <code>on_exit</code> functions if defined.</li> <li><code>delete=True</code> removes it completely from the <code>functions</code> registry.</li> </ul> </li> <li> <p><code>reload_mod(mod_name: str, spec='app', ...)</code>:</p> <ul> <li>Reloads an existing module. Useful for development.</li> </ul> </li> <li> <p><code>watch_mod(mod_name: str, ...)</code>:</p> <ul> <li>Monitors a module's source file(s) for changes and automatically reloads it. <pre><code># In development, watch 'MyDevModule' for changes\napp_instance.watch_mod(\"MyDevModule\")\n</code></pre></li> </ul> </li> <li> <p><code>load_all_mods_in_file(working_dir=\"mods\")</code> / <code>a_load_all_mods_in_file(...)</code> (async):</p> <ul> <li>Scans the specified directory (default <code>./mods/</code>) and loads all valid Python modules found.</li> </ul> </li> </ul>"},{"location":"utils/#to-isaa","title":"To isaa","text":""},{"location":"utils/#function-registration-and-execution","title":"Function Registration and Execution","text":"<ul> <li> <p><code>@app.tb(...)</code> Decorator (via <code>_create_decorator</code>):</p> <ul> <li>The primary way functions are registered with ToolBoxV2. See <code>example_mod.md</code> for details on usage.</li> <li>This decorator populates the <code>app.functions</code> dictionary.</li> </ul> </li> <li> <p><code>get_function(name: Enum | tuple, metadata=False, state=True, specification='app', ...)</code>:</p> <ul> <li>Retrieves a registered function.</li> <li><code>name</code>: Can be an Enum (from <code>all_functions_enums.py</code>) or a <code>(module_name, function_name)</code> tuple.</li> <li><code>metadata=True</code>: Returns a tuple <code>(function_data_dict, callable_function)</code>.</li> <li><code>state=True</code>: Returns a stateful version of the function (bound to its module instance if applicable).</li> <li><code>state=False</code>: Returns the raw, stateless function.</li> <li><code>specification</code>: The context/instance spec to get the function for.</li> </ul> </li> <li> <p><code>run_any(mod_function_name, ..., get_results=False, **kwargs)</code> / <code>a_run_any(...)</code> (async):</p> <ul> <li>Executes a registered function by its name (Enum or tuple).</li> <li>Handles argument passing, stateful/stateless execution, and error wrapping into a <code>Result</code> object.</li> <li><code>get_results=True</code>: Returns the <code>Result</code> object itself.</li> <li><code>get_results=False</code> (default): Returns the <code>data</code> payload from the <code>Result</code> object if successful.</li> <li>Automatically handles running the function's pre/post compute hooks and caching if configured via <code>@app.tb</code>. <pre><code># Synchronous execution\nresult_data = app_instance.run_any((\"MyModule\", \"my_function\"), arg1=\"hello\")\nfull_result_obj = app_instance.run_any((\"MyModule\", \"my_function\"), arg1=\"hello\", get_results=True)\n\n# Asynchronous execution\nasync_result_data = await app_instance.a_run_any((\"MyAsyncModule\", \"my_async_function\"))\n</code></pre></li> </ul> </li> <li> <p><code>run_http(mod_function_name, function_name=None, method=\"GET\", ...)</code> (async):</p> <ul> <li>Executes a function on a remote ToolBoxV2 instance via HTTP, using the app's <code>session</code> object.</li> </ul> </li> </ul>"},{"location":"utils/#application-lifecycle","title":"Application Lifecycle","text":"<ul> <li><code>exit()</code> / <code>a_exit()</code> (async):<ul> <li>Gracefully shuts down the application.</li> <li>Calls <code>on_exit</code> functions for all loaded modules.</li> <li>Saves configuration.</li> <li>Stops the main application loop (<code>alive = False</code>).</li> <li>Cleans up threads and the event loop if applicable.</li> </ul> </li> </ul>"},{"location":"utils/#utilities","title":"Utilities","text":"<ul> <li><code>print(text, *args, **kwargs)</code> / <code>sprint(text, *args, **kwargs)</code>:<ul> <li>Styled print functions, prepending <code>System$[app_id]:</code>. <code>sprint</code> is often used for more verbose/system-level messages and can be silenced.</li> </ul> </li> <li><code>debug_rains(e: Exception)</code>: If <code>app.debug</code> is true, prints a full traceback and re-raises the exception.</li> <li><code>set_flows(r: dict)</code> / <code>run_flows(name: str, **kwargs)</code>: Manages and executes predefined application flows (sequences of operations).</li> <li><code>get_username()</code> / <code>set_username(username: str)</code>: Manages the application's user identity.</li> <li><code>save_autocompletion_dict()</code> / <code>get_autocompletion_dict()</code>: Saves/loads a dictionary of modules and their functions for autocompletion features.</li> <li><code>save_registry_as_enums(directory: str, filename: str)</code>: Generates an <code>all_functions_enums.py</code>-like file from the currently registered functions.</li> <li><code>execute_all_functions(...)</code> / <code>a_execute_all_functions(...)</code> (async):<ul> <li>Runs all registered testable functions (marked with <code>test=True</code> in <code>@app.tb</code> or having <code>samples</code>).</li> <li>Useful for integration testing and profiling.</li> <li>Can filter by module (<code>m_query</code>) and function (<code>f_query</code>).</li> <li>Supports profiling via <code>cProfile</code>.</li> </ul> </li> <li><code>run_bg_task(task: Callable)</code>:<ul> <li>Runs a synchronous or asynchronous task in a separate background thread with its own event loop. Ensures proper handling of nested asyncio operations within the task.</li> </ul> </li> </ul>"},{"location":"utils/#session-management-appsession","title":"Session Management (<code>app.session</code>)","text":"<p>The <code>app.session</code> attribute holds an instance of the <code>Session</code> class (from <code>toolboxv2.utils.system.session</code>). It's used for: *   Authenticating with a remote ToolBoxV2 service (e.g., SimpleCore Hub). *   Making authenticated HTTP requests (<code>session.fetch</code>, <code>session.upload_file</code>, <code>session.download_file</code>). *   Manages JWT claims and private key authentication.</p> <pre><code># Example: Making an authenticated API call\n# Assumes app.session is already authenticated\nresponse_data = await app_instance.session.fetch(\"/api/MyRemoteModule/get_info\")\njson_payload = await response_data.json()\n</code></pre> <pre><code>### 2. `cli.md` - Documenting the Command Line Interface\n\nThis should explain how to use the `tb` (or `python -m toolboxv2`) command-line tool, detailing its arguments and their effects.\n\n```markdown\n# ToolBoxV2: Command Line Interface (CLI)\n\nToolBoxV2 provides a command-line interface (CLI) for managing and running applications. It's typically invoked as `tb` (if installed globally or via an alias) or `python -m toolboxv2`.\n\n## General Usage\n\n```bash\npython -m toolboxv2 [options] [sub-commands]\n# or\ntb [options] [sub-commands]\n</code></pre> <p>The CLI script (<code>__main__.py</code>) performs the following main steps: 1.  Parses command-line arguments. 2.  Initializes the <code>App</code> instance via <code>setup_app()</code> (which calls <code>get_app()</code>). 3.  Handles various options to:     *   Manage application data and configuration.     *   Control application modes (background, proxy, debug).     *   Load modules and manage their state.     *   Run tests or profilers.     *   Execute specific application flows or commands.</p>"},{"location":"utils/#key-cli-arguments","title":"Key CLI Arguments","text":"<p>The following are some of the primary arguments available. Use <code>tb -h</code> or <code>python -m toolboxv2 -h</code> for a full list.</p> <ul> <li> <p>Instance and Mode:</p> <ul> <li><code>-init [name]</code>: Initializes ToolBoxV2 with a specific instance name (default: <code>main</code>).</li> <li><code>-n, --name &lt;name&gt;</code>: Specifies an ID for the ToolBox instance (default: <code>main</code>). This affects data/config directory names.</li> <li><code>-m, --modi &lt;mode&gt;</code>: Starts a ToolBoxV2 interface/flow (e.g., <code>cli</code>, <code>bg</code>, or custom flows). Default is usually \"cli\".</li> <li><code>--debug</code>: Starts the application in debug mode (more verbose logging, potentially different behavior).</li> <li><code>--remote</code>: Starts in remote mode, often for connecting to a proxy or another instance.</li> <li><code>-bg, --background-application</code>: Starts an interface in the background as a detached process.</li> <li><code>-bgr, --background-application-runner</code>: Runs the background application logic in the current terminal (for daemons).</li> <li><code>-fg, --live-application</code>: Starts a proxy interface, connecting to a background daemon.</li> <li><code>--kill</code>: Kills the currently running local ToolBoxV2 instance (matching the <code>-m &lt;mode&gt;</code> and <code>-n &lt;name&gt;</code>).</li> </ul> </li> <li> <p>Module and Version Management:</p> <ul> <li><code>-l, --load-all-mod-in-files</code>: Loads all modules found in the <code>mods/</code> directory on startup.</li> <li><code>-sfe, --save-function-enums-in-file</code>: Generates/updates the <code>all_functions_enums.py</code> file based on loaded modules. Often used with <code>-l</code>.</li> <li><code>-v, --get-version</code>: Prints the version of ToolBoxV2 and all loaded modules.</li> <li><code>-i, --install &lt;name&gt;</code>: Installs a module or interface (likely via <code>CloudM</code> module).</li> <li><code>-r, --remove &lt;name&gt;</code>: Uninstalls a module or interface.</li> <li><code>-u, --update &lt;name_or_main&gt;</code>: Updates a module/interface or the core ToolBoxV2 (<code>main</code>).</li> </ul> </li> <li> <p>Development and Testing:</p> <ul> <li><code>--test</code>: Runs all unit tests (typically discovers and runs tests in the <code>tests/</code> directory).</li> <li><code>--profiler</code>: Runs all registered testable functions and profiles their execution using <code>cProfile</code>.</li> <li><code>--ipy</code>: Starts an IPython session with the ToolBoxV2 app pre-loaded. Provides magic commands like <code>%tb save/loadX/load/open/inject</code>.</li> </ul> </li> <li> <p>Service Management (<code>--sm</code>):</p> <ul> <li>Provides a sub-menu for managing ToolBoxV2 as a system service (Linux/systemd or Windows Startup).</li> <li>Options: Init, Start/Stop/Restart, Status, Uninstall, Show/Hide console window (Windows).</li> </ul> </li> <li> <p>Log Management (<code>--lm</code>):</p> <ul> <li>Provides a sub-menu for managing log files (e.g., removing or unstyling logs by date/level).</li> </ul> </li> <li> <p>Data and Configuration Management:</p> <ul> <li><code>--delete-config-all</code>: Deletes all configuration files. Use with caution!</li> <li><code>--delete-data-all</code>: Deletes all data folders. Use with caution!</li> <li><code>--delete-config</code>: Deletes the configuration file for the named instance.</li> <li><code>--delete-data</code>: Deletes the data folder for the named instance.</li> </ul> </li> <li> <p>Network Configuration (for interfaces):</p> <ul> <li><code>-p, --port &lt;port&gt;</code>: Specifies the port for an interface (default: <code>5000</code> or <code>6587</code> for background).</li> <li><code>-w, --host &lt;host&gt;</code>: Specifies the host for an interface (default: <code>0.0.0.0</code>).</li> </ul> </li> <li> <p>Direct Command Execution:</p> <ul> <li><code>-c, --command &lt;module_name&gt; &lt;function_name&gt; [arg1 arg2 ...]</code> (can be repeated): Executes a specific function.</li> <li><code>--kwargs &lt;key=value&gt;</code> (can be repeated): Provides keyword arguments for commands specified with <code>-c</code>.</li> </ul> </li> <li> <p>Conda Integration:</p> <ul> <li><code>conda [conda_args...]</code>: Special argument to pass commands directly to a <code>conda_runner.py</code> script (e.g., <code>tb conda env list</code>).</li> </ul> </li> <li> <p>API Runner:</p> <ul> <li><code>api [api_args...]</code>: Special argument to invoke <code>cli_api_runner.py</code>, likely for direct API interactions or testing.</li> </ul> </li> <li> <p>GUI:</p> <ul> <li><code>gui</code>: Starts the GUI version of ToolBoxV2 (imports and runs <code>toolboxv2.__gui__.start</code>).</li> </ul> </li> </ul>"},{"location":"utils/#cli-execution-flow-__main__py","title":"CLI Execution Flow (<code>__main__.py</code>)","text":"<ol> <li>Argument Parsing: <code>parse_args()</code> uses <code>argparse</code> to define and parse all CLI arguments.</li> <li>App Setup (<code>setup_app()</code>):<ul> <li>Initializes the <code>App</code> instance using <code>get_app()</code> with the parsed arguments and name.</li> <li>Sets up PID file for the current process.</li> <li>Optionally silences <code>app.sprint</code> if not in debug/verbose mode.</li> <li>Loads all modules if <code>-l</code> is specified.</li> <li>Handles <code>--update</code> logic.</li> </ul> </li> <li>Background/Live Application Handling:<ul> <li>If <code>-bgr</code>: Initializes <code>DaemonApp</code>.</li> <li>If <code>-bg</code>: Starts the application as a detached background process using <code>subprocess.Popen</code>.</li> <li>If <code>-fg</code> (live-application): Attempts to connect to a background daemon using <code>ProxyApp</code>.</li> </ul> </li> <li>Action Dispatching: Based on the parsed arguments, it performs actions like:<ul> <li>Module installation (<code>--install</code>).</li> <li>Log management (<code>--lm</code>).</li> <li>Service management (<code>--sm</code>).</li> <li>Saving function enums (<code>-sfe</code>).</li> <li>Printing versions (<code>-v</code>).</li> <li>Running the profiler (<code>--profiler</code>).</li> <li>Running flows based on <code>--modi</code>.</li> <li>Handling Docker commands (<code>--docker</code>).</li> <li>Killing an existing instance (<code>--kill</code>).</li> <li>Executing direct commands (<code>-c</code>).</li> </ul> </li> <li>Cleanup: Removes the PID file and calls <code>app.a_exit()</code> before exiting.</li> </ol>"},{"location":"utils/#example-cli-usage","title":"Example CLI Usage","text":"<pre><code># Get version information\npython -m toolboxv2 -v\n\n# Load all modules and save function enums\npython -m toolboxv2 -l -sfe\n\n# Run a specific function in MyModule\npython -m toolboxv2 -c MyModule my_function arg_value --kwargs param_name=kwarg_value\n\n# Start the application with a custom flow named 'web_server' in debug mode\npython -m toolboxv2 -m web_server --debug -n my_web_instance\n\n# Start a background daemon for the 'bg_processing' flow\npython -m toolboxv2 -m bg_processing -bg -n background_processor\n\n# Connect to the background daemon with a live proxy application\npython -m toolboxv2 -m cli -fg -n background_processor\n\n# Kill the 'web_server' modi instance named 'my_web_instance'\npython -m toolboxv2 -m web_server --kill -n my_web_instance\n</code></pre> <pre><code>### 3. `example_mod.md` - Documenting Module Creation\n\nThis needs to be updated to accurately reflect the `@app.tb(...)` decorator from `toolbox.py` and the `Result` and `RequestData` classes from `types.py`.\n\n```markdown\n# ToolBoxV2: Creating Modules\n\nToolBoxV2 modules are Python files or packages that extend the framework's functionality. They can define simple functions, stateful tools (classes inheriting from `MainTool`), or API endpoints.\n\n## Basic Module Structure\n\nA typical ToolBoxV2 module (`.py` file) includes:\n\n1.  **Imports:** Necessary libraries and ToolBoxV2 components.\n2.  **Module Metadata (Optional but Recommended):**\n    *   `Name` (or `name`): A string defining the module's canonical name.\n    *   `version`: A string for the module's version (e.g., \"1.0.0\").\n3.  **Function/Class Definitions:** The core logic of your module.\n4.  **Exporting Functions:** Functions are made available to ToolBoxV2 using the `@export` decorator (which is an alias for `app.tb`).\n\n## The `@export` Decorator (`app.tb`)\n\nThe `@export` decorator is the primary mechanism for registering functions and configuring their behavior within ToolBoxV2. It's obtained from an `App` instance.\n\n```python\nfrom toolboxv2 import get_app, App, Result, RequestData, MainTool\nfrom toolboxv2.utils.system.types import ToolBoxInterfaces # For specific interface types\nfrom typing import Optional, Dict, Any, List\nimport asyncio\n\n# Get the application instance (singleton)\n# The 'prefix' for get_app here is often the module's own name,\n# though the decorator will use its 'mod_name' parameter.\napp = get_app(\"MyModule\")\nexport = app.tb # Alias the decorator for convenience\n\n# --- Module Metadata (Best Practice) ---\nName = \"MyModule\"  # Used by the decorator if mod_name is not specified\nversion = \"1.0.1\"\n\n# --- Example Functions ---\n\n@export(mod_name=Name, version=version, helper=\"A simple greeting function.\")\ndef greet(name: str) -&gt; str:\n    \"\"\"Returns a greeting message.\"\"\"\n    return f\"Hello, {name} from MyModule!\"\n\n@export(mod_name=Name, version=version, row=True, helper=\"Returns raw data without Result wrapping.\")\ndef get_raw_data() -&gt; dict:\n    \"\"\"Demonstrates returning raw data.\"\"\"\n    return {\"key\": \"value\", \"number\": 123}\n\n@export(mod_name=Name, version=version, initial=True, helper=\"Runs when the module is first loaded.\")\ndef on_module_load():\n    \"\"\"Initialization logic for this module.\"\"\"\n    app.print(f\"{Name} module has been loaded and initialized!\")\n    # return Result.ok(info=\"MyModule initialized successfully\") # Optional: return a Result\n\n@export(mod_name=Name, version=version, exit_f=True, helper=\"Runs when the application is shutting down.\")\nasync def on_module_exit():\n    \"\"\"Cleanup logic for this module.\"\"\"\n    await asyncio.sleep(0.1) # Simulate async cleanup\n    app.print(f\"{Name} module is cleaning up.\")\n    # return Result.ok(info=\"MyModule cleaned up.\") # Optional\n\n@export(mod_name=Name, version=version, api=True, api_methods=['GET'], request_as_kwarg=True,\n        helper=\"An example API endpoint.\")\nasync def my_api_endpoint(request: Optional[RequestData] = None) -&gt; Result:\n    \"\"\"\n    Handles GET requests to /api/MyModule/my_api_endpoint.\n    Accesses request details if provided.\n    \"\"\"\n    if request:\n        app.print(f\"API request received: {request.request.method} {request.request.path}\")\n        app.print(f\"Query Params: {request.request.query_params}\")\n        app.print(f\"User from session: {request.session.user_name}\")\n    return Result.json(data={\"message\": \"API call successful!\", \"module_version\": version})\n\n@export(mod_name=Name, version=version, memory_cache=True, memory_cache_ttl=60)\ndef expensive_calculation(param: int) -&gt; int:\n    \"\"\"\n    An example of a function whose results will be cached in memory for 60 seconds.\n    \"\"\"\n    app.print(f\"Performing expensive calculation for {param}...\")\n    time.sleep(2) # Simulate work\n    return param * param\n\n# Example of a more complex function using App instance and returning a Result\n@export(mod_name=Name, version=version)\ndef process_data_with_app(app_instance: App, data_id: int) -&gt; Result:\n    \"\"\"\n    This function automatically receives the 'App' instance if its first parameter is type-hinted as 'App'.\n    This is determined by the 'state=True' logic in the decorator if 'app' is the first param.\n    Alternatively, use state=False for stateless functions.\n    \"\"\"\n    if not isinstance(app_instance, App): # Should always be App if first param is 'app'\n        return Result.default_internal_error(\"App instance not correctly passed.\")\n\n    # Use app_instance for logging, accessing config, other modules, etc.\n    app_instance.logger.info(f\"Processing data_id: {data_id} in {Name}\")\n    if data_id &lt; 0:\n        return Result.default_user_error(info=\"Data ID cannot be negative.\")\n    return Result.ok(data={\"processed_id\": data_id, \"status\": \"completed\"})\n</code></pre>"},{"location":"utils/#export-decorator-parameters","title":"<code>@export</code> Decorator Parameters:","text":"<ul> <li><code>name</code> (str, optional): The name to register the function under. Defaults to the function's actual name.</li> <li><code>mod_name</code> (str): The name of the module this function belongs to. If not provided, it tries to infer from <code>func.__module__</code> or a global <code>Name</code> in the module.</li> <li><code>version</code> (str, optional): Version string for this function/feature. Combined with the app's version.</li> <li><code>helper</code> (str, optional): A docstring or help text for the function.</li> <li><code>api</code> (bool, default <code>False</code>): If <code>True</code>, exposes this function as an HTTP API endpoint.<ul> <li>The URL pattern is typically <code>/api/&lt;mod_name&gt;/&lt;func_name&gt;</code>.</li> <li>For streaming, <code>/sse/&lt;mod_name&gt;/&lt;func_name&gt;</code>.</li> </ul> </li> <li><code>api_methods</code> (List[str], optional): Specifies allowed HTTP methods (e.g., <code>['GET', 'POST']</code>). Defaults to <code>['AUTO']</code> (GET if no body params, POST if body params).</li> <li><code>request_as_kwarg</code> (bool, default <code>False</code>): If <code>True</code> and <code>api=True</code>, the function will receive a <code>request: RequestData</code> keyword argument if it's defined in its signature.</li> <li><code>row</code> (bool, default <code>False</code>): If <code>True</code>, the function's raw return value is used directly. If <code>False</code> (default), the return value is automatically wrapped in a <code>Result.ok()</code> object if it's not already a <code>Result</code> or <code>ApiResult</code>.</li> <li><code>initial</code> (bool, default <code>False</code>): If <code>True</code>, this function is added to the module's \"on_start\" list and is called when the module is loaded by the <code>App</code> instance (if the module instance is a <code>MainTool</code> or similar, or if called directly).</li> <li><code>exit_f</code> (bool, default <code>False</code>): If <code>True</code>, this function is added to the module's \"on_exit\" list and is called when the <code>App</code> instance is shutting down or the module is removed.</li> <li><code>state</code> (bool, optional):<ul> <li>If <code>None</code> (default): Automatically determined. If the first parameter of the decorated function is named <code>self</code> or <code>app</code> (and type-hinted as <code>App</code>), <code>state</code> is considered <code>True</code>. Otherwise <code>False</code>.</li> <li>If <code>True</code>: The function is considered stateful. If its first parameter is <code>self</code>, it's assumed to be a method of a class instance (e.g., a <code>MainTool</code> subclass). If <code>app</code>, the <code>App</code> instance is passed.</li> <li>If <code>False</code>: The function is treated as stateless.</li> </ul> </li> <li><code>test</code> (bool, default <code>True</code>): Marks the function as testable. Used by <code>app.execute_all_functions()</code>.</li> <li><code>samples</code> (List[Dict[str, Any]], optional): A list of sample keyword arguments to be used when testing the function with <code>app.execute_all_functions()</code>.</li> <li><code>memory_cache</code> (bool, default <code>False</code>): Enables in-memory caching for the function's results.</li> <li><code>memory_cache_ttl</code> (int, default <code>300</code>): Time-to-live in seconds for memory cache entries.</li> <li><code>memory_cache_max_size</code> (int, default <code>100</code>): Max number of entries in the memory cache.</li> <li><code>file_cache</code> (bool, default <code>False</code>): Enables file-based caching for the function's results. (Stored in <code>app.data_dir/cache/...</code>).</li> <li><code>restrict_in_virtual_mode</code> (bool, default <code>False</code>): If <code>True</code>, restricts function in certain virtualized/proxied modes.</li> <li><code>level</code> (int, default <code>-1</code>): A general-purpose level or priority for the function.</li> <li><code>pre_compute</code> (Callable, optional): A function <code>(func, *args, **kwargs) -&gt; (args, kwargs)</code> called before the main function executes. It can modify args/kwargs.</li> <li><code>post_compute</code> (Callable, optional): A function <code>(result, func, *args, **kwargs) -&gt; result</code> called after the main function executes. It can modify the result.</li> <li><code>interface</code> (ToolBoxInterfaces | str, optional): Specifies the intended interface type (e.g., <code>ToolBoxInterfaces.cli</code>, <code>ToolBoxInterfaces.api</code>). Defaults to \"tb\".</li> </ul>"},{"location":"utils/#result-and-apiresult-objects","title":"<code>Result</code> and <code>ApiResult</code> Objects","text":"<ul> <li>Modules should typically return <code>Result</code> objects (or <code>ApiResult</code> for API endpoints) to provide standardized responses including success/error status, data payload, and informational messages.</li> <li>The <code>toolboxv2.utils.system.types.Result</code> class provides factory methods:<ul> <li><code>Result.ok(data=..., info=...)</code></li> <li><code>Result.json(data=..., info=...)</code> (for <code>api=True</code> functions)</li> <li><code>Result.html(data=..., info=...)</code></li> <li><code>Result.text(text_data=..., info=...)</code></li> <li><code>Result.binary(data=..., content_type=..., download_name=...)</code></li> <li><code>Result.redirect(url=..., status_code=...)</code></li> <li><code>Result.stream(stream_generator=..., info=..., cleanup_func=...)</code> (for SSE)</li> <li><code>Result.default_user_error(info=..., exec_code=...)</code></li> <li><code>Result.default_internal_error(info=..., exec_code=...)</code></li> <li><code>Result.custom_error(data=..., info=..., exec_code=...)</code></li> </ul> </li> <li>The <code>Result</code> object has a <code>task(background_task_callable)</code> method to schedule a background task to run after the main function returns.</li> </ul>"},{"location":"utils/#requestdata-object","title":"<code>RequestData</code> Object","text":"<ul> <li>For API functions (<code>api=True</code>) with <code>request_as_kwarg=True</code>, if the function signature includes <code>request: Optional[RequestData] = None</code>, it will receive an instance of <code>toolboxv2.utils.system.types.RequestData</code>.</li> <li><code>RequestData</code> provides access to:<ul> <li><code>request.method</code>, <code>request.path</code></li> <li><code>request.headers</code> (an instance of <code>Headers</code>, e.g., <code>request.headers.user_agent</code>, <code>request.headers.hx_request</code>)</li> <li><code>request.query_params</code> (dict)</li> <li><code>request.form_data</code> (dict, if applicable)</li> <li><code>request.body</code> (parsed JSON if <code>content_type</code> is <code>application/json</code>, otherwise raw bytes/str)</li> <li><code>session.SiID</code>, <code>session.user_name</code>, <code>session.level</code> (from the current user's session)</li> </ul> </li> </ul>"},{"location":"utils/#creating-a-maintool-based-module","title":"Creating a <code>MainTool</code>-based Module","text":"<p>For more complex, stateful modules, you can create a class that inherits from <code>toolboxv2.utils.system.main_tool.MainTool</code>.</p> <pre><code>from toolboxv2 import get_app, App, Result, MainTool\nfrom toolboxv2.utils.system.types import ToolBoxError\n\napp = get_app(\"MyToolModule\")\nexport = app.tb\n\nName = \"MyToolModule\"\nversion = \"0.5.0\"\n\nclass Tools(MainTool): # The class must be named 'Tools' for auto-detection by older App versions\n                      # or ensure your module file directly uses @export on methods if not named Tools.\n    # Or, you can export methods directly from any class:\n    # class MyCustomTool(MainTool):\n    #    @export(...)\n    #    def my_method(self, ...): ...\n\n    async def __ainit__(self): # Asynchronous initialization\n        # self.app is automatically available\n        # self.name, self.version, self.logger are set by MainTool's __await__\n        await super().__ainit__(name=Name, v=version, tool={\n            \"process_item\": self.process_item, # For older compatibility if functions were in 'tools' dict\n            \"get_status\": self.get_status\n        })\n        self.internal_state = \"initialized\"\n        self.app.print(f\"{self.name} (Tool) has been initialized with state: {self.internal_state}\")\n\n    @export(mod_name=Name, version=version) # Decorate methods to export them\n    def process_item(self, item_id: int, details: str) -&gt; Result:\n        # 'self' provides access to app, logger, name, version, config\n        self.app.logger.info(f\"{self.name} processing item: {item_id} - {details}\")\n        self.internal_state = f\"last_processed_{item_id}\"\n        if item_id == 0:\n            return self.return_result( # Helper from MainTool\n                error=ToolBoxError.input_error,\n                exec_code=1, # Custom error code\n                help_text=\"Item ID cannot be zero.\",\n                data_info=\"Validation failed\"\n            )\n        return Result.ok(data={\"item_id\": item_id, \"status\": \"processed by tool\"})\n\n    @export(mod_name=Name, version=version)\n    async def get_status(self) -&gt; str: # Example async method\n        await asyncio.sleep(0.01)\n        return f\"Tool {self.name} current state: {self.internal_state}\"\n\n    async def on_exit(self): # Not automatically called unless also decorated or part of a convention\n        self.app.print(f\"Tool {self.name} is shutting down its internal state.\")\n        # Perform cleanup\n\n# To ensure on_exit is called by the App framework:\n@export(mod_name=Name, version=version, exit_f=True)\nasync def custom_tool_exit_function(app_instance: App):\n    tool_instance = app_instance.get_mod(Name)\n    if tool_instance and hasattr(tool_instance, 'on_exit') and callable(tool_instance.on_exit):\n        await tool_instance.on_exit()\n</code></pre> <p>Key aspects of <code>MainTool</code>: *   Asynchronous Initialization: Use <code>async def __ainit__(self)</code> for setup. The <code>MainTool</code> itself is awaitable, and <code>__ainit__</code> is called when the instance is first awaited (e.g., by <code>app.load_mod</code> or <code>app.get_mod</code>). *   <code>self.app</code>: The <code>App</code> instance is available as <code>self.app</code>. *   <code>self.name</code>, <code>self.version</code>, <code>self.logger</code>: These are automatically set up. *   <code>self.return_result(...)</code>: A helper method for creating <code>Result</code> objects. *   Methods intended to be called via <code>app.run_any</code> should be decorated with <code>@export</code>.</p>"},{"location":"utils/#steps-to-create-a-valid-toolboxv2-module","title":"Steps to Create a Valid Toolboxv2 Module:","text":"<ol> <li>Define Module Structure: Organize your code with imports, metadata, and function/class definitions.</li> <li>Clarify Dependencies: Import necessary libraries. Handle missing optional dependencies gracefully if needed.</li> <li>Export Functions/Methods: Use the <code>@export(...)</code> decorator (e.g., <code>app.tb(...)</code>) to mark functions/methods that ToolBoxV2 should recognize.<ul> <li>Provide <code>mod_name</code> and <code>version</code>.</li> <li>Use other parameters (<code>api</code>, <code>row</code>, <code>initial</code>, <code>exit_f</code>, <code>memory_cache</code>, etc.) as needed.</li> <li>Ensure clear signatures and document parameters/return types (Python type hints are highly recommended).</li> </ul> </li> <li>Documentation and Versioning: Document your module and its functions. Use semantic versioning.</li> <li>Testing: Test your module thoroughly, including how it integrates with the ToolBoxV2 app (<code>app.run_any</code>, <code>app.get_mod</code>, etc.). Use the <code>test=True</code> and <code>samples</code> parameters in <code>@export</code> to facilitate automated testing via <code>app.execute_all_functions()</code>.</li> </ol>"},{"location":"utils/#to-isaa_1","title":"To isaa","text":""}]}