# LLM Gateway

**OpenAI-kompatible API fÃ¼r lokale LLM-Modelle mit llama.cpp**

Ein leichtgewichtiger Gateway-Server, der lokale GGUF-Modelle Ã¼ber eine OpenAI-kompatible REST-API bereitstellt. UnterstÃ¼tzt Text, Vision, Audio (Omni), Embeddings und TTS.

## Features

### ğŸš€ Core Features
- **OpenAI-kompatible API** - Drop-in Ersatz fÃ¼r OpenAI API
- **Multi-Model Slots** - Bis zu 7 Modelle gleichzeitig (Ports 4801-4807)
- **Smart Routing** - Automatische Modellauswahl basierend auf Request-Typ
- **Streaming** - Server-Sent Events fÃ¼r Chat Completions
- **Rate Limiting** - Konfigurierbare Limits pro User-Tier

### ğŸ¯ Modell-Typen
| Typ | Beschreibung | Capabilities |
|-----|--------------|--------------|
| `text` | Standard Chat-Modelle | Text |
| `vision` | Vision-Language Modelle (VL) | Text + Bild |
| `omni` | Multimodale Modelle | Text + Bild + Audio |
| `embedding` | Embedding-Modelle | Vektorisierung |
| `vision-embedding` | Vision + Embedding | Bild-Vektorisierung |
| `audio` | Whisper (Legacy) | Transkription |
| `tts` | Text-to-Speech | Sprachsynthese |

### ğŸ” User Management
- API-Key basierte Authentifizierung
- User-Tiers: `payg` (Pay-as-you-go), `sub` (Subscription), `admin`
- Balance-Tracking und Usage-Logging
- Signup-Request System

### ğŸ™ï¸ Live Voice API
- WebSocket-basierte Echtzeit-Sprachkonversation
- Wake-Word Detection (pre/post/mid Modi)
- Interrupt-Handling
- Paralleles LLM + TTS Streaming

## Installation

### Voraussetzungen
- Python 3.12+
- CMake, Git, Build Tools
- ~48GB RAM empfohlen fÃ¼r groÃŸe Modelle

### Linux/macOS
```bash
cd llm-gateway
chmod +x setup.sh
./setup.sh
```

### Windows
```powershell
cd llm-gateway
.\win_setup.ps1
```

### Mit ToolBoxV2 CLI
```bash
tb llm-gateway setup
tb llm-gateway start
```

## Konfiguration

Die Konfiguration liegt in `data/config.json`:

```json
{
  "slots": {
    "4801": null,
    "4802": null,
    "4803": null,
    "4804": null,
    "4805": null,
    "4806": null,
    "4807": null
  },
  "hf_token": "hf_xxx",
  "admin_key": "sk-admin-xxx",
  "default_threads": 10,
  "default_ctx_size": 8192,
  "pricing": {
    "input_per_1k": 0.0001,
    "output_per_1k": 0.0002
  },
  "rate_limits": {
    "payg": 25,
    "sub": 100
  },
  "performance": {
    "flash_attention": true,
    "mlock": true,
    "kv_cache_quantization": "q8_0",
    "batch_size": 512
  }
}
```

## API Endpoints

### OpenAI-kompatibel

| Endpoint | Methode | Beschreibung |
|----------|---------|--------------|
| `/v1/models` | GET | Liste verfÃ¼gbarer Modelle |
| `/v1/chat/completions` | POST | Chat Completion (Streaming) |
| `/v1/embeddings` | POST | Text Embeddings |
| `/v1/audio/transcriptions` | POST | Audio Transkription |
| `/v1/audio/speech` | POST | Text-to-Speech |

### Live Voice API

| Endpoint | Methode | Beschreibung |
|----------|---------|--------------|
| `/v1/audio/live` | POST | Session erstellen |
| `/v1/audio/live/ws/{token}` | WS | WebSocket Verbindung |
| `/v1/audio/live/{token}` | GET | Session Info |
| `/v1/audio/live/{token}` | DELETE | Session beenden |

### Admin Endpoints

| Endpoint | Methode | Beschreibung |
|----------|---------|--------------|
| `/admin/api/slots` | GET | Slot-Status |
| `/admin/api/slots/load` | POST | Modell laden |
| `/admin/api/slots/{slot}/unload` | POST | Modell entladen |
| `/admin/api/models/local` | GET | Lokale Modelle |
| `/admin/api/models/search` | GET | HuggingFace suchen |
| `/admin/api/models/download` | POST | Modell herunterladen |
| `/admin/api/users` | GET/POST | User-Verwaltung |
| `/admin/api/system` | GET | System-Stats |

### User Endpoints

| Endpoint | Methode | Beschreibung |
|----------|---------|--------------|
| `/user/api/me` | GET | User-Info |
| `/user/api/usage` | GET | Usage-Statistiken |
| `/user/api/models` | GET | VerfÃ¼gbare Modelle |
| `/user/api/ratelimit` | GET | Rate-Limit Status |

### Public Endpoints (ohne Auth)

| Endpoint | Methode | Beschreibung |
|----------|---------|--------------|
| `/health` | GET | Health Check |
| `/api/models` | GET | Modell-Liste |
| `/api/uptime` | GET | Uptime-Historie |
| `/api/signup` | POST | Signup-Request |

## Web Interfaces

- **Landing Page**: `http://localhost:4000/`
- **Admin Panel**: `http://localhost:4000/admin/`
- **User Dashboard**: `http://localhost:4000/user/`
- **Playground**: `http://localhost:4000/playground/`
- **Live Voice**: `http://localhost:4000/live`

## Verwendung

### Server starten
```bash
# Mit venv
source venv/bin/activate  # Linux/macOS
.\venv\Scripts\activate   # Windows

uvicorn server:app --host 0.0.0.0 --port 4000

# Mit ToolBoxV2
tb llm-gateway start
```

### Modell laden (Admin)
```bash
curl -X POST http://localhost:4000/admin/api/slots/load \
  -H "Authorization: Bearer sk-admin-xxx" \
  -H "Content-Type: application/json" \
  -d '{
    "slot": 4801,
    "model_path": "Qwen2.5-7B-Instruct-Q4_K_M.gguf",
    "model_type": "text"
  }'
```

### Chat Completion
```bash
curl http://localhost:4000/v1/chat/completions \
  -H "Authorization: Bearer sk-xxx" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen",
    "messages": [{"role": "user", "content": "Hello!"}],
    "stream": true
  }'
```

### Python Client
```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:4000/v1",
    api_key="sk-xxx"
)

response = client.chat.completions.create(
    model="qwen",
    messages=[{"role": "user", "content": "Hello!"}],
    stream=True
)

for chunk in response:
    print(chunk.choices[0].delta.content, end="")
```

## Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM Gateway (Port 4000)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   FastAPI   â”‚  â”‚ Rate Limit  â”‚  â”‚   Auth (API Keys)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                â”‚                     â”‚            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    Smart Router                         â”‚ â”‚
â”‚  â”‚  (Model Selection based on capabilities)                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                       â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
â”‚  4801   â”‚  â”‚  4802   â”‚  â”‚  4803   â”‚  â”‚  ...    â”‚
â”‚  text   â”‚  â”‚ vision  â”‚  â”‚  omni   â”‚  â”‚         â”‚
â”‚ llama-  â”‚  â”‚ llama-  â”‚  â”‚ llama-  â”‚  â”‚         â”‚
â”‚ server  â”‚  â”‚ server  â”‚  â”‚ server  â”‚  â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Verzeichnisstruktur

```
llm-gateway/
â”œâ”€â”€ server.py           # Haupt-Server (FastAPI)
â”œâ”€â”€ model_manager.py    # Modell-Verwaltung
â”œâ”€â”€ live_handler.py     # Live Voice API
â”œâ”€â”€ main.py             # Entry Point
â”œâ”€â”€ requirements.txt    # Python Dependencies
â”œâ”€â”€ setup.sh            # Linux Setup
â”œâ”€â”€ win_setup.ps1       # Windows Setup
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ config.json     # Konfiguration
â”‚   â”œâ”€â”€ gateway.db      # SQLite Datenbank
â”‚   â””â”€â”€ models/         # GGUF Modelle
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ index.html      # Landing Page
â”‚   â”œâ”€â”€ admin.html      # Admin Panel
â”‚   â”œâ”€â”€ user.html       # User Dashboard
â”‚   â”œâ”€â”€ playground.html # Chat Playground
â”‚   â””â”€â”€ live-playground.html  # Voice Playground
â””â”€â”€ build/              # llama.cpp Build
```

## Performance-Optimierungen

- **Flash Attention**: 20-30% Speedup
- **mlock**: Verhindert Swapping
- **KV-Cache Quantization**: Spart RAM bei groÃŸem Context
- **Continuous Batching**: Parallele Request-Verarbeitung

## Troubleshooting

### Modell lÃ¤dt nicht
- PrÃ¼fe RAM-VerfÃ¼gbarkeit (`/admin/api/system`)
- ErhÃ¶he Timeout fÃ¼r groÃŸe Modelle
- PrÃ¼fe mmproj fÃ¼r Vision-Modelle

### Rate Limit erreicht
- Warte 60 Sekunden
- Upgrade auf hÃ¶heren Tier
- Admin hat kein Limit

### WebSocket Verbindung fehlgeschlagen
- PrÃ¼fe Session-Token GÃ¼ltigkeit
- Session lÃ¤uft nach 15-20 Minuten ab

## Lizenz

Teil des ToolBoxV2 Projekts.
